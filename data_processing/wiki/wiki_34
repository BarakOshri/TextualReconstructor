<doc id="5224" url="http://en.wikipedia.org/wiki?curid=5224" title="Citizen Kane">
Citizen Kane

Citizen Kane is a 1941 American drama film directed, co-written, produced by, and starring Orson Welles. The picture was Welles' first feature film. The film was nominated for Academy Awards in nine categories; it won an Academy Award for Best Writing (Original Screenplay) by Herman Mankiewicz and Welles. Considered by many critics, filmmakers, and fans to be the greatest film ever made, "Citizen Kane" was voted the greatest film of all time in five consecutive "Sight & Sound"s polls of critics, until it was displaced by "Vertigo" in the 2012 poll. It topped the American Film Institute's 100 Years ... 100 Movies list in 1998, as well as AFI's 2007 update. "Citizen Kane" is particularly praised for its cinematography, music, and narrative structure, which were innovative for its time.
The story is a "film à clef" that examines the life and legacy of Charles Foster Kane, played by Welles, a character based in part upon the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick, and aspects of Welles' own life. Upon its release, Hearst prohibited mention of the film in any of his newspapers. Kane's career in the publishing world is born of idealistic social service, but gradually evolves into a ruthless pursuit of power. Narrated principally through flashbacks, the story is told through the research of a newsreel reporter seeking to solve the mystery of the newspaper magnate's dying word: "Rosebud".
After his success in the theatre with his Mercury Players, and his controversial 1938 radio broadcast of "The War of the Worlds" on "The Mercury Theatre on the Air", Welles was courted by Hollywood. He signed a contract with RKO Pictures in 1939. Unusual for an untried director, he was given the freedom to develop his own story, to use his own cast and crew, and to have final cut privilege. Following two abortive attempts to get a project off the ground, he wrote the screenplay for "Citizen Kane", collaborating on the effort with Herman Mankiewicz. Principal photography took place in 1940 and the film received its American release in 1941.
While a critical success, "Citizen Kane" failed to recoup its costs at the box office. The film faded from view after its release but was subsequently returned to the fore when it was praised by such French critics as Jean-Paul Sartre and André Bazin and given an American revival in 1956.
The film was released on Blu-ray Disc on September 13, 2011, for a special 70th anniversary edition. The Anniversary Edition comes with many bonuses, including storyboards, call sheets, a book, among others. 
Plot.
Charles Foster Kane, an enormously wealthy newspaper publisher, has been living alone in Florida on his vast palatial estate, Xanadu, for the last years of his life, with a "No Trespassing" sign on the gate. On his deathbed, he holds a snow globe and utters the single word, "Rosebud", before dying; the globe slips from his hand and smashes on the floor. Kane's death becomes sensational news around the world. Newsreel reporter Jerry Thompson becomes intrigued, and decides to learn all he can about Kane's private life to discover the meaning of "Rosebud".
The reporter interviews the great man's friends and associates, and Kane's story unfolds as a series of flashbacks. Thompson approaches Kane's second wife, Susan Alexander, now an alcoholic who runs her own nightclub, but she refuses to tell him anything and demands that he leave. Thompson then goes to the private archive of the late Walter Parks Thatcher, a banker who served as Kane's guardian during his childhood and adolescence. Through Thatcher's written memoirs, Thompson learns about Kane's childhood. Thompson then interviews Kane's personal business manager, Mr. Bernstein; his estranged best friend, Jedediah Leland; Susan, for a second time, successfully this time; and, finally, his butler, Raymond, at the Xanadu estate.
These flashbacks reveal that Kane's childhood was spent in poverty in Colorado (his parents ran a boarding house), until "the world's third largest gold mine" was discovered on the seemingly worthless property his mother had acquired. His mother, Mary, sends him away to the East to live with Thatcher, so that he may be properly educated. After gaining full control over his trust fund at the age of 25, Kane enters the newspaper business and embarks on a career of yellow journalism. He takes control of the newspaper, the "New York Inquirer", and hires the best journalists available. He then rises to power by successfully manipulating public opinion regarding the Spanish American War, marrying the niece of a President of the United States, and campaigning for the office of Governor of New York.
Kane's marriage disintegrates over the years, and he begins an affair with Susan Alexander, a singer. Both his wife and his political opponent discover the affair and this brings an abrupt end to both his marriage and his political aspirations. Kane marries Susan, and forces her into a humiliating operatic career for which she has neither the talent nor the ambition. Kane finally allows her to abandon her singing career after she attempts suicide. After years spent in boredom and isolation on the Xanadu estate, constantly under his dominance, Susan ultimately leaves Kane.
Kane spends his last years building his vast estate and lives alone, interacting only with his staff. The butler recounts that Kane had said "Rosebud" after Susan left him, right after seeing and pocketing a snow globe.
Back at Xanadu, Kane's vast number of belongings are catalogued: priceless works of art are intermingled with worthless pieces of modern furniture. Thompson finds that he is unable to solve the mystery and concludes that the meaning of "Rosebud" will forever remain an enigma. He theorizes that "Mr. Kane was a man who got everything he wanted, and then lost it. Maybe Rosebud was something he couldn't get, or something he lost." As the film ends, the camera reveals that Rosebud was the name of a sled from Kane's childhood–an allusion to the only time in his life that he was truly happy. The sled, thought to be junk, is burned in a basement furnace by Xanadu's departing staff.
Cast and characters.
The cast of "Citizen Kane" is listed at the American Film Institute Catalog of Feature Films.
The film's closing credits read, "Most of the principal actors are new to motion pictures. The Mercury Theatre is proud to introduce them." Welles, along with his partner John Houseman, had assembled them into a group known as the Mercury Players to perform his productions in the Mercury Theatre in 1937. After accepting his Hollywood contract in 1939, Welles worked between Los Angeles and New York where the Mercury Theatre continued their weekly radio broadcasts for "The Campbell Playhouse". Welles had wanted all the Mercury Players to debut in his first film, but the cancellation of "The Heart of Darkness" project in December 1939 created a financial crisis for the group and some of the actors worked elsewhere. This caused friction between Welles and Houseman, and their partnership ended.
RKO executives were dismayed that so many of the major roles went to unknowns, but Welles's contract left them with no say in the matter. The film features debuts from William Alland, Agnes Moorehead, Everett Sloane, Ruth Warrick and Welles himself.
Production.
Development.
Welles's notoriety following "The War of the Worlds" broadcast earned him Hollywood's interest, and RKO studio head George J. Schaefer's unusual contract. Welles made a deal with Schaefer on July 21, 1939, to produce, direct, write, and act in two feature films. The studio had to approve the story and the budget if it exceeded $500,000. Welles was allowed to develop the story without interference, cast his own actors and crew members, and have the privilege of final cut – unheard of at the time for a first-time director. He had spent the first five months of his RKO contract trying to get several projects going with no success. The "Hollywood Reporter" said, "They are laying bets over on the RKO lot that the Orson Welles deal will end up without Orson ever doing a picture there." First, Welles tried to adapt "Heart of Darkness", but there was concern over the idea of depicting it entirely with point of view shots. Welles considered adapting Cecil Day-Lewis' novel "The Smiler With The Knife", but realized that to challenge himself with a new medium, he had to write an original story.
Screenwriter Herman J. Mankiewicz, recuperating from a car accident, was in-between jobs. He had originally been hired by Welles to work on "The Campbell Playhouse" radio program and was available to work on the screenplay for Welles's film. The writer had only received two screenplay credits between 1935 and when he began work on "Citizen Kane", and he needed the job. There is dispute amongst historians regarding whose idea it was to use William Randolph Hearst as the basis for Charles Foster Kane. Welles claimed it was his idea while film critic Pauline Kael (in her 1971 essay "Raising Kane") and Welles's former business partner, John Houseman, claim that it was Mankiewicz's idea. For some time, Mankiewicz had wanted to write a screenplay about a public figure – perhaps a gangster – whose story would be told by the people that knew him.
Mankiewicz had already written an unperformed play about John Dillinger entitled "The Tree Will Grow". Welles liked the idea of multiple viewpoints but was not interested in playing Dillinger. Mankiewicz and Welles talked about picking someone else to use as a model. They hit on the idea of using Hearst as their central character. Mankiewicz had frequented Hearst's parties until his alcoholism got him barred. The writer resented this and became obsessed with Hearst and Marion Davies. Hearst had great influence and the power to retaliate within Hollywood, so Welles had Mankiewicz work on the script outside of the city. Because of the writer's drinking problem, Houseman went along to provide assistance and make sure that he stayed focused. Welles also sought inspiration from Howard Hughes and Samuel Insull (who built an opera house for his wife). Although Mankiewicz and Houseman got on well with Welles, they incorporated some of his traits into Kane, such as his temper.
During production, "Citizen Kane" was referred to as "RKO 281". Most of the filming took place between June 29, 1940, and October 23, 1940, in what is now Stage 19 on the Paramount lot in Hollywood. There was some location filming at Balboa Park in San Diego and the San Diego Zoo, and still photographs of Oheka Castle in Huntington, New York, were used in the opening montage, representing Kane's Xanadu estate. Welles prevented studio executives of RKO from visiting the set. He understood their desire to control projects, and he knew they were expecting him to do an exciting film that would correspond to his "The War of the Worlds" radio broadcast. Welles's RKO contract had given him complete control over the production of the film when he signed on with the studio, something that he never again was allowed to exercise when making motion pictures. According to an RKO cost sheet from May 1942, the film cost $839,727 compared to an estimated budget of $723,800.
Screenplay.
Development.
Mankiewicz as co-writer.
Robert Carringer, author of "The Making of Citizen Kane" (1985), described the early stages of the screenplay:
Welles's first step toward the realization of "Citizen Kane" was to seek the assistance of a screenwriting professional. Fortunately, help was near at hand. . . . When Welles moved to Hollywood, it happened that a veteran screenwriter, Herman Mankiewicz, was recuperating from an automobile accident and between jobs ... Mankiewicz was an expatriate from Broadway who had been writing for films for almost fifteen years.
Mankiewicz met newspaper magnate William Randolph Hearst as a result of his friendship with Charles Lederer, another Hollywood screenwriter, who was a close nephew of Marion Davies, Hearst's mistress. Pauline Kael wrote that "Mankiewicz found himself on story-swapping terms with the power behind it all, Hearst himself ... through his friendship with Charles Lederer." Mankiewicz eventually saw Hearst as "... a finagling, calculating, Machiavellian figure," noted Kael, "and he and Lederer often wrote and had printed parodies of Hearst newspapers ..."
Mankiewicz, according to film author Harlan Lebo, was also "… one of Hollywood's most notorious personalities." Mankiewicz was the older brother of producer-director Joseph Mankiewicz, was a former writer for "The New Yorker" and "The New York Times" before moving to Hollywood in 1926. By the time Welles contacted him he had "... established himself as a brilliant wit, a writer of extraordinary talent, [and] a warm friend to many of the screen world's brightest artists ... [he] produced dialogue of the highest caliber."
"Herman Mankiewicz was a legendary figure in Hollywood," wrote Welles's associate John Houseman:
The son of a respected New Jersey schoolteacher, one of a brilliant class at Columbia, he had fought the war as a Marine, worked for the "World" and the "Times", collaborated on two unsuccessful plays with two otherwise infallibly successful playwrights, George Kaufman and Marc Connelly, come to California for six weeks to work on a silent film for Lon Chaney and stayed for sixteen years as one of the highest paid and most troublesome men in the business. His behavior, public and private, was a scandal. A neurotic drinker and a compulsive gambler, he was also one of the most intelligent, informed, witty, humane and charming men I have ever known.
Speaking with Peter Bogdanovich in February 1969, Orson Welles said, "Nobody was "more" miserable, "more" bitter, and "funnier" than Mank ... a perfect monument of self-destruction. But, you know, when the bitterness wasn't focused straight at you, he was the best company in the world." When Bogdanovich asked how important Mankiewicz was to the "Citizen Kane" script, Welles responded, "Mankiewicz's contribution? It was enormous."
Welles had engaged Mankiewicz to do script work on the stalled film project "The Smiler with a Knife". Despite a violent quarrel with Houseman in December 1939, after which Houseman had resigned from the Mercury, Welles arranged a lunch at New York's 21 Club with his former partner, and proposed that he work with Mankiewicz on a new project — "… little more than a notion, but an exciting one …," Houseman wrote:
Mankiewicz was notoriously unreliable: I asked Orson why he didn't take over the idea and write it himself. He said he didn't want to do that. Besides, Mank had asked for me to work with him. In the name of our former association Orson urged me to fly out, talk to Mankiewicz and, if I shared his enthusiasm, stay and work with him as his collaborator and editor till the script was done. It was an absurd venture, and that night Orson and I flew back to California together.
In February 1940 Mankiewicz was put on the Mercury payroll to work on a script with Houseman, a screenplay initially called "Orson Welles #1", then "American", and finally, "Citizen Kane". Writing took place from late February or March through early May 1940.
After finishing the script for "Citizen Kane", Mankiewicz gave a copy to Lederer, which Kael regarded as foolish:
He was so proud of his script that he lent a copy to Charles Lederer. In some crazily naive way, Mankiewicz seems to have imagined that Lederer would be pleased by how good it was. But Lederer, apparently, was deeply upset and took the script to his aunt and Hearst. It went from them to Hearst's lawyers … It was probably as a result of Mankiewicz's idiotic indiscretion that the various forces were set in motion that resulted in the cancellation of the premiere at the Radio City Music Hall [and] the commercial failure of "Citizen Kane".
Lederer, however, told director Peter Bogdanovich that Kael was wrong in her conclusion, and that she never bothered to check with him about the facts. Lederer said he did not give Davies the script Mankiewicz loaned him: "I gave it "back" to him. He asked me if I thought Marion would be offended and I said I didn't think so."
Ideas and collaboration.
According to film historian Clinton Heylin, "... the idea of "Citizen Kane" was the original conception of Orson Welles, who in early 1940 first discussed the idea with John Houseman, who then suggested that both he and Welles leave for Los Angeles and discuss the idea with scriptwriter Herman Mankiewicz." He adds that Mankiewicz "... probably believed that Welles had little experience as an original scriptwriter ... [and] may even have felt that "John Citizen USA", Welles's working title, was a project he could make his own."
When Houseman returned to California, he sat by the bedside of Mankiewicz — who was convalescing with a triple fracture of his left leg — and heard the basic outline of the story. "It was something he had been thinking about for years," Houseman wrote, "the idea of telling a man's private life (preferably one that suggested a recognizable American figure), immediately following his death, through the intimate and often incompatible testimony of those who had known him at different times and in different circumstances."
Welles himself had ideas that meshed with that concept, as he described in a 1969 interview in the book, "This is Orson Welles":
I'd been nursing an old notion – the idea of telling the same thing several times – and showing exactly the same thing from wholly different points of view. Basically, the idea "Rashomon" used later on. Mank liked it, so we started searching for the man it was going to be about. Some big American figure – couldn't be a politician, because you'd have to pinpoint him. Howard Hughes was the first idea. But we got pretty quickly to the press lords.
Hearst as story model.
According to film critic and author Pauline Kael, Mankiewicz "... was already caught up in the idea of a movie about Hearst ..." when he was still working at "The New York Times" in 1925. She learned from his family's babysitter, Marion Fisher, that she once typed as "... he dictated a screenplay, organized in flashbacks. She recalls that he had barely started on the dictation, which went on for several weeks, when she remarked that it seemed to be about William Randolph Hearst, and he said, 'You're a smart girl.'"
In Hollywood, Mankiewicz had frequented Hearst's parties until his alcoholism got him barred. Hearst was also a person known to Welles. "Once that was decided", wrote author Don Kilbourne, "Mankiewicz, Welles, and John Houseman, a cofounder of the Mercury Theatre, rented a place in the desert, and the task of creating "Citizen Kane" began." This "place in the desert" was on the historic Verde ranch on the Mojave River in Victorville. In later years, Houseman gave Mankiewicz "total" credit for "the creation of "Citizen Kane's" script" and credited Welles with "the visual presentation of the picture."
Mankiewicz was put under contract by Mercury Productions and was to receive no credit for his work as he was hired as a script doctor. According to his contract with RKO, Welles would be given sole screenplay credit, and had already written a rough script consisting of 300 pages of dialogue with occasional stage directions under the title of "John Citizen, USA".
In an interview with Huw Weldon on March 13, 1960, Orson Welles said: "Mr. Hearst was quite a bit like Kane, although Kane isn't really founded on Hearst in particular, many people sat for it so to speak".
Authorship.
One of the long standing debates of "Citizen Kane" has been the proper accreditation of the authorship of the screenplay, which the credits attribute to both Herman J. Mankiewicz and Orson Welles. Mankiewicz biographer Richard Meryman notes that the dispute had various causes, including the way the film was promoted. For instance, when RKO opened the film in New York City on May 1, 1941, followed by showings at theaters in other large cities, the publicity programs that were printed included photographs of Welles as "... the one-man band, directing, acting, and writing." In a letter to his father afterward, Mankiewicz wrote, "I'm particularly furious at the incredibly insolent description of how Orson wrote his masterpiece. The fact is that there isn't one single line in the picture that wasn't in writing – writing from and by me – before ever a camera turned."
Film historian Otto Friedrich wrote, "... it made [Mankiewicz] unhappy to hear Welles quoted in Louella Parsons's column, before the question of screen credits was officially settled, as saying, 'So I wrote "Citizen Kane". ... Welles later claimed that he planned on a joint credit all along, but Mankiewicz claimed that Welles offered him a bonus of ten thousand dollars if he would let Welles take full credit."
Controversy over the authorship of the "Citizen Kane" screenplay was revived in 1971 by film critic Pauline Kael, whose essay, "Raising Kane," was printed in two installments in "The New Yorker" (February 20 and 27, 1971), and subsequently collected in "The Citizen Kane Book" (1971). According to Kael, Rita Alexander, Mankiewicz's personal secretary, stated that she "... took the dictation from Mankiewicz from the first paragraph to the last ... and later did the final rewriting and the cuts, and handled the script at the studio until after the film was shot. ... [and said] Welles didn't write (or dictate) one line of the shooting script of "Citizen Kane."" She added that "Welles himself came to dinner once or twice ... [and] she didn't meet him until after Mankiewicz had finished dictating the long first draft." However, Welles had his own secretary, Kathryn Trosper, who typed up Welles's suggestions and corrections, which were incorporated into the final script; Kael did not interview Trosper before producing her article.
Nevertheless, Kael maintained that Mankiewicz went to the Writers Guild and declared that he was the original author. According to Kael, "... he had ample proof of his authorship, and when he took his evidence to the Screen Writers Guild ... Welles was forced to split the credit and take second place in the listing." Charles Lederer, a screenwriter and a source for Kael's article, insisted that the credit never came to the Screen Writers Guild for arbitration.
Kael argued that Mankiewicz was the true author of the screenplay and therefore responsible for much of what made the film great. This angered many critics of the day, most notably critic-turned-filmmaker Peter Bogdanovich, a close friend of Welles who rebutted Kael's claims in an October 1972 article for "Esquire" titled "The "Kane" Mutiny." Other rebuttals included articles by Joseph McBride ("Film Heritage", Fall 1971) and Jonathan Rosenbaum ("Film Comment", Spring 1972 and Summer 1972), interviews with George Coulouris and Bernard Herrmann that appeared in "Sight & Sound" (Spring 1972), and remarks in Welles biographies by Barbara Leaming and Frank Brady. Rosenbaum also reviewed the controversy in his editor's notes to "This is Orson Welles" (1992).
"I happen to disagree with the premise of the whole book, because she tries to pretend that Welles is nothing and that a mediocre writer by the name of Mankiewicz was a hidden Voltaire," Bernard Herrmann said during a question-and-answer session following an October 1973 lecture at the George Eastman House Museum in Rochester, New York. "I'm not saying that Mankiewicz made no contribution. The titles clearly credit him. Orson says that he did make a valuable contribution. But really, without Orson, all of Mankiewicz's other pictures were nothing, before and after. With Orson, however, something happened to this wonderful man, but he could not have created "Citizen Kane"."
Robert L. Carringer likewise rebutted Kael's conclusions in an article titled "The Scripts of "Citizen Kane"" for the Winter 1978 edition of "Critical Enquiry". Carringer refers to early script drafts with Welles's incorporated handwritten contributions, and mentions the issues raised by Kael rested on the evidence of an early draft which was mostly written by Mankiewicz. However Carringer points out that subsequent drafts clarified Welles's contribution to the script:
Fortunately enough evidence to settle the matter has survived. A virtually complete set of script records for "Citizen Kane" has been preserved in the archives of RKO General Pictures in Hollywood, and these provide almost a day-to-day record of the history of the scripting ... The full evidence reveals that Welles's contribution to the "Citizen Kane" script was not only substantial but definitive.
Carringer notes that Mankiewicz' principal contribution was on the first two drafts of the screenplay, which he characterizes as being more like "rough gatherings" than actual drafts. Houseman accompanied Mankiewicz so as to ensure that the latter's drinking problem did not affect the screenplay. The early drafts established "... the plot logic and laid down the overall story contours, established the main characters, and provided numerous scenes and lines that would eventually appear in one form or another in the film." However he also noted that Kane in the early draft remained a caricature of Hearst rather than the fully developed character of the final film. The main quality missing in the early drafts but present in the final film is "... the stylistic wit and fluidity that is the most engaging trait of the film itself."
According to film critic David Thomson, "No one can now deny Herman Mankiewicz credit for the germ, shape, and pointed language of the screenplay, but no one who has seen the film as often as it deserves to be seen would dream that Welles is not its only begetter." Carringer considered that at least three scenes were solely Welles's work and, after weighing both sides of the argument, including sworn testimony from Mercury assistant Richard Baer, concluded, "We will probably never know for sure, but in any case Welles had at last found a subject with the right combination of monumentality, timeliness, and audacity." Harlan Lebo agrees, and adds, "... of far greater relevance is reaffirming the importance of the efforts that both men contributed to the creation of Hollywood's greatest motion picture."
Carringer notes that "Citizen Kane" was unusual in relation to his later films in that it was original material rather than adaptations of existing sources. He cites that Mankiewicz's main contribution was providing him with "... what any good first writer ought to be able to provide in such a case: a solid, durable story structure on which to build."
For his part, Welles stated the process of collaborating with Mankiewicz on the "Citizen Kane" screenplay in a letter to "The Times" (London), November 17, 1971:
The initial ideas for this film and its basic structure were the result of direct collaboration between us; after this we separated and there were two screenplays: one written by Mr. Mankiewicz, in Victorville, and the other, in Beverly Hills, by myself. ... The final version of the screenplay ... was drawn from both sources.
In his 1982 chronicle of the studio, "The RKO Story", scholar Richard B. Jewell concluded the following:
Besides producing, directing and playing the role of Kane, Welles deserved his co-authorship credit (with Herman J. Mankiewicz) on the screenplay. Film critic Pauline Kael argues otherwise in a 50,000 word essay on the subject, but her case against Welles is one-sided and unsupported by the facts.
Sources.
Charles Foster Kane.
Orson Welles never confirmed a principal source for the character of Charles Foster Kane. John Houseman, who edited and collaborated on the draft of the script written by Herman Mankiewicz, wrote that Kane is a synthesis of different personalities:
For the basic concept of Charles Foster Kane and for the main lines and significant events of his public life, Mankiewicz used as his model the figure of William Randolph Hearst. To this were added incidents and details invented or derived from other sources.
Houseman set out the obvious parallels, "... to which were grafted anecdotes from other giants of journalism, including Pulitzer, Northcliffe and Mank's first boss, Herbert Bayard Swope."
The film is commonly regarded as a fictionalized, unrelentingly hostile parody of William Randolph Hearst, in spite of Welles's statement that ""Citizen Kane" is the story of a wholly fictitious character." According to film historian Don Kilbourne, "... much of the information for "Citizen Kane" came from already-published material about Hearst ... [and] some of Kane's speeches are almost verbatim copies of Hearst's. When Welles denied that the film was about the still-influential publisher, he did not convince many people."
Hearst biographer David Nasaw finds the film's depiction of Hearst unfair:
Welles' "Kane" is a cartoon-like caricature of a man who is hollowed out on the inside, forlorn, defeated, solitary because he cannot command the total obedience, loyalty, devotion, and love of those around him. Hearst, to the contrary, never regarded himself as a failure, never recognized defeat, never stopped loving Marion [Davies] or his wife. He did not, at the end of his life, run away from the world to entomb himself in a vast, gloomy art-choked hermitage. Orson Welles may have been a great filmmaker, but he was neither a biographer nor a historian.
Arguing for the release of "Citizen Kane" before the RKO board, Welles pointed out the irony that it was Hearst himself who had brought so much attention to the film being about him, and that it was his own columnist, Louella Parsons, who was doing the most to publicize Kane's identification with Hearst. Public denials aside, Welles held the view that Hearst was a public figure and that the facts of a public figure's life were available for writers to reshape and restructure into works of fiction. Welles's legal advisor, Arnold Weissberger, put the issue in the form of a rhetorical question: "Will a man be allowed in effect to copyright the story of his life?"
In an interview in the 1992 book "This is Orson Welles", Welles said that he had excised one scene from Mankiewicz's first draft that had certainly been based on Hearst. "In the original script we had a scene based on a notorious thing Hearst had done, which I still cannot repeat for publication. And I cut it out because I thought it hurt the film and wasn't in keeping with Kane's character. If I'd kept it in, I would have had no trouble with Hearst. He wouldn't have dared admit it was him.
Pauline Kael wrote that a vestige of this abandoned subplot survives in a remark made by Susan Alexander to the reporter interviewing her: "Look, if you're smart, you'll get in touch with Raymond. He's the butler. You'll learn a lot from him. He knows where all the bodies are buried." Kael observed, "It's an odd, cryptic speech. In the first draft, Raymond "literally" knew where the bodies were buried: Mankiewicz had dished up a nasty version of the scandal sometimes referred to as the Strange Death of Thomas Ince." Referring to the suspicious 1924 death of the American film mogul after being a guest on Hearst's yacht, and noting that Kael's principal source was John Houseman, film scholar Jonathan Rosenbaum wrote that "it seems safe to conclude, even without her prodding, that some version of the story must have cropped up in Mankiewicz's first draft of the script, which Welles subsequently edited and added to."
One particular aspect of the character, Kane's profligate collecting of possessions, was directly taken from Hearst. "And it's very curious – a man who spends his entire life paying cash for objects he never looked at," Welles said. "He just acquired things, most of which were never opened, remained in boxes. It's really a quite accurate picture of Hearst to that extent." But Welles himself insisted that there were marked differences between his fictional creation and Hearst. He acknowledged that aspects of Kane were drawn from the lives of two business tycoons familiar from Welles's youth in Chicago — Samuel Insull and Harold Fowler McCormick.
A financier closely associated with Thomas Edison, Samuel Insull (1859–1938) was a man of humble origins who became the most powerful figure in the utilities field. He was married to a Broadway ingenue nearly 20 years his junior, and built the Chicago Civic Opera House. In 1925, after a 26-year absence, Gladys Wallis Insull returned to the stage in a charity revival of "The School for Scandal" that ran two weeks in Chicago. When the performance was repeated on Broadway in October 1925, Herman Mankiewicz — then the third-string theatre critic for "The New York Times" — was assigned to review the production. In an incident that became infamous, Mankiewicz returned to the press room drunk and wrote only the first sentence of a negative review before passing out on his typewriter. Mankiewicz resurrected the experience in writing the screenplay for "Citizen Kane", incorporating it into the narrative of Jedediah Leland.
In 1926 Insull took a six-year lease on Chicago's Studebaker Theatre and financed a repertory company in which his wife starred. Gladys Insull's nerves broke when her company failed to find success, and the lease expired at the same time Insull's $3 billion financial empire collapsed in the Depression. Like that of Charles Foster Kane, the life of Samuel Insull ended in bankruptcy and disgrace.
Like Kane, Harold McCormick was divorced by his aristocratic first wife, Edith Rockefeller, and lavishly promoted the opera career of his only modestly talented second wife, Ganna Walska.
According to composer David Raksin, Bernard Herrmann used to say that much of Kane's story was based on McCormick, but that there was also a good deal of Orson Welles himself in the flamboyant character. Welles lost his mother when he was nine years old and his father when he was 15. After this, he became the ward of Chicago's Dr. Maurice Bernstein. Bernstein is the last name of the only major character in "Citizen Kane" who receives a generally positive portrayal. Although Dr. Bernstein was nothing like the character in the film, Welles said, the use of the name "Bernstein" was a family joke. "I used to call people 'Bernstein' on the radio, all the time, too – just to make him laugh. ... Mank did all the best writing for Bernstein. I'd call that "the" most valuable thing he gave us."
Welles cited financier Basil Zaharoff as another inspiration for Kane. "I got the idea for the hidden-camera sequence in the Kane 'news digest' from a scene I did on "March of Time" in which Zaharoff, this great munitions-maker, was being moved around in his rose garden, just talking about the roses, in the last days before he died," Welles said. Robert L. Carringer reviewed the December 3, 1936, script of the radio obituary in which Welles played Zaharoff, and found other similarities. In the opening scene, Zaharoff's secretaries are burning masses of secret papers in the enormous fireplace of his castle. A succession of witnesses testify about the tycoon's ruthless practices. "Finally, Zaharoff himself appears — an old man nearing death, alone except for his servants in the gigantic palace in Monte Carlo that he had acquired for his longtime mistress. His dying wish is to be wheeled out 'in the sun by that rosebush.'"
Jedediah Leland.
In Hollywood in 1940, Orson Welles invited longtime friend and Mercury Theatre colleague Joseph Cotten to join a small group reading the script aloud for the first time. They got together around the pool at the Beverly Hills home of Herman Mankiewicz, Cotten wrote:
"I think I'll just listen," Welles said. "The title of this movie is "Citizen Kane", and I play guess who." He turned to me. "Why don't you think of yourself as Jedediah Leland? His name, by the way, is a combination of Jed Harris and your agent, Leland Hayward." "There all resemblance ceases," Herman reassured me. These afternoon garden readings continued, and as the Mercury actors began arriving, the story started to breathe.
"I regard Leland with enormous affection," Orson Welles told filmmaker Peter Bogdanovich. He explained to Bogdanovich that the character of Jed Leland was based on drama critic Ashton Stevens, George Stevens's uncle and a close boyhood friend of Welles:
What I knew about Hearst came more from him than from my father – though my father did know him well ... But Ashton had taught Hearst to play the banjo, which is how he first got to be a drama critic, and, you know, Ashton was really one of the great ones. The last of the dandies – he worked for Hearst for some 50 years or so, and adored him. A gentleman ... very much like Jed.
Regarded as the dean of American drama critics, Ashton Stevens (1872–1951) began his journalism career in 1894 in San Francisco and started working for the Hearst newspapers three years later. In 1910 he moved to Chicago, where he covered the theatre for 40 years and became a close friend of Orson Welles's guardian, Dr. Maurice Bernstein.
Screenwriter Herman Mankiewicz incorporated an incident from his own early career as a theatre critic for "The New York Times" into the narrative of Jed Leland. Mankiewicz was assigned to review the October 1925 opening of "The School for Scandal" — a production that marked the return of Gladys Wallis to the Broadway stage. A famous ingenue of the 1890s, Wallis had retired upon her marriage to Chicago utilities magnate Samuel Insull but now, 26 years later, used her husband's fortune to form her own repertory company. After her opening-night performance in the role of Lady Teazle, drama critic Mankiewicz returned to the press room "... full of fury and too many drinks ...," wrote biographer Richard Meryman:
He was outraged by the spectacle of a 56-year-old millionairess playing a gleeful 18-year-old, the whole production bought for her like a trinket by a man Herman knew to be an unscrupulous manipulator. Herman began to write: "Miss Gladys Wallis, an aging, hopelessly incompetent amateur, opened last night in ..." Then Herman passed out, slumped over the top of his typewriter.
The unconscious Mankiewicz was discovered by his boss, George S. Kaufman, who composed a terse announcement that the "Times" review would appear the following day.
Mankiewicz resurrected the incident for "Citizen Kane". After Kane's second wife makes her opera debut, critic Jed Leland returns to the press room drunk. He passes out over the top of his typewriter after writing the first sentence of his review: "Miss Susan Alexander, a pretty but hopelessly incompetent amateur ..."
Susan Alexander.
The common assumption that the character of Susan Alexander was based on Marion Davies was a major reason William Randolph Hearst tried to destroy "Citizen Kane". In his foreword to Davies's autobiography, published posthumously in 1975, Orson Welles draws a sharp distinction between the real-life actress and his fictional creation:
That Susan was Kane's wife and Marion was Hearst's mistress is a difference more important than might be guessed in today's changed climate of opinion. The wife was a puppet and a prisoner; the mistress was never less than a princess. Hearst built more than one castle, and Marion was the hostess in all of them: they were pleasure domes indeed, and the Beautiful People of the day fought for invitations. Xanadu was a lonely fortress, and Susan was quite right to escape from it. The mistress was never one of Hearst's possessions: he was always her suitor, and she was the precious treasure of his heart for more than 30 years, until his last breath of life. Theirs is truly a love story. Love is not the subject of "Citizen Kane".
Welles cited Samuel Insull's building of the Chicago Opera House, and business tycoon Harold Fowler McCormick's lavish promotion of the opera career of his second wife, as direct influences on the screenplay. McCormick divorced Edith Rockefeller and married aspiring opera singer Ganna Walska as her fourth husband. He spent thousands of dollars on voice lessons for her and even arranged for Walska to take the lead in a production of "Zaza" at the Chicago Opera in 1920. Contemporaries said Walska had a terrible voice; "The New York Times" headlines of the day read, "Ganna Walska Fails as Butterfly: Voice Deserts Her Again When She Essays Role of Puccini's Heroine" (January 29, 1925), and "Mme. Walska Clings to Ambition to Sing" (July 14, 1927).
"According to her 1943 memoirs, "Always Room at the Top," Walska had tried every sort of fashionable mumbo jumbo to conquer her nerves and salvage her voice," reported "The New York Times" in 1996. "Nothing worked. During a performance of Giordano's "Fedora" in Havana she veered so persistently off key that the audience pelted her with rotten vegetables. It was an event that Orson Welles remembered when he began concocting the character of the newspaper publisher's second wife for "Citizen Kane"."
Charles Lederer, Marion Davies's nephew, read a draft of the script before filming began on "Citizen Kane". "The script I read didn't have any flavor of Marion and Hearst," Lederer said. "Robert McCormick was the man it was about." (Lederer confuses Walska's husband Harold F. McCormick with another member of the powerful Chicago family, one who may also have inspired Welles – crusading publisher Robert R. McCormick of the "Chicago Tribune".) Although there were things based on Marion Davies – jigsaw puzzles and drinking – Lederer noted that they were exaggerated in the film to help define the characterization of Susan Alexander.
"As for Marion," Orson Welles said, "she was an extraordinary woman – nothing like the character Dorothy Comingore played in the movie."
Film tycoon Jules Brulatour's second and third wives, Dorothy Gibson and Hope Hampton, both fleeting stars of the silent screen who later had marginal careers in opera, are also believed to have provided inspiration for the Susan Alexander character. The interview with Susan Alexander Kane in the Atlantic City nightclub was based on a contemporary interview with Evelyn Nesbit Thaw in the run-down club where she was performing.
Jim Gettys.
The character of political boss Jim Gettys (Ray Collins) is based on Charles F. Murphy, a leader in New York City's infamous Tammany Hall political machine. William Randolph Hearst and Murphy were political allies in 1902, when Hearst was elected to the U.S. House of Representatives, but the two fell out in 1905 when Hearst ran for mayor of New York. Hearst turned his muckraking newspapers on Tammany Hall in the person of Murphy, who was called "... the most hungry, selfish and extortionate boss Tammany has ever known." Murphy ordered that under no condition was Hearst to be elected. Hearst ballots were dumped into the East River, and new ballots were printed favoring his opponent. Hearst was defeated by some 3,000 votes and his newspapers bellowed against the election fraud. A historic cartoon of Murphy in convict stripes appeared November 10, 1905, three days after the vote. The caption read, "Look out, Murphy! It's a Short Lockstep from Delmonico's to Sing Sing ... Every honest voter in New York wants to see you in this costume."
In "Citizen Kane", Boss Jim Gettys (named Edward Rogers in the shooting script) admonishes Kane for printing a cartoon showing him in prison stripes:
If I owned a newspaper and if I didn't like the way somebody else was doing things – some politician, say – I'd fight them with everything I had. Only I wouldn't show him in a convict suit with stripes — so his children could see the picture in the paper. Or his mother.
As he pursues Gettys down the stairs, Kane threatens to send him to Sing Sing.
As an inside joke, Welles named Gettys after the father-in-law of Roger Hill, his headmaster at the Todd School and a lifelong friend.
"Rosebud".
In "This is Orson Welles", Welles credits the "Rosebud" device – the journalist's search for the enigmatic meaning of Kane's last word, the device that frames the film – to screenwriter Herman Mankiewicz. "Rosebud remained, because it was the only way we could find to get off, as they used to say in vaudeville," Welles said. "It manages to work, but I'm still not too keen about it, and I don't think that he was, either." The dialogue eventually reflects the screenwriters' desire to diminish the importance of the word's meaning; "We did everything we could to take the mickey out of it," Welles said.
As he began his first draft of the "Citizen Kane" screenplay in early 1940, Mankiewicz mentioned "Rosebud" to his secretary. When she asked, "Who is rosebud?" he replied, "It isn't a who, it's an it." The symbol of Mankiewicz's own damaged childhood was a treasured bicycle, stolen while he visited the public library and, in punishment, never replaced. "He mourned that all his life," wrote Pauline Kael, who believed Mankiewicz put the emotion of that boyhood loss into the loss that haunted Kane.
In his 2002 book "Hearst Over Hollywood", Louis Pizzitola reports one historian's statement that "Rosebud" was a nickname given to William Randolph Hearst's mother by portrait and landscape painter Orrin Peck. The Peck family were intimates of the Hearsts, and Orrin Peck was said to be nearer to Phoebe Apperson Hearst than her own son. Another theory of the origin of "Rosebud" is the similarity with the dying wish of Basil Zaharoff (who is one of the inspirations for the central character), to be wheeled "by the rosebush".
In 1989 author Gore Vidal stated that "Rosebud" was a nickname which Hearst had used for the clitoris of his mistress, Marion Davies. Vidal said that Davies had told this intimate detail to his close nephew, Charles Lederer, who had mentioned it to him years later. The claim was repeated in the 1996 documentary "The Battle Over Citizen Kane" and again in the 1999 dramatic film "RKO 281".
Film critic Roger Ebert said, "Some people have fallen in love with the story that Herman Mankiewicz, the co-author with Welles of the screenplay, happened to know that 'Rosebud' was William Randolph Hearst's pet name for an intimate part of Marion Davies' anatomy."
Welles biographer Frank Brady traces the story to the popular press in the late 1970s:
How Orson (or Mankiewicz) could have ever discovered this most private utterance is unexplained and why it took over 35 years for such a suggestive rationale to emerge, although the origins of everything to do with "Citizen Kane" had continually been placed under literary and cinematic microscopes for decades, is also unknown. If this highly unlikely story is even partially true ... Hearst may have become upset at the implied connotation, although any such connection seems to have been innocent on Welles's part. In any event, this bizarre explanation for the origin of one of the most famous words ever spoken on the screen has now made its way into serious studies of Welles and "Citizen Kane".
British film critic Philip French asked Welles's associate John Houseman, who worked on the "Citizen Kane" script with Mankiewicz, whether there was any truth to the story:
"Absolutely none," he said, pointing out that it was inconceivable that he would not have heard of something so provocative at the time, or that Welles could have kept such a secret for over 40 years.
In 1991, Edward Castle, a reporter for "The Las Vegas Sun", contended that Welles may have borrowed the name of Native American folklorist, educator and author Rosebud Yellow Robe for "Rosebud". Castle claimed to have found both of their signatures on the same sign-in sheets at CBS Radio studios in New York, where they both worked on different shows in the late 1930s. The word "Rosebud" appears, however, in the first draft script written by Herman Mankiewicz, not Welles.
"Rosebud is the emblem of the security, hope and innocence of childhood, which a man can spend his life seeking to regain," summarized Roger Ebert. "It is the green light at the end of Gatsby's pier; the leopard atop Kilimanjaro, seeking nobody knows what; the bone tossed into the air in ""."
Filmmaking innovations.
Orson Welles said that his preparation before making "Citizen Kane" was to watch John Ford's "Stagecoach" 40 times. 
A lot of people ought to study "Stagecoach". I wanted to learn how to make movies, and that's such a classically perfect one — don't you think so? ... As it turned out, the first day I ever walked onto a set was my first day as a director. I'd learned whatever I knew in the projection room — from Ford. After dinner every night for about a month, I'd run "Stagecoach", often with some different technician or department head from the studio, and ask questions. "How was this done?" "Why was this done?" It was like going to school.
Cinematography.
Film scholars and historians view "Citizen Kane" as Welles's attempt to create a new style of filmmaking by studying various forms of film making, and combining them all into one. However, in an interview in March 1960 with the BBC's Huw Wheldon, Welles stated that his love for cinema began only when he started the work on "Citizen Kane", and when asked where he got the confidence as a first-time director to direct a film so radically different from contemporary cinema, he responded, "[From] ignorance ... sheer ignorance. There is no confidence to equal it. It's only when you know something about a profession that you are timid or careful."
The most innovative technical aspect of "Citizen Kane" is the extended use of deep focus. In nearly every scene in the film, the foreground, background and everything in between are all in sharp focus. This was done by cinematographer Gregg Toland through his experimentation with lenses and lighting. Toland described the achievement, made possible by the sensitivity of modern speed film, in an article for "Theatre Arts" magazine:
New developments in the science of motion picture photography are not abundant at this advanced stage of the game but periodically one is perfected to make this a greater art. Of these I am in an excellent position to discuss what is termed “Pan-focus”, as I have been active for two years in its development and used it for the first time in "Citizen Kane". Through its use, it is possible to photograph action from a range of eighteen inches from the camera lens to over two hundred feet away, with extreme foreground and background figures and action both recorded in sharp relief. Hitherto, the camera had to be focused either for a close or a distant shot, all efforts to encompass both at the same time resulting in one or the other being out of focus. This handicap necessitated the breaking up of a scene into long and short angles, with much consequent loss of realism. With pan-focus, the camera, like the human eye, sees an entire panorama at once, with everything clear and lifelike.
Any time deep focus was impossible – for example in the scene when Kane finishes a bad review of Alexander's opera while at the same time firing the person who started the review – an optical printer was used to make the whole screen appear in focus (visually layering one piece of film onto another). However, some apparently deep-focus shots were the result of in-camera effects, as in the famous scene where Kane breaks into Susan Alexander's room after her suicide attempt. In the background, Kane and another man break into the room, while simultaneously the medicine bottle and a glass with a spoon in it are in closeup in the foreground. The shot was an in-camera matte shot. The foreground was shot first, with the background dark. Then the background was lit, the foreground darkened, the film rewound, and the scene re-shot with the background action.
Another unorthodox method used in the film was the way low-angle shots were used to display a point of view facing upwards, thus allowing ceilings to be shown in the background of several scenes. Since films were primarily filmed on sound stages and not on location during the era of the Hollywood studio system, it was impossible to film at an angle that showed ceilings because the stages had none. In some instances, Welles's crew used muslin draped above the set to produce the illusion of a regular room with a ceiling. The boom microphones were hidden above the cloth, or in a trench dug into the floor, as in the scene where Kane meets Leland after his election loss.
Toland had approached Welles in 1940 to work on "Citizen Kane". Welles's reputation for experimentation in the theatre appealed to Toland and he found a sympathetic partner to "... test and prove several ideas generally being accepted as radical in Hollywood". Welles credited Toland on the same card as himself. "It's impossible to say how much I owe to Gregg. He was superb," Welles said.
Storytelling techniques.
"Citizen Kane" eschews the traditional linear, chronological narrative, and tells Kane's story entirely in flashback using different points of view, many of them from Kane's aged and forgetful associates, the cinematic equivalent of the unreliable narrator in literature. Welles also dispenses with the idea of a single storyteller and uses multiple narrators to recount Kane's life. The use of multiple narrators was unheard of in Hollywood films. Each narrator recounts a different part of Kane's life, with each story partly overlapping. The film depicts Kane as an enigma, a complicated man who, in the end, leaves viewers with more questions than answers as to his character, such as the newsreel footage where he is attacked for being both a communist and a fascist. The technique of using flashbacks had been used in earlier films such as "Wuthering Heights" in 1939 and "The Power and the Glory" in 1933, but no film was as immersed in this technique as "Citizen Kane". The use of the reporter Thompson acts as a surrogate for the audience, questioning Kane's associates and piecing together his life.
One of the narrative voices is the "News on the March" segment. Its stilted dialogue and portentous voiceover is a parody of "The March of Time" newsreel series which itself references an earlier newsreel which showed the 85-year-old arms czar Sir Basil Zaharoff getting wheeled to his train. Welles had earlier provided voiceovers for the "March of Time" radio show. "Citizen Kane" makes extensive use of stock footage to create the newsreel.
One of the story-telling techniques used in "Citizen Kane" was the use of montage to collapse time and space, using an episodic sequence on the same set while the characters changed costume and make-up between cuts so that the scene following each cut would look as if it took place in the same location, but at a time long after the previous cut. In the breakfast montage, Welles chronicles the breakdown of Kane's first marriage in five vignettes that condense 16 years of story time into two minutes of screen time. Welles said that the idea for the breakfast scene "... was stolen from "The Long Christmas Dinner" of Thornton Wilder ... a one-act play, which is a long Christmas dinner that takes you through something like 60 years of a family's life."
Special effects.
Welles also pioneered several visual effects in order to cheaply shoot things like crowd scenes and large interior spaces. For example, the scene where the camera in the opera house rises dramatically to the rafters to show the workmen showing a lack of appreciation for Susan Alexander Kane's performance, was shot by a camera craning upwards over the performance scene, then a curtain wipe to a miniature of the upper regions of the house, and then another curtain wipe matching it again with the scene of the workmen. Other scenes effectively employed miniatures to make the film look much more expensive than it truly was, such as various shots of Xanadu. A loud, full-screen closeup of a typewriter typing a single word (weak), magnifies the review for the "Chicago Inquirer".
Makeup.
The make-up artist Maurice Seiderman created the make-up for the film. RKO wanted the young Kane to look handsome and dashing, and Seiderman transformed the overweight Welles, beginning with his nose, which Welles always disliked. For the old Kane, Seiderman created a red plastic compound which he applied to Welles, allowing the wrinkles to move naturally. Kane's mustache was made of several hair tufts. Transforming Welles into the old Kane required six to seven hours, meaning he had to start at two in the morning to begin filming at nine. He would hold conferences while sitting in the make-up chair; sometimes working 16 hours a day. Even breaking a leg during filming could not stop him from directing around the clock, and he quickly returned to acting, using a steel leg brace.
Soundtrack.
"Before "Kane", nobody in Hollywood knew how to set music properly in movies," wrote filmmaker François Truffaut in a 1967 essay. ""Kane" was the first, in fact the only, great film that uses radio techniques."
Behind each scene, there is a resonance which gives it its color: the rain on the windows of the cabaret, "El Rancho," when the investigator goes to visit the down-and-out female singer who can only "work" Atlantic City; the echoes in the marble-lined Thatcher library; the overlapping voices whenever there are several characters. A lot of filmmakers know enough to follow Auguste Renoir's advice to fill the eyes with images at all costs, but only Orson Welles understood that the sound track had to be filled in the same way.
In addition to expanding on the potential of sound as a creator of moods and emotions, Welles pioneered a new aural technique, known as the "lightning-mix". Welles used this technique to link complex montage sequences via a series of related sounds or phrases. In offering a continuous sound track, Welles was able to join what would otherwise be extremely rough cuts together into a smooth narrative. For example, the audience witnesses Kane grow from a child into a young man in just two shots. As Kane's guardian hands him his sled, Kane begrudgingly wishes him a "Merry Christmas". Suddenly we are taken to a shot of his guardian fifteen years later, only to have the phrase completed for us: "and a Happy New Year". In this case, the continuity of the soundtrack, not the image, is what makes for a seamless narrative structure.
Welles also carried over techniques from radio not yet popular in films (though they would become staples). Using a number of voices, each saying a sentence or sometimes merely a fragment of a sentence, and splicing the dialogue together in quick succession, the result gave the impression of a whole town talking – and, equally important, what the town was talking about. Welles also favored the overlapping of dialogue, considering it more realistic than the stage and film tradition of characters not stepping on each other's sentences. He also pioneered the technique of putting the audio ahead of the visual in scene transitions (a J-cut); as a scene would come to a close, the audio would transition to the next scene before the visuals did.
Music.
In common with using personnel he had previously worked with in the Mercury Theatre, Welles recruited his close friend Bernard Herrmann to score "Citizen Kane". Herrmann was a longtime collaborator with Welles, providing music for almost all his radio broadcasts, including "The Fall of the City" (1937) and the "War of the Worlds" (1938) broadcast. The film was Herrmann's first motion picture score, and would be nominated for an Academy Award for Best Original Score, but would lose out to his own score for the film "All That Money Can Buy".
Herrmann's score for "Citizen Kane" was a watershed in film soundtrack composition and proved as influential as any of the film's other innovations, establishing him as an important voice in film soundtrack composition. The score eschewed the typical Hollywood practice of scoring a film with virtually non-stop music. Instead Herrmann used what he later described as '"radio scoring", musical cues which typically lasted between five and fifteen seconds to bridge the action or suggest a different emotional response. One of the most effective musical cues was the "Breakfast Montage." The scene begins with a graceful waltz theme and gets darker with each variation on that theme as the passage of time leads to the hardening of Kane's personality and the breakup of his marriage to Emily.
Herrmann realized that musicians slated to play his music were hired for individual unique sessions; there was no need to write for existing ensembles. This meant that he was free to score for unusual combinations of instruments, even instruments that are not commonly heard. In the opening sequence, for example, the tour of Kane's estate Xanadu, Herrmann introduces a recurring leitmotiv played by low woodwinds, including a quartet of alto flutes. Much of the music used in the newsreel was taken from other sources; examples include the "News on the March" music which was taken from RKO's music library, "Belgian March" by Anthony Collins, and accompanies the newsreel titles; and an excerpt from Alfred Newman's score for "Gunga Din" which is used as the background for the exploration of Xanadu. In the final sequence of the film, which shows the destruction of Rosebud in the fireplace of Kane's castle, Welles choreographed the scene while he had Herrmann's cue playing on the set.
For the operatic sequence which exposed Kane's protege Susan Alexander for the amateur she was, Herrmann composed a quasi-romantic scene, "Aria from Salammbô". There did exist two treatments of this 1862 novel by Gustave Flaubert: an opera by Ernest Reyer and an incomplete treatment by Modeste Mussorgsky. However, Herrmann made no reference to existing music. Herrmann put the aria in a key that would force the singer to strain to reach the high notes, culminating in a high D, well outside the range of Susan Alexander. Herrmann said he wanted to convey the impression of "... a terrified girl floundering in the quicksand of a powerful orchestra". On the soundtrack it was soprano Jean Forward who actually sang the vocal part for actress Dorothy Comingore.
In 1972 Herrmann said "I was fortunate to start my career with a film like "Citizen Kane", it's been a downhill run ever since!" Shortly before his death in 1985, Welles told director Henry Jaglom that the score was fifty percent responsible for the film's artistic success.
Herrmann was vocal in his criticism of Pauline Kael's claim that it was Mankiewicz, not Welles, who made the main thrust of the film, and also her assertions about the use of music in the film without consulting him:
Pauline Kael has written in "The Citizen Kane Book" (1971), that the production wanted to use Massenet's "Thais" but could not afford the fee. But Miss Kael never wrote or approached me to ask about the music. We could easily have afforded the fee. The point is that its lovely little strings would not have served the emotional purpose of the film.
Opera lovers are frequently amused by the parody of vocal coaching that appears in a singing lesson given to Susan Alexander by Signor Matiste. The character attempts to sing the famous cavatina "Una voce poco fa" from "Il barbiere di Siviglia" by Gioachino Rossini, but the lesson is interrupted when Alexander sings a high note flat.
At the beginning of Thompson's second interview of Susan Kane at her nightclub, the tune heard in the background is "In a Mizz", a 1939 jazz song by Charlie Barnet and Haven Johnson. "I kind of based the whole scene around that song," Orson Welles said. "The music is by Nat Cole — it's his trio." Later — beginning with the lyrics, "It can't be love" — "In a Mizz" is performed at the Everglades picnic, framing the fight in the tent between Susan and Kane. Musicians including bandleader Cee Pee Johnson (drums), Alton Redd (vocals), Raymond Tate (trumpet), Buddy Collette (alto sax) and Buddy Banks (tenor sax) are featured.
Reception.
Pre-release controversy.
Welles ran a closed set, limited access to dailies, and managed the publicity of "Kane", to ensure that its influence from Hearst's life was a secret. Publicity materials stated the film's inspiration was "Faust". RKO hoped to release the film in mid-February 1941. Writers for national magazines had early deadlines and so a rough cut was previewed for a select few on January 3, 1941. "Friday" magazine ran an article drawing point-by-point comparisons between Kane and Hearst and documented how Welles had led on Louella Parsons, Hollywood correspondent for Hearst papers, and made a fool of her in public. Reportedly, she was furious and demanded an immediate preview of the film. James G. Stewart, who was present at the screening, said that she walked out of the film. Soon after, Parsons called George Schaefer and threatened RKO with a lawsuit if they released "Kane". The next day, the front page headline in "Daily Variety" read, "HEARST BANS RKO FROM PAPERS." In two weeks, the ban was lifted for everything except "Kane."
"The Hollywood Reporter" ran a front-page story on January 13 that Hearst papers were about to run a series of editorials attacking Hollywood's practice of hiring refugees and immigrants for jobs that could be done by Americans. The goal was to put pressure on the other studios in order to force RKO to shelve "Kane". Soon afterwards, Schaefer was approached by Nicholas Schenck, head of MGM's parent company, with an offer on the behalf of Louis B. Mayer and other Hollywood executives to reimburse RKO if it would destroy the film. Once RKO's legal team reassured Schaefer, the studio announced on January 21 that "Kane" would be released as scheduled, and with one of the largest promotional campaigns in the studio's history. Schaefer brought Welles to New York City for a private screening of the film with the New York corporate heads of the studios and their lawyers. There was no objection to its release provided that certain changes, including the removal or softening of specific references that might offend Hearst, were made. Welles agreed, and editor Robert Wise (who became a celebrated film director in the 1950s and 60s) was brought in to cut the running time from two hours, two minutes, and 40 seconds to one hour, 59 minutes, and 16 seconds. That cut satisfied the corporate lawyers.
Hearst's response.
Hearing about the film enraged Hearst so much that he banned any advertising, reviewing, or mentioning of it in his papers, and had his journalists libel Welles. Following lobbying from Hearst, the head of Metro-Goldwyn-Mayer, Louis B. Mayer, acting on behalf of the whole film industry, made an offer to RKO Pictures of $805,000 to destroy all prints of the film and burn the negative. Welles used Hearst's opposition to "Citizen Kane" as a pretext for previewing the film in several opinion-making screenings in Los Angeles, lobbying for its artistic worth against the hostile campaign that Hearst was waging.
When George Schaefer of RKO rejected Hearst's offer to suppress the film, Hearst banned every newspaper and station in his media conglomerate from reviewing – or even mentioning – the film. He also had many movie theaters ban it, and many did not show it through fear of being socially exposed by his massive newspaper empire. The documentary "The Battle Over Citizen Kane" lays the blame for "Citizen Kane"s relative failure squarely at the feet of Hearst. The film did decent business at the box office; it went on to be the sixth highest grossing film in its year of release, a modest success its backers found acceptable. Nevertheless, the film's commercial performance fell short of its creators' expectations. In "The Chief: The Life of William Randolph Hearst", David Nasaw points out that Hearst's actions were not the only reason "Kane" failed, however: the innovations Welles made with narrative, as well as the dark message at the heart of the film (that the pursuit of success is ultimately futile) meant that a popular audience could not appreciate its merits.
In a pair of "Arena" documentaries about Welles's career produced and broadcast domestically by the BBC in 1982, Welles claimed that during opening week, a policeman approached him one night and told him: "Do not go to your hotel room tonight; Hearst has set up an undressed, underage girl to leap into your arms when you enter and a photographer to take pictures of you. Hearst is planning to publish it in all of his papers." Welles thanked the man and stayed out all night. However, it is not confirmed whether this was true. Welles also described how he accidentally bumped into Hearst in an elevator at the Fairmont Hotel when "Kane" was opening in San Francisco. Welles's father had been friends with Hearst, so Welles tried to comfortably ask if Hearst would see the film. Hearst ignored him. "As he was getting off at his floor, I said 'Charles Foster Kane would have accepted.' No reply", recalled the director. "And Kane would have you know. That was his style."
Although Hearst tried to suppress the film and limit its success, his efforts backfired in the long run, for now almost every reference to Hearst's life and career includes a reference to the parallels in the film. The irony of Hearst's attempts is that the film is now inexorably connected to him. This connection is reinforced by W. A. Swanberg's extensive biography entitled "Citizen Hearst".
Release and contemporary responses.
"Citizen Kane" was to open at RKO's flagship theatre, Radio City Music Hall, but did not; a possible factor was Louella Parsons's threat that "The American Weekly" would run a defamatory story on the grandfather of major RKO stockholder Nelson Rockefeller. Other exhibitors feared retaliation and refused to handle the film. Schaefer lined up a few theaters but Welles grew impatient and threatened RKO with a lawsuit. Hearst papers refused to accept advertising for the film. "Kane" opened at the RKO Palace on Broadway in New York on May 1, 1941, in Chicago on May 6, and in Los Angeles on May 8. "Kane" did well in cities and larger towns but fared poorly in more remote areas. RKO still had problems getting exhibitors to show the film. For example, one chain controlling more than 500 theaters got Welles's film as part of a package but refused to play it, reportedly out of fear of Hearst. The Hearst newspapers's disruption of the film's release damaged its box office performance and, as a result, "Citizen Kane" lost $160,000 during its initial run.
The reviews for the film were overwhelmingly positive, although some reviewers were challenged by Welles's break with Hollywood traditions. Kate Cameron, in her review for the "New York Daily-News", said that "Kane" was "... one of the most interesting and technically superior films that has ever come out of a Hollywood studio". In his review for the "New World Telegram", William Boehnel said that the film was "... staggering and belongs at once among the greatest screen achievements". Otis Ferguson, in his review for "The New Republic", said that "Kane" was "... the boldest free-hand stroke in major screen production since Griffith and Bitzer were running wild to unshackle the camera". John O'Hara, in "Newsweek", called it "... the best picture he'd ever seen."
The day following the premiere of "Citizen Kane," "The New York Times" critic Bosley Crowther wrote that "... it comes close to being the most sensational film ever made in Hollywood."
Count on Mr. Welles: he doesn't do things by halves. ... Upon the screen he discovered an area large enough for his expansive whims to have free play. And the consequence is that he has made a picture of tremendous and overpowering scope, not in physical extent so much as in its rapid and graphic rotation of thoughts. Mr. Welles has put upon the screen a motion picture that really moves.
Critic James Agate was decidedly negative in an October 1941 review, countering the superlatives given "Citizen Kane" by critics C. A. Lejeune and Dilys Powell. "Now imagine my horror, which includes self-distrust, at seeing no more in this film than the well-intentioned, muddled, amateurish thing one expects from high-brows. (Mr. Orson Welles's height of brow is enormous.) ... I thought the photography quite good, but nothing to write to Moscow about, the acting middling, and the whole thing a little dull."
Agate continued his review two weeks later:
"Citizen Kane" has entirely ousted the war as conversation fodder. Waiters ask me what I think of it, and the post is full of it. ... You know now that all the vulgar beef, beer and tobacco barons are vulgar because when they were about seven years of age somebody came and took away their skates. That is one explanation of this alleged world-shaking masterpiece, "Citizen Kane". Another point of view is that "Citizen Kane" is so great a masterpiece that it doesn't need explaining. ... In the meantime I continue to steer a middle course. I regard "Citizen Kane" as a quite good film which tries to run the psychological essay in harness with your detective thriller, and doesn't quite succeed.
In a 1941 review, Jorge Luis Borges called "Citizen Kane" a "metaphysical detective story", in that "... [its] subject (both psychological and allegorical) is the investigation of a man's inner self, through the works he has wrought, the words he has spoken, the many lives he has ruined ..." Borges noted that "Overwhelmingly, endlessly, Orson Welles shows fragments of the life of the man, Charles Foster Kane, and invites us to combine them and reconstruct him." As well, "Forms of multiplicity and incongruity abound in the film: the first scenes record the treasures amassed by Kane; in one of the last, a poor woman, luxuriant and suffering, plays with an enormous jigsaw puzzle on the floor of a palace that is also a museum." Borges points out, "At the end we realize that the fragments are not governed by a secret unity: the detested Charles Foster Kane is a simulacrum, a chaos of appearances."
Accolades.
14th Academy Awards  – 1941.
"Citizen Kane", with nine nominations, was the sixteenth film to get more than six Academy Award nominations. It was nominated for the following awards:
It was widely thought the film would win most of the awards it was nominated for, but it only won the Best Writing (Original Screenplay) Oscar.
Film editor Robert Wise recalled each time "Citizen Kane"s name was called out as a nominee, the crowd booed. Most of Hollywood did not want the film to see the light of day, considering the threats that William Randolph Hearst had made if it did. According to "Variety", bloc voting against Welles by screen extras denied him Best Picture and Actor awards. British film critic Barry Norman attributed this to Hearst's wrath.
Other awards.
The National Board of Review of Motion Pictures gave 1941 "Best Acting" awards to Orson Welles and George Coulouris, and the film itself "Best Picture." That same year, "The New York Times" named it one of the Ten Best Films of the year, and the New York Film Critics Circle Award for "Best Picture" also went to "Citizen Kane".
Subsequent re-evaluation and recognition.
By 1942 "Citizen Kane" had run its course theatrically and, apart from a few showings at big city arthouse cinemas, it largely vanished from America until 1956. In that period, "Kane"'s and Welles' reputation fell among American critics. In 1949 critic Richard Griffith in his overview of cinema, "The Film Till Now", dismissed "Kane" as "... tinpot if not crackpot Freud."
Due to World War II, "Citizen Kane" was little seen in Europe. It was not until 1946 that it was shown in France, where it gained considerable acclaim, particularly from film critics such as André Bazin and from "Cahiers du cinéma" writers, including future film directors François Truffaut and Jean-Luc Godard. In his 1950 essay "The Evolution of the Language of Cinema", Bazin placed "Citizen Kane" centre stage as a work which ushered in a new period in cinema.
In the United States, it was neglected and forgotten until its revival on television in the mid-1950s. Three key events in 1956 led to its re-evaluation in the United States: first, RKO was one of the first studios to sell its library to television, and early that year "Citizen Kane" started to appear on television; second, the film was re-released theatrically to coincide with Welles's return to the New York stage, where he played "King Lear;" and third, American film critic Andrew Sarris wrote "Citizen Kane: The American Baroque" for "Film Culture", and described it as "the great American film." During Expo 58, a poll of over 100 film historians named "Kane" one of the top ten greatest films ever made (the group gave first-place honors to "The Battleship Potemkin"). When a group of young film directors announced their vote for the top six, they were booed for not including the film.
In the decades since, its critical status as one of the greatest films ever made has grown, with numerous essays and books on it including Peter Cowie's "The Cinema of Orson Welles", Ronald Gottesman's "Focus on Citizen Kane", a collection of significant reviews and background pieces, and most notably Kael's essay, "Raising Kane", which promoted the value of the film to a much wider audience than it had reached before. Despite its criticism of Welles, it further popularized the notion of "Citizen Kane" as the great American film. The rise of art house and film society circuits also aided in the film's rediscovery.
The British magazine "Sight & Sound" has produced a Top Ten list surveying film critics every decade since 1952, and is regarded as one of the most respected barometers of critical taste. "Citizen Kane" was a runner up to the top 10 in its 1952 poll but was voted as the greatest film ever made in its 1962 poll, retaining the top spot in every subsequent poll until 2012, when "Vertigo" displaced it.
The film has also ranked number one in the following film "best of" lists: Editorial Jaguar, FIAF Centenary List, France Critics Top 10, Cahiers du cinéma 100 films pour une cinémathèque idéale, Kinovedcheskie Zapiski, Russia Top 10, Romanian Critics Top 10, Time Out Magazine Greatest Films, and Village Voice 100 Greatest Films. Roger Ebert called "Citizen Kane" the greatest film ever made: "But people don't always ask about the greatest film. They ask, 'What's your favorite movie?' Again, I always answer with "Citizen Kane"."
In 1989, the United States Library of Congress deemed the film "culturally, historically, or aesthetically significant" and selected it for preservation in the National Film Registry. "Citizen Kane" was one of the first 25 movies designated as national treasures. The legislation creating the registry was enacted in 1988, growing out of the debate over a movie director's right to block film colorization.
On February 18, 1999, the United States Postal Service honored "Citizen Kane" by including it in its Celebrate the Century series. The film was honored again February 25, 2003, in a series of U.S. postage stamps marking the 75th anniversary of the Academy of Motion Picture Arts and Sciences. Art director Perry Ferguson represents the behind-the-scenes craftsmen of filmmaking in the series; he is depicted completing a sketch for "Citizen Kane".
"Citizen Kane" was ranked number one in the American Film Institute's polls of film industry artists and leaders in 1998 and 2007. "Rosebud" was chosen the 17th most memorable movie quotation in a 2005 AFI poll. The film's score was one of 250 nominees for the top 25 film scores in American cinema in another 2005 AFI poll.
The film currently has a 100% rating at Rotten Tomatoes, based on 66 reviews by approved critics.
Legacy.
Despite the critical success of "Citizen Kane" it nevertheless marked a decline in Welles's fortunes. In the book "Whatever Happened to Orson Welles?", Joseph McBride argues that the problems in making "Citizen Kane" caused lasting damage to his career. The damage started with RKO violating its contract with him by taking his next film, "The Magnificent Ambersons", away from him and adding a happy ending against his will. Hollywood's treatment of Welles and his work ultimately led to his self-imposed exile in Europe for much of the rest of his career, where he found a more sympathetic audience.
The documentary "The Battle Over Citizen Kane" posits that Welles's own life story resembled that of Kane far more than Hearst's: an overreaching wunderkind who ended up mournful and lonely in his old age. "Citizen Kane"'s editor, Robert Wise, summarized: "Well, I thought often afterwards, only in recent years when I saw the film again two or three years ago when they had the fiftieth anniversary, and I suddenly thought to myself, well, Orson was doing an autobiographical film and didn't realize it, because it's rather much the same, you know. You start here, and you have a big rise and tremendous prominence and fame and success and whatnot, and then tail off and tail off and tail off. And at least the arc of the two lives were very much the same ..."
Peter Bogdanovich, who was friends with Welles in his later years, disagreed with this on his own commentary on the "Citizen Kane" DVD, saying that Kane was nothing like Welles. Kane, he said, "... had none of the qualities of an artist, Orson had all the qualities of an artist." Bogdanovich also noted that Welles was never bitter "... about all the bad things that happened to him ...," and was a man who enjoyed life in his final years.
In addition, critics have reassessed Welles' career after his death, saying that he wasn't a failed Hollywood filmmaker, but a successful independent filmmaker.
Film critic Kim Newman believed the film's influence was visible in the film noir that followed, as well as the 1942 Hepburn-Tracy film "Keeper of the Flame." Martin Scorsese ranks it as one of his favorite films of all time.
The film's structure influenced the biographical films "Lawrence of Arabia" and "" – which begin with the subject's death and show their life in flashbacks – as well as Welles's thriller "Mr. Arkadin".
The film, for its topic of mass media manipulation of public opinion, is also famous for having been frequently presented as the perfect example to demonstrate the power that media has on influencing the democratic process. This exemplary citation of the film lasted till the end of the 20th century, when the paradigm of mass media depicted in "Citizen Kane" needed to be updated to take into account more globalized and more internet-based media scenarios. Since the film was based on William Randolph Hearst's actions in the late 19th and early 20th centuries, that model of media influence lasted for almost a century. Media mogul Rupert Murdoch is sometimes labeled as a latter-day "Citizen Kane".
Film memorabilia.
In June 1982, Steven Spielberg spent $60,500 to buy a Rosebud sled, one of three balsa sleds used in the closing scenes and the only one that was not burned. Spielberg had paid homage to "Citizen Kane" in the final shot of the government warehouse in his 1981 film, "Raiders of the Lost Ark". Spielberg commented, "Rosebud will go over my typewriter to remind me that quality in movies comes first."
After the Spielberg purchase, news outlets began reporting the claim of Arthur Bauer, a retired helicopter pilot in New York, that he owned another Rosebud, the hardwood sled used in Buddy Swan's scenes as the young Charles Foster Kane at the beginning of "Citizen Kane". "I'm sure it could be true," Welles said when asked for comment. In early 1942, Bauer was a 12-year-old student in Brooklyn and a member of his school's film club. He entered and won an RKO Pictures publicity contest and selected Rosebud as his prize. In 1996, Bauer's estate offered the painted pine sled at auction through Christie's. Bauer's son told CBS News that his mother had once wanted to paint the sled and use it as a plant stand; "Instead, my dad said, 'No, just save it and put it in the closet.'" On December 15, 1996, the hardwood sled was sold to an anonymous bidder in Los Angeles for $233,500.
In December 2007, Welles's personal copy of the last revised draft of "Citizen Kane" before the shooting script was sold at Sotheby's in New York for $97,000. Welles's Oscar for best original screenplay was offered for sale at the same auction, but failed to reach its estimate of $800,000 to $1.2 million. The Oscar, believed to have been lost by Welles, was rediscovered in 1994. Owned by the Dax Foundation, a Los Angeles based charity, it was sold at auction in 2011 by an anonymous seller to an anonymous buyer for $861,542.
A working draft script for "Citizen Kane" — with its original title, "American" — was sold at auction by Sotheby's, March 5–6, 2014. A second-draft script marked "Mr. Welles' working copy" in pencil on the manilla cover, it was expected to bring between $25,080 and $33,440; it sold for $164,692. The same item had been sold by Christie's in December 1991, together with a working script from "The Magnificent Ambersons", for $11,000.
A collection of 24 pages from a script of "Citizen Kane" was sold at auction April 26, 2014, for $15,000. A collection of approximately 235 stills and production photos sold for $7,812.The materials were among those found in boxes and trunks of Welles's personal possessions by his daughter Beatrice Welles.
Distribution rights.
In 1955, RKO sold the American television rights to its film library, including "Citizen Kane", to C&C Television Corp. Television rights to the pre-1956 RKO library were acquired by United Artists in 1960. RKO kept the non-broadcast television rights to its library and formed RKO Home Video in 1984. RKO, which had licensed the film to other home video companies, reissued the film in 1985 on VHS and Beta, while the laserdisc video rights to "Citizen Kane" were carried over to The Criterion Collection for its inaugural release in 1984. Turner Broadcasting System acquired broadcast television rights to the library when it acquired MGM/UA in 1986. Unable to sustain the debt load, Turner split up MGM/UA and kept the MGM film library, including American television rights to the RKO library. Turner acquired full worldwide rights to the RKO library in 1987. The RKO Home Video unit was reorganized into Turner Home Entertainment that year. For the film's 50th anniversary in 1991, Turner Entertainment utilized Paramount Pictures as its distributor for the film's re-release to theaters. In 1996, Time Warner acquired Turner and Warner Home Video absorbed Turner Home Entertainment. Today, Time Warner's Warner Bros. unit has distribution rights for "Citizen Kane".
Prints.
The composited camera negative of "Citizen Kane" was destroyed in a New Jersey film laboratory fire in the 1970s. Subsequent prints were ultimately derived from a master positive (a fine-grain preservation element) made in the 1940s and originally intended for use in overseas distribution. The soundtrack had not been lost.
Modern techniques were used to produce a pristine print for a 50th Anniversary theatrical revival reissue in 1991 (released by Paramount Pictures). The 2003 British DVD edition is taken from a master positive held by the British Film Institute. The current US DVD version (released by Warner Home Video) is taken from another digital restoration, supervised by Turner's company. The transfer to Region 1 DVD has been criticized by some film experts for being too bright. Also, in the scene in Bernstein's office (chapter 10), rain falling outside the window has been digitally erased, probably because it was thought to be excessive film grain. These alterations are not present in the UK Region 2, which is also considered to be more accurate in terms of contrast and brightness.
In 2003, Welles's daughter Beatrice sued Turner Entertainment and RKO Pictures, claiming the Welles estate is the legal copyright holder of the film. Her attorney said Orson Welles had left RKO with an exit deal terminating his contracts with the studio, meaning Welles still had an interest in the film, and his previous contract giving the studio the copyright of the film was null and void. Beatrice Welles also claimed, if the courts did not uphold her claim of copyright, RKO nevertheless owed the estate 20% of the profits, from a previous contract which has not been lived up to. On May 30, 2007, the appeals panel agreed Welles could proceed with the lawsuit against Turner Entertainment; the opinion partially overturning the 2004 decision by a lower court judge who had found in favor of Turner Entertainment on the issue of video rights.
Colorization controversy.
In the 1980s, "Citizen Kane" became a catalyst in the controversy over the colorization of black-and-white films. In November 1986, "New York Times" film critic Vincent Canby wrote, "It's something of an irony that at a time when concerned movie makers are trying to raise funds for the preservation of films originally shot in color (and which are now fading fast), other people have come along with grandiose plans to 'colorize' all black-and-white films, including, so help me, "Citizen Kane" and "The Magnificent Ambersons.""
One high-profile proponent of film colorization was Ted Turner, whose Turner Entertainment Co. acquired exclusive rights for the RKO library. A Turner Entertainment spokesperson initially stated that "Citizen Kane" would not be colorized: "We don't think it's appropriate. We think it's fine as it is". But at a July 1988 news conference called to unveil the newly colored "Casablanca", Ted Turner said, ""Citizen Kane?" I'm thinking of colorizing it."
In January 1989 the Associated Press reported that two companies were producing color tests of "Citizen Kane" for Turner Entertainment. Criticism increased with the AP's report that filmmaker Henry Jaglom remembered that shortly before his death Orson Welles had implored him to protect "Kane" from being colorized.
On February 14, 1989, Turner Entertainment president Roger Mayer announced that work to colorize "Citizen Kane" had been stopped:
Our attorneys looked at the contract between RKO Pictures Inc. and Orson Welles and his production company, Mercury Productions Inc., and, on the basis of their review, we have decided not to proceed with colorization of the movie. … While a court test might uphold our legal right to colorize the film, provisions of the contract could be read to prohibit colorization without permission of the Welles estate. We have completed restoration of a printing negative which now enables us to show first-rate black-and-white prints of this masterpiece.
"It was rather well known that Welles had very, very complete controls. His contract is quite unusual," Mayer said. "What we are saying is that when a director has final cut, it is the ultimate in creative control. The other contracts we have checked out are not like this at all."
One minute of the colorized test footage of "Citizen Kane" was included in a special "Arena" documentary, "The Complete Citizen Kane", produced by the BBC in 1991.
In December 1989, Turner Home Entertainment released a colorized version of "The Magnificent Ambersons" on VHS.
The colorization controversy was a factor in the passage of the National Film Preservation Act of 1988. The legislation created the National Film Registry that in September 1989 inducted its first 25 films, including "Citizen Kane" and "Casablanca". "One major reason for doing this is to require people like the broadcaster Ted Turner who've been adding color to some movies and reediting others for television to put notices on those versions saying that the movies have been altered," reported ABC News anchor Peter Jennings.
Home media.
Blu-ray.
On September 13, 2011, "Citizen Kane" was released on Blu-ray disc and DVD in a 70th anniversary box set called "The Ultimate Collectors Edition". "This, quite simply, is the Blu-ray release of the year," wrote the "San Francisco Chronicle". "Anyone who thinks Blu-ray doesn't make much of a difference to an older, B&W movie hasn't seen this reference-quality work by Warner Home Video." ""Citizen Kane" is a film whose visual scintillation is ageless, and you have never seen it look as good as it does here, in Warner's newly restored, 1080p, AVC/MPEG-4 transfer," wrote DVD Talk. Supplements include separate commentary tracks by Roger Ebert and Peter Bogdanovich, carried forward from Warner's 60th anniversary edition DVD release; two additional films, "The Battle Over Citizen Kane" and "RKO 281"; and packaging extras that include a hardcover booklet and a folio containing a reproduction of the original souvenir program, miniature lobby cards and other memorabilia.
External links.
<Div/>

</doc>
<doc id="5225" url="http://en.wikipedia.org/wiki?curid=5225" title="Code">
Code

In communications and information processing, code is system of rules to convert information—such as a letter, word, sound, image, or gesture—into another, sometimes shortened or secret, form or representation for communication through a channel or storage in a medium. An early example is the invention language, which enabled a person, through speech, to communicate what he or she saw, heard, felt, or thought to others. But speech limits the range of communication to the distance a voice can carry, and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.
The process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands.
One reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaller or the arms of a semaphore tower encodes parts of the message, typically individual letters and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.
Theory.
In information theory and computer science, a code is usually considered as an algorithm which uniquely represents symbols from some source alphabet, by "encoded" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.
Before giving a mathematically precise definition, we give a brief example. The mapping 
is a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001011 can be grouped into codewords as 0 011 0 01 011, and these in turn can be decoded to the sequence of source symbols "acabc".
Using terms from formal language theory, the precise mathematical definition of this concept is as follows: Let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T, and the extension of formula_5 to a homomorphism of formula_6 into formula_7, which naturally maps each sequence of source symbols to a sequence of target symbols, is referred to as its extension.
Variable-length codes.
In this section we consider codes, which encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string.
Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.
A "prefix code" is a code with the "prefix property": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as "Huffman codes", even when the code was not produced by a Huffman algorithm.
Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard.
Kraft's inequality characterizes the sets of code word lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessary a prefix one, must satisfy Kraft's inequality.
Error-correcting codes.
Codes may also be used to represent data in a way more resistant
to errors in transmission or storage. Such a "code" is
called an error-correcting code, and works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed–Solomon, Reed–Muller, Walsh–Hadamard, Bose–Chaudhuri–Hochquenghem, Turbo, Golay, Goppa, low-density parity-check codes, and space–time codes.
Error detecting codes can be optimised to detect "burst errors", or "random errors".
Examples.
Codes in communication used for brevity.
A cable code replaces words (e.g., "ship" or "invoice") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and most important, less expensively.
Codes can be used for brevity. When telegraph messages were the state of the art in rapid long distance communication, elaborate systems of commercial codes that encoded complete phrases into single words (commonly five-letter groups) were developed, so that telegraphers became conversant with such "words" as "BYOXO" ("Are you trying to weasel out of our deal?"), "LIOUY" ("Why do you not answer my question?"), "BMULD" ("You're a skunk!"), or "AYYLU" ("Not clearly coded, repeat more clearly."). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.
Bletchley Park.
Bletchley Park is an estate located in the town of Bletchley, in Milton Keynes, Buckinghamshire, England, and is run by the Bletchley Park Trust as a heritage attraction. During the Second World War, Bletchley Park was the site of the United Kingdom's main decryption establishment, the Government Code and Cypher School (GC&CS), where ciphers and codes of several Axis countries were decrypted, most importantly the ciphers generated by the German Enigma and Lorenz machines. The place was known as "B.P." to the people who worked there.[1][2] For the many members of the Women's Royal Naval Service (Wrens) who worked at Bletchley Park, their posting was to HMS Pembroke V.
Character encodings.
Probably the most widely known data communications code so far (aka character representation) in use today is ASCII. In one or another (somewhat compatible) version, it is used by nearly all personal computers, terminals, printers, and other communication equipment. It represents 128 characters with seven-bit binary numbers—that is, as a string of seven 1s and 0s (bits). In ASCII a lowercase "a" is always 1100001, an uppercase "A" always 1000001, and so on. There are many other encodings, which represent each character by a byte (usually referred as code pages), integer code point (Unicode) or a byte sequence (UTF-8).
Genetic code.
Biological organisms contain genetic material that is used to control their function and development. This is DNA which contains units named genes that can produce proteins through a code (genetic code) in which a series of triplets (codons) of four possible nucleotides are translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein.
Gödel code.
In mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).
Other.
There are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, biological, etc.)
In marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from an internet retailer.
In military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry in the battlefield, etc.
Communication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.
Musical scores are the most common way to encode music.
Specific games, as chess, have their own code systems to record the matches (chess notation).
Cryptography.
In the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead. See code (cryptography).
Secret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomatic, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requisite is the previous agreement of the meaning by both the sender and the receiver.
Other examples.
Other examples of encoding include:
Other examples of decoding include:
Codes and acronyms.
Acronyms and abbreviations can be considered codes, and in a sense all languages and writing systems are codes for human thought.
International Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways, but are usually national, so the same code can be used for different stations if they are in different countries.
Occasionally a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean "end of story", and has been used in other contexts to signify "the end".

</doc>
<doc id="5228" url="http://en.wikipedia.org/wiki?curid=5228" title="Cheirogaleidae">
Cheirogaleidae

The Cheirogaleidae are the family of strepsirrhine primates containing the various dwarf and mouse lemurs. Like all other lemurs, cheirogaleids live exclusively on the island of Madagascar.
Characteristics.
Cheirogaleids are smaller than the other lemurs and, in fact, they are the smallest primates. They have soft, long fur, colored grey-brown to reddish on top, with a generally brighter underbelly. Typically, they have small ears, large, close-set eyes, and long hind legs. Like all strepsirrhines, they have fine claws at the second toe of the hind legs. They grow to a size of only 13 to 28 cm, with a tail that is very long, sometimes up to one and a half times as long as the body. They weigh no more than 500 grams, with some species weighing as little as 60 grams.
Dwarf and mouse lemurs are nocturnal and arboreal. They are excellent climbers and can also jump far, using their long tails for balance. When on the ground (a rare occurrence), they move by hopping on their hind legs. They spend the day in tree hollows or leaf nests. Cheirogaleids are typically solitary, but sometimes live together in pairs. 
Their eyes possess a tapetum lucidum, a light-reflecting layer that improves their night vision. Some species, such as the lesser dwarf lemur, store fat at the hind legs and the base of the tail, and hibernate. Unlike lemurids, they have long upper incisors, although they do have the comb-like teeth typical of all strepsirhines. They have the dental formula: 
Cheirogaleids are omnivores, eating fruits, flowers and leaves (and sometimes nectar), as well as insects, spiders, and small vertebrates.
The females usually have three pairs of nipples. After a meager 60-day gestation, they will bear two to four (usually two or three) young. After five to six weeks, the young are weaned and become fully mature near the end of their first year or sometime in their second year, depending on the species. In human care, they can live for up to 15 years, although their life expectancy in the wild is probably significantly shorter.
Classification.
The five genera of cheirogaleids contain 34 species.

</doc>
<doc id="5229" url="http://en.wikipedia.org/wiki?curid=5229" title="Callitrichidae">
Callitrichidae

The Callitrichidae (also called Arctopitheci or Hapalidae) is a family of New World monkeys, including marmosets and tamarins. At times, this group of animals has been regarded as a subfamily, called Callitrichinae, of the family Cebidae.
This taxon was traditionally thought to be a primitive lineage, from which all the larger bodied platyrrhines evolved. However, some works argue that callitrichids are actually a dwarfed lineage.
Ancestral stem-callitrichids would likely have been "normal" sized ceboids that were dwarfed through evolutionary time. This may exemplify a rare example of insular dwarfing in a mainland context, with the "islands" being formed by biogeographic barriers during arid climatic periods when forest distribution became patchy, and/or by the extensive river networks in the Amazon Basin.
All callitrichids are arboreal. They are the smallest of the simian primates. They eat insects, fruit, and the sap or gum from trees; occasionally they will take small vertebrates. The marmosets rely quite heavily on exudates, with some species (e.g. "Callithrix jacchus" and "Cebuella pygmaea") considered obligate exudativores.
Callitrichids typically live in small, territorial groups of about five or six animals. Their social organization is unique among primates and is called a "cooperative polyandrous group". This communal breeding system involves groups of multiple males and females, but only one female is reproductively active. Females mate with more than one male and everyone shares the responsibility of carrying the offspring.
They are the only primate group that regularly produces twins, which constitute over 80% of births in species that have been studied. Unlike other male primates, male callitrichids generally provide as much parental care as females. Parental duties may include carrying, protecting, feeding, comforting, and even engaging in play behavior with offspring. In some cases, such as in the cotton-top tamarin ("Saguinus oedipus"), males, particularly those that are paternal, will even show a greater involvement in caregiving than females. The typical social structure seems to constitute a breeding group, with several of their previous offspring living in the group and providing significant help in rearing the young.

</doc>
<doc id="5230" url="http://en.wikipedia.org/wiki?curid=5230" title="Cebidae">
Cebidae

The Cebidae is one of the five families of New World monkeys now recognised. It includes the capuchin monkeys and squirrel monkeys. These species are found throughout tropical and subtropical South and Central America.
Characteristics.
Cebid monkeys are arboreal animals that only rarely travel on the ground. They are generally small monkeys, ranging in size up to that of the Brown Capuchin, with a body length of 33 to 56 cm, and a weight of 2.5 to 3.9 kilograms. They are somewhat variable in form and coloration, but all have the wide, flat, noses typical of New World Monkeys. They are different from marmosets as they have additional molar tooth and a prehensile tail. 
They are omnivorous, mostly eating fruit and insects, although the proportions of these foods vary greatly between species. They have the dental formula:
Females give birth to one or two young after a gestation period of between 130 and 170 days, depending on species. They are social animals, living in groups of between five and forty individuals, with the smaller species typically forming larger groups. They are generally diurnal in habit.
Classification.
Previously, New World monkeys were divided between Callitrichidae and this family. For a few recent years, marmosets, tamarins, and lion tamarins were placed as a subfamily (Callitrichinae) in Cebidae, while moving other genera from Cebidae into the families Aotidae, Pitheciidae and Atelidae. The most recent classification of New World monkeys again splits the callitrichids off, leaving only the capuchins and squirrel monkeys in this family.

</doc>
<doc id="5232" url="http://en.wikipedia.org/wiki?curid=5232" title="Chondrichthyes">
Chondrichthyes

Chondrichthyes (; from Greek χονδρ- "chondr-" 'cartilage', ἰχθύς "ichthys" 'fish') or cartilaginous fishes are jawed fish with paired fins, paired nares, scales, a heart with its chambers in series, and skeletons made of cartilage rather than bone. The class is divided into two subclasses: Elasmobranchii (sharks, rays and skates) and Holocephali (chimaeras, sometimes called ghost sharks, which are sometimes separated into their own class).
Within the infraphylum Gnathostomata, cartilaginous fishes are distinct from all other jawed vertebrates, the extant members of which all fall into Teleostomi.
Anatomy.
Skeleton.
The skeleton is cartilaginous. The notochord, which is present in the young, is gradually replaced by cartilage. Chondrichthyes also lack ribs, so if they leave water, the larger species' own body weight would crush their internal organs long before they would suffocate.
As they do not have bone marrow, red blood cells are produced in the spleen and the epigonal organ (special tissue around the gonads, which is also thought to play a role in the immune system). They are also produced in the Leydig's organ which is only found in cartilaginous fishes, although some do not possess it. The subclass Holocephali, which is a very specialized group, lacks both the Leydig's and epigonal organ.
Appendages.
Their tough skin is covered with dermal teeth (again with Holocephali as an exception as the teeth are lost in adults, only kept on the clasping organ seen on the front of the male's head), also called placoid scales (or "dermal denticles") making it feel like sandpaper. In most species, all dermal denticles are oriented in one direction, making the skin feel very smooth if rubbed in one direction and very rough if rubbed in the other. Another exception are the electric rays, which have a thick and flabby body, with soft, loose skin devoid of dermal denticles and thorns.
Originally the pectoral and pelvic girdles, which do not contain any dermal elements, did not connect. In later forms, each pair of fins became ventrally connected in the middle when scapulocoracoid and pubioischiadic bars evolved. In rays, the pectoral fins have connected to the head and are very flexible.
One of the primary characteristics present in most sharks is the heterocercal tail, which aids in locomotion.
Body covering.
Chondrichthyes have toothlike scales called dermal denticles or placoid scales. Denticles provide two functions, protection, and in most cases streamlining. Mucous glands exist in some species as well.
It is assumed that their oral teeth evolved from dermal denticles which migrated into the mouth, but it could be the other way around as the teleost bony fish "Denticeps clupeoides" has most of its head covered by dermal teeth (as does, probably, "Atherion elymus", another bony fish). This is most likely a secondary evolved characteristic which means there is not necessarily a connection between the teeth and the original dermal scales.
The old placoderms did not have teeth at all, but had sharp bony plates in their mouth. Thus, it is unknown which of the dermal or oral teeth evolved first. Neither is it sure how many times it has happened if it turns out to be the case. It has even been suggested that the original bony plates of all the vertebrates are gone and that the present scales are just modified teeth, even if both teeth and the body armor have a common origin a long time ago. However, there is no evidence of this at the moment.
Respiratory system.
All Chondrichthyes breathe through five to seven pairs of gills, depending on the species. In general, pelagic species must keep swimming to keep oxygenated water moving through their gills, whilst demersal species can actively pump water in through their spiracles and out through their gills. However, this is only a general rule and many species differ.
A spiracle is a small hole found behind each eye. These can be tiny and circular, such as found on the nurse shark ("Ginglymostoma cirratum"), to extended and slit-like, such as found on the wobbegongs (Orectolobidae). Many larger, pelagic species such as the mackerel sharks (Lamnidae) and the thresher sharks (Alopiidae) no longer possess them.
Immune system.
Like all other jawed vertebrates, members of Chondrichthyes have an adaptive immune system.
Reproduction.
Fertilization is internal. Development is usually live birth (ovoviviparous species) but can be through eggs (oviparous). Some rare species are viviparous. There is no parental care after birth; however, some Chondrichthyes do guard their eggs.
Classification.
The class Chondrichthyes has two subclasses: the subclass Elasmobranchii (sharks and rays) and the subclass Holocephali (chimaeras).
Evolution.
Unequivocal fossils of cartilaginous fishes first appeared in the fossil record by about 395 million years ago, during the middle Devonian. The radiation of elasmobranches in the chart on the right is divided into the taxa: Cladoselache, Eugeneodontiformes, Symmoriida, Xenacanthiformes, Ctenacanthiformes, Hybodontiformes, Galeomorphi, Squaliformes and Batoidea.
By the start of the Early Devonian 419 mya (million years ago), jawed fishes had divided into four distinct clades: the placoderms and spiny sharks, both of which are now extinct, and the cartilaginous and bony fishes, both of which are still extant. The modern bony fishes, class Osteichthyes, appeared in the late Silurian or early Devonian, about 416 million years ago. Spiny sharks are not classified as true sharks or as cartilaginous fishes, but as a distinct group, class Acanthodii. However, both the cartilaginous and bony fishes may have arisen from either the placoderms or the spiny sharks. Cartilaginous fishes first appeared about 395 Ma. The first abundant genus of shark, "Cladoselache", appeared in the oceans during the Devonian Period.
A Bayesian analysis of molecular data suggests that the Holocephali and Elasmoblanchii diverged in the Silurian () and that the sharks and rays/skates split in the Carboniferous ().
Phylogeny.
 Subphylum Vertebrata
 └─Infraphylum Gnathostomata
 ├─Class Placodermi — "extinct" (armored gnathostomes)
 └Microphylum Eugnathostomata (true jawed vertebrates)
 ├─Class Chondrichthyes (cartilaginous fish)
 └─(unranked) Teleostomi (Acanthodii & Osteichthyes)
 ├─Class Acanthodii — "extinct" ("spiny sharks")
 └Superclass Osteichthyes (bony fish)
 ├─Class Actinopterygii (ray-finned fish)
 └─Class Sarcopterygii (lobe-finned fish)
  
 Note: lines show evolutionary relationships.

</doc>
<doc id="5233" url="http://en.wikipedia.org/wiki?curid=5233" title="Carl Linnaeus">
Carl Linnaeus

Carl Linnaeus (; 23 May 1707 – 10 January 1778), also known after his ennoblement as Carl von Linné , was a Swedish botanist, physician, and zoologist, who laid the foundations for the modern biological naming scheme of binomial nomenclature. He is known as the father of modern taxonomy, and is also considered one of the fathers of modern ecology. Many of his writings were in Latin, and his name is rendered in Latin as (after 1761 Carolus a Linné).
Linnaeus was born in the countryside of Småland, in southern Sweden. He received most of his higher education at Uppsala University, and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published a first edition of his "" in the Netherlands. He then returned to Sweden, where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and '60s, he continued to collect and classify animals, plants, and minerals, and published several volumes. At the time of his death, he was one of the most acclaimed scientists in Europe.
The Swiss philosopher Jean-Jacques Rousseau sent him the message: "Tell him I know no greater man on earth." The German writer Johann Wolfgang von Goethe wrote: "With the exception of Shakespeare and Spinoza, I know no one among the no longer living who has influenced me more strongly." Swedish author August Strindberg wrote: "Linnaeus was in reality a poet who happened to become a naturalist". Among other compliments, Linnaeus has been called "" (Prince of Botanists), "The Pliny of the North," and "The Second Adam".
In botany, the author abbreviation used to indicate Linnaeus as the authority for species' names is L. In older publications, sometimes the abbreviation "Linn." is found (for instance in: ). Linnaeus' remains comprise the type specimen for the species "Homo sapiens", following the International Code of Zoological Nomenclature, since the sole specimen he is known to have examined when writing the species description was himself.
Biography.
Childhood.
Carl Linnaeus was born in the village of Råshult in Småland, Sweden, on 23 May 1707. He was the first child of Nils Ingemarsson Linnaeus and Christina Brodersonia. His father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the University of Lund, he had to take on a family name. He adopted the Latinate name Linnæus after a giant linden tree (or lime tree), "" in Swedish, that grew on the family homestead. This name was spelled with the æ ligature. When Carl was born, he was named Carl Linnæus, with his father's family name. The son also always spelled it with the æ ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linnæus.
One of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Småland. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius. She subsequently gave birth to three daughters and another son, Samuel (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping). A year after Linnaeus' birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house.
Even in his early years, Linnaeus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnaeus was given his own patch of earth where he could grow plants.
Early education.
Linnaeus' father began teaching him Latin, religion, and geography at an early age; one account says that due to family use of Latin for conversation, the boy learned Latin before he learned Swedish. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander "was better calculated to extinguish a child's talents than develop them." Two years after his tutoring had begun, he was sent to the Lower Grammar School at Växjö in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus' interest in botany and gave him the run of his garden. He also introduced him to Johan Rothman, the state doctor of Småland and a teacher at Katedralskolan (a gymnasium) in Växjö. Also a botanist, Rothman broadened Linnaeus' interest in botany and helped him develop an interest in medicine. At the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal "read day and night, knowing like the back of my hand, Arvidh Månsson's Rydaholm Book of Herbs, Tillandz's Flora Åboensis, Palmberg's Serta Florea Suecana, Bromelii Chloros Gothica and Rudbeckii Hortus Upsaliensis..." 
Linnaeus entered the Växjö Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus' father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in Växjö and to teach him physiology and botany. Nils accepted this offer.
University studies.
Lund.
Rothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to Sébastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Skåne. He was registered as "", the Latin form of his full name, which he also used later for his Latin publications.
Professor Kilian Stobæus, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Skåne, together with students sharing the same interests.
Uppsala.
In August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.
In 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius' house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.
Rudbeck's former assistant, Nils Rosén, returned to the University in March 1731 with a degree in medicine. Rosén started giving anatomy lectures and tried to take over Linnaeus' botany lectures, but Rudbeck prevented that. Until December, Rosén gave Linnaeus private tutoring in medicine. In December, Linnaeus had a "disagreement" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.
Expedition to Lapland.
During a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus' hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.
Linnaeus began his expedition from Uppsala in May; he travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near Gävle he found great quantities of "Campanula serpyllifolia", later known as "Linnaea borealis", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.
Linnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Umeå, Luleå and Tornio. He returned from his six-month long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book "".
In ' Linnaeus' ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described "" as "the most classic and delightful" of Linnaeus's works.
It was also during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was traveling, Linnaeus remarked: "If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds."
Dalarna.
In 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at Røros.
Doctorate.
Back in Uppsala, Linnaeus' relations with Nils Rosén worsened, and thus he gladly accepted an invitation from the student Claes Sohlberg to spend the Christmas holiday in Falun with Sohlberg's family. Sohlberg's father was a mining inspector, and let Linnaeus visit the mines near Falun. Sohland's father suggested to Linnaeus he should bring Sohlberg to the Dutch Republic and continue to tutor him there for an annual salary. At that time, the Dutch Republic was one of the most revered places to study natural history and a common place for Swedes to take their doctoral degree; Linnaeus, who was interested in both of these, accepted.
In April 1735, Linnaeus and Sohlberg set out for the Netherlands, with Linnaeus to take a doctoral degree in medicine at the University of Harderwijk. On the way, they stopped in Hamburg, where they met the mayor, who proudly showed them a wonder of nature which he possessed: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered it was a fake: jaws and clawed feet from weasels and skins from snakes had been glued together. The provenance of the hydra suggested to Linnaeus it had been manufactured by monks to represent the Beast of Revelation. As much as this may have upset the mayor, Linnaeus made his observations public and the mayor's dreams of selling the hydra for an enormous sum were ruined. Fearing his wrath, Linnaeus and Sohlberg had to leave Hamburg quickly.
When Linnaeus reached Harderwijk, he began working toward a degree immediately; at the time, Harderwijk was known for awarding "instant" degrees after as little as a week. First he handed in a thesis on the cause of malaria he had written in Sweden, which he then defended in a public debate. He is now known to have been wrong about the cause, not having a microscope good enough to see malarial parasites, which were spread by mosquitoes living in the stagnant water around the clay soils.
The next step was to take an oral examination and to diagnose a patient. After less than two weeks, he took his degree and became a doctor, at the age of 28. During the summer, Linnaeus met a friend from Uppsala, Peter Artedi. Before their departure from Uppsala, Artedi and Linnaeus had decided should one of them die, the survivor would finish the other's work. Ten weeks later, Artedi drowned in one of the canals of Amsterdam, and his unfinished manuscript on the classification of fish was left to Linnaeus to complete.
Publishing of ".
One of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as ".
Linnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and "".
George Clifford.
In August, during Linnaeus' stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus' ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's "Natural History of Jamaica", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus became the botanical curator and house physician at Hartekamp, free to buy any book or plant he wanted.
In July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in '. Miller was impressed, and from then on started to arrange the garden according to Linnaeus' system. Linnaeus also traveled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed, however, to make Dillenius publicly accept his new classification system. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with "", with another sixty ("sexaginta") genera.
His work at Hartekamp led to another book, "", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name "Nepenthes", which Linnaeus used to describe a genus of pitcher plants.
Linnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never left Sweden again.
Return to Sweden.
When Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Moræa. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses in the academy by drawing of lots.
Because his finances had improved and were now sufficient to support a family, he received permission to marry his fiancée, Sara Elisabeth Moræa. Their wedding was held 26 June 1739. Seven months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.
In May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Rosén, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-year-old son followed him to live in Uppsala.
Öland and Gotland.
Ten days after he was appointed Professor, he undertook an expedition to the island provinces of Öland and Gotland with six students from the university, to look for plants useful in medicine. First, they travelled to Öland and stayed there until 21 June, when they sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in Öland and Gotland.
During the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius' scale was inverted compared to today, the boiling point at 0 °C and freezing point at 100 °C. In 1745, Linnaeus inverted the scale to its present standard.
Västergötland.
In the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of Västergötland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book "", published the next year. After returning from the journey the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.
In 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick—a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.
Scania.
In the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the Government. With him he brought his student, Olof Söderberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. The journey was successful, and Linnaeus' observations were published the next year in "".
Rector of Uppsala University.
In 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his "apostles". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.
"Philosophia Botanica".
Linnaeus published "Philosophia Botanica" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.
"Species Plantarum".
Linnaeus published "Species Plantarum", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order.
Ennoblement.
Linnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and Sävja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.
Since the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.
The Swedish king Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linné (Latinized as ""), 'Linné' being a shortened and gallicised version of 'Linnæus', and the German title 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus' favourite plants; it was given the scientific name "Linnaea borealis" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the center is an egg "to denote Nature, which is continued and perpetuated "in ovo"." At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads "": we extend our fame by our deeds.
After his ennoblement, Linnaeus continued teaching and writing. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, "the Linnaeus of the Austrian Empire", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected him and showed great interest in his work. He named a solanaceous genus, "Scopolia", the source of scopolamine, after him. Because of a great distance, they didn't ever meet.
Final years.
Linnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.
Linnaeus' last years were troubled by illness. He had suffered from a disease called the Uppsala fever in 1764, but survived thanks to the care of Rosén. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He suffered a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognize himself as their author.
In December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was interred in Uppsala Cathedral on 22 January.
His library and collections were left to his widow Sara and their children. Joseph Banks, an English botanist, wanted to buy the collection, but his son Carl refused and moved the collection to Uppsala. However, in 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.
The von Linné name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.
Apostles.
During Linnaeus' time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called "apostles". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus' system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes without Linnaeus' new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.
Early expeditions.
Christopher Tärnström, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. Tärnström never reached his destination, dying of a tropical fever on Côn Sơn Island the same year. Tärnström's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after Tärnström. Six other apostles later died on their expeditions, including Pehr Forsskål and Pehr Löfling.
Two years after Tärnström's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in "Species Plantarum" had been brought back by Kalm.
Cook expeditions and Japan.
Daniel Solander was living in Linnaeus' house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his oldest daughter's hand in marriage. On Linnaeus' recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the "Endeavour" in 1768–71. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the "Resolution" in 1772–75 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.
Perhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus' death.
Major publications.
"Systema Naturae".
The first edition of ' was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. In it, the unwieldy names mostly used at the time, such as "'", were supplemented with concise and now familiar "binomials", composed of the generic name, followed by a specific epithet – in the case given, "Physalis angulata". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.
' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.
" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).
' (1751) was a summary of Linnaeus' thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as well as ' (1737) and " (1735)), the "Philosophia" being simultaneously released in Stockholm.
Linnaean collections.
At the end of his lifetime the Linnean collection in Uppsala was considered as one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744–1745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerström (in 1748 and 1750) and Jonas Alströmer (in 1749). The relation between the museum and the private collection was not formalized and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 'the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers'. He had turned his own estate into a microcosm of that 'world museum'.
In April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his countryhouse at Hammarby near Uppsala. This resulted in a physical separation between the two collections, the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.
In Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linné's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linné's death his son confirmed that mice had caused "horrible damage" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.
In 1784 the botanist James Edward Smith purchased nearly all of the Linnean private scientific effects from the widow and daughter of Carl Linnaeus and transferred them to London. Not all material in Linné's private collection was transported to England. Thirty-three fish specimens preserved in alcohol were not sent and were later lost.
In London Smith tended to neglect the zoological parts of the collection, he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.
Much material which had been intensively studied by Linné in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720–1782) (in the Linnean publications referred to as "Museum Ludovicae Ulricae" or "M. L. U."). This collection was donated by his grandson King Gustav IV Adolf (1778–1837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710–1771) (in the Linnean sources known as "Museum Adolphi Friderici" or "Mus. Ad. Fr."), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.
Linnaean taxonomy.
The establishment of universally accepted conventions for the naming of organisms was Linnaeus' main contribution to taxonomy—his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the "Linnaean taxonomy"; the system of scientific classification now widely used in the biological sciences.
The Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera ("singular:" genus), which were divided into Species ("singular:" species). Below the rank of species he sometimes recognized taxa of a lower (unnamed) rank; these have since acquired standardised names such as "variety" in botany and "subspecies" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus' original system.
Linnaeus' groupings were based upon shared physical characteristics, and not simply upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid "observable characteristics" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus' time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.
Influences and economic beliefs.
Linnaeus' applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidized investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.
Views on mankind.
According to German biologist Ernst Haeckel, the question of man's origin began with Linnaeus. He helped future research in the natural history of man by describing humans just as he described any other plant or animal.
Anthropomorpha.
Linnaeus classified humans among the primates (as they were later called) beginning with the first edition of "". During his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, "Anthropomorpha", meaning "manlike." This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe a human as 'like a man'. In a letter to Gmelin from 1747, Linnaeus replied:
"It does not please [you] that I've placed Man among the Anthropomorpha, perhaps because of the term 'with human form', but man learns to know himself. Let's not quibble over words. It will be the same to me whatever name we apply. But I seek from you and from the whole world a generic difference between man and simian that [follows] from the principles of Natural History. I absolutely know of none. If only someone might tell me a single one! If I would have called man a simian or vice versa, I would have brought together all the theologians against me. Perhaps I ought to have by virtue of the law of the discipline."
The theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation–evolution controversy began in earnest with the publication of "On the Origin of Species" by Charles Darwin in 1859.
After such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including "Mammalia" and "Primates", the latter of which would replace "Anthropomorpha" as well as giving humans the full binomial "Homo sapiens". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place to rule over nature, not be a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, "One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'aoutomata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility."
Strange people in distant lands.
Linnaeus added a second species to the genus "Homo" in "" based on a figure and description by Jacobus Bontius from a 1658 publication: "Homo troglodytes" ("caveman") and published a third in 1771: "Homo lar". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus.
In early editions of "", many well-known legendary creatures were included such as the phoenix, dragon and manticore as well as cryptids like the satyrus, which Linnaeus collected into the catch-all category "Paradoxa". Broberg thought Linnaeus was trying to offer a natural explanation and demystify the world of superstition. Linnaeus tried to debunk some of these creatures, as he had with the hydra; regarding the purported remains of dragons, Linnaeus wrote that they were either derived from lizards or rays. For "Homo troglodytes" he asked the Swedish East India Company to search for one, but they did not find any signs of its existence. "Homo lar" has since been reclassified as "Hylobates lar", the lar gibbon.
Four races.
In the first edition of "", Linnaeus subdivided the human species into four varieties based on continent and skin colour: "Europæus albus" (white European), "Americanus rubescens" (red American), "Asiaticus fuscus" (brown Asian) and "Africanus niger" (black African). In the tenth edition of Systema Naturae he further detailed stereotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to "luridus" (yellow). Additionally, Linnaeus created a wastebasket taxon "monstrosus" for "wild and monstrous humans, unknown groups, and more or less abnormal people".
Commemoration.
Anniversaries of Linnaeus' birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Parliament of Sweden, Växjö University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus "Linnaea", the crater Linné on the Earth's moon and the cobalt sulfide mineral Linnaeite.
Commentary on Linnaeus.
Andrew Dickson White wrote in "" (1896):
Linnaeus ... was the most eminent naturalist of his time, a wide observer, a close thinker; but the atmosphere in which he lived and moved and had his being was saturated with biblical theology, and this permeated all his thinking. ... Toward the end of his life he timidly advanced the hypothesis that all the species of one genus constituted at the creation one species; and from the last edition of his "Systema Naturæ" he quietly left out the strongly orthodox statement of the fixity of each species, which he had insisted upon in his earlier works. ... warnings came speedily both from the Catholic and Protestant sides.
The mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, places Carl Linnaeus at the first position among top 100 historical figures.

</doc>
<doc id="5236" url="http://en.wikipedia.org/wiki?curid=5236" title="Coast">
Coast

A coastline or a seashore is the area where land meets the sea or ocean. A precise line that can be called a coastline cannot be determined due to the Coastline paradox.
The term "coastal zone" is a region where interaction of the sea and land processes occurs. Both the terms coast and coastal are often used to describe a geographic location or region; for example, New Zealand's West Coast, or the East and West Coasts of the United States.
A pelagic coast refers to a coast which fronts the open ocean, as opposed to a more sheltered coast in a gulf or bay. A shore, on the other hand, can refer to parts of the land which adjoin any large body of water, including oceans (sea shore) and lakes (lake shore). Similarly, the somewhat related term "bank" refers to the land alongside or sloping down to a river (riverbank) or to a body of water smaller than a lake. "Bank" is also used in some parts of the world to refer to an artificial ridge of earth intended to retain the water of a river or pond; in other places this may be called a levee.
While many scientific experts might agree on a common definition of the term "coast", the delineation of the extents of a coast differ according to jurisdiction, with many scientific and government authorities in various countries differing for economic and social policy reasons. According to the UN atlas, 44% of people live within of the sea.
Formation.
Tides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deprossosition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up river estuaries from the ocean.
Waves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Coastlines with longer shores have more room for the waves to disperse their energy, while coasts with cliffs and short shore faces give little room for the wave energy to be dispersed. In these areas the wave energy breaking against the cliffs is higher, and air and water are compressed into cracks in the rock, forcing the rock apart, breaking it down. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. This forms an abrasion or cliffed coast.
Sediment deposited by rivers is the dominant influence on the amount of sediment located on a coastline. Today riverine deposition at the coast is often blocked by dams and other human regulatory devices, which remove the sediment from the stream by causing it to be deposited inland.
Like the ocean which shapes them, coasts are a dynamic environment with constant change. The Earth's natural processes, particularly sea level rises, waves and various weather phenomena, have resulted in the erosion, accretion and reshaping of coasts as well as flooding and creation of continental shelves and drowned river valleys (rias).
Environmental importance.
The coast and its adjacent areas on and off shore are an important part of a local ecosystem: the mixture of fresh water and salt water in estuaries provides many nutrients for marine life. Salt marshes and beaches also support a diversity of plants, animals and insects crucial to the food chain.
The high level of biodiversity creates a high level of biological activity, which has attracted human activity for thousands of years.
Human impacts.
Human uses of coasts.
More and more of the world's people live in coastal regions. Many major cities are on or near good harbors and have port facilities. Some landlocked places have achieved port status by building canals.
The coast is a frontier that nations have typically defended against military invaders, smugglers and illegal migrants. Fixed coastal defenses have long been erected in many nations and coastal countries typically have a navy and some form of coast guard.
Coasts, especially those with beaches and warm water, attract tourists. In many island nations such as those of the Mediterranean, South Pacific and Caribbean, tourism is central to the economy. Coasts offer recreational activities such as swimming, fishing, surfing, boating, and sunbathing. Growth management can be a challenge for coastal local authorities who often struggle to provide the infrastructure required by new residents.
Threats to a coast.
Coasts also face many human-induced environmental impacts. The human influence on climate change is thought to contribute to an accelerated trend in sea level rise which threatens coastal habitats.
Pollution can occur from a number of sources: garbage and industrial debris; the transportation of petroleum in tankers, increasing the probability of large oil spills; small oil spills created by large and small vessels, which flush bilge water into the ocean.
Fishing has declined due to habitat degradation, overfishing, trawling, bycatch and climate change. Since the growth of global fishing enterprises after the 1950s, intensive fishing has spread from a few concentrated areas to encompass nearly all fisheries. The scraping of the ocean floor in bottom dragging is devastating to coral, sponges and other long-lived species that do not recover quickly. This destruction alters the functioning of the ecosystem and can permanently alter species composition and biodiversity. Bycatch, the capture of unintended species in the course of fishing, is typically returned to the ocean only to die from injuries or exposure. Bycatch represents about a quarter of all marine catch. In the case of shrimp capture, the bycatch is five times larger than the shrimp caught.
It is believed that melting Arctic ice will cause sea levels to rise and flood costal areas.
Conservation.
Extraordinary population growth in the 20th century has placed stress on the planet's ecosystems. For example, on Saint Lucia, harvesting mangrove for timber and clearing for fishing reduced the mangrove forests, resulting in a loss of habitat and spawning grounds for marine life that was unique to the area. These forests also helped to stabilize the coastline. Conservation efforts since the 1980s have partially restored the ecosystem.
Types of coast.
According to one principle of classification, an emergent coastline is a coastline which has experienced a fall in sea level, because of either a global sea level change, or local uplift. Emergent coastlines are identifiable by the coastal landforms, which are above the high tide mark, such as raised beaches. In contrast, a submergent coastline is one where the sea level has risen, due to a global sea level change, local subsidence, or isostatic rebound. Submergent coastlines are identifiable by their submerged, or "drowned" landforms, such as rias (drowned valleys) and fjords.
According to a second principle of classification, a concordant coastline is a coastline where bands of different rock types run parallel to the shore. These rock types are usually of varying resistance, so the coastline forms distinctive landforms, such as coves. Discordant coastlines feature distinctive landforms because the rocks are eroded by ocean waves. The less resistant rocks erode faster, creating inlets or bays; the more resistant rocks erode more slowly, remaining as headlands or outcroppings.
Other coastal categories:
Coastal landforms.
The following articles describe some coastal landforms
Coastal processes.
The following articles describe the various geologic processes that affect a coastal zone:
Wildlife.
Animals.
Some of the animals live along a typical coast. There are animals like puffins, sea turtles and rockhopper penguins. Sea snails and various kinds of barnacles live on the coast and scavenge on food deposited by the sea. Most coastal animals are used to humans in developed areas, such as dolphins and seagulls who eat food thrown for them by tourists. Since the coastal areas are all part of the littoral zone, there is a profusion of marine life found just off-coast.
There are many kinds of seabirds on the coast. Pelicans and cormorants join up with terns and oystercatchers to forage for fish and shellfish on the coast. There are sea lions on the coast of Wales and other countries.
Plants.
Coastal areas are famous for their kelp beds. Kelp is a fast-growing seaweed that grows up to a metre a day. Corals and sea anemones are true animals, but live a lifestyle similar to that of plants. Mangroves, seagrasses and salt marsh are important coastal vegetation types in tropical and temperate environments respectively.
Coastline statistics.
Coastline problem.
Shortly before 1951, Lewis Fry Richardson, in researching the possible effect of border lengths on the probability of war, noticed that the Portuguese reported their measured border with Spain to be 987 km, but the Spanish reported it as 1214 km. This was the beginning of the coastline problem, which is a mathematical uncertainty inherent in the measurement of boundaries that are irregular.
The prevailing method of estimating the length of a border (or coastline) was to lay out "n" equal straight-line segments of length "ℓ" with dividers on a map or aerial photograph. Each end of the segment must be on the boundary. Investigating the discrepancies in border estimation, Richardson discovered what is now termed the Richardson Effect: the sum of the segments is inversely proportional to the common length of the segments. In effect, the shorter the ruler, the longer the measured border; the Spanish and Portuguese geographers were simply using different-length rulers.
The result most astounding to Richardson is that, under certain circumstances, as "ℓ" approaches zero, the length of the coastline approaches infinity. Richardson had believed, based on Euclidean geometry, that a coastline would approach a fixed length, as do similar estimations of regular geometric figures. For example, the perimeter of a regular polygon inscribed in a circle approaches the circumference with increasing numbers of sides (and decrease in the length of one side). In geometric measure theory such a smooth curve as the circle that can be approximated by small straight segments with a definite limit is termed a rectifiable curve.
Measuring a coastline.
More than a decade after Richardson completed his work, Benoit Mandelbrot developed a new branch of mathematics, fractal geometry, to describe just such non-rectifiable complexes in nature as the infinite coastline. His own definition of the new figure serving as the basis for his study is:
A key property of the fractal is self-similarity; that is, at any scale the same general configuration appears. A coastline is perceived as bays alternating with promontories. In the hypothetical situation that a given coastline has this property of self-similarity, then no matter how greatly any one small section of coastline is magnified, a similar pattern of smaller bays and promontories superimposed on larger bays and promontories appears, right down to the grains of sand. At that scale the coastline appears as a momentarily shifting, potentially infinitely long thread with a stochastic arrangement of bays and promontories formed from the small objects at hand. In such an environment (as opposed to smooth curves) Mandelbrot asserts "coastline length turns out to be an elusive notion that slips between the fingers of those who want to grasp it."
There are different kinds of fractals. A coastline with the stated property is in "a first category of fractals, namely curves whose fractal dimension is greater than 1." That last statement represents an extension by Mandelbrot of Richardson's thought. Mandelbrot's statement of the Richardson Effect is:
where L, coastline length, a function of the measurement unit, ε, is approximated by the expression. F is a constant and D is a parameter that Richardson found depended on the coastline approximated by L. He gave no theoretical explanation but Mandelbrot identified D with a non-integer form of the Hausdorff dimension, later the fractal dimension. Rearranging the right side of the expression obtains:
where Fε-D must be the number of units ε required to obtain L. The fractal dimension is the number of the dimensions of the figure being used to approximate the fractal: 0 for a dot, 1 for a line, 2 for a square. D in the expression is between 1 and 2, for coastlines typically less than 1.5. The broken line measuring the coast does not extend in one direction nor does it represent an area, but is intermediate. It can be interpreted as a thick line or band of width 2ε. More broken coastlines have greater D and therefore L is longer for the same ε. Mandelbrot showed that D is independent of ε.

</doc>
<doc id="5237" url="http://en.wikipedia.org/wiki?curid=5237" title="Catatonia">
Catatonia

Catatonia is a state of neurogenic motor immobility and behavioral abnormality manifested by stupor. It was first described in 1874 by Karl Ludwig Kahlbaum, in "Die Katatonie oder das Spannungsirresein" ("Catatonia or Tension Insanity").
In the current "Diagnostic and Statistical Manual of Mental Disorders" published by the American Psychiatric Association (DSM-5) catatonia is not recognized as a separate disorder, but is associated with psychiatric conditions such as schizophrenia (catatonic type), bipolar disorder, post-traumatic stress disorder, depression and other mental disorders, as well as drug abuse or overdose (or both). It may also be seen in many medical disorders including infections (such as encephalitis), autoimmune disorders, focal neurologic lesions (including strokes), metabolic disturbances, alcohol withdrawal and abrupt or overly rapid benzodiazepine withdrawal.
It can be an adverse reaction to prescribed medication. It bears similarity to conditions such as encephalitis lethargica and neuroleptic malignant syndrome. There are a variety of treatments available; benzodiazepines are a first-line treatment strategy. Electro-convulsive therapy is also sometimes used. There is growing evidence for the effectiveness of NMDA antagonists for benzodiazepine resistant catatonia. Antipsychotics are sometimes employed but require caution as they can worsen symptoms and have serious adverse effects.
Clinical features.
Patients with catatonia may experience an extreme loss of motor skill or even constant hyperactive motor activity. Catatonic patients will sometimes hold rigid poses for hours and will ignore any external stimuli. Patients with catatonic excitement can suffer from exhaustion if not treated. Patients may also show stereotyped, repetitive movements.
They may show specific types of movement such as waxy flexibility, in which they maintain positions after being placed in them through someone else in which they resist movement in proportion to the force applied by the examiner. They may repeat meaningless phrases or speak only to repeat what the examiner says.
While catatonia is only identified as a symptom of schizophrenia in present psychiatric classifications, it is increasingly recognized as a syndrome with many faces. It appears as the Kahlbaum syndrome (motionless catatonia), malignant catatonia (neuroleptic malignant syndrome, toxic serotonin syndrome), and excited forms (delirious mania, catatonic excitement, oneirophrenia). 
It has also been recognized as grafted on to autism spectrum disorders.
Diagnostic criteria.
According to the DSM-V, "Catatonia Associated with Another Mental Disorder (Catatonia Specifier)" is diagnosed if the clinical picture is dominated by at least three of the following:
Rating scale.
Fink and Taylor developed a catatonia rating scale to identify the syndrome. A diagnosis is verified by a benzodiazepine or barbiturate test. The diagnosis is validated by the quick response to either benzodiazepines or electroconvulsive therapy (ECT). While proven useful in the past, barbiturates are no longer commonly used in psychiatry; thus the option of either benzodiazepines or ECT.
Treatment.
Initial treatment is aimed at providing symptomatic relief. Benzodiazepines are the first line of treatment, and high doses are often required. A test dose of 1–2 mg of intramuscular lorazepam will often result in marked improvement within half an hour. In France, zolpidem has also been used in diagnosis, and response may occur within the same time period. Ultimately the underlying cause needs to be treated.
Electroconvulsive therapy (ECT) is an effective treatment for catatonia as well as for most of the underlying causes (e.g. psychosis, mania, depression). Antipsychotics should be used with care as they can worsen catatonia and are the cause of neuroleptic malignant syndrome, a dangerous condition that can mimic catatonia and requires immediate discontinuation of the antipsychotic.
Excessive glutamate activity is believed to be involved in catatonia; when first-line treatment options fail, NMDA antagonists such as amantadine or memantine are used. Amantadine may have an increased incidence of tolerance with prolonged use and can cause psychosis, due to its additional effects on the dopamine system. Memantine has a more targeted pharmacological profile for the glutamate system, reduced incidence of psychosis and may therefore be preferred for individuals who cannot tolerate amantadine. Topiramate, is another treatment option for resistant catatonia; it produces its therapeutic effects by producing glutamate antagonism via modulation of AMPA receptors.

</doc>
<doc id="5244" url="http://en.wikipedia.org/wiki?curid=5244" title="Cipher">
Cipher

In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is "encipherment". To encipher or encode is to convert information from plain text into cipher or code. In non-technical usage, a 'cipher' is the same thing as a 'code'; however, the concepts are distinct in cryptography. In classical cryptography, ciphers were distinguished from codes. 
Codes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input. 
Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. For example, "UQJHSE" could be the code for "Proceed to the following coordinates". When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.
The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a "cryptovariable"). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.
Most modern ciphers can be categorized in several ways
Etymology.
"Cipher" is alternatively spelled "cypher"; similarly "ciphertext" and "cyphertext", and so forth.
The word "cipher" in former times meant "zero" and had the same origin: Middle French as ' and Medieval Latin as "cifra," from the Arabic صفر"' "ṣifr" = zero (see Zero—Etymology). "Cipher" was later used for any decimal digit, even any number. There are many theories about how the word "cipher" may have come to mean "encoding":It was firstly introduced by Abū ʿAbdallāh Muḥammad ibn Mūsā al-Khwārizmī.
Ibrahim Al-Kadi concluded that the Arabic word "sifr", for the digit zero, developed into the European technical term for encryption.
Versus codes.
In non-technical usage, a "(secret) code" typically means a "cipher". Within technical discussions, however, the words "code" and "cipher" refer to two different concepts. Codes work at the level of meaning—that is, words or phrases are converted into something else and this chunking generally shortens the message.
An example of this is the Telegraph Code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of Telegrams.
Ciphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms codes and ciphers are also used synonymously to substitution and transposition.
Historically, cryptography was split into a dichotomy of codes and ciphers; and coding had its own terminology, analogous to that for ciphers: ""encoding", "codetext", "decoding"" and so on.
However, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.
Types.
There are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.
Historical.
Historical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as Rot 13) and transposition ciphers (such as a Rail Fence Cipher). For example "GOOD DOG" can be encrypted as "PLLX XLP" where "L" substitutes for "O", "P" for "G", and "X" for "D" in the message. Transposition of the letters "GOOD DOG" can result in "DGOGDOO". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.
Simple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigenère) which changed the substitution alphabet for every letter. For example "GOOD DOG" can be encrypted as "PLSX TWF" where "L", "S", and "W" substitute for "O". With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.
During the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of "additive" substitution. In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.
Modern.
Modern encryption methods can be divided by two criteria: by type of key used, and by type of input data.
By type of key used ciphers are divided into:
In a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The Feistel cipher uses a combination of substitution and transposition techniques. Most block cipher algorithms are based on this structure. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a "public key" is published and enables any sender to perform encryption, while a "private key" is kept secret by the receiver and enables only him to perform correct decryption.
Ciphers can be distinguished into two types by the type of input data:
Key size and vulnerability.
In a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:
Since the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.
An example of this process can be found at which uses multiple reports to suggest that a symmetric cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 512 bits, all have similar difficulty at present.
Claude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.

</doc>
<doc id="5247" url="http://en.wikipedia.org/wiki?curid=5247" title="Country music">
Country music

Country music is a genre of American popular music that originated in Southern United States, in Atlanta, Georgia in the 1920s. It takes its roots from the southeastern genre of American folk music and Western music. Blues modes have been used extensively throughout its recorded history. Country music often consists of ballads and dance tunes with generally simple forms and harmonies accompanied by mostly string instruments such as banjos, electric and acoustic guitars, fiddles, and harmonicas. The term "country music" gained popularity in the 1940s in preference to the earlier term "hillbilly music"; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. The term "country music" is used today to describe many styles and sub genres. The origins of country music are the folk music of mostly white, working-class Americans, who blended popular songs, Irish and Celtic fiddle tunes, traditional ballads, and cowboy songs, and various musical traditions from European immigrant communities. In 2009 country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute in the United States.
Six generations of country music.
Immigrants to the Southern Appalachian Mountains of North America brought the music and instruments of Europe along with them for nearly 300 years. Country music was "introduced to the world as a Southern phenomenon." The first generation emerged in the early 1920s, with Atlanta's music scene playing a major role in launching country's earliest recording artists. Okeh Records began issuing hillbilly music records by Fiddlin' John Carson as early as 1923, followed by Columbia Records (series 15000D "Old Familiar Tunes") (Samantha Bumgarner) in 1924, and RCA Victor Records in 1927 (the Carter Family and Jimmie Rodgers). Many "hillbilly" musicians, such as Cliff Carlisle, recorded blues songs throughout the 1920s.
During the second generation (1930s–1940s), radio became a popular source of entertainment, and "barn dance" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California. The most important was the "Grand Ole Opry", aired starting in 1925 by WSM in Nashville and continuing to the present day. During the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a "hot string band," and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded "Boogie Woogie".
The third generation (1950s-1960s) started at the end of World War II with "mountaineer" string band music known as bluegrass, which emerged when Bill Monroe, along with Lester Flatt and Earl Scruggs were introduced by Roy Acuff at the Grand Ole Opry. Gospel music remained a popular component of country music. Another type of stripped-down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among poor whites in Texas and Oklahoma. It became known as honky tonk, and had its roots in Western swing and the ranchera music of Mexico and the border states. By the early 1950s a blend of Western swing, country boogie, and honky tonk was played by most country bands. Rockabilly was most popular with country fans in the 1950s, and 1956 could be called the year of rockabilly in country music. Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville Sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the "old values" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.
Fourth generation (1970s–1980s) music included outlaw country and country pop or soft pop, with roots in the countrypolitan sound, folk music, and soft rock. Between 1972 and 1975 singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles. During the early 1980s country artists continued to see their records perform well on the pop charts. In 1980 a style of "neocountry disco music" was popularized. During the mid-1980s a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts in favor of more traditional "back-to-basics" production.
During the fifth generation (1990s), country music became a worldwide phenomenon thanks to Garth Brooks. The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s.
The sixth generation (2000s–present) is exemplified by country singer Carrie Underwood. The influence of rock music in country has become more overt during the late 2000s and early 2010s. Attempts to combine punk and country were pioneered by Jason and the Scorchers, and in the 1980s Southern Californian cowpunk scene with bands like the Long Ryders. Hip-hop also made its mark on country music with the emergence of country rap. Most of the best-selling country songs of this era however were in the country pop genre, such as those by Lady Antebellum, Florida Georgia Line, and Taylor Swift.
First generation (1920s).
Atlanta's music scene played a major role in launching country's earliest recording artists in the early 1920s—many Appalachian people had come to the city to work in its cotton mills and brought their music with them. It would remain a major recording center for two decades and a major performance center for four decades, up to the first country music TV shows on local Atlanta stations in the 1950s.
Some record companies in Atlanta turned away early artists such as Fiddlin' John Carson, while others realized that his music would fit perfectly with the lifestyle of the country's agricultural workers. The first commercial recordings of what was considered country music were "Arkansas Traveler" and "Turkey in the Straw" by fiddlers Henry Gilliland & A.C. (Eck) Robertson on June 30, 1922, for Victor Records and released in April 1923. Columbia Records began issuing records with "hillbilly" music (series 15000D "Old Familiar Tunes") as early as 1924.
A year later, on June 14, 1923, Fiddlin' John Carson recorded "Little Log Cabin in the Lane" for Okeh Records. Vernon Dalhart was the first country singer to have a nationwide hit in May 1924 with "Wreck of the Old 97". The flip side of the record was "Lonesome Road Blues", which also became very popular. In April 1924, "Aunt" Samantha Bumgarner and Eva Davis became the first female musicians to record and release country songs.
Many "hillbilly" musicians, such as Cliff Carlisle, recorded blues songs throughout the decade and into the 1930s. Other important early recording artists were Riley Puckett, Don Richardson, Fiddlin' John Carson, Uncle Dave Macon, Al Hopkins, Ernest V. Stoneman, Charlie Poole and the North Carolina Ramblers and The Skillet Lickers. The steel guitar entered country music as early as 1922, when Jimmie Tarlton met famed Hawaiian guitarist Frank Ferera on the West Coast.
Jimmie Rodgers and the Carter Family are widely considered to be important early country musicians. Their songs were first captured at a historic recording session in Bristol, Tennessee, on August 1, 1927, where Ralph Peer was the talent scout and sound recordist. A scene in the movie "O Brother, Where Art Thou?" depicts a similar occurrence in the same timeframe.
Rodgers fused hillbilly country, gospel, jazz, blues, pop, cowboy, and folk, and many of his best songs were his compositions, including "Blue Yodel", which sold over a million records and established Rodgers as the premier singer of early country music.
Beginning in 1927, and for the next 17 years, the Carters recorded some 300 old-time ballads, traditional tunes, country songs and gospel hymns, all representative of America's southeastern folklore and heritage.
Second generation (1930s–1940s).
One effect of the Great Depression was to reduce the number of records that could be sold. Radio became a popular source of entertainment, and "barn dance" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California.
The most important was the "Grand Ole Opry", aired starting in 1925 by WSM in Nashville and continuing to the present day. Some of the early stars on the "Opry" were Uncle Dave Macon, Roy Acuff and African American harmonica player DeFord Bailey. WSM's 50,000-watt signal (in 1934) could often be heard across the country.
Many musicians performed and recorded songs in any number of styles. Moon Mullican, for example, played Western swing but also recorded songs that can be called rockabilly. Between 1947 and 1949, country crooner Eddy Arnold placed eight songs in the top 10.
Singing cowboys and Western swing.
During the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Some of the popular singing cowboys from the era were Gene Autry, the Sons of the Pioneers, and Roy Rogers. Country music and western music were frequently played together on the same radio stations, hence the term "country and western" music.
And it wasn't only cowboys; cowgirls contributed to the sound in various family groups. Patsy Montana opened the door for female artists with her history-making song "I Want To Be a Cowboy's Sweetheart". This would begin a movement toward opportunities for women to have successful solo careers.
Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a "hot string band," and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Spade Cooley and Tex Williams also had very popular bands and appeared in films. At its height, Western swing rivaled the popularity of big band swing music.
Changing instrumentation.
Drums were scorned by early country musicians as being "too loud" and "not pure", but by 1935 Western swing big band leader Bob Wills had added drums to the Texas Playboys. In the mid-1940s, the Grand Ole Opry did not want the Playboys' drummer to appear on stage. Although drums were commonly used by rockabilly groups by 1955, the less-conservative-than-the-Grand Ole Opry "Louisiana Hayride" kept its infrequently used drummer back stage as late as 1956. By the early 1960s, however, it was rare that a country band didn't have a drummer.
Bob Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. A decade later (1948) Arthur Smith achieved top 10 US country chart success with his MGM Records recording of "Guitar Boogie", which crossed over to the US pop chart, introducing many people to the potential of the electric guitar. For several decades Nashville session players preferred the warm tones of the Gibson and Gretsch archtop electrics, but a "hot" Fender style, using guitars which became available beginning in the early 1950s, eventually prevailed as the signature guitar sound of country.
Hillbilly boogie.
Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded "Boogie Woogie". The trickle of what was initially called hillbilly boogie, or okie boogie (later to be renamed country boogie), became a flood beginning in late 1945. One notable release from this period was The Delmore Brothers' "Freight Train Boogie", considered to be part of the combined evolution of country music and blues towards rockabilly. In 1948, Arthur "Guitar Boogie" Smith achieved top ten US country chart success with his MGM Records recordings of "Guitar Boogie" and "Banjo Boogie", with the former crossing over to the US pop charts. Other country boogie artists included Merrill Moore and Tennessee Ernie Ford. The hillbilly boogie period lasted into the 1950s and remains one of many subgenres of country into the 21st century.
Bluegrass, folk and gospel.
By the end of World War II, "mountaineer" string band music known as bluegrass had emerged when Bill Monroe joined with Lester Flatt and Earl Scruggs, introduced by Roy Acuff at the Grand Ole Opry. Gospel music, too, remained a popular component of country music. Red Foley, the biggest country star following World War II, had one of the first million-selling gospel hits ("Peace in the Valley") and also sang boogie, blues and rockabilly.
In the post-war period, country music was called "folk" in the trades, and "hillbilly" within the industry. In 1944, "The Billboard" replaced the term "hillbilly" with "folk songs and blues," and switched to "country" or "country and Western" in 1949.
Honky Tonk.
Another type of stripped down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among poor whites in Texas and Oklahoma. It became known as honky tonk and had its roots in Western swing and the ranchera music of Mexico and the border states, particularly Texas, together with the blues of the American South. Bob Wills and His Texas Playboys personified this music which has been described as "a little bit of this, and a little bit of that, a little bit of black and a little bit of white ... just loud enough to keep you from thinking too much and to go right on ordering the whiskey." East Texan Al Dexter had a hit with "Honky Tonk Blues", and seven years later "Pistol Packin' Mama". These "honky tonk" songs associated barrooms, were performed by the likes of Ernest Tubb, Kitty Wells (the first major female country solo singer), Ted Daffan, Floyd Tillman, and the Maddox Brothers and Rose, Lefty Frizzell and Hank Williams, would later be called "traditional" country. Williams' influence in particular would prove to be enormous, inspiring many of the pioneers of rock and roll, such as Elvis Presley and Jerry Lee Lewis, as well as Chuck Berry and Ike Turner, while providing a framework for emerging honky tonk talents like George Jones. Webb Pierce was the top-charting country artist of the 1950s, with 13 of his singles spending 113 weeks at number one. He charted 48 singles during the decade; 31 reached the top ten and 26 reached the top four.
Third generation (1950s–1960s).
By the early 1950s a blend of Western swing, country boogie, and honky tonk was played by most country bands. Western music, influenced by the cowboy ballads and Tejano music rhythms of the southwestern U.S. and northern Mexico, reached its peak in popularity in the late 1950s, most notably with the song "El Paso", first recorded by Marty Robbins in September 1959.
The country music scene largely kept the music of the folk revival and folk rock at a distance, despite the similarity in instrumentation and origins (see, for instance, The Byrds' negative reception during their appearance on the "Grand Ole Opry"). The main concern was politics: the folk revival was largely driven by progressive activists, a stark contrast to the culturally conservative audiences of country music. Only a handful of folk artists, such as Burl Ives, John Denver and Canadian musician Gordon Lightfoot, would cross over into country music after the folk revival died out.
During the mid-1950s a new style of country music became popular, eventually to be referred to as rockabilly.
Rockabilly.
Rockabilly was most popular with country fans in the 1950s, and 1956 could be called the year of rockabilly in country music. Rockabilly was a mixture of rock-and-roll and hillbilly music. During this period Elvis Presley converted over to country music. He played a huge role in the music industry during this time. The number two, three and four songs on "Billboard's" charts for that year were Elvis Presley, "Heartbreak Hotel"; Johnny Cash, "I Walk the Line"; and Carl Perkins, "Blue Suede Shoes".
Cash and Presley placed songs in the top 5 in 1958 with No. 3 "Guess Things Happen That Way/Come In, Stranger" by Cash, and No. 5 by Presley "Don't/I Beg of You." Presley acknowledged the influence of rhythm and blues artists and his style, saying "The colored folk been singin' and playin' it just the way I'm doin' it now, man for more years than I know." But he also said, "My stuff is just hopped-up country." Within a few years, many rockabilly musicians returned to a more mainstream style or had defined their own unique style.
Country music gained national television exposure through "Ozark Jubilee" on ABC-TV and radio from 1955 to 1960 from Springfield, Missouri. The program showcased top stars including several rockabilly artists, some from the Ozarks. As Webb Pierce put it in 1956, "Once upon a time, it was almost impossible to sell country music in a place like New York City. Nowadays, television takes us everywhere, and country music records and sheet music sell as well in large cities as anywhere else."
The late 1950s saw the emergence of the Lubbock sound, but by the end of the decade, backlash as well as traditional artists such as Ray Price, Marty Robbins, and Johnny Horton began to shift the industry away from the rock n' roll influences of the mid-1950s.
The Nashville and countrypolitan sounds.
Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. Under the direction of producers such as Chet Atkins, Paul Cohen, Owen Bradley, and later Billy Sherrill, the sound brought country music to a diverse audience and helped revive country as it emerged from a commercially fallow period.
This subgenre was notable for borrowing from 1950s pop stylings: a prominent and smooth vocal, backed by a string section and vocal chorus. Instrumental soloing was de-emphasized in favor of trademark "licks". Leading artists in this genre included Jim Reeves, Skeeter Davis, The Browns, Patsy Cline, and Eddy Arnold. The "slip note" piano style of session musician Floyd Cramer was an important component of this style.
Nashville's pop song structure became more pronounced and it morphed into what was called countrypolitan. Countrypolitan was aimed straight at mainstream markets, and it sold well throughout the later 1960s into the early 1970s (a rarity in an era where American popular music was being decimated by the British Invasion). Top artists included Tammy Wynette, Lynn Anderson, and Charlie Rich, as well as such former "hard country" artists as Ray Price and Marty Robbins.
Despite the appeal of the Nashville sound, many traditional country artists emerged during this period and dominated the genre: Loretta Lynn, Merle Haggard, Buck Owens, Porter Wagoner, and Sonny James among them.
Country soul - crossover.
In 1962, Ray Charles surprised the pop world by turning his attention to country and western music, topping the charts and rating number three for the year on "Billboard's" pop chart with the "I Can't Stop Loving You" single, and recording the landmark album "Modern Sounds in Country and Western Music".
The Bakersfield sound.
Another genre of country music grew out of hardcore honky tonk with elements of Western swing and originated north-northwest of Los Angeles in Bakersfield, California. Influenced by one-time West Coast residents Bob Wills and Lefty Frizzell, by 1966 it was known as the Bakersfield sound. It relied on electric instruments and amplification, in particular the Telecaster electric guitar, more than other subgenres of country of the era, and can be described as having a sharp, hard, driving, no-frills, edgy flavor. Leading practitioners of this style were Buck Owens, Merle Haggard, Tommy Collins, Gary Allan, and Wynn Stewart, each of whom had his own style.
Country rock.
The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the "old values" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.
Early innovators in this new style of music in the 1960s and 1970s included Bob Dylan, who was the first to revert to country music with his 1967 album "John Wesley Harding" (and even more so with that album's follow-up, "Nashville Skyline"), followed by Gene Clark, Clark's former band The Byrds (with Gram Parsons on "Sweetheart of the Rodeo") and its spin-off The Flying Burrito Brothers (also featuring Gram Parsons), guitarist Clarence White, Michael Nesmith (The Monkees and the First National Band), the Grateful Dead, Neil Young, Commander Cody, The Allman Brothers, The Marshall Tucker Band, Poco, Buffalo Springfield, and The Eagles, among many. The Rolling Stones also got into the act with songs like "Honky Tonk Women" and "Dead Flowers".
Described by AllMusic as the "father of country-rock", Gram Parsons' work in the early 1970s was acclaimed for its purity and for his appreciation for aspects of traditional country music. Though his career was cut tragically short by his 1973 death, his legacy was carried on by his protégé and duet partner Emmylou Harris; Harris would release her debut solo in 1975, an amalgamation of country, rock and roll, folk, blues and pop.
Subsequent to the initial blending of the two polar opposite genres, other offspring soon resulted, including Southern rock, heartland rock and in more recent years, alternative country.
In the decades that followed, artists such as Juice Newton, Alabama, Hank Williams, Jr. (and, to an even greater extent, Hank Williams III), Gary Allan, Shania Twain, Brooks & Dunn, Faith Hill, Garth Brooks, Dwight Yoakam, Steve Earle, Dolly Parton, Rosanne Cash and Linda Ronstadt moved country further towards rock influence.
Decline of Western music and the cowboy ballad.
By the late 1960s, Western music, in particular the cowboy ballad, was in decline. Relegated to the "country and Western" genre by marketing agencies, popular Western recording stars released albums to only moderate success. Rock-and-roll dominated music sales, and Hollywood recording studios dropped most of their Western artists. The shift in country music production to Nashville also played a role, where the Nashville sound, country rock, and rockabilly music styles predominated over both "cowboy" artists and the more recent Bakersfield sound. The latter was largely limited to Buck Owens, Merle Haggard, and a few other bands. In the process, country and western music as a genre lost most of its southwestern, ranchera, and Tejano musical influences. However the cowboy ballad and honky-tonk music would be resurrected and reinterpreted in the 1970s with the growth in popularity of "outlaw country" music from Texas and Oklahoma.
Fourth generation (1970s–1980s).
Outlaw country.
Derived from the traditional Western and honky tonk musical styles of the late 1950s and 1960s, including Ray Price (whose band, the "Cherokee Cowboys", included Willie Nelson and Roger Miller) and mixed with the anger of an alienated subculture of the nation during the period, outlaw country revolutionized the genre of country music.
"After I left Nashville (the early 70s), I wanted to relax and play the music that I wanted to play, and just stay around Texas, maybe Oklahoma. Waylon and I had that outlaw image going, and when it caught on at colleges and we started selling records, we were O.K. The whole outlaw thing, it had nothing to do with the music, it was something that got written in an article, and the young people said, 'Well, that's pretty cool.' And started listening." (Willie Nelson)
The term "outlaw country" is traditionally associated with Hank Williams, Jr., Willie Nelson, Waylon Jennings, David Allan Coe, Whitey Morgan and the 78's, John Prine, Billy Joe Shaver, Gary Stewart, Townes Van Zandt, Kris Kristofferson, Michael Martin Murphey, and the later career renaissance of Johnny Cash, with a few female vocalists such as Jessi Colter and Sammi Smith. It was encapsulated in the 1976 album "Wanted! The Outlaws". A related subgenre is Red Dirt.
Country pop.
Country pop or soft pop, with roots in the countrypolitan sound, folk music, and soft rock, is a subgenre that first emerged in the 1970s. Although the term first referred to country music songs and artists that crossed over to top 40 radio, country pop acts are now more likely to cross over to adult contemporary music. It started with pop music singers like Glen Campbell, Bobbie Gentry, John Denver, Olivia Newton-John, Anne Murray, Marie Osmond, B. J. Thomas, The Bellamy Brothers, and Linda Ronstadt having hits on the country charts.
Between 1972 and 1975, singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles ("Rocky Mountain High", "Sunshine on My Shoulders", "Annie's Song", "Thank God I'm a Country Boy", and "I'm Sorry"), and was named Country Music Entertainer of the Year in 1975. The year before, Olivia Newton-John, an Australian pop singer, won the "Best Female Country Vocal Performance" as well as the Country Music Association's most coveted award for females, "Female Vocalist of the Year". In response George Jones, Tammy Wynette, and other traditional Nashville country artists dissatisfied with the new trend formed the short-lived Association of Country Entertainers in 1974.
During the mid-1970s, Dolly Parton, a highly successful mainstream country artist since the late 1960s, mounted a high profile campaign to cross over to pop music, culminating in her 1977 hit "Here You Come Again", which topped the U.S. country singles chart, and also reached No. 3 on the pop singles charts. Parton's male counterpart, Kenny Rogers, came from the opposite direction, aiming his music at the country charts, after a successful career in pop, rock and folk music, achieving success the same year with "Lucille", which topped the country charts and reached No. 5 on the U.S. pop singles charts. Parton and Rogers would both continue to have success on both country and pop charts simultaneously, well into the 1980s. Artists like Crystal Gayle, Ronnie Milsap and Barbara Mandrell would also find success on the pop charts with their records.
In 1975, author Paul Hemphill stated in the "Saturday Evening Post", "Country music isn't really country anymore; it is a hybrid of nearly every form of popular music in America."
During the early 1980s, country artists continued to see their records perform well on the pop charts. Willie Nelson and Juice Newton each had two songs in the top 5 of the Billboard Hot 100 in the early eighties: Nelson charted "Always on My Mind" (No. 5, 1982) and "To All the Girls I've Loved Before" (No. 5, 1984), and Newton achieved success with "Queen of Hearts" (No. 2, 1981) and "Angel of the Morning" (No. 4, 1981). Four country songs topped the "Billboard" Hot 100 in the 1980s: "Lady" by Kenny Rogers, from the late fall of 1980; "9 to 5" by Dolly Parton, "I Love a Rainy Night" by Eddie Rabbitt (these two back-to-back at the top in early 1981); and "Islands in the Stream", a duet by Dolly Parton and Kenny Rogers in 1983, a pop-country crossover hit written by Barry, Robin, and Maurice Gibb of the Bee Gees. Newton's "Queen of Hearts" almost reached No. 1, but was kept out of the spot by the pop ballad juggernaut "Endless Love" by Diana Ross and Lionel Richie. Although there were few crossover hits in the latter half of the 1980s, one song—Roy Orbison's "You Got It", from 1989—made the top 10 of both the "Billboard" Hot Country Singles" and Hot 100 charts.
The record-setting, multi-platinum group Alabama was named Artist of the Decade for the 1980s by the Academy of Country Music.
Neocountry.
In 1980, a style of "neocountry disco music" was popularized by the film "Urban Cowboy", which also included more traditional songs such as "The Devil Went Down to Georgia" by the Charlie Daniels Band. A related subgenre is Texas country music.
Sales in record stores rocketed to $250 million in 1981; by 1984, 900 radio stations began programming country or neocountry pop full-time. As with most sudden trends, however, by 1984 sales had dropped below 1979 figures.
Truck driving country.
Truck driving country music is a genre of country music
and is a fusion of honky-tonk, country rock and the Bakersfield sound.
It has the tempo of country rock and the emotion of honky-tonk, and its lyrics focus on a truck driver's lifestyle.
Truck driving country songs often deal with trucks and love. Well-known artists who sing truck driving country include Dave Dudley, Red Sovine, Dick Curless, Red Simpson, Del Reeves, The Willis Brothers and Jerry Reed, with C. W. McCall and Cledus Maggard (pseudonyms of Bill Fries and Jay Huguely, respectively) being more humorous entries in the subgenre. Dudley is known as the father of truck driving country.
Neotraditionalist movement.
During the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, "back-to-basics" production. Led by Randy Travis, whose 1986 debut album "Storms of Life" sold four million copies and was "Billboard's" year-end top country album of 1987, many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt,( Keith Whitley)Alan Jackson, Ricky Skaggs, Patty Loveless, Kathy Mattea, George Strait and The Judds.
Fifth generation (1990s).
Country music was aided by the FCC's Docket 80-90, which led to a significant expansion of FM radio in the 1980s by adding numerous higher-fidelity FM signals to rural and suburban areas. At this point, country music was mainly heard on rural AM radio stations; the expansion of FM was particularly helpful to country music, which migrated to FM from the AM band as AM became overcome by talk radio (the country music stations that stayed on AM developed the classic country format for the AM audience). At the same time, beautiful music stations already in rural areas began abandoning the format (leading to its effective demise) to adopt country music as well. This wider availability of country music led to producers seeking to polish their product for a wider audience. Another force leading to changes in the country music industry was the changing sound of rock music, which was increasingly being influenced by the noisier, less melodic alternative rock scene. "New country" ended up absorbing rock influence from more electric musicians that were too melodic for modern rock but too electric for the classic country music sound. (A number of "classic rock" artists, especially Southern rock ones such as Charlie Daniels and Lynyrd Skynyrd, are more closely associated with the modern country music scene than that of the modern rock scene.)
In the 1990s, country music became a worldwide phenomenon thanks to Garth Brooks., who enjoyed one of the most successful careers in popular music history, breaking records for both sales and concert attendance throughout the decade. The RIAA has certified his recordings at a combined (128× platinum), denoting roughly 113 million U.S. shipments. Other artists that experienced success during this time included Clint Black, Sammy Kershaw, Aaron Tippin, Travis Tritt, Alan Jackson and the newly formed duo of Brooks & Dunn; George Strait, whose career began in the 1980s, also continued to have widespread success in this decade and beyond. Toby Keith began his career as a more pop-oriented country singer in the 1990s, evolving into an outlaw persona in the late 1990s with "Pull My Chain" and its follow-up, "Unleashed".
Female artists such as Reba McEntire, Patty Loveless, Faith Hill, Martina McBride, Deana Carter, LeAnn Rimes, Mindy McCready, Lorrie Morgan, Shania Twain, and Mary Chapin Carpenter all released platinum-selling albums in the 1990s.
The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s. Their 1998 debut album "Wide Open Spaces" went on to become certified 12x platinum while their 1999 album "Fly" went on to become 10x platinum. After their third album, "Home", was released in 2003, the band made political news in part because of lead singer Natalie Maines's comments disparaging then-President George W. Bush while the band was overseas (Maines stated that she and her bandmates were ashamed to be from the same state as Bush, who had just commenced the Iraq War a few days prior). The comments caused a rift between the band and the country music scene, and the band's fourth (and, to date, final) album, 2006's "Taking the Long Way", took a more rock-oriented direction; the album was commercially successful overall but largely ignored among country audiences. (The band is currently on hiatus as Maines pursues a solo career; in the meantime, the two other members are continuing with their side project, the Court Yard Hounds.)
In the early-mid-1990s, country western music was influenced by the popularity of line dancing. This influence was so great that Chet Atkins was quoted as saying, "The music has gotten pretty bad, I think. It's all that damn line dancing." By the end of the decade, however, at least one line dance choreographer complained that good country line dance music was no longer being released.
In contrast, artists such as Don Williams and George Jones who had more or less had consistent chart success through the 1970s and 1980s suddenly had their fortunes fall rapidly around 1991 as these new artists rose to prominence.
Sixth generation (2000s–present).
Richard Marx crossed over with his "Days in Avalon" album, which features five country songs and several singers and musicians. Alison Krauss sang background vocals to Marx's single "Straight from My Heart." Also, Bon Jovi had a hit single, "Who Says You Can't Go Home", with Jennifer Nettles of Sugarland. Kid Rock's collaboration with Sheryl Crow, "Picture," was a major crossover hit in 2001 and began Kid Rock's transition from hard rock to a country-rock hybrid that would later produce another major crossover hit, 2008's "All Summer Long." Darius Rucker, former frontman for the 1990s pop-rock band Hootie & the Blowfish, began a country solo career in the late 2000s, one that to date has produced three albums and several hits on both the country charts and the Billboard Hot 100. Singer-songwriter Unknown Hinson became famous for his appearance in the Charlotte television show "Wild, Wild, South", after which Hinson started his own band and toured in southern states. Other rock stars who featured a country song on their albums were Don Henley and Poison.
In 2005, country singer Carrie Underwood rose to fame as the winner of the fourth season of "American Idol" and has since become one of the most prominent recording artists of last ten years, with worldwide sales of more than 64 million records and six Grammy Awards. With her first single, "Inside Your Heaven", Underwood became the only solo country artist to have a #1 hit on the "Billboard" Hot 100 chart in the 2000–2009 decade and also broke "Billboard" chart history as the first country music artist ever to debut at No. 1 on the Hot 100. Underwood's debut album, "Some Hearts", became the best-selling solo female debut album in country music history, the fastest-selling debut country album in the history of the SoundScan era and the best-selling country album of the last 10 years, being ranked by "Billboard" as the #1 Country Album of the 2000–2009 decade. She has also become the female country artist with the most number one hits on the "Billboard" Hot Country Songs chart in the Nielsen SoundScan era (1991–present), having 12 No. 1s and breaking her own "Guinness Book" record of ten. In 2007, Underwood won the Grammy Award for Best New Artist, becoming only the second Country artist in history (and the first in a decade) to win it. She also made history by becoming the seventh woman to win Entertainer of the Year at the Academy of Country Music Awards, and the first woman in history to win the award twice, as well as twice consecutively. "Time" has listed Underwood as one of the 100 most influential people in the world.
Underwood was one of several country stars produced by a television series in the 2000s. In addition to Underwood, "American Idol" launched the careers of Kellie Pickler, Josh Gracin, Bucky Covington, Kristy Lee Cook, Danny Gokey and Scotty McCreery (as well as that of occasional country singer Kelly Clarkson) in the decade, and would continue to launch country careers in the 2010s. The series "Nashville Star", while not nearly as successful as "Idol", did manage to bring Miranda Lambert and Chris Young to mainstream success, also launching the careers of lower-profile musicians such as Buddy Jewell, Sean Patrick McGraw, and Canadian musician George Canyon. "Can You Duet?" produced the duos Steel Magnolia and Joey + Rory.
Teen sitcoms also have had an impact on modern country music; in 2008, actress Jennette McCurdy (best known as the sidekick Sam on the teen sitcom "iCarly") released her first single, "So Close", following that with the single "Generation Love" in 2011. Another teen sitcom star, Miley Cyrus (of "Hannah Montana"), also had a crossover hit in the late 2000s with "The Climb" and another with a duet with her father, Billy Ray Cyrus, with "Ready, Set, Don't Go." Jana Kramer, an actress in the teen drama "One Tree Hill", released a country album in 2012 that has produced two hit singles as of 2013. Actress Hayden Panettiere began recording country songs as part of her role in the TV series "Nashville".
In 2010, the group Lady Antebellum won five Grammys, including the coveted Song of the Year and Record of the Year for "Need You Now".
A large number of duos and vocal groups have begun to emerge on the charts in the 2010s, many of which feature close harmony in the lead vocals. In addition to Lady Antebellum, groups such as The Quebe Sisters Band, Little Big Town, The Band Perry, Gloriana, Thompson Square, Eli Young Band and the Zac Brown Band have emerged to occupy a large portion of the new country artists in the popular scene.
One of the most commercially successful artists of the late 2000s and early 2010s has been singer-songwriter Taylor Swift. Swift first became widely known in 2006 when her debut single, "Tim McGraw," was released when Swift was age 16. In 2006, Taylor released her first studio album, "Taylor Swift", which spent 275 weeks on Billboard 200, one of the longest runs of any album on that chart. In 2008, Taylor Swift released her second studio album, "Fearless", which made her the second-longest Number One charted on Billboard 200 and the second best-selling album (just behind Adele's "21") within the past 5 years. At the 2010 Grammys, Taylor Swift was 20 and won Album of the Year for "Fearless", which made her the youngest artist to win this award. Swift has received seven Grammys already. Buoyed by her teen idol status among girls and a change in the methodology of compiling the "Billboard" charts to favor pop-crossover songs, Swift's 2012 single "We Are Never Ever Getting Back Together" spent the most weeks at the top of Billboard's Hot Country Songs chart of any song in nearly five decades. The song however is not considered country by many as Swift had transitioned from country to pop in her career.
The influence of rock music in country has become more overt during the late 2000s and early 2010s as artists like Eric Church, Jason Aldean, and Brantley Gilbert have had success; Aaron Lewis, former frontman for the rock group Staind, had a moderately successful entry into country music in 2011 and 2012. Also rising in the late 2000s and early 2010s was the insertion of rap and spoken-word elements into country songs; artists such as Cowboy Troy and Colt Ford have focused almost exclusively on country rap (also known as hick hop) while other, more mainstream artists (such as Big & Rich and Jason Aldean) have used it on occasion. In the 2010s, Bro-country, a genre noted primarily for its themes on drinking and partying, girls, and driving, became particularly popular. Notable artists associated with this genre are Luke Bryan, Jason Aldean, Blake Shelton, and Florida Georgia Line whose song "Cruise" became the best-selling country song of all time.
Alt-country.
Attempts to combine punk and country were pioneered by Jason and the Scorchers, and in the 1980s Southern Californian cowpunk scene with bands like the Long Ryders. These styles merged fully in Uncle Tupelo's 1990 LP "No Depression", which is widely credited as being the first alt-country album, and gave its name to the online notice board, and eventually magazine, that underpinned that movement. Members and figures associated with Uncle Tupelo formed three major bands in the genre: Wilco, Son Volt, and Bottle Rockets. Other influential bands included Blue Mountain, Whiskeytown and Ryan Adams, Blood Oranges, Bright Eyes, Lucinda Williams, and Drive-By Truckers. Some alt-country songs have been crossover hits, including Ryan Adams's "When The Stars Go Blue," which charted when performed by Tim McGraw.
International.
Canada.
Outside of the United States, Canada has the largest country music fan and artist base, something that is to be expected given the two countries' proximity and cultural parallels. Mainstream country music is culturally ingrained in the prairie provinces, Ontario, and in Atlantic Canada. Celtic traditional music developed in Atlantic Canada in the form of Scottish, Acadian and Irish folk music popular amongst Irish, French and Scottish immigrants to Canada's Atlantic Provinces (Newfoundland, Nova Scotia, New Brunswick, and Prince Edward Island). Like the southern United States and Appalachia, all four regions are of heavy British Isles stock and rural; as such, the development of traditional music in the Maritimes somewhat mirrored the development of country music in the US South and Appalachia. Country and Western music never really developed separately in Canada; however, after its introduction to Canada, following the spread of radio, it developed quite quickly out of the Atlantic Canadian traditional scene. While true Atlantic Canadian traditional music is very Celtic or "sea shanty" in nature, even today, the lines have often been blurred. Certain areas often are viewed as embracing one strain or the other more openly. For example, in Newfoundland the traditional music remains very unique and Irish in nature, whereas traditional musicians in other parts of the region may play both genres interchangeably.
"Don Messer's Jubilee" was a Halifax, Nova Scotia-based country/folk variety television show that was broadcast nationally from 1957 to 1969. In Canada it out-performed "The Ed Sullivan Show" broadcast from the United States and became the top-rated television show throughout much of the 1960s. "Don Messer's Jubilee" followed a consistent format throughout its years, beginning with a tune named "Goin' to the Barndance Tonight", followed by fiddle tunes by Messer, songs from some of his "Islanders" including singers Marg Osburne and Charlie Chamberlain, the featured guest performance, and a closing hymn. It ended with "Till We Meet Again".
The guest performance slot gave national exposure to numerous Canadian folk musicians, including Stompin' Tom Connors and Catherine McKinnon. Some Maritime country performers went on to further fame beyond Canada. Hank Snow, Wilf Carter (also known as Montana Slim), and Anne Murray are the three most notable.
The cancellation of the show by the public broadcaster in 1969 caused a nationwide protest, including the raising of questions in the Parliament of Canada.
The Prairie provinces, due to their western cowboy and agrarian nature, are the true heartland of Canadian country music. While the Prairies never developed a traditional music culture anything like the Maritimes, the folk music of the Prairies often reflected the cultural origins of the settlers, who were a mix of Scottish, Ukrainian, German and others. For these reasons polkas and Western music were always popular in the region, and with the introduction of the radio, mainstream country music flourished. As the culture of the region is western and frontier in nature, the specific genre of country and western is more popular today in the Prairies than in any other part of the country. No other area of the country embraces all aspects of the culture, from two-step dancing, to the cowboy dress, to rodeos, to the music itself, like the Prairies do. The Atlantic Provinces, on the other hand, produce far more traditional musicians, but they are not usually specifically country in nature, usually bordering more on the folk or Celtic genres.
Many traditional country artists are present in eastern and western Canada. They make common use of fiddle and pedal steel guitar styles. Some notable Canadian country artists include Shania Twain, Anne Murray, k.d. lang, Gordon Lightfoot, Buffy Sainte-Marie, George Canyon, Blue Rodeo, Tommy Hunter, Rita MacNeil, Stompin' Tom Connors, Stan Rogers, Ronnie Prophet, Carroll Baker, The Rankin Family, Ian Tyson, Johnny Reid, Paul Brandt, Jason McCoy, George Fox, Carolyn Dawn Johnson, Hank Snow, Don Messer, Wilf Carter, Michelle Wright, Terri Clark, Prairie Oyster, Family Brown, Johnny Mooring, Marg Osburne, Doc Walker, Emerson Drive, The Wilkinsons, Corb Lund and the Hurtin' Albertans, Crystal Shawanda, Dean Brody, Shane Yellowbird, Gord Bamford, Chad Brownlee, The Road Hammers, and The Higgins.
Australia.
Australian country music has a long tradition. Influenced by American country music, it has developed a distinct style, shaped by British and Irish folk ballads and Australian bush balladeers like Henry Lawson and Banjo Paterson. Country instruments, including the guitar, banjo, fiddle and harmonica, create the distinctive sound of country music in Australia and accompany songs with strong storyline and memorable chorus.
Folk songs sung in Australia between the 1780s and 1920s, based around such themes as the struggle against government tyranny, or the lives of bushrangers, swagmen, drovers, stockmen and shearers, continue to influence the genre. This strain of Australian country, with lyrics focusing on Australian subjects, is generally known as "bush music" or "bush band music". "Waltzing Matilda", often regarded as Australia's unofficial national anthem, is a quintessential Australian country song, influenced more by British and Irish folk ballads than by American country and western music. The lyrics were composed by the poet Banjo Paterson in 1895. Other popular songs from this tradition include "The Wild Colonial Boy", "Click Go the Shears", "The Queensland Drover" and "The Dying Stockman". Later themes which endure to the present include the experiences of war, of droughts and flooding rains, of Aboriginality and of the railways and trucking routes which link Australia's vast distances.
Pioneers of a more Americanised popular country music in Australia included Tex Morton (known as "The Father of Australian Country Music") in the 1930s. Other early stars included Buddy Williams, Shirley Thoms and Smoky Dawson. Buddy Williams (1918–1986) was the first Australian-born to record country music in Australia in the late 1930s and was the pioneer of a distinctly Australian style of country music called the bush ballad that others such as Slim Dusty would make popular in later years. During World War II, many of Buddy Williams recording sessions were done whilst on leave from the Army. At the end of the war, Williams would go on to operate some of the largest travelling tent rodeo shows Australia has ever seen.
In 1952, Dawson began a radio show and went on to national stardom as a singing cowboy of radio, TV and film. Slim Dusty (1927–2003) was known as the "King of Australian Country Music" and helped to popularise the Australian bush ballad. His successful career spanned almost six decades, and his 1957 hit "A Pub with No Beer" was the biggest-selling record by an Australian to that time, and with over seven million record sales in Australia he is the most successful artist in Australian musical history. Dusty recorded and released his one-hundredth album in the year 2000 and was given the honour of singing "Waltzing Matilda" in the closing ceremony of the Sydney 2000 Olympic Games. Dusty's wife Joy McKean penned several of his most popular songs.
Chad Morgan, who began recording in the 1950s, has represented a vaudeville style of comic Australian country; Frank Ifield achieved considerable success in the early 1960s, especially in the UK Singles Charts, and Reg Lindsay was one of the first Australians to perform at Nashville's Grand Ole Opry in 1974. Eric Bogle's 1972 folk lament to the Gallipoli Campaign "And the Band Played Waltzing Matilda" recalled the British and Irish origins of Australian folk-country. Singer-songwriter Paul Kelly, whose music style straddles folk, rock, and country, is often described as the poet laureate of Australian music. 
By the 1990s, country music had attained crossover success in the pop charts, with artists like James Blundell and James Reyne singing "Way Out West", and country star Kasey Chambers winning the ARIA for Best Female Artist in 2003. The crossover influence of Australian country is also evident in the music of successful contemporary bands The Waifs and the John Butler Trio. Nick Cave has been heavily influenced by the country artist Johnny Cash. In 2000, Cash, covered Cave's "The Mercy Seat" on the album ', seemingly repaying Cave for the compliment he paid by covering Cash's "The Singer" (originally "The Folk Singer") on his "Kicking Against the Pricks" album. Subsequently, Cave cut a duet with Cash on a version of Hank Williams' "I'm So Lonesome I Could Cry" for Cash's ' album (2002).
Popular contemporary performers of Australian country music include John Williamson (who wrote the iconic "True Blue"), Lee Kernaghan (whose hits include "Boys from the Bush" and "The Outback Club"), Gina Jeffreys, Forever Road and Sara Storer. In the United States, Olivia Newton-John, Sherrié Austin and Keith Urban have attained great success.
Country music has been a particularly popular form of musical expression among Indigenous Australians. Troy Cassar-Daley is among Australia's successful contemporary indigenous performers, and Kev Carmody and Archie Roach employ a combination of folk-rock and country music to sing about Aboriginal rights issues.
The Tamworth Country Music Festival began in 1973 and now attracts up to 100,000 visitors annually. Held in Tamworth, New South Wales (country music capital of Australia), it celebrates the culture and heritage of Australian country music. During the festival the CMAA holds the Country Music Awards of Australia ceremony awarding the Golden Guitar trophies.
Other significant country music festivals include the Whittlesea Country Music Festival (near Melbourne) and Boyup Brook Country Music Festival (Western Australia) in February; the Bamera Country Music Festival in June (South Australia), the National Country Muster held in Gympie during August, the Mildura Country Music Festival for "independent" performers during October, and the Canberra Country Music Festival held in the national capital during November. Some festivals are quite unique in their location: Grabine State Park in New South Wales promotes Australian country through the Grabine Music Muster Festival; Marilyns Country Music Festival is a unique event held in South Australia's Smoky Bay in September and is the only music festival in the world using an oyster barge as a stage.
"Country HQ" showcases new talent on the rise in the country music scene down under. CMC (the Country Music Channel), a 24 hour music channel dedicated to non-stop country music, can be viewed on pay TV and features once a year the Golden Guitar Awards, CMAs and CCMAs alongside international shows such as "The Wilkinsons", "The Road Hammers", and "Country Music Across America".
Other international country music.
Tom Roland, from the Country Music Association International, explains country music's global popularity: "In this respect, at least, Country Music listeners around the globe have something in common with those in the United States. In Germany, for instance, Rohrbach identifies three general groups that gravitate to the genre: people intrigued with the American cowboy icon, middle-aged fans who seek an alternative to harder rock music and younger listeners drawn to the pop-influenced sound that underscores many current Country hits."
One of the first Americans to perform country music abroad was George Hamilton IV. He was the first country musician to perform in the Soviet Union; he also toured in Australia and the Middle East. He was deemed the "International Ambassador of Country Music" for his contributions to the globalization of country music. Johnny Cash, Emmylou Harris, Keith Urban, and Dwight Yoakam have also made numerous international tours.
The Country Music Association undertakes various initiatives to promote country music internationally.
 In the United Kingdom, a country-derived genre known as skiffle peaked in the 1950s thanks to the efforts of Lonnie Donegan; though the genre as a whole was very short-lived, most of the bands involved with the British Invasion began their careers as skiffle musicians. American country-western musician Slim Whitman was even more successful in the UK than he was in the United States during the same decade. With a handful of exceptions (such as the surprise success of Faron Young's top-5 UK hit "It's Four in the Morning," which did far better in the UK than the U.S. upon its 1971 release), country music has not been well received in the UK; when American country artists such as Garth Brooks, Dwight Yoakam and Alan Jackson started making transatlantic tours in the 1990s, they were treated largely with scorn by the British press. Welsh singer Tom Jones, during his period doing country music in the late 1970s and 1980s, failed to chart any hits in his native UK but placed several hits on the U.S. country charts. There is a single exception to this general view of country music in the UK: in Glasgow, Scotland, with a large population with Irish and Highland ancestry, country music is popular enough to have created a demand for the city's own Grand Ole Opry club, which opened in 1974 and remains popular.
In Brazil, a musical genre known as música sertaneja, the most popular genre of music in Brazil, is very similar to American country music, sharing the music's rich history of development in the countryside.
In South America, on the last weekend of September, the yearly San Pedro Country Music Festival takes place in the town of San Pedro, Argentina. The festival features bands from different places of Argentina, as well as international artists from Brazil, Uruguay, Chile, Peru and the United States.
In India, the Anglo-Indian community is well known for enjoying and performing country music. An annual concert festival called "Blazing Guitars" held in Chennai brings together Anglo-Indian musicians from all over the country (including some who have emigrated to places like Australia).
In Ireland TG4 began a quest for Ireland's next country star called "Glór Tíre", translated as "Country Voice". It is now in its sixth season and is one of TG4's most watched TV shows. Over the past ten years country and gospel recording artist James Kilbane has reached multi-platinum success with his mix of Christian and traditional country influenced albums. James Kilbane like many other Irish artists are today working closer with Nashville. A recent success in the Irish arena has been Crystal Swing.
In Sweden, Rednex rose to stardom combining country music with electro-pop in the 1990s. In 1994, the group had a worldwide hit with their version of the traditional Southern tune "Cotton-Eyed Joe". Artists popularizing more traditional country music in Sweden have been Ann-Louise Hanson, Hasse Andersson, Kikki Danielsson, Elisabeth Andreassen and Jill Johnson.
In Poland an international country music festival, known as Piknik Country (picnic country), has been organized in Mrągowo in Masuria since 1983.
There are more and more country music artists in France. Some of the most important are Liane Edwards, , Rockie Mountains, Tahiana, and Lili West. French rock and roll superstar Eddy Mitchell is also very inspired by Americana and country music.
In Iran, country music has appeared in recent years. According to "Melody Music Magazine", the pioneer of country music in Iran is the English-speaking country music band Dream Rovers, whose founder, singer and songwriter is Erfan Rezayatbakhsh (elf). The band was formed in 2007 in Tehran, and during this time they have been trying to introduce and popularize country music in Iran by releasing two studio albums and performing live at concerts, despite the difficulties that the Islamic regime in Iran makes for bands that are active in the western music field.
Performers and shows.
US cable television.
Six U.S. cable TV networks are at least partly devoted to the genre: Country Music Television and CMT Pure Country (both owned by Viacom), Rural Free Delivery TV (owned by Rural Media Group), Great American Country (owned by the E. W. Scripps Company), Heartland (owned by Luken Communications), and ZUUS Country (owned by ZUUS Media).
The first American country music video cable channel was The Nashville Network, launched in the early 1980s. In 2000, after it and CMT fell under the same corporate ownership, the channel was renamed and reformatted as The "National" Network, a general-interest network, and eventually became Spike TV. TNN was later revived from 2012 to 2013 after Jim Owens Entertainment acquired the trademark and licensed it to Luken Communications; that channel renamed itself Heartland after Luken was embroiled in an unrelated dispute that left the company bankrupt.
Canadian television.
Only one television channel is currently dedicated to country music in Canada: CMT (Canada) owned by Corus Entertainment (90%) and Viacom (10%). In the past, country music had an extensive presence, especially on the Canadian national broadcaster, CBC Television. The show "Don Messer's Jubilee" had a great impact on country music in Canada; for instance, it was the program that launched Anne Murray's career. "Country Hoedown" and its successor, "The Tommy Hunter Show", ran for a combined 32 years on the CBC, from 1960 to 1992; in its last nine years on air, the U.S. cable network TNN carried Hunter's show.
Australian cable television.
The only network dedicated to country music in Australia is the Country Music Channel owned by XYZnetworks.
Criticism of Country Music.
Country music has been subjected to criticism on the ground that each song has the same melody and the same theme. (specifically the unofficial "Bro-country" songs). In 2013 rock singer Tom Petty called country music "bad rock with a fiddle".

</doc>
<doc id="5248" url="http://en.wikipedia.org/wiki?curid=5248" title="Cold War (1947–53)">
Cold War (1947–53)

The Cold War (1947–1953) is the period within the Cold War from the Truman Doctrine in 1947 to the conclusion of the Korean War in 1953. The Cold War began almost immediately following World War II and lasted through most of the 20th century.
Creation of the Eastern Bloc.
During World War II, the Soviet Union annexed several countries as Soviet Socialist Republics within the Union of Soviet Socialist Republics. Most of those countries had been ceded to it by the secret agreement portion of the Molotov-Ribbentrop Pact with Nazi Germany. These later annexed territories include Eastern Poland (incorporated into two different SSRs), Latvia (became Latvian SSR), Estonia (became Estonian SSR), Lithuania (became Lithuanian SSR), part of eastern Finland (became part of the Karelo-Finnish SSR) and northern Romania (became the Moldavian SSR).
Several of the other countries it occupied that were not directly annexed into the Soviet Union became Soviet satellite states. In East Germany after local election losses, a forced merger of political parties in the Socialist Unity Party ("SED"), followed by elections in 1946 where political opponents were oppressed. In the non-USSR annexed portion of Poland, less than a third of Poland's population voted in favor of massive communist land reforms and industry nationalizations in a policies referendum known as "3 times YES" ("3 razy TAK"; "3xTAK"), whereupon a second vote rigged election was held to get the desired result. Fraudulent Polish elections held in January 1947 resulted in Poland's official transformation to the People's Republic of Poland.
The List of World Leaders at the Beginning of these Years are as follows:
1947- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Chiang Kai-Shek (Allied China)
1948- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1949- Clement Attlee (UK): Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1950- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1951- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1952- Winston Churchill (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1953- Winston Churchill (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
In Hungary, when the Soviets installed a communist government, Mátyás Rákosi was appointed General Secretary of the Hungarian Communist Party, which began one of the harshest dictatorships in Europe under the People's Republic of Hungary. In Bulgaria, toward the end of World War II, the Soviet Union crossed the border and created the conditions for a communist coup d'état on the following night. The Soviet military commander in Sofia assumed supreme authority, and the communists whom he instructed, including Kimon Georgiev (who was not a communist himself, but a member of the elitarian political organization "Zveno", working together with the communists), took full control of domestic politics in the People's Republic of Bulgaria.
With Soviet backing, the Communist Party of Czechoslovakia assumed undisputed control over the government of Czechoslovakia in the Czechoslovak coup d'état of 1948, ushering in a dictatorship. In the Romanian general election elections of 1946, the Romanian Communist Party (PCR) employed widespread intimidation tactics and electoral fraud to obtain 80 percent of the vote and, thereafter, eliminated the role of the centrist parties and forced mergers, the result of which was that, by 1948, most non-Communist politicians were either executed, in exile or in prison. In the December 1945 Albanian election, the only effective ballot choices were those of the communist Democratic Front (Albania), led by Enver Hoxha. In 1946, Albania was declared the People's Republic of Albania.
Initially, Stalin directed systems in the Eastern Bloc countries that rejected Western institutional characteristics of market economies, democratic governance (dubbed "bourgeois democracy" in Soviet parlance) and the rule of law subduing discretional intervention by the state. They were economically communist and depended upon the Soviet Union for significant amounts of materials. While in the first five years following World War II, massive emigration from these states to the West occurred, restrictions implemented thereafter stopped most East-West migration, except that under limited bilateral and other agreements.
Containment.
The immediate post-1945 period may have been the historical high point for the popularity of communist ideology. The burdens the Red Army and the Soviet Union endured had earned it massive respect which, had it been fully exploited by Joseph Stalin, had a good chance of resulting in a communist Europe. Communist parties achieved a significant popularity in such nations as China, Greece, Iran, and the Republic of Mahabad. Communist parties had already come to power in Romania, Bulgaria, Albania, and Yugoslavia. The United Kingdom and the United States were concerned that electoral victories by communist parties in any of these countries could lead to sweeping economic and political change in Western Europe.
Morgenthau and Marshall Plans.
Having lost 27 million people in the war, the Soviet Union was determined to destroy Germany's capacity for another war, and pushed for such in wartime conferences. The resulting Morgenthau plan policy foresaw returning Germany to a pastoral state without heavy industry. Because of the increasing costs of food imports to avoid mass-starvation in Germany, and with the danger of losing the entire nation to communism, the U.S. government abandoned the Morgenthau plan in September 1946 with Secretary of State James F. Byrnes' speech Restatement of Policy on Germany.
In January 1947, Truman appointed General George Marshall as Secretary of State, and enacted JCS 1779, which decreed that an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany." The directive comported with the view of General Lucius D. Clay and the Joint Chief of Staff over growing communist influence in Germany, as well as of the failure of the rest of the European economy to recover without the German industrial base on which it previously had been dependent. Administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets. After six weeks of negotiations, Molotov refused the demands and the talks were adjourned. Marshall was particularly discouraged after personally meeting with Stalin, who expressed little interest in a solution to German economic problems. The United States concluded that a solution could not wait any longer. In a June 5, 1947 speech, comporting with the Truman Doctrine, Marshall announced a comprehensive program of American assistance to all European countries wanting to participate, including the Soviet Union and those of Eastern Europe, called the Marshall Plan.
Fearing American political, cultural and economic penetration, Stalin eventually forbade Soviet Eastern bloc countries of the newly formed Cominform from accepting Marshall Plan aid. In Czechoslovakia, that required a Soviet-backed Czechoslovak coup d'état of 1948, the brutality of which shocked Western powers more than any event so far and set in a motion a brief scare that war would occur and swept away the last vestiges of opposition to the Marshall Plan in the United States Congress.
The Greek Civil War and the Truman Doctrine.
Both East and West regarded Greece as a nation well within the sphere of influence of Britain. Stalin had respected his agreement with Winston Churchill to not intervene, but Yugoslavia and Albania defied the USSR's advice and sent supplies during the Greek Civil War to the partisan forces of the Communist Party of Greece, the ELAS (National Popular Liberation Army). The UK had given aid to the royalist Greek forces and ELAS leaders who, failing to realize that there would be no Soviet aid and having boycotted the elections, were at a disadvantaged position. However, by 1947, the near-bankrupt British government could no longer maintain its massive overseas commitments. In addition to granting independence to India and handing back the Palestinian Mandate to the United Nations, the British government decided to withdraw from both Greece and nearby Turkey. This would have left the two nations, in particular Greece, on the brink of a communist-led revolution.
Notified that British aid to Greece and Turkey would end in less than six weeks, and already hostile towards and suspicious of Soviet intentions, because of their reluctance to withdraw from Iran, the Truman administration decided that additional action was necessary. With Congress solidly in Republican hands, and with isolationist sentiment strong among the U.S. public, Truman adopted an ideological approach. In a meeting with congressional leaders, the argument of "apples in a barrel infected by one rotten one" was used to convince them of the significance in supporting Greece and Turkey. It was to become the "domino theory". On the morning of March 12, 1947, President Harry S. Truman appeared before Congress to ask for $400 million of aid to Greece and Turkey. Calling on congressional approval for the United States to "support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures," or in short a policy of "containment", Truman articulated a presentation of the ideological struggle that became known as the "Truman Doctrine." Although based on a simplistic analysis of internal strife in Greece and Turkey, it became the single dominating influence over U.S. policy until at least the Vietnam War.
Truman's speech had a tremendous effect. The anti-communist feelings that had just begun to hatch in the U.S. were given a great boost, and a silenced Congress voted overwhelmingly in approval of aid. The United States would not withdraw back to the Western Hemisphere as it had after World War I. From then on, the U.S. actively engaged any communist threats anywhere in the globe under the ostensible causes of "freedom", "democracy" and "human rights." The U.S. brandished its role as the leader of the "free world." Meanwhile, the Soviet Union brandished its position as the leader of the "progressive" and "anti-imperialist" camp.
U.S. Military Doctrine Debate.
After World War II, the generals of the newly formed U.S. Air Force propounded a new doctrine: that strategic bombing, particularly with nuclear weapons, was the sole decisive element necessary to win any future war; and was therefore the sole means necessary to deter an adversary from launching a Pearl Harbor like surprise attack or war against the United States. To implement this doctrine, which the Air Force and its supporters regarded as the highest national priority, the Air Force proposed that it should be funded by the Congress to build a large fleet of U.S. based long-range strategic heavy bombers. The Air Force generals argued that this project should receive large amounts of funding, beginning with the B-36 Peacemaker bomber.
The admirals of the Navy disagreed. Pointing to the overwhelming dominance of the aircraft carrier in the Pacific Theater, they asked the United States Congress to fund a large fleet of "supercarriers" and their supporting battle groups, beginning with the USS "United States" (CVA-58). The Navy leadership believed that wars could not be won by strategic bombing alone, with or without the use of nuclear weapons. The Navy also maintained that to decide, at the outset of any future conflict, to initiate the widespread use of nuclear weapons—attacking the major population centers of the enemy homeland—was immoral.
"Nazi-Soviet relations" and "Falsifiers of History".
Relations further deteriorated when, in January 1948, the U.S. State Department also published a collection of documents titled "Nazi-Soviet Relations, 1939–1941: Documents from the Archives of The German Foreign Office", which contained documents recovered from the Foreign Office of Nazi Germany revealing Soviet conversations with Germany regarding the Molotov-Ribbentrop Pact, including its secret protocol dividing eastern Europe, the 1939 German-Soviet Commercial Agreement, and discussions of the Soviet Union potentially becoming the fourth Axis Power.
In response, one month later, the Soviet Union published "Falsifiers of History", a Stalin edited and partially re-written book attacking the West. The book did not attempt to directly counter or deal with the documents published in "Nazi-Soviet Relations" and rather, focused upon Western culpability for the outbreak of war in 1939. It argues that "Western powers" aided Nazi rearmament and aggression, including that American bankers and industrialists provided capital for the growth of German war industries, while deliberately encouraging Hitler to expand eastward. The book also included the claim that, during the Pact's operation, Stalin rejected Hitler's offer to share in a division of the world, without mentioning the Soviet offers to join the Axis. Historical studies, official accounts, memoirs and textbooks published in the Soviet Union used that depiction of events until the Soviet Union's dissolution.
Berlin Blockade.
After the Marshall Plan, the introduction of a new currency to Western Germany to replace the debased Reichsmark and massive electoral losses for communist parties in 1946, in June 1948, the Soviet Union cut off surface road access to Berlin. On the day of the Berlin Blockade, a Soviet representative told the other occupying powers "We are warning both you and the population of Berlin that we shall apply economic and administrative sanctions that will lead to circulation in Berlin exclusively of the currency of the Soviet occupation zone."
Thereafter, street and water communications were severed, rail and barge traffic was stopped and the Soviets initially stopped supplying food to the civilian population in the non-Soviet sectors of Berlin. Because Berlin was located within the Soviet-occupied zone of Germany and the other occupying powers had previously relied on Soviet good will for access to Berlin, the only available methods of supplying the city were three limited air corridors.
By February 1948, because of massive post-war military cuts, the entire United States army had been reduced to 552,000 men. Military forces in non-Soviet Berlin sectors totaled only 8,973 Americans, 7,606 British and 6,100 French. Soviet military forces in the Soviet sector that surrounded Berlin totaled one and a half million men. The two United States regiments in Berlin would have provided little resistance against a Soviet attack. Believing that Britain, France and the United States had little option other than to acquiesce, the Soviet Military Administration in Germany celebrated the beginning of the blockade. Thereafter, a massive aerial supply campaign of food, water and other goods was initiated by the United States, Britain, France and other countries. The Soviets derided "the futile attempts of the Americans to save face and to maintain their untenable position in Berlin." The success of the airlift eventually caused the Soviets to lift their blockade in May 1949.
Tito-Stalin Split.
After disagreements between Yugoslavian leader Josip Broz Tito and the Soviet Union regarding Greece and the People's Republic of Albania, a Tito-Stalin split occurred, followed by Yugoslavia being expelled from the Cominform in June 1948 and a brief failed Soviet putsch in Belgrade. The split created two separate communist forces in Europe. A vehement campaign against "Titoism" was immediately started in the Eastern Bloc, describing agents of both the West and Tito in all places engaging in subversive activity. This resulted in the persecution of many major party cadres, including those in East Germany.
NATO.
The United States joined Britain, France, Canada, Denmark, Portugal, Norway, Belgium, Iceland, Luxembourg, Italy, and the Netherlands in 1949 to form the North Atlantic Treaty Organization (NATO), the United States' first "entangling" European alliance in 170 years. West Germany, Spain, Greece, and Turkey would later join this alliance. The Eastern leaders retaliated against these steps by integrating the economies of their nations in Comecon, their version of the Marshall Plan; exploding the first Soviet atomic device in 1949; signing an alliance with People's Republic of China in February 1950; and forming the Warsaw Pact, Eastern Europe's counterpart to NATO, in 1955. The Soviet Union, Albania, Czechoslovakia, Hungary, East Germany, Bulgaria, Romania, and Poland founded this military alliance.
NSC-68.
U.S. officials quickly moved to escalate and expand "containment." In a secret 1950 document, NSC-68, they proposed to strengthen their alliance systems, quadruple defense spending, and embark on an elaborate propaganda campaign to convince the U.S. public to fight this costly cold war. Truman ordered the development of a hydrogen bomb. In early 1950, the U.S. took its first efforts to oppose communist forces in Vietnam; planned to form a West German army, and prepared proposals for a peace treaty with Japan that would guarantee long-term U.S. military bases there.
Chinese Civil War.
Shortly after World War II, the civil war resumed in China between the Kuomintang (KMT) led by Generalissimo Chiang Kai-shek and the Communist Party of China led by Mao Zedong. The USSR had signed a Treaty of Friendship with the Kuomintang in 1945 and disavowed support for the Chinese Communists. The outcome was closely fought, with the Communists finally prevailing with superior military tactics. Although the Nationalists had an advantage in numbers of men and weapons, initially controlled a much larger territory and population than their adversaries, and enjoyed considerable international support, they were exhausted by the long war with Japan and the attendant internal responsibilities. In addition, the Chinese Communists were able to fill the political vacuum left in Manchuria after Soviet forces withdrew from the area and thus gained China's prime industrial base. The Chinese Communists were able to fight their way from the north and northeast, and virtually all of mainland China was taken by the end of 1949. On October 1, 1949, Mao Zedong proclaimed the People's Republic of China (PRC). Chiang Kai-shek and 600,000 Nationalist troops and 2 million refugees, predominantly from the government and business community, fled from the mainland to the island of Taiwan. In December 1949, Chiang proclaimed Taipei the temporary capital of the Republic of China (ROC) and continued to assert his government as the sole legitimate authority in China.
The continued hostility between the Communists on the mainland and the Nationalists on Taiwan continued throughout the Cold War. Though the United States refused to aide Chiang Kai-shek in his hope to "recover the mainland," it continued supporting the Republic of China with military supplies and expertise to prevent Taiwan from falling into PRC hands. Through the support of the Western bloc (most Western countries continued to recognize the ROC as the sole legitimate government of China), the Republic of China on Taiwan retained China's seat in the United Nations until 1971.
Korean War.
In early 1950, the United States made its first commitment to form a peace treaty with Japan that would guarantee long-term U.S. military bases. Some observers (including George Kennan) believed that the Japanese treaty led Stalin to approve a plan to invade U.S.-supported South Korea on June 25, 1950. Korea had been divided at the end of World War II along the 38th parallel into Soviet and U.S. occupation zones, in which a communist government was installed in the North by the Soviets, and an elected government in the South came to power after UN-supervised elections in 1948.
In June 1950, Kim Il-sung's North Korean People's Army invaded South Korea. Fearing that communist Korea under a Kim Il Sung dictatorship could threaten Japan and foster other communist movements in Asia, Truman committed U.S. forces and obtained help from the United Nations to counter the North Korean invasion. The Soviets boycotted UN Security Council meetings while protesting the Council's failure to seat the People's Republic of China and, thus, did not veto the Council's approval of UN action to oppose the North Korean invasion. A joint UN force of personnel from South Korea, the United States, Britain, Turkey, Canada, Australia, France, the Philippines, the Netherlands, Belgium, New Zealand and other countries joined to stop the invasion. After a Chinese invasion to assist the North Koreans, fighting stabilized along the 38th parallel, which had separated the Koreas. Truman faced a hostile China, a Sino-Soviet partnership, and a defense budget that had quadrupled in eighteen months.
The Korean Armistice Agreement was signed in July 1953 after the death of Stalin, who had been insisting that the North Koreans continue fighting. In North Korea, Kim Il-sung created a highly centralized and brutal dictatorship, according himself unlimited power and generating a formidable cult of personality.
Hydrogen Bomb.
A hydrogen bomb—which produced nuclear fusion instead of nuclear fission—was first tested by the United States in November 1952 and the Soviet Union in August 1953. Such bombs were first deployed in the 1960s.
Culture and media.
Fear of a nuclear war spurred the production of public safety films by the United States federal government's Civil Defense branch that demonstrated ways on protecting oneself from a Soviet nuclear attack. The 1951 children's film "Duck and Cover" is a prime example.
George Orwell's classic dystopia Nineteen Eighty-Four was published in 1949. The novel explores life in an imagined future world where a totalitarian government has achieved terrifying levels of power and control. With Nineteen Eighty-Four, Orwell taps into the anti-communist fears that would continue to haunt so many in the West for decades to come. In a Cold War setting his descriptions could hardly fail to evoke comparison to Soviet communism and the seeming willingness of Stalin and his successors to control those within the Soviet bloc by whatever means necessary. Orwell's famous allegory of totalitarian rule, Animal Farm, published in 1945, provoked similar anti-communist sentiments.

</doc>
<doc id="5249" url="http://en.wikipedia.org/wiki?curid=5249" title="Crony capitalism">
Crony capitalism

Crony capitalism is a term describing an economy in which success in business depends on close relationships between business people and government officials. It may be exhibited by favoritism in the distribution of legal permits, government grants, special tax breaks, or other forms of state interventionism.
Crony capitalism is believed to arise when business cronyism and related self-serving behavior by businesses or businesspeople spills over into politics and government, or when self-serving friendships and family ties between businessmen and the government influence the economy and society to the extent that it corrupts public-serving economic and political ideals.
The term "crony capitalism" made a significant impact in the public arena as an explanation of the Asian financial crisis. It is also used to describe governmental decisions favoring "cronies" of governmental officials. In this context, the term is often used interchangeably with corporate welfare; to the extent that there is a difference, it may be the extent to which a government action can be said to benefit individuals rather than entire industries.
Crony capitalism in practice.
Crony capitalism exists along a continuum. In its lightest form, crony capitalism consists of collusion among market players which is officially tolerated or encouraged by the government. While perhaps lightly competing against each other, they will present a unified front (sometimes called a trade association or industry trade group) to the government in requesting subsidies or aid or regulation. Newcomers to a market may find it difficult to find loans, acquire shelf space, or receive official sanction. Some such systems are very formalized, such as sports leagues and the Medallion System of the Taxicabs of New York City, but often the process is more subtle, such as expanding training and certification exams to make it more expensive for new entrants to enter a market and thereby limit competition. In technological fields, there may evolve a system whereby new entrants may be accused of infringing on patents that the established competitors never assert against each other. Distribution networks will refuse to aid the entrant. In spite of this, some competitors will succeed when the legal barriers are light, especially where the old guard has become inefficient and is failing to meet the needs of the market. Of course, some of these upstarts may then join with the established networks to help deter any other new competitors. Examples of this have been argued to include the "keiretsu" of post-war Japan, the print media in India, the "chaebol" of South Korea, and the powerful families who control much of the investment in Latin America.
However, while these sorts of collusion are all examples of cronyism, the term crony capitalism is generally used when these practices come to dominate the economy as a whole or to dominate the most valuable industries in an economy. Intentionally ambiguous laws and regulations are common in such systems. Taken strictly, such laws would greatly impede practically all business; in practice, they are only erratically enforced. The specter of having such laws suddenly brought down upon a business provides incentive to stay in the good graces of political officials. Troublesome rivals who have overstepped their bounds can have the laws suddenly enforced against them, leading to fines or even jail time. Even in high-income democracies with well-established legal systems and freedom of the press a larger state is associated with more political corruption.
The states first known for crony capitalism were those involved in the 1997 Asian Financial Crisis such as Thailand and Indonesia. In these cases, the term initially was used to point out how family members of the ruling leaders become extremely wealthy with no non-political justification. Southeast Asian nations still score very poorly in rankings measuring this. Hong Kong, and Malaysia are perhaps most noted for this, and the term quickly came to by applied to the system of Oligarchs in Russia.
Other states with notable examples of crony capitalism include India, in particular, the system after the 1990s liberalization whereby land and other resources were given at throwaway prices in the name of public private partnerships, Argentina;, and Greece. Wu Jinglian, one of China's leading economists and a longtime champion of its transition to free markets, says that it faces two starkly contrasting futures: a market economy under the rule of law or crony capitalism.
Many rather prosperous nations have also had varying amounts of cronyism throughout their history including the United Kingdom - especially in the 1600s and 1700s, United States, and Japan.
Crony Capitalism Index.
"The Economist" benchmarks countries based on a "Crony Capitalism Index" calculated via how much economic activity occurs in industries prone to cronyism. Its 2014 Crony Capitalism Index ranking listed Hong Kong, Russia and Malaysia in the top 3 spots.
Crony Capitalism in sections of an economy.
More direct government involvement in a specific sector can also lead to specific areas of crony capitalism, even if the economy as a whole may be competitive. This is most common in natural resource sectors through the granting of mining or drilling concessions, but it is also possible through a process known as regulatory capture where the government agencies in charge of regulating an industry come to be controlled by that industry. Governments will often, in good faith, establish government agencies to regulate an industry. However, the members of an industry have a very strong interest in the actions of that regulatory body, while the rest of the citizenry are only lightly affected. As a result, it is not uncommon for current industry players to gain control of the "watchdog" and to use it against competitors. This typically takes the form of making it very expensive for a new entrant to enter the market.
A 1824 landmark U.S. Supreme Court ruling overturned a New York State-granted monopoly ("a veritable model of state munificence" facilitated by one of the Founding Fathers, Robert R. Livingston) for the then-revolutionary technology of steamboats. Leveraging the Supreme Court's establishment of Congressional supremacy over commerce, the Interstate Commerce Commission was established in 1887 with the intent of regulating railroad "robber barons". President Grover Cleveland appointed Thomas M. Cooley, a railroad ally, as its first chairman and a permit system was used to deny access to new entrants and legalize price fixing.
The defense industry in the United States is often described as an example of crony capitalism in an industry. Connections with the Pentagon and lobbyists in Washington are described by critics as more important than actual competition, due to the political and secretive nature of defense contracts. In the Airbus-Boeing WTO dispute, Airbus (which receives outright subsidies from European governments) has stated Boeing receives similar subsidies, which are hidden as inefficient defense contracts. Other American defense companies were put under scrutany for no-bid contracts for Iraq war and Hurricane Katrina related contracts purportedly due to having cronies in the Bush administration.
Gerald P. O'Driscoll, former vice president at the Federal Reserve Bank of Dallas, stated that Fannie Mae and Freddie Mac became examples of crony capitalism. Government backing let Fannie and Freddie dominate mortgage underwriting. "The politicians created the mortgage giants, which then returned some of the profits to the pols - sometimes directly, as campaign funds; sometimes as "contributions" to favored constituents."
Creation of crony capitalism in developing economies.
In its worst form, crony capitalism can devolve into simple corruption, where any pretense of a free market is dispensed with. Bribes to government officials are considered "de rigueur" and tax evasion is common; this is seen in many parts of Africa, for instance. This is sometimes called plutocracy (rule by wealth) or kleptocracy (rule by theft).
Corrupt governments may favor one set of business owners who have close ties to the government over others. This may also be done with racial, religious, or ethnic favoritism; for instance, Alawites in Syria have a disproportionate share of power in the government and business there. (President Assad is an Alawite.) This can be explained by considering personal relationships as a social network. As government and business leaders try to accomplish various things, they naturally turn to other powerful people for support in their endeavors. These people form hubs in the network. In a developing country those hubs may be very few, thus concentrating economic and political power in a small interlocking group.
Normally, this will be untenable to maintain in business; new entrants will affect the market. However, if business and government are entwined, then the government can maintain the small-hub network.
Raymond Vernon, specialist in economics and international affairs,
wrote that the Industrial Revolution began in Great Britain, because they were the first to successfully limit the power of veto groups (typically cronies of those with power in government) to block innovations. "Unlike most other national environments, the British environment of the early 19th century contained relatively few threats to those who improved and applied existing inventions, whether from business competitors, labor, or the government itself. In other European countries, by contrast, the merchant guilds ... were a pervasive source of veto for many centuries. This power was typically bestowed upon them by government". For example, a Russian inventor produced a steam engine in 1766 and disappeared without a trace. "[A] steam powered horseless carriage produced in France in 1769 was officially suppressed." James Watt began experimenting with steam in 1763, got a patent in 1769, and began commercial production in 1775.
Political viewpoints.
While the problem is generally accepted across the political spectrum, ideology shades the view of the problem's causes and therefore its solutions. Political views mostly fall into two camps which might be called the socialist and capitalist critique. The socialist position is braodly that economic, or wealthy, interests are given too much power to influence the government, and the capitalist position is broadly that the government is given too much power to influence the economy.
Socialist critique.
Critics of capitalism including socialists and other anti-capitalists often assert that crony capitalism is the inevitable result of "any" capitalist system. Jane Jacobs described it as a natural consequence of collusion between those managing power and trade, while Noam Chomsky has argued that the word "crony" is superfluous when describing capitalism. Since businesses make money and money leads to political power, business will inevitably use their power to influence governments. Much of the impetus behind campaign finance reform in the United States and in other countries is an attempt to prevent economic power being used to take political power.
Ravi Batra argues that "all official economic measures adopted since 1981...have devastated the middle class" and that the Occupy Wall Street movement should push for their repeal and thus end the influence of the super wealthy in the political process, which he considers a manifestation of crony capitalism.
Socialist economists, such as Robin Hahnel, have criticized the term as an ideologically motivated attempt to cast what is in their view the fundamental problems of capitalism as avoidable irregularities. Socialist economists dismiss the term as an apologetic for failures of neoliberal policy and, more fundamentally, their perception of the weaknesses of market allocation.
Capitalist critique.
Capitalists generally oppose crony capitalism as well, but consider it an aberration brought on by governmental favors incompatible with 'true' free market. In this view, crony capitalism is the result of an excess of socialist-style interference in the market, which inherently will result in a toxic combination of corporations and government officials running the sector of the economy. Some advocates prefer to equate this problem with terms such as "corporatism, a modern form of mercantilism" to emphasize that the only way to run a profitable business in such systems is to have help from corrupt government officials in such a system. Even if the initial regulation was well-intentioned (to curb actual abuses), and even if the initial lobbying by corporations was well-intentioned (to reduce illogical regulations), the mixture of business and government stifle competition, a collusive result called regulatory capture. In his book "The Myth of the Robber Barons", Burton W. Folsom, Jr. distinguished those that engage in crony capitalism—designated by him "political entrepreneurs"—from those who compete in the marketplace without special aid from government, whom he calls "market entrepreneurs" who succeed "by producing a quality product at a competitive price"

</doc>
<doc id="5252" url="http://en.wikipedia.org/wiki?curid=5252" title="Lists of universities and colleges">
Lists of universities and colleges

This is a list of Lists of universities and colleges.

</doc>
<doc id="5253" url="http://en.wikipedia.org/wiki?curid=5253" title="Constitution">
Constitution

A constitution is a set of fundamental principles or established precedents according to which a state or other organization is governed. These rules together make up, i.e. "constitute", what the entity is. When these principles are written down into a single document or set of legal documents, those documents may be said to embody a "written" constitution; if they are written down in a single comprehensive document, it is said to embody a "codified" constitution.
Constitutions concern different levels of organizations, from sovereign states to companies and unincorporated associations. A treaty which establishes an international organization is also its constitution, in that it would define how that organization is constituted. Within states, a constitution defines the principles upon which the state is based, the procedure in which laws are made and by whom. Some constitutions, especially codified constitutions, also act as limiters of state power, by establishing lines which a state's rulers cannot cross, such as fundamental rights. An example is the constitution of the United States of America.
The Constitution of India is the longest written constitution of any sovereign country in the world, containing 444 articles in 22 parts, 12 schedules and 118 amendments, with 117,369 words in its English-language translation, while the United States Constitution is the shortest written constitution, at 7 articles and 27 amendments.
Etymology.
The term "constitution" comes through French from the Latin word "constitutio", used for regulations and orders, such as the imperial enactments ("constitutiones principis": edicta, mandata, decreta, rescripta). Later, the term was widely used in canon law for an important determination, especially a decree issued by the Pope, now referred to as an "apostolic constitution".
General features.
Generally, every modern written constitution confers specific powers to an organization or institutional entity, established upon the primary condition that it abide by the said constitution's limitations. According to Scott Gordon, author of "Controlling the State: Constitutionalism from Ancient Athens to Today" a political organization is constitutional to the extent that it "contain[s] institutionalized mechanisms of power control for the protection of the interests and liberties of the citizenry, including those that may be in the minority."
The Latin term "ultra vires" describes activities of officials within an organization or polity that fall outside the constitutional or statutory authority of those officials. For example, a students' union may be prohibited as an organization from engaging in activities not concerning students; if the union becomes involved in non-student activities these activities are considered "ultra vires" of the union's charter, and nobody would be compelled by the charter to follow them. An example from the constitutional law of sovereign states would be a provincial government in a federal state trying to legislate in an area exclusively enumerated to the federal government in the constitution, such as ratifying a treaty. "Ultra vires" gives a legal justification for the forced cessation of such action, which might be enforced by the people with the support of a decision of the judiciary, in a case of judicial review. A violation of rights by an official would be "ultra vires" because a (constitutional) right is a restriction on the powers of government, and therefore that official would be exercising powers he doesn't have.
In most but not all modern states the constitution has supremacy over ordinary Statutory law (see Uncodified constitution below); in such states when an official act is unconstitutional, i.e. it is not a power granted to the government by the constitution, that act is "null and void", and the nullification is "ab initio", that is, from inception, not from the date of the finding. It was never "law", even though, if it had been a statute or statutory provision, it might have been adopted according to the procedures for adopting legislation. Sometimes the problem is not that a statute is unconstitutional, but the application of it is, on a particular occasion, and a court may decide that while there are ways it could be applied that are constitutional, that instance was not allowed or legitimate. In such a case, only the application may be ruled unconstitutional. Historically, the remedy for such violations have been petitions for common law writs, such as "quo warranto".
History and development.
Pre-modern constitutions.
Ancient Mesopotamia.
Excavations in modern-day Iraq by Ernest de Sarzec in 1877 found evidence of the earliest known code of justice, issued by the Sumerian king Urukagina of Lagash "ca" 2300 BC. Perhaps the earliest prototype for a law of government, this document itself has not yet been discovered; however it is known that it allowed some rights to his citizens. For example, it is known that it relieved tax for widows and orphans, and protected the poor from the usury of the rich.
After that, many governments ruled by special codes of written laws. The oldest such document still known to exist seems to be the Code of Ur-Nammu of Ur ("ca" 2050 BC). Some of the better-known ancient law codes include the code of Lipit-Ishtar of Isin, the code of Hammurabi of Babylonia, the Hittite code, the Assyrian code and Mosaic law.
Antiquity.
In 621 BC a scribe named Draco codified the cruel oral laws of the city-state of Athens; this code prescribed the death penalty for many offences (nowadays very severe rules are often called "Draconian"). In 594 BC Solon, the ruler of Athens, created the new "Solonian Constitution". It eased the burden of the workers, and determined that membership of the ruling class was to be based on wealth (plutocracy), rather than by birth (aristocracy). Cleisthenes again reformed the Athenian constitution and set it on a democratic footing in 508 BC.
Aristotle ("ca" 350 BC) was one of the first in recorded history to make a formal distinction between ordinary law and constitutional law, establishing ideas of constitution and constitutionalism, and attempting to classify different forms of constitutional government. The most basic definition he used to describe a constitution in general terms was "the arrangement of the offices in a state". In his works "Constitution of Athens", "Politics", and "Nicomachean Ethics" he explores different constitutions of his day, including those of Athens, Sparta, and Carthage. He classified both what he regarded as good and what he regarded as bad constitutions, and came to the conclusion that the best constitution was a mixed system, including monarchic, aristocratic, and democratic elements. He also distinguished between citizens, who had the right to participate in the state, and non-citizens and slaves, who did not.
The Romans first codified their constitution in 450 BC as the "Twelve Tables". They operated under a series of laws that were added from time to time, but Roman law was never reorganised into a single code until the "Codex Theodosianus" (AD 438); later, in the Eastern Empire the "Codex repetitæ prælectionis" (534) was highly influential throughout Europe. This was followed in the east by the "Ecloga" of Leo III the Isaurian (740) and the "Basilica" of Basil I (878).
The "Edicts of Ashoka" established constitutional principles for the 3rd century BC Maurya king's rule in Ancient India. For constitutional principles almost lost to antiquity, see the code of Manu.
Many of the Germanic people that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic "Code of Euric" (471). This was followed by the "Lex Burgundionum", applying separate codes for Germans and for Romans; the "Pactus Alamannorum"; and the Salic Law of the Franks, all written soon after 500. In 506, the "Breviarum" or "Lex Romana" of Alaric II, king of the Visigoths, adopted and consolidated the "Codex Theodosianus" together with assorted earlier Roman laws. Systems that appeared somewhat later include the "Edictum Rothari" of the Lombards (643), the "Lex Visigothorum" (654), the "Lex Alamannorum" (730) and the "Lex Frisionum" ("ca" 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of Æthelberht of Kent (602). In ca. 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the "Doom book" code of laws for England.
Japan's "Seventeen-article constitution" written in 604, reportedly by Prince Shōtoku, is an early example of a constitution in Asian political history. Influenced by Buddhist teachings, the document focuses more on social morality than institutions of government "per se" and remains a notable early attempt at a government constitution.
Middle Ages.
The Constitution of Medina (, Ṣaḥīfat al-Madīna), also known as the Charter of Medina, was drafted by the Islamic prophet Muhammad. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans. The document was drawn up with the explicit concern of bringing to an end the bitter inter tribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish, and pagan communities of Medina bringing them within the fold of one community—the Ummah.
The precise dating of the Constitution of Medina remains debated but generally scholars agree it was written shortly after the Hijra (622). It effectively established the first Islamic state. The Constitution established: the security of the community, religious freedoms, the role of Medina as a haram or sacred place (barring all violence and weapons), the security of women, stable tribal relations within Medina, a tax system for supporting the community in time of conflict, parameters for exogenous political alliances, a system for granting protection of individuals, a judicial system for resolving disputes, and also regulated the paying of Blood money (the payment between families or tribes for the slaying of an individual in lieu of lex talionis).
In Wales, the "Cyfraith Hywel" was codified by Hywel Dda c. 942–950.
The "Pravda Yaroslava", originally combined by Yaroslav the Wise the Grand Prince of Kiev, was granted to Great Novgorod around 1017, and in 1054 was incorporated into the "Russkaya Pravda", that became the law for all of Kievan Rus. It survived only in later editions of the 15th century.
In England, Henry I's proclamation of the Charter of Liberties in 1100 bound the king for the first time in his treatment of the clergy and the nobility. This idea was extended and refined by the English barony when they forced King John to sign "Magna Carta" in 1215. The most important single article of the "Magna Carta", related to "habeas corpus", provided that the king was not permitted to imprison, outlaw, exile or kill anyone at a whim—there must be due process of law first. This article, Article 39, of the "Magna Carta" read:
"No free man shall be arrested, or imprisoned, or deprived of his property, or outlawed, or exiled, or in any way destroyed, nor shall we go against him or send against him, unless by legal judgement of his peers, or by the law of the land."
This provision became the cornerstone of English liberty after that point. The social contract in the original case was between the king and the nobility, but was gradually extended to all of the people. It led to the system of Constitutional Monarchy, with further reforms shifting the balance of power from the monarchy and nobility to the House of Commons.
The Nomocanon of Saint Sava () was the first Serbian constitution from 1219. This legal act was well developed. St. Sava's Nomocanon was the compilation of Civil law, based on Roman Law and Canon law, based on Ecumenical Councils and its basic purpose was to organize functioning of the young Serbian kingdom and the Serbian church. Saint Sava began the work on the Serbian Nomocanon in 1208 while being at Mount Athos, using "The Nomocanon in Fourteen Titles", "Synopsis of Stefan the Efesian", "Nomocanon of John Scholasticus", Ecumenical Councils' documents, which he modified with the canonical commentaries of Aristinos and John Zonaras, local church meetings, rules of the Holy Fathers, the law of Moses, translation of Prohiron and the Byzantine emperors' Novellae (most were taken from Justinian's Novellae). The Nomocanon was completely new compilation of civil and canonical regulations, taken from the Byzantine sources, but completed and reformed by St. Sava to function properly in Serbia. Beside decrees that organized the life of church, there are various norms regarding civil life, most of them were taken from Prohiron. Legal transplants of Roman-Byzantine law became the basis of the Serbian medieval law. The essence of Zakonopravilo was based on Corpus Iuris Civilis.
Stefan Dušan, Emperor of Serbs and Greeks, enacted Dušan's Code () in Serbia, in two state congresses: in 1349 in Skopje and in 1354 in Serres. It regulated all social spheres, so it was the second Serbian constitution, after St. Sava's Nomocanon (Zakonopravilo). The Code was based on Roman-Byzantine law. The legal transplanting is notable with the articles 171 and 172 of Dušan's Code, which regulated the juridical independence. They were taken from the Byzantine code Basilika (book VII, 1, 16–17).
In 1222, Hungarian King Andrew II issued the Golden Bull of 1222.
Between 1220 and 1230, a Saxon administrator, Eike von Repgow, composed the "Sachsenspiegel", which became the supreme law used in parts of Germany as late as 1900.
In 1998, S. Kouyaté reconstructed from oral tradition what he claims is a 14th-century charter of the Mali Empire, called the "Kouroukan Fouga".
Around 1240, the Coptic Egyptian Christian writer, 'Abul Fada'il Ibn al-'Assal, wrote the "Fetha Negest" in Arabic. 'Ibn al-Assal took his laws partly from apostolic writings and Mosaic law, and partly from the former Byzantine codes. There are a few historical records claiming that this law code was translated into Ge'ez and entered Ethiopia around 1450 in the reign of Zara Yaqob. Even so, its first recorded use in the function of a constitution (supreme law of the land) is with Sarsa Dengel beginning in 1563. The "Fetha Negest" remained the supreme law in Ethiopia until 1931, when a modern-style Constitution was first granted by Emperor Haile Selassie I.
The Golden Bull of 1356 was a decree issued by a "Reichstag" in Nuremberg headed by Emperor Charles IV that fixed, for a period of more than four hundred years, an important aspect of the constitutional structure of the Holy Roman Empire.
In China, the Hongwu Emperor created and refined a document he called "Ancestral Injunctions" (first published in 1375, revised twice more before his death in 1398). These rules served in a very real sense as a constitution for the Ming Dynasty for the next 250 years.
In Catalonia, the Catalan constitutions were promulgated by the court from 1283 until 1716, when Philip V of Spain gave the Nueva Planta decrees, finishing with the historical laws of Catalonia. These Constitutions were usually made as a royal initiative, but required the favorable vote of the Catalan Courts, the medieval antecedent of the modern Parliaments. These laws had, as the other modern constitutions, preeminence over other laws, and they could not be contradicted by mere decrees or edicts of the king.
In 1392 the "Carta de Logu" was legal code of the Giudicato of Arborea promulgated by the "giudicessa" Eleanor. It was in force in Sardinia until it was superseded by the code of Charles Felix in April 1827. The Carta was a work of great importance in Sardinian history. It was an organic, coherent, and systematic work of legislation encompassing the civil and penal law.
Iroquois "Great Law of Peace".
The "Gayanashagowa", the oral constitution of the Iroquois nation also known as the Great Law of Peace, established a system of governance in which sachems (tribal chiefs) of the members of the Iroquois League made decisions on the basis of universal consensus of all chiefs following discussions that were initiated by a single tribe. The position of sachem descended through families, and were allocated by senior female relatives.
Historians including Donald Grinde, Bruce Johansen and others believe that the Iroquois constitution provided inspiration for the United States Constitution and in 1988 was recognised by a resolution in Congress. The thesis is not considered credible by some scholars. Stanford University historian Jack N. Rakove stated that "The voluminous records we have for the constitutional debates of the late 1780s contain no significant references to the Iroquois" and stated that there are ample European precedents to the democratic institutions of the United States. Francis Jennings noted that the statement made by Benjamin Franklin frequently quoted by proponents of the thesis does not support for this idea as it is advocating for a union against these "ignorant savages" and called the idea "absurd". Anthropologist Dean Snow stated that though Franklin's Albany Plan may have drawn some inspiration from the Iroquois League, there is little evidence that either the Plan or the Constitution drew substantially from this source and argues that "...such claims muddle and denigrate the subtle and remarkable features of Iroquois government. The two forms of government are distinctive and individually remarkable in conception."
Modern constitutions.
The oldest written document still governing a sovereign nation today is that of San Marino. The "Leges Statutae Republicae Sancti Marini" was written in Latin and consists of six books. The first book, with 62 articles, establishes councils, courts, various executive officers and the powers assigned to them. The remaining books cover criminal and civil law, judicial procedures and remedies. Written in 1600, the document was based upon the "Statuti Comunali" (Town Statute) of 1300, itself influenced by the "Codex Justinianus", and it remains in force today.
In 1639, the Colony of Connecticut adopted the Fundamental Orders, which was the first North American constitution, and is the basis for every new Connecticut constitution since, and is also the reason for Connecticut's nickname, "the Constitution State".
The English Protectorate that was set up by Oliver Cromwell after the English Civil War promulgated the first detailed written constitution adopted by a modern state; it was called the Instrument of Government. This formed the basis of government for the short lived republic from 1653 to 1657 by providing a legal rationale for the increasing power of Cromwell, after Parliament consistently failed to govern effectively.
The constitution set up a state council consisting of 21 members while executive authority was vested in the office of "Lord Protector of the Commonwealth"; this position was designated as a non-hereditary life appointment.
The Instrument had difficulty in gaining widespread acceptance as it was widely rejected by both the radicals and Royalists, and Parliament refused to accept it as the basis of its authority. It was eventually replaced by the even more short-lived "Humble Petition and Advice" in May 1657 which finally met its demise in conjunction with the death of Cromwell and the Restoration.
"Agreements and Constitutions of Laws and Freedoms of the Zaporizian Host" was the first European constitution in a modern sense. It was written in 1710 by Pylyp Orlyk, "hetman" of the Zaporozhian Host. This "Constitution of Pylyp Orlyk" (as it is widely known) was written to establish a free Zaporozhian-Ukrainian Republic, with the support of Charles XII of Sweden. It is notable in that it established a democratic standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's "Spirit of the Laws". This Constitution also limited the executive authority of the "hetman", and established a democratically elected Cossack parliament called the General Council. However, Orlyk's project for an independent Ukrainian State never materialized, and his constitution, written in exile, never went into effect.
Other examples of European constitutions of this era were the Corsican Constitution of 1755 and the Swedish Constitution of 1772.
All of the British colonies in North America that were to become the 13 original United States, adopted their own constitutions in 1776 and 1777, during the American Revolution (and before the later Articles of Confederation and United States Constitution), with the exceptions of Massachusetts, Connecticut and Rhode Island. The Commonwealth of Massachusetts adopted its Constitution in 1780, the oldest still-functioning constitution of any U.S. state; while Connecticut and Rhode Island officially continued to operate under their old colonial charters, until they adopted their first state constitutions in 1818 and 1843, respectively.
Democratic constitutions.
What is sometimes called the "enlightened constitution" model was developed by philosophers of the Age of Enlightenment such as Thomas Hobbes, Jean-Jacques Rousseau, and John Locke. The model proposed that constitutional governments should be stable, adaptable, accountable, open and should represent the people (i.e., support democracy).
The United States Constitution, ratified June 21, 1788, was influenced by the British constitutional system and the political system of the United Provinces, plus the writings of Polybius, Locke, Montesquieu, and others. The document became a benchmark for republicanism and codified constitutions written thereafter.
Next were the Polish–Lithuanian Commonwealth Constitution of May 3, 1791, and the French Constitution of September 3, 1791.
On March 19, 1812 a enlightened constitution was ratified in Spain by a parliament gathered in Cadiz, the only Spanish continental city which was safe of French occupation. The Spanish Constitution served as a model for other liberal constitutions of several South-European and Latin American nations like, for example, Portuguese Constitution of 1822, constitutions of various Italian states during Carbonari revolts (i.e., in the Kingdom of the Two Sicilies), the Norwegian constitution of 1814, or the Mexican Constitution of 1824.
In Brazil, the Constitution of 1824 expressed the option for the monarchy as political system after Brazilian Independence. The leader of the national emancipation process was the Portuguese prince Pedro I, elder son of the king of Portugal. Pedro was crowned in 1822 as first emperor of Brazil. The country was ruled by Constitutional monarchy until 1889, when finally adopted the Republican model.
In Denmark, as a result of the Napoleonic Wars, the absolute monarchy lost its personal possession of Norway to another absolute monarchy, Sweden. However the Norwegians managed to infuse a radically democratic and liberal constitution in 1814, adopting many facets from the American constitution and the revolutionary French ones; but maintaining a hereditary monarch limited by the constitution, like the Spanish one.
The Serbian revolution initially led to a proclamation of a proto-constitution in 1811; the full-fledged Constitution of Serbia followed few decades later, in 1835.
The Constitution of Canada came into force on July 1, 1867 as the British North America Act, an act of the British Parliament. The BNA Act unified the colonies of Canada East (Quebec), Canada West (Ontario), Nova Scotia and New Brunswick into the self-governing Dominion of Canada. Over a century later, the BNA Act was patriated to the Canadian Parliament and augmented with the Canadian Charter of Rights and Freedoms. Since then, the written constitution as a whole has been known as the "Constitution Acts, 1867 to 1982", while the original BNA Act is called the "Constitution Act, 1867". Apart from the "Constitution Acts, 1867 to 1982", Canada's constitution also has unwritten elements based in common law and convention. Canadian author and philosopher John Ralston Saul describes the Canadian Constitution as "the second-oldest working constitution in the world."
Principles of constitutional design.
After tribal people first began to live in cities and establish nations, many of these functioned according to unwritten customs, while some developed autocratic, even tyrannical monarchs, who ruled by decree, or mere personal whim. Such rule led some thinkers to take the position that what mattered was not the design of governmental institutions and operations, as much as the character of the rulers. This view can be seen in Plato, who called for rule by "philosopher-kings." Later writers, such as Aristotle, Cicero and Plutarch, would examine designs for government from a legal and historical standpoint.
The Renaissance brought a series of political philosophers who wrote implied criticisms of the practices of monarchs and sought to identify principles of constitutional design that would be likely to yield more effective and just governance from their viewpoints. This began with revival of the Roman law of nations concept and its application to the relations among nations, and they sought to establish customary "laws of war and peace" to ameliorate wars and make them less likely. This led to considerations of what authority monarchs or other officials have and don't have, from where that authority derives, and the remedies for the abuse of such authority.
A seminal juncture in this line of discourse arose in England from the Civil War, the Cromwellian Protectorate, the writings of Thomas Hobbes, Samuel Rutherford, the Levellers, John Milton, and James Harrington, leading to the debate between Robert Filmer, arguing for the divine right of monarchs, on the one side, and on the other, Henry Neville, James Tyrrell, Algernon Sidney, and John Locke. What arose from the latter was a concept of government being erected on the foundations of first, a state of nature governed by natural laws, then a state of society, established by a social contract or compact, which bring underlying natural or social laws, before governments are formally established on them as foundations.
Along the way several writers examined how the design of government was important, even if the government were headed by a monarch. They also classified various historical examples of governmental designs, typically into democracies, aristocracies, or monarchies, and considered how just and effective each tended to be and why, and how the advantages of each might be obtained by combining elements of each into a more complex design that balanced competing tendencies. Some, such as Montesquieu, also examined how the functions of government, such as legislative, executive, and judicial, might appropriately be separated into branches. The prevailing theme among these writers was that the design of constitutions is not completely arbitrary or a matter of taste. They generally held that there are underlying principles of design that constrain all constitutions for every polity or organization. Each built on the ideas of those before concerning what those principles might be.
The later writings of Orestes Brownson would try to explain what constitutional designers were trying to do. According to Brownson there are, in a sense, three "constitutions" involved: The first the "constitution of nature" that includes all of what was called "natural law." The second is the "constitution of society", an unwritten and commonly understood set of rules for the society formed by a social contract before it establishes a government, by which it establishes the third, a "constitution of government". The second would include such elements as the making of decisions by public conventions called by public notice and conducted by established rules of procedure. Each constitution must be consistent with, and derive its authority from, the ones before it, as well as from a historical act of society formation or constitutional ratification. Brownson argued that a state is a society with effective dominion over a well-defined territory, that consent to a well-designed constitution of government arises from presence on that territory, and that it is possible for provisions of a written constitution of government to be "unconstitutional" if they are inconsistent with the constitutions of nature or society. Brownson argued that it is not ratification alone that makes a written constitution of government legitimate, but that it must also be competently designed and applied.
Other writers have argued that such considerations apply not only to all national constitutions of government, but also to the constitutions of private organizations, that it is not an accident that the constitutions that tend to satisfy their members contain certain elements, as a minimum, or that their provisions tend to become very similar as they are amended after experience with their use. Provisions that give rise to certain kinds of questions are seen to need additional provisions for how to resolve those questions, and provisions that offer no course of action may best be omitted and left to policy decisions. Provisions that conflict with what Brownson and others can discern are the underlying "constitutions" of nature and society tend to be difficult or impossible to execute, or to lead to unresolvable disputes.
Constitutional design has been treated as a kind of metagame in which play consists of finding the best design and provisions for a written constitution that will be the rules for the game of government, and that will be most likely to optimize a balance of the utilities of justice, liberty, and security. An example is the metagame Nomic.
Governmental constitutions.
Most commonly, the term "constitution" refers to a set of rules and principles that define the nature and extent of government. Most constitutions seek to regulate the relationship between institutions of the state, in a basic sense the relationship between the executive, legislature and the judiciary, but also the relationship of institutions within those branches. For example, executive branches can be divided into a head of government, government departments/ministries, executive agencies and a civil service/administration. Most constitutions also attempt to define the relationship between individuals and the state, and to establish the broad rights of individual citizens. It is thus the most basic law of a territory from which all the other laws and rules are hierarchically derived; in some territories it is in fact called "Basic Law".
Key features.
The following are features of democratic constitutions that have been identified by political scientists to exist, in one form or another, in virtually all national constitutions.
Codification.
A fundamental classification is codification or lack of codification. A codified constitution is one that is contained in a single document, which is the single source of constitutional law in a state. An uncodified constitution is one that is not contained in a single document, consisting of several different sources, which may be written or unwritten; see constitutional convention.
Codified constitution.
Most states in the world have codified constitutions.
Codified constitutions are often the product of some dramatic political change, such as a revolution. The process by which a country adopts a constitution is closely tied to the historical and political context driving this fundamental change. The legitimacy (and often the longevity) of codified constitutions has often been tied to the process by which they are initially adopted and some scholars have that high constitutional turnover within a given country may itself be detrimental to separation of powers and the rule of law.
States that have codified constitutions normally give the constitution supremacy over ordinary statute law. That is, if there is any conflict between a legal statute and the codified constitution, all or part of the statute can be declared "ultra vires" by a court, and struck down as unconstitutional. In addition, exceptional procedures are often required to amend a constitution. These procedures may include: convocation of a special constituent assembly or constitutional convention, requiring a supermajority of legislators' votes, the consent of regional legislatures, a referendum process, and/or other procedures that make amending a constitution more difficult than passing a simple law.
Constitutions may also provide that their most basic principles can never be abolished, even by amendment. In case a formally valid amendment of a constitution infringes these principles protected against any amendment, it may constitute a so-called "unconstitutional constitutional law".
Codified constitutions normally consist of a ceremonial preamble, which sets forth the goals of the state and the motivation for the constitution, and several articles containing the substantive provisions. The preamble, which is omitted in some constitutions, may contain a reference to God and/or to fundamental values of the state such as liberty, democracy or human rights.
Uncodified constitution.
, only two sovereign states have uncodified constitutions, namely New Zealand and the United Kingdom. The Basic Laws of Israel are arguably its equivalent to a constitution.
Uncodified constitutions are the product of an "evolution" of laws and conventions over centuries. By contrast to codified constitutions (in the Westminster System that originated in England), uncodified constitutions include written sources: e.g. constitutional statutes enacted by the Parliament and also unwritten sources: constitutional conventions, observation of precedents, royal prerogatives, custom and tradition, such as always holding the General Election on Thursdays; together these constitute the British constitutional law.
Written versus unwritten; codified versus uncodified.
Some constitutions are largely, but not wholly, codified. For example, in the Constitution of Australia, most of its fundamental political principles and regulations concerning the relationship between branches of government, and concerning the government and the individual are codified in a single document, the Constitution of the Commonwealth of Australia. However, the presence of statutes with constitutional significance, namely the Statute of Westminster, as adopted by the Commonwealth in the Statute of Westminster Adoption Act 1942, and the Australia Act 1986 means that Australia's constitution is not contained in a single constitutional document. It means the Constitution of Australia is uncodified, it also contain constitutional conventions, thus is partially unwritten.
The Constitution of Canada, which evolved from the British North America Acts until severed from nominal British control by the Canada Act 1982 (analogous to the Australia Act 1986), is a similar example. Canada's constitution consists of almost 30 different statutes.
The terms "written constitution" and "codified constitution" are often used interchangeably, as are "unwritten constitution" and "uncodified constitution", although this usage is technically inaccurate. A codified constitution is a written constitution contained in a single document, states that do not have such a document have uncodified constitutions but not entirely unwritten constitutions since much of an uncodified constitution is usually written in laws, such as the Basic Laws of Israel or the Parliament Acts of the United Kingdom.
Entrenchment.
The presence or lack of entrenchment is a fundamental feature of constitutions. An entrenched constitution cannot be altered in any way by a legislature as part of its normal business concerning ordinary statutory laws, but can only be amended by a different and more onerous procedure. There may be a requirement for a special body to be set up, or the proportion of favourable votes of members of existing legislative bodies may be required to be higher to pass a constitutional amendment than for statutes. The entrenched clauses of a constitution can create different degrees of entrenchment, ranging from simply excluding constitutional amendment from the normal business of a legislature, to making certain amendments either more difficult than normal modifications, or forbidden under any circumstances.
Entrenchment is an inherent feature in most codified constitutions. A codified constitution will incorporate the rules which must be followed for the constitution itself to be changed.
The US constitution is an example of an entrenched constitution, and the UK constitution is an example of a constitution that is not entrenched (or codified). In some states the text of the constitution may be changed; in others the original text is not changed, and amendments are passed which add to and may override the original text and earlier amendments.
Procedures for constitutional amendment vary between states. In a nation with a federal system of government the approval of a majority of state or provincial legislatures may be required. Alternatively, a national referendum may be required. Details are to be found in the articles on the constitutions of the various nations and federal states in the world.
In constitutions that are not entrenched, no special procedure is required for modification. Lack of entrenchment is a characteristic of uncodified constitutions; the constitution is not recognised with any higher legal status than ordinary statutes. In the UK, for example laws which modify written or unwritten provisions of the constitution are passed on a simple majority in Parliament. No special "constitutional amendment" procedure is required. The principle of parliamentary sovereignty holds that no sovereign parliament may be bound by the acts of its predecessors; and there is no higher authority that can create law which binds Parliament. The sovereign is nominally the head of state with important powers, such as the power to declare war; the uncodified and unwritten constitution removes all these powers in practice.
In practice democratic governments do not use the lack of entrenchment of the constitution to impose the will of the government or abolish all civil rights, as they could in theory do, but the distinction between constitutional and other law is still somewhat arbitrary, usually following historical principles embodied in important past legislation. For example, several British Acts of Parliament such as the Bill of Rights, Human Rights Act and, prior to the creation of Parliament, Magna Carta are regarded as granting fundamental rights and principles which are treated as almost constitutional. Several rights that in another state might be guaranteed by constitution have indeed been abolished or modified by the British parliament in the early 21st century, including the unconditional right to trial by jury, the right to silence without prejudicial inference, permissible detention before a charge is made extended from 24 hours to 42 days, and the right not to be tried twice for the same offence.
Absolutely unmodifiable articles.
The strongest level of entrenchment exists in those constitutions that state that some of their most fundamental principles are absolute, i.e. certain articles may not be amended under any circumstances. An amendment of a constitution that is made consistently with that constitution, except that it violates the absolute non-modifiability, can be called an "unconstitutional constitutional law". Ultimately it is always possible for a constitution to be overthrown by internal or external force, for example, a revolution (perhaps claiming to be justified by the right to revolution) or invasion.
In the Constitution of India, the Supreme Court has created the Doctrine of Basic Structure in Kesavananda Bharti's case (1973) stating that the essential features of the Basic structure cannot be amended by the Parliament. The Court has identified judicial review, independence of Judiciary, free and fair election, core of Fundamental Rights as a few of the essential features which are unamendable. However, the Supreme Court did not identify specific provisions which are in the category of absolute entrenchment. A critical analysis of the Doctrine of Basic Structure appears in Professor M.K. Bhandari's book "Basic Structure of Indian Constitution - A Critical Reconsideration".
An example of absolute unmodifiability is the German Federal Constitution. This states in Articles 1 and 20 that the state powers, which derive from the people, must protect human dignity on the basis of human rights, which are directly applicable law binding on all three branches of government, which is a democratic and social federal republic; that legislation must be according to the rule of law; and that the people have the right of resistance as a last resort against any attempt to abolish the constitutional order. Article 79, Section 3 states that these articles cannot be changed, even according to the methods of amendment defined elsewhere in the document.
Another example is the Constitution of Honduras, which has an article stating that the article itself and certain other articles cannot be changed in any circumstances. Article 374 of the Honduras Constitution asserts this unmodifiability, stating, "It is not possible to reform, in any case, the preceding article, the present article, the constitutional articles referring to the form of government, to the national territory, to the presidential period, the prohibition to serve again as President of the Republic, the citizen who has performed under any title in consequence of which she/he cannot be President of the Republic in the subsequent period." This unmodifiability article played an important role in the 2009 Honduran constitutional crisis.
Distribution of sovereignty.
Constitutions also establish where sovereignty is located in the state. There are three basic types of distribution of sovereignty according to the degree of centralisation of power: unitary, federal, and confederal. The distinction is not absolute.
In a unitary state, sovereignty resides in the state itself, and the constitution determines this. The territory of the state may be divided into regions, but they are not sovereign and are subordinate to the state. In the UK, the constitutional doctrine of Parliamentary sovereignty dictates than sovereignty is ultimately contained at the centre. Some powers have been devolved to Northern Ireland, Scotland, and Wales (but not England). Some unitary states (Spain is an example) devolve more and more power to sub-national governments until the state functions in practice much like a federal state.
A federal state has a central structure with at most a small amount of territory mainly containing the institutions of the federal government, and several regions (called "states", "provinces", etc.) which compose the territory of the whole state. Sovereignty is divided between the centre and the constituent regions. The constitutions of Canada and the United States establish federal states, with power divided between the federal government and the provinces or states. Each of the regions may in turn have its own constitution (of unitary nature).
A confederal state comprises again several regions, but the central structure has only limited coordinating power, and sovereignty is located in the regions. Confederal constitutions are rare, and there is often dispute to whether so-called "confederal" states are actually federal.
To some extent a group of states which do not constitute a federation as such may by treaties and accords give up parts of their sovereignty to a supranational entity. For example the countries constituting the European Union have agreed to abide by some Union-wide measures which restrict their absolute sovereignty in some ways, e.g., the use of the metric system of measurement instead of national units previously used.
Separation of powers.
Constitutions usually explicitly divide power between various branches of government. The standard model, described by the Baron de Montesquieu, involves three branches of government: executive, legislative and judicial. Some constitutions include additional branches, such as an auditory branch. Constitutions vary extensively as to the degree of separation of powers between these branches.
Lines of accountability.
In presidential and semi-presidential systems of government, department secretaries/ministers are accountable to the president, who has patronage powers to appoint and dismiss ministers. The president is accountable to the people in an election.
In parliamentary systems, Cabinet Ministers are accountable to Parliament, but it is the prime minister who appoints and dismisses them. In the case of the United Kingdom and other countries with a Monarchy, it is the Monarch who appoints and dismisses ministers, on the advice of the Prime Minister. In turn the prime minister will resign if the government loses the confidence of the parliament (or a part of it). Confidence can be lost if the government loses a vote of no confidence or, depending on the country, loses a particularly important vote in parliament such as vote on the budget. When a government loses confidence it stays in office until a new government is formed; something which normally but not necessarily required the holding of a general election.
State of emergency.
Many constitutions allow the declaration under exceptional circumstances of some form of state of emergency during which some rights and guarantees are suspended. This deliberate loophole can be and has been abused to allow a government to suppress dissent without regard for human rights—see the article on state of emergency.
Façade constitutions.
Italian political theorist Giovanni Sartori noted the existence of national constitutions which are a façade for authoritarian sources of power. While such documents may express respect for human rights or establish an independent judiciary, they may be ignored when the government feels threatened, or never put into practice. An extreme example was the Constitution of the Soviet Union that on paper supported freedom of assembly and freedom of speech; however, citizens who transgressed unwritten limits were summarily imprisoned. The example demonstrates that the protections and benefits of a constitution are ultimately provided not through its written terms but through deference by government and society to its principles. A constitution may change from being real to a façade and back again as democratic and autocratic governments succeed each other.
Constitutional courts.
Constitutions are often, but by no means always, protected by a legal body whose job it is to interpret those constitutions and, where applicable, declare void executive and legislative acts which infringe the constitution. In some countries, such as Germany, this function is carried out by a dedicated constitutional court which performs this (and only this) function. In other countries, such as Ireland, the ordinary courts may perform this function in addition to their other responsibilities. While elsewhere, like in the United Kingdom, the concept of declaring an act to be unconstitutional does not exist.
A constitutional violation is an action or legislative act that is judged by a constitutional court to be contrary to the constitution, that is, unconstitutional. An example of constitutional violation by the executive could be a public office holder who acts outside the powers granted to that office by a constitution. An example of constitutional violation by the legislature is an attempt to pass a law that would contradict the constitution, without first going through the proper constitutional amendment process.
Some countries, mainly those with uncodified constitutions, have no such courts at all. For example the United Kingdom has traditionally operated under the principle of parliamentary sovereignty under which the laws passed by United Kingdom Parliament could not be questioned by the courts.
See also.
"Judicial philosophies of constitutional interpretation (note: generally specific to United States constitutional law)"

</doc>
<doc id="5254" url="http://en.wikipedia.org/wiki?curid=5254" title="Common law">
Common law

Common law (also known as case law or precedent) is law developed by judges through decisions of courts and similar tribunals, as opposed to statutes adopted through the legislative process or regulations issued by the executive branch.
A "common law system" is a legal system that gives great precedential weight to common law, on the principle that it is unfair to treat similar facts differently on different occasions. The body of precedent is called "common law" and it binds future decisions. In cases where the parties disagree on what the law is, a common law court looks to past precedential decisions of relevant courts. If a similar dispute has been resolved in the past, the court is usually bound to follow the reasoning used in the prior decision (this principle is known as "stare decisis"). If, however, the court finds that the current dispute is fundamentally distinct from all previous cases (called a "matter of first impression"), judges have the authority and duty to make law by creating precedent. Thereafter, the new decision becomes precedent, and will bind future courts.
In practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity. However, "stare decisis", the principle that similar cases should be decided according to consistent principled rules so that they will reach similar results, lies at the heart of all common law systems.
One third of the world's population (approximately 2.3 billion people) live in common law jurisdictions or in systems mixed with civil law. Particularly common law is in England where it originated in the Middle Ages, and in countries that trace their legal heritage to England as former colonies of the British Empire, including India, the United States, 49 of its 50 states (excluding Louisiana), Pakistan, Nigeria, Bangladesh, Canada and all its provinces except Quebec, Malaysia, Ghana, Australia, Sri Lanka, Hong Kong, Singapore, Burma, Ireland, New Zealand, Jamaica, Trinidad and Tobago, Cyprus, Barbados, South Africa, Zimbabwe, Cameroon, Namibia, Liberia, Sierra Leone, Botswana, Guyana, Israel and Fiji.
Primary connotations.
The term "common law" has three main connotations and several historical meanings worth mentioning:
1. Common law as opposed to statutory law and regulatory law.
Connotation 1 distinguishes the authority that promulgated a law. For example, most areas of law in most Anglo-American jurisdictions include "statutory law" enacted by a legislature, "regulatory law" promulgated by executive branch agencies pursuant to delegation of rule-making authority from the legislature, and common law (connotation 1) or "case law", "i.e.", decisions issued by courts (or quasi-judicial tribunals within agencies). This first connotation can be further differentiated into
2. Common law legal systems as opposed to civil law legal systems.
Connotation 2 differentiates "common law" jurisdictions and legal systems from "civil law" or "code" jurisdictions. Common law (connotation 2) systems place great weight on court decisions, which are considered "law" with the same force of law as statutes—for nearly a millennium, common law (connotation 2) courts have had the authority to make law where no legislative statute exists, and statutes mean what courts interpret them to mean. By contrast, in civil law jurisdictions (the legal tradition that prevails, or is combined with common law, in Europe and most non-Islamic, non-common law countries), courts lack authority to act where there is no statute, and judicial precedent is given less interpretive weight (which means that a judge deciding a given case has more freedom to interpret the text of a statute independently, and less predictably), and scholarly literature is given more. For example, the Napoleonic code expressly forbade French judges to pronounce general principles of law.
As a rule of thumb, common law (connotation 2) systems trace their history to England, while civil law systems trace their history through the Napoleonic Code back to the Corpus Juris Civilis of Roman law.
The contrast between common law and civil law systems is elaborated in "Contrasts between common law and civil law systems" and "Alternatives to common law systems", below.
3. Law as opposed to equity.
Connotation 3 differentiates "common law" (or just "law") from "equity". Before 1873, England had two parallel court systems: courts of "law" that could only award money damages and recognized only the legal owner of property, and courts of "equity" (courts of chancery) that could issue injunctive relief (that is, a court order to a party to do something, give something to someone, or stop doing something) and recognized trusts of property. This split propagated to many of the colonies, including the United States (see "Reception Statutes", below). For most purposes, most jurisdictions, including the U.S. federal system and most states, have merged the two courts. Additionally, even before the separate courts were merged, most courts were permitted to apply both law (connotation 3) and equity, though under potentially different procedural law. Nonetheless, the historical distinction between "law" (in connotation 3) and "equity" remains important today when the case involves issues such as the following:
4. Historical uses.
In addition, there are several historical uses of the term that provide some background as to its meaning.
In one archaic usage, "common law" refers to the pre-Christian system of law, imported by the Saxons to England, and dating to before the Norman conquest, and before there was any consistent law to be applied. This definition is found or alluded to in some internet dictionaries.
"Common law" as the term is used today in common law countries contrasts with "ius commune" While historically the "ius commune" became a secure point of reference in continental European legal systems, in England it was not a point of reference at all.
The English Court of Common Pleas dealt with lawsuits in which the Monarch had no interest, i.e., between commoners.
Additionally, from at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice. The term "common law" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.
These definitions are archaic, their relevance having dissipated with the development of the English legal system over the centuries, but they do explain the origin of the term as used today.
Basic principles of common law.
Common law adjudication.
In a common law jurisdiction several stages of research and analysis are required to determine "what the law is" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. Later decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines "what the law is". Then, one applies that law to the facts.
The common law evolves to meet changing social needs and improved understanding.
Justice Holmes cautioned that “the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions.” Justice Cardozo noted the “common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively,” but “[i]ts method is inductive, and it draws its generalizations from particulars.”
The common law (connotation 1) is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law (connotation 1) evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as legislatures tend to delay action until a situation is totally intolerable. For these reasons, legislative changes tend to be large, jarring and disruptive (sometimes positively, sometimes negatively, and sometimes with unintended consequences).
One example of the gradual change that typifies evolution of the common law (connotation 1) is the gradual change in liability for negligence. For example, the traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were in privity of contract. Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, "Winterbottom v. Wright", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The "Winterbottom" court recognized that there would be "absurd and outrageous consequences" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract ("privity") with the negligent party.
A first exception to this rule arose in an 1852 case by New York's highest court, "Thomas v. Winchester", which held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put "human life in imminent danger." "Thomas" used this as a reason to create an exception to the "privity" rule. In, 1909, New York held in "Statler v. Ray Mfg. Co." that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn "was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed."
Yet the privity rule survived. In "Cadillac Motor Car Co. v. Johnson", (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was "no question that the wheel was made of dead and ‘dozy‘ wood, quite insufficient for its purposes." The "Cadillac" court was willing to acknowledge that the case law supported exceptions for "an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor." However, held the "Cadillac" court, "one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud,"
Finally, in the famous case of "MacPherson v. Buick Motor Co.", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to "Cadillac" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:
Cardozo's new "rule" exists in no prior case, but is inferrable as a synthesis of the "thing of danger" principle stated in them, merely extending it to "foreseeable danger" even if "the purposes for which it was designed" were not themselves "a source of great danger." "MacPherson" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of "Winterbottom", that "absurd and outrageous consequences" must be avoided, and he does so by drawing a new line in the last sentence quoted above: "There must be knowledge of a danger, not merely possible, but probable." But while adhering to the underlying principle that "some" boundary is necessary, "MacPherson" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.
This illustrates two crucial principles that are often not well understood by non-lawyers. (a) The common law evolves, this evolution is in the hands of judges, and judges have "made law" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.
Interaction of constitutional, statutory and common law.
In common law legal systems (connotation 2), the common law (connotation 1) is crucial to understanding almost all important areas of law. For example, in England and Wales, in English Canada, and in most states of the United States, the basic law of contracts, torts and property do not exist in statute, but only in common law (though there may be isolated modifications enacted by statute). As another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.
In almost all areas of the law (even those where there is a statutory framework, such as contracts for the sale of goods, or the criminal law), legislature-enacted statutes generally give only terse statements of general principle, and the fine boundaries and definitions exist only in the common law (connotation 1(a)). To find out what the precise law is that applies to a particular set of facts, one has to locate precedential decisions on the topic, and reason from those decisions by analogy. To consider but one example, the First Amendment to the United States Constitution states "Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof"—but interpretation (that is, determining the fine boundaries, and resolving the tension between the "establishment" and "free exercise" clauses) of each of the important terms was delegated by Article III of the Constitution to the judicial branch, so that the current legal boundaries of the Constitutional text can only be determined by consulting the common law.
In common law jurisdictions (connotation 2), legislatures operate under the assumption that statutes will be interpreted against the backdrop of the pre-existing common law (connotation 1) and custom. For example, in most U.S. states, the criminal statutes are primarily codification of pre-existing common law. (Codification is the process of enacting a statute that collects and restates pre-existing law in a single document—when that pre-existing law is common law, the common law remains relevant to the interpretation of these statutes.) In reliance on this assumption, modern statutes often leave a number of terms and fine distinctions unstated—for example, a statute might be very brief, leaving the precise definition of terms unstated, under the assumption that these fine distinctions will be inherited from pre-existing common law. (For this reason, many modern American law schools teach the common law of crime as it stood in England in 1789, because that centuries-old English common law is a necessary foundation to interpreting modern criminal statutes.)
With the transition from English law, which had common law crimes, to the new legal system under the U.S. Constitution, which prohibited "ex post facto" laws at both the federal and state level, the question was raised whether there could be common law crimes in the United States. It was settled in the case of "United States v. Hudson and Goodwin", , which decided that federal courts had no jurisdiction to define new common law crimes, and that there must always be a (constitutional) statute defining the offense and the penalty for it.
Still, many states retain selected common law crimes. For example, in Virginia, the definition of the conduct that constitutes the crime of robbery exists only in the common law, and the robbery statute only sets the punishment. Virginia Code section 1-200 establishes the continued existence and vitality of common law principles and provides that "The common law of England, insofar as it is not repugnant to the principles of the Bill of Rights and Constitution of this Commonwealth, shall continue in full force within the same, and be the rule of decision, except as altered by the General Assembly."
By contrast to statutory codification of common law, some statutes displace common law, for example to create a new cause of action that did not exist in the common law, or to legislatively overrule the common law. An example is the tort of wrongful death, which allows certain persons, usually a spouse, child or estate, to sue for damages on behalf of the deceased. There is no such tort in English common law; thus, any jurisdiction that lacks a wrongful death statute will not allow a lawsuit for the wrongful death of a loved one. Where a wrongful death statute exists, the compensation or other remedy available is limited to the remedy specified in the statute (typically, an upper limit on the amount of damages). Courts generally interpret statutes that create new causes of action narrowly—that is, limited to their precise terms—because the courts generally recognize the legislature as being supreme in deciding the reach of judge-made law unless such statute should violate some "second order" constitutional law provision ("cf". judicial activism).
Where a tort is rooted in common law (connotation 1(a)), all traditionally recognized damages for that tort may be sued for, whether or not there is mention of those damages in the current statutory law. For instance, a person who sustains bodily injury through the negligence of another may sue for medical costs, pain, suffering, loss of earnings or earning capacity, mental and/or emotional distress, loss of quality of life, disfigurement and more. These damages need not be set forth in statute as they already exist in the tradition of common law. However, without a wrongful death statute, most of them are extinguished upon death.
In the United States, the power of the federal judiciary to review and invalidate unconstitutional acts of the federal executive branch is stated in the constitution, Article III sections 1 and 2: "The judicial Power of the United States, shall be vested in one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish. ... The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority..." The first famous statement of "the judicial power" was "Marbury v. Madison", . Later cases interpreted the "judicial power" of Article III to establish the power of federal courts to consider or overturn any action of Congress or of any state that conflicts with the Constitution.
Overruling precedent—the limits of "stare decisis".
The United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.
Most of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting "en banc" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.
Other courts, for example, the Court of Customs and Patent Appeals and the Supreme Court, always sit "en banc", and thus the "later" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts—for example, Supreme Court interpretations of the constitution or federal statutes—are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.
In the UK, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify decisions of lower courts. From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.
Canada's system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.
Common law as a foundation for commercial economies.
The reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful. This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.
In contrast, in non-common-law countries, and jurisdictions with very weak respect for precedent (example, the U.S. Patent Office), fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less "a priori" guidance and must often leave a bigger "safety margin" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.
This is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York—and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a "choice of law clause" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on.) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. (Other sources cite the tax advantages of Delaware.) On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.
Likewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.
This is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures—but, conversely, that tends to make the statute more difficult to read (the United States tax code is an example). Nonetheless, as a practical matter, no civil law legislature can ever address the full spectrum of factual possibilities in the breadth, depth and detail of the case law of the common law courts of even a smaller jurisdiction, and that deeper, more complete body of law provides additional predictability that promotes commerce.
History.
The term "common law" originally derives from the 1150s and 1160s, when Henry II of England established the secular English tribunals. The "common law" was the law that emerged as "common" throughout the realm (as distinct from the various legal codes that preceded it, such as Mercian law, the Danelaw and the law of Wessex) as the king's judges followed each other's decisions to create a unified common law throughout England. The doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.
The form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.
Medieval English common law.
In the late 800s, Alfred the Great assembled the Doom book (not to be confused with the more-famous Domesday Book from 200 years later), which collected the existing laws of Kent, Wessex, and Mercia, and attempted to blend in the Mosaic code, Christian principles, and Germanic customs dating as far as the fifth century.
Before the Norman conquest in 1066, justice was administered primarily by what is today known as the county courts (the modern "counties" were referred to as "shires" in pre-Norman times), presided by the diocesan bishop and the sheriff, exercising both ecclesiastical and civil jurisdiction. Trial by jury began in these courts.
In 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law "common" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system—citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.
Henry II developed the practice of sending judges from his own central court to hear the various disputes throughout the country. His judges would resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as "stare decisis" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name "common law."
Henry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. Eventually, Becket was murdered inside Canterbury Cathedral by four knights who believed themselves to be acting on Henry's behalf. Whether Henry actually intended to bring about the assassination of Becket is debatable, but there is no question that at the time of the murder, the two men were embroiled in a bitter dispute regarding the power of Royal Courts to exercise jurisdiction over former clergymen. The murder of the Archbishop gave rise to a wave of popular outrage against the King. Henry was forced to repeal the disputed laws and to abandon his efforts to hold church members accountable for secular crimes (see also Constitutions of Clarendon).
Judge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. It is important to understand that common law is the older and more traditional source of law, and legislative power is simply a layer applied on top of the older common law foundation. Since the 12th century, courts have had parallel and co-equal authority to make law—"legislating from the bench" is a traditional and essential function of courts, which was carried over into the U.S. system as an essential component of the "judicial power" specified by Article III of the U.S. constitution. Justice Oliver Wendell Holmes, Jr. observed in 1917 that "judges do and must legislate." There are legitimate debates on how the powers of courts and legislatures should be balanced. However, a view that courts lack law-making power is historically inaccurate and constitutionally unsupportable.
Influences of foreign legal systems.
Roman law.
The term "common law" (connotation 2) is often used as a contrast to Roman-derived "civil law", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.
By the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton’s "De Legibus et Consuetudinibus Angliae" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian’s "Institutes". The impact Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into "in rem" (typically, actions against a "thing" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and "in personam" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone’s "Commentaries on the Laws of England", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.
Propagation of the common law to the colonies and Commonwealth by reception statutes.
A reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence English law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedents originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, except Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.
Other examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.
Decline of Latin maxims, and adding flexibility to "stare decisis".
Well into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, "One cannot be a judge in one's own cause" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such at those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims. See also Thomas Jefferson's letter to Thomas Cooper.
Reliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, was under full attack by the late 19th century. Oliver Wendell Holmes, Jr. in his famous article, "The Path of the Law", commented, "It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past." Justice Holmes noted that study of maxims might be sufficient for "the man of the present," but "the man of the future is the man of statistics and the master of economics." In an 1880 lecture at Harvard, he wrote:
The life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics.
In the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.
Reliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors "should" influence adjudication is the subject of active debate, but that judges "do" draw of learning from other fields and jurisdictions is a fact of modern legal life.
1870 through 20th century, and the procedural merger of law and equity.
As early as the 15th century, it became the practice that litigants who felt they had been cheated by the common-law system would petition the King in person. For example, they might argue that an award of damages (at common law (connotation 3)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail. A famous example is the fictional case of "Jarndyce v. Jarndyce" in "Bleak House", by Charles Dickens.
In England, courts of law (connotation 3) and equity were combined by the Judicature Acts of 1873 and 1875, with equity being supreme in case of conflict.
In the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the "civil action." Fed.R.Civ.P. 2. The distinction survives to the extent that issues that were "common law (connotation 3)" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and "equity" issues are decided by a judge.
Delaware, Mississippi, and Tennessee still have separate courts of law and equity, for example, the Court of Chancery. In many states there are separate divisions for law and equity within one court.
Common law pleading and its abolition in the early 20th century.
For centuries, through the 19th century, the common law recognized only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into one of them: Debt, Detinue, Covenant, Special Assumpsit, General Assumpsit, Trespass, Trover, Replevin, Case (or Trespass on the Case), and Ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific "magic words" encrusted over the centuries. Under the old common law pleading standards, a suit by a "pro se" ("for oneself," without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.
One of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant "a short and plain statement" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.
Contrasts between common law and civil law systems.
Adversarial system vs. inquisitorial system.
Common law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. In contrast, civil law systems usually use an inquisitorial system in which an examining magistrate serves two roles by developing the evidence and arguments for one side and then the other during the investigation phase.
The examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.
The proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change her or his evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.
There are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages ("e.g.", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.
General principles of law.
Both common law and civil law jurisdictions have formed what they variously call "pure common law" or "general principles of law" to define what the law is in the absence of, or gap in, legislation.
Constant jurisprudence.
Unlike "stare decisis", the sharp separation of powers between the judiciary and executive, and distinction between "jurisprudence constante" and regulatory law (called "administrative law" in civil law systems), is maintained by considering judge-made law to be non-binding.
Contrasting role of treatises and academic writings in common law and civil law systems.
The role of the legal academy presents a significant "cultural" difference between common law (connotation 2) and civil law jurisdictions.
In common law jurisdictions, legal treatises compile common law decisions and state overarching principles that (in the author's opinion) explain the results of the cases. However, in common law jurisdictions, treatises are not the law, and lawyers and judges tend to use these treatises as only "finding aids" to locate the relevant cases. In common law jurisdictions, scholarly work is seldom cited as authority for what the law is. When common law courts rely on scholarly work, it is almost always only for factual findings, policy justification, or the history and evolution of the law, but the court's legal conclusion is reached through analysis of relevant statutes and common law, seldom scholarly commentary.
In contrast, in civil law jurisdictions, courts give the writings of law professors significant weight, partly because civil law decisions traditionally were very brief, sometimes no more than a paragraph stating who wins and who loses. The rationale had to come from somewhere else: the academy often filled that role. This balance may shift as civil law court decisions move in the direction of common law reasoning.
Common law legal systems in the present day.
The common law constitutes the basis of the legal systems of: England and Wales and Northern Ireland in the UK, Ireland, federal law in the United States and the law of individual U.S. states (except Louisiana), federal law throughout Canada and the law of the individual provinces and territories (except Quebec), Australia (both federal and individual states), Kenya, New Zealand, South Africa, India, Myanmar, Malaysia, Bangladesh, Brunei, Pakistan, Singapore, Hong Kong, Antigua and Barbuda, Barbados, Bahamas, Belize, Dominica, Grenada, Jamaica, St Vincent and the Granadines, Saint Kitts and Nevis, Trinidad and Tobago, and many other generally English-speaking countries or Commonwealth countries (except the UK's Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the law of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. India uses common law except in the state of Goa which retains the Portuguese civil code. Guyana and Saint Lucia have mixed Common Law and Civil Law systems.
Scotland.
Scotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the Corpus Juris Civilis with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scots common law differed in that the use of "precedents" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a "precedent", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a stare decisis akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same although in many matters (particularly those of UK-wide interest) they are very similar. Comparable pluralistic (or 'mixed') legal systems operate in Quebec, Louisiana and South Africa.
States of the United States (1600s on).
New York (1600s).
The state of New York, which also has a civil law history from its Dutch colonial days, also began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. When the English finally regained control of New Netherland they forced, as a punishment unique in the history of the British Empire, the English imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.
Louisiana (1700s).
Uniquely among U.S. states, Louisiana's codified system, the Louisiana Civil Code, is based on principles of law from continental Europe instead of common law. These principles derive ultimately from Roman law, transmitted through Spanish and French law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.
Historically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.
California (1850s).
The U.S. state of California has a system based on common law, but it has codified the law in the manner of the civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case "Li v. Yellow Cab Co.", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)
United States federal system (1789 and 1938).
The United States federal government (as opposed to the states) has a variant on a common law system. United States federal courts only act as interpreters of statutes and the constitution by elaborating and precisely defining the broad language (connotation 1(b) above), but, unlike state courts, do not act as an independent source of common law (connotation 1(a) above).
Before 1938, the federal courts, like almost all other common law courts, decided the law on any issue where the relevant legislature (either the U.S. Congress or state legislature, depending on the issue), had not acted, by looking to courts in the same system, that is, other federal courts, even on issues of state law, and even where there was no express grant of authority from Congress or the Constitution.
In 1938, the U.S. Supreme Court in "Erie Railroad Co. v. Tompkins" (1938), overruled earlier precedent, and held "There is no federal general common law," thus confining the federal courts to act only as interpreters of law originating elsewhere. "E.g.", "Texas Industries v. Radcliff", (without an express grant of statutory authority, federal courts cannot create rules of intuitive justice, for example, a right to contribution from co-conspirators). Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.
Later courts have limited "Erie" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. "See, e.g.", "Clearfield Trust Co. v. United States", (giving federal courts the authority to fashion common law rules with respect to issues of federal power, in this case negotiable instruments backed by the federal government); "see also" "International News Service v. Associated Press", (1918) (creating a cause of action for misappropriation of "hot news" that lacks any statutory grounding, but that is one of the handful of federal common law actions that survives today); "National Basketball Association v. Motorola, Inc.", 105 F.3d 841, 843–44, 853 (2d Cir. 1997) (noting continued vitality of INS "hot news" tort under New York state law, but leaving open the question of whether it survives under federal law). Except on Constitutional issues, Congress is free to legislatively overrule federal courts' common law.
India (19th century and 1948).
Indian Law is largely based on English common law because of the long period of British colonial influence during the period of the British Raj.
After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act of 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government.
Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.
Indian family law is complex, with each religion adhering to its own specific laws. In most states, registering marriages and divorces is not compulsory. There are separate laws governing Hindus, Muslims, Christians, Sikhs and followers of other religions. The exception to this rule is in the state of Goa, where a Portuguese uniform civil code is in place, in which all religions have a common law regarding marriages, divorces and adoption.
Ancient India represented a distinct tradition of law, and had an historically independent school of legal theory and practice. The "Arthashastra", dating from 400 BCE and the "Manusmriti", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character. When India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.
There are 1160 laws as of September 2007.
Canada (1867).
Canada has separate federal and provincial legal systems. The division of jurisdiction between the federal and provincial Parliaments is specified in the Canadian constitution.
Each province is considered a separate jurisdiction with respect to common law matters. As such, only the provincial legislature may enact legislation to amend private law. Each has its own procedural law, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. This is the highest court in provincial jurisdiction, only subject to the Supreme Court of Canada in terms of appeal of their decisions. All but one of the provinces of Canada use a common law system (the exception being Quebec, which uses a civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).
Canadian federal statutes must use the terminology of both the common law and civil law for those matters; this is referred to as legislative bijuralism.
Federal Courts operate under a separate system throughout Canada and deal with narrower subject matter than superior courts in provincial jurisdiction. They hear cases reserved for federal jurisdiction by the Canadian constitution, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal Federal Court of Appeal is the appellate level court in federal jurisdiction and hears cases in multiple cities, and unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.
Criminal law is uniform throughout Canada. It is based on the constitution and federal statutory Criminal Code, as interpreted by the Supreme Court of Canada. The administration of justice and enforcement of the criminal code are the responsibilities of the provinces.
Nicaragua.
Nicaragua's legal system also is a mixture of the English Common Law and the Civil Law. This situation was brought through the influence of British administration of the Eastern half of the country from the mid-17th century until about 1905, the William Walker period from about 1855 through 1857, USA interventions/occupations during the period from 1909 to 1933, the influence of USA institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of USA culture and institutions.
Israel (1948).
Israel has a mixed system of common law and civil law. While Israeli law is undergoing codification, its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).
Roman Dutch Common law.
Roman Dutch Commons law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri-Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French "code civil" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 15th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.
Alternatives to common law systems.
The main alternative to the common law system is the civil law system, which is used in Continental Europe, and most of the rest of the world. The contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.
Examples of common law being replaced by statute or codified rule in the United States include criminal law (since 1812, U.S. courts have held that criminal law must be embodied in statute if the public is to have fair notice), commercial law (the Uniform Commercial Code in the early 1960s) and procedure (the Federal Rules of Civil Procedure in the 1930s and the Federal Rules of Evidence in the 1970s). But note that in each case, the statute sets the general principles, but the interstitial common law process (connotation 1(b)) determines the scope and application of the statute.
An example of convergence from the other direction is shown in , in which the European Court of Justice held that questions it has already answered need not be resubmitted. This brought in a distinctly common law principle into an essentially civil law jurisdiction.
The former Soviet Bloc and other Socialist countries used a Socialist law system.
Much of the Muslim world uses Sharia (also called Islamic law).
Scholarly works.
Lord Chief Justice Edward Coke, a 17th-century English jurist and Member of Parliament, wrote several legal texts that formed the basis for the modern common law, with lawyers in both England and America learning their law from his "Institutes" and "Reports" until the end of the 18th century. His works are still cited by common law courts around the world.
The next definitive historical treatise on the common law is "Commentaries on the Laws of England", written by Sir William Blackstone and first published in 1765–1769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.
While he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes, Jr. published a short volume called "The Common Law", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law "is"; rather, Holmes describes the common law "process". Law professor John Chipman Gray's "The Nature and Sources of the Law", an examination and survey of the common law, is also still commonly read in U.S. law schools.
In the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.
Scots "common law" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called "Institutional Texts" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, "Jus Feudale" (1655) and Stair, "The Institutions of the Law of Scotland" (1681).

</doc>
<doc id="5255" url="http://en.wikipedia.org/wiki?curid=5255" title="Civil law">
Civil law

Civil law may refer to:

</doc>
<doc id="5257" url="http://en.wikipedia.org/wiki?curid=5257" title="Court of appeals (disambiguation)">
Court of appeals (disambiguation)

A court of appeals is an appellate court generally.
Court of Appeals may refer to:

</doc>
<doc id="5259" url="http://en.wikipedia.org/wiki?curid=5259" title="Common descent">
Common descent

In evolutionary biology, a group of organisms share common descent if they have a common ancestor. There is strong evidence that all living organisms on Earth are descended from a common ancestor, called the last universal ancestor or LUA (or last universal common ancestor, LUCA). 
Common ancestry between organisms of different species arises during speciation, in which new species are established from a single ancestral population. Organisms which share a more recent common ancestor are more closely related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage (universal common descent), with each being the descendant from a single original species, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about monophyly of life. 
Universal common descent through an evolutionary process, that there was only one progenitor for all life forms, was first proposed by Charles Darwin in "On the Origin of Species", which ended with "There is a grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one". The theory has been recently popularized by Richard Dawkins, in "The Ancestor's Tale", and others.
History.
In the 1740s, Pierre-Louis Moreau de Maupertuis made the first known suggestion in a series of essays that all organisms may have had a common ancestor, and that they had diverged through random variation and natural selection. In "Essai de Cosmologie", Maupertuis noted:
Could one not say that, in the fortuitous combinations of the productions of nature, as there must be some characterized by a certain relation of fitness which are able to subsist, it is not to be wondered at that this fitness is present in all the species that are currently in existence? Chance, one would say, produced an innumerable multitude of individuals; a small number found themselves constructed in such a manner that the parts of the animal were able to satisfy its needs; in another infinitely greater number, there was neither fitness nor order: all of these latter have perished. Animals lacking a mouth could not live; others lacking reproductive organs could not perpetuate themselves ... The species we see today are but the smallest part of what blind destiny has produced ...
In 1790, Immanuel Kant wrote in "Kritik der Urtheilskraft" ("Critique of Judgement") that the analogy of animal forms implies a common original type, and thus a common parent.
In 1795, Charles Darwin's grandfather, Erasmus Darwin, asked:
[W]ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?
In 1859, Charles Darwin's "The Origin of Species" was published. The views about common descent expressed therein were that it was possible that there was only one progenitor for all life forms.
"Therefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed." (p 484)
Darwin's famous closing sentence describes the "grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one." (p 490)
Evidence of universal common descent.
Common biochemistry and genetic code.
All known forms of life are based on the same fundamental biochemical organisation: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources, etc. Furthermore, the genetic code (the "translation table" according to which DNA information is translated into proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of the theory of universal common descent. Analysis of the small differences in the genetic code has also provided support for universal common descent. An example would be Cytochrome c which most organisms actually share. A statistical comparison of various alternative hypotheses has shown that universal common ancestry is significantly more probable than models involving multiple origins.
Selectively neutral similarities.
Similarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for the theory of universal common descent.
Such evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code for the same amino acid. Thus, if two species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for a recent common ancestor.
Other similarities.
The universality of many aspects of cellular life is often pointed to as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry, rather than universal common descent and therefore resulted in convergent evolution.
Phylogenetic trees.
Another important piece of evidence is that it is possible to construct detailed phylogenetic trees (that is, "genealogic trees" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010 an analysis of available genetic data, mapping them to phylogenetic trees, gave "firm quantitative support for the unity of life. ...there is now strong quantitative support, by a formal test, for the unity of life. It should be noted, however, the 'formal' test is criticised for not including consideration of convergent evolution, and Theobald has defended the method against this claim.
Traditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.
Illustrations of common descent.
Artificial selection.
Artificial selection demonstrates the diversity that can exist among organisms that share a relatively recent common ancestor. In artificial selection, one species is bred selectively at each generation, allowing only those organisms that exhibit desired characteristics to reproduce. These characteristics become increasingly well-developed in successive generations. Artificial selection was successful long before science discovered the genetic basis. 
Dog breeding.
The diversity of domesticated dogs is an example of the power of artificial selection. All breeds share common ancestry, having descended from wolves. Humans selectively bred them to enhance specific characteristics, such as color and length or body size. This created a range of breeds that include the Chihuahua, Great Dane, Basset Hound, Pug, and Poodle. Wild wolves, which did not undergo artificial selection, are relatively uniform in comparison.
Wild cabbage.
Early farmers cultivated many popular vegetables from the Brassica oleracea (wild cabbage) by artificially selecting for certain attributes. Common vegetables such as cabbage, kale, broccoli, cauliflower, kohlrabi and Brussels sprouts are all descendants of the wild cabbage plant. Brussels sprouts were created by artificially selecting for large bud size. Broccoli was bred by selecting for large flower stalks. Cabbage was created by selecting for short petioles. Kale was bred by selecting for large leaves.
Natural selection.
Natural selection is the evolutionary process by which heritable traits that increase an individual's fitness become more common, and heritable traits that decrease an individual's fitness become less common.
Darwin's finches.
During Charles Darwin's studies on the Galápagos Islands, Darwin observed 13 species of finches that are closely related and differ most markedly in the shape of their beaks. The beak of each species is suited to the food available in its particular environment, suggesting that beak shapes evolved by natural selection. Large beaks were found on the islands where the primary source of food for the finches are nuts and therefore the large beaks allowed the birds to be better equipped for opening the nuts and staying well nourished. Slender beaks were found on the finches which found insects to be the best source of food on the island they inhabited; their slender beaks allowed the birds to be better equipped for pulling out the insects from their tiny hiding places. The finch is also found on the main land and it is thought that they migrated to the islands and began adapting to their environment through natural selection.

</doc>
<doc id="5261" url="http://en.wikipedia.org/wiki?curid=5261" title="Celtic music">
Celtic music

Celtic music is a broad grouping of musical genres that evolved out of the folk musical traditions of the Celtic people of Western Europe. It refers to both orally-transmitted traditional music and recorded music and the styles vary considerably to include everything from "trad" (traditional) music to a wide range of hybrids.
"Celtic music" means two things mainly. First, it is the music of the peoples identifying themselves as Celts. Secondly, it refers to whatever qualities may be unique to the musics of the Celtic Nations. Many notable Celtic musicians such as Alan Stivell and Paddy Moloney claim that the different Celtic musics have much in common. These common melodic practices may be used widely across Celtic Music:
These two latter usage patterns may simply be remnants of formerly widespread melodic practices.
Often, the term "Celtic music" is applied to the music of Ireland and Scotland because both lands have produced well-known distinctive styles which actually have genuine commonality and clear mutual influences. The definition is further complicated by the fact that Irish independence has allowed Ireland to promote 'Celtic' music as a specifically Irish product. However, these are modern geographical references to a people who share a common Celtic ancestry and consequently, a common musical heritage.
These styles are known because of the importance of Irish and Scottish people in the English speaking world, especially in the United States, where they had a profound impact on American music, particularly bluegrass and country music. The music of Wales, Cornwall, the Isle of Man, Brittany, Galicia, Cantabria and Asturias (Spain) and Portugal are also considered Celtic music, the tradition being particularly strong in Brittany, where Celtic festivals large and small take place throughout the year, and in Wales, where the ancient eisteddfod tradition has been revived and flourishes. Additionally, the musics of ethnically Celtic peoples abroad are vibrant, especially in Canada and the United States. In Canada the provinces of Atlantic Canada are known for being a home of Celtic music, most notably on the islands of Newfoundland, Cape Breton and Prince Edward Island. The traditional music of Atlantic Canada is heavily influenced by the Irish, Scottish and Acadian ethnic makeup of much of the region's communities. In some parts of Atlantic Canada, such as Newfoundland, Celtic music is as or more popular than in the old country. Further, some older forms of Celtic music that are rare in Scotland and Ireland today, such as the practice of accompanying a fiddle with a piano, or the Gaelic spinning songs of Cape Breton remain common in the Maritimes. Much of the music of this region is Celtic in nature, but originates in the local area and celebrates the sea, seafaring, fishing and other primary industries.
Divisions.
In "Celtic Music: A Complete Guide", June Skinner Sawyers acknowledges six Celtic nationalities divided into two groups according to their linguistic heritage. The Q-Celtic nationalities are the Irish, Scottish and Manx peoples, while the P-Celtic groups are the Cornish, Bretons and Welsh peoples. Musician Alan Stivell uses a similar dichotomy, between the Gaelic (Irish/Scottish/Manx) and the Brythonic (Breton/Welsh/Cornish) branches, which differentiate "mostly by the extended range (sometimes more than two octaves) of Irish and Scottish melodies and the closed range of Breton and Welsh melodies (often reduced to a half-octave), and by the frequent use of the pure pentatonic scale in Gaelic music."
There is also tremendous variation between "Celtic" regions. Ireland, Scotland, and Brittany have living traditions of language and music, and there has been a recent major revival of interest in Wales, Cornwall and the Isle of Man. Galicia has a Celtic language revival movement to revive the Q-Celtic "Gallaic language" used into Roman times. Most of the Iberian Peninsula had a similar Celtic language in pre-Roman times. A Brythonic language was used in parts of Galicia and Asturias into early Medieval times brought by Britons fleeing the Anglo-Saxon invasions via Brittany. The Romance language currently spoken in Galicia, Galician (Galego) is closely related to the Portuguese language used mainly in Brazil and Portugal. Galician music is claimed to be "Celtic". The same is true of the music of Asturias, Cantabria, and that of Northern Portugal (some say even traditional music from Central Portugal can be labeled Celtic).
Breton artist Alan Stivell was one of the earliest musicians to use the word "Celtic" and "Keltia" in his marketing materials, starting in the early 1960s as part of the worldwide folk music revival of that era with the term quickly catching on with other artists worldwide. Today, the genre is well established and incredibly diverse.
Forms.
There are musical genres and styles specific to each Celtic country, due in part to the influence of individual song traditions and the characteristics of specific languages:
Festivals.
The Celtic music scene involves a large number of music festivals. Some of the most prominent include:
Celtic fusion.
The oldest musical tradition which fits under the label of Celtic fusion originated in the rural American south in the early colonial period and incorporated Scottish, Scots-Irish, Irish,Welsh, English, and African influences. Variously referred to as roots music, American folk music, or old-time music, this tradition has exerted a strong influence on all forms of American music, including country, blues, and rock and roll. In addition to its lasting effects on other genres, it marked the first modern large-scale mixing of musical traditions from multiple ethnic and religious communities within the Celtic diaspora.
In the 1960s several bands put forward modern adaptations of Celtic music pulling influences from several of the Celtic nations at once to create a modern pan-celtic sound. A few of those include bagadoù (Breton pipe bands), Fairport Convention, Pentangle, Steeleye Span and Horslips.
In the 1970s Clannad made their mark initially in the folk and traditional scene, and then subsequently went on to bridge the gap between traditional Celtic and pop music in the 1980s and 1990s, incorporating elements from new-age, smooth jazz, and folk rock. Traces of Clannad's legacy can be heard in the music of many artists, including Enya, Donna Taggart, Altan, Capercaillie, The Corrs, Loreena McKennitt, Anúna, Riverdance and U2. The solo music of Clannad's lead singer, Moya Brennan (often referred to as the First Lady of Celtic Music) has further enhanced this influence.
Later, beginning in 1982 with The Pogues' invention of Celtic folk-punk and Stockton's Wing blend of Irish traditional and Pop, Rock and Reggie, there has been a movement to incorporate Celtic influences into other genres of music. Bands like Flogging Molly, Black 47, Dropkick Murphys, The Young Dubliners, The Tossers introduced a hybrid of Celtic rock, punk, reggae, hardcore and other elements in the 1990s that has become popular with Irish-American youth.
Today there are Celtic-influenced sub genres of virtually every type of popular music including electronica, rock, metal, punk, hip hop, reggae, new-age, Latin, Andean and pop. Collectively these modern interpretations of Celtic music are sometimes referred to as Celtic fusion.
Other modern adaptations.
Outside of America, the first deliberate attempts to create a "Pan-Celtic music" were made by the Breton Taldir Jaffrennou, having translated songs from Ireland, Scotland, and Wales into Breton between the two world wars. One of his major works was to bring "Hen Wlad Fy Nhadau" (the Welsh national anthem) back in Brittany and create lyrics in Breton. Eventually this song became "Bro goz va zadoù" ("Old land of my fathers") and is the most widely accepted Breton anthem. In the 70s, the Breton Alan Cochevelou (future Alan Stivell) began playing a mixed repertoire from the main Celtic countries on the Celtic harp his father created. 
Probably the most successful all inclusive Celtic music composition in recent years is Shaun Daveys composition 'The Pilgrim'. This suite depicts the journey of St. Colum Cille through the Celtic nations of Ireland, Scotland, the Isle of Man, Wales, Cornwall, Brittany and Galicia. The suite which includes a Scottish pipe band, Irish and Welsh harpists, Galician gaitas, Irish uilleann pipes, the bombardes of Brittany, two vocal soloists and a narrator is set against a background of a classical orchestra and a large choir.
Modern music may also be termed "Celtic" because it is written and recorded in a Celtic language, regardless of musical style. Many of the Celtic languages have experienced resurgences in modern years, spurred on partly by the action of artists and musicians who have embraced them as hallmarks of identity and distinctness. In 1971, the Irish band "Skara Brae" recorded its only LP (simply called "Skara Brae"), all songs in Irish. In 1978 Runrig recorded an album in Scottish Gaelic. In 1992 Capercaillie recorded "A Prince Among Islands", the first Scottish Gaelic language record to reach the UK top 40. In 1996, a song in Breton represented France in the 41st Eurovision Song Contest, the first time in history that France had a song without a word in French. Since about 2005, Oi Polloi (from Scotland) have recorded in Scottish Gaelic. Mill a h-Uile Rud (a Scottish Gaelic punk band from Seattle) recorded in the language in 2004.
Several contemporary bands have Welsh language songs, such as Ceredwen, which fuses traditional instruments with trip-hop beats, the Super Furry Animals, Fernhill, and so on (see the Music of Wales article for more Welsh and Welsh-language bands). The same phenomenon occurs in Brittany, where many singers record songs in Breton, traditional or modern (hip hop, rap, and so on.).

</doc>
<doc id="5267" url="http://en.wikipedia.org/wiki?curid=5267" title="Constellation">
Constellation

In modern astronomy, a constellation is an internationally defined area of the celestial sphere. These areas are grouped around patterns that represent the shapes that give the name to the constellations. When astronomers say an object is "in" a given constellation, they mean it is within the boundaries of one of these defined areas of sky, as the patterns may have several variants in its representation.
There are also numerous historical constellations not recognized by the IAU or constellations recognized in regional traditions of astronomy or astrology, such as Chinese, Hindu and Australian Aboriginal.
Terminology.
The Late Latin term "constellātiō" can be translated as "set with stars".
The term was first used in astrology, of asterisms that supposedly exerted influence, attested in Ammianus (4th century).
In English the term was used from the 14th century, also in astrology, of conjunctions of planets.
The modern astronomical sense of "area of the celestial sphere around a specific asterism" dates to the mid 16th century.
Colloquial usage does not distinguish the senses of "asterism" and "area surrounding an asterism". The modern system of constellations used in astronomy focuses primarily on constellations as grid-like segments of the celestial sphere rather than as patterns, while the term for a star-pattern is asterism. For example, the asterism known as the Big Dipper corresponds to the seven brightest stars of the larger IAU constellation of Ursa Major.
The term circumpolar constellation is used for any constellation that, from a particular latitude on Earth, never sets below the horizon. From the north pole, all constellations north of the celestial equator are circumpolar constellations. In the northern latitudes, the informal term "equatorial constellation" has sometimes been used for constellations that lie to the south of the circumpolar constellations. Depending on the definition, equatorial constellations can include those that lie entirely between declinations 45° north and 45° south, or those that pass overhead between the tropics of Cancer and Capricorn. They generally include all constellations that intersect the celestial equator.
History.
The current list of 88 constellations recognized by the International Astronomical Union since 1922 is based on the 48 listed by Ptolemy in his "Almagest" in the 2nd century.
Ptolemy's catalogue is informed by Eudoxus of Cnidus, a Greek astronomer of the 4th century BC who introduced earlier Babylonian astronomy to the Hellenistic culture. Of the 48 constellations listed by Ptolemy, thirty can be shown to have a much longer history, reaching back into at least the Late Bronze Age. This concerns the zodiacal constellations in particular.
Ancient Near East.
The oldest catalogues of stars and constellations are from Old Babylonian astronomy, beginning in the Middle Bronze Age. The numerous Sumerian names in these catalogues suggest that they build on older, but otherwise unattested, Sumerian traditions of the Early Bronze Age.
The classical Zodiac is a product of a revision of the Old Babylonian system in later Neo-Babylonian astronomy 6th century BC.
Knowledge of the Neo-Babylonian zodiac is also reflected in the Hebrew Bible. E. W. Bullinger interpreted the creatures appearing in the books of Ezekiel (and thence in Revelation) as the middle signs of the four quarters of the Zodiac, with the Lion as Leo, the Bull is Taurus, the Man representing Aquarius and the Eagle standing in for Scorpio.
The biblical Book of Job also makes reference to a number of constellations, including "bier", "fool" and "heap" (Job 9:9, 38:31-32), rendered as "Arcturus, Orion and Pleiades" by the KJV, but "`Ayish" "the bier" actually corresponding to Ursa Major. The term "Mazzaroth" , a "hapax legomenon" in Job 38:32, may be the Hebrew word for the zodiacal constellations.
The Greeks adopted the Babylonian system in the 4th century BC. A total of twenty Ptolemaic constellations are directly continued from the Ancient Near East. Another ten have the same stars but different names.
Ancient Egyptian star charts and astronomical ceilings.
In ancient Egypt, the observation of stars such as Sirius in the day and night sky were used from a very ancient period, in order to predict the Nile Flood. This practical observation of the stars was also associated with a very complex cosmology that involved various gods and spirits, some of whom were associated with stars and heavenly bodies, such as Sothis/Sopdet, who was likely associated with Sirius and Sah who was associated with Orion. This cosmology and practice of astronomy eventually led to the Egyptians producing decanal clocks on coffin lids and star charts featuring their gods and star observations on the ceilings of tombs and temples. Over time these became more complex, featuring various human and anthropomorphic figures representing the planets, stars and various constellations. This tradition was later combined with Greek and Babylonian astronomical systems under the Ptolemies culminating in the Zodiac of Dendera. The first circular zodiac showing all the constellations we are familiar with, along with Egyptian Constellations, Decans and Planets.
Hindu or Indian Constellation.
Nakshatra (Devanagari: "") is the term for lunar mansion in Hindu astrology. A nakshatra is one of 27 (sometimes also 28) sectors along the ecliptic. Their names are related to the most prominent asterisms in the respective sectors.
The starting point for the nakshatras is the point on the ecliptic directly opposite to the star Spica called "Chitrā" in Sanskrit (other slightly different definitions exist). It is called "Meshādi" or the "start of Aries".
The ecliptic is divided into each of the "nakshatra"s eastwards starting from this point.
The number of nakshatras reflects the number of days in a sidereal month (modern value: 27.32 days), the width of a nakshatra traversed by the moon in about one day. Each nakshatra is further subdivided into four quarters (or "pada"s). These play a role in popular Hindu astrology, where each "pada" is associated with a syllable, conventionally chosen as the first syllable of the given name of a child born when the moon was in the corresponding "pada".
The nakshatras of traditional bhartiya astronomy are based on a list of 28 asterisms found in the Atharvaveda (AVŚ 19.7) and also in the "Shatapatha Brahmana". The first astronomical text that lists them is the "Vedanga Jyotisha".
In classical Hindu mythology (Mahabharata, Harivamsa), the creation of the nakshatras is attributed to Daksha. They are personified as daughters of the deity and as mythological wives of Chandra, the moon god, or alternatively the daughters of Kashyapa, the brother of Daksha.
Each of the nakshatras is governed as 'lord' by one of the nine graha in the following sequence: Ketu (South Lunar Node), Shukra (Venus), Ravi or Surya (Sun), Chandra (Moon), Mangala (Mars), Rahu (North Lunar Node), Guru or Brihaspati (Jupiter), Shani (Saturn) and Budha (Mercury). This cycle repeats itself three times to cover all 27 nakshatras. The lord of each nakshatra determines the planetary period known as the dasha, which is considered of major importance in forecasting the life path of the individual in Hindu astrology.
In Vedic Sanskrit, the term "" may refer to any heavenly body, or to "the stars" collectively. The classical sense of "lunar mansion" is first found in the Atharvaveda, and becomes the primary meaning of the term in Classical Sanskrit.
Greece-Roman.
There is only limited information on indigenous Greek constellations. Some evidence is found in Hesiod.
Greek astronomy essentially adopted the older Babylonian system in the Hellenistic era, first introduced to Greece by Eudoxus of Cnidus in the 4th century BC.
The original work of Eudoxus is lost, but it survives as a versification by Aratus, dating to the 3rd century BC.
The most complete existing works dealing with the mythical origins of the constellations are by the Hellenistic writer termed pseudo-Eratosthenes and an early Roman writer styled pseudo-Hyginus.
The basis of western astronomy as taught during Late Antiquity and until the Early Modern period is the "Almagest" by Ptolemy, written in the 2nd century.
Classical Chinese constellations.
In classical Chinese astronomy, the northern sky is divided geometrically, into five "enclosures" and twenty-eight mansions along the ecliptic, grouped into Four Symbols of seven asterisms each.
The 28 lunar mansions are one of the most important and also the most ancient structures in the Chinese sky, attested from the 5th century BC.
Parallels to the earliest Babylonian (Sumerian) star catalogues suggest that the ancient Chinese system did not arise independently from that of the Ancient Near west and East.
Classical Chinese astronomy is recorded in the Han period and appears in the form of three schools, which are attributed to astronomers of the Zhanguo period. The constellations of the three schools were conflated into a single system by Chen Zhuo, an astronomer of the 3rd century (Three Kingdoms period).
Chen Zhuo's work has been lost, but information on his system of constellations survives in Tang period records, notably by Qutan Xida.
The oldest extant Chinese star chart dates to the Tang period and was preserved as part of the Dunhuang Manuscripts. Native Chinese astronomy flourished during the Song Dynasty, and during the Yuan Dynasty became increasingly influenced by medieval Islamic astronomy.
Early Modern era.
The constellations around the South Pole were not observable from north of the equator, by Babylonians, Greeks, Chinese or Arabs.
The modern constellations in this region were defined during the age of exploration, notably by Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman at the end of sixteenth century.
They were depicted by Johann Bayer in his star atlas "Uranometria" of 1603. Several more were created by Nicolas Louis de Lacaille in his star catalogue, published in 1756.
Some modern proposals for new constellations were not successful; an example is Quadrans, eponymous of the Quadrantid meteors, now divided between Boötes and Draco.
The classical constellation of Argo Navis was broken up into several different constellations, for the convenience of stellar cartographers.
By the end of the Ming Dynasty, Xu Guangqi introduced 23 asterisms of the southern sky based on the knowledge of western star charts. These asterisms have since been incorporated into the traditional Chinese star maps.
IAU constellations.
In 1922, Henry Norris Russell aided the IAU (International Astronomical Union) in dividing the celestial sphere into 88 official constellations. Where possible, these modern constellations usually share the names of their Graeco-Roman predecessors, such as Orion, Leo or Scorpius.
The aim of this system is area-mapping, i.e. the division of the celestial sphere into contiguous fields.
Out of the 88 modern constellations, 36 lie predominantly in the northern sky, and the other 52 predominantly in the southern.
In 1930, the boundaries between the 88 constellations were devised by Eugène Delporte along vertical and horizontal lines of right ascension and declination. However, the data he used originated back to epoch B1875.0, which was when Benjamin A. Gould first made the proposal to designate boundaries for the celestial sphere, a suggestion upon which Delporte would base his work. The consequence of this early date is that due to the precession of the equinoxes, the borders on a modern star map, such as epoch J2000, are already somewhat skewed and no longer perfectly vertical or horizontal. This effect will increase over the years and centuries to come.
Asterisms.
The stars of the main asterism within a constellation are usually given Greek letters in their order of brightness, the so-called Bayer designation introduced by Johann Bayer in 1603.
A total of 1,564 stars are so identified, out of approximately 10,000 stars visible to the naked eye.
The brightest stars, usually the stars that make up the constellation's eponymous asterism, also retain proper names, often from Arabic.
For example, the "Little Dipper" asterism of the constellation Ursa Minor has ten stars with Bayer designation, α UMi to π UMi. Of these ten stars, seven have a proper name, viz.
Polaris (α UMi), Kochab (β UMi), Pherkad (γ UMi), Yildun (δ UMi), Urodelus (ε UMi), Ahfa al Farkadain (ζ UMi) and Anwar al Farkadain (η UMi).
The stars within an asterism rarely have any substantial astrophysical relationship to each other, and their apparent proximity when viewed from Earth disguises the fact that they are far apart, some being much farther from Earth than others. However, there are some exceptions: many of the stars in the constellation of Ursa Major (including most of the Big Dipper) are genuinely close to one another, travel through the galaxy with similar velocities, and are likely to have formed together as part of a cluster that is slowly dispersing. These stars form the Ursa Major moving group.
Dark cloud constellations.
The Great Rift, a series of dark patches in the Milky Way, is more visible and striking in the southern hemisphere than in the northern. It vividly stands out when conditions are otherwise so dark that the Milky Way's central region casts shadows on the ground. Some cultures have discerned shapes in these patches and have given names to these "dark cloud constellations." Members of the Inca civilization identified various dark areas or dark nebulae in the Milky Way as animals, and associated their appearance with the seasonal rains. Australian Aboriginal astronomy also describes dark cloud constellations, the most famous being the "emu in the sky" whose head is formed by the Coalsack.
Further reading.
Atlases and celestial maps.
"General & Nonspecialized – Entire Celestial Heavens":
"Northern Celestial Hemisphere & North Circumpolar Region":
"Equatorial, Ecliptic, & Zodiacal Celestial Sky":
"Southern Celestial Hemisphere & South Circumpolar Region":

</doc>
<doc id="5269" url="http://en.wikipedia.org/wiki?curid=5269" title="Character">
Character

Character(s) may refer to:

</doc>
<doc id="5270" url="http://en.wikipedia.org/wiki?curid=5270" title="Car (disambiguation)">
Car (disambiguation)

A car is a wheeled motor vehicle used for transporting passengers.
Car or CAR may also refer to:

</doc>
<doc id="5272" url="http://en.wikipedia.org/wiki?curid=5272" title="Printer (computing)">
Printer (computing)

In computing, a printer is a peripheral which makes a persistent human-readable representation of graphics or text on paper or similar physical media. The two most common printer mechanisms are black and white laser printers used for common documents, and color ink jet printers which can produced high-quality photograph-quality output.
The world's first computer printer was a 19th-century mechanically driven apparatus invented by Charles Babbage for his difference engine. This system used a series of metal rods with characters printed on them and stuck a roll of paper against the rods to print the characters. The first commercial printers generally used mechanisms from electric typewriters and teletype machines, which operated in a similar fashion. The demand for higher speed led to the development of new systems specifically for computer use. Among the systems widely used through the 1980s were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high-quality line art like blueprints.
The introduction of the low-cost laser printer in 1984 with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter, set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as laser printer in terms of flexibility, but produced somewhat lower quality output (depending on the paper) from much less expensive mechanisms. Inkjet systems rapidly displaced dot matrix and daisy wheel printers from the market. By the 2000s high-quality printers of this sort had fallen under the $100 price point and became commonplace.
The rapid update of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a "physical backup" is of little benefit today. Even the desire for printed output for "offline reading" while on mass transit or aircraft has been displaced by e-book readers and tablet computers. Today, traditional printers are being used more for special purposes, like printing photographs or artwork, and are no longer a must-have peripheral.
Starting around 2010, 3D printing has become an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. These devices are in their earliest stages of development and have not yet become commonplace.
Types of printers.
"Personal" printers are primarily designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.
"Networked" or "shared" printers are "designed for high-volume, high-speed printing." They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.
A "virtual printer" is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer.
A "3D printer" is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.
Technology.
The choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies don't work with certain types of physical media, such as carbon paper or transparencies.
A second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.
Cheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.
Modern print technology.
The following printing technologies are routinely found in modern printers:
Toner-based printers.
A laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.
Another toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.
Liquid inkjet printers.
Inkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.
Solid ink printers.
Solid ink printers, also known as phase-change printers, are a type of thermal transfer printer. They use solid sticks of CMYK-coloured ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. The printhead sprays the ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as colour office printers, and are excellent at printing on transparencies and other non-porous media. Solid ink printers can produce excellent results. Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. In addition, this type of printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tek sold the printing business to Xerox in 2001.
Dye-sublimation printers.
A dye-sublimation printer (or dye-sub printer) is a printer which employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper or canvas. The process is usually to lay one colour at a time using a ribbon that has colour panels. Dye-sub printers are intended primarily for high-quality colour applications, including colour photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.
Inkless printers.
Thermal printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colours can be achieved with special papers and different temperatures and heating rates for different colours; these coloured sheets are not required in black-and-white output. One example is the ZINK technology (Zero INK Technology).
Obsolete and special-purpose printing technologies.
The following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.
Impact printers.
Impact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example). All but the dot matrix printer rely on the use of "fully formed characters", letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by "overstriking", that is, printing two or more impressions in the same character position. Impact printers varieties include, typewriter-derived printers, teletypewriter-derived printers, daisy wheel printers, dot matrix printers and line printers. Dot matrix printers remain in common use in businesses where multi-part forms are printed, such as car rental services. "An overview of impact printing" contains a detailed description of many of the technologies used.
Typewriter-derived printers.
Several different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known "golf ball" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.
Teletypewriter-derived printers.
The common teleprinter could easily be interfaced to the computer and became very popular except for those computers manufactured by IBM. Some models used a "typebox" that was positioned, in the X- and Y-axes, by a mechanism and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.
Daisy wheel printers.
Daisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the "daisy wheel", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as "letter-quality printers" because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.
Dot-matrix printers.
The term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms ("type").
Dot-matrix printers can be broadly divided into two major classes:
Dot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.
In the 1970s & 80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). 24-pin print heads were able to print at a higher quality. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favor for general use.
Some dot matrix printers, such as the NEC P6300, can be upgraded to print in colour. This is achieved through the use of a four-colour ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Colour graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, colour graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.
Dot matrix printers are still commonly used in low-cost, low-quality applications like cash registers, or in demanding, very high volume applications like invoice printing. The fact that they use an impact printing method allows them to be used to print multi-part documents using carbonless copy paper, like sales invoices and credit card receipts, whereas other printing methods are unusable with paper of this type. Dot-matrix printers are now (as of 2005) rapidly being superseded even as receipt printers.
Line printers.
Line printers, as the name implies, print an entire line of text at a time. Four principal designs existed. 
In each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper.
Line printers were the fastest of all impact printers and were used for bulk printing in large computer centres. A line printer could print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operated with tight tolerances and required regular preventive maintenance (PM) to produce top quality print. They were virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many computer operating systems, which use the abbreviations "lp", "lpr", or "LPT" to refer to printers.
Liquid ink electrostatic printers.
Liquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than ±0.1%. (All laser printers have an accuracy of ±1%.)
Worldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.
Plotters.
Pen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology. Some plotters used roll-fed paper, and therefore had minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.
Other printers.
A number of other sorts of printers are important for historical reasons, or for special purpose uses:
Attributes.
Printer control languages.
Most printers other than line printers accept control characters or unique character sequences to control various printer functions. These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set. The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers. 
Today, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.
Printing speed.
The speed of early printers was measured in units of "characters per minute" (cpm) for character printers, or "lines per minute" (lpm) for line printers. Modern printers are measured in "pages per minute" (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially colour images. PPM are most of the time referring to A4 paper in Europe and letter paper in the United States, resulting in a 5-10% difference.
Sales.
Since 2005, the world's top selling brand of inkjet and laser printers has been HP, which now has 46% of sales in inkjet and 55.5% in laser printers.
Printing mode.
The data received by a printer may be:
Some printers can process all four types of data, others not.
Today it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.
Monochrome, colour and photo printers.
A monochrome printer can only produce an image consisting of one colour, usually black. A monochrome printer may also be able to produce various tones of that color, such as a grey-scale. A colour printer can produce images of multiple colours. A photo printer is a colour printer that can produce images that mimic the colour range (gamut) and resolution of prints made from photographic film. Many can be used on a standalone basis without a computer, using a memory card or USB connector.
Business model.
Often the "razor and blades" business model is applied. That is, a company may sell a printer at cost, and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.
Other manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on the ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: "cheap printer – expensive ink" or "expensive printer – cheap ink". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.
Printer steganography.
Printer steganography is a type of steganography – "hiding data within data" – produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.
Wireless printers.
More than half of all printers sold at U.S. retail in 2010 were wireless-capable, but nearly three-quarters of consumers who have access to those printers weren't taking advantage of the increased access to print from multiple devices according to the new Wireless Printing Study.

</doc>
<doc id="5278" url="http://en.wikipedia.org/wiki?curid=5278" title="Copyright">
Copyright

Copyright is a legal right created by the law of a country, that grants the creator of an original work exclusive rights to its use and distribution, usually for a limited time, with the intention of enabling the creator (e.g. the photographer of a photograph or the author of a book) to receive compensation for their intellectual effort.
Copyright is a form of intellectual property (as patents, trademarks and trade secrets are), applicable to any expressible form of an idea or information that is substantive and discrete.
It is often shared, then percentage holders are commonly called rightsholders: legally, contractually and in associated "rights" business functions.
Generally rightsholders have "the right to copy", but also the right to be credited for the work, to determine who may adapt the work to other forms, who may perform the work, who may financially benefit from it, and other related rights.
Copyright initially was conceived as a way for government to restrict printing; the contemporary intent of copyright is to promote the creation of new works by giving authors control of and profit from them. Copyrights are said to be territorial, which means that they do not extend beyond the territory of a specific state unless that state is a party to an international agreement. Today, however, this is less relevant since most countries are parties to at least one such agreement. While many aspects of national copyright laws have been standardized through international copyright agreements, copyright laws of most countries have some unique features.
Typically, the duration of copyright is the whole life of the creator plus fifty to a hundred years from the creator's death, or a finite period for anonymous or corporate creations. Some jurisdictions have required formalities to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a civil matter, though some jurisdictions do apply criminal sanctions.
Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright, and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of their intellectual property rights, and sought additional legal and technological enforcement.
Justification.
The usual justification of copyright is to enable creators of intellectual wealth to financially support themselves and give them a motive to continue publishing their creations. Without copyright, artists, authors, journalists, photographers, or anyone else who creates non-material economic wealth would have to find another way to support themselves. For example, they could publish a small subset of their creations and then request payment before they published more (also see the street performer protocol). In a capitalist economic system, food and housing cost money; so authors, directors, painters, photographers, poets and other creators must find other jobs to support themselves if they can't get compensated for their creative work. With copyright in place, the author of a book or the photographer of a photograph can charge users who want to get a copy of their creations and thus support themselves. Before copyright, authors generally requested a large-sum one-off payment from the printer of their book before publishing it. With copyright in place, and assuming efficient enforcement, authors, photographers and other intellectual workers can publish their creations immediately and wait for licensing requests from people who want to use or re-publish their works. Examples of this model for funding photography are Alamy, Corbis, Getty Images, and other stock photography image banks).
Anti-copyright critics claim copyright law protects corporate interests while criminalizing legitimate uses, while proponents argue the law is fair and just, protecting the interest of the creator.
History.
Copyright came about with the invention of the printing press and with wider public literacy. As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th century. Charles II of England was concerned by the unregulated copying of books and passed the Licensing of the Press Act 1662 by Act of Parliament, which established a register of licensed books and required a copy to be deposited with the Stationers' Company, essentially continuing the licensing of material that had long been in effect.
The British Statute of Anne (1710) further alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing... Books, and other Writings, without the Consent of the Authors... to their very great Detriment, and too often to the Ruin of them and their Families:". A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.
Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia. In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per se.
The Statute of Anne was the first real copyright act, and gave the publishers rights for a fixed period, after which the copyright expired. Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as sound recordings, films, photographs, software, and architectural works.
Prior to the passage of the United States Constitution, several states passed their own copyright laws between 1783 and 1787, the first being Connecticut. Contemporary scholars and patriots such as Noah Webster, John Trumbull (poet), and Joel Barlow were instrumental in securing the passage of these statutes. 
The Copyright Clause of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could "profit" from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.
The original length of copyright in the United States was 14 years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the public domain, so it could be used and built upon by others.
Thomas Jefferson, who strongly advocated the ability of the public to share and build upon the works of others, proposed as part of the Bill of Rights that a short timespan be protected:
The 1886 Berne Convention first established recognition of copyrights among sovereign nations, rather than merely bilaterally. Under the Berne Convention, copyrights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention. As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100 years later with the passage of the "Copyright, Designs and Patents Act of 1988". The United States did not sign the Berne Convention until 1989.
The United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as "all rights reserved"), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms. The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.
The regulations of the Berne Convention are incorporated into the World Trade Organization's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application. The 2002 WIPO Copyright Treaty enacted greater restrictions on the use of technology to copy works in the nations that ratified it.
Scope.
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by jurisdiction, but these can include poems, theses, plays and other literary works, motion pictures, choreography, musical compositions, sound recordings, paintings, drawings, sculptures, photographs, computer software, radio and television broadcasts, and industrial designs. Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.
Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed. For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's. Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, Steamboat Willie is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection. 
In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses (See fair use, fair dealing). It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas). However there are protections available for those areas copyright does not cover – such as trademarks and patents.
Copyright laws are standardized somewhat through international conventions such as the Berne Convention and Universal Copyright Convention. These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union or World Trade Organization require their member states to comply with them.
Obtaining and enforcing copyright.
Typically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some 'skill, labour, and judgment' that has gone into it. In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a trademark instead.
Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.
In all countries where the Berne Convention standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights. However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as "prima facie" evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees. (In the USA, registering "after" an infringement only enables one to receive actual damages and lost profits.)
The original holder of the copyright may be the "employer" of the author rather than the author himself, if the work is a "work for hire". For example, in English law the "Copyright, Designs and Patents Act" 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire."
Copyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions. While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: Legal aspects of file sharing)
Cost of enforcing copyright.
In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative and or court costs. These costs, including time, should be taken into consideration when evaluating the benefits of enforcing copyright. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.
Copyright notices in the United States.
Before 1989, the use of a copyright notice – consisting of the copyright symbol (, the letter C inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder – was part of U. S. statutory requirements. Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (, the letter P inside a circle), which indicates a sound recording copyright, with the letter P indicating a "phonorecord". Similarly, the phrase "All rights reserved" was once required to assert copyright.
In 1989 the United States enacted the Berne Convention Implementation Act, amending the 1976 Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic. However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit – using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.
"Poor man's copyright".
A widely circulated strategy to avoid the cost of copyright registration is referred to as the "poor man's copyright". It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the postmark to establish the date. This technique has not been recognized in any published opinions of the United States courts. The United States Copyright Office makes it clear that the technique is no substitute for actual registration. The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is. 
Exclusive rights.
Several exclusive rights typically attach to the holder of a copyright:
The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to "prohibit" certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than "permitting" people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the unregistered design right in English law and European law. The rights of the copyright holder also permit him/her to "not" use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared. There is also debate on whether copyright should be considered a property right or a moral right.
Useful articles.
If a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted.
Limitations and exceptions to copyright.
Idea–expression dichotomy and the merger doctrine.
The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of Baker v. Selden, has since been codified by the Copyright Act of 1976 at 17 U.S.C. § 102(b).
The first-sale doctrine and exhaustion of rights.
Copyright law does "not" restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or CD. In the United States this is known as the first-sale doctrine, and was established by the courts to clarify the legality of reselling books in second-hand bookstores. Some countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.
In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement moral rights, a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.
Fair use and fair dealing.
Copyright does not prohibit all copying or replication. In the United States, the fair use doctrine, codified by the Copyright Act of 1976 as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:
In the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation. The concept is sometimes not well defined; however in Canada, private copying for personal use has been expressly permitted by statute since 1999. In "Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)", 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the "Copyright Act 1968" (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. legal advice). Under current Australian law it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.
In the United States the AHRA (Audio Home Recording Act Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.
Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The Digital Millennium Copyright Act prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner. An appellate court has held that fair use is not a defense to engaging in such distribution.
Accessible copies.
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.
Transfer and licensing, and assignment.
A copyright, or aspects of it, may be assigned or transferred from one party to another. For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the Internet, however the record industry attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy and/or distribute the work in a particular region or for a specified period of time.
A transfer or licence may have to meet particular formal requirements in order to be effective, for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under U.S. law. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.
Copyright may also be licensed. Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g. musical works in the United States used for radio broadcast or performance). This is also called a compulsory license, because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made. Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP, BMI, and SESAC) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.
Free licences.
There are a large number of free licenses, where users are granted several rights, for example those mention in the Free Software Definition, Open Source Definition, Debian Free Software Guidelines or Definition of Free Cultural Works. Examples of free licences are the GNU General Public License, BSD license and some Creative Commons licenses.
Founded in 2001, by James Boyle, Lawrence Lessig, and Hal Abelson the Creative Commons (CC) is a non-profit organization which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of copyright license options to the public, free of charge. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.
Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available. These are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work. As of 2009 approximately 130 million individuals had received such licenses.
Duration.
Copyright term.
Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been published, and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States and the United Kingdom), copyrights expire at the end of the calendar year in question.
The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.
In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain. In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain, except that books originally published outside the US by non-Americans are exempt from this requirement, if they are still under copyright in their home country.
But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.
In 1998 the length of a copyright in the United States was increased by 20 years under the Copyright Term Extension Act. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.
As a curiosity, the famous work "Peter Pan, or The Boy Who Wouldn't Grow Up" has a complex – and disputed – story of copyright expiry.
Public domain.
Copyright, like other intellectual property rights, is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright. Public domain works should not be confused with works that are publicly available. Works posted in the internet, for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.
Copyright infringement.
The illegitimate use of materials held by copyright is typically referred to as piracy. For a work to be considered pirated, its illegitimate use must have occurred in a nation that has domestic copyright laws and/or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty. Improper use of materials outside of this legislation is deemed "unauthorized edition", not piracy.
Piracy primarily targets software, film and music. However, the illegal copying of books and other text works remains common, especially for educational reasons. Statistics regarding the effects of piracy are difficult to determine. Studies have attempted to estimate a monetary loss for industries affected by piracy by predicting what portion of pirated works would have been formally purchased if they had not been freely available. Estimates in 2007 stated 18.2 billion potential losses in consumer dollars lost as a result of piracy activities in the United States. International estimates suggest losses in the billions throughout the last decade. However other reports indicate that piracy does not have an adverse effect on the entertainment industry.
A 2014 University study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.

</doc>
<doc id="5282" url="http://en.wikipedia.org/wiki?curid=5282" title="Catalan language">
Catalan language

Catalan (; autonym: "català" or ) is a Romance language named for its origins in Catalonia, in what is northeastern Spain and adjoining parts of France. It is the national and only official language of Andorra, and a co-official language of the Spanish autonomous communities of Catalonia, the Balearic Islands, and the Valencian Community (where the language is known as Valencian, and there exist regional standards). It also has semi-official status in the city of Alghero on the Italian island of Sardinia. It is also spoken with no official recognition in parts of the Spanish autonomous communities of Aragon (La Franja) and Murcia (Carche), and in the historic French region of Roussillon/Northern Catalonia, roughly equivalent to the department of Pyrénées-Orientales.
According to the Statistical Institute of Catalonia in 2008 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language. The Generalitat of Catalunya spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories.
Catalan evolved from Vulgar Latin around the eastern Pyrenees in the 9th century. During the Low Middle Ages it saw a golden age as the literary and dominant language of the Crown of Aragon, and was widely used all over the Mediterranean. The union of Aragon with the other territories of Spain in 1479 marked the start of the decline of the language. In 1659 Spain ceded Northern Catalonia to France, and Catalan was banned in both states in the early 18th century. 19th-century Spain saw a Catalan literary revival, which culminated in the 1913 orthographic standardization, and the officialization of the language during the Second Spanish Republic (1931–39). However, the Francoist dictatorship (1939–75) banned the language again.
Since the Spanish transition to democracy (1975–1982), Catalan has been recognized as an official language, language of education, and language of mass media, all of which have contributed to its increased prestige. There is no parallel in Europe of such a large, bilingual, non-state speech community.
Catalan dialects are relatively uniform, and are mutually intelligible. They are divided into two blocks, Eastern and Western, differing mostly in pronunciation. The terms "Catalan" and "Valencian" (respectively used in Catalonia and the Valencian Community) are two names for different dialects of the same language. Standard Catalan, a variety accepted by virtually all speakers (though considered strange-sounding in the Valencian Community, especially because of the phonological merger of unstressed /e/ and /a/), is regulated by the Institute of Catalan Studies (IEC).
Catalan shares many traits with its neighboring Romance languages. However, despite being mostly situated in the Iberian Peninsula, Catalan is more different from Ibero-Romance (Spanish, Portuguese) in terms of vocabulary, pronunciation, and grammar than from Gallo-Romance (French, Italian, Occitan, etc.). These similarities are most notable with Occitan.
Catalan has an inflectional grammar, with two genders (masculine, feminine), and two numbers (singular, plural). Pronouns are also inflected for case, animacy and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages.
Etymology and pronunciation.
The word "Catalan" derives from the territory of Catalonia, itself of disputed etymology. The main theory suggests that "Catalunya" (Latin "Gathia Launia") derives from the name "Gothia" or "Gauthia" ("Land of the Goths"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence "Gothland" > "Gothlandia" > "Gothalania" > "Catalonia" theoretically derived.
In English, the term referring to a person first appears in mid 14th century as "Catelaner", followed in the 15th century as "Catellain" (from French). It is first attested a language name by 1792. The term "Catalonian" is first attested in 1707. "Catalan" can be pronounced as , or .
The endonym is pronounced in the Eastern Catalan dialects, and in the Western dialects. In the Valencian Community, the term "valencià" () is frequently used instead. The names "Catalan" and "Valencian" are two names for the same language. See also status of Valencian below.
History.
Middle Ages.
By the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988.
In the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries.
During the 11th and 12th centuries the Catalan rulers expanded up to north of the Ebro river, and in the 13th century they conquered the Land of Valencia and the Balearic Islands. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century.
In the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232–1315), the Four Great Chronicles (13th–14th centuries), and the Valencian school of poetry culminating in Ausiàs March (1397–1459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms "one of the 'great languages' of medieval Europe".
Martorell's outstanding novel of chivalry "Tirant lo Blanc" (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan.
Start of the modern era.
With the union of the crowns of Castille and Aragon (1479), the use of Spanish gradually became more prestigious. Starting in the 16th century, Catalan literature experienced a decline, the language came under the influence of Spanish, and the urban and literary classes became bilingual.
French state: 18th to 20th centuries.
With the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region.
Shortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the nonstandard languages of France ("patois"), such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque.
Following the French capture of Algeria (1833), that region saw several waves of Catalan-speaking settlers. People from the Spanish Alacant province settled around Oran, whereas Algiers received immigration from Northern Catalonia and Minorca. Their speech was known as "patuet". By 1911, the number of Catalan speakers was around 100,000. After the declaration of independence of Algeria in 1962, almost all the Catalan speakers fled to Northern Catalonia (as "Pieds-Noirs") or Alacant.
Nowadays, France only recognizes French as an official language. Nevertheless, on 10 December 2007, the General Council of the Pyrénées-Orientales officially recognized Catalan as one of the languages of the department and seeks to further promote it in public life and education.
Spanish state: 18th to 20th centuries.
The decline of Catalan continued in the 16th and 17th centuries. The Catalan defeat in the War of Spanish Succession (1714) initiated a series of measures imposing the use of Spanish in legal documentation.
In parallel, however, the 19th century saw a Catalan literary revival ("Renaixença"), which has continued up to the present day. This period starts with Aribau's "Ode to the Homeland" (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimerà (drama).
In the 19th century, the region of Carche, in the province of Murcia was repopulated with Catalan speakers from the Land of Valencia.
The Second Spanish Republic (1931–1939) saw a brief period of tolerance, with most restrictions against Catalan being lifted. However, the establishment of the Francoist dictatorship lead to a total ban on the language in 1940. Despite some gradual relaxations allowing the publication of some books and magazines, Catalan was excluded from all public institutions until the adoption of the 1978 constitution.
Present day.
Since the Spanish transition to democracy (1975–1982), Catalan has been institutionalizated as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is no parallel of a large, bilingual, European, non-state speech community. The teaching of Catalan is mandatory in all schools, and it is not possible to use Spanish for studying in the public education system of Catalonia.
 There is also some intergenerational shift towards Catalan.
In Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several Andorranization policies have been enforced, like Catalan medium education.
On the other hand, there are several language shift processes currently taking place. In Northern Catalonia, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association "La Bressola" promotes a network of community-run schools engaged in Catalan language immersion programs.
In the Alicante province Catalan is being replaced by Spanish, and in Alghero by Italian. There are also well ingrained diglossic attitudes against Catalan in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands.
Classification and relationship with other Romance languages.
The ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists, particularly those from the Castillian sphere, like Menéndez Pidal.
According to Pèire Bèc, its specific classification is as follows:
Catalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term "Occitan language" (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages.
Relationship with other Western Romance languages.
Catalan shares many traits with the other neighboring Romance languages (Italian, Sardinian, Occitan, and Spanish). However, despite being mostly situated in the Iberian Peninsula, Catalan has marked differences with the Ibero-Romance group (Spanish and Portuguese) in terms of pronunciation, grammatical, and especially vocabulary; showing instead its closest affinity with Occitan and to a lesser extent Gallo-Romance (French, Franco-Provençal, Gallo-Italian).
According to Ethnologue, the lexical similarity between Catalan and other Romance languages is 87% with Italian, 85% with Portuguese and Spanish, 76% with Ladin, 75% with Sardinian, and 73% with Romanian.
During much of its history, and especially during the Francoist dictatorship (1939–1975), the Catalan language has often been degraded as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in a number of respects closer to Occitan (and French).
There is evidence that, at least from the 2nd century, the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation has arisen generally because Spanish, Asturian, and Galico-Portuguese share certain peripheral archaisms (Spanish "hervir", Asturian/Portuguese "ferver" vs. Catalan "bullir", Occitan "bolir" "to boil") and innovatory regionalisms (Sp "novillo", Ast "nuviellu" vs. Cat "torell", Oc "taurèl" "bullock"), while Catalan has a shared history with the Western Romance innovative core, especially Occitan.
The Germanic superstrate has had different outcomes in Spanish and Catalan. For example, Catalan "fang" "mud" and "rostir" "to roast", of Germanic origin, contrast with Spanish "lodo" and "asar", of Latin origin; whereas Catalan "filosa" "spinning wheel" and "pols" "temple", of Latin origin, contrast with Spanish "rueca" and "sien", of Germanic origin.
The same happens with Arabic loanwords. Thus, Catalan "alfàbia" "large earthenware jar" and "rajola" "tile", of Arabic origin, contrast with Spanish "tinaja" and "teja", of Latin origin; whereas Catalan "oli" "oil" and "oliva" "olive", of Latin origin, contrast with Spanish "aceite" and "aceituna". However, the Arabic element in Spanish is generally much more prevalent.
Situated between two large linguistic blocks (Ibero-Romance and Gallo-Romance), Catalan has many unique lexical choices, such as "enyorar" "to miss somebody", "apaivagar" "to calm down somebody", or "rebutjar" "reject".
Geographic distribution.
Catalan-speaking territories.
These territories are sometimes referred to as the "Països Catalans" (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions.
Number of speakers.
The number of people known to be fluent in Catalan varies depending on the sources used. A 2004 study did not count the total number of speakers, but estimated a total of 9–9.5 million by matching the percentage of speakers to the population of each area where Catalan is spoken. The web site of the Generalitat de Catalunya estimated that as of 2004 there were 9,118,882 speakers of Catalan. These figures only reflect potential speakers; today it is the native language of only 35.6% of the Catalan population. According to "Ethnologue: Languages of the World", ed. 17, Catalan had a total of 7.2 million native speakers in 2010, and 5 million second-language speakers in 1994.
The most important social characteristic of the Catalan language is that all the areas where it is spoken are bilingual in practice: together with the French language in Roussillon, with Italian in Alghero, with Spanish and French in Andorra and with Spanish in the rest of the territories.
Level of knowledge of the Catalan language.
(% of the population 15 year old and older).
Social use.
(% of the population 15 year old and older).
Phonology.
The Catalan phonology varies depending on the dialect. Notable features include:
In contrast with other Romance languages, Catalan has many monosyllabic words; and those ending in a wide variety consonants and some consonant clusters. Also, Catalan has final obstruent devoicing, thus featuring many couplets like "amic" "(male friend") vs. "amiga" ("female friend").
Central Catalan is considered the standard pronunciation of the language. The descriptions below are mostly for this variety. For the differences in pronunciation of the different dialects, see the section pronunciation of dialects in this article.
Vowels.
Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /ə/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /ɛ e/.
In Central Catalan, unstressed vowels reduce to three: ; ; remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article).
Consonants.
The consonant system of Catalan is rather conservative, shared with most modern Western Romance languages.
Dialects.
Overview.
The dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Alguerese dialect.
Catalan is split in two major dialectal blocks: Eastern Catalan, and Western Catalan. The main difference lies in the treatment of unstressed "a" and "e"; which have merged to /ə/ in Eastern dialects, but which remain distinct as /a/ and /e/ in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary.
Western Catalan comprises the two dialects of North-Western Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Rossellonese, and Alguerese. Each dialect can be further subdivided in several subdialects.
Central Catalan is considered the standard pronunciation of the language and has the highest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona.
Pronunciation.
Vowels.
Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /ə/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /ɛ e/.
In Eastern Catalan (except Majorcan), unstressed vowels reduce to three: ; ; remains distinct. There are a few instances of unreduced [e], [o] in some words. Alguerese has lowered [ə] to [a].
In Majorcan, unstressed vowels reduce to four: follow the Eastern Catalan reduction pattern; however reduce to , with remaining distinct, as in Western Catalan.
In Western Catalan, unstressed vowels reduce to five: ; ; remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases.
Central, Western, and Balearic differ in the lexical incidence of stressed /e/ and /ɛ/. Usually, words with /ɛ/ in Central Catalan correspond to /ə/ in Balearic and /e/ in Western Catalan. Words with /e/ in Balearic almost always have /e/ in Central and Western Catalan as well. As a result, Central Catalan has a much higher incidence of /e/.
Morphology.
In verbs, 1st person present indicative desinence is -"e" (∅ in verbs of the 2nd and 3rd conjugation), or -"o".<br>E.g. "parle", "tem", "sent" (Valencian); "parlo", "temo", "sento" (North-Western). In verbs, 1st person present indicative desinence is -"o", -"i" or ∅ in all conjugations. <br>E.g. "parlo" (Central), "parl" (Balearic), "parli" (Northern), ('I speak').
In verbs, the inchoative desinences are -"isc"/-"ixo", -"ix", -"ixen", -"isca".
In verbs, the inchoative desinences are -"eixo", -"eix", -"eixen", -"eixi".
In nouns and adjectives, maintenance of of medieval plurals in proparoxytone words.<br>E.g. "hòmens" 'men', "jóvens" 'youth'.
In nouns and adjectives, loss of of medieval plurals in proparoxytone words.<br>E.g. "homes" 'men', "joves" 'youth'.
Vocabulary.
Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.
Standards.
Standard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of Valencia and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia.
The most notable difference between both standards is some tonic accentuation, for instance: "francès, anglès" (IEC) – "francés, anglés" (AVL). Nevertheless, AVL's standard keeps the grave accent , without pronouncing this as , in some words like: "què" ('what'), or "València". Other divergences include the use of (AVL) in some words instead of like in "ametla"/"ametlla" ('almond'), "espatla"/"espatlla" ('back'), the use of elided demonstratives ("este" 'this', "eixe" 'that') in the same level as reinforced ones ("aquest, aqueix") or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, like subjunctive mood or inchoative conjugation in -"ix"- at the same level as -"eix"- or the priority use of -"e" morpheme in 1st person singular in present indicative (-"ar" verbs): "jo compre" instead of "jo compro" ('I buy').
In the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing "cantam" as much as "cantem" ('we sing') but the University says that the priority form in the Balearic Islands must be "cantam" in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: "jo compr" ('I buy'), "jo tem" ('I fear'), "jo dorm" ('I sleep').
In Alghero, the IEC has adapted its standard to the Alguerese dialect. In this standard one can find, among other features: the definite article "lo" instead of "el", special possessive pronouns and determinants "la mia" ('mine'), "lo sou/la sua" ('his/her'), "lo tou/la tua" ('yours'), and so on, the use of "-v-" in the imperfect tense in all conjugations: "cantava", "creixiva", "llegiva"; the use of many archaic words, usual words in Alguerese: "manco" instead of "menys" ('less'), "calqui u" instead of "algú" ('someone'), "qual/quala" instead of "quin/quina" ('which'), and so on; and the adaptation of weak pronouns.
In 2011, the Aragonese government passed a decree for the establishment of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon). The new entity, designated as "Acadèmia Aragonesa del Català", shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja.
Status of Valencian.
The vast majority of linguists hold that Catalan and Valencian are the same language, a position that is also shared by the majority of Valencian scholars. The official regulating body of the language of the Valencian Community, the Academy of the Valencian Language ("Acadèmia Valenciana de la Llengua," AVL) also declares the linguistic unity between Valencian and Catalan.
Valencian is classified as a Western dialect, along with the North-Western varieties spoken in Western Catalonia (provinces of Lleida and most of Tarragona). The various forms of Catalan and Valencian are mutually intelligible (ranging from 90% to 95%)
The AVL, created by the Valencian government, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on that of the Institute of Catalan Studies ("Institut d'Estudis Catalans", IEC). Currently, the majority of people who write in Valencian use this standard.
Despite the position of the official organizations, which is also shared with the residents of Catalonia, the majority of Valencian speakers consider their language unique and different from Catalan. This position is promoted by people who do not use Valencian regularly. There is a minority of scholars who defend the position of the Royal Academy of Valencian Culture ("Acadèmia de Cultura Valenciana", RACV), which uses for Valencian an independent standard from Catalan.
This clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical.
Vocabulary.
Word choices.
Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.
Literary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there is a tendency of favoring words of Northern dialects in detriment of others, even though nowadays there is a greater freedom of choice.
Latin and Greek learned words.
Like other languages, Catalan has a large list of learned words from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. On the fourteenth and fifteenth centuries Catalan had a number of Greco-Latin learned words much superior to other Romance languages, as it can be attested for example in Roís de Corella's writings.
Word formation.
The process of word derivation in Catalan follows the same principles as the other Romance languages, where agglutination is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example "elèctric [ə'lɛktrik] ("electrical") vs. "electricitat" [ələktrisi'tat]. Prefixes are usually appended to verbs, for as in preveure" ("foresee").
There is greater regularity in the process of word-compouding, where one can find compounded words as much as in English.
Writing system.
Catalan uses the Roman alphabet, with some added symbols and digraphs. The Catalan orthography is systematic and largely phonologically based.
Grammar.
The grammar of Catalan is similar to other Romance languages. Features include:
Gender and number inflection.
In gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the disapparition of the typical masculine suffix "-o". Thus, the alternance of "-o"/"-a", has been replaced by "ø"/"-a". There are only a few exceptions, like "minso"/"minsa" ("scarce"). Many not completely predictable morphological alternations may occur, like:
Catalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has "noi"/"noia" ("boy"/"girl") and "gall"/"gallina" ("cock"/"hen"), whereas French has "garçon"/"fille" and "coq"/"poule".
There is a tendency to abandon traditionally gender-invariable adjectives in favour of marked ones, something prevalent in Occitan and French. Thus, one can find "bullent"/"bullenta" ("boiling") in contrast with traditional "bullent"/"bullent".
Like in the other Western Romance languages, the main plural expression is the suffix "-s", which may create morphological alternations akin the ones found in gender inflection, albeit more rarely.
The most important one is the addition of "-o-" before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: "el pols"/"els polsos" ("the pulse"/"the pulses") vs. "la pols"/"les pols" ("the dust"/"the dusts").
Determiners.
The inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, like "dels" ("of + the [plural]"), but not as many as Italian (which has "sul", "col", "nel", etc.).
Central Catalan has abandoned almost completely unstressed possessives ("mon", etc.) in favour of constructions of article + stressed forms ("el meu", etc.), a feature shared with Italian.
Personal pronouns.
The morphology of Catalan personal pronouns is complex, specially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian; French has such a different system that comparisons are not feasible). Features include the neuter gender ("ho") and the great degree of freedom when combining different unstressed pronouns (65 combinations).
The Catalan pronouns exhibit T–V distinction, like all other Romance languages (and most European languages, but not English). This feature implies the use of a different set of second person pronouns for formality.
This flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have "m'hi recomanaren" ("they recommended me to him"), whereas in French one must say "ils m'ont recommandé à lui", and Spanish "me recomendaron a él". This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish).
Verbs.
Like all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs.
The Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the analytic indicative perfect with a periphrastic form of "anar" ("to go") + infinitive.
Catalan verbs are traditionally divided into three conjugations, with vowel themes "-a-", "-e-", "-i-", the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of "servir", with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding.
Syntax.
The grammar of Catalan follows the general pattern of Western Romance languages. The primary word order is SVO (subject–verb–object).
Catalan names.
In the Spanish state (as in Portugal), every person has officially two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction "i" ("and").
Sample text.
Selected text from Manuel de Pedrolo's 1970 novel "Un amor fora ciutat" ("A love affair outside the city").
External links.
Institutions
About the Catalan language
Monolingual dictionaries
Bilingual and multilingual dictionaries
Automated translation systems
Phrasebooks
Learning resources
Catalan-language online encyclopedia

</doc>
<doc id="5285" url="http://en.wikipedia.org/wiki?curid=5285" title="STS-51-F">
STS-51-F

STS-51-F (also known as Spacelab 2) was the nineteenth flight of NASA's Space Shuttle program, and the eighth flight of Space Shuttle "Challenger". It launched from Kennedy Space Center, Florida, on July 29, 1985, and landed just under eight days later on August 6.
While STS-51-F's primary payload was the Spacelab 2 laboratory module, the payload which received the most publicity was the Carbonated Beverage Dispenser Evaluation, which was an experiment in which both Coca-Cola and Pepsi tried to make their carbonated drinks available to astronauts.
During launch the "Challenger" experienced multiple sensor failings in its SSMEs and had to perform an "Abort to Orbit" (ATO) emergency procedure. It is the only mission to have carried out an abort of any kind. As a result of the ATO, the mission was carried out at a slightly lower orbital altitude.
Crew.
Crew notes.
As with previous Spacelab missions, the crew was divided between two 12-hour shifts. Acton, Bridges and Henize made up the "Red Team" while Bartoe, England and Musgrave comprised the "Blue Team"; commander Fullerton could take either shift when needed. "Challenger" carried two EMUs in the event of an emergency spacewalk, which would have been performed by England and Musgrave.
Launch.
STS-51-F's first launch attempt on July 12, 1985 was halted with the countdown at T-3 seconds after main engine ignition, when a malfunction of the number two Space Shuttle Main Engine (SSME) coolant valve caused the shutdown of all three main engines. "Challenger" launched successfully on its second attempt at July 29, 1985, 17:00 EDT, after a delay of one hour and 37 minutes due to a problem with the table maintenance block update uplink.
Three minutes and 31 seconds into the ascent, one of the center engine's two high pressure fuel turbopump turbine discharge temperature sensors failed. Two minutes and 12 seconds later, the second sensor failed, causing the shutdown of the center engine. This was the only in-flight main engine failure of the shuttle program. Approximately 8 minutes into the flight, one of the same temperature sensors in the right engine failed, and the remaining right engine temperature sensor displayed readings near the redline for engine shutdown. Booster Systems Engineer Jenny M. Howard acted quickly to command the crew to inhibit any further automatic SSME shutdowns based on readings from the remaining sensors, preventing the potential shutdown of a second engine and a possible abort mode that may have resulted in the loss of the vehicle and crew.
The failed SSME resulted in an Abort to Orbit (ATO) trajectory, whereby the shuttle achieved a lower-than-planned orbital altitude.
Mission summary.
STS-51-F's primary payload was the laboratory module Spacelab 2. A special part of the modular Spacelab system, the "igloo", which was located at head of a three-pallet train, provided on-site support to instruments mounted on pallets. The main mission objective was to verify performance of Spacelab systems, determine the interface capability of the orbiter, and measure the environment created by the spacecraft. Experiments covered life sciences, plasma physics, astronomy, high-energy astrophysics, solar physics, atmospheric physics and technology research. Despite mission replanning necessitated by "Challenger"'s abort to orbit trajectory, the Spacelab mission was declared a success.
The flight marked the first time the ESA Instrument Pointing System (IPS) was tested in orbit. This unique pointing instrument was designed with an accuracy of one arcsecond. Initially, some problems were experienced when it was commanded to track the Sun, but a series of software fixes were made and the problem was corrected. In addition, Tony England became the second amateur radio operator to transmit from space during the mission.
The Spacelab Infrared Telescope (IRT) was also flown on the mission. The IRT was a 15.2 cm aperture helium-cooled infrared telescope, observing light between wavelengths of 1.7 to 118 μm. The experiment experienced some problems, such as heat emissions from the Shuttle corrupting data, but still returned useful astronomical data.
The Plasma Diagnostics Package (PDP), which had been previously flown on STS-3, made its return on the mission, and was part of a set of plasma physics experiments designed to study the Earth's ionosphere. During the third day of the mission, it was grappled out of the payload bay by the Remote Manipulator System and released for six hours. During this time, "Challenger" maneuvered around the PDP as part of a proximity operations exercise. The PDP was successfully grappled by the RMS and returned to the payload bay at the beginning of the fourth day of the mission.
In a heavily-publicized marketing experiment, astronauts aboard STS-51-F drank carbonated beverages from specially-designed cans provided by competitors Coca-Cola and Pepsi. Post-flight, the astronauts revealed that they preferred Tang, in part because it could be mixed on-orbit with existing chilled water supplies, whereas there was no dedicated refrigeration equipment on board to chill the soda, which also fizzed excessively in microgravity.
Landing.
"Challenger" landed at Edwards Air Force Base, California, on August 6, 1985, at 12:45:26 pm PDT. Its rollout distance was . The mission had been extended by 17 orbits for additional payload activities due to the Abort to Orbit. The orbiter arrived back at Kennedy Space Center on August 11, 1985.
Mission insignia.
The mission insignia was designed by Houston artist Skip Bradley. Space Shuttle "Challenger" is depicted ascending toward the heavens in search of new knowledge in the field of solar and stellar astronomy, with its Spacelab 2 payload. The constellations Leo and Orion are shown in the positions they were in relative to the Sun during the flight. The nineteen stars indicate that the mission is the 19th shuttle flight.

</doc>
<doc id="5288" url="http://en.wikipedia.org/wiki?curid=5288" title="Classical period (music)">
Classical period (music)

The dates of the Classical period in Western music are generally accepted as being between about 1730 and 1820. However, the term "classical music" is used in a colloquial sense as a synonym for Western art music, which describes a variety of Western musical styles from the ninth century to the present, and especially from the sixteenth or seventeenth to the nineteenth. This article is about the specific period from 1730 to 1820.
The Classical period falls between the Baroque and the Romantic periods. The best-known composers from this period are Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Franz Schubert; other notable names include Luigi Boccherini, Muzio Clementi, Antonio Soler, Antonio Salieri, François Joseph Gossec, Johann Stamitz, Carl Friedrich Abel, Carl Philipp Emanuel Bach, and Christoph Willibald Gluck. Ludwig van Beethoven is also regarded either as a Romantic composer or a composer who was part of the transition to the Romantic. 
Franz Schubert is also something of a transitional figure, as are Johann Nepomuk Hummel, Mauro Giuliani, Friedrich Kuhlau, Fernando Sor, Luigi Cherubini, Jan Ladislav Dussek, and Carl Maria von Weber. The period is sometimes referred to as the era of "Viennese Classic" or "Classicism" (), since Wolfgang Amadeus Mozart, Joseph Haydn, Antonio Salieri, and Ludwig van Beethoven all worked at some time in Vienna, and Franz Schubert was born there.
Classicism.
In the middle of the 18th century, Europe began to move toward a new style in architecture, literature, and the arts, generally known as Classicism. This style sought to emulate the ideals of Classical antiquity, especially those of Classical Greece. While still tightly linked to Court culture and absolutism, with its formality and emphasis on order and hierarchy, the new style was also "cleaner". It favored clearer divisions between parts, brighter contrasts and colors, and simplicity rather than complexity. In addition, the typical size of orchestras began to increase.
The remarkable development of ideas in "natural philosophy" had already established itself in the public consciousness. In particular, Newton's physics was taken as a paradigm: structures should be well-founded in axioms and be both well-articulated and orderly. This taste for structural clarity began to affect music, which moved away from the layered polyphony of the Baroque period toward a style known as homophony, in which the melody is played over a subordinate harmony. This move meant that chords became a much more prevalent feature of music, even if they interrupted the melodic smoothness of a single part. As a result, the tonal structure of a piece of music became more audible.
The new style was also encouraged by changes in the economic order and social structure. As the 18th century progressed, the nobility became the primary patrons of instrumental music, while public taste increasingly preferred comic opera. This led to changes in the way music was performed, the most crucial of which was the move to standard instrumental groups and the reduction in the importance of the "continuo"—the rhythmic and harmonic ground of a piece of music, typically played by a keyboard (harpsichord or organ) and potentially by several other instruments. One way to trace the decline of the continuo and its figured chords is to examine the disappearance of the term "obbligato", meaning a mandatory instrumental part in a work of chamber music. In Baroque compositions, additional instruments could be added to the continuo according to preference; in Classical compositions, all parts were specifically noted, though not always "notated", so the term "obbligato" became redundant. By 1800, it was practically extinct.
Economic changes also had the effect of altering the balance of availability and quality of musicians. While in the late Baroque a major composer would have the entire musical resources of a town to draw on, the forces available at a hunting lodge were smaller and more fixed in their level of ability. This was a spur to having primarily simple parts to play, and in the case of a resident virtuoso group, a spur to writing spectacular, idiomatic parts for certain instruments, as in the case of the Mannheim orchestra. In addition, the appetite for a continual supply of new music, carried over from the Baroque, meant that works had to be performable with, at best, one rehearsal. Indeed, even after 1790 Mozart writes about "the rehearsal", with the implication that his concerts would have only one.
Since polyphonic texture was no longer the main focus of music (excluding the development section) but rather a single melodic line with accompaniment, there was greater emphasis on notating that line for dynamics and phrasing. The simplification of texture made such instrumental detail more important, and also made the use of characteristic rhythms, such as attention-getting opening fanfares, the funeral march rhythm, or the minuet genre, more important in establishing and unifying the tone of a single movement.
Forms such as the concerto and sonata were more heavily defined and given more specific rules, whereas the symphony was created in this period (this is popularly attributed to Joseph Haydn). The "concerto grosso" (a concerto for more than one musician) began to be replaced by the "solo concerto" (a concerto featuring only one soloist), and therefore began to place more importance on the particular soloist's ability to show off. There were, of course, some "concerti grossi" that remained, the most famous of which being Mozart's Sinfonia Concertante for Violin and Viola in E flat Major.
Main characteristics.
Classical music has a lighter, clearer texture than Baroque music and is less complex. It is mainly homophonic—melody above chordal accompaniment (but counterpoint is by no means forgotten, especially later in the period). It also make use of Style gallant in the classical period which was drawn in opposition to the strictures of the Baroque style, emphasizing light elegance in place of the Baroque's dignified seriousness and impressive grandeur.
Variety and contrast within a piece became more pronounced than before. Variety of keys, melodies, rhythms and dynamics (using "crescendo, diminuendo" and "sforzando"), along with frequent changes of mood and timbre were more commonplace in the Classical period than they had been in the Baroque. Melodies tended to be shorter than those of Baroque music, with clear-cut phrases and clearly marked cadences. The orchestra increased in size and range; the harpsichord continuo fell out of use, and the woodwind became a self-contained section. As a solo instrument, the harpsichord was replaced by the piano (or fortepiano). Early piano music was light in texture, often with Alberti bass accompaniment, but it later became richer, more sonorous and more powerful.
Importance was given to instrumental music—the main kinds were sonata, trio, string quartet, symphony, concerto, serenade and divertimento. Sonata form developed and became the most important form. It was used to build up the first movement of most large-scale works, but also other movements and single pieces (such as overtures).
History.
The Baroque/Classical transition c. 1730–1760.
At first the new style took over Baroque forms—the ternary "da capo aria" and the "sinfonia" and "concerto"—but composed with simpler parts, more notated ornamentation and more emphatic division into sections. However, over time, the new aesthetic caused radical changes in how pieces were put together, and the basic layouts changed. Composers from this period sought dramatic effects, striking melodies, and clearer textures. The Italian composer Domenico Scarlatti was an important figure in the transition from Baroque to Classical. His unique compositional style is strongly related to that of the early Classical period. He is best known for composing more than five hundred one-movement keyboard sonatas. In Spain, Antonio Soler also produced valuable keyboard sonatas, more varied in form than those of Scarlatti, with some pieces in three or four movements. 
Baroque music generally uses many harmonic fantasies and does not concentrate that much on the structure of the musical piece, musical phrases and motives. In the classical period, the harmonic functions are simpler. However, the structure of the piece, the phrases and motives, are much more important in the tunes than in the Baroque period. 
Another important break with the past was the radical overhaul of opera by Christoph Willibald Gluck, who cut away a great deal of the layering and improvisational ornament and focused on the points of modulation and transition. By making these moments where the harmony changes more focal, he enabled powerful dramatic shifts in the emotional color of the music. To highlight these episodes he used changes in instrumentation, melody, and mode. Among the most successful composers of his time, Gluck spawned many emulators, one of whom was Antonio Salieri. Their emphasis on accessibility brought huge successes in opera, and in vocal music more widely: songs, oratorios, and choruses. These were considered the most important kinds of music for performance and hence enjoyed greatest success in the public estimation.
The phase between the Baroque and the rise of the Classical, with its broad mixture of competing ideas and attempts to unify the different demands of taste, economics and "worldview", goes by many names. It is sometimes called "Galant", "Rococo", or "pre-Classical", or at other times "early Classical". It is a period where some composers still working in the Baroque style flourish, though sometimes thought of as being more of the past than the present—Bach, Handel, and Telemann all composed well beyond the point at which the homophonic style is clearly in the ascendant. Musical culture was caught at a crossroads: the masters of the older style had the technique, but the public hungered for the new. This is one of the reasons C. P. E. Bach was held in such high regard: he understood the older forms quite well and knew how to present them in new garb, with an enhanced variety of form.
Circa 1750–1775.
By the late 1750s there were flourishing centers of the new style in Italy, Vienna, Mannheim, and Paris; dozens of symphonies were composed and there were bands of players associated with theatres. Opera or other vocal music was the feature of most musical events, with concertos and symphonies (arising from the overture) serving as instrumental interludes and introductions for operas and church services. Over the course of the Classical period, symphonies and concertos developed and were presented independently of vocal music. 
The "normal" ensemble—a body of strings supplemented by winds—and movements of particular rhythmic character were established by the late 1750s in Vienna. However, the length and weight of pieces was still set with some Baroque characteristics: individual movements still focused on one "affect" or had only one sharply contrasting middle section, and their length was not significantly greater than Baroque movements. There was not yet a clearly enunciated theory of how to compose in the new style. It was a moment ripe for a breakthrough.
Many consider this breakthrough to have been made by C. P. E. Bach, Gluck, and several others. Indeed, C. P. E. Bach and Gluck are often considered founders of the Classical style.
The first great master of the style was the composer Joseph Haydn. In the late 1750s he began composing symphonies, and by 1761 he had composed a triptych ("Morning", "Noon", and "Evening") solidly in the contemporary mode. As a vice-Kapellmeister and later Kapellmeister, his output expanded: he composed over forty symphonies in the 1760s alone. And while his fame grew, as his orchestra was expanded and his compositions were copied and disseminated, his voice was only one among many.
While some suggest that he was overshadowed by Mozart and Beethoven, it would be difficult to overstate Haydn's centrality to the new style, and therefore to the future of Western art music as a whole. At the time, before the pre-eminence of Mozart or Beethoven, and with Johann Sebastian Bach known primarily to connoisseurs of keyboard music, Haydn reached a place in music that set him above all other composers except perhaps George Frideric Handel. He took existing ideas, and radically altered how they functioned—earning him the titles "father of the symphony" and "father of the string quartet".
One of the forces that worked as an impetus for his pressing forward was the first stirring of what would later be called Romanticism—the "Sturm und Drang", or "storm and stress" phase in the arts, a short period where obvious emotionalism was a stylistic preference. Haydn accordingly wanted more dramatic contrast and more emotionally appealing melodies, with sharpened character and individuality. This period faded away in music and literature: however, it influenced what came afterward and would eventually be a component of aesthetic taste in later decades.
The "Farewell Symphony", No. 45 in F Minor, exemplifies Haydn's integration of the differing demands of the new style, with surprising sharp turns and a long adagio to end the work. In 1772, Haydn completed his Opus 20 set of six string quartets, in which he deployed the polyphonic techniques he had gathered from the previous era to provide structural coherence capable of holding together his melodic ideas. For some, this marks the beginning of the "mature" Classical style, in which the period of reaction against late Baroque complexity yielded to a period of integration Baroque and Classical elements.
Circa 1775–1790.
Haydn, having worked for over a decade as the music director for a prince, had far more resources and scope for composing than most and also the ability to shape the forces that would play his music. This opportunity was not wasted, as Haydn, beginning quite early on his career, sought to press forward the technique of building ideas in music. His next important breakthrough was in the Opus 33 string quartets (1781), in which the melodic and the harmonic roles segue among the instruments: it is often momentarily unclear what is melody and what is harmony. This changes the way the ensemble works its way between dramatic moments of transition and climactic sections: the music flows smoothly and without obvious interruption. He then took this integrated style and began applying it to orchestral and vocal music.
Haydn's gift to music was a way of composing, a way of structuring works, which was at the same time in accord with the governing aesthetic of the new style. However, a younger contemporary, Wolfgang Amadeus Mozart, brought his genius to Haydn's ideas and applied them to two of the major genres of the day: opera, and the virtuoso concerto. Whereas Haydn spent much of his working life as a court composer, Mozart wanted public success in the concert life of cities. This meant opera, and it meant performing as a virtuoso. Haydn was not a virtuoso at the international touring level; nor was he seeking to create operatic works that could play for many nights in front of a large audience. Mozart wanted both. Moreover, Mozart also had a taste for more chromatic chords (and greater contrasts in harmonic language generally), a greater love for creating a welter of melodies in a single work, and a more Italianate sensibility in music as a whole. He found, in Haydn's music and later in his study of the polyphony of Bach, the means to discipline and enrich his gifts.
Mozart rapidly came to the attention of Haydn, who hailed the new composer, studied his works, and considered the younger man his only true peer in music. In Mozart, Haydn found a greater range of instrumentation, dramatic effect and melodic resource; the learning relationship moved in two directions.
Mozart's arrival in Vienna in 1780 brought an acceleration in the development of the Classical style. There Mozart absorbed the fusion of Italianate brilliance and Germanic cohesiveness that had been brewing for the previous 20 years. His own taste for brilliances, rhythmically complex melodies and figures, long cantilena melodies, and virtuoso flourishes was merged with an appreciation for formal coherence and internal connectedness. It is at this point that war and inflation halted a trend to larger orchestras and forced the disbanding or reduction of many theater orchestras. This pressed the Classical style inwards: toward seeking greater ensemble and technical challenge—for example, scattering the melody across woodwinds, or using thirds to highlight the melody taken by them. This process placed a premium on chamber music for more public performance, giving a further boost to the string quartet and other small ensemble groupings.
It was during this decade that public taste began, increasingly, to recognize that Haydn and Mozart had reached a higher standard of composition. By the time Mozart arrived at age 25, in 1781, the dominant styles of Vienna were recognizably connected to the emergence in the 1750s of the early Classical style. By the end of the 1780s, changes in performance practice, the relative standing of instrumental and vocal music, technical demands on musicians, and stylistic unity had become established in the composers who imitated Mozart and Haydn. During this decade Mozart composed his most famous operas, his six late symphonies that helped to redefine the genre, and a string of piano concerti that still stand at the pinnacle of these forms.
One composer who was influential in spreading the more serious style that Mozart and Haydn had formed is Muzio Clementi, a gifted virtuoso pianist who tied with Mozart in a musical "duel" before the emperor in which they each improvised and performed their compositions. Clementi's sonatas for the piano circulated widely, and he became the most successful composer in London during the 1780s. Also in London at this time was Jan Ladislav Dussek, who, like Clementi, encouraged piano makers to extend the range and other features of their instruments, and then fully exploited the newly opened possibilities. The importance of London in the Classical period is often overlooked, but it served as the home to the Broadwood's factory for piano manufacturing and as the base for composers who, while less notable than the "Vienna School", had a decisive influence on what came later. They were composers of many fine works, notable in their own right. London's taste for virtuosity may well have encouraged the complex passage work and extended statements on tonic and dominant.
Circa 1790–1820.
When Haydn and Mozart began composing, symphonies were played as single movements—before, between, or as interludes within other works—and many of them lasted only ten or twelve minutes; instrumental groups had varying standards of playing, and the continuo was a central part of music-making. 
In the intervening years, the social world of music had seen dramatic changes. International publication and touring had grown explosively, and concert societies formed. Notation became more specific, more descriptive—and schematics for works had been simplified (yet became more varied in their exact working out). In 1790, just before Mozart's death, with his reputation spreading rapidly, Haydn was poised for a series of successes, notably his late oratorios and "London" symphonies. Composers in Paris, Rome, and all over Germany turned to Haydn and Mozart for their ideas on form.
The time was again ripe for a dramatic shift. In the 1790s, a new generation of composers, born around 1770, emerged. While they had grown up with the earlier styles, they heard in the recent works of Haydn and Mozart a vehicle for greater expression. In 1788 Luigi Cherubini settled in Paris and in 1791 composed "Lodoiska", an opera that raised him to fame. Its style is clearly reflective of the mature Haydn and Mozart, and its instrumentation gave it a weight that had not yet been felt in the grand opera. His contemporary Étienne Méhul extended instrumental effects with his 1790 opera "Euphrosine et Coradin", from which followed a series of successes.
The most fateful of the new generation was Ludwig van Beethoven, who launched his numbered works in 1794 with a set of three piano trios, which remain in the repertoire. Somewhat younger than the others, though equally accomplished because of his youthful study under Mozart and his native virtuosity, was Johann Nepomuk Hummel. Hummel studied under Haydn as well; he was a friend to Beethoven and Franz Schubert. He concentrated more on the piano than any other instrument, and his time in London in 1791 and 1792 generated the composition and publication in 1793 of three piano sonatas, opus 2, which idiomatically used Mozart's techniques of avoiding the expected cadence, and Clementi's sometimes modally uncertain virtuoso figuration. Taken together, these composers can be seen as the vanguard of a broad change in style and the center of music. They studied one another's works, copied one another's gestures in music, and on occasion behaved like quarrelsome rivals.
The crucial differences with the previous wave can be seen in the downward shift in melodies, increasing durations of movements, the acceptance of Mozart and Haydn as paradigmatic, the greater use of keyboard resources, the shift from "vocal" writing to "pianistic" writing, the growing pull of the minor and of modal ambiguity, and the increasing importance of varying accompanying figures to bring "texture" forward as an element in music. In short, the late Classical was seeking a music that was internally more complex. The growth of concert societies and amateur orchestras, marking the importance of music as part of middle-class life, contributed to a booming market for pianos, piano music, and virtuosi to serve as examplars. Hummel, Beethoven, and Clementi were all renowned for their improvising.
Direct influence of the Baroque continued to fade: the figured bass grew less prominent as a means of holding performance together, the performance practices of the mid-18th century continued to die out. However, at the same time, complete editions of Baroque masters began to become available, and the influence of Baroque style continued to grow, particularly in the ever more expansive use of brass. Another feature of the period is the growing number of performances where the composer was not present. This led to increased detail and specificity in notation; for example, there were fewer "optional" parts that stood separately from the main score.
The force of these shifts became apparent with Beethoven's 3rd Symphony, given the name "Eroica", which is Italian for "heroic", by the composer. As with Stravinsky's "The Rite of Spring", it may not have been the first in all of its innovations, but its aggressive use of every part of the Classical style set it apart from its contemporary works: in length, ambition, and harmonic resources as well.
First Viennese School.
The First Viennese School is a name mostly used to refer to three composers of the Classical period in late-18th-century Vienna: W. A. Mozart, Haydn, and Beethoven. Franz Schubert is occasionally added to the list.
In German speaking countries, the term "Wiener Klassik" (lit. "Viennese classical era/art") is used. That term is often more broadly applied to the Classical era in music as a whole, as a means to distinguish it from other periods that are colloquially referred to as "classical", namely Baroque and Romantic music. 
The term "Viennese School" was first used by Austrian musicologist Raphael Georg Kiesewetter in 1834, although he only counted Haydn and Mozart as members of the school. Other writers followed suit, and eventually Beethoven was added to the list. The designation "first" is added today to avoid confusion with the Second Viennese School.
Whilst, Schubert apart, these composers certainly knew each other (with Haydn and Mozart even being occasional chamber-music partners), there is no sense in which they were engaged in a collaborative effort in the sense that one would associate with 20th-century schools such as the Second Viennese School, or Les Six. Nor is there any significant sense in which one composer was "schooled" by another (in the way that Berg and Webern were taught by Schoenberg), though it is true that Beethoven for a time received lessons from Haydn. 
Attempts to extend the First Viennese School to include such later figures as Anton Bruckner, Johannes Brahms, and Gustav Mahler are merely journalistic, and never encountered in academic musicology.
Classical influence on later composers.
Musical eras seldom disappear at once; instead, features are replaced over time, until the old is simply felt as "old-fashioned". The Classical style did not "die" so much as transform under the weight of changes.
One crucial change was the shift towards harmonies centering around "flatward" keys: shifts in the subdominant direction. In the Classical style, major key was far more common than minor, chromaticism being moderated through the use of "sharpward" modulation, and sections in the minor mode were often merely for contrast. Beginning with Mozart and Clementi, there began a creeping colonization of the subdominant region. With Schubert, subdominant moves flourished after being introduced in contexts in which earlier composers would have confined themselves to dominant shifts. This introduced darker colors to music, strengthened the minor mode, and made structure harder to maintain. Beethoven contributed to this by his increasing use of the fourth as a consonance, and modal ambiguity—for example, the opening of the D Minor Symphony.
Franz Schubert, Carl Maria von Weber, and John Field are among the most prominent in this generation of "Classical Romantics", along with the young Felix Mendelssohn. Their sense of form was strongly influenced by the Classical style, and they were not yet "learned" (imitating rules which were codified by others), but they directly responded to works by Beethoven, Mozart, Clementi, and others, as they encountered them. The instrumental forces at their disposal were also quite "Classical" in number and variety, permitting similarity with Classical works.
However, the forces destined to end the hold of the Classical style gathered strength in the works of each of the above composers. The most commonly cited one is harmonic innovation. Also important is the increasing focus on having a continuous and rhythmically uniform accompanying figuration: Beethoven's Moonlight Sonata was the model for hundreds of later pieces—where the shifting movement of a rhythmic figure provides much of the drama and interest of the work, while a melody drifts above it. Greater knowledge of works, greater instrumental expertise, increasing variety of instruments, the growth of concert societies, and the unstoppable domination of the piano—which created a huge audience for sophisticated music—all contributed to the shift to the "Romantic" style.
Drawing the line between these two styles is impossible: some sections of Mozart's works, taken alone, are indistinguishable in harmony and orchestration from music written 80 years later—and composers continue to write in normative Classical styles into the 20th century. Even before Beethoven's death, composers such as Louis Spohr were self-described Romantics, incorporating, for example, more extravagant chromaticism in their works. 
However, Vienna's fall as the most important musical center for orchestral composition is generally felt to mark the Classical style's final eclipse—and the end of its continuous organic development of one composer learning in close proximity to others. Franz Liszt and Frédéric Chopin visited Vienna when young, but they then moved on to other vistas. Composers such as Carl Czerny, while deeply influenced by Beethoven, also searched for new ideas and new forms to contain the larger world of musical expression and performance in which they lived.
Renewed interest in the formal balance and restraint of 18th century classical music led in the early 20th century to the development of so-called Neoclassical style, which numbered Stravinsky and Prokofiev among its proponents, at least at certain times in their careers.

</doc>
<doc id="5295" url="http://en.wikipedia.org/wiki?curid=5295" title="Character encoding">
Character encoding

In computing, a character encoding is used to represent a repertoire of characters by some kind of an encoding system. Depending on the abstraction level and context, corresponding code points and the resulting code space may be regarded as bit patterns, octets, natural numbers, electrical pulses, etc. A character encoding is used in both computation, data storage, and transmission of textual data. Terms such as character set, character map, codeset or code page are sometimes used as near synonyms; however, these terms have related but distinct meanings described in the article.
Early character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written language, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent more of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.
History.
Early binary repertoires include Bacon's cipher, Braille, International maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode.
Morse code was introduced in the 1840s and is used to encode each letter of the Latin alphabet, each Arabic numeral, and some other characters via a series of long and short presses of a telegraph key. Representations of characters encoded using Morse code varied in length.
The Baudot code, a 5-bit encoding, was created by Émile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No. 2 (ITA2) in 1930.
Fieldata, a 6- or 7-bit code, was introduced by the U.S. Army Signal Corps in the late 1950s.
IBM's Binary Coded Decimal (BCD) was a 6-bit encoding scheme used by IBM in as early as 1959 in its 1401 and 1620 computers, and in its 7000 Series (for example, 704, 7040, 709 and 7090 computers), as well as in associated peripherals. BCD extended existing simple 4-bit numeric encoding to include alphabetic and special characters, mapping it easily to punch-card encoding which was already in widespread use. It was the precursor to EBCDIC.
ASCII was introduced in 1963 and is a 7-bit encoding scheme used to encode letters, numerals, symbols, and device control codes as fixed-length codes using integers.
IBM's Extended Binary Coded Decimal Interchange Code (usually abbreviated EBCDIC) is an 8-bit encoding scheme developed in 1963.
The limitations of such sets soon became apparent, and a number of ad hoc methods were developed to extend them. The need to support more writing systems for different languages, including the CJK family of East Asian scripts, required support for a far larger number of characters and demanded a systematic approach to character encoding rather than the previous ad hoc approaches.
The frustrating dilemma that researchers in this field encountered in the 1980s as they tried to develop universally interchangeable character encodings was that on the one hand, it seemed to be necessary to add more bits to accommodate additional characters. On the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users at the time), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users). 
The compromise solution that was eventually hit upon with Unicode, as further explained below, was to break the longstanding assumption (dating back to the old telegraph codes) that each character should always directly correspond to a particular pattern of encoded bits. Instead, characters would be first mapped to an intermediate stage in the form of abstract numbers known as code points. Then those code points would be encoded in a variety of ways and with various default numbers of bits per character (code units) depending upon context. To encode code points higher than the length of the code unit, such as above 256 for 8-bit units, the solution was to implement variable-width encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.
Code unit.
A "code unit" is a bit sequence used to encode the characters of a repertoire.
Encodings associate their meaning with either a single code unit value or a sequence of code units as one value.
Unicode encoding model.
Unicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a modern, unified character encoding. Rather than mapping characters directly to octets (bytes), they separately define what characters are available, their numbering, how those numbers are encoded as a series of "code units" (limited-size numbers), and finally how those units are encoded as a stream of octets. The idea behind this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model correctly one needs more precise terms than "character set" and "character encoding." The terms used in the modern model follow:
A "character repertoire" is the full set of abstract characters that a system supports. The repertoire may be closed, i.e. no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series), or it may be open, allowing additions (as is the case with Unicode and to a limited extent the Windows code pages). The characters in a given repertoire reflect decisions that have been made about how to divide writing systems into basic information units. The basic variants of the Latin, Greek, and Cyrillic alphabets, can be broken down into letters, digits, punctuation, and a few "special characters" like the space, which can all be arranged in simple linear sequences that are displayed in the same order they are read. Even with these alphabets, however, diacritics pose a complication: they can be regarded either as part of a single character containing a letter and diacritic (known as a precomposed character), or as separate characters. The former allows a far simpler text handling system but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems. Other writing systems, such as Arabic and Hebrew, are represented with more complex character repertoires due to the need to accommodate things like bidirectional text and glyphs that are joined together in different ways for different situations.
A "coded character set" (CCS) specifies how to represent a repertoire of characters using a number of (typically non-negative) integer values called "code points". For example, in a given repertoire, a character representing the capital letter "A" in the Latin alphabet might be assigned to the integer 65, the character for "B" to 66, and so on. A complete set of characters and corresponding integers is a "coded character set". Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different codes. In a coded character set, each code point only represents one character, i.e., a coded character set is a function.
A "character encoding form" (CEF) specifies the conversion of a coded character set's integer codes into a set of limited-size integer "code values" that facilitate storage in a system that represents numbers in binary form using a fixed number of bits (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units would only be able to directly represent integers from 0 to 65,535 in each unit, but larger integers could be represented if more than one 16-bit unit could be used. This is what a CEF accommodates: it defines a way of mapping a "single" code "point" from a range of, say, 0 to 1.4 million, to a series of "one or more" code "values" from a range of, say, 0 to 65,535.
The simplest CEF system is simply to choose large enough units that the values from the coded character set can be encoded directly (one code point to one code value). This works well for coded character sets that fit in 8 bits (as most legacy non-CJK encodings do) and reasonably well for coded character sets that fit in 16 bits (such as early versions of Unicode). However, as the size of the coded character set increases (e.g. modern Unicode requires at least 21 bits/character), this becomes less and less efficient, and it is difficult to adapt existing systems to use larger code values. Therefore, most systems working with later versions of Unicode use either UTF-8, which maps Unicode code points to variable-length sequences of octets, or UTF-16, which maps Unicode code points to variable-length sequences of 16-bit words.
Next, a "character encoding scheme" (CES) specifies how the fixed-size integer code values should be mapped into an octet sequence suitable for saving on an octet-based file system or transmitting over an octet-based network. With Unicode, a simple character encoding scheme is used in most cases, simply specifying whether the bytes for each integer should be in big-endian or little-endian order (even this isn't needed with UTF-8). However, there are also compound character encoding schemes, which use escape sequences to switch between several simple schemes (such as ISO/IEC 2022), and compressing schemes, which try to minimise the number of bytes used per code unit (such as SCSU, BOCU, and Punycode). See comparison of Unicode encodings for a detailed discussion.
Finally, there may be a "higher level protocol" which supplies additional information that can be used to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.
The Unicode model reserves the term "character map" for historical systems which directly assign a sequence of characters to a sequence of bytes, covering all of CCS, CEF and CES layers.
Character sets, maps and code pages.
In computer science, the terms "character encoding", "character map", "character set" and "code page" were historically synonymous, as the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units – usually with a single character per code unit. The terms now have related but distinct meanings, reflecting the efforts of standards bodies to use precise terminology when writing about and unifying many different encoding systems. Regardless, the terms are still used interchangeably, with "character set" being nearly ubiquitous.
A "code page" usually means a byte oriented encoding, but with regard to some suite of encodings (covering different scripts), where many characters share the same codes in most or all those code pages. Well known code page suites are "Windows" (based on Windows-1252) and "IBM"/"DOS" (based on code page 437), see Windows code page for details. Most, but not all, encodings referred to as code pages are single-byte encodings (but see octet on byte size.)
IBM's Character Data Representation Architecture (CDRA) designates with coded character set identifiers (CCSIDs) and each of which is variously called a "charset", "character set", "code page", or "CHARMAP".
The term "code page" does not occur in Unix or Linux where "charmap" is preferred, usually in the larger context of locales.
Contrasted to CCS above, a "character encoding" is a map from abstract characters to code words. A "character set" in HTTP (and MIME) parlance is the same as a character encoding (but not the same as CCS).
"Legacy encoding" is a term sometimes used to characterize old character encodings, but with an ambiguity of sense. Most of its use is in the context of Unicodification, where it refers to encodings that fail to cover all Unicode code points, or, more generally, using a somewhat different character repertoire: several code points representing one Unicode character, or versa (see e.g. code page 437). Some sources refer to an encoding as "legacy" only because it preceded Unicode. All Windows code pages are usually referred to as legacy, both because they antedate Unicode and because they are unable to represent all 221 possible Unicode code points.
Character encoding translation.
As a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between encoding schemes as a form of data transcoding. Some of these are cited below.
Cross-platform:
Unix-like: 
Windows:

</doc>
<doc id="5296" url="http://en.wikipedia.org/wiki?curid=5296" title="Cogency">
Cogency


</doc>
<doc id="5298" url="http://en.wikipedia.org/wiki?curid=5298" title="Control character">
Control character

In computing and telecommunication, a control character or non-printing character is a code point (a number) in a character set, that does not in itself represent a written symbol.
It is in-band signaling in the context of character encoding.
All entries in the ASCII table below code 32 (technically the C0 control code set) and 127 are of this kind, including BEL (which is intended to cause an audible signal in the receiving terminal), SYN (which is a synchronization signal), and ENQ (a signal that is intended to trigger a response at the receiving end, to see if it is still present). The Extended Binary Coded Decimal Interchange Code (EBCDIC) character set contains 65 control codes, including all of the ASCII control codes as well as additional codes which are mostly used to control IBM peripherals. Unicode makes a distinction between Control characters (C0 and C1 control codes) versus Formatting characters (such as the Zero-width non-joiner). 
Other characters are mainly printing, printable, or graphic characters, except perhaps for the "space" character (see ASCII printable characters). 
History.
Procedural signs in Morse code are a form of control character.
A form of control characters were introduced in the 1870 Baudot code: NUL and DEL.
The 1901 Murray code added the carriage return (CR) and line feed (LF), and other versions of the Baudot code included other control characters.
The bell character (BEL), which rang a bell to alert operators, was also an early teletype control character.
Control characters have also been called "format effectors".
In ASCII.
The control characters in ASCII still in common use include:
Occasionally one might encounter modern uses of other codes, such as code 4 (End of transmission), used to end a Unix shell session or PostScript printer transmission. For the full list of control characters, see ASCII.
Even though many control characters are rarely used, the concept of sending device-control information intermixed with printable characters is so useful that device makers found a way to send hundreds of device instructions. Specifically, they used ASCII code 27 (escape), followed by a series of characters called a "control sequence" or "escape sequence". The mechanism was invented by Bob Bemer, the father of ASCII.
Typically, code 27 was sent first in such a sequence to alert the device that the following characters were to be interpreted as a control sequence rather than as plain characters, then one or more characters would follow to specify some detailed action, after which the device would go back to interpreting characters normally. For example, the sequence of code 27, followed by the printable characters "[2;10H", would cause a DEC VT-102 terminal to move its cursor to the 10th cell of the 2nd line of the screen. Several standards exist for these sequences, notably ANSI X3.64. But the number of non-standard variations in use is large, especially among printers, where technology has advanced far faster than any standards body can possibly keep up with.
In Unicode.
In Unicode, "Control-characters" are U+0000—U+001F (C0 controls), U+007F (delete), and U+0080—U+009F (C1 controls). Their General Category is "Cc". Formatting codes are distinct, in General Category "Cf". The Cc control characters have no Name in Unicode. They may be indicated informally as "<control-001A>".
Display.
There are a number of techniques to display non-printing characters, which may be illustrated with the bell character in ASCII encoding:
How control characters map to keyboards.
ASCII-based keyboards have a key labelled "Control", "Ctrl", or (rarely) "Cntl" which is used much like a shift key, being pressed in combination with another letter or symbol key. In one implementation, the control key generates the code 64 places below the code for the (generally) uppercase letter it is pressed in combination with (i.e., subtract 64 from ASCII code value in decimal of the (generally) uppercase letter). The other implementation is to take the ASCII code produced by the key and bitwise AND it with 31, forcing bits 6 and 7 to zero. For example, pressing "control" and the letter "g" or "G" (code 103 or 71 in base 10, which is 01000111 in binary, produces the code 7 (Bell, 7 in base 10, or 00000111 in binary). The NULL character (code 0) is represented by Ctrl-@, "@" being the code immediately before "A" in the ASCII character set. For convenience, a lot of terminals accept Ctrl-Space as an alias for Ctrl-@. In either case, this produces one of the 32 ASCII control codes between 0 and 31. This approach is not able to represent the DEL character because of its value (code 127), but Ctrl-? is often used for this character, as subtracting 64 from a '?' gives −1, which if masked to 7 bits is 127.
When the control key is held down, letter keys produce the same control characters regardless of the state of the shift or caps lock keys. In other words, it does not matter whether the key would have produced an upper-case or a lower-case letter. The interpretation of the control key with the space, graphics character, and digit keys (ASCII codes 32 to 63) vary between systems. Some will produce the same character code as if the control key were not held down. Other systems translate these keys into control characters when the control key is held down. The interpretation of the control key with non-ASCII ("foreign") keys also varies between systems.
Control characters are often rendered into a printable form known as caret notation by printing a caret (^) and then the ASCII character that has a value of the control character plus 64. Control characters generated using letter keys are thus displayed with the upper-case form of the letter. For example, ^G represents code 7, which is generated by pressing the G key when the control key is held down.
Keyboards also typically have a few single keys which produce control character codes. For example, the key labelled "Backspace" typically produces code 8, "Tab" code 9, "Enter" or "Return" code 13 (though some keyboards might produce code 10 for "Enter").
Many keyboards include keys that do not correspond to any ASCII printable or control character, for example cursor control arrows and word processing functions. The associated keypresses are communicated to computer programs by one of four methods: appropriating otherwise unused control characters; using some encoding other than ASCII; using multi-character control sequences; or using an additional mechanism outside of generating characters. "Dumb" computer terminals typically use control sequences. Keyboards attached to stand-alone personal computers made in the 1980s typically use one (or both) of the first two methods. Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that are pressed, including any of the four methods described above.
The design purpose.
The control characters were designed to fall into a few groups: printing and display control, data structuring, transmission control, and miscellaneous.
Printing and display control.
Printing control characters were first used to control the physical mechanism of printers, the earliest output device. An early implementation of this idea was the out-of-band ASA carriage control characters. Later, control characters were integrated into the stream of data to be printed.
The carriage return character (CR), when sent to such a device, causes it to put the character at the edge of the paper at which writing begins (it may, or may not, also move the printing position to the next line).
The line feed character (LF/NL) causes the device to put the printing position on the next line. It may (or may not), depending on the device and its configuration, also move the printing position to the start of the next line (whichever direction is first—left in Western languages and right in Hebrew and Arabic). 
The vertical and horizontal tab characters (VT and HT/TAB) cause the output device to move the printing position to the next tab stop in the direction of reading. 
The form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line. 
The backspace character (BS) moves the printing position one character space backwards. On printers, this is most often used so the printer can overprint characters to make other, not normally available, characters. On terminals and other electronic output devices, there are often software (or hardware) configuration choices which will allow a destruct backspace (i.e., a BS, SP, BS sequence) which erases, or a non-destructive one which does not. 
The shift in and shift out characters (SO and SI) selected alternate character sets, fonts, underlining or other printing modes. Escape sequences were often used to do the same thing. 
With the advent of computer terminals that did not physically print on paper and so offered more flexibility regarding screen placement, erasure, and so forth, printing control codes were adapted. Form feeds, for example, usually cleared the screen, there being no new paper page to move to. More complex escape sequences were developed to take advantage of the flexibility of the new terminals, and indeed of newer printers. The concept of a control character had always been somewhat limiting, and was extremely so when used with new, much more flexible, hardware. Control sequences (sometimes implemented as escape sequences) could match the new flexibility and power and became the standard method. However, there were, and remain, a large variety of standard sequences to choose from.
Data structuring.
The separators (File, Group, Record, and Unit: FS, GS, RS and US) were made to structure data, usually on a tape, in order to simulate punched cards.
End of medium (EM) warns that the tape (or other recording medium) is ending.
While many systems use CR/LF and TAB for structuring data, it is possible to encounter the separator control characters in data that needs to be structured. The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings. Their numeric values are contiguous with the space character, which can be considered a member of the group, as a word separator.
Transmission control.
The transmission control characters were intended to structure a data stream, and to manage re-transmission or graceful failure, as needed, in the face of transmission errors.
The start of heading (SOH) character was to mark a non-data section of a data stream—the part of a stream containing addresses and other housekeeping data. The start of text character (STX) marked the end of the header, and the start of the textual part of a stream. The end of text character (ETX) marked the end of the data of a message. A widely used convention is to make the two characters preceding ETX a checksum or CRC for error-detection purposes. The end of transmission block character (ETB) was used to indicate the end of a block of data, where data was divided into such blocks for transmission purposes.
The escape character (ESC) was intended to "quote" the next character, if it was another control character it would print it instead of performing the control function. It is almost never used for this purpose today.
The substitute character (SUB) was intended to request a translation of the next character from a printable character to another value, usually by setting bit 5 to zero. This is handy because some media (such as sheets of paper produced by typewriters) can transmit only printable characters. However, on MS-DOS systems with files opened in text mode, "end of text" or "end of file" is marked by this Ctrl-Z character, instead of the Ctrl-C or Ctrl-D, which are common on other operating systems.
The cancel character (CAN) signalled that the previous element should be discarded. The negative acknowledge character (NAK) is a definite flag for, usually, noting that reception was a problem, and, often, that the current element should be sent again. The acknowledge character (ACK) is normally used as a flag to indicate no problem detected with current element. 
When a transmission medium is half duplex (that is, it can transmit in only one direction at a time), there is usually a master station that can transmit at any time, and one or more slave stations that transmit when they have permission. The enquire character (ENQ) is generally used by a master station to ask a slave station to send its next message. A slave station indicates that it has completed its transmission by sending the end of transmission character (EOT).
The device control codes (DC1 to DC4) were originally generic, to be implemented as necessary by each device. However, a universal need in data transmission is to request the sender to stop transmitting when a receiver can't take more data right now. Digital Equipment Corporation invented a convention which used 19, (the device control 3 character (DC3), also known as control-S, or XOFF) to "S"top transmission, and 17, (the device control 1 character (DC1), aka control-Q, or XON) to start transmission. It has become so widely used that most don't realize it is not part of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such transmission flow control signals must be used, to avoid potential deadlock conditions, however. 
The data link escape character (DLE) was intended to be a signal to the other end of a data link that the following character is a control character such as STX or ETX. For example a packet may be structured in the following way (DLE) <STX> <PAYLOAD> (DLE) <ETX>.
Miscellaneous codes.
Code 7 (BEL) is intended to cause an audible signal in the receiving terminal.
Many of the ASCII control characters were designed for devices of the time that are not often seen today. For example, code 22, "synchronous idle" (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a transmitted word— this is a feature of "asynchronous" communication. "Synchronous" communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.)
Code 0 (ASCII code name NUL) is a special case. In paper tape, it is the case when there are no holes. It is convenient to treat this as a "fill" character with no meaning otherwise. Since the position of a NUL character has no holes punched, it can be replaced with any other character at a later time, so it was typically used to reserve space, either for correcting errors or for inserting information that would be available at a later time or in another place. In computing it is often used for padding in fixed length records and more commonly, to mark the end of a string.
Code 127 (DEL, a.k.a. "rubout") is likewise a special case. Its 7-bit code is "all-bits-on" in binary, which essentially erased a character cell on a paper tape when overpunched. Paper tape was a common storage medium when ASCII was developed, with a computing history dating back to WWII code breaking equipment at Biuro Szyfrów. Paper tape became obsolete in the 1970s, so this clever aspect of ASCII rarely saw any use after that. (However it should be noted that non-erasable Programmable ROMs are typically implemented as arrays of fusible elements, each representing a bit, which can only be switched one way, usually from one to zero. In such PROMs, the DEL and NUL characters can be used in the same way that they were used on punched tape: one to reserve meaningless fill bytes that can be written later, and the other to convert written bytes to meaningless fill bytes. For PROMs that switch one to zero, the roles of NUL and DEL are reversed; also, DEL will only work with 7-bit characters, which are rarely used today; for 8-bit content, the character code 255, commonly defined as a nonbreaking space character, can be used instead of DEL.) Some systems (such as the original Apples) converted it to a backspace. But because its code is in the range occupied by other printable characters, and because it had no official assigned glyph, many computer equipment vendors used it as an additional printable character (often an all-black "box" character useful for erasing text by overprinting with ink).
Many file systems do not allow control characters in the filenames, as they may have reserved functions.

</doc>
<doc id="5299" url="http://en.wikipedia.org/wiki?curid=5299" title="Carbon">
Carbon

Carbon (from "coal") is a chemical element with symbol C and atomic number 6. As a member of group 14 on the periodic table, it is nonmetallic and tetravalent — making four electrons available to form covalent chemical bonds. There are three naturally occurring isotopes, with 12C and 13C being stable, while 14C is radioactive, decaying with a half-life of about 5,730 years. Carbon is one of the few elements known since antiquity.
There are several allotropes of carbon of which the best known are graphite, diamond, and amorphous carbon. The physical properties of carbon vary widely with the allotropic form. For example, diamond is highly transparent, while graphite is opaque and black. Diamond is the hardest naturally-occurring material known, while graphite is soft enough to form a streak on paper (hence its name, from the Greek word "γράφω" which means "to write"). Diamond has a very low electrical conductivity, while graphite is a very good conductor. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials.
All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form. They are chemically resistant and require high temperature to react even with oxygen. The most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and other transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil and methane clathrates. Carbon forms a vast number of compounds, more than any other element, with almost ten million compounds described to date, which in turn are a tiny fraction of such compounds that are theoretically possible under standard conditions.
Carbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. It is present in all known life forms, and in the human body carbon is the second most abundant element by mass (about 18.5%) after oxygen. This abundance, together with the unique diversity of organic compounds and their unusual polymer-forming ability at the temperatures commonly encountered on Earth, make this element the chemical basis of all known life.
Characteristics.
The different forms or "allotropes" of carbon (see below) include the hardest naturally occurring substance, diamond, and also one of the softest known substances, graphite. Moreover, it has an affinity for bonding with other small atoms, including other carbon atoms, and is capable of forming multiple stable covalent bonds with such atoms. As a result, carbon is known to form almost ten million different compounds; the large majority of all chemical compounds. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point as its triple point is at 10.8 ± 0.2 MPa and 4,600 ± 300 K (~4,330 °C or 7,820 °F), so it sublimes at about 3,900 K.
Carbon sublimes in a carbon arc which has a temperature of about 5,800 K (5,530 °C; 9,980 °F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest melting point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper that are weaker reducing agents at room temperature.
Carbon compounds form the basis of all known life on Earth, and the carbon-nitrogen cycle provides some of the energy produced by the Sun and other stars. Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures carbon reacts with oxygen to form carbon oxides, and will reduce such metal oxides as iron oxide to the metal. This exothermic reaction is used in the iron and steel industry to control the carbon content of steel:
with sulfur to form carbon disulfide and with steam in the coal-gas reaction:
Carbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel, and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.
As of 2009, graphene appears to be the strongest material ever tested. However, the process of separating it from graphite will require some technological development before it is economical enough to be used in industrial processes.
The system of carbon allotropes spans a range of extremes:
Allotropes.
Atomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with different molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite, glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).
The amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, which is essentially graphite but not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot) and activated carbon. At normal pressures carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a π-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.
At very high pressures carbon forms the more compact allotrope diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, thus making a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance in terms of resistance to scratching. Contrary to the popular belief that "diamonds are forever", they are in fact thermodynamically unstable under normal conditions and transform into graphite. However, due to a high activation energy barrier, the transition into graphite is so extremely slow at room temperature as to be unnoticeable. Under some conditions, carbon crystallizes as lonsdaleite. This form has a hexagonal crystal lattice where all atoms are covalently bonded. Therefore, all properties of lonsdaleite are close to those of diamond.
Fullerenes have a graphite-like structure, but instead of purely hexagonal packing, they also contain pentagons (or even heptagons) of carbon atoms, which bend the sheet into spheres, ellipses or cylinders. The properties of fullerenes (split into buckyballs, buckytubes and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names "fullerene" and "buckyball" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C60 buckminsterfullerene). Carbon nanotubes are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid bucky tube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.
Of the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2 kg/m3. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure -(C:::C)n-. Carbon in this modification is linear with "sp" orbital hybridization, and is a polymer with alternating single and triple bonds. This type of carbyne is of considerable interest to nanotechnology as its Young's modulus is forty times that of the hardest known material – diamond.
Occurrence.
Carbon is the fourth most abundant chemical element in the universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the solar system was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.
In 2014 NASA announced a for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed "a couple of billion years" (according to NASA scientists) after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
In combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 810 gigatonnes of carbon) and dissolved in all water bodies (approximately 36,000 gigatonnes of carbon). Around 1,900 gigatonnes of carbon are present in the biosphere. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal "reserves" (not "resources") amount to around 900 gigatonnes with perhaps 18 000 Gt of resources. Oil reserves are around 150 gigatonnes. Proven sources of natural gas are about 175 1012 cubic metres (representing about 105 gigatonnes carbon), but it is estimated that there are also about 900 1012 cubic metres of "unconventional" gas such as shale gas, representing about 540 gigatonnes of carbon.
Carbon is also locked up as methane hydrates in polar regions and under the seas. Various estimates of the amount of carbon this represents have been made: 500 to 2500 Gt, or 3000 Gt.
In the past, quantities of hydrocarbons were greater. According to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. However, another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2000 Gt.
Carbon is a major component in very large masses of carbonate rock (limestone, dolomite, marble and so on). Coal is the largest commercial source of mineral carbon, accounting for 4,000 gigatonnes or 80% of fossil carbon fuel. It is also rich in carbon – for example, anthracite contains 92–98%.
As for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic "necks", or "pipes". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. There are also deposits in Arkansas, Canada, the Russian Arctic, Brazil and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. However, though diamonds are found naturally, about 30% of all industrial diamonds used in the U.S. are now made synthetically.
Carbon-14 is formed in upper layers of the troposphere and the stratosphere, at altitudes of 9–15 km, by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton.
Carbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in our solar system. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.
Isotopes.
Isotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (12C) forms 98.93% of the carbon on Earth, while carbon-13 (13C) forms the remaining 1.07%. The concentration of 12C is further increased in biological materials because biochemical reactions discriminate against 13C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in NMR experiments is done with the isotope 13C.
Carbon-14 (14C) is a naturally occurring radioisotope which occurs in trace amounts on Earth of up to 1 part per trillion (0.0000000001%), mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158 MeV β− emission. Because of its relatively short half-life of 5730 years, 14C is virtually absent in ancient rocks, but is created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. The abundance of 14C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000 years.
There are 15 known isotopes of carbon and the shortest-lived of these is 8C which decays through proton emission and alpha decay and has a half-life of 1.98739x10−21 s. The exotic 19C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.
Formation in stars.
Formation of the carbon atomic nucleus requires a nearly simultaneous triple collision of alpha particles (helium nuclei) within the core of a giant or supergiant star which is known as the triple-alpha process, as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. This happens in conditions of temperatures over 100 megakelvin and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang. Instead, the interiors of stars in the horizontal branch transform three helium nuclei into carbon by means of this triple-alpha process. In order to be available for formation of life as we know it, this carbon must then later be scattered into space as dust, in supernova explosions, as part of the material which later forms second, third-generation star systems which have planets accreted from such dust. The Solar System is one such third-generation star system. Another of the fusion mechanisms powering stars is the CNO cycle, in which carbon acts as a catalyst to allow the reaction to proceed.
Rotational transitions of various isotopic forms of carbon monoxide (for example, 12CO, 13CO, and C18O) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.
Carbon cycle.
Under terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it somewhere and dispose of it somewhere else. The paths that carbon follows in the environment make up the carbon cycle. For example, plants draw carbon dioxide out of their environment and use it to build biomass, as in carbon respiration or the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, whereas some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; dead plant or animal matter may become petroleum or coal, which can burn with the release of carbon, should bacteria not consume it.
Compounds.
Organic compounds.
Carbon has the ability to form very long chains of interconnecting C-C bonds. This property is called catenation. Carbon-carbon bonds are strong, and stable. This property allows carbon to form an almost infinite number of compounds; in fact, there are more known carbon-containing compounds than all the compounds of the other chemical elements combined except those of hydrogen (because almost all organic compounds contain hydrogen as well).
The simplest form of an organic molecule is the hydrocarbon—a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. Chain length, side chains and functional groups all affect the properties of organic molecules.
Carbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons which are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals and as fossil fuels.
When combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, and aromatic esters, carotenoids and terpenes. With nitrogen it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells.
Inorganic compounds.
Commonly carbon-containing compounds which are associated with minerals or which do not contain hydrogen or fluorine, are treated separately from classical organic compounds; however the definition is not rigid (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide () is similar.
The other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN–), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example it can form the nitride cyanogen molecule ((CN)2), similar to diatomic halides. Other uncommon oxides are carbon suboxide (), the unstable dicarbon monoxide (C2O), carbon trioxide (CO3), cyclopentanepentone (C5O5) cyclohexanehexone (C6O6), and mellitic anhydride (C12O9).
With reactive metals, such as tungsten, carbon forms either carbides (C4–), or acetylides () to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond.
Organometallic compounds.
Organometallic compounds by definition contain at least one carbon-metal bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), η2-alkene compounds (for example, Zeise's salt), and η3-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls exist (for example, tetracarbonylnickel); some workers consider the carbon monoxide ligand to be purely inorganic, and not organometallic.
While carbon is understood to exclusively form four bonds, an interesting compound containing an octahedral hexacoordinated carbon atom has been reported. The cation of the compound is [(Ph3PAu)6C]2+. This phenomenon has been attributed to the aurophilicity of the gold ligands.
History and etymology.
The English name "carbon" comes from the Latin "carbo" for coal and charcoal, whence also comes the French "charbon", meaning charcoal. In German, Dutch and Danish, the names for carbon are "Kohlenstoff", "koolstof" and "kulstof" respectively, all literally meaning coal-substance.
Carbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500 BCE in China, while carbon in the form of charcoal was made around Roman times by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.
In 1722, René Antoine Ferchault de Réaumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram.
In 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave "aerial acid" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. However, in their publication they proposed the name "carbone" (Latin "carbonum") for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.
A new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers – Robert Curl, Harold Kroto and Richard Smalley – received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms lead to the discovery of further exotic allotropes, including glassy carbon, and the realization that "amorphous carbon" is not strictly amorphous.
Production.
Graphite.
Commercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica and feldspars in schists, gneisses and metamorphosed sandstones and limestone as lenses or veins, sometimes of a meter or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made simply by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.
There are three types of natural graphite—amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry "amorphous" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States.
Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.
According to the USGS, world production of natural graphite was 1.1 million tonnes in 2010, to which China contributed 800,000 t, India 130,000 t, Brazil 76,000 t, North Korea 30,000 t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998 million was produced in 2009.
Diamond.
The diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).
Only a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.
Historically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BCE to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.
Diamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the Diamond fields in South Africa. Production has increased over time and now an accumulated total of 4.5 billion carats have been mined since that date. About 20% of that amount has been mined in the last 5 years alone, and during the last ten years 9 new mines have started production while 4 more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.
In the United States, diamonds have been found in Arkansas, Colorado and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.
Today, most commercially viable diamond deposits are in Russia, Botswana, Australia and the Democratic Republic of Congo. In 2005, Russia produced almost one-fifth of the global diamond output, reports the British Geological Survey. Australia has the richest diamantiferous pipe with production reaching peak levels of per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada, Siberia (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe), Brazil, and in Northern and Western Australia.
Applications.
Carbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is used by the petrochemical industry to produce, amongst other things, gasoline and kerosene, through a distillation process, in refineries. Cellulose is a natural, carbon-containing polymer produced by plants in the form of cotton, linen, and hemp. Cellulose is mainly used for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.
The uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a molding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors and as a neutron moderator in nuclear reactors.
Charcoal is used as a drawing material in artwork, for grilling, and in many other uses including iron smelting. Wood, coal and oil are used as fuel for production of energy and space heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials. Carbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.
Carbon black is used as the black pigment in printing ink, artist's oil paint and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification and kitchen extractor hoods and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron. Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron and titanium, are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone and metal.
Diamonds.
The diamond industry can be broadly separated into two basically distinct categories: one dealing with gem-grade diamonds and another for industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets act in dramatically different ways.
A large trade in gem-grade diamonds exists. Unlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity: there is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.
The market for industrial-grade diamonds operates much differently from its gem-grade counterpart. Industrial diamonds are valued mostly for their hardness and heat conductivity, making many of the gemological characteristics of diamond, including clarity and color, mostly irrelevant. This helps explain why 80% of mined diamonds (equal to about 100 million carats or 20 tonnes annually), unsuitable for use as gemstones and known as "bort", are destined for industrial use. In addition to mined diamonds, synthetic diamonds found industrial applications almost immediately after their invention in the 1950s; another 3 billion carats (600 tonnes) of synthetic diamond is produced annually for industrial use. The dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most uses of diamonds in these technologies do not require large diamonds; in fact, most diamonds that are gem-quality except for their small size, can find an industrial use. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances being made in the production of synthetic diamonds, future applications are beginning to become feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable to build microchips from, or the use of diamond as a heat sink in electronics.
Precautions.
Pure carbon has extremely low toxicity to humans and can be handled and even ingested safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract, for example. Consequently once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and Ötzi the Iceman was found to have carbon tattoos that survived during his life and for 5200 years after his death. However, inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease coalworker's pneumoconiosis. Similarly, diamond dust used as an abrasive can do harm if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harmful effects may result from contamination of the carbon particles, with organic chemicals or heavy metals for example, rather than from the carbon itself.
Carbon generally has low toxicity to almost all life on Earth; however, to some creatures it can still be toxic. For instance, carbon nanoparticles are deadly to "Drosophila".
Carbon may also burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air, for example in coal mine waste tips.
In nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250 °C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.
The great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant "Ricinus communis", cyanide (CN−) and carbon monoxide; and such essentials to life as glucose and protein.

</doc>
<doc id="5300" url="http://en.wikipedia.org/wiki?curid=5300" title="Computer data storage">
Computer data storage

Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers.
The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Often the fast, volatile technologies (which lose data when powered off) are referred to as "memory", while slower permanent technologies are referred to as "storage", but these terms can also be used interchangeably. In the Von Neumann architecture, the CPU consists of two main parts: control unit and arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory; the latter performs arithmetic and logical operations on data.
Functionality.
Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialised devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.
Data organization and representation.
A modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 1 or 0. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate "the binary representation of the piece of information", or simply data. For example, the complete works of Shakespeare, about 1250 pages in print, can be stored in about five megabytes (40 million bits) with one byte per character.
Data is encoded by assigning a bit pattern to each character, digit, or multimedia object. Many standards exist for encoding (e.g., character encodings like ASCII, image encodings like JPEG, video encodings like MPEG-4).
By adding bits to each encoded unit, the redundancy allows the computer both to detect errors in coded data and to correct them based on mathematical algorithms. Errors occur regularly in low probabilities due to random bit value flipping, or "physical bit fatigue", loss of the physical bit in storage its ability to maintain distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random bit flip (e.g., due to random radiation) is typically corrected upon detection. A bit, or a group of malfunctioning physical bits (not always the specific defective bit is known; group definition depends on specific storage device) is typically automatically fenced-out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried.
Data compression methods allow in many cases to represent a string of bits by a shorter bit string ("compress") and reconstruct the original string ("decompress") when needed. This utilizes substantially less storage (tens of percents) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data in a database compressed or not.
For security reasons certain types of data (e.g., credit-card information) may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots.
Hierarchy of storage.
Generally, the lower a storage is in the hierarchy, the lesser its bandwidth and the greater its access latency is from the CPU. This traditional division of storage to primary, secondary, tertiary and off-line storage is also guided by cost per bit. In contemporary usage, "memory" is usually semiconductor storage read-write random-access memory, typically DRAM (Dynamic-RAM) or other forms of fast but temporary storage. "Storage" consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down). Historically, "memory" has been called "core", "main memory", "real storage" or "internal memory" while storage devices have been referred to as "secondary storage", "external memory" or "auxiliary/peripheral storage".
Primary storage.
"Primary storage" (or "main memory" or "internal memory"), often referred to simply as "memory", is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.
Historically, early computers used delay lines, Williams tubes, or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic core memory. Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive.
This led to modern random-access memory (RAM). It is small-sized, light, but quite expensive at the same time. (The particular types of RAM used for primary storage are also volatile, i.e. they lose the information when not powered).
As shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM:
Main memory is directly or indirectly connected to the central processing unit via a "memory bus". It is actually two buses (not on the diagram): an address bus and a data bus. The CPU firstly sends a number through an address bus, a number called memory address, that indicates the desired location of data. Then it reads or writes the data itself using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of virtual memory or other tasks.
As the RAM types used for primary storage are volatile (cleared at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile "secondary" storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of "random access").
Many types of "ROM" are not literally "read only", as updates are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some embedded systems run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, rather use large capacities
of secondary storage, which is non-volatile as well, and not as costly.
Recently, "primary storage" and "secondary storage" in some uses refer to what was historically called, respectively, "secondary storage" and "tertiary storage".
Secondary storage.
"Secondary storage" (also known as external memory or auxiliary storage), differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfers the desired data using intermediate area in primary storage. Secondary storage does not lose the data when the device is powered down—it is non-volatile. Per unit, it is typically also two orders of magnitude less expensive than primary storage. Modern computer systems typically have two orders of magnitude more secondary storage than primary storage and data are kept for a longer time there.
In modern computers, hard disk drives are usually used as secondary storage. The time taken to access a given byte of information stored on a hard disk is typically a few thousandths of a second, or milliseconds. By contrast, the time taken to access a given byte of information stored in random-access memory is measured in billionths of a second, or nanoseconds. This illustrates the significant access-time difference which distinguishes solid-state memory from rotating magnetic storage devices: hard disks are typically about a million times slower than memory. Rotating optical storage devices, such as CD and DVD drives, have even longer access times. With disk drives, once the disk read/write head reaches the proper placement and the data of interest rotates under it, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks.
When data reside on disk, block access to hide latency offers a ray of hope in designing efficient external memory algorithms. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based upon sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel in order to increase the bandwidth between primary and secondary memory.
Some other examples of secondary storage technologies are: flash memory (e.g. USB flash drives or keys), floppy disks, magnetic tape, paper tape, punched cards, standalone RAM disks, and Iomega Zip drives.
The secondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories, providing also additional information (called metadata) describing the owner of a certain file, the access time, the access permissions, and other information.
Most computer operating systems use the concept of virtual memory, allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks ("pages") to secondary storage devices (to a swap file or page file), retrieving them later when they are needed. As more of these retrievals from slower secondary storage are necessary, the more the overall system performance is degraded.
Tertiary storage.
"Tertiary storage" or "tertiary memory", provides a third level of storage. Typically it involves a robotic mechanism which will "mount" (insert) and "dismount" removable mass storage media into a storage device according to the system's demands; this data is often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5–60 seconds vs. 1–10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries and optical jukeboxes.
When a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.
"Off-line storage" is a computer data storage on a medium or a device that is not under the control of a processing unit. The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.
Off-line storage is used to transfer information, since the detached medium can be easily physically transported. Additionally, in case a disaster, for example a fire, destroys the original data, a medium in a remote location will probably be unaffected, enabling disaster recovery. Off-line storage increases general information security, since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.
In modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are most popular, and to much lesser extent removable hard disk drives. In enterprise uses, magnetic tape is predominant. Older examples are floppy disks, Zip disks, or punched cards.
Characteristics of storage.
Storage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.
Volatility.
An uninterruptible power supply can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems (e.g., see the EMC Symmetrix) have integrated batteries that maintain volatile storage for several hours.
Fundamental storage technologies.
, the most commonly used data storage technologies are semiconductor, magnetic, and optical, while paper still sees some limited usage. "Media" is a common name for what actually holds the data in the storage device. Some other fundamental storage technologies have also been used in the past or are proposed for development.
Semiconductor.
Semiconductor memory uses semiconductor-based integrated circuits to store information. A semiconductor memory chip may contain millions of tiny transistors or capacitors. Both "volatile" and "non-volatile" forms of semiconductor memory exist. In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor memory or dynamic random access memory. Since the turn of the century, a type of non-volatile semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers.
As early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.
Magnetic.
Magnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is "non-volatile". The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms:
In early computers, magnetic storage was also used as:
Optical.
Optical storage, the typical optical disc, stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is "non-volatile". The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are currently in common use:
Magneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is "non-volatile", "sequential access", slow write, fast read storage used for tertiary and off-line storage.
3D optical data storage has also been proposed.
Paper.
Paper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole.
A few technologies allow people to make marks on paper that are easily read by machine—these are widely used for tabulating votes and grading standardized tests. Barcodes made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it.
Related technologies.
Redundancy.
While a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices:
Device mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in a same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle also recovery from disasters (see disaster recovery above).
Network connectivity.
A secondary or tertiary storage may connect to a computer utilizing computer networks.
This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree.
Robotic storage.
Large quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as tape libraries, and in optical storage field optical jukeboxes, or optical disk libraries per analogy. Smallest forms of either technology containing just one drive device are referred to as autoloaders or autochangers.
Robotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide terabytes or petabytes of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots.
Robotic storage is used for backups, and for high-capacity archives in imaging, medical, and video industries. Hierarchical storage management is a most known archiving strategy of automatically "migrating" long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are "retrieved" back to disk.

</doc>
<doc id="5302" url="http://en.wikipedia.org/wiki?curid=5302" title="Conditional">
Conditional

Conditional may refer to:
In grammar and linguistics:

</doc>
<doc id="5304" url="http://en.wikipedia.org/wiki?curid=5304" title="Cone (disambiguation)">
Cone (disambiguation)

A cone is a basic geometrical shape.
Cone may also refer to:

</doc>
<doc id="5306" url="http://en.wikipedia.org/wiki?curid=5306" title="Chemical equilibrium">
Chemical equilibrium

In a chemical reaction, chemical equilibrium is the state in which both reactants and products are present in concentrations which have no further tendency to change with time. Usually, this state results when the forward reaction proceeds at the same rate as the reverse reaction. The reaction rates of the forward and backward reactions are generally not zero, but equal. Thus, there are no net changes in the concentrations of the reactant(s) and product(s). Such a state is known as dynamic equilibrium.
Historical introduction.
The concept of chemical equilibrium was developed after Berthollet (1803) found that some chemical reactions are reversible. For any reaction mixture to exist at equilibrium, the rates of the forward and backward (reverse) reactions are equal. In the following chemical equation with arrows pointing both ways to indicate equilibrium, A and B are reactant chemical species, S and T are product species, and α, β, σ, and τ are the stoichiometric coefficients of the respective reactants and products:
The equilibrium position of a reaction is said to lie "far to the right" if, at equilibrium, nearly all the reactants are consumed. Conversely the equilibrium position is said to be "far to the left" if hardly any product is formed from the reactants.
Guldberg and Waage (1865), building on Berthollet’s ideas, proposed the law of mass action:
where A, B, S and T are active masses and k+ and k− are rate constants. Since at equilibrium forward and backward rates are equal:
and the ratio of the rate constants is also a constant, now known as an equilibrium constant.
By convention the products form the numerator.
However, the law of mass action is valid only for concerted one-step reactions that proceed through a single transition state and is not valid in general because rate equations do not, in general, follow the stoichiometry of the reaction as Guldberg and Waage had proposed (see, for example, nucleophilic aliphatic substitution by SN1 or reaction of hydrogen and bromine to form hydrogen bromide). Equality of forward and backward reaction rates, however, is a necessary condition for chemical equilibrium, though it is not sufficient to explain why equilibrium occurs.
Despite the failure of this derivation, the equilibrium constant for a reaction is indeed a constant, independent of the activities of the various species involved, though it does depend on temperature as observed by the van 't Hoff equation. Adding a catalyst will affect both the forward reaction and the reverse reaction in the same way and will not have an effect on the equilibrium constant. The catalyst will speed up both reactions thereby increasing the speed at which equilibrium is reached.
Although the macroscopic equilibrium concentrations are constant in time, reactions do occur at the molecular level. For example, in the case of acetic acid dissolved in water and forming acetate and hydronium ions,
a proton may hop from one molecule of acetic acid on to a water molecule and then on to an acetate anion to form another molecule of acetic acid and leaving the number of acetic acid molecules unchanged. This is an example of dynamic equilibrium. Equilibria, like the rest of thermodynamics, are statistical phenomena, averages of microscopic behavior.
Le Chatelier's principle (1884) gives an idea of the behavior of an equilibrium system when changes to its reaction conditions occur. "If a dynamic equilibrium is disturbed by changing the conditions, the position of equilibrium moves to partially reverse the change". For example, adding more S from the outside will cause an excess of products, and the system will try to counteract this by increasing the reverse reaction and pushing the equilibrium point backward (though the equilibrium constant will stay the same).
If mineral acid is added to the acetic acid mixture, increasing the concentration of hydronium ion, the amount of dissociation must decrease as the reaction is driven to the left in accordance with this principle. This can also be deduced from the equilibrium constant expression for the reaction:
If {H3O+} increases {CH3CO2H} must increase and {CH3CO2−} must decrease. The H2O is left out as it is a pure liquid and its concentration is undefined.
A quantitative version is given by the reaction quotient.
J. W. Gibbs suggested in 1873 that equilibrium is attained when the Gibbs free energy of the system is at its minimum value (assuming the reaction is carried out at constant temperature and pressure). What this means is that the derivative of the Gibbs energy with respect to reaction coordinate (a measure of the extent of reaction that has occurred, ranging from zero for all reactants to a maximum for all products) vanishes, signalling a stationary point. This derivative is called the reaction Gibbs energy (or energy change) and corresponds to the difference between the chemical potentials of reactants and products at the composition of the reaction mixture. This criterion is both necessary and sufficient. If a mixture is not at equilibrium, the liberation of the excess Gibbs energy (or Helmholtz energy at constant volume reactions) is the “driving force” for the composition of the mixture to change until equilibrium is reached. The equilibrium constant can be related to the standard Gibbs free energy change for the reaction by the equation
where R is the universal gas constant and T the temperature.
When the reactants are dissolved in a medium of high ionic strength the quotient of activity coefficients may be taken to be constant. In that case the concentration quotient, Kc,
where [A] is the concentration of A, etc., is independent of the analytical concentration of the reactants. For this reason, equilibrium constants for solutions are usually determined in media of high ionic strength. Kc varies with ionic strength, temperature and pressure (or volume). Likewise Kp for gases depends on partial pressure. These constants are easier to measure and encountered in high-school chemistry courses.
Thermodynamics.
At constant temperature and pressure, one must consider the Gibbs free energy, G, while at constant temperature and volume, one must consider the Helmholtz free energy: A, for the reaction; and at constant internal energy and volume, one must consider the entropy for the reaction: S.
The constant volume case is important in geochemistry and atmospheric chemistry where pressure variations are significant. Note that, if reactants and products were in standard state (completely pure), then there would be no reversibility and no equilibrium. The mixing of the products and reactants contributes a large entropy (known as entropy of mixing) to states containing equal mixture of products and reactants. The combination of the standard Gibbs energy change and the Gibbs energy of mixing determines the equilibrium state.
In this article only the constant pressure case is considered. The relation between the Gibbs free energy and the equilibrium constant can be found by considering chemical potentials.
At constant temperature and pressure, the Gibbs free energy, G, for the reaction depends only on the extent of reaction: ξ (Greek letter xi), and can only decrease according to the second law of thermodynamics. It means that the derivative of G with ξ must be negative if the reaction happens; at the equilibrium the derivative being equal to zero.
In general an equilibrium system is defined by writing an equilibrium equation for the reaction
In order to meet the thermodynamic condition for equilibrium, the Gibbs energy must be stationary, meaning that the derivative of G with respect to the extent of reaction: ξ, must be zero. It can be shown that in this case, the sum of chemical potentials of the products is equal to the sum of those corresponding to the reactants. Therefore, the sum of the Gibbs energies of the reactants must be the equal to the sum of the Gibbs energies of the products.
where μ is in this case a partial molar Gibbs energy, a chemical potential. The chemical potential of a reagent A is a function of the
activity, {A} of that reagent.
Substituting expressions like this into the Gibbs energy equation:
Now
At constant pressure and temperature we obtain:
This results in:
By substituting the chemical potentials:
the relationship becomes:
Therefore
At equilibrium formula_26
leading to:
and
Obtaining the value of the standard Gibbs energy change, allows the calculation of the equilibrium constant
Addition of reactants or products.
For a reactional system at equilibrium: formula_30; formula_31.
formula_33
and
formula_34
then
formula_35
formula_37, the reaction quotient decreases.
formula_38 and formula_39 : The reaction will shift to the right (i.e. in the forward direction, and thus more products will form).
formula_41 and formula_42 : The reaction will shift to the left (i.e. in the reverse direction, and thus less products will form).
"Note" that activities and equilibrium constants are dimensionless numbers.
Treatment of activity.
The expression for the equilibrium constant can be rewritten as the product of a concentration quotient, "K"c and an activity coefficient quotient, Γ.
This is a set of "(m+k)" equations in "(m+k)" unknowns (the formula_44 and the formula_45) and may, therefore, be solved for the equilibrium concentrations formula_44 as long as the chemical potentials are known as functions of the concentrations at the given temperature and pressure. (See Thermodynamic databases for pure substances).
This method of calculating equilibrium chemical concentrations is useful for systems with a large number of different molecules. The use of "k" atomic element conservation equations for the mass constraint is straightforward, and replaces the use of the stoichiometric coefficient equations.

</doc>
<doc id="5308" url="http://en.wikipedia.org/wiki?curid=5308" title="Combination">
Combination

In mathematics, a combination is a way of selecting members from a grouping, such that (unlike permutations) the order of selection does not matter. In smaller cases it is possible to count the number of combinations. For example given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange.
More formally, a "k"-combination of a set "S" is a subset of "k" distinct elements of "S". If the set has "n" elements, the number of "k"-combinations is equal to the binomial coefficient
which can be written using factorials as formula_2 whenever formula_3, and which is zero when formula_4.
The set of all "k"-combinations of a set "S" is sometimes denoted by formula_5.
Combinations refer to the combination of "n" things taken "k" at a time without repetition. To refer to combinations in which repetition is allowed, the terms "k"-selection, "k"-multiset, or "k"-combination with repetition are often used. If, in the above example, it was possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.
Although the set of three fruits was small enough to write a complete list of combinations, with large sets this becomes impractical. For example, a poker hand can be described as a 5-combination ("k" = 5) of cards from a 52 card deck ("n" = 52). The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. There are 2,598,960 such combinations, and the chance of drawing any one hand at random is 1 / 2,598,960.
Number of "k"-combinations.
The number of "k"-combinations from a given set "S" of "n" elements is often denoted in elementary combinatorics texts by "C"("n", "k"), or by a variation such as formula_6, formula_7, formula_8 or even formula_9 (the latter form was standard in French, Russian, Chinese and Polish texts). The same number however occurs in many other mathematical contexts, where it is denoted by formula_10 (often read as "n choose k"); notably it occurs as a coefficient in the binomial formula, hence its name binomial coefficient. One can define formula_10 for all natural numbers "k" at once by the relation
from which it is clear that formula_13 and formula_14 for "k" > "n". To see that these coefficients count "k"-combinations from "S", one can first consider a collection of "n" distinct variables "X""s" labeled by the elements "s" of "S", and expand the product over all elements of "S":
it has 2"n" distinct terms corresponding to all the subsets of "S", each subset giving the product of the corresponding variables "X""s". Now setting all of the "X""s" equal to the unlabeled variable "X", so that the product becomes , the term for each "k"-combination from "S" becomes "X""k", so that the coefficient of that power in the result equals the number of such "k"-combinations.
Binomial coefficients can be computed explicitly in various ways. To get all of them for the expansions up to , one can use (in addition to the basic cases already given) the recursion relation
which follows from =; this leads to the construction of Pascal's triangle.
For determining an individual binomial coefficient, it is more practical to use the formula
The numerator gives the number of "k"-permutations of "n", i.e., of sequences of "k" distinct elements of "S", while the denominator gives the number of such "k"-permutations that give the same "k"-combination when the order is ignored.
When "k" exceeds "n"/2, the above formula contains factors common to the numerator and the denominator, and canceling them out gives the relation
This expresses a symmetry that is evident from the binomial formula, and can also be understood in terms of "k"-combinations by taking the complement of such a combination, which is an -combination.
Finally there is a formula which exhibits this symmetry directly, and has the merit of being easy to remember:
where "n"! denotes the factorial of "n". It is obtained from the previous formula by multiplying denominator and numerator by !, so it is certainly inferior as a method of computation to that formula.
The last formula can be understood directly, by considering the "n"! permutations of all the elements of "S". Each such permutation gives a "k"-combination by selecting its first "k" elements. There are many duplicate selections: any combined permutation of the first "k" elements among each other, and of the final ("n" − "k") elements among each other produces the same combination; this explains the division in the formula.
From the above formulas follow relations between adjacent numbers in Pascal's triangle in all three directions:
Together with the basic cases formula_23, these allow successive computation of respectively all numbers of combinations from the same set (a row in Pascal's triangle), of "k"-combinations of sets of growing sizes, and of combinations with a complement of fixed size .
Example of counting combinations.
As a concrete example, one can compute the number of five-card hands possible from a standard fifty-two card deck as:
Alternatively one may use the formula in terms of factorials and cancel the factors in the numerator against parts of the factors in the denominator, after which only multiplication of the remaining factors is required:
Another alternative computation, equivalent to the first, is based on writing
which gives
When evaluated in the following order, , this can be computed using only integer arithmetic. The reason is that when each division occurs, the intermediate result that is produced is itself a binomial coefficient, so no remainders ever occur.
Using the symmetric formula in terms of factorials without performing simplifications gives a rather extensive calculation:
Enumerating "k"-combinations.
One can enumerate all "k"-combinations of a given set of "n" elements in some fixed order, which establishes a bijection from an interval of formula_10 integers with the set of those "k"-combinations. Assuming is itself ordered, for instance }, there are two natural possibilities for ordering its "k"-combinations: by comparing their smallest elements first (as in the illustrations above) or by comparing their largest elements first. The latter option has the advantage that adding a new largest element to will not change the initial part of the enumeration, but just add the new "k"-combinations of the larger set after the previous ones. Repeating this process, the enumeration can be extended indefinitely with "k"-combinations of ever larger sets. If moreover the intervals of the integers are taken to start at 0, then the "k"-combination at a given place "i" in the enumeration can be computed easily from "i", and the bijection so obtained is known as the combinatorial number system. It is also known as "rank"/"ranking" and "unranking" in computational mathematics.
There are many ways to enumerate "k" combinations. One way is to visit all the binary numbers less than formula_30. Chose those numbers having "k" nonzero bits. The positions of these 1 bits in such a number is a specific "k"-combination of the set {1...,"n"}.
Number of combinations with repetition.
A "k"-combination with repetitions, or "k"-multicombination, or multisubset of size "k" from a set "S" is given by a sequence of "k" not necessarily distinct elements of "S", where order is not taken into account: two sequences of which one can be obtained from the other by permuting the terms define the same multiset. In other words, the number of ways to sample "k" elements from a set of "n" elements allowing for duplicates (i.e., with replacement) but disregarding different orderings (e.g. {2,1,2} = {1,2,2}). Associate an index to each element of "S" and think of the elements of "S" as "types" of objects, then we can let formula_31 denote the number of elements of type "i" in a multisubset. The number of multisubsets of size "k" is then the number of nonnegative integer solutions of the Diophantine equation:
If "S" has "n" elements, the number of such "k"-multisubsets is denoted by,
a notation that is analogous to the binomial coefficient which counts "k"-subsets. This expression, "n" multichoose "k", is also given by a binomial coefficient:
This relationship can be easily seen using a representation known as stars and bars. A solution of the above Diophantine equation can be represented by formula_35 "stars", a separator (a "bar"), then formula_36 more stars, another separator, and so on. The total number of stars in this representation is "k" and the number of bars is "n" - 1 (since no separator is needed at the very end). Thus, a string of "k" + "n" - 1 symbols (stars and bars) corresponds to a solution if there are "k" stars in the string. Any solution can be represented by choosing "k" out of positions to place stars and filling the remaining positions with bars. For example, the solution formula_37 of the equation formula_38 can be represented by
formula_39.
The number of such strings is the number of ways to place 10 stars in 13 positions, formula_40 which is the number of 10-multisubsets of a set with 4 elements.
As with binomial coefficients, there are several relationships between these multichoose expressions. For example, for formula_41,
This identity follows from interchanging the stars and bars in the above representation.
Example of counting multisubsets.
For example, if you have four types of donuts ("n" = 4) on a menu to choose from and you want three donuts ("k" = 3), the number of ways to choose the donuts with repetition can be calculated as
This result can be verified by listing all the 3-multisubsets of the set "S" = {1,2,3,4}. This is displayed in the following table. The second column shows the nonnegative integer solutions formula_44 of the equation formula_45 and the last column gives the stars and bars representation of the solutions.
Number of "k"-combinations for all "k".
The number of "k"-combinations for all "k" is the number of subsets of a set of "n" elements. There are several ways to see that this number is 2"n". In terms of combinations, formula_46, which is the sum of the "n"th row (counting from 0) of the binomial coefficients in Pascal's triangle. These combinations (subsets) are enumerated by the 1 digits of the set of base 2 numbers counting from 0 to 2"n"  -  1, where each digit position is an item from the set of "n".
Given 3 cards numbered 1 to 3, there are 8 distinct combinations (subsets), including the empty set:
Representing these subsets (in the same order) as base 2 numbers:
Probability: sampling a random combination.
There are various algorithms to pick out a random combination from a given set or list. Rejection sampling is extremely slow for large sample sizes. One way to select a "k"-combination efficiently from a population of size "n" is to iterate across each element of the population, and at each step pick that element with a dynamically changing probability of formula_48. (see reservoir sampling).

</doc>
<doc id="5309" url="http://en.wikipedia.org/wiki?curid=5309" title="Software">
Software

Computer software, or simply software is any set of machine-readable instructions that directs a computer's processor to perform specific operations. Computer software contrasts with computer hardware, which is the physical component of computers. Computer hardware and software require each other and neither can be realistically used without the other.
Computer software includes computer programs, libraries and their associated documentation. The word software is also sometimes used in a more narrow sense, meaning application software only. Software is stored in computer memory and cannot be touched i.e. it is intangible.
At the lowest level, executable code consists of machine language instructions specific to an individual processor – typically a central processing unit (CPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location inside the computer – an effect that is not directly observable to the user. An instruction may also (indirectly) cause something to appear on a display of the computer system – a state change which should be visible to the user. The processor carries out the instructions in the order they are provided, unless it is instructed to "jump" to a different instruction, or interrupted.
Software written in a machine language is known as "machine code". However, in practice, software is usually written in high-level programming languages that are easier and more efficient for humans to use (closer to natural language) than machine language. High-level languages are translated, using compilation or interpretation or a combination of the two, into machine language. Software may also be written in a low-level assembly language, essentially, a vaguely mnemonic representation of a machine language using a natural language alphabet. Assembly language is translated into machine code using an assembler.
Types of software.
On virtually all computer platforms, software can be grouped into a few broad categories.
Purpose, or domain of use.
Based on the goal, computer software can be divided into:
Programming tools.
Programming tools are also software in the form of programs or applications that software developers (also known as "programmers, coders, hackers" or "software engineers") use to create, debug, maintain (i.e. improve or fix), or otherwise support software. Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.
Software topics.
Architecture.
Users often see things differently from programmers. People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.
Execution.
Computer software has to be "loaded" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to "execute" the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation – moving data, carrying out a computation, or altering the control flow of instructions.
Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly. So, this is sometimes avoided by using "pointers" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.
Quality and reliability.
Software quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs." Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs. Many bugs are discovered and eliminated (debugged) through software testing. However, software testing rarely – if ever – eliminates every bug; some programmers say that "every program has at least one more bug" (Lubarsky's Law). All major software companies, such as Microsoft, Novell and Sun Microsystems, have their own software testing departments with the specific goal of just testing. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be quite large. For instance, NASA has extremely rigorous software testing procedures for many operating systems and communication functions. Many NASA-based operations interact and identify each other through command programs called software. This enables many people who work at NASA to check and evaluate functional systems overall. Programs containing command software enable hardware engineering and system operations to function much easier together.
License.
The software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.
Proprietary software can be divided into two types: 
Open source software, on the other hand, comes with a free software license, granting the recipient the rights to modify and redistribute the software.
Patents.
Software patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a "detailed idea (e.g. an algorithm) on how to implement" a piece of software, or a component of a piece of software. Ideas for useful things that software could "do", and user "requirements", are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either - the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid - although since "all" useful software has effects on the physical world, this requirement may be open to debate.
Software patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers - for example the patent for Aspect-Oriented Programming (AOP), which purported to claim rights over "any" programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the US, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.
Design and implementation.
Design and implementation of software varies depending on the complexity of the software. For instance, design and creation of Microsoft Word software will take much more time than designing and developing Microsoft Notepad because of the difference in functionalities in each one.
Software is usually designed and created (coded/written/programmed) in integrated development environments (IDE) like Eclipse, Emacs and Microsoft Visual Studio that can simplify the process and compile the program. As noted in different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) are categorized for different purposes. For instance, JavaBeans library is used for designing enterprise applications, Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. Underlying computer programming concepts like quicksort, hash table, array, and binary tree can be useful to creating software. When a program is designed, it relies on the API. For instance, if a user is designing a Microsoft Windows desktop application, he/she might use the .NET Windows Forms library to design the desktop application and call its APIs like "Form1.Close()" and "Form1.Show()" to close or open the application and write the additional operations him/herself that it need to have. Without these APIs, the programmer needs to write these APIs him/herself. Companies like Sun Microsystems, Novell, and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.
Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.
A person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning.
Industry and organizations.
A great variety of software companies and programmers in the world comprise a software industry. Software can be quite a profitable industry: Bill Gates, the founder of Microsoft was the richest person in the world in 2009 largely due to his ownership of a significant number of shares in Microsoft, the company responsible for Microsoft Windows and Microsoft Office software products. 
Non-profit software organizations include the Free Software Foundation, GNU Project and Mozilla Foundation. Software standard organizations like the W3C, IETF develop software standards so that most software can interoperate through standards such as XML, HTML and HTTP.
Other well-known large software companies include Oracle, Novell, SAP, Symantec, Adobe Systems, and Corel, while small companies often provide innovation.

</doc>
<doc id="5311" url="http://en.wikipedia.org/wiki?curid=5311" title="Computer programming">
Computer programming

Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable programs. It involves activities such as analysis, understanding, and generically solving such problems resulting in an algorithm, verification of requirements of the algorithm including its correctness and its resource consumption, implementation (commonly referred to as coding) of the algorithm in a target programming language. Source code is written in one or more programming languages (such as C, C++, C#, Java, Python, Smalltalk, JavaScript, etc.). The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solve a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal logic.
Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term "software development" is used for this larger process with the term "programming", "implementation", or "coding" reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.
Overview.
Within software engineering, programming (the "implementation") is regarded as one phase in a software development process.
There is an on-going debate on the extent to which the writing of programs is an art form, a craft, or an engineering discipline. In general, good programming is considered to be the measured application of all three, with the goal of producing an efficient and evolvable software solution (the criteria for "efficient" and "evolvable" vary considerably). The discipline differs from many other technical professions in that programmers, in general, do not need to be licensed or pass any standardized (or governmentally regulated) certification tests in order to call themselves "programmers" or even "software engineers." Because the discipline covers many areas, which may or may not include critical applications, it is debatable whether licensing is required for the profession as a whole. In most cases, the discipline is self-governed by the entities which require the programming, and sometimes very strict environments are defined (e.g. United States Air Force use of AdaCore and security clearance). However, representing oneself as a "professional software engineer" without a license from an accredited institution is illegal in many parts of the world.
Another on-going debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes. This debate is analogous to that surrounding the Sapir–Whorf hypothesis in linguistics and cognitive science, which postulates that a particular spoken language's nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.
History.
Ancient cultures had no conception of computing beyond arithmetic, algebra, and geometry, occasionally aping elements of calculus (e.g. the method of exhaustion). The only mechanical device that existed for numerical computation at the beginning of human history was the abacus, invented in Sumeria circa 2500 BC. Later, the Antikythera mechanism, invented some time around 100 BC in ancient Greece, is the first known mechanical calculator utilizing gears of various sizes and configuration to perform calculations, which tracked the metonic cycle still used in lunar-to-solar calendars, and which is consistent for calculating the dates of the Olympiads. The Kurdish medieval scientist Al-Jazari built programmable automata in 1206 AD. One system employed in these devices was the use of pegs and cams placed into a wooden drum at specific locations, which would sequentially trigger levers that in turn operated percussion instruments. The output of this device was a small drummer playing various rhythms and drum patterns. The Jacquard loom, which Joseph Marie Jacquard developed in 1801, uses a series of pasteboard cards with holes punched in them. The hole pattern represented the pattern that the loom had to follow in weaving cloth. The loom could produce entirely different weaves using different sets of cards. Charles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. The first computer program was written for the Analytical Engine by mathematician Ada Lovelace to calculate a sequence of Bernoulli numbers. The synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to conceive and produce, led to the modern development of computer programming. Development of computer programming accelerated through the Industrial Revolution.
In the 1880s, Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, above, had been for lists of instructions (not data) to drive programmed machines such as Jacquard looms and mechanized musical instruments. "After some initial trials with paper tape, he settled on punched cards..." To process these punched cards, first known as "Hollerith cards" he invented the keypunch, sorter, and tabulator unit record machines. These inventions were the foundation of the data processing industry. In 1896 he founded the "Tabulating Machine Company" (which later became the core of IBM). The addition of a control panel (plugboard) to his 1906 Type I Tabulator allowed it to do different jobs without having to be physically rebuilt. By the late 1940s, there were several unit record calculators, such as the IBM 602 and IBM 604, whose control panels specified a sequence (list) of operations and thus were programmable machines.
The invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.
In 1954, FORTRAN was invented; it was the first high level programming language to have a functional implementation, as opposed to just a design on paper. (A high-level language is, in very general terms, any programming language that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction "higher" than that of an assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. ). The program text, or "source", is converted into machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for "Formula Translation". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)
As time has progressed, computers have made giant leaps in the area of processing power. This has brought about newer programming languages that are more abstracted from the underlying hardware. Popular programming languages of the modern era include ActionScript, C, C++, C#, Haskell, PHP, Java, JavaScript, Objective-C, Perl, Python, Ruby, Smalltalk, SQL, Visual Basic, and dozens more. Although these high-level languages usually incur greater overhead, the increase in speed of modern computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages typically are easier to learn and allow the programmer to develop applications much more efficiently and with less source code. However, high-level languages are still impractical for a few programs, such as those where low-level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, particularly in the United States, Europe, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly subject to offshore outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.
Modern programming.
Quality requirements.
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most relevant:
Readability of source code.
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display.
Algorithmic complexity.
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into "orders" using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
Methodologies.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
Measuring language usage.
It is very difficult to determine what are the most popular of modern programming languages. Some languages are very popular for particular kinds of applications (e.g., COBOL is still strong in the corporate data center, often on large mainframes, FORTRAN in engineering applications, scripting languages in Web development, and C in embedded applications), while some languages are regularly used to write many different kinds of applications. Also many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a previous language with new functionality added (for example C++ adds object-orientedness to C, and Java adds memory management and bytecode to C++, and as a consequence loses efficiency and the ability for low-level manipulation).
Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Debugging.
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems.
Debugging is often done with IDEs like Eclipse, Kdevelop, NetBeans, , and Visual Studio. Standalone debuggers like gdb are also used, and these often provide less of a visual environment, usually using a command line.
Programming languages.
Different programming languages support different styles of programming (called "programming paradigms"). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Allen Downey, in his book "How To Think Like A Computer Scientist", writes:
Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Programmers.
Computer programmers are those who write computer software. Their jobs usually involve:

</doc>
<doc id="5312" url="http://en.wikipedia.org/wiki?curid=5312" title="The Consolation of Philosophy">
The Consolation of Philosophy

The Consolation of Philosophy () is a philosophical work by Boethius, written around the year 524. It has been described as the single most important and influential work in the West on Medieval and early Renaissance Christianity, and is also the last great Western work of the Classical Period.
Description.
"The Consolation of Philosophy" was written during a one-year imprisonment Boethius served while awaiting trial – and eventual horrific execution – for the crime of treason under the Ostrogothic King Theodoric the Great. Boethius was at the very heights of power in Rome and was brought down by treachery. This experience inspired the text, which reflects on how evil can exist in a world governed by God (the problem of theodicy), and how happiness can be attainable amidst fickle fortune, while also considering the nature of happiness and God. It has been described as "by far the most interesting example of prison literature the world has ever seen."
Even though reference is often made to God, the book is not strictly religious. A link is often assumed, yet there is no reference made to Jesus Christ or Christianity or any other specific religion other than a few oblique references to Pauline scripture, such as the symmetry between the opening lines of Book 4 Chapter 3 and 1 Corinthians 9:24. God is however represented not only as an eternal and all-knowing being, but as the source of all Good.
Boethius writes the book as a conversation between himself and Lady Philosophy. She consoles Boethius by discussing the transitory nature of fame and wealth ("no man can ever truly be secure until he has been forsaken by Fortune"), and the ultimate superiority of things of the mind, which she calls the "one true good". She contends that happiness comes from within, and that one's virtue is all that one truly has, because it is not imperilled by the vicissitudes of fortune.
Boethius engages questions such as the nature of predestination and free will, why evil men often prosper and good men fall into ruin, human nature, virtue, and justice. He speaks about the nature of free will versus determinism when he asks if God knows and sees all, or does man have free will. To quote V.E. Watts on Boethius, "God is like a spectator at a chariot race; He watches the action the charioteers perform, but this does not cause them." On human nature, Boethius says that humans are essentially good and only when they give in to “wickedness” do they “sink to the level of being an animal.” On justice, he says criminals are not to be abused, rather treated with sympathy and respect, using the analogy of doctor and patient to illustrate the ideal relationship between prosecutor and criminal.
In the "Consolation", Boethius answered religious questions without reference to Christianity, relying solely on natural philosophy and the Classical Greek tradition. He believed in the correspondence between faith and reason. The truths found in Christianity would be no different from the truths found in philosophy. In the words of Henry Chadwick, "If the "Consolation" contains nothing distinctively Christian, it is also relevant that it contains nothing specifically pagan either...[it] is a work written by a Platonist who is also a Christian, but is not a Christian work."
Influence.
From the Carolingian epoch to the end of the Middle Ages and beyond, this was the most widely copied work of secular literature in Europe. It was one of the most popular and influential philosophical works, read by statesmen, poets, and historians, as well as of philosophers and theologians. It is through Boethius that much of the thought of the Classical period was made available to the Western Medieval world. It has often been said Boethius was the “last of the Romans and the first of the Scholastics”.
The philosophical message of the book fits well with the religious piety of the Middle Ages. Readers were encouraged not to seek worldly goods such as money and power, but to seek internalized virtues. Evil had a purpose, to provide a lesson to help change for good; while suffering from evil was seen as virtuous. Because God ruled the universe through Love, prayer to God and the application of Love would lead to true happiness. The Middle Ages, with their vivid sense of an overruling fate, found in Boethius an interpretation of life closely akin to the spirit of Christianity. "The Consolation of Philosophy" stands, by its note of fatalism and its affinities with the Christian doctrine of humility, midway between the pagan philosophy of Seneca the Younger and the later Christian philosophy of consolation represented by Thomas Aquinas.
The book is heavily influenced by Plato and his dialogues (as was Boethius himself). Its popularity can in part be explained by its Neoplatonic and Christian ethical messages, although current scholarly research is still far from clear exactly why and how the work became so vastly popular in the Middle Ages. 
Translations into the vernacular were done by famous notables, including King Alfred (Old English), Jean de Meun (Old French), Geoffrey Chaucer (Middle English), Queen Elizabeth I (Early Modern English), and Notker Labeo (Old High German).
Found within the "Consolation" are themes that have echoed throughout the Western canon: the female figure of wisdom that informs Dante, the ascent through the layered universe that is shared with Milton, the reconciliation of opposing forces that find their way into Chaucer in The Knight's Tale, and the Wheel of Fortune so popular throughout the Middle Ages.
Citations from it occur frequently in Dante's "Divina Commedia". Of Boethius, Dante remarked "“The blessed soul who exposes the deceptive world to anyone who gives ear to him.”
Boethian influence can be found nearly everywhere in Geoffrey Chaucer's poetry, e.g. in "Troilus and Criseyde", "The Knight's Tale", "The Clerk's Tale", "The Franklin's Tale", "The Parson's Tale" and "The Tale of Melibee", in the character of Lady Nature in "The Parliament of Fowls" and some of the shorter poems, such as "Truth", "The Former Age" and "Lak of Stedfastnesse". Chaucer translated the work in his "Boece".
The Italian composer Luigi Dallapiccola used some of the text in his choral work "Canti di prigionia" (1938). The Australian composer Peter Sculthorpe quoted parts of it in his opera or music theatre work "Rites of Passage" (1972–73), which was commissioned for the opening of the Sydney Opera House but was not ready in time.
Tom Shippey in "The Road to Middle-earth" says how “Boethian” much of the treatment of evil is in Tolkien's "The Lord of the Rings". Shippey says that Tolkien knew well the translation of Boethius that was made by King Alfred and he quotes some “Boethian” remarks from Frodo, Treebeard and Elrond.
Boethius and "Consolatio Philosophiae" are cited frequently by the main character Ignatius J. Reilly in the Pulitzer Prize-winning "A Confederacy of Dunces" (1980).
Boethius is also referenced in the 2002 film "24 Hour Party People", when Tony Wilson's character is recited an excerpt of "The Consolation of Philosophy" by a panhandler to whom he has just given pocket change. This reference is related to viewing history as a wheel, with the fortunes of man rising and falling as cyclically as its spokes. It speaks largely to the wheel's allegorical metaphor to the vicissitudes of life. 
It is a prosimetrical text, meaning that it is written in alternating sections of prose and metered verse. In the course of the text, Boethius displays a virtuosic command of the forms of Latin poetry. It is classified as a Menippean satire, a fusion of allegorical tale, platonic dialogue, and lyrical poetry.
In the 20th century there were close to four hundred manuscripts still surviving, a testament to its former popularity.

</doc>
<doc id="5313" url="http://en.wikipedia.org/wiki?curid=5313" title="Crouching Tiger, Hidden Dragon">
Crouching Tiger, Hidden Dragon

Crouching Tiger, Hidden Dragon is a 2000 martial arts film. An American-Chinese-Hong Kong-Taiwanese co-production, the film was directed by Ang Lee and featured an international cast of ethnic Chinese actors, including Chow Yun-fat, Michelle Yeoh, Zhang Ziyi and Chang Chen. The film was based on the fourth novel in a pentalogy, known in China as the "Crane Iron Pentalogy", by wuxia novelist Wang Dulu. The martial arts and action sequences were choreographed by Yuen Wo Ping.
Made on a US$17 million budget, with dialogue in Mandarin, "Crouching Tiger, Hidden Dragon" became a surprise international success, grossing $213.5 million. It grossed US$128 million in the United States, becoming the highest-grossing foreign-language film in American history. It has won over 40 awards. The film won the Academy Award for Best Foreign Language Film (Taiwan) and three other Academy Awards, and was nominated for six other Academy Awards, including Best Picture. The film also won four BAFTAs and two Golden Globe Awards, one for Best Foreign Film. Along with its awards success, "Crouching Tiger" continues to be hailed as one of the greatest and most influential foreign language films in the United States, especially coming out of China. It has been praised for its martial arts sequences, story, and cinematography.
Plot.
The film is set in the Qing Dynasty during the 43rd year (1779) of the reign of the Qianlong Emperor. Li Mu Bai (Chow Yun-fat) is an accomplished Wudang swordsman. Long ago, his master was murdered by Jade Fox (Cheng Pei-pei), a woman who sought to learn Wudang skills. Mu Bai is also a good friend of Yu Shu Lien (Michelle Yeoh), a female warrior. Mu Bai and Shu Lien have developed feelings for each other, but they have never acknowledged or acted on them. Mu Bai, intending to give up his warrior life, asks Shu Lien to transport his sword, also referred to as the "Green Destiny", to the city of Beijing, as a gift for their friend Sir Te (Sihung Lung). At Sir Te's estate, Shu Lien meets Jen (Zhang Ziyi), the daughter of Governor Yu (Li Fazeng), a visiting Manchu aristocrat. Jen, destined for an arranged marriage and yearning for adventure, seems envious of Shu Lien's warrior lifestyle.
One evening, a masked thief sneaks into Sir Te's estate and steals the sword. Mu Bai and Shu Lien trace the theft to Governor Yu's compound and learn that Jade Fox has been posing as Jen's governess for many years. Mu Bai makes the acquaintance of Inspector Tsai (Wang Deming), a police investigator from the provinces, and his daughter May (Li Li), who have come to Peking in pursuit of Fox. Fox challenges the pair and Sir Te's servant Master Bo (Gao Xi'an) to a showdown that night. Following a protracted battle, the group is on the verge of defeat when Mu Bai arrives and outmaneuvers Fox. Before Mu Bai can kill Fox, the masked thief reappears and partners with Fox to fight. Fox resumes the fight and kills Tsai before fleeing with the thief (who is revealed to be Fox's protegée, Jen). After seeing Jen fight Mu Bai, Fox realizes Jen had been secretly studying the Wudang manual and has surpassed her in skill.
At night, a desert bandit named Lo (Chang Chen) breaks into Jen's bedroom and asks her to leave with him. A flashback reveals that in the past, when Governor Yu and his family were traveling in the western deserts, Lo and his bandits had raided Jen's caravan and Lo had stolen her comb. She chased after him, following him to his desert cave seemingly in a quest to get her comb back. However, the pair soon fell passionately in love. Lo eventually convinced Jen to return to her family, though not before telling her a legend of a man who jumped off a cliff to make his wishes come true. Because the man's heart was pure, he did not die. Lo came to Peking to persuade Jen not to go through with her arranged marriage. However, Jen refuses to leave with him. Later, Lo interrupts Jen's wedding procession, begging her to come away with him. Nearby, Shu Lien and Mu Bai convince Lo to wait for Jen at Mount Wudang, where he will be safe from Jen's family, who are furious with him. Jen runs away from her husband on the wedding night before the marriage could be consummated. Disguised in male clothing, she is accosted at an inn by a large group of warriors; armed with the "Green Destiny" and her own superior skill, she emerges victorious.
Jen visits Shu Lien, who tells her that Lo is waiting for her at Mount Wudang. After an angry dispute, the two women engage in a duel. Wielding the "Green Destiny", Jen destroys each weapon that Shu Lien wields until losing to a broken sword held at her neck. When Shu Lien shows mercy and lowers the sword, Jen injures Shu Lien's arm. Mu Bai arrives and pursues Jen into a bamboo forest. Following a duel where Mu Bai regains possession of the "Green Destiny", he decides to throw the sword over a waterfall. In pursuit, Jen dives into an adjoining river to retrieve the sword and is then rescued by Fox. Fox puts Jen into a drugged sleep and places her in a cavern; Mu Bai and Shu Lien discover her there. Fox suddenly reappears and attacks the others with poisoned darts. Mu Bai blocks the needles with his sword and avenges his master's death by mortally wounding Fox, only to realize that one of the darts hit him in the neck. Fox dies, confessing that her goal had been to kill Jen, because she was furious that Jen hid the secrets of Wudang from her.
As Jen exits to retrieve an ingredient for the antidote for the poisoned dart, Mu Bai prepares to die. With his last breaths, he finally confesses his love for Shu Lien. He dies in her arms as Jen returns, too late to save him. The "Green Destiny" is returned to Sir Te. Jen later goes to Mount Wudang and spends one last night with Lo. The next morning, Lo finds Jen standing on a balcony overlooking the edge of the mountain. In an echo of the legend that they spoke about in the desert, she asks him to make a wish. He complies, wishing for them to be together, back in the desert. Jen then leaps over the side of the mountain.
Themes and interpretations.
Title.
The name "Crouching Tiger Hidden Dragon" is a literal translation of the Chinese idiom "臥虎藏龍" which describes a place or situation that is full of danger. It is from a poem of the ancient Chinese poet Yu Xin's (513-581) that reads "暗石疑藏虎，盤根似臥龍", which means "behind the rock in the dark probably hides a tiger, and the coiling giant root resembles a crouching dragon." The last character in Xiaohu and Jiaolong's names mean "Tiger" and "Dragon" respectively.
Teacher-student relationship.
A teacher's desire to have a worthy student, the obligations between a student and a master, and tensions in these relationships are central to the characters' motives, conflicts between the characters, and the unfolding of the film's plot. Li Mu Bai is burdened with the responsibility for avenging his master's death, and turns his back on retirement to live up to this obligation. His fascination with the prospect of having Jen as a disciple also motivates his behavior, and that of Jade Fox.
Regarding conflicts in the student-teacher relationship, the potential for exploitation created by the subordinate position of the student and the tensions that exist when a student surpasses or resists a teacher are explored. Jen hides her mastery of martial arts from her teacher, Jade Fox, which leads both to their parting of ways and to Jade Fox's attempt on Jen's life. At the same time, Jade Fox's own unorthodox relationship with a Wudang master (who she claims would not teach her, but did take sexual advantage of her) brought her to a life of crime. At times, Li Mu Bai and Jen's conversations more than hint that the desire for a teacher-student relationship could turn into a romantic relationship. Jen responds to these feelings, and her desire to not submit to a teacher, by turning away from Li Mu Bai when she jumps in the lake after the "Green Destiny".
Poison.
Poison is also a significant theme in the film. In the world of martial arts, poison is considered the act of one who is too cowardly and dishonorable to fight; and indeed, the only character that explicitly fits these characteristics is Jade Fox. The poison is a weapon of her bitterness, and quest for vengeance: she poisons the master of Wudang, attempts to poison Jen and succeeds in killing Mu Bai using a poisoned needle.
However, the poison is not only of the physical sort: Jade Fox’s tutelage of Jen has left Jen spiritually poisoned, which can be seen in the lying, stealing and betrayal Jen commits. Even though she is the one who initially trained Jen, Jen is never seen to use poison herself. This indicates that there is hope yet to reform her and integrate her into society. In further play on this theme by the director, Jade Fox, as she dies, refers to the poison from a young child, "the deceit of an eight-year-old girl", obviously referring to what she considers her own spiritual poisoning by her young apprentice Jen. Li Mu Bai himself warns that without guidance, Jen could become a "poison dragon".
Production.
Filming.
Although its Academy Award was presented to Taiwan, "Crouching Tiger, Hidden Dragon" was in fact an international co-production between companies in four regions: the Chinese company China Film Co-Production Corporation; the American companies Columbia Pictures Film Production Asia, Sony Pictures Classics and Good Machine; the Hong Kong company EDKO Film; and the Taiwanese Zoom Hunt International Productions Company, Ltd; as well as the unspecified United China Vision, and Asia Union Film & Entertainment Ltd., created solely for this film.
The film was made in Beijing, with location shooting in the Anhui, Hebei, Jiangsu and Xinjiang provinces of China. The first phase of shooting was in the Gobi Desert where it would consistently rain. Director Ang Lee noted that "I didn't take one break in eight months, not even for half a day. I was miserable—I just didn't have the extra energy to be happy. Near the end, I could hardly breathe. I thought I was about to have a stroke." The stunt work was mostly performed by the actors themselves and Ang Lee stated in an interview that computers were used "only to remove the safety wires that held the actors". "Most of the time you can see their faces," he added, "That's really them in the trees."
Another compounding issue were the varying accents of the four lead actors: Chow Yun-fat is from Hong Kong and spoke Cantonese natively and Michelle Yeoh is from Malaysia and spoke English. Only Zhang Ziyi spoke with a native Mandarin accent that Ang Lee wanted. Chow Yun Fat said that on "the first day [of shooting] I had to do 28 takes just because of the language. That's never happened before in my life."
Because the film specifically targeted Western audiences rather than the domestic audiences who were already used to Wuxia films, English subtitles were needed. Ang Lee, who was educated in the West, personally edited the subtitles to ensure they were satisfactory for Western audiences.
Soundtrack.
The score was composed by Tan Dun, originally performed by Shanghai Symphony Orchestra, Shanghai National Orchestra, and Shanghai Percussion Ensemble. It also features many solo passages for cello played by Yo-Yo Ma. The "last track" ("A Love Before Time") features Coco Lee. The music for the entire film was produced in two weeks.
Marketing.
Video game.
The film was also adapted into a video game.
Novels.
Originally written as a novel series by Wang Du Lu starting in the late 1930s, the film is adapted from the storyline of the fourth book in the series, "Crouching Tiger, Hidden Dragon".
Comics.
A comic series was developed from the plot of the film as well.
Television.
A Taiwanese television series based on the original novel was produced. It was later compiled into a DVD, "New Crouching Tiger, Hidden Dragon", for the West in 2004. The DVD film was over an hour and half longer than the original theatrical film.
Reception.
Critical response.
"Crouching Tiger, Hidden Dragon" was very well received in the Western world, receiving critical acclaim and numerous awards. The review aggregator Rotten Tomatoes reported that 97% of critics gave the film positive reviews, based on 150 reviews, while Metacritic reported the film had an average score of 93 out of 100, based on 31 reviews.
Some Chinese-speaking viewers were bothered by the accents of the leading actors. Neither Chow (a native Cantonese speaker) nor Yeoh (who was born and raised in Malaysia) speaks Mandarin as a mother tongue. All four main actors spoke with different accents: Chow speaks with a Cantonese accent; Yeoh with a Malaysian accent; Chang Chen a Taiwanese accent; and Zhang Ziyi a Beijing accent. Yeoh responded to this complaint in a December 28, 2000, interview with "Cinescape". She argued that "My character lived outside of Beijing, and so I didn't have to do the Beijing accent". When the interviewer, Craig Reid, remarked that "My mother-in-law has this strange Sichuan-Mandarin accent that's hard for me to understand", Yeoh responded: "Yes, provinces all have their very own strong accents. When we first started the movie, Cheng Pei Pei was going to have her accent, and Chang Zhen was going to have his accent, and this person would have that accent. And in the end nobody could understand what they were saying. Forget about us, even the crew from Beijing thought this was all weird".
The film led to a boost in popularity of Chinese wuxia films in the western world, where they were previously little known, and led to films such as "House of Flying Daggers" and "Hero" marketed towards western audiences. The film also provided the breakthrough role for Zhang Ziyi's career, who noted that:
The character of Lo, or "Dark Cloud" the desert bandit, influenced the development of the protagonist of the "Prince of Persia" series of video games.
The film is ranked at No. 497 on "Empire" magazine's 2008 list of the 500 greatest movies of all time and at No. 66 in the magazine's 100 Best Films of World Cinema, published in 2010.
Box office.
The film premiered in cinemas on December 8, 2000 in limited release within the US. During its opening weekend, the film opened in 15th place grossing $663,205 in business, showing at 16 locations. On January 12, 2001, "Crouching Tiger, Hidden Dragon" premiered in cinemas in wide release throughout the US grossing $8,647,295 in business, ranking in 6th place. The film, "Save the Last Dance" came in 1st place during that weekend grossing $23,444,930. The film's revenue dropped by almost 30% in its second week of release, earning $6,080,357. For that particular weekend, the film fell to 8th place screening in 837 theaters. "Save the Last Dance", remained unchanged in first place grossing $15,366,047 in box office revenue. During its final week in release, "Crouching Tiger, Hidden Dragon" opened in a distant 50th place with $37,233 in revenue. The film went on to top out domestically at $128,078,872 in total ticket sales through a 31-week theatrical run. Internationally, the film took in an additional $85,446,864 in box office business for a combined worldwide total of $213,525,736. For 2000 as a whole, the film would cumulatively rank at a worldwide box office performance position of 19.
Accolades.
Gathering widespread critical acclaim at the Toronto and New York film festivals, the film also became a favorite when Academy Awards nominations were announced in 2001. The film was however screened out of competition at the 2000 Cannes Film Festival.
Sequel.
In January 2013, it was reported that a sequel to the movie would begin shooting in May, with Harvey Weinstein producing. It is to have fight choreography by Yuen Woo Ping and a script by John Fusco, which will be based on the fifth and final book of the Crane-Iron Series, "Iron Knight, Silver Vase".
On March 18, 2013, actor Donnie Yen confirmed rumors that he had been offered a role in the new film. Around the same time, there were also conflicting reports on whether Michelle Yeoh has been approached to reprise her role of Yu Shu Lien.
On May 16, 2013 it was officially announced that the sequel had been officially greenlit. The film was retitled "Crouching Tiger, Hidden Dragon - The Green Destiny" (it was initially announced as "Iron Knight, Silver Vase" - the same title from its source material). Donnie Yen was confirmed to star as Silent Wolf while Michelle Yeoh was confirmed to be reprising her role as Yu Shu Lien.
On August 20, 2013, it was reported that Ziyi Zhang is in talks to reprise her role as Jen Yu. But Zhang's agent Ji Lingling told the media that was not true and said: "Zhang would reprise her role only if the director was Ang Lee."
On June 10, 2014, it was announced that Nicholas Tse was offered the role of "Iron Knight" (the son of Jen Yu and Lo) while Xia Zitong was confirmed to star as "Silver Vase".
While it was initially announced that production on the film will begin June 2014 and will shoot in New Zealand and China, shooting was later bumped to August 2014.
On June 16, 2014, it was announced that the film will be a co-production between Pegasus Media, China Film Group Corporation, and the Weinstein Company, with the film expected to be released in 2016.
On July 30, 2014, actor Harry Shum, Jr. is cast in the role of Tie-Fang.
In September 2014, it was announced that this sequel will be released simultaneously on Netflix and select IMAX theatres on August 28, 2015.
On September 30, 2014, three major theater companies, Regal Entertainment Group, Cinemark Theatres and AMC Theatres announced that they would not carry the film in any of their cinemas.
References.
The theme of Janet Jackson's song "China Love" was related to the film by MTV News, in which Jackson sings of the daughter of an emperor in love with a warrior, unable to sustain relations when forced to marry into royalty.
The Indian film "Magadheera" copies its beginning scene from this film.
There is an episode of Jimmy Neutron called "Crouching Jimmy, Hidden Sheen" which parodies the film.

</doc>
<doc id="5314" url="http://en.wikipedia.org/wiki?curid=5314" title="Charlemagne">
Charlemagne

Charlemagne (; 2 April 742/747/74828 January 814), also known as Charles the Great (, , ) or Charles I, was the King of the Franks from 768, the King of Italy from 774, and from 800 the first emperor in western Europe since the collapse of the Western Roman Empire three centuries earlier. The expanded Frankish state he founded is called the Carolingian Empire.
The oldest son of Pepin the Short and Bertrada of Laon, Charlemagne became king in 768 following the death of his father. He was initially co-ruler with his brother Carloman I. Carloman's sudden death in 771 under unexplained circumstances left Charlemagne as the undisputed ruler of the Frankish Kingdom. Charlemagne continued his father's policy towards the papacy and became its protector, removing the Lombards from power in northern Italy, and leading an incursion into Muslim Spain. He also campaigned against the peoples to his east, Christianizing them upon penalty of death, at times leading to events such as the Massacre of Verden. Charlemagne reached the height of his power in 800 when he was crowned "emperor" by Pope Leo III on Christmas Day at Old St. Peter's Basilica.
Called the "Father of Europe" ("pater Europae"), Charlemagne united most of Western Europe for the first time since the Roman Empire. His rule spurred the Carolingian Renaissance, a period of cultural and intellectual activity within the Catholic Church. Both the French and German monarchies considered their kingdoms to be descendants of Charlemagne's empire.
Charlemagne died in 814, having ruled as emperor for just over thirteen years. He was laid to rest in his imperial capital of Aachen in what is today Germany. His son Louis the Pious succeeded him.
Political background.
By the 6th century, the western Germanic Franks had been Christianised, and Francia, ruled by the Merovingians, was the most powerful of the kingdoms that succeeded the Western Roman Empire. Following the Battle of Tertry, however, the Merovingians declined into a state of powerlessness, for which they have been dubbed the "rois fainéants" ("do-nothing kings"). Almost all government powers of any consequence were exercised by their chief officer, the mayor of the palace.<ref name=France/Neustria></ref>
In 687, Pippin of Herstal, mayor of the palace of Austrasia, ended the strife between various kings and their mayors with his victory at Tertry and became the sole governor of the entire Frankish kingdom. Pippin himself was the grandson of two of the most important figures of the Austrasian Kingdom, Saint Arnulf of Metz and Pippin of Landen. Pippin of Herstal was eventually succeeded by his illegitimate son Charles, later known as Charles Martel (Charles the Hammer).
After 737, Charles governed the Franks without a king on the throne but declined to call himself "king". Charles was succeeded in 741 by his sons Carloman and Pepin the Short, the father of Charlemagne. To curb separatism in the periphery of the realm, in 743 the brothers placed on the throne Childeric III, who was to be the last Merovingian king. After Carloman resigned office in 746 to enter the church by preference as a monk, Pepin brought the question of the kingship before Pope Zachary, asking whether it was logical for a king to have no royal power. The pope handed down his decision in 749. He decreed that it was better for Pepin, who had the powers of high office as Mayor, to be called king, so as not to confuse the hierarchy. He therefore ordered him to become "true king."<ref name=France/Pippin></ref>
In 750, Pepin was elected by an assembly of the Franks, anointed by the archbishop, and then raised to the office of king. Branding Childeric III as "the false king," the Pope ordered him into a monastery. Thus was the Merovingian dynasty replaced by the Carolingian dynasty, named after Pepin's father, Charles Martel. In 753 Pope Stephen II fled from Italy to Francia appealing for assistance for the rights of St. Peter to Pepin. He was supported in this appeal by Carloman, Charles' brother. In return the pope could only provide legitimacy, which he did by again anointing and confirming Pepin, this time adding his young sons Carolus and Carloman to the royal patrimony, now heirs to the great realm that already covered most of western and central Europe. In 754 Pepin accepted the Pope's invitation to visit Italy on behalf of St. Peter's rights, dealing successfully with the Lombards.
Under the Carolingians, the Frankish kingdom spread to encompass an area including most of Western Europe; the division of the kingdom formed the bases for modern France and Germany. The religious, political, and artistic evolutions originating from a centrally positioned Francia made a defining imprint on the whole of Europe.
Rise to power.
Early life.
Date of birth.
The most likely date of Charlemagne's birth is reconstructed from several sources. The date of 742 — calculated from Einhard's date of death of January 814 at age 72 - predates the marriage of his parents in 744. The year given in the "Annales Petaviani", 747, would be more likely, except that it contradicts Einhard and a few other sources in making Charlemagne less than a seventy years old at his death. The month and day of April 2 is established by a calendar from Lorsch Abbey.
In 747, that day fell on Easter, a coincidence that likely would have been remarked upon by chroniclers but was not. If Easter was being used as the beginning of the calendar year, then 2 April 747 could have been, by modern reckoning, 2 April 748 (not on Easter). The date favored by the preponderance of evidence is 2 April 742, based on Charlemagne's being a septuagenarian at the time of his death. This date would appear to support the idea that Charlemagne was born illegitimate, which is not, however, mentioned by Einhard.
Place of birth.
Charlemagne’s exact birthplace is unknown, although historians have suggested Aachen in modern-day Germany, and Liège (Herstal) in present-day Belgium as possible locations. Aachen and Liège are close to the region from where both the Merovingian and Carolingian families originated. Other cities have been suggested, including Düren, Gauting, Mürlenbach, Quierzy and Prüm. No definitive evidence as to which is the right candidate exists.
Charlemagne was the eldest child of Pepin the Short (714 – 24 September 768, reigned from 751) and his wife Bertrada of Laon (720 – 12 July 783), daughter of Caribert of Laon and Bertrada of Cologne. Records name only Carloman, Gisela, and three short-lived children named Pepin, Chrothais and Adelais as his younger siblings.
The ambiguous high office.
The most powerful officers of the Frankish people, the Mayor of the Palace (Maior Domus) and one or more kings (rex, reges), were appointed by election of the people; that is, no regular elections were held, but they were held as required to elect officers "ad quos summa imperii pertinebat", "to whom the highest matters of state pertained." Evidently interim decisions could be made by the Pope, which ultimately needed to be ratified using an assembly of the people, which met once a year.
Before he was elected king in 750, Pepin the Short was initially a Mayor, a high office he held "as though hereditary" (velut hereditario fungebatur). Einhard explains that "the honor" was usually "given by the people" to the distinguished, but Pepin the Great and his brother Carloman the Wise received it as though hereditary, as did their father, Charles Martel. There was, however, a certain ambiguity about quasi-inheritance. The office was treated as joint property: one Mayorship held by two brothers jointly. Each, however, had his own geographic jurisdiction. When Carloman decided to resign, becoming ultimately a Benedictine at Monte Cassino, the question of the disposition of his quasi-share was settled by the pope. He converted the Mayorship into a Kingship and awarded the joint property to Pepin, who now had the full right to pass it on by inheritance.
This decision was not accepted by all members of the family. Carloman had consented to the temporary tenancy of his own share, which he intended to pass on to his own son, Drogo, when the inheritance should be settled at someone's death. By the Pope's decision, in which Pepin had a hand, Drogo was to be disqualified as an heir in favor of his cousin Charles. He took up arms in opposition to the decision and was joined by Grifo, a half-brother of Pepin and Carloman, who had been given a share by Charles Martel, but was stripped of it and held under loose arrest by his half-brothers after an attempt to seize their shares by military action. By 753 all was over. Grifo perished in combat in the Battle of Saint-Jean-de-Maurienne while Drogo was hunted down and taken into custody.
On the death of Pepin, 24 September 768, the kingship passed jointly to his sons, "with divine assent" (divino nutu). According to the "Life", Pepin died in Paris. The Franks "in general assembly" (generali conventu) gave them both the rank of king (reges) but "partitioned the whole body of the kingdom equally" (totum regni corpus ex aequo partirentur). The "annals" tell a slightly different version. The king died at St. Denis, which is, however, still in Paris. The two "lords" (domni) were "elevated to kingship" (elevati sunt in regnum), Charles on 9 October in Noyon, Carloman on an unspecified date in Soissons. If born in 742, Charles was 26 years old, but he had been campaigning at his father's right hand for several years, which may help to account for his military skill. Carloman was 17.
The language in either case suggests that there were not two inheritances, which would have created distinct kings ruling over distinct kingdoms, but a single joint inheritance and a joint kingship tenanted by two equal kings, Charles and his brother Carloman. As before, distinct jurisdictions were awarded. Charles received Pepin's original share as Mayor: the outer parts of the kingdom bordering on the sea, namely Neustria, western Aquitaine, and the northern parts of Austrasia; while Carloman was awarded his uncle's former share, the inner parts: southern Austrasia, Septimania, eastern Aquitaine, Burgundy, Provence, and Swabia, lands bordering Italy. The question whether these jurisdictions were joint shares reverting to the other brother if one brother died or were inherited property passed on to the descendants of the brother who died was never definitely settled by the Frankish people. It came up repeatedly over the succeeding decades until the grandsons of Charlemagne created distinct sovereign kingdoms.
Aquitanian rebellion.
An inheritance in the countries formerly under Roman law (ius or iustitia) represented not only a transmission of the properties and privileges but also the encumbrances and obligations attached to the inheritance. Pepin at his death had been in process of building an empire, a difficult task. According to Russell:"In those times, to build a kingdom from an aggregation of small states was itself no great difficulty ... But to keep the state intact after it had been formed was a colossal task ... Each of the minor states ... had its little sovereign ... who ... gave himself chiefly to ... plotting, pillaging and fighting."
Formation of a new Aquitania.
Aquitania under Rome had been in southern Gaul, Romanized and speaking a Romance language. Similarly Hispania had been populated by peoples who spoke various languages, including Celtic, but the area was now populated entirely by Romance language speakers. Between Aquitania and Hispania were the Euskaldunak, Latinized to Vascones, or Basques, living in Basque country, Vasconia, which extended, according to the distributions of place names attributable to the Basques, most densely in the western Pyrenees but also as far south as the upper Ebro River in Spain and as far north as the Garonne River in France. The French name, Gascony, derives from Vasconia. The Romans were never able to entirely subject Vasconia. The parts they did, in which they placed the region's first cities, were sources of legions in the Roman army valued for their fighting abilities. The border with Aquitania was Toulouse.
At about 660 the Duchy of Vasconia united with the Duchy of Aquitania to form a single kingdom under Felix of Aquitaine, governing from Toulouse. This was a joint kingship with a 28-year-old Basque king, Lupus I. The kingdom was sovereign and independent. On the one hand Vasconia gave up predation to become a player on the field of European politics. On the other, whatever arrangements Felix had made with the weak Merovingians were null and void. At Felix's death in 670 the joint property of the kingship reverted entirely to Lupus. As the Basques had no law of joint inheritance, but practiced primogeniture, Lupus in effect founded a hereditary dynasty of Basque kings of an expanded Aquitania.
Acquisition of Aquitania by the Carolingians.
The Latin chronicles on the end of Visigothic Hispania leave much to be desired, such as identification of characters, filling in the gaps, and reconciliation of numerous contradictions. The Saracen (Muslim) sources, however, present a more coherent view, such as in the "Ta'rikh iftitah al-Andalus" ("History of the Conquest of al-Andalus") by Ibn al-Qūṭiyya (a name meaning "the son of the Gothic woman," referring to the granddaughter of the last king of all Visigothic Spain, who married a Saracen). Ibn al-Qūṭiyya, who had another, much longer name, must have been relying to some degree on family oral tradition.
According to Ibn al-Qūṭiyya, the last Visigothic king of a united Hispania died before his three sons, Almund, Romulo, and Ardabast, reached majority. Their mother was regent at Toledo, but Roderic, army chief of staff, staged a rebellion, capturing Cordova. Of all the possible outcomes, he chose to impose a joint rule over distinct jurisdictions on the true heirs. Evidence of a division of some sort can be found in the distribution of coins imprinted with the name of each king and in the king lists. Wittiza is succeeded by Roderic, who reigned for seven and a half years, followed by a certain Achila (Aquila), who reigned three and a half years. If the reigns of both terminated with the incursion of the Saracens, then Roderic appears to have reigned a few years before the majority of Achila. The latter's kingdom is securely placed to the northeast, while Roderic seems to have taken the rest, notably Portugal.
The Saracens crossed the mountains to claim Ardo's Septimania, only to encounter the Basque dynasty of Aquitania, always the allies of the Goths. Odo the Great of Aquitania was at first victorious at the Battle of Toulouse in 721. Saracen troops gradually massed in Septimania and in 732 an army under Emir Abd al-Rahman abd Allah al-Ghafiqi advanced into Vasconia, and Odo was defeated at the Battle of the River Garonne. They took Bordeaux and were advancing toward Tours when Odo, powerless to stop them, appealed to his arch-enemy, Charles Martel, mayor of the Franks. In one of the first of the lightning marches for which the Carolingian kings became famous, Charles and his army appeared in the path of the Saracens between Tours and Poitiers, and in the Battle of Tours decisively defeated and killed al-Ghafiqi. The Moors would come back twice more, only to suffer defeat at Charles' hands twice more - at the River Berre near Narbonne in 737 and a second time in the Dauphine in 740. Odo's price for salvation from the Saracens was incorporation into Frankish kingdom, a decision that was repugnant to him and also to his heirs.
Loss and recovery of Aquitania.
After the death of his son, Hunald allied himself with free Lombardy. However, Odo had ambiguously left the kingdom jointly to his two sons, Hunald and Hatto. The latter, loyal to Francia, now went to war with his brother over full possession. Victorious, Hunald blinded and imprisoned his brother, only to be so stricken by conscience that he resigned and entered the church as a monk to do penance according to Carolingian sources. His son Waifer took an early inheritance, becoming duke of Aquitania, and ratified the alliance with Lombardy. Waifer decided to honor it, repeating his father's decision, which he justified by arguing that any agreements with Charles Martel became invalid on Martel's death. Since Aquitania was now Pepin's inheritance, according to some the latter and his son, the young Charles, hunted down Waifer, who could only conduct a guerrilla war, and executed him.
Among the contingents of the Frankish army were Bavarians under Tassilo III, Duke of Bavaria, an Agilofing, the hereditary Bavarian royal family. Grifo had installed himself as Duke of Bavaria, but Pepin replaced him with a member of the royal family yet a child, Tassilo, whose protector he had become after the death of his father. The loyalty of the Agilolfings was perpetually in question, but Pepin exacted numerous oaths of loyalty from Tassilo. However, the latter had married Liutperga, a daughter of Desiderius, king of Lombardy. At a critical point in the campaign Tassilo with all his Bavarians left the field. Out of reach of Pepin, he repudiated all loyalty to Francia. Pepin had no chance to respond as he grew ill and within a few weeks after the execution of Waifer died himself.
The first event of the brothers' reign was the uprising of the Aquitainians and Gascons, in 769, in that territory split between the two kings. One year before, Pepin had finally defeated Waifer, Duke of Aquitaine, after waging a destructive, ten-year war against Aquitaine. Now, one Hunald (seemingly other than Hunald the duke) led the Aquitainians as far north as Angoulême. Charles met Carloman, but Carloman refused to participate and returned to Burgundy. Charles went to war, leading an army to Bordeaux, where he set up a fort at Fronsac. Hunald was forced to flee to the court of Duke Lupus II of Gascony. Lupus, fearing Charles, turned Hunald over in exchange for peace, and he was put in a monastery. Gascon lords also surrendered, and Aquitaine and Gascony were finally fully subdued by the Franks.
Union perforce.
The brothers maintained lukewarm relations with the assistance of their mother Bertrada, but in 770 Charles signed a treaty with Duke Tassilo III of Bavaria and married a Lombard Princess (commonly known today as Desiderata), the daughter of King Desiderius, to surround Carloman with his own allies. Though Pope Stephen III first opposed the marriage with the Lombard princess, he would soon have little to fear from a Frankish-Lombard alliance.
Less than a year after his marriage, Charlemagne repudiated Desiderata and quickly married a 13-year-old Swabian named Hildegard. The repudiated Desiderata returned to her father's court at Pavia. Her father's wrath was now aroused, and he would have gladly allied with Carloman to defeat Charles. Before any open hostilities could be declared, however, Carloman died on 5 December 771, seemingly of natural causes. Carloman's widow Gerberga fled to Desiderius' court in Lombardy with her sons for protection.
Italian campaigns.
Conquest of the Lombard kingdom.
At his succession in 772, Pope Adrian I demanded the return of certain cities in the former exarchate of Ravenna in accordance with a promise at the succession of Desiderius. Instead, Desiderius took over certain papal cities and invaded the Pentapolis, heading for Rome. Adrian sent embassies to Charlemagne in autumn requesting he enforce the policies of his father, Pepin. Desiderius sent his own embassies denying the pope's charges. The embassies both met at Thionville, and Charlemagne upheld the pope's side. Charlemagne demanded what the pope had requested, and Desiderius promptly swore never to comply. Charlemagne and his uncle Bernard crossed the Alps in 773 and chased the Lombards back to Pavia, which they then besieged. Charlemagne temporarily left the siege to deal with Adelchis, son of Desiderius, who was raising an army at Verona. The young prince was chased to the Adriatic littoral, and he fled to Constantinople to plead for assistance from Constantine V, who was waging war with Bulgaria.<ref name=Einhard/Lombard></ref>
The siege lasted until the spring of 774, when Charlemagne visited the pope in Rome. There he confirmed his father's grants of land, with some later chronicles claiming—falsely—that he also expanded them, granting Tuscany, Emilia, Venice, and Corsica. The pope granted him the title "patrician". He then returned to Pavia, where the Lombards were on the verge of surrendering. In return for their lives, the Lombards surrendered and opened the gates in early summer. Desiderius was sent to the abbey of Corbie, and his son Adelchis died in Constantinople a patrician. Charles, unusually, had himself crowned with the Iron Crown and made the magnates of Lombardy do homage to him at Pavia. Only Duke Arechis II of Benevento refused to submit and proclaimed independence. Charlemagne was then master of Italy as king of the Lombards. He left Italy with a garrison in Pavia and a few Frankish counts in place the same year.
There was still instability, however, in Italy. In 776, Dukes Hrodgaud of Friuli and Hildeprand of Spoleto rebelled. Charlemagne rushed back from Saxony and defeated the duke of Friuli in battle; the duke was slain. The duke of Spoleto signed a treaty. Their co-conspirator, Arechis, was not subdued, and Adelchis, their candidate in Byzantium, never left that city. Northern Italy was now faithfully his.
Southern Italy.
In 787 Charlemagne directed his attention toward the Duchy of Benevento, where Arechis was reigning independently. Charlemagne besieged Salerno, and Arechis submitted to vassalage. However, with his death in 792, Benevento again proclaimed independence under his son Grimoald III. Grimoald was attacked by armies of Charles or his sons many times, but Charlemagne himself never returned to the Mezzogiorno, and Grimoald never was forced to surrender to Frankish suzerainty.
Charles and his children.
During the first peace of any substantial length (780–782), Charles began to appoint his sons to positions of authority within the realm, in the tradition of the kings and mayors of the past. In 781, he made his two younger sons kings, having them crowned by the Pope. The elder of these two, Carloman, was made king of Italy, taking the Iron Crown which his father had first worn in 774, and in the same ceremony was renamed "Pippin." The younger of the two, Louis, became king of Aquitaine. Charlemagne ordered Pippin and Louis to be raised in the customs of their kingdoms, and he gave their regents some control of their subkingdoms, but real power was always in his hands, though he intended his sons to inherit their realms some day. Nor did he tolerate insubordination in his sons: in 792, he banished his eldest, though possibly illegitimate, son, Pippin the Hunchback, to the monastery of Prüm, because the young man had joined a rebellion against him.
Charles was determined to have his children educated, including his daughters, as he himself was not. His children were taught all the arts, and his daughters were learned in the way of being a woman. His sons took archery, horsemanship, and other outdoor activities.
The sons fought many wars on behalf of their father when they came of age. Charles was mostly preoccupied with the Bretons, whose border he shared and who insurrected on at least two occasions and were easily put down, but he was also sent against the Saxons on multiple occasions. In 805 and 806, he was sent into the Böhmerwald (modern Bohemia) to deal with the Slavs living there (Bohemian tribes, ancestors of the modern Czechs). He subjected them to Frankish authority and devastated the valley of the Elbe, forcing a tribute on them. Pippin had to hold the Avar and Beneventan borders but also fought the Slavs to his north. He was uniquely poised to fight the Byzantine Empire when finally that conflict arose after Charlemagne's imperial coronation and a Venetian rebellion. Finally, Louis was in charge of the Spanish March and also went to southern Italy to fight the duke of Benevento on at least one occasion. He took Barcelona in a great siege in 797 (see below).
Charlemagne's attitude toward his daughters has been the subject of much discussion. He kept them at home with him and refused to allow them to contract sacramental marriages – possibly to prevent the creation of cadet branches of the family to challenge the main line, as had been the case with Tassilo of Bavaria – yet he tolerated their extramarital relationships, even rewarding their common-law husbands, and treasured the illegitimate grandchildren they produced for him. He also, apparently, refused to believe stories of their wild behavior. After his death the surviving daughters were banished from the court by their brother, the pious Louis, to take up residence in the convents they had been bequeathed by their father. At least one of them, Bertha, had a recognised relationship, if not a marriage, with Angilbert, a member of Charlemagne's court circle.
Carolingian expansion to the south.
Vasconia and the Pyrenees.
The destructive war led by Pepin in Aquitaine, although brought to a satisfactory conclusion for the Franks, proved the Frankish power structure south of the Loire was feeble and unreliable. After the defeat and death of Waifer of Aquitaine in 768, while Aquitaine submitted again to the Carolingian dynasty, a new rebellion broke out in 769 led by Hunald II, maybe son of Waifer. He took refuge with the ally duke Lupus II of Gascony, but probably out of fear of Charlemagne's reprisal, handed him over to the new King of the Franks besides pledging loyalty to him, which seemed to confirm the peace in the Basque area south of the Garonne.
However, wary of new Basque uprisings, Charlemagne seems to have tried to diminish duke Lupus's power by appointing a certain Seguin as count of Bordeaux (778) and other counts of Frankish background in bordering areas (, County of Fézensac), a decision that seriously undermined the authority of the duke of Gascony (Vasconia). The Basque duke in turn seems to have contributed decisively or schemed the Battle of Roncevaux Pass (referred to as "Basque treachery"). The defeat of Charlemagne's army in Roncevaux (778) confirmed him in his determination to rule directly by establishing the Kingdom of Aquitaine (son Louis the Pious proclaimed first king) based on a power base of Frankish officials, distributing lands among colonisers and allocating lands to the Church, which he took as ally. A Christianization program was put on place across the high Pyrenees (778).
The new political arrangement for Vasconia didn´t sit well with local lords. As of 788 we hear of Adalric fighting and capturing Chorson, Carolingian count of Toulouse. He was eventually released, but Charlemagne, enraged at the compromise, decided to depose him and appointed his trustee William of Orange. William in turn fought the Basques and defeated them after banishing Adalric (790).
From 781 (Pallars, Ribagorça) to 806 (Pamplona under Frankish influence), taking the County of Toulouse for a power base, Charlemagne managed to assert Frankish authority over the Pyrenees by bringing to heel the south-western marches of Toulouse (790) and establishing vassal counties on the southern Pyrenees that were to make up the Marca Hispanica. As of 794, we hear for the first time of a Frankish vassal, the Basque lord Belasko ("al-Galashki", 'the Gaul') in the lands of Álava, but Pamplona remained in Cordovan and local hands up to 806. Belasko and the counties in the Marca Hispánica provided the necessary springboard to attack the Andalusians (expedition led by William Count of Toulouse and Louis the Pious to capture Barcelona in 801), in a way that Charlemagne had succeeded in expanding the Carolingian rule all around the Pyrenees by 812, although events in the Duchy of Vasconia (rebellion in Pamplona, count overthrown in Aragon, duke Seguin of Bordeaux deposed, uprising of the Basque lords, etc.) were to prove it ephemeral on his death.
Roncesvalles campaign.
According to the Muslim historian Ibn al-Athir, the Diet of Paderborn had received the representatives of the Muslim rulers of Zaragoza, Girona, Barcelona, and Huesca. Their masters had been cornered in the Iberian peninsula by Abd ar-Rahman I, the Umayyad emir of Cordova. These "Saracen" (Moorish and Muladi) rulers offered their homage to the great king of the Franks in return for military support. Seeing an opportunity to extend Christendom and his own power and believing the Saxons to be a fully conquered nation, Charlemagne agreed to go to Spain.
In 778, he led the Neustrian army across the Western Pyrenees, while the Austrasians, Lombards, and Burgundians passed over the Eastern Pyrenees. The armies met at Saragossa and Charlemagne received the homage of the Muslim rulers, Sulayman al-Arabi and Kasmin ibn Yusuf, but the city did not fall for him. Indeed, Charlemagne was facing the toughest battle of his career where the Muslims had the upper hand and forced him to retreat. He decided to go home, since he could not trust the Basques, whom he had subdued by conquering Pamplona. He turned to leave Iberia, but as he was passing through the Pass of Roncesvalles one of the most famous events of his long reign occurred. The Basques fell on his rearguard and baggage train, utterly destroying it. The Battle of Roncevaux Pass, though less a battle than a mere skirmish, left many famous dead: among which were the seneschal Eggihard, the count of the palace Anselm, and the warden of the Breton March, Roland, inspiring the subsequent creation of the Song of Roland ("La Chanson de Roland").
Wars with the Moors.
The conquest of Italy brought Charlemagne in contact with the Saracens who, at the time, controlled the Mediterranean. Pippin, his son, was much occupied with Saracens in Italy. Charlemagne conquered Corsica and Sardinia at an unknown date and in 799 the Balearic Islands. The islands were often attacked by Saracen pirates, but the counts of Genoa and Tuscany (Boniface) kept them at bay with large fleets until the end of Charlemagne's reign. Charlemagne even had contact with the caliphal court in Baghdad. In 797 (or possibly 801), the caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian elephant named Abul-Abbas and a clock.
In Hispania, the struggle against the Moors continued unabated throughout the latter half of his reign. His son Louis was in charge of the Spanish border. In 785, his men captured Girona permanently and extended Frankish control into the Catalan littoral for the duration of Charlemagne's reign (and much longer, it remained nominally Frankish until the Treaty of Corbeil in 1258). The Muslim chiefs in the northeast of Islamic Spain were constantly revolting against Cordovan authority, and they often turned to the Franks for help. The Frankish border was slowly extended until 795, when Girona, Cardona, Ausona, and Urgell were united into the new Spanish March, within the old duchy of Septimania.
In 797 Barcelona, the greatest city of the region, fell to the Franks when Zeid, its governor, rebelled against Cordova and, failing, handed it to them. The Umayyad authority recaptured it in 799. However, Louis of Aquitaine marched the entire army of his kingdom over the Pyrenees and besieged it for two years, wintering there from 800 to 801, when it capitulated. The Franks continued to press forward against the emir. They took Tarragona in 809 and Tortosa in 811. The last conquest brought them to the mouth of the Ebro and gave them raiding access to Valencia, prompting the Emir al-Hakam I to recognize their conquests in 812.
Eastern campaigns.
Saxon Wars.
Charlemagne was engaged in almost constant battle throughout his reign, often at the head of his elite "scara" bodyguard squadrons, with his legendary sword Joyeuse in hand. In the Saxon Wars, spanning thirty years and eighteen battles, he conquered Saxonia and proceeded to convert the conquered to Christianity.
The Germanic Saxons were divided into four subgroups in four regions. Nearest to Austrasia was Westphalia and furthest away was Eastphalia. In between these two kingdoms was that of Engria and north of these three, at the base of the Jutland peninsula, was Nordalbingia.
In his first campaign, Charlemagne forced the Engrians in 773 to submit and cut down an Irminsul pillar near Paderborn. The campaign was cut short by his first expedition to Italy. He returned in 775, marching through Westphalia and conquering the Saxon fort of Sigiburg. He then crossed Engria, where he defeated the Saxons again. Finally, in Eastphalia, he defeated a Saxon force, and its leader Hessi converted to Christianity. Charlemagne returned through Westphalia, leaving encampments at Sigiburg and Eresburg, which had been important Saxon bastions. All of Saxony but Nordalbingia was under his control, but Saxon resistance had not ended.
Following his campaign in Italy to subjugate the dukes of Friuli and Spoleto, Charlemagne returned very rapidly to Saxony in 776, where a rebellion had destroyed his fortress at Eresburg. The Saxons were once again brought to heel, but their main leader, Widukind, managed to escape to Denmark, home of his wife. Charlemagne built a new camp at Karlstadt. In 777, he called a national diet at Paderborn to integrate Saxony fully into the Frankish kingdom. Many Saxons were baptised as Christians.
In the summer of 779, he again invaded Saxony and reconquered Eastphalia, Engria, and Westphalia. At a diet near Lippe, he divided the land into missionary districts and himself assisted in several mass baptisms (780). He then returned to Italy and, for the first time, there was no immediate Saxon revolt. Saxony was peaceful from 780 to 782.
He returned to Saxony in 782 and instituted a code of law and appointed counts, both Saxon and Frank. The laws were draconian on religious issues; for example, the "Capitulatio de partibus Saxoniae" prescribed death to Saxon pagans who refused to convert to Christianity. This revived a renewal of the old conflict. That year, in autumn, Widukind returned and led a new revolt. In response, at Verden in Lower Saxony, Charlemagne is recorded as having ordered the execution of 4,500 Saxon prisoners, known as the Massacre of Verden ("Verdener Blutgericht"). The killings triggered three years of renewed bloody warfare (783–785). During this war the Frisians were also finally subdued and a large part of their fleet was burned. The war ended with Widukind accepting baptism.
Thereafter, the Saxons maintained the peace for seven years, but in 792 the Westphalians again rose against their conquerors. The Eastphalians and Nordalbingians joined them in 793, but the insurrection did not catch on and was put down by 794. An Engrian rebellion followed in 796, but the presence of Charlemagne, Christian Saxons and Slavs quickly crushed it. The last insurrection of the independent-minded people occurred in 804, more than thirty years after Charlemagne's first campaign against them. This time, the most restive of them, the Nordalbingians, found themselves effectively disempowered from rebellion for the time being. According to Einhard:
The war that had lasted so many years was at length ended by their acceding to the terms offered by the King; which were renunciation of their national religious customs and the worship of devils, acceptance of the sacraments of the Christian faith and religion, and union with the Franks to form one people.
Submission of Bavaria.
By 774 AD Charlemagne invaded the Kingdom of Lombardy, he later annexed the Lombardian territories and assumed its crown, placing the Papal States under Frankish protection. The Duchy of Spoleto south of Rome was also acquired in 774, while in the central western parts of Europe, the Duchy of Bavaria was absorbed and the Bavarian policy continued of establishing tributary marches, (borders protected in return for tribute or taxes) among the Slavic Serbs, and Czechs. The remaining power confronting the Franks in the east were the Avars, however Charlemagne went on acquiring other Slav areas, including Bohemia, Macedonia, Moravia, Austria and Croatia.
In 789, Charlemagne turned his attention to Bavaria. He claimed Tassilo was an unfit ruler, due to his oath-breaking. The charges were exaggerated, but Tassilo was deposed anyway and put in the monastery of Jumièges. In 794, he was made to renounce any claim to Bavaria for himself and his family (the Agilolfings) at the synod of Frankfurt. Bavaria was subdivided into Frankish counties, as had been done with Saxony.
Avar campaigns.
In 788, the Avars, a pagan Asian horde that had settled down in what is today Hungary (Einhard called them Huns), invaded Friuli and Bavaria. Charlemagne was preoccupied with other matters until 790, when he marched down the Danube and ravaged Avar territory to the Győr. A Lombard army under Pippin then marched into the Drava valley and ravaged Pannonia. The campaigns would have continued if the Saxons had not revolted again in 792, breaking seven years of peace.
For the next two years, Charlemagne was occupied, along with the Slavs, against the Saxons. Pippin and Duke Eric of Friuli continued, however, to assault the Avars' ring-shaped strongholds. The great Ring of the Avars, their capital fortress, was taken twice. The booty was sent to Charlemagne at his capital, Aachen, and redistributed to all his followers and even to foreign rulers, including King Offa of Mercia. Soon the Avar tuduns had lost the will to fight and traveled to Aachen to subject themselves to Charlemagne as vassals and Christians. Charlemagne accepted their surrender and sent one native chief, baptised Abraham, back to Avaria with the ancient title of khagan. Abraham kept his people in line, but in 800, the Bulgarians under Khan Krum also attacked the remains of Avar state.
In 803 Charlemagne sent a huge Bavarian army into Pannonia, defeating and bringing an end to the Avar confederation. In November of the same year, Charlemagne went to Regensburg where the Avar leaders acknowledged him as their own ruler. In 805 the Avar khagan, who had already been baptised, went to Aachen to ask permission to settle with his people south-eastward from Vienna. The Transdanubian territories became integral parts of the Frankish realm, which was abolished by the Magyars in 899-900.
Northeast Slav expeditions.
In 789, in recognition of his new pagan neighbours, the Slavs, Charlemagne marched an Austrasian-Saxon army across the Elbe into Obotrite territory. The Slavs immediately submitted, led by their leader Witzin. Charlemagne then accepted the surrender of the Wiltzes under Dragovit and demanded many hostages. Charlemagne also demanded the permission to send missionaries into this pagan region unmolested. The army marched to the Baltic before turning around and marching to the Rhine, winning much booty with no harassment. The tributary Slavs became loyal allies. In 795, when the Saxons broke the peace, the Abotrites and Wiltzes rose in arms with their new master against the Saxons. Witzin died in battle and Charlemagne avenged him by harrying the Eastphalians on the Elbe. Thrasuco, his successor, led his men to conquest over the Nordalbingians and handed their leaders over to Charlemagne, who greatly honoured him. The Abotrites remained loyal until Charles' death and fought later against the Danes.
Southeast Slav expeditions.
When Charlemagne incorporated much of Central Europe, he brought the Frankish state face to face with the Avars and Slavs in the southeast. The most southeast Frankish neighbors were Croats, who settled in Pannonian Croatia and Dalmatian Croatia. While fighting the Avars, the Franks had called for their support. During the 790s, when Charlemagne campaigned against the Avars, he won a major victory in 796. Pannonian Croatian duke Vojnomir of Pannonian Croatia aided Charlemagne, and the Franks made themselves overlords over the Croatians of northern Dalmatia, Slavonia, and Pannonia.
The Frankish commander Eric of Friuli wanted to extend his dominion by conquering Littoral Croatian Duchy. During that time, Dalmatian Croatia was ruled by duke Višeslav of Croatia, who was one of the first known Croatian dukes. In the Battle of Trsat, the forces of Eric fled their positions and were totally routed by the forces of Višeslav. Eric himself was among the killed, and his death and defeat proved a great blow for the Carolingian Empire.
Charlemagne also directed his attention to the Slavs to the west of the Avar khaganate: the Carantanians and Carniolans. These people were subdued by the Lombards and Bavarii, were made tributaries, but were never fully incorporated into the Frankish state.
Imperium.
Coronation.
In 799, Pope Leo III had been mistreated by the Romans, who tried to put out his eyes and tear out his tongue. Leo escaped and fled to Charlemagne at Paderborn, asking him to intervene in Rome and restore him. Charlemagne, advised by scholar Alcuin of York, agreed to travel to Rome, doing so in November 800 and holding a council on 1 December. On 23 December Leo swore an oath of innocence. At Mass, on Christmas Day (25 December), when Charlemagne knelt at the altar to pray, the Pope crowned him "Imperator Romanorum" ("Emperor of the Romans") in Saint Peter's Basilica. In so doing, the Pope was effectively nullifying the legitimacy of Empress Irene of Constantinople:
Charlemagne's coronation as Emperor, though intended to represent the continuation of the unbroken line of Emperors from Augustus to Constantine VI, had the effect of setting up two separate (and often opposing) Empires and two separate claims to imperial authority. For centuries to come, the Emperors of both West and East would make competing claims of sovereignty over the whole.
Einhard says that Charlemagne was ignorant of the Pope's intent and did not want any such coronation:
[H]e at first had such an aversion that he declared that he would not have set foot in the Church the day that they [the imperial titles] were conferred, although it was a great feast-day, if he could have foreseen the design of the Pope. A number of modern scholars, however, suggest that Charlemagne was indeed aware of the coronation; certainly he cannot have missed the bejeweled crown waiting on the altar when he came to pray; something even contemporary sources support when thoroughly analysed.
In any event, Charlemagne used these circumstances to claim that he was the renewer of the Roman Empire, which had apparently fallen into degradation under the Byzantines. In his official charters, Charles preferred the style "Karolus serenissimus Augustus a Deo coronatus magnus pacificus imperator Romanum gubernans imperium" ("Charles, most serene Augustus crowned by God, the great, peaceful emperor ruling the Roman empire") to the more direct "Imperator Romanorum" ("Emperor of the Romans").
Imperial Diplomacy.
The iconoclasm of the Byzantine Isaurian Dynasty was endorsed by the Franks. The Second Council of Nicaea reintroduced the veneration of icons under Empress Irene. The council was not recognized by Charlemagne since no Frankish emissaries had been invited, even though Charlemagne ruled more than three provinces of the old Roman empire and was considered equal in rank to the Byzantine emperor. And while the Pope supported the reintroduction of the iconic veneration, he politically digressed from Byzantium. He certainly desired to increase the influence of the papacy, to honour his saviour Charlemagne, and to solve the constitutional issues then most troubling to European jurists in an era when Rome was not in the hands of an emperor. Thus, Charlemagne's assumption of the imperial title was not a usurpation in the eyes of the Franks or Italians. It was, however, seen as such in Byzantium, where it was protested by Irene and her successor Nicephorus I — neither of whom had any great effect in enforcing their protests.
The Byzantines, however, still held several territories in Italy: Venice (what was left of the Exarchate of Ravenna), Reggio (in Calabria), Brindisi (in Apulia), and Naples (the "Ducatus Neapolitanus"). These regions remained outside of Frankish hands until 804, when the Venetians, torn by infighting, transferred their allegiance to the Iron Crown of Pippin, Charles' son. The "Pax Nicephori" ended. Nicephorus ravaged the coasts with a fleet, initiating the only instance of war between the Byzantines and the Franks. The conflict lasted until 810, when the pro-Byzantine party in Venice gave their city back to the Byzantine Emperor, and the two emperors of Europe made peace: Charlemagne received the Istrian peninsula and in 812 the emperor Michael I Rhangabes recognised his status as Emperor, although not necessarily as "Emperor of the Romans".
Danish attacks.
After the conquest of Nordalbingia, the Frankish frontier was brought into contact with Scandinavia. The pagan Danes, "a race almost unknown to his ancestors, but destined to be only too well known to his sons" as Charles Oman described them, inhabiting the Jutland peninsula, had heard many stories from Widukind and his allies who had taken refuge with them about the dangers of the Franks and the fury which their Christian king could direct against pagan neighbours.
In 808, the king of the Danes, Godfred, built the vast Danevirke across the isthmus of Schleswig. This defence, last employed in the Danish-Prussian War of 1864, was at its beginning a long earthenwork rampart. The Danevirke protected Danish land and gave Godfred the opportunity to harass Frisia and Flanders with pirate raids. He also subdued the Frank-allied Wiltzes and fought the Abotrites.
Godfred invaded Frisia, joked of visiting Aachen, but was murdered before he could do any more, either by a Frankish assassin or by one of his own men. Godfred was succeeded by his nephew Hemming, who concluded the Treaty of Heiligen with Charlemagne in late 811.
Death.
In 813, Charlemagne called Louis the Pious, king of Aquitaine, his only surviving legitimate son, to his court. There Charlemagne crowned his son with his own hands as co-emperor and sent him back to Aquitaine. He then spent the autumn hunting before returning to Aachen on 1 November. In January, he fell ill with pleurisy. In deep depression (mostly because many of his plans were not yet realized), he took to his bed on 21 January and as Einhard tells it:
He died January twenty-eighth, the seventh day from the time that he took to his bed, at nine o'clock in the morning, after partaking of the Holy Communion, in the seventy-second year of his age and the forty-seventh of his reign.
He was buried the same day as his death, in Aachen Cathedral, although the cold weather and the nature of his illness made such a hurried burial unnecessary. The earliest surviving "planctus", the "Planctus de obitu Karoli", was composed by a monk of Bobbio, which he had patronised. A later story, told by Otho of Lomello, Count of the Palace at Aachen in the time of Otto III, would claim that he and Emperor Otto had discovered Charlemagne's tomb: the emperor, they claimed, was seated upon a throne, wearing a crown and holding a sceptre, his flesh almost entirely incorrupt. In 1165, Frederick I re-opened the tomb again and placed the emperor in a sarcophagus beneath the floor of the cathedral. In 1215 Frederick II re-interred him in a casket made of gold and silver.
Charlemagne's death greatly affected many of his subjects, particularly those of the literary clique who had surrounded him at Aachen. An anonymous monk of Bobbio lamented:
From the lands where the sun rises to western shores, people are crying and wailing ... the Franks, the Romans, all Christians, are stung with mourning and great worry ... the young and old, glorious nobles, all lament the loss of their Caesar ... the world laments the death of Charles ... O Christ, you who govern the heavenly host, grant a peaceful place to Charles in your kingdom. Alas for miserable me.
He was succeeded by his surviving son, Louis, who had been crowned the previous year. His empire lasted only another generation in its entirety; its division, according to custom, between Louis's own sons after their father's death laid the foundation for the modern states of Germany and France.
Administration.
As an administrator, Charlemagne stands out for his many reforms: monetary, governmental, military, cultural, and ecclesiastical. He is the main protagonist of the "Carolingian Renaissance."
Military.
It has long been held that the dominance of Charlemagne's military was based on a "cavalry revolution" led by Charles Martel in 730s. However, the stirrup, which made the "shock cavalry" lance charge possible, was not introduced to the Frankish kingdom until the late eighth century. Instead, Charlemagne's success rested primarily on novel siege technologies and excellent logistics.
However, large numbers of horses were used by the Frankish military during the age of Charlemagne. This was because horses provided a quick, long-distance method of transporting troops, which was critical to building and maintaining such a large empire.
Economic and monetary reforms.
Charlemagne had an important role in determining the immediate economic future of Europe. Pursuing his father's reforms, Charlemagne abolished the monetary system based on the gold , and he and the Anglo-Saxon King Offa of Mercia took up the system set in place by Pippin. There were strong pragmatic reasons for this abandonment of a gold standard, notably a shortage of gold itself.
The gold shortage was a direct consequence of the conclusion of peace with Byzantium, which resulted in ceding Venice and Sicily to the East and losing their trade routes to Africa. The resulting standardisation economically harmonized and unified the complex array of currencies which had been in use at the commencement of his reign, thus simplifying trade and commerce.
Charlemagne established a new standard, the (from the Latin , the modern pound), which was based upon a pound of silver—a unit of both money and weight—which was worth 20 sous (from the Latin [which was primarily an accounting device and never actually minted], the modern shilling) or 240 (from the Latin , the modern penny). During this period, the and the were counting units; only the was a coin of the realm.
Charlemagne instituted principles for accounting practice by means of the Capitulare de villis of 802, which laid down strict rules for the way in which incomes and expenses were to be recorded.
Early in Charlemagne's rule he tacitly allowed the Jews to monopolize money lending. Then lending money for interest was proscribed in 814, being against Church law at the time, Charlemagne introduced the "Capitulary for the Jews", a prohibition on Jews engaging in money-lending due to the religious convictions of the majority of his constituents, in essence banning it across the board. A reversal of his earlier recorded general policy. In addition to this macro-oriented reform of the economy, Charlemagne also performed a significant number of microeconomic reforms, such as direct control of prices and levies on certain goods and commodities.
His Capitulary for the Jews, however was not representative of his overall economic relationship or attitude, toward the Frankish Jews and certainly not his earlier relationship with them, which had evolved over his lifespan. His paid personal physician for example was Jewish, he employed at least one Jew for his diplomatic missions, Isaac was his personal representative to the Muslim caliphate of Baghdad. Letters have been credited to him as well inviting Jews to settle in his kingdom, for economic purposes, generally welcoming them through his overall, progressive policies.
Charlemagne applied this system to much of the European continent, and Offa's standard was voluntarily adopted by much of England. After Charlemagne's death, continental coinage degraded, and most of Europe resorted to using the continued high-quality English coin until about 1100.
Education reforms.
A part of Charlemagne's success as warrior and administrator and ruler can be traced to his admiration for learning and education. His reign and the era it ushered in are often referred to as the Carolingian Renaissance because of the flowering of scholarship, literature, art, and architecture which characterize it. Charlemagne, brought into contact with the culture and learning of other countries (especially Moorish Spain, Anglo-Saxon England, and Lombard Italy) due to his vast conquests, greatly increased the provision of monastic schools and scriptoria (centres for book-copying) in Francia.
Most of the presently surviving works of classical Latin were copied and preserved by Carolingian scholars. Indeed, the earliest manuscripts available for many ancient texts are Carolingian. It is almost certain that a text which survived to the Carolingian age survives still.
The pan-European nature of Charlemagne's influence is indicated by the origins of many of the men who worked for him: Alcuin, an Anglo-Saxon from York; Theodulf, a Visigoth, probably from Septimania; Paul the Deacon, Lombard; Peter of Pisa and Paulinus of Aquileia, Italians; and Angilbert, Angilram, Einhard, and Waldo of Reichenau, Franks.
Charlemagne took a serious interest in scholarship, promoting the liberal arts at the court, ordering that his children and grandchildren be well-educated, and even studying himself (in a time when even leaders who promoted education did not take time to learn themselves) under the tutelage of Peter of Pisa, from whom he learned grammar; Alcuin, with whom he studied rhetoric, dialectic (logic), and astronomy (he was particularly interested in the movements of the stars); and Einhard, who assisted him in his studies of arithmetic.
His great scholarly failure, as Einhard relates, was his inability to write: when in his old age he began attempts to learn—practicing the formation of letters in his bed during his free time on books and wax tablets he hid under his pillow—"his effort came too late in life and achieved little success", and his ability to read – which Einhard is silent about, and which no contemporary source supports—has also been called into question.
In 800, Charlemagne enlarged the hostel at the Muristan in Jerusalem and added a library to it. He certainly had not been personally in Jerusalem.
Church reforms.
Unlike his father, Pippin, and Uncle, Carloman, Charlemagne expanded the reform program of the church. The deepening of the spiritual life was later to be seen as central to public policy and royal governance. His reform focused on the strengthening of the church's power structure, improving the skill and moral quality of the clergy, standardizing liturgical practices, improvements on the basic tenets of the faith and moral, and the rooting out of paganism. His authority was now extended over church and state. He could discipline clerics, control ecclesiastical property and define orthodox doctrine. Despite the harsh legislation and sudden change, he had grown a well developed support from the clergy who approved his desire to deepen the piety and morals of his Christian subjects.
Writing reforms.
During Charles' reign, the Roman half uncial script and its cursive version, which had given rise to various continental minuscule scripts, were combined with features from the insular scripts that were being used in Irish and English monasteries. Carolingian minuscule was created partly under the patronage of Charlemagne. Alcuin of York, who ran the palace school and scriptorium at Aachen, was probably a chief influence in this.
The revolutionary character of the Carolingian reform, however, can be over-emphasised; efforts at taming the crabbed Merovingian and Germanic hands had been underway before Alcuin arrived at Aachen. The new minuscule was disseminated first from Aachen and later from the influential scriptorium at Tours, where Alcuin retired as an abbot.
Political reforms.
Charlemagne engaged in many reforms of Frankish governance, but he continued also in many traditional practices, such as the division of the kingdom among sons.
Organization.
The Carolingian king exercised the "bannum", the right to rule and command. He had supreme jurisdiction in judicial matters, made legislation, led the army, and protected both the Church and the poor. His administration was an attempt to organize the kingdom, church, and nobility around him. However, the effort was heavily dependent upon the efficiency, loyalty, and support of his subjects.
Imperial coronation.
Historians have debated for centuries whether Charlemagne was aware of the Pope's intent to crown him Emperor prior to the coronation (Charlemagne declared that he would not have entered Saint Peter's had he known), but that debate has often obscured the more significant question of "why" the Pope granted the title and why Charlemagne chose to accept it once he did.
Roger Collins points out "[t]hat the motivation behind the acceptance of the imperial title was a romantic and antiquarian interest in reviving the Roman empire is highly unlikely." For one thing, such romance would not have appealed either to Franks or Roman Catholics at the turn of the ninth century, both of whom viewed the Classical heritage of the Roman Empire with distrust. The Franks took pride in having "fought against and thrown from their shoulders the heavy yoke of the Romans" and "from the knowledge gained in baptism, clothed in gold and precious stones the bodies of the holy martyrs whom the Romans had killed by fire, by the sword and by wild animals", as Pippin III described it in a law of 763 or 764.
Furthermore, the new title—carrying with it the risk that the new emperor would "make drastic changes to the traditional styles and procedures of government" or "concentrate his attentions on Italy or on Mediterranean concerns more generally"—risked alienating the Frankish leadership.
For both the Pope and Charlemagne, the Roman Empire remained a significant power in European politics at this time, and continued to hold a substantial portion of Italy, with borders not very far south of the city of Rome itself—this is the empire historiography has labelled the Byzantine Empire, for its capital was Constantinople (ancient Byzantium) and its people and rulers were Greek; it was a thoroughly Hellenic state. Indeed, Charlemagne was usurping the prerogatives of the Roman Emperor in Constantinople simply by sitting in judgement over the Pope in the first place:
For the Pope, then, there was "no living Emperor at the that time" though Henri Pirenne disputes this saying that the coronation "was not in any sense explained by the fact that at this moment a woman was reigning in Constantinople." Nonetheless, the Pope took the extraordinary step of creating one. The papacy had since 727 been in conflict with Irene's predecessors in Constantinople over a number of issues, chiefly the continued Byzantine adherence to the doctrine of iconoclasm, the destruction of Christian images; while from 750, the secular power of the Byzantine Empire in central Italy had been nullified.
By bestowing the Imperial crown upon Charlemagne, the Pope arrogated to himself "the right to appoint ... the Emperor of the Romans, ... establishing the imperial crown as his own personal gift but simultaneously granting himself implicit superiority over the Emperor whom he had created." And "because the Byzantines had proved so unsatisfactory from every point of view—political, military and doctrinal—he would select a westerner: the one man who by his wisdom and statesmanship and the vastness of his dominions ... stood out head and shoulders above his contemporaries."
With Charlemagne's coronation, therefore, "the Roman Empire remained, so far as either of them [Charlemagne and Leo] were concerned, one and indivisible, with Charles as its Emperor", though there can have been "little doubt that the coronation, with all that it implied, would be furiously contested in Constantinople."
How realistic either Charlemagne or the Pope felt it to be that the people of Constantinople would ever accept the King of the Franks as their Emperor, we cannot know; Alcuin speaks hopefully in his letters of an "Imperium Christianum" ("Christian Empire"), wherein, "just as the inhabitants of the [Roman Empire] had been united by a common Roman citizenship", presumably this new empire would be united by a common Christian faith, certainly this is the view of Pirenne when he says "Charles was the Emperor of the "ecclesia" as the Pope conceived it, of the Roman Church, regarded as the universal Church". The "Imperium Christianum" was further supported at a number of synods all across the Europe by Paulinus of Aquileia.
What is known, from the Byzantine chronicler Theophanes, is that Charlemagne's reaction to his coronation was to take the initial steps toward securing the Constantinopolitan throne by sending envoys of marriage to Irene, and that Irene reacted somewhat favorably to them.
It is important to distinguish between the universalist and localist conceptions of the empire, which have been the source of considerable controversy among historians. According to the former, the empire was a universal monarchy, a "commonwealth of the whole world, whose sublime unity transcended every minor distinction"; and the emperor "was entitled to the obedience of Christendom." According to the latter, the emperor had no ambition for universal dominion; his policy was limited in the same way as that of every other ruler, and when he made more far-reaching claims his object was normally to ward off the attacks either of the pope or of the Byzantine emperor. According to this view, also, the origin of the empire is to be explained by specific local circumstances rather than by far-flung theories.
According to Werner Ohnsorge, for a long time it had been the custom of Byzantium to designate the German princes as spiritual "sons" of the Byzantines. What might have been acceptable in the fifth century, to the pride of the Franks in the eighth century was provoking and insulting. Charles came to the realization that the great Roman emperor, who claimed to be the head of the world hierarchy of states, in reality was no greater than Charles himself, a king as other kings, since beginning in 629 he had entitled himself "Basileus" (translated literally as "king"). Ohnsorge finds it significant that the chief wax seal of Charles, which bore only the inscription: "Christe, protege Carolum regem Francorum [Christ, protect Charles, king of the Franks], was used from 772 to 813, even during the imperial period and was not replaced by a special imperial seal; indicating that Charles felt himself to be king of the Franks and wished only for the greatness of his Frankish people. Finally, Ohnsorge points out that in the spring of 813 at Aachen Charles crowned his youngest, only surviving son, Louis, as emperor without recourse to Rome and only with the acclamation of his Franks, also the form in which this acclamation was offered was no longer Roman, but Frankish-Christian; thus demonstrating both independence from Rome, and a Frankish understanding of empire different from Rome's.
The title of emperor remained in his family for years to come, however, as brothers fought over who had the supremacy in the Frankish state. The papacy itself never forgot the title nor abandoned the right to bestow it. When the family of Charles ceased to produce worthy heirs, the pope gladly crowned whichever Italian magnate could best protect him from his local enemies.
This devolution led, as could have been expected, to the dormancy of the title for almost forty years (924–962). Finally, in 962, in a radically different Europe from Charlemagne's, a new Roman Emperor was crowned in Rome by a grateful pope. This emperor, Otto the Great, brought the title into the hands of the kings of Germany for almost a millennium, for it was to become the Holy Roman Empire, a true imperial successor to that of Charles, if not Augustus.
Divisio regnorum.
In 806, Charlemagne first made provision for the traditional division of the empire on his death. For Charles the Younger he designated Austrasia and Neustria, Saxony, Burgundy, and Thuringia. To Pippin he gave Italy, Bavaria, and Swabia. Louis received Aquitaine, the Spanish March, and Provence. There was no mention of the imperial title however, which has led to the suggestion that, at that particular time, Charlemagne regarded the title as an honorary achievement which held no hereditary significance.
This division might have worked, but it was never to be tested. Pippin died in 810 and Charles in 811. Charlemagne then reconsidered the matter, and in 813, crowned his youngest son, Louis, co-emperor and co-King of the Franks, granting him a half-share of the empire and the rest upon Charlemagne's own death. The only part of the Empire which Louis was not promised was Italy, which Charlemagne specifically bestowed upon Pippin's illegitimate son Bernard.
Personality.
Language.
By Charlemagne's time the French vernacular had already diverged significantly from Latin. This is evidenced by one of the regulations of the Council of Tours (813), which required that the parish priests preach either in the "rusticam Romanam linguam" (Romance) or "Theotiscam" (the Germanic vernacular) rather than in Latin. The goal of this rule was to make the sermons comprehensible to the common people, who must therefore have been either Romance speakers or Germanic speakers. Charlemagne himself probably spoke a Rhenish Franconian dialect of Old High German.
Apart from his native language he also spoke Latin and understood a bit of Greek, according to his biographer Einhard ("Grecam vero melius intellegere quam pronuntiare poterat", "he could understand Greek better than he could speak it").
The largely fictional account of Charlemagne's Iberian campaigns by Pseudo-Turpin, written some three centuries after his death, gave rise to the legend that the king also spoke Arabic.
Appearance.
Charlemagne's personal appearance is known from a good description by a personal associate, Einhard, author after his death of the biography "Vita Karoli Magni". Einhard tells in his twenty-second chapter:
"He was heavily built, sturdy, and of considerable stature, although not exceptionally so, since his height was seven times the length of his own foot. He had a round head, large and lively eyes, a slightly larger nose than usual, white but still attractive hair, a bright and cheerful expression, a short and fat neck, and he enjoyed good health, except for the fevers that affected him in the last few years of his life. Toward the end, he dragged one leg. Even then, he stubbornly did what he wanted and refused to listen to doctors, indeed he detested them, because they wanted to persuade him to stop eating roast meat, as was his wont, and to be content with boiled meat."
The physical portrait provided by Einhard is confirmed by contemporary depictions of the emperor, such as coins and his bronze statue kept in the Louvre. In 1861, Charlemagne's tomb was opened by scientists who reconstructed his skeleton and estimated it to be measured . An estimate of his height from a X-ray and CT Scan of his tibia performed in 2010 is . This puts him in the 99th percentile of tall people of his period, given that average male height of his time was . The width of the bone suggested he was gracile but not robust in body build.
Dress.
Charlemagne wore the traditional costume of the Frankish people, described by Einhard thus:
"He used to wear the national, that is to say, the Frank, dress—next his skin a linen shirt and linen breeches, and above these a tunic fringed with silk; while hose fastened by bands covered his lower limbs, and shoes his feet, and he protected his shoulders and chest in winter by a close-fitting coat of otter or marten skins."
He wore a blue cloak and always carried a sword with him. The typical sword was of a golden or silver hilt. He wore fancy jewelled swords to banquets or ambassadorial receptions. Nevertheless:
"He despised foreign costumes, however handsome, and never allowed himself to be robed in them, except twice in Rome, when he donned the Roman tunic, chlamys, and shoes; the first time at the request of Pope Hadrian, the second to gratify Leo, Hadrian's successor."
He could rise to the occasion when necessary. On great feast days, he wore embroidery and jewels on his clothing and shoes. He had a golden buckle for his cloak on such occasions and would appear with his great diadem, but he despised such apparel, according to Einhard, and usually dressed like the common people.
Family.
Marriages and heirs.
Charlemagne had eighteen children over the course of his life with eight of his ten known wives or concubines. Nonetheless, he only had four legitimate grandsons, the four sons of his fourth son, Louis. In addition, he had a grandson (Bernard of Italy, the only son of his third son, Pippin of Italy), who was born illegitimate but included in the line of inheritance. So, despite eighteen children, the claimants to his inheritance were few.
Name.
He was named Karl ("Charles" in French and English, "Carolus" in Latin) after his grandfather, Charles Martel. Later Old French historians dubbed him "Charles le Magne" (Charles the Great), becoming Charlemagne in English after the Norman conquest of England. The epithet Carolus Magnus was widely used, leading to numerous translations into many languages of Europe. He was known in German as Karl der Große; Dutch, Karel de Grote; Danish, Karl den Store; Italian, Carlo Magno; Catalan, Carlemany; Serbo-Croatian, Karlo Veliki; Spanish, Carlomagno; and various others.
Charles' achievements gave a new meaning to his name. In many European languages, the very word for "king" derives from his name; e.g., , , , , , , , , , , , , , , . This development parallels that of the name of the Caesars in the original Roman Empire, which became "kaiser" and "czar", among others.
Cultural uses.
Charlemagne had an immediate afterlife. The author of the "Visio Karoli Magni" written around 865 uses facts gathered apparently from Einhard and his own observations on the decline of Charlemagne's family after the dissensions war (840–43) as the basis for a visionary tale of Charles' meeting with a prophetic spectre in a dream.
Charlemagne, being a model knight as one of the Nine Worthies, enjoyed an important afterlife in European culture. One of the great medieval literary cycles, the Charlemagne cycle or the "Matter of France", centres on the deeds of Charlemagne—the Emperor with the Flowing Beard of "Roland" fame—and his historical commander of the border with Brittany, Roland, and the paladins who are analogous to the knights of the Round Table or King Arthur's court. Their tales constitute the first "chansons de geste".
Charlemagne himself was accorded sainthood inside the Holy Roman Empire after the twelfth century. His canonisation by Antipope Paschal III, to gain the favour of Frederick Barbarossa in 1165, was never recognised by the Holy See, which annulled all of Paschal's ordinances at the Third Lateran Council in 1179. His name does not appear among the 28 saints named Charles who are listed in the Roman Martyrology. His beatification has been acknowledged as "cultus confirmed" and is celebrated on 28 January. In the Divine Comedy the spirit of Charlemagne appears to Dante in the Heaven of Mars, among the other "warriors of the faith."
In 809–810, Charlemagne called together a church council in Aachen, which confirmed the unanimous belief in the West that the Holy Spirit proceeds from the Father and the Son ("ex Patre Filioque") and sanctioned inclusion in the Nicene Creed of the phrase "Filioque" (and the Son). For this Charlemagne sought the approval of Pope Leo III. The Pope, while affirming the doctrine and approving its use in teaching, opposed its inclusion in the text of the Creed as adopted in the 381 First Council of Constantinople. This spoke of the procession of the Holy Spirit from the Father, without adding phrases such as "and the Son", "through the Son", or "alone". Stressing his opposition, the Pope had the original text inscribed in Greek and Latin on two heavy shields, which were displayed in Saint Peter's Basilica.
In 1867, an equestrian statue of Charlemagne, was made by Louis Jehotte and was inaugurated in 1868 on the Boulevard d'Avroy in Liège. In the niches of the neo-roman pedestal are six statues of the ancestors of Charlemagne (Sainte Begge, Pépin de Herstal, Charles Martel, Bertrude, Pépin de Landen and Pépin le Bref).
The city of Aachen has, since 1949, awarded an international prize (called the "Karlspreis der Stadt Aachen") in honour of Charlemagne. It is awarded annually to "personages of merit who have promoted the idea of western unity by their political, economic and literary endeavours." Winners of the prize include Count Richard Coudenhove-Kalergi, the founder of the pan-European movement, Alcide De Gasperi, and Winston Churchill.
In its national anthem, El Gran Carlemany, the nation of Andorra credits Charlemagne with its independence.
Charlemagne is quoted by Dr Henry Jones Sr. (played by Sean Connery) in "Indiana Jones and the Last Crusade". After using his umbrella to induce a flock of seagulls to smash through the glass cockpit of a pursuing German fighter plane, Henry Jones remarks, "I suddenly remembered my Charlemagne: 'Let my armies be the rocks and the trees and the birds in the sky'." Despite the quote's popularity since the movie, there is no evidence that Charlemagne actually said this.
"The Economist", the weekly news and international affairs newspaper, features a one-page article every week entitled "Charlemagne", focusing generally on European affairs and, more usually and specifically, on the European Union and its politics.
There is a play named "Carelman Charitham" in the Indian art-form Chavittu Nadakam which is based on the life of Charlemagne.
Actor and singer Christopher Lee's Symphonic Metal concept album ' and its Heavy Metal follow-up ' feature the events of Charlemagne's life.
A 2010 episode of "QI" discussed the mathematics completed by Mark Humphrys that calculated that all modern Europeans are highly likely to share Charlemagne as a common ancestor. (also see Most Recent Common Ancestor)
The 2002 survival-horror video game "" contains a segment wherein the player takes control of one of Charlemagne's loyal messengers. The goal of the player is to protect the emperor against an insidious cult seeking to murder Charlemagne in order to stifle political and religious reform.
In April 2014, on occasion of the 1200th anniversary of the death of Charlemagne there was a 2-week public art installation "Mein Karl" by Ottmar Hörl at Katschhof place between city hall and cathedral of Aachen, displaying 500 Charlemagne statues.
Books and libraries.
Charlemagne was a lover of books. He had clerics translate Christian creeds and prayers into their respective vernaculars. Book production was completed slowly by hand, and took place mainly in large monastic libraries. Books were so in demand during Charlemagne’s time that monastic libraries lent out some books, but only if that borrower put up valuable collateral. At Charlemagne’s court, a court library was founded.
Charlemagne was a bibliophile and founded a court library during the ninth century. Some claims that add to Charlemagne's credit as a lover of books include during meals having books read out loud to him, and loving the books by St. Augustine. He also had Alcuin at his court, although originally he was from North Umbria in modern England, Alcuin was a proponent of education and wrote thoughtfully on Christian religion. In the court library created by Charlemagne a number of copies of books were produced but those copies were made to be distributed by Charlemagne.

</doc>
<doc id="5315" url="http://en.wikipedia.org/wiki?curid=5315" title="Character encodings in HTML">
Character encodings in HTML

HTML (Hypertext Markup Language) has been in use since 1991, but HTML 4.0 (December 1997) was the first standardized version where international characters were given reasonably complete treatment. When an HTML document includes special characters outside the range of seven-bit ASCII two goals are worth considering: the information's integrity, and universal browser display.
Specifying the document's character encoding.
There are several ways to specify which character encoding is used in the document. First, the web server can include the character encoding or "codice_1" in the Hypertext Transfer Protocol (HTTP) codice_2 header, which would typically look like this:
 Content-Type: text/html; charset=ISO-8859-4
This method gives the HTTP server a convenient way to alter document's encoding according to content negotiation; certain HTTP server software can do it, for example Apache with the module mod_charset_lite.
For HTML it is possible to include this information inside the codice_3 element near the top of the document:
HTML5 also allows the following syntax to mean exactly the same:
XHTML documents have a third option: to express the character encoding via XML declaration, as follows:
Note that as the character encoding can't be known until this declaration is parsed, there can be a problem knowing which encoding is used for the declaration itself. The main principle is that the declaration shall be encoded in pure ASCII, and therefore (if the declaration is inside the file) the encoding needs to be an ASCII extension. In order to allow encodings not backwards compatible with ASCII, browsers must be able to parse declarations in such encodings. Examples of such encodings are UTF-16BE and UTF-16LE.
As of HTML5 the recommended charset is UTF-8. An "encoding sniffing algorithm" is defined in the specification to determine the character encoding of the document based on multiple sources of input, including:
For ASCII-compatible character encodings the consequence of choosing incorrectly is that characters outside the printable ASCII range (32 to 126) usually appear incorrectly. This presents few problems for English-speaking users, but other languages regularly—in some cases, always—require characters outside that range. In CJK environments where there are several different multi-byte encodings in use, auto-detection is also often employed. Finally, browsers usually permit the user to override "incorrect" charset label manually as well.
It is increasingly common for multilingual websites and websites in non-Western languages to use UTF-8, which allows use of the same encoding for all languages. UTF-16 or UTF-32, which can be used for all languages as well, are less widely used because they can be harder to handle in programming languages that assume a byte-oriented ASCII superset encoding, and they are less efficient for text with a high frequency of ASCII characters, which is usually the case for HTML documents.
Successful viewing of a page is not necessarily an indication that its encoding is specified correctly. If the page's creator and reader are both assuming some platform-specific character encoding, and the server does not send any identifying information, then the reader will nonetheless see the page as the creator intended, but other readers on different platforms or with different native languages will not see the page as intended.
Character references.
In addition to native character encodings, characters can also be encoded as "character references", which can be "numeric character references" (decimal or hexadecimal) or "character entity references". Character entity references are also sometimes referred to as "named entities", or "HTML entities" for HTML. HTML's usage of character references derives from SGML.
HTML character references.
A "numeric character reference" in HTML refers to a character by its Universal Character Set/Unicode "code point", and uses the format
or
where "nnnn" is the code point in decimal form, and "hhhh" is the code point in hexadecimal form. The "x" must be lowercase in XML documents. The "nnnn" or "hhhh" may be any number of digits and may include leading zeros. The "hhhh" may mix uppercase and lowercase, though uppercase is the usual style.
Not all web browsers or email clients used by receivers of HTML documents, or text editors used by authors of HTML documents, will be able to render all HTML characters. Most modern software is able to display most or all of the characters for the user's language, and will draw a box or other clear indicator for characters they cannot render.
For codes from 0 to 127, the original 7-bit ASCII standard set, most of these characters can be used without a character reference. Codes from 160 to 255 can all be created using character entity names. Only a few higher-numbered codes can be created using entity names, but all can be created by decimal number character reference.
Character entity references can also have the format codice_6 where "name" is a case-sensitive alphanumeric string. For example, "λ" can also be encoded as codice_7 in an HTML document. The character entity references codice_8, codice_9, codice_10 and codice_11 are predefined in HTML and SGML, because codice_12, codice_13, codice_14 and codice_15 are already used to delimit markup. This notably does not include XML's codice_16 (') entity. For a list of all named HTML character entity references (about 250), see List of XML and HTML character entity references.
Unnecessary use of HTML character references may significantly reduce HTML readability. If the character encoding for a web page is chosen appropriately, then HTML character references are usually only required for markup delimiting characters as mentioned above, and for a few special characters (or none at all if a native Unicode encoding like UTF-8 is used). Incorrect HTML entity escaping may also open up security vulnerabilities for injection attacks such as cross-site scripting. If HTML attributes are left unquoted, certain characters, most importantly whitespace, such as space and tab, must be escaped using entities. Other languages related to HTML have their own methods of escaping characters.
Illegal characters.
HTML forbids the use of the characters with Universal Character Set/Unicode code points
The Unicode standard also forbids:
These characters are not allowed by numeric character references. However, references to characters 128–159 are commonly interpreted by lenient web browsers as if they were references to the characters assigned to "bytes" 128–159 (decimal) in the Windows-1252 character encoding. This is in violation of HTML and SGML standards, and the characters are already assigned to higher code points, so HTML documents should always use the higher code points. For example the trademark sign (™) should represented with codice_17 and not with codice_18.
The characters 9 (tab), 10 (linefeed), and 13 (carriage return) are allowed in HTML documents, but, along with 32 (space) are all considered "whitespace". The "form feed" control character, which would be at 12, is not allowed in HTML documents, but is also mentioned as being one of the "white space" characters – perhaps an oversight in the specifications. In HTML, most consecutive occurrences of white space characters, except in a codice_19 block, are interpreted as comprising a single "word separator" for rendering purposes. A word separator is typically rendered a single en-width space in European languages, but not in all the others.
XML character references.
Unlike traditional HTML with its large range of character entity references, in XML there are only five predefined character entity references. These are used to escape characters that are markup sensitive in certain contexts:
All other character entity references have to be defined before they can be used. For example, use of codice_25 (which gives é, Latin lower-case E with acute accent, U+00E9 in Unicode) in an XML document will generate an error unless the entity has already been defined. XML also requires that the codice_26 in hexadecimal numeric references be in lowercase: for example codice_27 rather than codice_28. XHTML, which is an XML application, supports the HTML entity set, along with XML's predefined entities.

</doc>
<doc id="5320" url="http://en.wikipedia.org/wiki?curid=5320" title="Carbon nanotube">
Carbon nanotube

Carbon nanotubes (CNTs) are allotropes of carbon with a cylindrical nanostructure. Nanotubes have been constructed with length-to-diameter ratio of up to 132,000,000:1, significantly larger than for any other material. These cylindrical carbon molecules have unusual properties, which are valuable for nanotechnology, electronics, optics and other fields of materials science and technology. In particular, owing to their extraordinary thermal conductivity and mechanical and electrical properties, carbon nanotubes find applications as additives to various structural materials. For instance, nanotubes form a tiny portion of the material(s) in some (primarily carbon fiber) baseball bats, golf clubs, or car parts.
Nanotubes are members of the fullerene structural family. Their name is derived from their long, hollow structure with the walls formed by one-atom-thick sheets of carbon, called graphene. These sheets are rolled at specific and discrete ("chiral") angles, and the combination of the rolling angle and radius decides the nanotube properties; for example, whether the individual nanotube shell is a metal or semiconductor. Nanotubes are categorized as single-walled nanotubes (SWNTs) and multi-walled nanotubes (MWNTs). Individual nanotubes naturally align themselves into "ropes" held together by van der Waals forces, more specifically, pi-stacking.
Applied quantum chemistry, specifically, orbital hybridization best describes chemical bonding in nanotubes. The chemical bonding of nanotubes is composed entirely of "sp"2 bonds, similar to those of graphite. These bonds, which are stronger than the "sp"3 bonds found in alkanes and diamond, provide nanotubes with their unique strength.
Types of carbon nanotubes and related structures.
Terminology.
There is no consensus on some terms describing carbon nanotubes in scientific literature: both "-wall" and "-walled" are being used in combination with "single", "double", "triple" or "multi", and the letter C is often omitted in the abbreviation; for example, multi-walled carbon nanotube (MWNT).
Single-walled.
Most single-walled nanotubes (SWNT) have a diameter of close to 1 nanometer, with a tube length that can be many millions of times longer. The structure of a SWNT can be conceptualized by wrapping a one-atom-thick layer of graphite called graphene into a seamless cylinder. The way the graphene sheet is wrapped is represented by a pair of indices ("n","m"). The integers "n" and "m" denote the number of unit vectors along two directions in the honeycomb crystal lattice of graphene. If "m" = 0, the nanotubes are called zigzag nanotubes, and if "n" = "m", the nanotubes are called armchair nanotubes. Otherwise, they are called chiral. The diameter of an ideal nanotube can be calculated from its (n,m) indices as follows
where "a" = 0.246 nm.
SWNTs are an important variety of carbon nanotube because most of their properties change significantly with the ("n","m") values, and this dependence is non-monotonic (see Kataura plot). In particular, their band gap can vary from zero to about 2 eV and their electrical conductivity can show metallic or semiconducting behavior. Single-walled nanotubes are likely candidates for miniaturizing electronics. The most basic building block of these systems is the electric wire, and SWNTs with diameters of an order of a nanometer can be excellent conductors. One useful application of SWNTs is in the development of the first intermolecular field-effect transistors (FET). The first intermolecular logic gate using SWCNT FETs was made in 2001. A logic gate requires both a p-FET and an n-FET. Because SWNTs are p-FETs when exposed to oxygen and n-FETs otherwise, it is possible to protect half of an SWNT from oxygen exposure, while exposing the other half to oxygen. This results in a single SWNT that acts as a "not" logic gate with both p and n-type FETs within the same molecule.
Single-walled nanotubes are dropping precipitously in price, from around $1500 per gram as of 2000 to retail prices of around $50 per gram of as-produced 40–60% by weight SWNTs as of March 2010.
Multi-walled.
Multi-walled nanotubes (MWNT) consist of multiple rolled layers (concentric tubes) of graphene. There are two models that can be used to describe the structures of multi-walled nanotubes. In the "Russian Doll" model, sheets of graphite are arranged in concentric cylinders, e.g., a (0,8) single-walled nanotube (SWNT) within a larger (0,17) single-walled nanotube. In the "Parchment" model, a single sheet of graphite is rolled in around itself, resembling a scroll of parchment or a rolled newspaper. The interlayer distance in multi-walled nanotubes is close to the distance between graphene layers in graphite, approximately 3.4 Å. The Russian Doll structure is observed more commonly. Its individual shells can be described as SWNTs, which can be metallic or semiconducting. Because of statistical probability and restrictions on the relative diameters of the individual tubes, one of the shells, and thus the whole MWNT, is usually a zero-gap metal.
Double-walled carbon nanotubes (DWNT) form a special class of nanotubes because their morphology and properties are similar to those of SWNT but their resistance to chemicals is significantly improved. This is especially important when functionalization is required (this means grafting of chemical functions at the surface of the nanotubes) to add new properties to the CNT. In the case of SWNT, covalent functionalization will break some C=C double bonds, leaving "holes" in the structure on the nanotube and, thus, modifying both its mechanical and electrical properties. In the case of DWNT, only the outer wall is modified. DWNT synthesis on the gram-scale was first proposed in 2003 by the CCVD technique, from the selective reduction of oxide solutions in methane and hydrogen.
The telescopic motion ability of inner shells and their unique mechanical properties will permit the use of multi-walled nanotubes as main movable arms in coming nanomechanical devices. Retraction force that occurs to telescopic motion caused by the Lennard-Jones interaction between shells and its value is about 1.5 nN.
Torus.
In theory, a nanotorus is a carbon nanotube bent into a torus (doughnut shape). Nanotori are predicted to have many unique properties, such as magnetic moments 1000 times larger than previously expected for certain specific radii. Properties such as magnetic moment, thermal stability, etc. vary widely depending on radius of the torus and radius of the tube.
Nanobud.
Carbon nanobuds are a newly created material combining two previously discovered allotropes of carbon: carbon nanotubes and fullerenes. In this new material, fullerene-like "buds" are covalently bonded to the outer sidewalls of the underlying carbon nanotube. This hybrid material has useful properties of both fullerenes and carbon nanotubes. In particular, they have been found to be exceptionally good field emitters. In composite materials, the attached fullerene molecules may function as molecular anchors preventing slipping of the nanotubes, thus improving the composite’s mechanical properties.
Three-dimensional carbon nanotube architectures.
Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures maybe used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.
In addition, the mechanical behaviour of carbon nanotube micro-architectures can easily be modified by the infiltration and deposition of thin conformal coatings.
Graphenated carbon nanotubes (g-CNTs).
Graphenated CNTs are a relatively new hybrid that combines graphitic foliates grown along the sidewalls of multiwalled or bamboo style CNTs. Yu "et al." reported on "chemically bonded graphene leaves" growing along the sidewalls of CNTs. Stoner "et al." described these structures as "graphenated CNTs" and reported in their use for enhanced supercapacitor performance. Hsu "et al." further reported on similar structures formed on carbon fiber paper, also for use in supercapacitor applications. The foliate density can vary as a function of deposition conditions (e.g. temperature and time) with their structure ranging from few layers of graphene (< 10) to thicker, more graphite-like.
The fundamental advantage of an integrated graphene-CNT structure is the high surface area three-dimensional framework of the CNTs coupled with the high edge density of graphene. Graphene edges provide significantly higher charge density and reactivity than the basal plane, but they are difficult to arrange in a three-dimensional, high volume-density geometry. CNTs are readily aligned in a high density geometry (i.e., a vertically aligned forest) but lack high charge density surfaces—the sidewalls of the CNTs are similar to the basal plane of graphene and exhibit low charge density except where edge defects exist. Depositing a high density of graphene foliates along the length of aligned CNTs can significantly increase the total charge capacity per unit of nominal area as compared to other carbon nanostructures.
Nitrogen-doped carbon nanotubes.
Nitrogen doped carbon nanotubes (N-CNTs), can be produced through five main methods, chemical vapor deposition, high-temperature and high-pressure reactions, gas-solid reaction of amorphous carbon with NH3 at high temperature, solid reaction, and solvothermal synthesis.
N-CNTs can also be prepared by a CVD method of pyrolysizing melamine under Ar at elevated temperatures of 800–980 °C. However synthesis by CVD of melamine results in the formation of bamboo-structured CNTs. XPS spectra of grown N-CNTs reveals nitrogen in five main components, pyridinic nitrogen, pyrrolic nitrogen, quaternary nitrogen, and nitrogen oxides. Furthermore synthesis temperature affects the type of nitrogen configuration.
Nitrogen doping plays a pivotal role in lithium storage, as it creates defects in the CNT walls allowing for Li ions to diffuse into interwall space. It also increases capacity by providing more favorable bind of N-doped sites. N-CNTs are also much more reactive to metal oxide nanoparticle deposition which can further enhance storage capacity, especially in anode materials for Li-ion batteries. However boron-doped nanotubes have been shown to make batteries with triple capacity.
Peapod.
A carbon peapod is a novel hybrid carbon material which traps fullerene inside a carbon nanotube. It can possess interesting magnetic properties with heating and irradiating. It can also be applied as an oscillator during theoretical investigations and predictions.
Cup-stacked carbon nanotubes.
Cup-stacked carbon nanotubes (CSCNTs) differ from other quasi-1D carbon structures, which normally behave as quasi-metallic conductors of electrons. CSCNTs exhibit semiconducting behaviors due to the stacking microstructure of graphene layers.
Extreme carbon nanotubes.
The observation of the "longest" carbon nanotubes grown so far are over 1/2 m (550 mm long) was reported in 2013. These nanotubes were grown on Si substrates using an improved chemical vapor deposition (CVD) method and represent electrically uniform arrays of single-walled carbon nanotubes.
The "shortest" carbon nanotube is the organic compound cycloparaphenylene, which was synthesized in early 2009.
The "thinnest" carbon nanotube is armchair (2,2) CNT with a diameter of 3 Å. This nanotube was grown inside a multi-walled carbon nanotube. Assigning of carbon nanotube type was done by combination of high-resolution transmission electron microscopy (HRTEM), Raman spectroscopy and density functional theory (DFT) calculations.
The "thinnest freestanding" single-walled carbon nanotube is about 4.3 Å in diameter. Researchers suggested that it can be either (5,1) or (4,2) SWCNT, but exact type of carbon nanotube remains questionable. (3,3), (4,3) and (5,1) carbon nanotubes (all about 4 Å in diameter) were unambiguously identified using aberration-corrected high-resolution transmission electron microscopy inside double-walled CNTs.
The "highest density" of CNTs was achieved in 2013, grown on a conductive titanium-coated copper surface that was coated with co-catalysts cobalt and molybdenum at lower than typical temperatures of 450 °C. The tubes averaged a height of 0.38 μm and a mass density of 1.6 g cm−3. The material showed ohmic conductivity (lowest resistance ∼22 kΩ).
Properties.
Strength.
Carbon nanotubes are the strongest and stiffest materials yet discovered in terms of tensile strength and elastic modulus respectively. This strength results from the covalent sp2 bonds formed between the individual carbon atoms. In 2000, a multi-walled carbon nanotube was tested to have a tensile strength of 63 gigapascals (GPa). (For illustration, this translates into the ability to endure tension of a weight equivalent to 64220 N (14,158 lbs) on a cable with cross-section of 1 mm2.) Further studies, such as one conducted in 2008, revealed that individual CNT shells have strengths of up to ~100 GPa, which is in agreement with quantum/atomistic models. Since carbon nanotubes have a low density for a solid of 1.3 to 1.4 g/cm3, its specific strength of up to 48,000 kN·m·kg−1 is the best of known materials, compared to high-carbon steel's 154 kN·m·kg−1.
Under excessive tensile strain, the tubes will undergo plastic deformation, which means the deformation is permanent. This deformation begins at strains of approximately 5% and can increase the maximum strain the tubes undergo before fracture by releasing strain energy.
Although the strength of individual CNT shells is extremely high, weak shear interactions between adjacent shells and tubes leads to significant reductions in the effective strength of multi-walled carbon nanotubes and carbon nanotube bundles down to only a few GPa. This limitation has been recently addressed by applying high-energy electron irradiation, which crosslinks inner shells and tubes, and effectively increases the strength of these materials to ~60 GPa for multi-walled carbon nanotubes and ~17 GPa for double-walled carbon nanotube bundles.
CNTs are not nearly as strong under compression. Because of their hollow structure and high aspect ratio, they tend to undergo buckling when placed under compressive, torsional, or bending stress.
EExperimental observation; TTheoretical prediction
The above discussion referred to axial properties of the nanotube, whereas simple geometrical considerations suggest that carbon nanotubes should be much softer in the radial direction than along the tube axis. Indeed, TEM observation of radial elasticity suggested that even the van der Waals forces can deform two adjacent nanotubes. Nanoindentation experiments, performed by several groups on multiwalled carbon nanotubes and tapping/contact mode atomic force microscope measurements performed on single-walled carbon nanotubes, indicated a Young's modulus of the order of several GPa, confirming that CNTs are indeed rather soft in the radial direction.
Hardness.
Standard single-walled carbon nanotubes can withstand a pressure up to 25 GPa without deformation. They then undergo a transformation to superhard phase nanotubes. Maximum pressures measured using current experimental techniques are around 55 GPa. However, these new superhard phase nanotubes collapse at an even higher, albeit unknown, pressure.
The bulk modulus of superhard phase nanotubes is 462 to 546 GPa, even higher than that of diamond (420 GPa for single diamond crystal).
Kinetic properties.
Multi-walled nanotubes are multiple concentric nanotubes precisely nested within one another. These exhibit a striking telescoping property whereby an inner nanotube core may slide, almost without friction, within its outer nanotube shell, thus creating an atomically perfect linear or rotational bearing.
This is one of the first true examples of molecular nanotechnology, the precise positioning of atoms to create useful machines. Already, this property has been utilized to create the world's smallest rotational motor. Future applications such as a gigahertz mechanical oscillator are also envisioned.
Electrical properties.
Because of the symmetry and unique electronic structure of graphene, the structure of a nanotube strongly affects its electrical properties. For a given ("n","m") nanotube, if "n" = "m", the nanotube is metallic; if "n" − "m" is a multiple of 3, then the nanotube is semiconducting with a very small band gap, otherwise the nanotube is a moderate semiconductor. Thus all armchair ("n" = "m") nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.
However, this rule has exceptions, because curvature effects in small diameter tubes can strongly influence electrical properties. Thus, a (5,0) SWCNT that should be semiconducting in fact is metallic according to the calculations. Likewise, zigzag and chiral SWCNTs with small diameters that should be metallic have a finite gap (armchair nanotubes remain metallic). In theory, metallic nanotubes can carry an electric current density of 4 × 109 A/cm2, which is more than 1,000 times greater than those of metals such as copper, where for copper interconnects current densities are limited by electromigration.
Because of its nanoscale cross-section, electrons propagate only along the tube's axis. As a result, carbon nanotubes are frequently referred to as one-dimensional conductors. The maximum electrical conductance of a single-walled carbon nanotube is 2"G"0, where "G"0 = 2"e"2/"h" is the conductance of a single ballistic quantum channel.
Intrinsic superconductivity has been reported, although other experiments found no evidence of this, leaving the claim a subject of debate.
Thermal properties.
All nanotubes are expected to be very good thermal conductors along the tube, exhibiting a property known as "ballistic conduction", but good insulators laterally to the tube axis. Measurements show that a SWNT has a room-temperature thermal conductivity along its axis of about 3500 W·m−1·K−1; compare this to copper, a metal well known for its good thermal conductivity, which transmits 385 W·m−1·K−1. A SWNT has a room-temperature thermal conductivity across its axis (in the radial direction) of about 1.52 W·m−1·K−1, which is about as thermally conductive as soil. The temperature stability of carbon nanotubes is estimated to be up to 2800 °C in vacuum and about 750 °C in air.
Defects.
As with any material, the existence of a crystallographic defect affects the material properties. Defects can occur in the form of atomic vacancies. High levels of such defects can lower the tensile strength by up to 85%. An important example is the Stone Wales defect, which creates a pentagon and heptagon pair by rearrangement of the bonds. Because of the very small structure of CNTs, the tensile strength of the tube is dependent on its weakest segment in a similar manner to a chain, where the strength of the weakest link becomes the maximum strength of the chain.
Crystallographic defects also affect the tube's electrical properties. A common result is lowered conductivity through the defective region of the tube. A defect in armchair-type tubes (which can conduct electricity) can cause the surrounding region to become semiconducting, and single monatomic vacancies induce magnetic properties.
Crystallographic defects strongly affect the tube's thermal properties. Such defects lead to phonon scattering, which in turn increases the relaxation rate of the phonons. This reduces the mean free path and reduces the thermal conductivity of nanotube structures. Phonon transport simulations indicate that substitutional defects such as nitrogen or boron will primarily lead to scattering of high-frequency optical phonons. However, larger-scale defects such as Stone Wales defects cause phonon scattering over a wide range of frequencies, leading to a greater reduction in thermal conductivity.
Toxicity.
The toxicity of carbon nanotubes has been an important question in nanotechnology. As of 2007, such research has just begun. The data are still fragmentary and subject to criticism. Preliminary results highlight the difficulties in evaluating the toxicity of this heterogeneous material. Parameters such as structure, size distribution, surface area, surface chemistry, surface charge, and agglomeration state as well as purity of the samples, have considerable impact on the reactivity of carbon nanotubes. However, available data clearly show that, under some conditions, nanotubes can cross membrane barriers, which suggests that, if raw materials reach the organs, they can induce harmful effects such as inflammatory and fibrotic reactions.
Under certain conditions CNTs can enter human cells and accumulate in the cytoplasm, causing cell death.
Results of rodent studies collectively show that regardless of the process by which CNTs were synthesized and the types and amounts of metals they contained, CNTs were capable of producing inflammation, epithelioid granulomas (microscopic nodules), fibrosis, and biochemical/toxicological changes in the lungs. Comparative toxicity studies in which mice were given equal weights of test materials showed that SWCNTs were more toxic than quartz, which is considered a serious occupational health hazard when chronically inhaled. As a control, ultrafine carbon black was shown to produce minimal lung responses.
Carbon nanotubes deposit in the alveolar ducts by aligning lengthwise with the airways; the nanotubes will often combine with metals. The needle-like fiber shape of CNTs is similar to asbestos fibers. This raises the idea that widespread use of carbon nanotubes may lead to pleural mesothelioma, a cancer of the lining of the lungs, or peritoneal mesothelioma, a cancer of the lining of the abdomen (both caused by exposure to asbestos). A recently published pilot study supports this prediction. Scientists exposed the mesothelial lining of the body cavity of mice to long multiwalled carbon nanotubes and observed asbestos-like, length-dependent, pathogenic behavior that included inflammation and formation of lesions known as granulomas.
Authors of the study conclude:
Although further research is required, the available data suggest that under certain conditions, especially those involving chronic exposure, carbon nanotubes can pose a serious risk to human health.
Synthesis.
Techniques have been developed to produce nanotubes in sizable quantities, including arc discharge, laser ablation, high-pressure carbon monoxide disproportionation, and chemical vapor deposition (CVD). Most of these processes take place in a vacuum or with process gases. CVD growth of CNTs can occur in vacuum or at atmospheric pressure. Large quantities of nanotubes can be synthesized by these methods; advances in catalysis and continuous growth are making CNTs more commercially viable.
Arc discharge.
Nanotubes were observed in 1991 in the carbon soot of graphite electrodes during an arc discharge, by using a current of 100 amps, that was intended to produce fullerenes. However the first macroscopic production of carbon nanotubes was made in 1992 by two researchers at NEC's Fundamental Research Laboratory. The method used was the same as in 1991. During this process, the carbon contained in the negative electrode sublimates because of the high-discharge temperatures.
The yield for this method is up to 30% by weight and it produces both single- and multi-walled nanotubes with lengths of up to 50 micrometers with few structural defects.
Laser ablation.
In laser ablation, a pulsed laser vaporizes a graphite target in a high-temperature reactor while an inert gas is bled into the chamber. Nanotubes develop on the cooler surfaces of the reactor as the vaporized carbon condenses. A water-cooled surface may be included in the system to collect the nanotubes.
This process was developed by Dr. Richard Smalley and co-workers at Rice University, who at the time of the discovery of carbon nanotubes, were blasting metals with a laser to produce various metal molecules. When they heard of the existence of nanotubes they replaced the metals with graphite to create multi-walled carbon nanotubes. Later that year the team used a composite of graphite and metal catalyst particles (the best yield was from a cobalt and nickel mixture) to synthesize single-walled carbon nanotubes.
The laser ablation method yields around 70% and produces primarily single-walled carbon nanotubes with a controllable diameter determined by the reaction temperature. However, it is more expensive than either arc discharge or chemical vapor deposition.
Plasma torch.
Single-walled carbon nanotubes can also be synthesized by a thermal plasma method. It was first invented in 2000 at INRS (Institut National de la Recherche Scientifique in Varennes, Canada), by Olivier Smiljanic. In this method, the aim is to reproduce the conditions prevailing in the arc discharge and laser ablation approaches, but a carbon-containing gas is used instead of graphite vapors to supply the carbon necessary for the production of SWNT. Doing so, the growth of SWNT is more efficient (decomposing a carbon containing gas can be 10 times less energy-consuming than graphite vaporization). It is also continuous and occurs at low cost. To produce a continuous process, a gas mixture composed of argon, ethylene and ferrocene is introduced into a microwave plasma torch, where it is atomized by the atmospheric pressure plasma, which has the form of an intense 'flame'. The fumes created by the flame are found to contain SWNT, metallic and carbon nanoparticles and amorphous carbon.
Another way to produce single-walled carbon nanotubes with a plasma torch, is to use the induction thermal plasma method, implemented in 2005 by groups from the University of Sherbrooke and the National Research Council of Canada. The method is similar to arc-discharge in that both use ionized gas to reach the high temperature necessary to vaporize carbon-containing substances and the metal catalysts necessary for the ensuing nanotube growth. The thermal plasma is induced by high frequency oscillating currents in a coil, and is maintained in flowing inert gas. Typically, a feedstock of carbon black and metal catalyst particles is fed into the plasma, and then cooled down to form single-walled carbon nanotubes. Different single-wall carbon nanotube diameter distributions can be synthesized.
The induction thermal plasma method can produce up to 2 grams of nanotube material per minute, which is higher than the arc-discharge or the laser ablation methods.
Chemical vapor deposition (CVD).
The catalytic vapor phase deposition of carbon was reported in 1952 and 1959, but it was not until 1993 that carbon nanotubes were formed by this process. In 2007, researchers at the University of Cincinnati (UC) developed a process to grow aligned carbon nanotube arrays of length 18 mm on a FirstNano ET3000 carbon nanotube growth system.
During CVD, a substrate is prepared with a layer of metal catalyst particles, most commonly nickel, cobalt, iron, or a combination. The metal nanoparticles can also be produced by other ways, including reduction of oxides or oxides solid solutions. The diameters of the nanotubes that are to be grown are related to the size of the metal particles. This can be controlled by patterned (or masked) deposition of the metal, annealing, or by plasma etching of a metal layer. The substrate is heated to approximately 700 °C. To initiate the growth of nanotubes, two gases are bled into the reactor: a process gas (such as ammonia, nitrogen or hydrogen) and a carbon-containing gas (such as acetylene, ethylene, ethanol or methane). Nanotubes grow at the sites of the metal catalyst; the carbon-containing gas is broken apart at the surface of the catalyst particle, and the carbon is transported to the edges of the particle, where it forms the nanotubes. This mechanism is still being studied. The catalyst particles can stay at the tips of the growing nanotube during growth, or remain at the nanotube base, depending on the adhesion between the catalyst particle and the substrate. Thermal catalytic decomposition of hydrocarbon has become an active area of research and can be a promising route for the bulk production of CNTs. Fluidised bed reactor is the most widely used reactor for CNT preparation. Scale-up of the reactor is the major challenge.
CVD is the most widely used method for the production of carbon nanotubes. For this purpose, the metal nanoparticles are mixed with a catalyst support such as MgO or Al2O3 to increase the surface area for higher yield of the catalytic reaction of the carbon feedstock with the metal particles. One issue in this synthesis route is the removal of the catalyst support via an acid treatment, which sometimes could destroy the original structure of the carbon nanotubes. However, alternative catalyst supports that are soluble in water have proven effective for nanotube growth.
If a plasma is generated by the application of a strong electric field during growth (plasma-enhanced chemical vapor deposition), then the nanotube growth will follow the direction of the electric field. By adjusting the geometry of the reactor it is possible to synthesize vertically aligned carbon nanotubes (i.e., perpendicular to the substrate), a morphology that has been of interest to researchers interested in electron emission from nanotubes. Without the plasma, the resulting nanotubes are often randomly oriented. Under certain reaction conditions, even in the absence of a plasma, closely spaced nanotubes will maintain a vertical growth direction resulting in a dense array of tubes resembling a carpet or forest.
Of the various means for nanotube synthesis, CVD shows the most promise for industrial-scale deposition, because of its price/unit ratio, and because CVD is capable of growing nanotubes directly on a desired substrate, whereas the nanotubes must be collected in the other growth techniques. The growth sites are controllable by careful deposition of the catalyst. In 2007, a team from Meijo University demonstrated a high-efficiency CVD technique for growing carbon nanotubes from camphor. Researchers at Rice University, until recently led by the late Richard Smalley, have concentrated upon finding methods to produce large, pure amounts of particular types of nanotubes. Their approach grows long fibers from many small seeds cut from a single nanotube; all of the resulting fibers were found to be of the same diameter as the original nanotube and are expected to be of the same type as the original nanotube.
Super-growth CVD.
Super-growth CVD (water-assisted chemical vapor deposition) was developed by Kenji Hata, Sumio Iijima and co-workers at AIST, Japan. In this process, the activity and lifetime of the catalyst are enhanced by addition of water into the CVD reactor. Dense millimeter-tall nanotube "forests", aligned normal to the substrate, were produced. The forests growth rate could be expressed, as
In this equation, β is the initial growth rate and formula_3 is the characteristic catalyst lifetime.
Their specific surface exceeds 1,000 m2/g (capped) or 2,200 m2/g (uncapped), surpassing the value of 400–1,000 m2/g for HiPco samples. The synthesis efficiency is about 100 times higher than for the laser ablation method. The time required to make SWNT forests of the height of 2.5 mm by this method was 10 minutes in 2004. Those SWNT forests can be easily separated from the catalyst, yielding clean SWNT material (purity >99.98%) without further purification. For comparison, the as-grown HiPco CNTs contain about 5–35% of metal impurities; it is therefore purified through dispersion and centrifugation that damages the nanotubes. Super-growth avoids this problem. Patterned highly organized single-walled nanotube structures were successfully fabricated using the super-growth technique.
The mass density of super-growth CNTs is about 0.037 g/cm3. It is much lower than that of conventional CNT powders (~1.34 g/cm3), probably because the latter contain metals and amorphous carbon.
The super-growth method is basically a variation of CVD. Therefore, it is possible to grow material containing SWNT, DWNTs and MWNTs, and to alter their ratios by tuning the growth conditions. Their ratios change by the thinness of the catalyst. Many MWNTs are included so that the diameter of the tube is wide.
The vertically aligned nanotube forests originate from a "zipping effect" when they are immersed in a solvent and dried. The zipping effect is caused by the surface tension of the solvent and the van der Waals forces between the carbon nanotubes. It aligns the nanotubes into a dense material, which can be formed in various shapes, such as sheets and bars, by applying weak compression during the process. Densification increases the Vickers hardness by about 70 times and density is 0.55 g/cm3. The packed carbon nanotubes are more than 1 mm long and have a carbon purity of 99.9% or higher; they also retain the desirable alignment properties of the nanotubes forest.
Natural, incidental, and controlled flame environments.
Fullerenes and carbon nanotubes are not necessarily products of high-tech laboratories; they are commonly formed in such mundane places as ordinary flames, produced by burning methane, ethylene, and benzene, and they have been found in soot from both indoor and outdoor air. However, these naturally occurring varieties can be highly irregular in size and quality because the environment in which they are produced is often highly uncontrolled. Thus, although they can be used in some applications, they can lack in the high degree of uniformity necessary to satisfy the many needs of both research and industry. Recent efforts have focused on producing more uniform carbon nanotubes in controlled flame environments. Such methods have promise for large-scale, low-cost nanotube synthesis based on theoretical models, though they must compete with rapidly developing large scale CVD production.
Removal of catalysts.
Nanoscale metal catalysts are important ingredients for fixed- and fluidized-bed CVD synthesis of CNTs. They allow increasing the growth efficiency of CNTs and may give control over their structure and chirality. During synthesis, catalysts can convert carbon precursors into tubular carbon structures but can also form encapsulating carbon overcoats. Together with metal oxide supports they may therefore attach to or become incorporated into the CNT product. The presence of metal impurities can be problematic for many applications. Especially catalyst metals like nickel, cobalt or yttrium may be of toxicological concern. While unencapsulated catalyst metals may be readily removable by acid washing, encapsulated ones require oxidative treatment for opening their carbon shell. The effective removal of catalysts, especially of encapsulated ones, while preserving the CNT structure is a challenge and has been addressed in many studies. A new approach to break carbonaceaous catalyst encapsulations is based on rapid thermal annealing.
Application-related issues.
Many electronic applications of carbon nanotubes crucially rely on techniques of selectively producing either semiconducting or metallic CNTs, preferably of a certain chirality. Several methods of separating semiconducting and metallic CNTs are known, but most of them are not yet suitable for large-scale technological processes. The most efficient method relies on density-gradient ultracentrifugation, which separates surfactant-wrapped nanotubes by the minute difference in their density. This density difference often translates into difference in the nanotube diameter and (semi)conducting properties. Another method of separation uses a sequence of freezing, thawing, and compression of SWNTs embedded in agarose gel. This process results in a solution containing 70% metallic SWNTs and leaves a gel containing 95% semiconducting SWNTs. The diluted solutions separated by this method show various colors. The separated carbon nanotubes using this method have been applied to electrodes, e.g. electric double-layer capacitor. Moreover, SWNTs can be separated by the column chromatography method. Yield is 95% in semiconductor type SWNT and 90% in metallic type SWNT.
In addition to separation of semiconducting and metallic SWNTs, it is possible to sort SWNTs by length, diameter, and chirality. The highest resolution length sorting, with length variation of <10%, has thus far been achieved by size exclusion chromatography (SEC) of DNA-dispersed carbon nanotubes (DNA-SWNT). SWNT diameter separation has been achieved by density-gradient ultracentrifugation (DGU) using surfactant-dispersed SWNTs and by ion-exchange chromatography (IEC) for DNA-SWNT. Purification of individual chiralities has also been demonstrated with IEC of DNA-SWNT: specific short DNA oligomers can be used to isolate individual SWNT chiralities. Thus far, 12 chiralities have been isolated at purities ranging from 70% for (8,3) and (9,5) SWNTs to 90% for (6,5), (7,5) and (10,5)SWNTs. There have been successful efforts to integrate these purified nanotubes into devices, e. g. FETs.
An alternative to separation is development of a selective growth of semiconducting or metallic CNTs. Recently, a new CVD recipe that involves a combination of ethanol and methanol gases and quartz substrates resulting in horizontally aligned arrays of 95–98% semiconducting nanotubes was announced.
Nanotubes are usually grown on nanoparticles of magnetic metal (Fe, Co), which facilitates production of electronic (spintronic) devices. In particular, control of current through a field-effect transistor by magnetic field has been demonstrated in such a single-tube nanostructure.
Current applications.
Current use and application of nanotubes has mostly been limited to the use of bulk nanotubes, which is a mass of rather unorganized fragments of nanotubes. Bulk nanotube materials may never achieve a tensile strength similar to that of individual tubes, but such composites may, nevertheless, yield strengths sufficient for many applications. Bulk carbon nanotubes have already been used as composite fibers in polymers to improve the mechanical, thermal and electrical properties of the bulk product.
Other current applications include:
There is also ongoing research in using carbon nanotubes as a scaffold for diverse microfabrication techniques.
Potential applications.
The strength and flexibility of carbon nanotubes makes them of potential use in controlling other nanoscale structures, which suggests they will have an important role in nanotechnology engineering. The highest tensile strength of an individual multi-walled carbon nanotube has been tested to be 63 GPa. Carbon nanotubes were found in Damascus steel from the 17th century, possibly helping to account for the legendary strength of the swords made of it. Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures maybe used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.
Biomedical.
Researchers from Rice University and State University of New York - Stony Brook have shown that the addition of low weight % of carbon nanotubes can lead to significant improvements in the mechanical properties of biodegradable polymeric nanocomposites for applications in bone tissue engineering. Dispersion of low weight % of graphene (~0.02 wt.%) results in significant increases in compressive and flexural mechanical properties of polymeric nanocomposites. Researchers at Rice University, Radboud University Nijmegen Medical Centre and University of California, Riverside have shown that carbon nanotubes and their polymer nanocomposites are suitable scaffold materials for bone cell proliferation and bone formation.
In November 2012 researchers at the American National Institute of Standards and Technology (NIST) proved that single-wall carbon nanotubes may help protect DNA molecules from damage by oxidation.
A highly effective method of delivering carbon nanotubes into cells is Cell squeezing, a high-throughput vector-free microfluidic platform for intracellular delivery developed at the Massachusetts Institute of Technology in the labs of Robert S. Langer.
Structural.
Because of the carbon nanotube's superior mechanical properties, many structures have been proposed ranging from everyday items like clothes and sports gear to combat jackets and space elevators. However, the space elevator will require further efforts in refining carbon nanotube technology, as the practical tensile strength of carbon nanotubes must be greatly improved.
For perspective, outstanding breakthroughs have already been made. Pioneering work led by Ray H. Baughman at the NanoTech Institute has shown that single and multi-walled nanotubes can produce materials with toughness unmatched in the man-made and natural worlds.
Carbon nanotubes are also a promising material as building blocks in bio-mimetic hierarchical composite materials given their exceptional mechanical properties (~1 TPa in modulus, and ~100 GPa in strength). Initial attempts to incorporate CNTs into hierarchical structures led to mechanical properties that were significantly lower than these achievable limits. Windle "et al." have used an "in situ" chemical vapor deposition (CVD) spinning method to produce continuous CNT yarns from CVD-grown CNT aerogels. With this technology, they fabricated CNT yarns with strengths as high as ~9 GPa at small gage lengths of ~1 mm, however, defects resulted in a reduction of specific strength to ~1 GPa at 20 mm gage length. Espinosa "et al." developed high performance DWNT-polymer composite yarns by twisting and stretching ribbons of randomly oriented bundles of DWNTs thinly coated with polymeric organic compounds. These DWNT-polymer yarns exhibited an unusually high energy to failure of ~100 J·g−1 (comparable to one of the toughest natural materials – spider silk), and strength as high as ~1.4 GPa. Effort is ongoing to produce CNT composites that incorporate tougher matrix materials, such as Kevlar, to further improve on the mechanical properties toward those of individual CNTs.
Because of the high mechanical strength of carbon nanotubes, research is being made into weaving them into clothes to create stab-proof and bulletproof clothing. The nanotubes would effectively stop the bullet from penetrating the body, although the bullet's kinetic energy would likely cause broken bones and internal bleeding.
Electrical circuits.
Nanotube-based transistors, also known as carbon nanotube field-effect transistors (CNTFETs), have been made that operate at room temperature and that are capable of digital switching using a single electron. However, one major obstacle to realization of nanotubes has been the lack of technology for mass production. In 2001 IBM researchers demonstrated how metallic nanotubes can be destroyed, leaving semiconducting ones behind for use as transistors. Their process is called "constructive destruction," which includes the automatic destruction of defective nanotubes on the wafer. This process, however, only gives control over the electrical properties on a statistical scale.
The potential of carbon nanotubes was demonstrated in 2003 when room-temperature ballistic transistors with ohmic metal contacts and high-k gate dielectric were reported, showing 20–30x higher ON current than state-of-the-art Si MOSFETs. This presented an important advance in the field as CNT was shown to potentially outperform Si. At the time, a major challenge was ohmic metal contact formation. In this regard, palladium, which is a high-work function metal was shown to exhibit Schottky barrier-free contacts to semiconducting nanotubes with diameters >1.7 nm.
The first nanotube integrated memory circuit was made in 2004. One of the main challenges has been regulating the conductivity of nanotubes. Depending on subtle surface features a nanotube may act as a plain conductor or as a semiconductor. A fully automated method has however been developed to remove non-semiconductor tubes.
Another way to make carbon nanotube transistors has been to use random networks of them. By doing so one averages all of their electrical differences and one can produce devices in large scale at the wafer level. This approach was first patented by Nanomix Inc. (date of original application June 2002). It was first published in the academic literature by the United States Naval Research Laboratory in 2003 through independent research work. This approach also enabled Nanomix to make the first transistor on a flexible and transparent substrate.
Large structures of carbon nanotubes can be used for thermal management of electronic circuits. An approximately 1 mm–thick carbon nanotube layer was used as a special material to fabricate coolers, this material has very low density, ~20 times lower weight than a similar copper structure, while the cooling properties are similar for the two materials.
In 2013, researchers demonstrated a Turing-complete prototype micrometer-scale computer. Carbon nanotube transistors as logic-gate circuits with densities comparable to modern CMOS technology has not yet been demonstrated.
Electrical cables and wires.
Wires for carrying electrical current may be fabricated from pure nanotubes and nanotube-polymer composites. It has already been demonstrated that carbon nanotube wires can successfully be used for power or data transmission. Recently small wires have been fabricated with specific conductivity exceeding copper and aluminum; these cables are the highest conductivity carbon nanotube and also highest conductivity non-metal cables.
Recently, composite of carbon nanotube and copper have been shown to exhibit nearly one hundred times higher current-carrying-capacity than pure copper or gold. Significantly, the electrical conductivity of such a composite is similar to pure Cu. Thus, this Carbon nanotube-copper (CNT-Cu) composite possesses the highest observed current-carrying capacity among electrical conductors. Thus for a given cross-section of electrical conductor, the CNT-Cu composite can withstand and transport one hundred times higher current compared to metals such as copper and gold.
Actuators.
The exceptional electrical and mechanical properties of carbon nanotubes have made them alternatives to the traditional electrical actuators for both microscopic and macroscopic applications. Carbon nanotubes are very good conductors of both electricity and heat, and they are also very strong and elastic molecules in certain directions.
Paper batteries.
A paper battery is a battery engineered to use a paper-thin sheet of cellulose (which is the major constituent of regular paper, among other things) infused with aligned carbon nanotubes. The nanotubes act as electrodes; allowing the storage devices to conduct electricity. The battery, which functions as both a lithium-ion battery and a supercapacitor, can provide a long, steady power output comparable to a conventional battery, as well as a supercapacitor’s quick burst of high power—and while a conventional battery contains a number of separate components, the paper battery integrates all of the battery components in a single structure, making it more energy efficient.
Solar cells.
One of the promising applications of single-walled carbon nanotubes (SWNTs) is their use in solar panels, due to their strong UV/Vis-NIR absorption characteristics. Research has shown that they can provide a sizeable increase in efficiency, even at their current unoptimized state. Solar cells developed at the New Jersey Institute of Technology use a carbon nanotube complex, formed by a mixture of carbon nanotubes and carbon buckyballs (known as fullerenes) to form snake-like structures. Buckyballs trap electrons, but they can't make electrons flow. Add sunlight to excite the polymers, and the buckyballs will grab the electrons. Nanotubes, behaving like copper wires, will then be able to make the electrons or current flow.
Additional research has been conducted on creating SWNT hybrid solar panels to increase the efficiency further. These hybrids are created by combining SWNT's with photexcitable electron donors to increase the number of electrons generated. It has been found that the interaction between the photoexcited porphyrin and SWNT generates electro-hole pairs at the SWNT surfaces. This phenomenon has been observed experimentally, and contributes practically to an increase in efficiency up to 8.5%.
Hydrogen storage.
In addition to being able to store electrical energy, there has been some research in using carbon nanotubes to store hydrogen to be used as a fuel source. By taking advantage of the capillary effects of the small carbon nanotubes, it is possible to condense gases in high density inside single-walled nanotubes. This allows for gases, most notably hydrogen (H2), to be stored at high densities without being condensed into a liquid. Potentially, this storage method could be used on vehicles in place of gas fuel tanks for a hydrogen-powered car. A current issue regarding hydrogen-powered vehicles is the onboard storage of the fuel. Current storage methods involve cooling and condensing the H2 gas to a liquid state for storage which causes a loss of potential energy (25–45%) when compared to the energy associated with the gaseous state. Storage using SWNTs would allow one to keep the H2 in its gaseous state, thereby increasing the storage effciency. This method allows for a volume to energy ratio slightly smaller to that of current gas powered vehicles, allowing for a slightly lower but comparable range.
An area of controversy and frequent experimentation regarding the storage of hydrogen by adsorption in carbon nanotubes is the efficiency by which this process occurs. The effectiveness of hydrogen storage is integral to its use as a primary fuel source since hydrogen only contains about one fourth the energy per unit volume as gasoline.
Experimental capacity.
One experiment sought to determine the amount of hydrogen stored in CNTs by utilizing elastic recoil detection analysis (ERDA). CNTs (primarily SWNTs) were synthesized via chemical vapor disposition (CVD) and subjected to a two-stage purification process including air oxidation and acid treatment, then formed into flat, uniform discs and exposed to pure, pressurized hydrogen at various temperatures. When the data was analyzed, it was found that the ability of CNTs to store hydrogen decreased as temperature increased. Moreover, the highest hydrogen concentration measured was ~0.18%; significantly lower than commercially viable hydrogen storage needs to be. A separate experimental work performed by using a gravimetric method also revealed the maximum hydrogen uptake capacity of CNTs to be as low as 0.2%.
In another experiment, CNTs were synthesized via CVD and their structure was characterized using Raman spectroscopy. Utilizing microwave digestion, the samples were exposed to different acid concentrations and different temperatures for various amounts of time in an attempt to find the optimum purification method for SWNTs of the diameter determined earlier. The purified samples were then exposed to hydrogen gas at various high pressures, and their adsorption by weight percent was plotted. The data showed that hydrogen adsorption levels of up to 3.7% are possible with a very pure sample and under the proper conditions. It is thought that microwave digestion helps improve the hydrogen adsorption capacity of the CNTs by opening up the ends, allowing access to the inner cavities of the nanotubes.
Limitations on efficient hydrogen adsorption.
The biggest obstacle to efficient hydrogen storage using CNTs is the purity of the nanotubes. To achieve maximum hydrogen adsorption, there must be minimum graphene, amorphous carbon, and metallic deposits in the nanotube sample. Current methods of CNT synthesis require a purification step. However, even with pure nanotubes, the adsorption capacity is only maximized under high pressures, which are undesirable in commercial fuel tanks.
Supercapacitor.
MIT Research Laboratory of Electronics uses nanotubes to improve supercapacitors. The activated charcoal used in conventional ultracapacitors has many small hollow spaces of various size, which create together a large surface to store electric charge. But as charge is quantized into elementary charges, i.e. electrons, and each such elementary charge needs a minimum space, a significant fraction of the electrode surface is not available for storage because the hollow spaces are not compatible with the charge's requirements. With a nanotube electrode the spaces may be tailored to size—few too large or too small—and consequently the capacity should be increased considerably.
Radar absorption.
Radars work in the microwave frequency range, which can be absorbed by MWNTs. Applying the MWNTs to the aircraft would cause the radar to be absorbed and therefore seem to have a smaller signature. One such application could be to paint the nanotubes onto the plane. Recently there has been some work done at the University of Michigan regarding carbon nanotubes usefulness as stealth technology on aircraft. It has been found that in addition to the radar absorbing properties, the nanotubes neither reflect nor scatter visible light, making it essentially invisible at night, much like painting current stealth aircraft black except much more effective. Current limitations in manufacturing, however, mean that current production of nanotube-coated aircraft is not possible. One theory to overcome these current limitations is to cover small particles with the nanotubes and suspend the nanotube-covered particles in a medium such as paint, which can then be applied to a surface, like a stealth aircraft.
Textile.
The previous studies on the use of CNTs for textile functionalization were focused on fiber spinning for improving physical and mechanical properties. Recently a great deal of attention has been focused on coating CNTs on textile fabrics. Various methods have been employed for modifying fabrics using CNTs. Shim et al. produced intelligent e-textiles for Human Biomonitoring using a polyelectrolyte-based coating with CNTs. Additionally, Panhuis et al. dyed textile material by immersion in either a poly (2-methoxy aniline-5-sulfonic acid) PMAS polymer solution or PMAS-SWNT dispersion with enhanced conductivity and capacitance with a durable behavior. In another study, Hu and coworkers coated single-walled carbon nanotubes with a simple “dipping and drying” process for wearable electronics and energy storage applications. In the recent study, Li and coworkers using elastomeric separator and almost achieved a fully stretchable supercapacitor based on buckled single-walled carbon nanotube macrofilms. The electrospun polyurethane was used and provided sound mechanical stretchability and the whole cell achieve excellent charge-discharge cycling stability. CNTs have an aligned nanotube structure and a negative surface charge. Therefore, they have similar structures to direct dyes, so the exhaustion method is applied for coating and absorbing CNTs on the fiber surface for preparing multifunctional fabric including antibacterial, electric conductive, flame retardant and electromagnetic absorbance properties.
Optical power detectors.
A spray-on mixture of carbon nanotubes and ceramic demonstrates unprecedented ability to resist damage while absorbing laser light. Such coatings that absorb as the energy of high-powered lasers without breaking down are essential for optical power detectors that measure the output of such lasers. These are used, for example, in military equipment for defusing unexploded mines. The composite consists of multiwall carbon nanotubes and a ceramic made of silicon, carbon and nitrogen. Including boron boosts the breakdown temperature. The nanotubes and graphene-like carbon transmit heat well, while the oxidation-resistant ceramic boosts damage resistance.
Creating the coating involves dispersing he nanotubes in toluene, to which a clear liquid polymer containing boron was added. The mixture was heated to . The result is crushed into a fine powder, dispersed again in toluene and sprayed in a thin coat on a copper surface.
The coating absorbed 97.5 percent of the light from a far-infrared laser and tolerated 15 kilowatts per square centimeter for 10 seconds. Damage tolerance is about 50 percent higher than for similar coatings, e.g., nanotubes alone and carbon paint.
Acoustics.
Carbon nanotubes have also been applied in the acoustics(such as loudspeaker and earphone). In 2008 it was shown that a sheet of nanotubes can operate as a loudspeaker if an alternating current is applied. The sound is not produced through vibration but thermoacoustically.
In 2013, a carbon nanotube (CNT) thin yarn thermoacoustic earphone together with CNT thin yarn thermoacoustic chip was demonstrated by a research group of Tsinghua-Foxconn Nanotechnology Research Center in Tsinghua University, using a Si-based semi-conducting technology compatible fabrication process.
Environmental remediation.
A CNT nano-structured sponge (nanosponge) containing sulfur and iron is more effective at soaking up water contaminants such as oil, fertilizers, pesticides and pharmaceuticals. Their magnetic properties make them easier to retrieve once the clean-up job is done. The sulfur and iron increases sponge size to around . It also increases porosity due to beneficial defects, creating buoyancy and reusability. Iron, in the form of ferrocene makes the structure easier to control and enables recovery using magnets. Such nanosponges increase the absorption of the toxic organic solvent dichlorobenzene from water by 3.5 times. The sponges can absorb vegetable oil up to 150 times their initial weight and can absorb engine oil as well.
Earlier, a magnetic boron-doped MWNT nanosponge that could absorb oil from water. The sponge was grown as a forest on a substrate via chemical vapor disposition. Boron puts kinks and elbows into the tubes as they grow and promotes the formation of covalent bonds. The nanosponges retain their elastic property after 10,000 compressions in the lab. The sponges are both superhydrophobic, forcing them to remain at the water's surface and oleophilic, drawing oil to them.
Other applications.
Carbon nanotubes have been implemented in nanoelectromechanical systems, including mechanical memory elements (NRAM being developed by Nantero Inc.) and nanoscale electric motors (see Nanomotor or Nanotube nanomotor).
In May 2005, Nanomix Inc. placed on the market a hydrogen sensor that integrated carbon nanotubes on a silicon platform. Since then, Nanomix has been patenting many such sensor applications, such as in the field of carbon dioxide, nitrous oxide, glucose, DNA detection, etc.
Eikos Inc of Franklin, Massachusetts and Unidym Inc. of Silicon Valley, California are developing transparent, electrically conductive films of carbon nanotubes to replace indium tin oxide (ITO). Carbon nanotube films are substantially more mechanically robust than ITO films, making them ideal for high-reliability touchscreens and flexible displays. Printable water-based inks of carbon nanotubes are desired to enable the production of these films to replace ITO. Nanotube films show promise for use in displays for computers, cell phones, PDAs, and ATMs.
A nanoradio, a radio receiver consisting of a single nanotube, was demonstrated in 2007.
A flywheel made of carbon nanotubes could be spun at extremely high velocity on a floating magnetic axis in a vacuum, and potentially store energy at a density approaching that of conventional fossil fuels. Since energy can be added to and removed from flywheels very efficiently in the form of electricity, this might offer a way of storing electricity, making the electrical grid more efficient and variable power suppliers (like wind turbines) more useful in meeting energy needs. The practicality of this depends heavily upon the cost of making massive, unbroken nanotube structures, and their failure rate under stress.
Carbon nanotube springs have the potential to indefinitely store elastic potential energy at ten times the density of lithium-ion batteries with flexible charge and discharge rates and extremely high cycling durability.
Ultra-short SWNTs (US-tubes) have been used as nanoscaled capsules for delivering MRI contrast agents in vivo.
Carbon nanotubes provide a certain potential for metal-free catalysis of inorganic and organic reactions. For instance, oxygen groups attached to the surface of carbon nanotubes have the potential to catalyze oxidative dehydrogenations or selective oxidations. Nitrogen-doped carbon nanotubes may replace platinum catalysts used to reduce oxygen in fuel cells. A forest of vertically aligned nanotubes can reduce oxygen in alkaline solution more effectively than platinum, which has been used in such applications since the 1960s. Here, the nanotubes have the added benefit of not being subject to carbon monoxide poisoning.
Wake Forest University engineers are using multiwalled carbon nanotubes to enhance the brightness of field-induced polymer electroluminescent technology, potentially offering a step forward in the search for safe, pleasing, high-efficiency lighting. In this technology, moldable polymer matrix emits light when exposed to an electrical current. It could eventually yield high-efficiency lights without the mercury vapor of compact fluorescent lamps or the bluish tint of some fluorescents and LEDs, which has been linked with circadian rhythm disruption.
Candida albicans has been used in combination with carbon nanotubes (CNT) to produce stable electrically conductive bio-nano-composite tissue materials that have been used as temperature sensing elements.
Discovery.
A 2006 editorial written by Marc Monthioux and Vladimir Kuznetsov in the journal "Carbon" described the interesting and often-misstated origin of the carbon nanotube. A large percentage of academic and popular literature attributes the discovery of hollow, nanometer-size tubes composed of graphitic carbon to Sumio Iijima of NEC in 1991.
In 1952 L. V. Radushkevich and V. M. Lukyanovich published clear images of 50 nanometer diameter tubes made of carbon in the Soviet "Journal of Physical Chemistry". This discovery was largely unnoticed, as the article was published in the Russian language, and Western scientists' access to Soviet press was limited during the Cold War. It is likely that carbon nanotubes were produced before this date, but the invention of the transmission electron microscope (TEM) allowed direct visualization of these structures.
Carbon nanotubes have been produced and observed under a variety of conditions prior to 1991. A paper by Oberlin, Endo, and Koyama published in 1976 clearly showed hollow carbon fibers with nanometer-scale diameters using a vapor-growth technique. Additionally, the authors show a TEM image of a nanotube consisting of a single wall of graphene. Later, Endo has referred to this image as a single-walled nanotube.
In 1979, John Abrahamson presented evidence of carbon nanotubes at the 14th Biennial Conference of Carbon at Pennsylvania State University. The conference paper described carbon nanotubes as carbon fibers that were produced on carbon anodes during arc discharge. A characterization of these fibers was given as well as hypotheses for their growth in a nitrogen atmosphere at low pressures.
In 1981, a group of Soviet scientists published the results of chemical and structural characterization of carbon nanoparticles produced by a thermocatalytical disproportionation of carbon monoxide. Using TEM images and XRD patterns, the authors suggested that their “carbon multi-layer tubular crystals” were formed by rolling graphene layers into cylinders. They speculated that by rolling graphene layers into a cylinder, many different arrangements of graphene hexagonal nets are possible. They suggested two possibilities of such arrangements: circular arrangement (armchair nanotube) and a spiral, helical arrangement (chiral tube).
In 1987, Howard G. Tennett of Hyperion Catalysis was issued a U.S. patent for the production of "cylindrical discrete carbon fibrils" with a "constant diameter between about 3.5 and about 70 nanometers..., length 102 times the diameter, and an outer region of multiple essentially continuous layers of ordered carbon atoms and a distinct inner core..."
Iijima's discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods in 1991 and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, then they would exhibit remarkable conducting properties helped create the initial buzz that is now associated with carbon nanotubes. Nanotube research accelerated greatly following the independent discoveries by Bethune at IBM and Iijima at NEC of "single-walled" carbon nanotubes and methods to specifically produce them by adding transition-metal catalysts to the carbon in an arc discharge.
The arc discharge technique was well-known to produce the famed Buckminster fullerene on a preparative scale, and these results appeared to extend the run of accidental discoveries relating to fullerenes. The original observation of fullerenes in mass spectrometry was not anticipated, and the first mass-production technique by Krätschmer and Huffman was used for several years before realizing that it produced fullerenes.
The discovery of nanotubes remains a contentious issue. Many believe that Iijima's report in 1991 is of particular importance because it brought carbon nanotubes into the awareness of the scientific community as a whole.
References.
"This article incorporates public domain text from National Institute of Environmental Health Sciences (NIEHS) as quoted."

</doc>
<doc id="5321" url="http://en.wikipedia.org/wiki?curid=5321" title="Czech Republic">
Czech Republic

The Czech Republic ( ; , , short form "Česko" ) is a landlocked country in Central Europe. The country is bordered by Germany to the west, Austria to the south, Slovakia to the south-east and Poland to the north-east. Prague is the capital and largest city, with 1.3 million residents. The Czech Republic includes the historical territories of Bohemia and Moravia, and Czech Silesia.
The Czech state, formerly known as Bohemia ("Čechy"), was formed in the late 9th century as the Duchy of Bohemia, at that time under the dominance of the powerful Great Moravian Empire. After the fall of the Empire in 907, the centre of power was transferred from Moravia to Bohemia under the Přemyslids. In 1004, the duchy was formally recognized as a part of the Holy Roman Empire, rising to the status of Kingdom of Bohemia in 1212. During the rule of the Přemyslids and their successors, the Luxembourgs, Bohemia expanded in size until reaching its greatest territorial extent in the 14th century. During the reform-driven Hussite wars in the 15th century, the kingdom faced economic embargoes and five crusades from the Holy Roman Empire, defeating all of them.
Following the Battle of Mohács in 1526, the Kingdom of Bohemia was gradually integrated into the Habsburg Monarchy as one of its three principal parts, alongside the Archduchy of Austria and the Kingdom of Hungary. The Bohemian Revolt (1618–20) against the catholic Habsburgs led to the Thirty Years' War, after which the monarchy consolidated its rule, re-imposed Catholicism, and adopted a policy of gradual Germanization. With the dissolution of the Holy Roman Empire in 1806, the Bohemian kingdom became part of the Austrian Empire. In the 19th century the Czech lands became the industrial powerhouse of the monarchy and the core of the Republic of Czechoslovakia, which was formed in 1918 following the collapse of the Austro-Hungarian Empire after World War I. After 1933, Czechoslovakia remained the only democracy in central Europe.
Following the Munich Agreement and the Polish annexation of Zaolzie, Czechoslovakia fell under German occupation during World War II. By 1945, a major portion of the country was liberated by the Red Army, and the subsequent gratitude towards the Soviets, combined with disillusionment with the West for failing to intervene, led the Communist Party of Czechoslovakia to victory in the 1946 elections. Following the 1948 coup d'état, Czechoslovakia became a single-party communist state under Soviet influence. In 1968, increasing dissatisfaction with the regime culminated in a reform movement known as the Prague Spring, which ended with an invasion by the armies of the Warsaw Pact countries (with the exception of Romania). Czechoslovakia remained occupied until the 1989 Velvet Revolution, when the communist regime collapsed and a multiparty parliamentary republic was formed. On 1 January 1993, Czechoslovakia peacefully dissolved, with its constituent states becoming the independent states of the Czech Republic and Slovakia.
The Czech Republic is a developed country with advanced, high income economy and high living standards. The UN ranks the country 14th in the inequality-adjusted human development.
The Czech Republic also ranks as the eleventh-most peaceful country, while achieving strong performance in democratic governance and infant mortality. It is a pluralist parliamentary representative democracy with membership in the European Union, NATO, the OECD, the OSCE, and the Council of Europe.
Etymology.
The traditional English name "Bohemia" derives from Latin "Boiohaemum", which means "home of the Boii". The current name comes from the endonym "Čech", borrowed through Polish and spelt accordingly.
The name comes from the Slavic tribe (Czechs, ) and, according to legend, their leader Čech, who brought them to Bohemia, to settle on Říp Mountain. The etymology of the word "Čech" can be traced back to the Proto-Slavic root "*čel-", meaning "member of the people; kinsman", thus making it cognate to the Czech word "člověk" (a person).
The country has been traditionally divided into three lands, namely Bohemia ("Čechy") in the west, Moravia (Morava) in the southeast, and Czech Silesia ("Slezsko"; the smaller, south-eastern part of historical Silesia, most of which is located within modern Poland) in the northeast. Known officially as the "Lands of the Bohemian Crown" since the 14th century, a number of other names for the country have been used, including the Lands of the Bohemian Crown, Czech/Bohemian lands, Bohemian Crown, and the Lands of the Crown of Saint Wenceslas. When the country regained its independence after the dissolution of the Austro-Hungarian empire in 1918, the new name of "Czechoslovakia" was coined to reflect the union of the Czech and Slovak nations within the one country.
Following the dissolution of Czechoslovakia at the end of 1992, the Czech part of the former nation found itself without a common single-word name in English. In 1993, the Czech Ministry of Foreign Affairs suggested the name Czechia ("Česko" in Czech) as an official alternative in all situations other than formal official documents and the full names of government institutions; however, this has not become widespread in English.
History.
Prehistory.
Archaeologists have found evidence of prehistoric human settlements in the area, dating back to the Paleolithic era. The figurine Venus of Dolní Věstonice found here is the oldest known ceramic article in the world.
In the classical era, from the 3rd century BC Celtic migrations, the Boii and later in the 1st century, Germanic tribes of Marcomanni and Quadi settled there. Their king Maroboduus is the first documented ruler of Bohemia. During the Migration Period around the 5th century, many Germanic tribes moved westwards and southwards out of Central Europe.
Slavic people from the Black Sea-Carpathian region settled in the area (a movement that was also stimulated by the onslaught of peoples from Siberia and Eastern Europe: Huns, Avars, Bulgars and Magyars). In the sixth century they moved westwards into Bohemia, Moravia and some of present day Austria and Germany. During the 7th century, the Frankish merchant Samo, supporting the Slavs fighting their Avar rulers, became the ruler of the first known Slav state in Central Europe. The Moravian principality arose in the 8th century and reached its zenith in the 9th, when it held off the influence of the Franks and won the protection of the Pope.
Bohemia.
The Duchy of Bohemia emerged in the late 9th century, when it was unified by the Přemyslid dynasty. In 10th century Boleslaus I, Duke of Bohemia conquered Moravia, Silesia and expanded farther to the east. The Kingdom of Bohemia was, as the only kingdom in the Holy Roman Empire, a significant regional power during the Middle Ages. It was part of the Empire from 1002 till 1806, with the exception of the years 1440–1526. In 1212, King Přemysl Ottokar I (bearing the title "king" since 1198) extracted the Golden Bull of Sicily (a formal edict) from the emperor, confirming Ottokar and his descendants' royal status; the Duchy of Bohemia was raised to a Kingdom. The bull declared that the King of Bohemia would be exempt from all future obligations to the Holy Roman Empire except for participation in imperial councils. German immigrants settled in the Bohemian periphery in the 13th century. Germans populated towns and mining districts and, in some cases, formed German colonies in the interior of Bohemia. In 1235, the Mongols launched an invasion of Europe. After the Battle of Legnica, the Mongols carried their raids into Moravia. The Mongols subsequently invaded and defeated Hungary.
King Přemysl Ottokar II earned the nickname "Iron and Golden King" because of his military power and wealth. He acquired Austria, Styria, Carinthia and Carniola, thus spreading the Bohemian territory to the Adriatic Sea. He met his death at the Battle on the Marchfeld in 1278 in a war with his rival, King Rudolph I of Germany. Ottokar's son Wenceslaus II acquired the Polish crown in 1300 for himself and the Hungarian crown for his son. He built a great empire stretching from the Danube river to the Baltic Sea. In 1306, the last king of Přemyslid line was murdered in mysterious circumstances in Olomouc while he was resting. After a series of dynastic wars, the House of Luxembourg gained the Bohemian throne.
The 14th century, in particular, the reign of the Czech king Charles IV (1316–1378), who also became the King of Italy, King of the Romans and Holy Roman Emperor, is considered the Golden Age of Czech history. Of particular significance was the founding of Charles University in Prague in 1348, Charles Bridge, Charles Square. Much of Prague Castle and the cathedral of Saint Vitus in Gothic style were completed during his reign. He unified Brandenburg (until 1415), Lusatia (until 1635), and Silesia (until 1742) under the Czech crown. The Black Death, which had raged in Europe from 1347 to 1352, decimated the Kingdom of Bohemia in 1380, killing about 10% of the population.
In the 15th century, the religious and social reformer Jan Hus formed a movement later named after him. Although Hus was named a heretic and burnt in Constance in 1415, his followers seceded from the Catholic Church and in the Hussite Wars (1419–1434) defeated five crusades organized against them by the Holy Roman Emperor Sigismund. Petr Chelčický continued with the Czech Hussite Reformation movement. During the next two centuries, 90% of the inhabitants became adherents of the Hussite Christian movement.
After 1526 Bohemia came increasingly under Habsburg control as the Habsburgs became first the elected and then the hereditary rulers of Bohemia. The Defenestration of Prague and subsequent revolt against the Habsburgs in 1618 marked the start of the Thirty Years' War, which quickly spread throughout Central Europe. In 1620, the rebellion in Bohemia was crushed at the Battle of White Mountain, and the ties between Bohemia and the Habsburgs' hereditary lands in Austria were strengthened. The war had a devastating effect on the local population; the people were given the choice either to convert to Catholicism or leave the country.
The following period, from 1620 to the late 18th century, has often been called colloquially the "Dark Age". The population of the Czech lands declined by a third through war, disease, famine and the expulsion of Protestant Czechs. The Habsburgs prohibited all religions other than Catholicism, which was expressed by baroque architecture. The flowering of baroque culture shows the ambiquity of this historical period.
Ottoman Turks and Tatars invaded Moravia in 1663. In 1679–1680 the Czech lands faced a devastating plague and an uprising of serfs.
The reigns of Maria Theresa of Austria and her son Joseph II, Holy Roman Emperor and co-regent from 1765, were characterized by enlightened absolutism. In 1742, most of Silesia (except the southernmost area), at that time the possession of the Bohemian crown, was seized by King Frederick II of Prussia in the Silesian Wars. In 1757 the Prussians invaded Bohemia and after the Battle of Prague (1757) occupied the city. More than one quarter of Prague was destroyed and St. Vitus Cathedral also suffered heavy damage. However, soon after, at the Battle of Kolín Frederick was defeated and had to leave Prague and retreat from Bohemia. In 1770 and 1771 Great Famine killed about one tenth of the Czech population, or 250,000 inhabitants, and radicalized the countryside leading to peasant uprisings.
After the fall of the Holy Roman Empire, Bohemia became part of the Austrian Empire and later of Austria–Hungary. Serfdom was not completely abolished until 1848. After the Revolutions of 1848, Emperor Franz Josef I of Austria instituted an absolute monarchy in an effort to balance competing ethnic interests in the empire.
Czechoslovakia.
An estimated 1.4 million Czech soldiers fought in World War I, of whom some 150,000 died. More than 90,000 Czech volunteers formed the Czechoslovak Legions in France, Italy and Russia, where they fought against the Central Powers and later against Bolshevik troops. In 1918, during the collapse of the Habsburg Empire at the end of World War I, the independent republic of Czechoslovakia, which joined the winning Allied powers, was created. This new country incorporated the Bohemian Crown (Bohemia, Moravia and Silesia) and parts of the Kingdom of Hungary (Slovakia and the Carpathian Ruthenia) with significant German, Hungarian, Polish and Ruthenian speaking minorities.
In 1929 compared to 1913, the gross domestic product increased by 52% and industrial production by 41%. In 1938 Czechoslovakia held a 10th place in the world industrial production.
Although Czechoslovakia was a unitary state, it provided what were at the time rather extensive rights to its minorities and remained the only democracy in this part of Europe in the interwar period. The effects of the Great Depression including high unemployment and massive propaganda from Nazi Germany, however, resulted in discontent and strong support among ethnic Germans for a break from Czechoslovakia.
Adolf Hitler took advantage of this opportunity and, using Konrad Henlein's separatist Sudeten German Party, gained the largely German speaking Sudetenland (and its substantial Maginot Line-like border fortifications) through the 1938 Munich Agreement (signed by Nazi Germany, France, Britain and Italy). Czechoslovakia was not invited to the conference and felt betrayed by the United Kingdom and France, so Czechs and Slovaks call the Munich Agreement the Munich Betrayal because the military alliance Czechoslovakia had with France and Britain proved useless.
Despite the mobilization of 1.2 million-strong Czechoslovak army and the Franco-Czech military alliance. Poland annexed the Zaolzie area around Český Těšín. Hungary gained parts of Slovakia and the Subcarpathian Rus as a result of the First Vienna Award in November 1938. The remainders of Slovakia and the Subcarpathian Rus gained greater autonomy, with the state renamed to "Czecho-Slovakia". After Nazi Germany threatened to annex part of Slovakia, allowing the remaining regions to be partitioned by Hungary and Poland, Slovakia chose to maintain its national and territorial integrity, seceding from Czecho-Slovakia in March 1939, and allying itself, as demanded by Germany, with Hitler's coalition.
The remaining Czech territory was occupied by Germany, which transformed it into the so-called Protectorate of Bohemia and Moravia. The protectorate was proclaimed part of the Third Reich, and the president and prime minister were subordinate to the Nazi Germany's "Reichsprotektor" ("imperial protector"). Subcarpathian Rus declared independence as the Republic of Carpatho-Ukraine on 15 March 1939 but was invaded by Hungary the same day and formally annexed the next day. Approximately 345,000 Czechoslovak citizens, including 277,000 Jews, were killed or executed while hundreds of thousands of others were sent to prisons and concentration camps or used as forced labour. Up to two-thirds of the citizens were in groups targeted by the Nazis for deportation or death. A Nazi concentration camp existed at Terezín, north of Prague.
There was a strong Czech resistance to Nazi occupation, both at home and abroad, most notably with the assassination of Nazi German leader Reinhard Heydrich by Czechoslovakian soldiers Jozef Gabčík and Jan Kubiš in a Prague suburb on 27 May 1942. The Czechoslovak government-in-exile and its army fighting against the Germans were acknowledged by the Allies; Czech/Czechoslovak troops fought from the very beginning of the war in Poland, France, the UK, North Africa, the Middle East and the Soviet Union. The German occupation ended on 9 May 1945, with the arrival of the Soviet and American armies and the Prague uprising. An estimated 140,000 Soviet soldiers died in liberating Czechoslovakia from German rule.
In 1945–1946, almost the entire German minority in Czechoslovakia, about 3 million people, were expelled to Germany and Austria. During this time, thousands of Germans were held in prisons and detention camps or used as forced labour. In the summer of 1945, there were several massacres. The only Germans not expelled were some 250,000 who had been active in the resistance against the Nazi Germans or were considered economically important, though many of these emigrated later. Following a Soviet-organised referendum, the Subcarpathian Rus never returned under Czechoslovak rule but became part of the Ukrainian Soviet Socialist Republic, as the Zakarpattia Oblast in 1946.
Czechoslovakia uneasily tried to play the role of a "bridge" between the West and East. However, the Communist Party of Czechoslovakia rapidly increased in popularity, with a general disillusionment with the West, because of the pre-war Munich Agreement, and a favourable popular attitude towards the Soviet Union, because of the Soviets' role in liberating Czechoslovakia from German rule. In the 1946 elections, the Communists gained 38% of the votes and became the largest party in the Czechoslovak parliament. They formed a coalition government with other parties of the National Front and moved quickly to consolidate power. The decisive step took place in February 1948, during a series of events characterized by Communists as a "revolution" and by anti-Communists as a "takeover", the Communist People's Militias secured control of key locations in Prague, and a new all-Communist government was formed.
For the next 41 years, Czechoslovakia was a Communist state within the Eastern Bloc. This period is characterized by lagging behind the West in almost every aspect of social and economic development. The country's GDP per capita fell from the level of neighboring Austria below that of Greece or Portugal in the 1980s. The Communist government completely nationalized the means of production and established a command economy. The economy grew rapidly during the 1950s but slowed down in the 1960s and 1970s and stagnated in the 1980s. The political climate was highly repressive during the 1950s, including numerous show trials and hundreds of thousands of political prisoners, but became more open and tolerant in the late 1960s, culminating in Alexander Dubček's leadership in the 1968 Prague Spring, which tried to create "socialism with a human face" and perhaps even introduce political pluralism. This was forcibly ended by invasion by all Warsaw Pact member countries with the exception of Romania and Albania on 21 August 1968.
The invasion was followed by a harsh program of "Normalization" in the late 1960s and the 1970s. Until 1989, the political establishment relied on censorship of the opposition. Dissidents published Charter 77 in 1977, and the first of a new wave of protests were seen in 1988. Between 1948 and 1989 more than 250,000 Czechs and Slovaks were sent to prison, and over 400,000 emigrated.
Velvet revolution and independence.
In November 1989, Czechoslovakia returned to a liberal democracy through the peaceful "Velvet Revolution". However, Slovak national aspirations strengthened and on 1 January 1993, the country peacefully split into the independent Czech Republic and Slovakia. Both countries went through economic reforms and privatisations, with the intention of creating a capitalist economy. This process was largely successful; in 2006 the Czech Republic was recognised by the World Bank as a "developed country", and in 2009 the Human Development Index ranked it as a nation of "Very High Human Development".
From 1991, the Czech Republic, originally as part of Czechoslovakia and since 1993 in its own right, has been a member of the Visegrád Group and from 1995, the OECD. The Czech Republic joined NATO on 12 March 1999 and the European Union on 1 May 2004. On 21 December 2007 the Czech Republic joined the Schengen Area. It held the Presidency of the European Union for the first half of 2009.
Politics.
The Czech Republic is a pluralist multi-party parliamentary representative democracy, with the Prime Minister as head of government. The Parliament ("Parlament České republiky") is bicameral, with the Chamber of Deputies () (200 members) and the Senate () (81 members).
The President of the Czech Republic was being selected by a joint session of the parliament for a five-year term, with no more than two consecutive terms. This system was practised between years 1993–2012. Since 2013 the presidential election is direct.
The president is a formal head of state with limited specific powers, most importantly to return bills to the parliament, nominate constitutional court judges for the Senate's approval and dissolve the parliament under certain special and unusual circumstances. He also appoints the prime minister, as well the other members of the cabinet on a proposal by the prime minister. From 2013 on, the president is elected by the public, not the parliament. Miloš Zeman was the first directly elected Czech President.
The Prime Minister is the head of government and wields considerable powers, including the right to set the agenda for most foreign and domestic policy, mobilize the parliamentary majority and choose government ministers.
The members of the Chamber of Deputies are elected for a four-year term by proportional representation, with a 5% election threshold. There are 14 voting districts, identical to the country's administrative regions. The Chamber of Deputies, the successor to the Czech National Council, has the powers and responsibilities of the now defunct federal parliament of the former Czechoslovakia.
The members of the Senate are elected in single-seat constituencies by two-round runoff voting for a six-year term, with one-third elected every even year in the autumn. The first election was in 1996, for differing terms. This arrangement is modeled on the U.S. Senate, but each constituency is roughly the same size and the voting system used is a two-round runoff. The Senate is unpopular among the public and suffers from low election turnout, overall roughly 30% in the first round and 20% in the second.
Foreign relations.
Membership in the European Union is central to the Czech Republic's foreign policy. The Czech Republic held the Presidency of the Council of the European Union for the first half of 2009.
Czech officials have supported dissenters in Burma, Belarus, Moldova and Cuba.
Military.
The Czech armed forces consist of the Army, Air Force and of specialized support units. The president of the Czech Republic is Commander-in-Chief of the armed forces. In 2004 the army transformed itself into a fully professional organization and compulsory military service was abolished. The country has been a member of NATO since 12 March 1999. Defense spending is around 1.8% of the GDP (2006). Currently, as a member of NATO, the Czech military are participating in ISAF and KFOR operations and have soldiers in Afghanistan and Kosovo. Main equipment includes: multi-role fighters JAS-39 Gripen, combat aircraft Aero L-159 Alca, attack helicopters Mi-24, armored vehicles Pandur II, OT-64, OT-90, BVP-2 and Czech modernized tanks T-72 (T-72M4CZ).
Administrative divisions.
Since 2000, the Czech Republic has been divided into thirteen regions (Czech: "kraje", singular "kraj") and the capital city of Prague. Each region has its own elected regional assembly ("krajské zastupitelstvo") and "hejtman" (a regional governor). In Prague, the assembly and presidential powers are executed by the city council and the mayor.
The older seventy-six districts ("okresy", singular "okres") including three "statutory cities" (without Prague, which had special status) lost most of their importance in 1999 in an administrative reform; they remain as territorial divisions and seats of various branches of state administration.
 Capital city. Office location.
Geography.
The Czech Republic lies mostly between latitudes 48° and 51° N (a small area lies north of 51°), and longitudes 12° and 19° E.
The Czech landscape is exceedingly varied. Bohemia, to the west, consists of a basin drained by the Elbe () and the Vltava (or Moldau) rivers, surrounded by mostly low mountains, such as the Krkonoše range of the Sudetes. The highest point in the country, Sněžka at , is located here. Moravia, the eastern part of the country, is also quite hilly. It is drained mainly by the Morava River, but it also contains the source of the Oder River ().
Water from the landlocked Czech Republic flows to three different seas: the North Sea, Baltic Sea and Black Sea. The Czech Republic also leases the Moldauhafen, a lot in the middle of the Hamburg Docks, which was awarded to Czechoslovakia by Article 363 of the Treaty of Versailles, to allow the landlocked country a place where goods transported down river could be transferred to seagoing ships. The territory reverts to Germany in 2028.
Phytogeographically, the Czech Republic belongs to the Central European province of the Circumboreal Region, within the Boreal Kingdom. According to the World Wide Fund for Nature, the territory of the Czech Republic can be subdivided into four ecoregions: the Central European mixed forests, Pannonian mixed forests, Western European broadleaf forests and Carpathian montane conifer forests.
There are four national parks in the Czech Republic. The oldest is
Krkonoše National Park (Biosphere Reserve), Šumava National Park (Biosphere Reserve), Podyjí National Park, Bohemian Switzerland.
Climate.
The Czech Republic has a temperate continental climate, with warm summers and cold, cloudy and snowy winters. The temperature difference between summer and winter is relatively high, due to the landlocked geographical position.
Within the Czech Republic, temperatures vary greatly, depending on the elevation. In general, at higher altitudes, the temperatures decrease and precipitation increases. The wettest area in the Czech Republic is found around Bílý Potok in Jizera Mountains and the driest region is the Louny District to the northwest of Prague. Another important factor is the distribution of the mountains; therefore, the climate is quite varied.
At the highest peak of Sněžka (), the average temperature is only , whereas in the lowlands of the South Moravian Region, the average temperature is as high as . The country's capital, Prague, has a similar average temperature, although this is influenced by urban factors.
The coldest month is usually January, followed by February and December. During these months, there is usually snow in the mountains and sometimes in the major cities and lowlands. During March, April and May, the temperature usually increases rapidly, especially during April, when the temperature and weather tends to vary widely during the day. Spring is also characterized by high water levels in the rivers, due to melting snow with occasional flooding.
The warmest month of the year is July, followed by August and June. On average, summer temperatures are about – higher than during winter. Summer is also characterized by rain and storms.
Autumn generally begins in September, which is still relatively warm and dry. During October, temperatures usually fall below or and deciduous trees begin to shed their leaves. By the end of November, temperatures usually range around the freezing point.
The coldest temperature ever measured was in Litvínovice near České Budějovice in 1929, at and the hottest measured, was at in Dobřichovice in 2012.
Most rain falls during the summer. Sporadic rainfall is relatively constant throughout the year (in Prague, the average number of days per month experiencing at least 0.1 mm of rain varies from 12 in September and October to 16 in November) but concentrated heavy rainfall (days with more than 10 mm per day) are more frequent in the months of May to August (average around two such days per month).
Economy.
The Czech Republic possesses a developed, high-income economy with a per capita GDP rate that is 81% of the European Union average. One of the most stable and prosperous of the post-Communist states, the Czech Republic saw growth of over 6% annually in the three years before the outbreak of the recent global economic crisis. Growth has been led by exports to the European Union, especially Germany, and foreign investment, while domestic demand is reviving.
Most of the economy has been privatised, including the banks and telecommunications. A 2009 survey in cooperation with the Czech Economic Association found that the majority of Czech economists favor continued liberalization in most sectors of the economy.
The country is part of the Schengen Area from 1 May 2004, having abolished border controls, completely opening its borders with all of its neighbours, Germany, Austria, Poland and Slovakia, on 21 December 2007. The Czech Republic became a member of the World Trade Organisation on 1 January 1995.
Although the country is economically better positioned than other EU Members to adopt the euro, the change is not expected before 2019, due to political reluctance on the matter.
The Programme for International Student Assessment, coordinated by the OECD, currently ranks the Czech education system as the 15th best in the world, higher than the OECD average. The Czech Republic is ranked 30th in the 2012 Index of Economic Freedom.
Leading Czech transportation companies include Škoda Auto (automobiles), Škoda Transportation (tramways, trolleybuses, metro), Tatra (the third oldest car maker in the world), Karosa (buses), Aero Vodochody (airplanes) and Jawa Motors (motorcycles). http://www.worlddiplomacy.org States that "Elections in 2013 brought a new government for the Czech Republic, however the economy continued to contract in 2013 but should be set to return to growth in 2014."
Energy.
Production of Czech electricity exceeds consumption by about 10 TWh per year, which are exported. Nuclear power presently provides about 30 percent of the total power needs, its share is projected to increase to 40 percent. In 2005, 65.4 percent of electricity was produced by steam and combustion power plants (mostly coal); 30 percent by nuclear plants; and 4.6 percent from renewable sources, including hydropower. The largest Czech power resource is Temelín Nuclear Power Station, another nuclear power plant is in Dukovany.
The Czech Republic is reducing its dependence on highly polluting low-grade brown coal as a source of energy. Natural gas is procured from Russian Gazprom, roughly three-fourths of domestic consumption and from Norwegian companies, which make up most of the remaining one-fourth. Russian gas is imported via Ukraine (Druzhba pipeline), Norwegian gas is transported through Germany. Gas consumption (approx. 100 TWh in 2003–2005) is almost double electricity consumption. South Moravia has small oil and gas deposits.
Transportation infrastructure.
Václav Havel Airport in Prague is the main international airport in the country. In 2010, it handled 11.6 million passengers, which makes it the busiest airport in Central and Eastern Europe. In total, Czech Republic has 46 airports with paved runways, six of which provide international air services in Brno, Karlovy Vary, Mošnov (near Ostrava), Pardubice, Prague and Kunovice (near Uherské Hradiště).
České dráhy (the Czech railways) is the main railway operator in the Czech Republic, with about 180 million passengers carried yearly. Its cargo division, ČD Cargo, is the fifth largest railway cargo operator in the European Union. With of tracks, the Czech Republic has one of the densest railway networks in Europe. Of that number, is electrified, are single-line tracks and are double and multiple-line tracks. In 2006 the new Italian tilting trains Pendolino ČD Class 680 entered service. They have reached a speed of 237 km/h setting a new Czech railway speed record.
Russia, via pipelines through Ukraine and to a lesser extent, Norway, via pipelines through Germany, supply the Czech Republic with liquid and natural gas.
The road network in the Czech Republic is long. and 738,4 km of motorways and 439,1 km of expressways. The speed limit is 50 km/h within towns, 90 km/h outside of towns and 130 km/h on expressways.
Communications.
The Czech Republic ranks in the top 10 countries worldwide with the fastest average internet speed.
The Czech Republic has the most Wi-Fi subscribers in the European Union. By the beginning of 2008, there were over 800 mostly local WISPs, with about 350,000 subscribers in 2007. Plans based on either GPRS, EDGE, UMTS or CDMA2000 are being offered by all three mobile phone operators (T-Mobile, Vodafone, Telefónica O2) and internet provider U:fon. Government-owned Český Telecom slowed down broadband penetration. At the beginning of 2004, local-loop unbundling began and alternative operators started to offer ADSL and also SDSL. This and later privatisation of Český Telecom helped drive down prices.
On 1 July 2006, Český Telecom was acquired by globalized company (Spain owned) Telefónica group and adopted new name Telefónica O2 Czech Republic. As of June 2014, VDSL and ADSL2+ are offered in many variants, with download speeds of up to 40 Mbit/s and upload speeds of up to 2Mbit/s. Cable internet is gaining popularity with its higher download speeds ranging from 2 Mbit/s to 240 Mbit/s.
Science.
The Czech lands have a long and rich scientific tradition. The research based on cooperation between universities, Academy of Sciences and specialised research centers brings new inventions and impulses in this area. Important inventions include the modern contact lens, the separation of modern blood types, and the production of the Semtex plastic explosive. Prominent scientists who lived and worked in historically Czech lands include:
A number of other scientists are also connected in some way with the Czech Lands, including astronomers Johannes Kepler and Tycho Brahe, the founder of the psychoanalytic school of psychiatry Sigmund Freud, physicists Christian Doppler, Ernst Mach, Nikola Tesla, Albert Einstein, engineer Viktor Kaplan and logician Kurt Gödel.
Tourism.
The Czech economy gets a substantial income from tourism. In 2011, Prague was the sixth most visited city in Europe. In 2001, the total earnings from tourism reached 118 billion CZK, making up 5.5% of GNP and 9% of overall export earnings. The industry employs more than 110,000 people – over 1% of the population.
The country's reputation has suffered with guidebooks and tourists reporting overcharging by taxi drivers and pickpocketing problems mainly in Prague. Since 2005, Prague's mayor, Pavel Bém, has worked to improve this reputation by cracking down on petty crime and, aside from these problems, Prague is a safe city. Also, the Czech Republic as a whole generally has a low crime rate. For tourists, the Czech Republic is considered a safe destination to visit. The low crime rate makes most cities and towns safe to walk around even after dark.
There are several centres of tourist activity. The spa towns, such as Karlovy Vary, Mariánské Lázně and Františkovy Lázně, are particularly popular relaxing holiday destinations.
Architectural heritage is another object of visitor´s interest - it includes many castles and chateaux from different historical epoques, namely Karlštejn Castle, Český Krumlov and the Lednice–Valtice area.
There are 12 cathedrals and 15 churches elevated to the rank of basilica by the Pope, calm monasteries, many modern and ancient churches - for example Pilgrimage Church of Saint John of Nepomuk is one of those inscribed on the World Heritage List. Away from the towns, areas such as Český ráj, Šumava and the Krkonoše Mountains attract visitors seeking outdoor pursuits.
The country is also known for its various museums, very popular are puppetry and marionettes exhibitions with a number of puppet festivals throughout the country.
Aquapalace Praha in Čestlice near Prague, is the biggest water park in central Europe.
The Czech Republic has a number of beer festivals, including: Czech Beer Festival (the biggest Czech beer festival, it is 17 days long and held every year in May in Prague), Pilsner Fest (every year in August in Plzeň), The "Olomoucký pivní festival" (in Olomouc) or festival "Slavnosti piva v Českých Budějovicích" (in České Budějovice).
Demographics.
According to preliminary results of the 2011 census, the majority of the inhabitants of the Czech Republic are Czechs (63.7%), followed by Moravians (4.9%), Slovaks (1.4%), Poles (0.4%), Germans (0.2%) and Silesians (0.1%). As the 'nationality' was an optional item, a substantial number of people left this field blank (26.0%). According to some estimates, there are about 250,000 Romani people in the Czech Republic.
There were 436,116 foreigners residing in the country in October 2009, according to the Czech Interior Ministry, with the largest groups being Ukrainian (132,481), Slovak (75,210), Vietnamese (61,102), Russian (29,976), Polish (19,790), German (14,156), Moldovan (10,315), Bulgarian (6,346), Mongolian (5,924), American (5,803), Chinese (5,314), British (4,461), Belarusian (4,441), Serbian (4,098), Romanian (4,021), Kazakh (3,896), Austrian (3,114), Italian (2,580), Dutch (2,553), French (2,356), Croatian (2,351), Bosnian (2,240), Armenian (2,021), Uzbek (1,969), Macedonian (1,787) and Japanese (1,581).
The Jewish population of Bohemia and Moravia, 118,000 according to the 1930 census, was virtually annihilated by the Nazi Germans during the Holocaust. There were approximately 4,000 Jews in the Czech Republic in 2005. The former Czech prime minister, Jan Fischer, is of Jewish origin and faith.
The total fertility rate (TFR) in 2013 was estimated at 1.29 children born/woman, which is below the replacement rate of 2.1, and one of the lowest in the world. In 2013, 45% of births were to unmarried women. The life expectancy in 2013 was estimated at 77.56 years (74.29 years male, 81.01 years female).
Immigration increased the population by almost 1% in 2007. About 77,000 people immigrate to the Czech Republic annually. Vietnamese immigrants began settling in the Czech Republic during the Communist period, when they were invited as guest workers by the Czechoslovak government. In 2009, there were about 70,000 Vietnamese in the Czech Republic. Most decide to stay in the country permanently.
At the turn of the 20th century, Chicago was the city with the third largest Czech population, after Prague and Vienna. According to the 2010 US census, there are 1,533,826 Americans of full or partial Czech descent.
Religion.
The Czech Republic has one of the least religious populations in the world. Historically, the Czech people have been characterised as "tolerant and even indifferent towards religion". According to the 2011 census, 34% of the population stated they had no religion, 10.3% was Roman Catholic, 0.8% was Protestant (0.5% Czech Brethren and 0.4% Hussite), and 9% followed other forms of religion both denominational or not (of which 863 people answered they are Pagan) 45% of the population did not answer the question about religion. From 1991 to 2001 and further to 2011 the adherence to Roman Catholicism decreased from 39 to 27 and then to 10; Protestantism similarly declined from 3.7% to 2% and then to 0.8%.
According to a Eurobarometer Poll in 2010, 16% of Czech citizens responded that "they believe there is a God" (the lowest rate among the countries of the European Union), whereas 44% answered that "they believe there is some sort of spirit or life force" and 37% said that "they do not believe there is any sort of spirit, God or life force".
Culture.
Music.
Music in the Czech lands has its roots in more than 1,000-year-old sacred music. The first surviving references Lord, Have Mercy on Us come from the end of the 10th century and in the traditional folk music of Bohemia, Moravia and Silesia and in the long-term high-culture classical music tradition. Since the early eras of artificial music, Czech musicians and composers have often been influenced by genuine folk music (such as polka which originated in Bohemia). Notable Czech composers include Antonín Dvořák, Bedřich Smetana, Gustav Mahler (he was born and grew up in the Czech lands), Adam Michna, Jan Dismas Zelenka, Josef Mysliveček, Leoš Janáček, Josef Suk, Bohuslav Martinů, Erwin Schulhoff and Petr Eben.
Literature.
Czech literature is the literature written by Czechs, mostly in the Czech language, although other languages like Old Church Slavonic, Latin or German have been also used, such as by author Franz Kafka, who—while bilingual in Czech and German—wrote his works in German, during the era of Austrian rule.
Czech literature is divided into several main time periods: the Middle Ages; the Hussite period; the years of re-Catholicization and the baroque; the Enlightenment and Czech reawakening in the 19th century; the avantgarde of the interwar period; the years under Communism and the Prague Spring; and the literature of the post-Communist Czech Republic. Czech literature and culture played a major role on at least two occasions, when Czechs lived under oppression and political activity was suppressed. On both of these occasions, in the early 19th century and then again in the 1960s, the Czechs used their cultural and literary effort to strive for political freedom, establishing a confident, politically aware nation. Jaroslav Seifert was awarded the Nobel Prize in Literature.
A famous antiwar comedy novel "The Good Soldier Švejk" by Jaroslav Hašek is the most translated Czech book in history. It was depicted by Karel Steklý in two color films "The Good Soldier Schweik" in 1956 and 1957.
Theatre.
Theatre of the Czech Republic has rich tradition with roots in the Middle Ages. In the 19th century, the theatre played an important role in the national awakening movement and later, in the 20th century it became a part of the modern European theatre art.
Film.
The Barrandov Studios in Prague are the largest film studios in country and one of the largest in Europe. The Czech Republic has many popular film locations. Filmmakers have come to Prague to shoot scenery no longer found in Berlin, Paris and Vienna. The city of Karlovy Vary was used as a location for the 2006 James Bond film Casino Royale. Czech Lion is the highest award for Czech film achievement. The international Karlovy Vary film festival is one of the oldest in the world.
Video games.
Video games are considered by some experts to be the biggest cultural export of the country. There are some globally successful video game developers based in the Czech Republic such as Bohemia Interactive, Keen Software House, Illusion Softworks, Amanita Design or Madfinger Games. Video games produced in the Czech Republic include Space Engineers, , ARMA 3, , Vietcong, Machinarium, Shadowgun and DayZ.
Art.
The Czech Republic is known worldwide for its individually made, mouth blown and decorated art glass and crystal. One of the best Czech painter and decorative artist was Alphonse Mucha (1860–1939) mainly known for art nouveau posters and his cycle of 20 large canvases named The Slav Epic, which depicts the history of Czechs and other Slavic peoples. The Slav Epic can be since 2012 seen in Veletržní Palace of National Gallery in Prague, which manages the largest collection of art in the Czech Republic.
Other famous Czech artists are:
Cuisine.
Czech cuisine is marked by a strong emphasis on meat dishes. Pork is quite common; beef and chicken are also popular. Goose, duck, rabbit and wild game are served. Fish is rare, with the occasional exception of fresh trout and carp, which is served at Christmas.
Czech beer has a long and important history. The first brewery is known to have existed in 1118 and the Czech Republic has the highest beer consumption per capita in the world. The famous Pilsner style beer originated in the western Bohemian city of Plzeň, and further south the town of České Budějovice, known as Budweis in German, lent its name to its beer, eventually known as Budweiser Budvar. Apart from these and other major brands, the Czech Republic also boasts a growing number of top quality small breweries and mini-breweries seeking to continue the age-old tradition of quality and taste, whose output matches the best in the world: Štiřín, Chýně, Oslavany, Kácov. Tourism is slowly growing around the Southern Moravian region too, which has been producing wine since the Middle Ages; about 94% of vineyards in the Czech Republic are Moravian. Aside from Slivovitz, Czech beer and wine, the Czechs also produce two unique liquors, Fernet Stock and Becherovka. Kofola is a non-alcoholic domestic cola soft drink which competes with Coca Cola and Pepsi in popularity.
Unique Czech dishes include roast pork with bread dumplings and stewed cabbage "Vepřo-knedlo-zelo", roast sirloin beef with steamed dumplings and cream-of-vegetable sauce "Svíčková na smetaně", tomato sauce "Rajská" or dill sauce "Koprovka", roast duck with bread or potato dumplings and braised red cabbage, a variety of beef and pork goulash stews "Guláš", fried cheese "Smažák" or the famous potato pancakes "Bramboráky", besides a large variety of delicate local sausages, wurst, pâtés and smoked meats and other traditional local foods. Czech desserts include a wide variety of whipped cream, chocolate and fruit pastries and tarts, crepes, creme desserts and cheese, poppy seed filled and other types of traditional cakes such as "buchty", "koláče" and štrúdl.
Sports.
Sports play a part in the life of many Czechs, who are generally loyal supporters of their favorite teams or individuals. The three leading sports in the Czech Republic are ice hockey, football and sport shooting, with the first two drawing the largest attention of both the media and supporters. Tennis is also a very popular sport in the Czech Republic. The many other sports with professional leagues and structures include basketball, volleyball, team handball, track and field athletics and floorball. The Czech ice hockey team won the gold medal at the 1998 Winter Olympics and has won six gold medals at the World Championships including three straight from 1999 to 2001. In total the country has won 14 gold medals in summer (plus 49 as Czechoslovakia) and five gold medals (plus two as Czechoslovakia) in winter Olympic history.
Sport is a source of strong waves of patriotism, usually rising several days or weeks before an event. The events considered the most important by Czech fans are: the Ice Hockey World Championships, Olympic Ice hockey tournament, UEFA European Football Championship, UEFA Champions League, FIFA World Cup and qualification matches for such events. In general, any international match of the Czech ice hockey or football national team draws attention, especially when played against a traditional rival.
The Czech Republic also has great influence on tennis with such players as, Ivan Lendl, 8 times Grand Slam singles champion, 2010 Wimbledon Championships – Men's Singles finalist Tomáš Berdych, 2011 Wimbledon Championships – Women's Singles champion, Petra Kvitová, 1998 Wimbledon Women's Singles title Jana Novotná, 2011 Wimbledon Championships – Women's Doubles champion Květa Peschke and 18 time Grand Slam Champion Martina Navratilova.
The favourite Czech individual or group sport is hiking mainly in Czech mountains. Even one meaning of the word "tourist" in the Czech language is a trekker or a hiker. It is ideal sport also for beginners, because thanks to the more than 100 years long tradition, there is a unique system of the trekkings markers, one of the best in Europe. It contains the net around 40 000 km perfectly marked short or long distanced trails crossing the whole country and all Czech mountains - not only in Šumava Mountains, but also in Vysočina, Krušné hory, Jizerské hory, Beskydy, Jeseníky, Orlické hory and Giant Mountains - Krkonoše.

</doc>
<doc id="5322" url="http://en.wikipedia.org/wiki?curid=5322" title="Czechoslovakia">
Czechoslovakia

Czechoslovakia (Czech and , "Česko-Slovensko"; also written "Czecho-Slovakia") was a sovereign state in Central Europe that existed from October 1918, when it declared its independence from the Austro-Hungarian Empire, until its peaceful dissolution into the Czech Republic and Slovakia on 1 January 1993.
From 1939 to 1945, following its forced division and partial incorporation into Nazi Germany, the state did not "de facto" exist but its government-in-exile continued to operate. On 29 June 1945, a treaty was signed between Czechoslovakia and the Soviet Union, ceding Carpatho-Ukraine to the USSR.
From 1948 to 1990 Czechoslovakia had a command or planned economy, removing price controls after a period of preparation.
Basic characteristics.
The country was of generally irregular terrain. The western area was part of the north-central European uplands. The eastern region was composed of the northern reaches of the Carpathian Mountains and lands of the Danube River basin.
The weather was predominantly continental, but varied from the moderate temperature of Western Europe in the west, to the more severe weather of Eastern Europe and the western Soviet Union in the east.
History.
Foundation.
Origins.
The area was long a part of the Austro Hungarian Empire until the Empire collapsed at the end of World War I. The new state was founded by Tomáš Garrigue Masaryk (1850–1937), who served as its first president from 14 November 1918 to 14 December 1935. He was succeeded by his close ally, Edvard Beneš (1884–1948).
The roots of Czech nationalism go back to the 19th century, when philologists and educators, influenced by Romanticism, promoted the Czech language and pride in the Czech people. Nationalism became a mass movement in the last half of the 19th century. Taking advantage of the opportunities for limited participation in political life available under the Austrian rule, Czech leaders such as historian František Palacký (1798–1876) founded many patriotic, self-help organizations which provided a chance for many of their compatriots to participate in communal life prior to independence. Palacký supported Austroslavism and worked for reorganized and federal Austrian Empire, which would protect Middle-European people against Russian and German threats. 
An advocate of democratic reform and Czech autonomy within Austria-Hungary, Masaryk was elected twice to "Reichsrat" (Austrian Parliament), the first time being from 1891 to 1893 in the Young Czech Party and again from 1907 to 1914 in the Czech Realist Party, which he founded in 1889 with Karel Kramář and Josef Kaizl.
During World War I small numbers of Czechs, the Czechoslovak Legions, fought with the Allies in France and Italy, while large numbers deserted to Russia, in exchange for their support for the independence of Czechoslovakia from the Austrian Empire. With the outbreak of World War I, Masaryk began working for Czech independence in union with Slovakia. With Edvard Beneš and Milan Rastislav Štefánik, Masaryk visited several Western countries and won support from influential publicists.
Bohemia and Moravia, under Austrian rule, were Czech-speaking industrial centres, while Slovakia, which was part of Hungary, was an undeveloped agrarian region. Conditions were much better for the development of a mass national movement in the Czech lands than in Slovakia. Nevertheless, the two regions united and created a new nation.
Founding.
The Bohemian Kingdom officially ceased to exist in 1918 by transformation into Czechoslovakia. Czechoslovakia was founded in October 1918, as one of the successor states of Austro-Hungarian Empire at the end of World War I and as part of the Treaty of St. Germain. It consisted of the present day territories of the Bohemia, Moravia, Slovakia and Carpathian Ruthenia. Its territory included some of the most industrialized regions of the former Austria-Hungary.
Ethnicity.
The new country was a multi-ethnic state. The population consisted of Czechs (51%), Slovaks (16%), Germans (22%), Hungarians (5%) and Rusyns (4%). Many of the Germans, Hungarians, Ruthenians and Poles and some Slovaks, felt oppressed because the political elite did not generally allow political autonomy for minority ethnic groups. This policy, combined with increasing Nazi propaganda especially in the industrialized German-speaking Sudetenland, led to unrest among the non-Czech population.
The state proclaimed the official ideology that there are no Czechs and Slovaks, but only one nation of Czechoslovaks (see Czechoslovakism), to the disagreement of Slovaks and other ethnic groups. Once a unified Czechoslovakia was restored after World War II (after the country had been divided during the war), the conflict between the Czechs and the Slovaks surfaced again. The governments of Czechoslovakia and other eastern European nations deported ethnic Germans to the West, reducing the presence of minorities in the nation. Most of the Jews had been killed during the war by the Nazis and their allies.
Interwar.
The period between the two world wars saw the flowering of democracy in Czechoslovakia. Of all the new states established in central Europe after 1918, only Czechoslovakia preserved a democratic government until the war broke out. The persistence of democracy suggests that Czechoslovakia was better prepared to maintain democracy than were other countries in the region. Thus, despite regional disparities, its level of development was much higher than that of neighboring states. The population was generally literate, and contained fewer alienated groups. The influence of these conditions was augmented by the political values of Czechoslovakia's leaders and the policies they adopted. Under Masaryk, Czech and Slovak politicians promoted progressive social and economic conditions that served to defuse discontent.
Foreign minister Beneš became the prime architect of the Czechoslovak-Romanian-Yugoslav alliance (the "Little Entente", 1921–38) directed against Hungarian attempts to reclaim lost areas. Beneš worked closely with France. Far more dangerous was the German element, which after 1933 became allied with the Nazis in Germany. The increasing feeling of inferiority among the Slovaks, who were hostile to the more numerous Czechs, weakened the country in the late 1930s. Many Slovaks supported an extreme nationalist movement and welcomed the puppet Slovak state set up under Hitler's control in 1939.
After 1933, Czechoslovakia remained the only democracy in central and eastern Europe.
Munich Agreement and German occupation.
In 1938, Adolf Hitler demanded control of the Sudetenland. Britain and France ceded control in the Appeasement at the Munich Conference, ignoring the military alliance Czechoslovakia had with France. In 1939, the remainder ("rump") of Czechoslovakia was invaded by Nazi Germany and divided into the Protectorate of Bohemia and Moravia and the puppet Slovak State. Much of Slovakia and all of Subcarpathian Ruthenia were annexed by Hungary. Poland occupied Zaolzie, an area with Polish minority, in October 1938.
The eventual goal of the German state under Nazi leadership was to eradicate Czech nationality through assimilation, deportation, and extermination of the Czech intelligentsia; the intellectual elites and middle class made up a considerable number of the 200,000 people who passed through concentration camps and the 250,000 who died during German occupation. Under Generalplan Ost, it was assumed that around 50% Czechs would be fit for Germanization. The Czech intellectual elites were to be removed not only from Czech territories but from Europe completely. The authors of Generalplan Ost believed it would be best if they emigrated overseas, as even in Siberia they were considered a threat to German rule. Just like Jews, Poles, Serbs, and several other nations, Czechs were considered to be untermenschen by the Nazi state.
The deportation of Jews to concentration camps was organized under the direction of Reinhard Heydrich, and the fortress town of Terezín was made into a ghetto way station for Jewish families. On 4 June 1942 Heydrich died after being wounded by an assassin in Operation Anthropoid. Heydrich's successor, Colonel General Kurt Daluege, ordered mass arrests and executions and the destruction of the villages of Lidice and Ležáky. In 1943 the German war effort was accelerated. Under the authority of Karl Hermann Frank, German minister of state for Bohemia and Moravia, some 350,000 Czech labourers were dispatched to the Reich. Within the protectorate, all non-war-related industry was prohibited. Most of the Czech population obeyed quiescently up until the final months preceding the end of the war, while thousands were involved in the resistance movement.
For the Czechs of the Protectorate Bohemia and Moravia, German occupation was a period of brutal oppression. Czech losses resulting from political persecution and deaths in concentration camps totaled between 36,000 and 55,000. The Jewish population of Bohemia and Moravia (118,000 according to the 1930 census) was virtually annihilated. Many Jews emigrated after 1939; more than 70,000 were killed; 8,000 survived at Terezín. Several thousand Jews managed to live in freedom or in hiding throughout the occupation.
Despite the estimated 136,000 deaths at the hands of the Nazi regime, the population in the Reichsprotektorate saw a net increase during the war years of approximately 250,000 in line with an increased birth rate.
On 9 May 1945, Soviet Red Army troops entered Prague.
Communist Czechoslovakia.
After World War II, pre-war Czechoslovakia was re-established, with the exception of Subcarpathian Ruthenia, which was annexed by the Soviet Union and incorporated into the Ukrainian Soviet Socialist Republic. The Beneš decrees were promulgated concerning ethnic Germans (see Potsdam Agreement) and ethnic Hungarians. Under the decrees, citizenship was abrogated for people of German and Hungarian ethnic origin, who had accepted German or Hungarian citizenship during the occupations. In 1948, this provision was cancelled for the Hungarians, but only partially for the Germans. The government then confiscated the property of the Germans and expelled about 90% of the ethnic German population, over 2 million people. Those who remained were collectively accused of supporting the Nazis after the Munich Agreement, as 97.32% of Sudeten Germans voted for the NSDAP in the December 1938 elections. Almost every decree explicitly stated that the sanctions did not apply to antifascists. Some 250,000 Germans, many married to Czechs, some antifascists, and also those required for the post-war reconstruction of the country, remained in Czechoslovakia. The Beneš Decrees still causes controversy among nationalist groups in the Czech Republic, Germany, Austria and Hungary.
Carpathian Ruthenia was occupied by (and in June 1945 formally ceded to) the Soviet Union. In the 1946 parliamentary election, the Communist Party of Czechoslovakia was the winner in the Czech lands, and the Democratic Party won in Slovakia. In February 1948 the Communists seized power. Although they would maintain the fiction of political pluralism through the existence of the National Front, except for a short period in the late 1960s (the Prague Spring) the country was characterized by the absence of liberal democracy. Since citizens lacked significant electoral methods of registering protest against government policies, periodically there were street protests that became violent. Such was the case in the town of Plzeň, where riots occurred in 1953, reflecting economic discontent. Police and army units put down the rebellion, and hundreds were injured but no one was killed. The Director of the United States’ Central Intelligence Agency, Allen W. Dulles, was subsequently recorded as having told an associate in a private conversation, “The horrible thing in that Czechoslovakian thing was that nobody got killed. I’d have felt much better about that, and the Czechoslovakian people would have stood much higher in the world’s estimation, if there had been a thousand or ten thousand people killed in that. We kill more people on the roads every day for no purpose.” While its economy remained more advanced than those of its neighbors in Eastern Europe, Czechoslovakia grew increasingly economically weak relative to Western Europe.
In 1968, when the reformer Alexander Dubček was appointed to the key post of First Secretary of the Czechoslovak Communist Party, there was a brief period of liberalization known as the Prague Spring. In response, after failing to persuade the Czechoslovak leaders to change course, five other Eastern Bloc members of the Warsaw Pact invaded. Soviet tanks rolled into Czechoslovakia on the night of 20–21 August 1968. The General Secretary of the Soviet Communist Party Leonid Brezhnev viewed this intervention as vital to the preservation of the Soviet, socialist system and vowed to intervene in any state that sought to replace Marxism-Leninism with capitalism. In the week after the invasion there was a spontaneous campaign of civil resistance against the occupation. This resistance involved a wide range of acts of non-cooperation and defiance: this was followed by a period in which the Czechoslovak Communist Party leadership, having been forced in Moscow to make concessions to the Soviet Union, gradually put the brakes on their earlier liberal policies. In April 1969 Dubček was finally dismissed from the First Secretaryship of the Czechoslovak Communist Party. Meanwhile, one plank of the reform programme had been carried out: in 1968-9, Czechoslovakia was turned into a federation of the Czech Socialist Republic and Slovak Socialist Republic. The theory was that under the federation, social and economic inequities between the Czech and Slovak halves of the state would be largely eliminated. A number of ministries, such as education, now became two formally equal bodies in the two formally equal republics. However, the centralised political control by the Czechoslovak Communist Party severely limited the effects of federalisation.
The 1970s saw the rise of the dissident movement in Czechoslovakia, represented among others by Václav Havel. The movement sought greater political participation and expression in the face of official disapproval, manifested in limitations on work activities, which went as far as a ban on professional employment, the refusal of higher education for the dissidents' children, police harassment and prison.
After 1989.
In 1989, the Velvet Revolution restored democracy. This occurred at around the same time as the fall of communism in Romania, Bulgaria, Hungary and Poland. Within three years communist rule was extirpated from Europe.
Unlike Yugoslavia and the Soviet Union, the end of communism in this country did not automatically mean the end of the "communist" name: the word "socialist" was removed from the name on 29 March 1990 and replaced by "federal".
In 1992, because of growing nationalist tensions in the government, Czechoslovakia was peacefully dissolved by parliament. On 1 January 1993 it formally separated into two independent countries: the Czech Republic and the Slovak Republic.
Foreign policy.
International agreements and membership.
In the 1930s, the nation formed a military alliance with France, which collapsed in the Munich Agreement of 1938. After World War II, active participant in Council for Mutual Economic Assistance (Comecon), Warsaw Pact, United Nations and its specialized agencies; signatory of conference on Security and Cooperation in Europe.
Politics.
After World War II, a political monopoly was held by the Communist Party of Czechoslovakia (KSC). Gustáv Husák was elected first secretary of the KSC in 1969 (changed to general secretary in 1971) and president of Czechoslovakia in 1975. Other parties and organizations existed but functioned in subordinate roles to the KSC. All political parties, as well as numerous mass organizations, were grouped under umbrella of the National Front. Human rights activists and religious activists were severely repressed.
Constitutional development.
Czechoslovakia had the following constitutions during its history (1918–1992):
Economy.
After World War II, the economy was centrally planned, with command links controlled by the communist party, similarly to the Soviet Union. The large metallurgical industry was dependent on imports of iron and non-ferrous ores.
Resource base.
After World War II, the country was short of energy, relying on imported crude oil and natural gas from Soviet Union, domestic brown coal, and nuclear and hydroelectric energy. Energy constraints a major factor in the 1980s.
Education.
Education was free at all levels and compulsory from age 6 to 15. The vast majority of the population was literate. There was a highly developed system of apprenticeship training and vocational schools supplemented general secondary schools and institutions of higher education.
Religion.
In 1991: Roman Catholics 46%, Evangelical Lutheran 5.3%, Atheist 30%, n/a 17%, but there were huge differences in religious practices between the two constituent republics; see Czech Republic and Slovakia.
Health, social welfare and housing.
After World War II, free health care was available to all citizens. National health planning emphasised preventive medicine; factory and local health care centres supplemented hospitals and other inpatient institutions. There was substantial improvement in rural health care during the 1960s and 1970s.
Mass media.
During Communist rule, the mass media in Czechoslovakia were controlled by the Communist Party. Private ownership of any publication or agency of the mass media was generally forbidden, although churches and other organizations published small periodicals and newspapers. Even with this information monopoly in the hands of organizations under KSČ control, all publications were reviewed by the government's Office for Press and Information.
Sports.
The Czechoslovakia national football team was a consistent performer on the international scene, with eight appearances in the FIFA World Cup Finals, finishing in second place in 1934 and 1962. The team also won the European Football Championship in 1976, came in third in 1980 and won the Olympic gold in 1980.
The Czechoslovak national ice hockey team won many medals from the world championships and Olympic Games. Peter Šťastný, Jaromír Jágr, Dominik Hašek, Peter Bondra, Petr Klíma, Marián Gáborík, and Pavol Demitra all come from Czechoslovakia.
Emil Zátopek, winner of four Olympic gold medals in athletics, is considered one of the top athletes in history.
Věra Čáslavská was an Olympic gold medallist in gymnastics, winning seven gold medals and four silver medals. She represented Czechoslovakia in three consecutive Olympics.
The famous tennis players Ivan Lendl, Miloslav Mečíř, Hana Mandlíková, Martina Hingis, Martina Navratilova and Daniela Hantuchová were born in Czechoslovakia.

</doc>
<doc id="5323" url="http://en.wikipedia.org/wiki?curid=5323" title="Computer science">
Computer science

Computer science is the scientific and practical approach to computation and its applications. It is the systematic study of the feasibility, structure, expression, and mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to information, whether such information is encoded as bits in a computer memory or transcribed in genes and protein structures in a biological cell. A computer scientist specializes in the theory of computation and the design of computational systems.
Its subfields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental properties of Computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real-world visual applications. Still other fields focus on the challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, whilst the study of computer programming itself investigates various aspects of the use of programming language and complex systems. Human-computer interaction considers the challenges in making computers and computations useful, usable, and universally accessible to humans.
Computer science deals with the theoretical foundations of information and computation, together with practical techniques for the implementation and application of these foundations
History.
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Further, algorithms for performing computations have existed since antiquity, even before sophisticated computing equipment were created. The ancient Sanskrit treatise Shulba Sutras , or "Rules of the Chord", is a book of algorithms written in 800 BCE for constructing geometric objects like altars using a peg and chord, an early precursor of the modern field of computational geometry.
Blaise Pascal designed and constructed the first working mechanical calculator, Pascal's calculator, in 1642. In 1673 Gottfried Leibniz demonstrated a digital mechanical calculator, called the 'Stepped Reckoner'. He may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he released his simplified arithmometer, which was the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first "automatic mechanical calculator", his difference engine, in 1822, which eventually gave him the idea of the first "programmable mechanical calculator", his Analytical Engine. He started developing this machine in 1834 and "in less than two years he had sketched out many of the salient features of the modern computer. A crucial step was the adoption of a punched card system derived from the Jacquard loom" making it infinitely programmable. In 1843, during the translation of a French article on the "analytical engine", Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first computer program. Around 1885, Herman Hollerith invented the tabulator which used punched cards to process statistical information; eventually his company became part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's "analytical engine", which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".
During the 1940s, as new and more powerful computing machines were developed, the term "computer" came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science degree program in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.
Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population. It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704 and later the IBM 709 computers, which were widely used during the exploration period of such devices. "Still, working with the IBM [computer] was frustrating...if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again". During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.
Time has seen significant improvements in the usability and effectiveness of computing technology. Modern society has seen a significant shift in the users of computer technology, from usage only by experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of human aid was needed for efficient use - in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.
Major achievements.
Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society - in fact, along with electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the Information Revolution, seen as the third major leap in human technological progress after the Industrial Revolution (1750-1850 CE) and the Agricultural Revolution (8000-5000 BCE).
These contributions include:
Philosophy.
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).
Name of the field.
The term "computer science" appears in a 1959 article in Communications of the ACM,
in which Louis Fein argues for the creation of a "Graduate School in Computer Sciences" analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such programs, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term "computing science", to emphasize precisely that difference. Danish scientist Peter Naur suggested the term "datalogy", to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. Also, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the "Communications of the ACM" – "turingineer", "turologist", "flow-charts-man", "applied meta-mathematician", and "applied epistemologist". Three months later in the same journal, "comptologist" was suggested, followed next year by "hypologist". The term "computics" has also been suggested. In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. "informatique" (French), "Informatik" (German), "informatica" (Italy, The Netherlands), "informática" (Spain, Portugal), "informatika" (Slavic languages and Hungarian) or "pliroforiki" ("πληροφορική", which means informatics) in Greek. Similar words have also been adopted in the UK (as in "the School of Informatics of the University of Edinburgh").
A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes." The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, statistics, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel and Alan Turing, and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.
The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software engineering" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.
The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Areas of computer science.
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.
CSAB, formerly called "Computing Sciences Accreditation Board" – which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE-CS) – identifies four areas that it considers crucial to the discipline of computer science: "theory of computation", "algorithms and data structures", "programming methodology and languages", and "computer elements and architecture". In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and telecommunications, database systems, parallel computation, distributed computation, computer-human interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.
Theoretical computer science.
The broader field of theoretical computer science encompasses both the classical theory of computation and a wide range of other topics that focus on the more abstract, logical, and mathematical aspects of computing.
Theory of computation.
According to Peter J. Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated?" The study of the theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous "P=NP?" problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.
Information and coding theory.
Information theory is related to the quantification of information. This was developed by Claude E. Shannon to find
fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data. 
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
Algorithms and data structures.
Algorithms and data structures is the study of commonly used computational methods and their computational efficiency.
Programming language theory.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Applied computer science.
Applied computer science aims at identifying certain computer science concepts that can be used directly in solving real world problems.
Artificial intelligence.
This branch of computer science aims to or is required to synthesise goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning and communication which are found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence (AI) research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development which require computational understanding and modeling such as finance and economics, data mining and the physical sciences. The starting-point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered although the "Turing Test" is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture and engineering.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. The field often involves disciplines of computer engineering and electrical engineering, selecting and interconnecting hardware components to create computers that meet functional, performance, and cost goals.
Computer Performance Analysis.
Computer Performance Analysis is the study of work flowing through computers with the general goals of improving throughput, controlling response time, using resources efficiently, eliminating bottlenecks, and predicting performance under anticipated peak loads.
Computer graphics and visualization.
Computer graphics is the study of digital visual contents, and involves synthese and manipulations of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Computer security and cryptography.
Computer security is a branch of computer technology, whose objective includes protection of information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.
Computational science.
Computational science (or scientific computing) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. In practical use, it is typically the application of computer simulation and other forms of computation to problems in various scientific disciplines.
Computer networks.
This branch of computer science aims to manage networks between computers worldwide.
Concurrent, parallel and distributed systems.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. A distributed system extends the idea of concurrency onto multiple computers connected through a network. Computers within the same distributed system have their own private memory, and information is often exchanged amongst themselves to achieve a common goal.
Databases.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages.
Health informatics.
Health Informatics in computer science deals with computational techniques for solving problems in health care.
Software engineering.
Software engineering is the study of designing, implementing, and modifying software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software— it doesn't just deal with the creation or manufacture of new software, but its internal maintenance and arrangement. Both computer applications software engineers and computer systems software engineers are projected to be among the fastest growing occupations from 2008 and 2018.
The great insights of computer science.
The philosopher of computing Bill Rapaport noted three "Great Insights of Computer Science" 
Note that the 3 rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it's more elementary than structured programming.)
Academia.
Conferences.
Conferences are strategic events of the Academic Research in computer science. During those conferences, researchers from the public and private sectors present their recent work and meet. Proceedings of these conferences are an important part of the computer science literature.
Education.
Some universities teach computer science as a theoretical study of computation and algorithmic reasoning. These programs often feature the theory of computation, analysis of algorithms, formal methods, concurrency theory, databases, computer graphics, and systems analysis, among others. They typically also teach computer programming, but treat it as a vessel for the support of other fields of computer science rather than a central focus of high-level study. The ACM/IEEE-CS Joint Curriculum Task Force "Computing Curriculum 2005" (and 2008 update) gives a guideline for university curriculum.
Other colleges and universities, as well as secondary schools and vocational programs that teach computer science, emphasize the practice of advanced programming rather than the theory of algorithms and computation in their computer science curricula. Such curricula tend to focus on those skills that are important to workers entering the software industry. The process aspects of computer programming are often referred to as software engineering.
While computer science professions increasingly drive the U.S. economy, computer science education is absent in most American K-12 curricula. A report entitled was released in October 2010 by and , and revealed that only 14 states have adopted significant education standards for high school computer science. The report also found that only nine states count high school computer science courses as a core academic subject in their graduation requirements. In tandem with "Running on Empty", a new non-partisan advocacy coalition - - was founded to influence federal and state policy, such as the , which calls for grants to states to develop plans for improving computer science education and supporting computer science teachers.
Within the United States a gender gap in computer science education has been observed as well. Research conducted by the WGBH Educational Foundation and the Association for Computing Machinery (ACM) revealed that more than twice as many high school boys considered computer science to be a “very good” or “good” college major than high school girls. In addition, the high school Advanced Placement (AP) exam for computer science has displayed a disparity in gender. Compared to other AP subjects it has the lowest number of female participants, with a composition of about 15 percent women. This gender gap in computer science is further witnessed at the college level, where 31 percent of undergraduate computer science degrees are earned by women and only 8 percent of computer science faculty consists of women. According to an published by the in August 2012, the number of women graduates in the computer science field has declined to 13 percent.
A 2014 Mother Jones article, "We Can Code It", advocates for adding computer literacy and coding to the K-12 curriculum in the United States, and notes that computer science is not incorporated into the requirements for the Common Core State Standards Initiative.
References.
"Computer Software Engineer." U.S. Bureau of Labor Statistics. U.S. Bureau of Labor Statistics, n.d. Web. 05 Feb. 2013.

</doc>
<doc id="5324" url="http://en.wikipedia.org/wiki?curid=5324" title="Catalan">
Catalan

Catalan may refer to something of, from, or related to Catalonia.
Catalan may also refer to:

</doc>
<doc id="5326" url="http://en.wikipedia.org/wiki?curid=5326" title="Creationism">
Creationism

Creationism is the belief that the Universe and living organisms originate "from specific acts of divine creation." For young Earth creationists, this includes a literalistic reading of the Book of Genesis and the rejection of evolution. As science developed during the 18th century and forward, various views aimed at reconciling the Abrahamic and Genesis creation narratives with science developed in Western societies. Those holding that species had been created separately (such as Philip Gosse in 1857) were generally called "advocates of creation" but were also called "creationists," as in private correspondence between Charles Darwin and his friends. As the creation–evolution controversy developed over time, the term "anti-evolutionists" became common. In 1929 in the United States, the term "creationism" first became associated with Christian fundamentalists, specifically with their rejection of human evolution and belief in a young Earth—although this usage was contested by other groups, such as old Earth creationists and evolutionary creationists, who hold different concepts of creation, such as the acceptance of the age of the Earth and biological evolution as understood by the scientific community.
Today, the American Scientific Affiliation, a prominent religious organisation in the US, recognizes that there are different opinions among creationists on the method of creation, while acknowledging unity on the Abrahamic belief that God "created the universe." Since the 1920s, literalist creationism in America has contested scientific theories, such as that of evolution, which derive from natural observations of the Universe and life. Literalist creationists believe that evolution cannot adequately account for the history, diversity, and complexity of life on Earth. Fundamentalist creationists of the Christian faith usually base their belief on a literal reading of the Genesis creation narrative. Other religions have different deity-led creation myths, while different members of individual faiths vary in their acceptance of scientific findings.
When scientific research produces empirical evidence and theoretical conclusions which contradict a literalist creationist interpretation of scripture, young Earth creationists often reject the conclusions of the research or its underlying scientific theories or its methodology. This tendency has led to political and theological controversy. Two disciplines somewhat allied with creationism—creation science and intelligent design—have been labelled "pseudoscience" by scientists. The most notable disputes concern the evolution of living organisms, the idea of common descent, the geological history of the Earth, the formation of the solar system and the origin of the universe.
Theistic evolution, also known as Evolutionary Creationism, is an attempt to reconcile religion with scientific findings on the age of the Earth and evolution. The term covers a range of views including Old Earth creationism.
History.
The term "creationist" to describe a proponent of creationism was first used in a letter by Charles Darwin in 1856. In the 1920s, the term became particularly associated with Christian fundamentalist movements that insisted on a literalist interpretation of the Genesis creation narrative and likewise opposed the idea of human evolution. These groups succeeded in getting teaching of evolution banned in American public schools, then from the mid-1960s the young Earth creationists promoted the teaching of "scientific creationism" using "Flood geology" in public school science classes as support for a purely literal reading of the Book of Genesis. After the legal judgment of the case "Daniel v. Waters" (1975) ruled that teaching creationism in public schools contravened the Establishment Clause of the First Amendment to the United States Constitution, the content was stripped of overt biblical references and renamed creation science. When the court case "Edwards v. Aguillard" (1987) ruled that creation science similarly contravened the constitution, all references to "creation" in a draft school textbook were changed to refer to intelligent design, which was presented by creationists as a new scientific theory. The "Kitzmiller v. Dover" (2005) ruling concluded that intelligent design is not science and contravenes the constitutional restriction on teaching religion in public school science classes. In September 2012, Bill Nye ("The Science Guy") expressed his concern that "creationist views" threaten science education and innovations in the US.
Early and medieval times.
The first century Jewish philosopher Philo of Alexandria admired the literal narrative of passages concerning the Patriarchs, but in other passages viewed the literal interpretation as being for those unable to see an underlying deeper meaning. For example, he noted that Moses said the world was created in six days, but did not consider this as a length of time as "we must think of God as doing all things simultaneously" and the six days were mentioned because of a need for order and according with a perfect number. Genesis was about real events, but God through Moses described them in figurative or allegorical language.
The early Christian Church Fathers largely read creation history as an allegory, and followed Philo's ideas of time beginning with an instantaneous creation without the convention that a day was the conventional time period. Christian orthodoxy rejected the second century Gnostic belief that the Book of Genesis was purely allegorical, but without taking a purely literal view of the texts. Thus, Origen believed that the physical world is ‘literally’ a creation of God, but did not take the chronology or the days as ‘literal’. Similarly, Saint Basil the Great in the fourth century while literal in many ways, described creation as instantaneous and timeless, being immeasurable and indivisible.
Augustine of Hippo in "On the Literal Meaning of Genesis" was insistent that the Book of Genesis describes the creation of physical objects, but also shows creation occurring simultaneously, with the days of creation being categories for didactic reasons, a logical framework which has nothing to do with time. For him, light was the illumination of angels rather than visible light, and spiritual light was just as literal as physical light. Augustine emphasized that the text was difficult to understand and should be reinterpreted as new knowledge became available. In particular, Christians should not make absurd dogmatic interpretations of scripture which contradict what people know from physical evidence.
In the 13th century, Thomas Aquinas, like Augustine, asserted the need to hold the truth of scripture without wavering while cautioning "that since Holy Scripture can be explained in a multiplicity of senses, one should not adhere to a particular explanation, only in such measure as to be ready to abandon it if it be proved with certainty to be false; lest holy Scripture be exposed to the ridicule of unbelievers, and obstacles be placed to their believing."
Impact of the Reformation.
From 1517 the Protestant Reformation brought a new emphasis on lay literacy. Martin Luther taught young Earth creationism, that creation took six literal days about 6000 years ago. John Calvin also rejected instantaneous creation, but criticised those who, contradicting the contemporary understanding of nature, asserted that there are "waters above the heavens."
Discoveries of new lands brought knowledge of a huge diversity of life, and a new belief developed that each of these biological species had been individually created by God. In 1605, Francis Bacon emphasized that the works of God in nature teach us how to interpret the word of God in the Bible, and his Baconian method introduced the empirical approach which became central to modern science. Natural theology developed the study of nature with the expectation of finding evidence supporting Christianity, and numerous attempts were made to reconcile new knowledge with the biblical deluge myth and story of Noah's Ark.
In 1650 the Archbishop of Armagh, James Ussher, published the Ussher chronology based on Bible history giving a date for Creation of 4004 BC. This was generally accepted, but the development of modern geology in the 18th and 19th centuries found geological strata and fossil sequences indicating an ancient Earth. Catastrophism was favoured in England as supporting the biblical flood, but this was found to be untenable and by 1850 all geologists and most Evangelical Christians had adopted various forms of old Earth creationism, while continuing to firmly reject evolution.
Modern science.
From around the start of the 19th century, ideas such as Jean-Baptiste Lamarck's concept of transmutation of species had gained some supporters in Paris and Edinburgh, mostly amongst anatomists. The anonymous publication of "Vestiges of the Natural History of Creation" in 1844 aroused wide public interest with support from Quakers and Unitarians, but was strongly criticised by the scientific community, which called for solidly backed science. In 1859, Charles Darwin's "On the Origin of Species" provided that evidence from an authoritative and respected source, and within a decade or so convinced scientists that evolution occurs. This view clashed with that of conservative evangelicals in the Church of England, but their attention quickly turned to the much greater uproar about "Essays and Reviews" by liberal Anglican theologians, which introduced into the controversy "higher criticism" begun by Erasmus centuries earlier. This book re-examined the Bible and cast doubt on a literal interpretation. By 1875 most American naturalists supported ideas of theistic evolution, often involving special creation of human beings.
At this time those holding that species had been separately created were generally called "advocates of creation," but they were occasionally called "creationists" in private correspondence between Darwin and his friends. The term appears in letters Darwin wrote between 1856 and 1863, and was also used in a response by Charles Lyell.
Types of creationism.
Several attempts have been made to categorize the different types of creationism, and create a "taxonomy" of creationists. Creationism (broadly construed) covers a spectrum of beliefs which have been categorized into the general types listed below.
Young Earth creationism.
Young Earth creationists believe that God created the Earth within the last ten thousand years, literally as described in the Genesis creation narrative, within the approximate time-frame of biblical genealogies (detailed for example in the Ussher chronology). Most young Earth creationists believe that the Universe has a similar age as the Earth. A few assign a much older age to the Universe than to Earth. Creationist cosmologies attempt to give the Universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the Universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the Universe their much longer timelines. However, this view has theological implications—an intentional appearance of age is form of false evidence and so a form of deception.
The Christian organizations Institute for Creation Research (ICR) and the Creation Research Society (CRS) both promote young Earth creationism in the US. Another organization with similar views, Answers in Genesis (AiG)—based in both the US and the United Kingdom—has opened the Creation Museum in Petersburg, Kentucky, to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the US, and the UK. Among Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas.
Creation science.
Creation science, or initially scientific creationism, is pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. It is the attempt to present scientific evidence interpreted with Genesis axioms that supports the claims of creationism. Various claims of creationists include such ideas as creationist cosmologies which accommodate a Universe on the order of thousands of years old, attacks on the science of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the destruction of the global flood recorded in the Book of Genesis (see Flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in "created kinds" or "Baramin" (see creationist biology) due to mutations.
Old Earth creationism.
Old Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is not to be taken strictly literally. This group generally believes that the age of the Universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.
Old Earth creationism itself comes in at least three types:
Gap creationism.
Gap creationism, also called "restoration creationism," holds that life was recently created on a pre-existing old Earth. This theory relies on a particular interpretation of . It is considered that the words "formless" and "void" in fact denote waste and ruin, taking into account the original Hebrew and other places these words are used in the Old Testament. Genesis 1:1-2 is consequently translated:
Thus, the six days of creation (verse 3 onwards) start sometime after the Earth was "without form and void." This allows an indefinite "gap" of time to be inserted after the original creation of the Universe, but prior to the creation according to Genesis, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and Universe, while maintaining a literal interpretation of the biblical text.
Some gap theorists expand the basic theory by proposing a "primordial creation" of biological life within the "gap" of time. This is thought to be "the world that then was" mentioned in 2 Peter 3:3-7. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this "world that then was," which may also be associated with Lucifer's rebellion. These views became popular with publications of Hebrew Lexicons such as the Strong's Concordance, and Bible commentaries such as the "Scofield Reference Bible" and the Companion Bible.
Day-age creationism.
Day-age creationism states that the "six days" of the Book of Genesis are not ordinary 24-hour days, but rather much longer periods (for instance, each "day" could be the equivalent of millions, or billions of years of human time). This theory often states that the Hebrew word "yôm," in the context of Genesis 1, can be properly interpreted as "age." Some adherents claim we are still living in the seventh age ("seventh day").
Strictly speaking, day-age creationism is not so much a creationist theory as a hermeneutic option which may be combined with theories such as progressive creationism.
Progressive creationism.
Progressive creationism holds that species have changed or evolved in a process continuously guided by God, with various ideas as to how the process operated—though it is generally taken that God directly intervened in the natural order at key moments in Earth history. This view accepts most of modern physical science including the age of the Earth, but rejects much of modern evolutionary biology or looks to it for evidence that evolution by natural selection alone is incorrect. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this theory.
Progressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age theory or framework/metaphoric/poetic views.
Neo-creationism.
Neo-Creationists intentionally distance themselves from other forms of creationism, preferring to be known as wholly separate from creationism as a philosophy. Neo-creationism aims to restate creationism in terms more likely to be well received by the public, policy makers, educators and the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture, and to bring the debate before the public.
Neo-creationism sees ostensibly objective mainstream science as a dogmatically atheistic religion. Neo-creationists argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements. They argue that this effectively excludes any possible religious insight from contributing to a scientific understanding of the Universe. Neo-creationists also argue that science, as an "atheistic enterprise," lies at the root of many of contemporary society's ills including social unrest and family breakdown.
The intelligent design movement arguably represents the most recognized form of neo-creationism in the US. Unlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible. Common to all forms of neo-creationism is a rejection of naturalism, usually made together with a tacit admission of supernaturalism, and an open and often hostile opposition to what they term "Darwinism," meaning evolution.
Intelligent design.
Intelligent design (ID) is the pseudoscientific view that "certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection." All of its leading proponents are associated with the Discovery Institute, a think tank whose Wedge strategy aims to replace the scientific method with "a science consonant with Christian and theistic convictions" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and some have even begun referring to it as "intelligent design creationism."
ID originated as a re-branding of creation science in an attempt to get round a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.
In the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In "Kitzmiller v. Dover", the court found that intelligent design is not science and "cannot uncouple itself from its creationist, and thus religious, antecedents," and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in "Edwards v. Aguillard" and "Epperson v. Arkansas" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.
Obscure and largely discounted beliefs.
In astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the Cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.
Articles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis for example, Joshua 10:12 where the Sun and Moon are said to stop in the sky, and Psalms 93:1 where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published "Galileo Was Wrong: The Church Was Right" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the Universe was created and requires a geocentric worldview. Most contemporary creationist organizations reject such perspectives.
Omphalos hypothesis.
The Omphalos hypothesis argues that in order for the world to be functional, God must have created a mature Earth with mountains and canyons, rock strata, trees with growth rings, and so on; therefore "no" evidence that we can see of the presumed age of the Earth and age of the Universe can be taken as reliable. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to light that originates in far-off stars and galaxies (see The "starlight problem").
Theistic evolution.
Theistic evolution, or evolutionary creation, is a belief that "the personal God of the Bible created the universe and life through evolutionary processes." According to the American Scientific Affiliation:
Through the 19th century the term "creationism" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of "Vestiges of the Natural History of Creation", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When "On the Origin of Species" was published, the cleric Charles Kingsley wrote of evolution as "just as noble a conception of Deity." Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to "creation," though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or "modus operandi", of the first cause, design, and published a pamphlet defending the book in theistic terms, "Natural Selection not inconsistent with Natural Theology". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.
Some theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as "evolutionary creation." In Evolution versus Creationism, Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.
It generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the Universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a "literal" description, but rather as a literary framework or allegory.
From a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.
In one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies "have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man." Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as "creationism" in holding that divine intervention brought about the origin of life or that divine Laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation–evolution controversy its proponents generally take the "evolutionist" side. This sentiment was expressed by Fr. George Coyne, (Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.
While supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. 
Religious views.
Christianity.
 most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation narrative. The US is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.
Most contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former Archbishop of Canterbury, Rowan Williams, "...for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time."
Leaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.
Many Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the Universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical "Humani Generis". In 1996, Pope John Paul II stated that "new knowledge has led to the recognition of the theory of evolution as more than a hypothesis," but, referring to previous papal writings, he concluded that "if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God."
In the U.S., Evangelical Christians have continued to believe in a literal Genesis. Members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations are the most likely to reject the evolutionary interpretation of the origins of life. The historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1-2:3 and Genesis 2:4-25, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.
Christian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or "spiritual" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Mary Baker Eddy showed a preference for Darwin's theory of evolution over others.
Hinduism.
According to Hindu creationism all species on Earth including humans have "devolved" or come down from a high state of pure consciousness. Hindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: "Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago." Hindu creationism is a form of old Earth creationism, according to Hindu creationists the Universe may even be older than billions of years. These views are based on the Vedas which depict an extreme antiquity of the Universe and history of the Earth.
Islam.
Islamic creationism is the belief that the Universe (including humanity) was directly created by God as explained in the Qur'an. It usually views the Book of Genesis as a corrupted version of God's message. The creation myths in the Qur'an are vaguer and allow for a wider range of interpretations similar to those in other Abrahamic religions.
Islam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the Universe is supported by the Qur'an. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.
Khalid Anees, president of the Islamic Society of Britain, at a conference called 'Creationism: Science and Faith in Schools', made points including the following:There is no contradiction between what is revealed in the Koran and natural selection and survival of the fittest. However, Muslims do not agree that one species can develop from another.
Writing for "The Boston Globe", Drake Bennett noted: "Without a Book of Genesis to account for ... Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims." However, some Muslims, such as Adnan Oktar (also known as Harun Yahya), do not agree that one species can develop from another.
But there is also a growing movement of Islamic creationism. Similar to Christian creationism, there is concern regarding the perceived conflicts between the Qur'an and the main points of evolutionary theory. The main location for this has been in Turkey, where fewer than 25% of people believe in evolution.
There are several verses in the Qur'an which some modern writers have interpreted as being compatible with the expansion of the Universe, Big Bang and Big Crunch theories:
Ahmadiyya.
The Ahmadiyya movement activey promotes evolutionary theory. Ahmadis interpret scripture from the Qur'an to support the concept of macroevolution and give precedence to scientific theories. Furthermore, unlike orthodox Muslims, Ahmadis believe that mankind has gradually evolved from different species. Ahmadis regard Adam as being the first Prophet of Godas opposed to him being the first man on Earth. Rather than wholly adopting the theory of natural selection, Ahmadis promote the idea of a "guided evolution," viewing each stage of the evolutionary process as having been selectively woven by God. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus "Revelation, Rationality, Knowledge & Truth" (1998) that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community.
Judaism.
Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work. For Orthodox Jews who seek to reconcile discrepancies between science and the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, our own epistemological limits are to blame for any apparent irreconcilable point. They point to various discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They point out the fact that even the root word for "world" in the Hebrew language—עולם (Olam)—means hidden—נעלם (Neh-Eh-Lahm). Just as they believe God created man and trees and the light on its way from the stars in their adult state, so too can they believe that the world was created in its "adult" state, with the understanding that there are, and can be, no physical ways to verify it. This belief has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University. Also, relatively old Kabbalistic sources from well before the scientifically apparent age of the Universe was first determined are in close concord with modern scientific estimates of the age of the Universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and the David ben Solomon ibn Abi Zimra. Other interesting parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically).
Bahá'í Faith.
Bahá'u'lláh, the Bahá'í Faith founder, taught that the Universe has "neither beginning nor ending," and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, `Abdu'l-Bahá gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in "Some Answered Questions", "Paris Talks" and "The Promulgation of Universal Peace". `Abdu'l-Bahá described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.
Creationism by country.
Creationism is widely accepted and taught throughout the Middle East. Although it has been prominent in the US but not widely accepted in academia, it has been making a resurgence in other countries as well.
Europe.
In recent years the teaching of creationism has become a subject of debate in a variety of countries including Germany, the UK, Italy, the Netherlands, Poland, and Serbia.
Creation science has been heavily promoted in immigrant communities in Western Europe, primarily by Adnan Oktar. On October 4, 2007, the Parliamentary Assembly of the Council of Europe adopted "The dangers of creationism in education", a resolution on the attempt by American-inspired creationists to promote creationism in European schools. It concludes "The war on the theory of evolution and on its proponents most often originates in forms of religious extremism closely linked to extreme right-wing political movements... some advocates of strict creationism are out to replace democracy by theocracy... If we are not careful, the values that are the very essence of the Council of Europe will be under direct threat from creationist fundamentalists."
Germany.
In 1978, British Professor A. E. Wilder-Smith, who came to Germany after World War II and lectured at Marburg and other cities, published a book arguing against evolution with a secular, well known publishing house, titled "The Natural Sciences Know Nothing of Evolution" (1978). At the end of the year Horst W. Beck became a creationist. Both an engineer and theologian, he was a leading figure in the "Karl-Heim-Gesellschaft" (Karl Heim Society) and had previously published articles and books defending theistic evolution. Together with other members of the society, which they soon left, he followed the arguments of Willem Ouweneel, a Dutch biologist lecturing in Germany. Beck soon found other scientists who had changed their view or were "hidden" creationists. Under his leadership, the first creationist society was founded ("Wort und Wissen"—Word and Knowledge). Three book series were soon published, an independent creationist monthly journal started ("Factum"), and the first German article in the "Creation Research Society Quarterly" was published.
In 2006, a documentary on the Arte television network, "Von Göttern und Designern" ("Genesis vs. Darwin"), by filmmaker Frank Papenbroock, demonstrated that creationism had already been taught in biology classes in at least two schools in Giessen, Hesse, without this being noticed. During this, the Education Minister of Hessen, Karin Wolff, said she believed creationism should be taught in biology class as a theory, like the theory of evolution: "I think it makes sense to bring up multidisciplinary and interdisciplinary problems for discussion." In 2009, an article on the German news site Spiegel Online stated approximately 20% of people disbelieve evolutionary theory in Germany. More recently, a 2011 Ipsos poll commissioned by Reuters found 12% of Germans identify as creationists.
Romania.
In Romania, in 2002, the Ministry of Education approved the use of a biology book endorsing creationism, titled "Biologie clasa a IX-a – Măiestrie şi strălucire divină în biosferă" ("Biology Class IX – Divine Mastery and Light in the Biosphere"), in public high schools. Following a protest of the Romanian Humanist Association the Romanian Ministry of Education replied that the book is not a "textbook" but merely an "accessory." The president of the Association labeled the reply as "disappointing" since, whether a textbook or an accessory, the book remains available for usage in schools. Reports indicate that at least one teacher in Oradea did use the book.
Russia.
Russia is home to the Moscow Creation Society. The department of extracurricular and alternative education of the Ministry of Education and Science of the Russian Federation has cosponsored numerous creationist conferences. Since 1994, Alexander Asmolov, the previous deputy minister of education, has urged that creationism be taught to help restore academic freedom in Russia after years of state-enforced scientific orthodoxy. In February 2007, a 16-year-old girl and her father launched a court case against the Ministry of Education and Science, backed by the Russian Orthodox Church, challenging the teaching of just one "theory" of biology in school textbooks as a breach of her human rights.
A 2005 poll reportedly found 26% of Russians accepting evolution and 49% accepting creationism. But a 2003 poll reported that 44% agreed with "Human beings are developed from earlier species of animals," and a 2009 poll reported (PDF) that 48% of Russians who "know something about Charles Darwin and his theory of evolution" agreed that there was sufficient evidence for the theory. The 2009 poll indicated that 53% of Russians agreed with "Evolutionary theories should be taught in science lessons in schools together with other possible perspectives, such as intelligent design and creationism," with 13% preferring that such perspectives be taught instead of evolution; only 10% agreed with "Evolutionary theories alone should be taught in science lessons in schools."
Serbia.
On September 7, 2004, the Serbian Minister for Education and Sport, Ljiljana Čolić, temporarily banned evolution from being taught in the country. After state-wide outcry she resigned on September 16, 2004, from her post.
Switzerland.
A 2006 international survey found that 30% of the Swiss reject evolution, one of the highest national percentages in Europe. Another survey in 2007, commissioned by the fringe Christian organization Pro Genesis, controversially claims 80%. This resulted in schools in the Canton of Bern printing science textbooks that presented creationism as a valid alternative theory to evolution. Scientists and education experts harshly criticized the move, which quickly prompted school authorities to revise the books.
United Kingdom.
Since the development of evolutionary theory by Charles Darwin in England, where his portrait appears on the back of the revised Series E £10 note issued in 2000, significant shifts in British public opinion have occurred. A 2006 survey for the BBC showed that "more than a fifth of those polled were convinced by the creationist argument," a massive decrease from the almost total acceptance of creationism before Darwin published his theory. A 2010 Angus Reid poll found that "In Britain, two-thirds of respondents (68%) side with evolution while less than one-in-five (16%) choose creationism. At least seven-in-ten respondents in the South of England (70%) and Scotland (75%) believe human beings evolved from less advanced life forms over millions of years." A subsequent 2010 YouGov poll on the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% did not know.
Speaking at the British Science Association's British Science Festival at the University of Liverpool in 2008, Professor Michael Reiss estimated that about only 10% of children were from a family that supported a creationist rather than evolutionary viewpoint. Richard Dawkins has been quoted saying "I have spoken to a lot of science teachers in schools here in Britain who are finding an increasing number of students coming to them and saying they are Young Earth creationists."
The director of education at the Royal Society has said that creationism should be discussed in school science lessons, rather than be excluded, to explain why creationism had no scientific basis. Wales has the largest proportion of theistic evolutionists—the belief that evolution is part of God's plan (38%). Northern Ireland has the highest proportion of people who believe in 'intelligent design' (16%), which holds that certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection. Some private religious schools in the UK teach creationism rather than evolution. However, the teaching of creationism is illegal in any school that receives state funding.
Muslim world.
A 2007 study of religious patterns found that only 8% of Egyptians, 11% of Malaysians, 14% of Pakistanis, 16% of Indonesians, and 22% of Turks agree that Darwin's theory is probably or most certainly true, and a 2006 survey reported that about a quarter of Turkish adults agreed that human beings evolved from earlier animal species. Surveys carried out by researchers affiliated with McGill University's Evolution Education Research Centre found that in Egypt and Pakistan, while the official high school curriculum does include evolution, many of the teachers there do not believe in it themselves, and will often tell their students so.
Currently in Egypt, evolution is taught in schools but Saudi Arabia and Sudan have both banned the teaching of evolution in schools. In recent times, creationism has become more widespread in other Islamic countries.
The results of a survey of the adherence to creation science of 5,700 teachers from 14 countries was presented during the 2008 XIII IOSTE Symposium in Izmir, Turkey. Lebanon, Senegal, Tunisia, Morocco and Algeria had 62% to 81% of creationist teachers (with no difference between biologists and others). Romania and Burkina Faso had 45% to 48% of creationist teachers in Romania and Burkina Faso, with no difference between biologists and other in Romania, but a clear difference (p<0.001) in Burkina Faso (with 61% of creationists for the not biology teachers). Portugal and Cyprus had 15% to 30% of creationist teachers, with no significant difference between biologists, but a significant difference in Portugal (p=0.004, 17% and 26%).
Iran.
Iranian scientific development, especially the health-related aspects of biology, has been a goal of the Islamic government since the revolution of 1979. Since Iranian traditional practice of Shi'a religion is not preoccupied with Qur'anic literalism as in case of Saudi Wahhabism but ijtihad, many influential Iranian Shi'ite scholars, including several who were closely involved in Iranian Revolution, are not opposed to evolutionary ideas in general, disagreeing that evolution necessarily conflicts with the Muslim mainstream. Iranian pupils, since 5th grade of elementary school, learn only about evolution, thus portraying geologists and scientists in general as authoritative voices of scientific knowledge.
Turkey.
Since the 1980s, creationism in Turkey has grown significantly and is now the government's official position on origins. In 1985, the conservative political party then in control of the country’s education ministry added creationist explanations alongside the passages on evolution in the standard high school biology textbook. In Turkey, unlike in the US, the public school curriculum is set by the national government. In 2008, Richard Dawkins' website was banned in Turkey. However, the ban was lifted in July 2011. In 2009, the Turkish government agency Scientific and Technological Research Council of Turkey (TÜBİTAK), publisher of the popular Turkish science magazine "Bilim ve Teknik" ("Science and Technology"), was accused of stripping a cover story about the life and work of Charles Darwin from the March 2009 issue of the Council's publication just before it went to press. The planned portrait of Darwin for the magazine's cover was replaced and the editor of the magazine, Çiğdem Atakuman, claims that she was removed from her post. Most of the Turkish population expressed support for the censorship. In 2012, it was found that the government's internet content filter, designed to prevent the public having access to pornographic websites, also blocked the words 'evolution' and 'Darwin' on one mode of the filter.
Australia.
In the late 1970s, Answers in Genesis, a creationist research organization, was founded in Australia. In 1994, Answers in Genesis expanded from Australia and New Zealand to the US. It subsequently expanded into the UK, Canada, South Africa and New Zealand. Creationists in Australia have been the leading influence on the development of creation science in the US for the last 20 years. Two of the three main international creation science organizations all have original roots within Australia—Answers in Genesis and Creation Ministries. Ken Ham, Andrew Snelling, Jason Lisle, Jonathan Sarfati and Tasman Bruce Walker have all had significant impact on the development of creationism in Australia, and have brought their teaching to the US.
In 1980, the Queensland state government of Joh Bjelke-Petersen allowed the teaching of creationism as science to school children. On May 29, 2010, it was announced that creationism and intelligent design will be discussed in history classes as part of the new national curriculum. It will be placed in the subject of ancient history, under the topic of "controversies." One Australian scientist who adheres to creation science is Dr Pierre Gunnar Jerlström.
Professor Ian Plimer, an anti-creationist geologist, reported being attacked by creationists. A few public lectures have been given in rented rooms at universities, by visiting American speakers, and speakers with doctorates purchased by mail from Florida sites. A court case taken by Plimer against prominent creationists found "that the creationists had stolen the work of others for financial profit, that the creationists told lies under oath and that the creationists were engaged in fraud." The debate was featured on the science television program "Quantum". In 1989, Plimer debated American creationist Duane Gish.
Asia.
South Korea.
Since 1981, the Korea Association for Creation Research has grown to 16 branches, with 1000 members and 500 Ph.Ds. On August 22–24, 1991, recognizing the 10th anniversary of KACR, an International Symposium on Creation Science was held with 4,000 in attendance. In 1990, the book "The Natural Sciences" was written by Dr. Young-Gil Kim and 26 other fellow scientists in Korea with a creationist viewpoint. The textbook drew the interest of college communities, and today, many South Korean universities are using it.
Since 1991, creation science has become a regular university course at Myongji University, which has a centre for creation research. Since that time, other universities have begun to offer creation science courses. At Handong Global University, creationist Dr. Young-Gil Kim was inaugurated as president in March 1995. At Myongji University, creationist Dr. Woongsang Lee is a biology professor. The Korea Advanced Institute of Science and Technology is where the Research Association of Creation Science was founded and many graduate students are actively involved. In 2008, a survey found that 36% of South Koreans disagreed with the statement that "Human beings, as we know them today, developed from earlier species of animals." In May 2012, publishers of high school science textbooks decided to remove references to evolution following a petition by a creationist group. However, the ensuing controversy prompted the government to appoint a panel of scientists to look into the matter, and the government urged the publishers to keep the references to evolution following the recommendation of the panel.
Americas.
Brazil.
Brazil has had two creationist societies since the 1970s—the Brazilian Association for Creation Research and the Brazilian Creation Society. According to a 2004 survey, 31% of Brazil believe that "the first humans were created no more than 10,000 years ago."
United States.
In the US some religious communities have refused to accept naturalistic explanations and tried to counter them. The term started to become associated with Christian fundamentalist opposition to human evolution and belief in a young Earth in 1929. Several US states passed laws against the teaching of evolution in public schools, as upheld in the Scopes Trial. Evolution was omitted entirely from school textbooks in most of the US until the 1960s. Since then, renewed efforts to introduce teaching creationism in American public schools in the form of Flood geology, creation science, and intelligent design have been consistently held to contravene the constitutional separation of church and state by a succession of legal judgments. The meaning of the term creationism was contested, but by the 1980s it had been co-opted by proponents of creation science and Flood geology.
Most of the anti-evolutionists of the 1920s believed in forms of old Earth creationism, which accepts geological findings and other methods of dating the Earth and believes that these findings do not contradict the Book of Genesis, but rejects evolution. At that time only a minority held to young Earth creationism, proponents of which believe that the Earth is thousands rather than billions of years old, and typically believe that the days in chapter one of the Book of Genesis are 24 hours in length. In the 1960s, this became the most prominent form of anti-evolution. From the 1860s forms of theistic evolution had developed; this term refers to beliefs in creation which are compatible with the scientific view of evolution and the age of the Earth, as held by mainstream Christian denominations. There are other religious people who support creationism, but in terms of allegorical interpretations of the Book of Genesis.
By the start of the 20th century, evolution was widely accepted and was beginning to be taught in American public schools. After World War I, popular belief that German aggression resulted from a Darwinian doctrine of "survival of the fittest" inspired William Jennings Bryan to campaign against the teaching of Darwinian ideas of human evolution. In the 1920s, the Fundamentalist–Modernist Controversy led to an upsurge of fundamentalist religious fervor in which schools were prevented from teaching evolution through state laws such as Tennessee’s 1925 Butler Act, and by getting evolution removed from biology textbooks nationwide. "Creationism" became associated in common usage with opposition to evolution.
In 1961 in the US, an attempt to repeal the Butler Act failed. "The Genesis Flood" by Henry M. Morris brought the Seventh-day Adventist biblically literal Flood geology of George McCready Price to a wider audience, popularizing the idea of young Earth creationism, and by 1965 the term "scientific creationism" had gained currency. The 1968 "Epperson v. Arkansas" judgment ruled that state laws prohibiting the teaching of evolution violate the Establishment Clause of the First Amendment to the United States Constitution which prohibits state aid to religion. and when in 1975 "Daniel v. Waters" ruled that a state law requiring biology textbooks discussing "origins or creation of man and his world" to give equal treatment to creation as per the Book of Genesis was unconstitutional, a new group identifying themselves as creationists promoted 'creation science' which omitted explicit biblical references.
In 1981, the state of Arkansas passed a law, Act 590, mandating that "creation science" be given equal time in public schools with evolution, and defining creation science as positing the "creation of the universe, energy, and life from nothing," as well as explaining the Earth's geology by "the occurrence of a worldwide flood." This was ruled unconstitutional at "McLean v. Arkansas" in January 1982 as the creationists' methods were not scientific but took the literal wording of the Book of Genesis and attempted to find scientific support for it. Louisiana introduced similar legislation that year. A series of judgments and appeals led to the 1987 Supreme Court ruling in "Edwards v. Aguillard" that it too violated the Establishment Clause of the First Amendment to the United States Constitution.
"Creation science" could no longer be taught in public schools, and in drafts of the creation science school textbook "Of Pandas and People" all references to creation or creationism were changed to refer to intelligent design. Proponents of the intelligent design movement organised widespread campaigning to considerable effect. They officially denied any links to creation or religion, and claimed that "creationism" only referred to young Earth creationism with Flood geology; but in "Kitzmiller v. Dover" the court found intelligent design to be religious, and unable to dissociate itself from its creationist roots, as part of the ruling that teaching intelligent design in public school science classes was unconstitutional.
The percentage of people in the US who accept the idea of human evolution declined from 45% in 1985 to 40% in 2005. A Gallup poll reported that the percentage of people in the US who believe in a strict interpretation of creationism had fallen to 40% in 2010 after a high of 46% in 2006. The highest the percentage has risen between 1982 and 2010 was 47% in 1994 and 2000 according to the report. The report found that Americans who are less educated are more likely to hold a creationist view while those with a college education are more likely to hold a view involving evolution. 47% of those with no more than a high school education believe in creationism while 22% of those with a post graduate education hold that view. The poll also found that church attendance dramatically increased adherence to a strict creationist view (22% for those who do not attend church, 60% for those who attend weekly). The higher percentage of Republicans who identified with a creationist view is described as evidence of the strong relationship between religion and politics in the US. Republicans also attend church weekly more than Democratic or independent voters. Non-Republican voters are twice as likely to hold a nontheistic view of evolution than Republican voters.
Among US states, acceptance of evolution has a strong negative correlation with religiosity and a strong positive relationship with science degrees awarded, bachelor degree attainment, advanced degree attainment, average teacher salary, and GDP per capita. In other words, states in which more people say that religion is very important to their lives tend to show less acceptance of evolution. The better the education of individuals, their educational system, or the higher their income, the more they accept evolution, though the US as a country has a comparatively well educated population but lower acceptance of evolution than other countries.
Prevalence.
Most vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in "Science", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.
Australia.
A 2009 Nielsen poll showed that almost a quarter of Australians believe "the biblical account of human origins". Forty-two percent believe in a "wholly scientific" explanation for the origins of life, while 32 percent believe in an evolutionary process "guided by God."
Canada.
A 2012 survey by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked "Where did human beings come from — did we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?"
Europe.
In Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.
In the UK, a 2006 poll on the "origin and development of life" asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake.
In Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.
There continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled "The dangers of creationism in education" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.
Serbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana Čolić, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. "After a deluge of protest from scientists, teachers and opposition parties" says the BBC report, Čolić's deputy made the statement, "I have come here to confirm Charles Darwin is still alive" and announced that the decision was reversed. Čolić resigned after the government said that she had caused "problems that had started to reflect on the work of the entire government."
Poland saw a major controversy over creationism in 2006 when the Deputy Education Minister, Mirosław Orzechowski, denounced evolution as "one of many lies" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, "as long as most scientists in our country say that it is the right theory." Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.
United States.
According to a 2014 Gallup poll, about 42% of Americans believe that "God created human beings pretty much in their present form at one time within the last 10,000 years or so." Another 31% believe that "human beings have developed over millions of years from less advanced forms of life, but God guided this process,"and 19% believe that "human beings have developed over millions of years from less advanced forms of life, but God had no part in this process."
Belief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, "Newsweek" reported: "By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'"
A 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.
According to a study published in "Science", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).
According to a 2011 Fox News poll, 45% of Americans believe in Creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by
Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).
In September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationist theory, believing that teaching children that creationism is the only true answer and without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.
Education controversies.
In the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to "Teach the Controversy" in science classes have conflated science with religion.
People for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with Creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:
In such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.
Criticism.
Christian criticism.
Many Christians disagree with the teaching of creationism. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an "endeavor designed to demonstrate that religion and science can be compatible."
In his 2002 article "Intelligent Design as a Theological Problem," George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking "of a God who acted openly and left his fingerprints on all the evidence."). Murphy argues that this view of God is incompatible with the Christian understanding of God as "the one revealed in the cross and resurrection of Christ." The basis of this theology is Isaiah 45:15, "Verily thou art a God that hidest thyself, O God of Israel, the Saviour."
Murphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require Divine action. On the contrary, for the crucifixion to occur, God had to limit or "empty" Himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:
Murphy concludes that,"Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation."For Murphy, a theology of the cross requires that Christians accept a "methodological" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a "metaphysical" naturalism, which proposes that nature is all that there is.
Teaching of creationism.
Other Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was "a kind of category mistake, as if the Bible were a theory like other theories." He also said: "My worry is creationism can end up reducing the doctrine of creation rather than enhancing it." The views of the Episcopal Church - a major American-based branch of the Anglican Communion - on teaching creationism resemble those of Williams.
In April 2010, the American Academy of Religion issued "Guidelines for Teaching About Religion in K‐12 Public Schools in the United States" which included guidance that creation science or intelligent design should not be taught in science classes, as "Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning." However, they, as well as other "worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others."
Randy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article "The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?" They conclude that "Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States."
Scientific criticism.
Science is a system of knowledge based on observation, empirical evidence and testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Some creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results. Some scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favour verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected.

</doc>
