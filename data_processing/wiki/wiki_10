<doc id="1845" url="http://en.wikipedia.org/wiki?curid=1845" title="Alternative medicine">
Alternative medicine

Alternative medicine is any practice that is presented as having the healing effects of medicine, but is not based on evidence gathered using the scientific method. It consists of a wide range of health care practices, products and therapies. Examples include new and traditional medicine practices such as homeopathy, herbal medicine, naturopathy, chiropractic, energy medicine, various forms of acupuncture, Traditional Chinese medicine, Ayurvedic medicine, and Christian faith healing. The treatments are those that are not part of the conventional, science-based healthcare system.
Complementary medicine is alternative medicine used together with conventional medical treatment in a belief, not proven by using scientific methods, that it "complements" the treatment. CAM is the abbreviation for Complementary and alternative medicine. Integrative medicine (or integrative health) is the combination of the practices and methods of alternative medicine with conventional medicine.
 and treatments are usually not included in the degree courses of medical schools, or used in conventional medicine, where treatments are based on what is proven using the scientific method. Alternative therapies lack such scientific validation, and their effectiveness is either unproved or disproved. Alternative medicine is usually based on religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, or fraud. Regulation and licensing of alternative medicine and health care providers varies from country to country, and state to state.
The scientific community has criticized alternative medicine as being based on misleading statements, quackery, pseudoscience, antiscience, fraud, or poor scientific methodology. Promoting alternative medicine has been called dangerous and unethical. Testing alternative medicine has been called a waste of scarce medical research resources. Critics have said "there is really no such thing as alternative medicine, just medicine that works and medicine that doesn't", or "Can there be any reasonable 'alternative' (to medicine based on science)?"
Types of alternative medicine.
Alternative medicine consists of a wide range of health care practices, products, and therapies. The shared feature is a claim to heal that is not based on the scientific method. Alternative medicine practices are diverse in their foundations and methodologies. Alternative medicine practices may be classified by their cultural origins or by the types of beliefs upon which they are based. Methods may incorporate or base themselves on traditional medicinal practices of a particular culture, folk knowledge, supersition, spiritual beliefs, belief in supernatural energies (antiscience), pseudoscience, errors in reasoning, propaganda, fraud, new or different concepts of health and disease, and any bases other than being proven by scientific methods. Different cultures may have their own unique traditional or belief based practices developed recently or over thousands of years, and specific practices or entire systems of practices.
Systems based on unscientific beliefs or traditional practices.
Alternative medical practices can be based on an underlying belief system inconsistent with science, or on traditional cultural practices.
Unscientific belief systems.
Alternative medical systems can be based on a common belief systems that are not consistent with facts of science, such as in Naturopathy or Homeopathy.
Homeopathy.
Homeopathy is a system developed in a belief that a substance that causes the symptoms of a disease in healthy people will cure similar symptoms in sick people. It was developed before knowledge of atoms and molecules, and of basic chemistry, which shows that repeated dilution as practiced in homeopathy produces only water and that homeopathy is false. Homeopathy is considered quackery in the medical community.
Naturopathic medicine.
Naturopathic medicine is based on a belief that the body heals itself using a supernatural vital energy that guides bodily processes, a view in conflict with the paradigm of evidence-based medicine. Many naturopaths have opposed vaccination, and "scientific evidence does not support claims that naturopathic medicine can cure cancer or any other disease".
Traditional ethnic systems.
Alternative medical systems may be based on traditional medicine practices, such as Traditional Chinese medicine, Ayurveda in India, or practices of other cultures around the world.
Traditional Chinese Medicine.
Traditional Chinese Medicine is a combination of traditional practices and beliefs developed over thousands of years in China, together with modifications made by the Communist party. Common practices include herbal medicine, acupuncture (insertion of needles in the body at specified points), massage (Tui na), exercise (qigong), and dietary therapy. The practices are based on belief in a supernatural energy called qi, considerations of Chinese Astrology and Chinese numerology, traditional use of herbs and other substances found in China, a belief that a map of the body is contained on the tongue which reflects changes in the body, and an incorrect model of the anatomy and physiology of internal organs.
The Chinese Communist Party Chairman Mao Zedong, in response to the lack of modern medical practitioners, revived acupuntcure and its theory was rewritten to adhere to the political, economic and logistic necessities of providing for the medical needs of China's population. In the 1950s the "history" and theory of Traditional Chinese Medicine was rewritten as communist propaganda, at Mao's insistence, to correct the supposed "bourgeois thought of Western doctors of medicine" (p.Â 109). Acupuncture gained attention in the United States when President Richard Nixon visited China in 1972, and the delegation was shown a patient undergoing major surgery while fully awake, ostensibly receiving acupuncture rather than anesthesia. Later it was found that the patients selected for the surgery had both a high pain tolerance and received heavy indoctrination before the operation; these demonstration cases were also frequently receiving morphine surreptitiously through an intravenous drip that observers were told contained only fluids and nutrients.
Ayurvedic medicine.
Ayurvedic medicine is a traditional medicine of India. It includes a belief that health can be influenced by use of traditional herbs, and by achieving the spiritual balance of the religions of Hinduism and Buddhism. Ayurveda stresses the use of plant-based medicines and treatments, with some animal products, and added minerals, including sulfur, arsenic, lead, copper sulfate. Andrew Weil, an American promoter of alternative medicine, wrote that in Ayurvedic medicine, "being 'healthy' is more than the absence of disease - it is a radiant state of vigor and energy, which is achieved by balance, or moderation, in food intake, sleep, sexual intercourse and other activities of daily life, complemented by various treatments including a wide variety of plant-based medicines".
Safety concerns have been raised about Ayurveda, with two U.S. studies finding about 20 percent of Ayurvedic Indian-manufactured patent medicines contained toxic levels of heavy metals such as lead, mercury and arsenic. Other concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities. Incidents of heavy metal poisoning have been attributed to the use of these compounds in the United States.
Supernatural energies and misunderstanding of energy in physics.
Bases of belief may include belief in existence of supernatural energies undetected by the science of physics, as in biofields, or in belief in properties of the energies of physics that are inconsistent with the laws of physics, as in energy medicine.
Biofields.
Biofield therapies are intended to influence energy fields that, it is purported, surround and penetrate the body. Writers such as noted astrophysicist and advocate of skeptical thinking (Scientific skepticism) Carl Sagan (1934-1996) have described the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.
Acupuncture is a component of Traditional Chinese Medicine. In acupuncture, it is believed that a supernatural energy called qi flows through the universe and through the body, and helps propel the blood, blockage of which leads to disease. It is believed that insertion of needles at various parts of the body determined by astrological calculations can restore balance to the blocked flows, and thereby cure disease.
Chiropractic was developed in the belief that manipulating the spine affects the flow of a supernatural vital energy and thereby affects health and disease.
In the western version of Japanese Reiki, the palms are placed on the patient near Chakras, believed to be centers of supernatural energies, in a belief that the supernatural energies can be transferred from the palms of the practitioner, to heal the patient.
Energy medicines.
Bioelectromagnetic-based therapies use verifiable electromagnetic fields, such as pulsed fields, alternating-current, or direct-current fields in an unconventional manner. Magnetic healing does not claim existence of supernatural energies, but asserts that magnets can be used to defy the laws of physics to influence health and disease.
Holistic health and mind body medicine.
Mind-body medicine takes a holistic approach to health that explores the interconnection between the mind, body, and spirit. It works under the premise that the mind can affect "bodily functions and symptoms". Mind body medicines includes healing claims made in yoga, meditation, deep-breathing exercises, guided imagery, hypnotherapy, progressive relaxation, qi gong, and tai chi.
Yoga, a method of traditional stretches, excercises, and meditations in Hinduism, may also be classified as an energy medicine insofar as its healing effects are believed by to be due to a healing "life energy" that is absorbed into the body through the breath, and is thereby believed to treat a wide variety of illnesses and complaints.
Since the 1990's, tai chi (t'ai chi ch'uan) classes that purely emphasise health have become popular in hospitals, clinics, as well as community and senior centers. This has occurred as the baby boomers generation has aged and the art's reputation as a low-stress training method for seniors has become better known. There has been some divergence between those that say they practice t'ai chi ch'uan primarily for self-defence, those that practice it for its aesthetic appeal (see "wushu" below), and those that are more interested in its benefits to physical and mental health.
Qigong, chi kung, or chi gung, is a practice of aligning body, breath, and mind for health, meditation, and martial arts training. With roots in Chinese Traditional Chinese Medicine, philosophy, and martial arts, qigong is traditionally viewed as a practice to cultivate and balance qi (chi) or what has been translated as "life energy".
Herbal remedies and other substances used.
Substance based practices use substances found in nature such as herbs, foods, non-vitamin supplments and megavitmins, and minerals, and includes traditional herbal remedies with herbs specific to regions in which the cultural practices arose.
Herbalism, herbology, or herbal medicine, is use of plants for medicinal purposes, and the study of such use. Plants have been the basis for medical treatments through much of human history, and such traditional medicine is still widely practiced today.
Nonvitamin supplements include fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil or pills, and ginseng, when used under a claim to have healing effects.
Although the practice of herbalism is not strictly based on evidence gathered using the scientific method, modern medicine, does, however, make use of many plant-derived compounds as the basis for evidence-tested pharmaceutical drugs, and phytotherapy works to apply modern standards of effectiveness testing to herbs and medicines that are derived from natural sources.
The scope of herbal medicine is sometimes extended to include fungal and bee products, as well as minerals, shells and certain animal parts. "Herbal" remedies in this case, may include use of nonherbal toxic chemicals from a nonbiological sources, such as use of the poison lead in Traditional Chinese Medicine.
Body manipulation.
Manipulative and body-based practices feature manipulation or movement of body parts, such as is done in bodywork and chiropractic manipulation.
Osteopathic manipulative medicine, also known as osteopathic manipulative treatment, is a core set of techniques of osteopathy and osteopathic medicine distinguishing these fields from mainstream medicine.
Religion, faith healing, and prayer.
Religion based healing practices, such as use of prayer and the laying of hands in Christian faith healing, and shamanism, rely on belief in divine or spiritual intervention for healing.
Shamanism is a practice of many cultures around the world, in which a practitioner reaches an altered states of consciousness in order to encounter and interact with the spirit world or channel supernatural energies in the belief they can heal.
Alternative medicines based on exploitation of ignorance and flawed reasoning.
Some alternative medicine practices may be based on pseudoscience, ignorance, or flawed reasoning. This can lead to fraud.
Homeopathy was developed prior to knowledge of the theory of molecules and basic chemistry, which proved that its "remedies" contained nothing more than distilled water.
Practitioners of electricity and magnetism based healing methods may deliberately exploit a patient's ignorance of physics in order to defraud them.
Definitions and terminology.
Terms and definitions.
The expression "complementary and alternative medicine" (CAM) resists easy definition because the health systems and practices to which it refers are diffuse and its boundaries are poorly defined.
Healthcare practices categorized as alternative may differ in their historical origin, theoretical basis, diagnostic technique, therapeutic practice and in their relationship to the medical mainstream. Some alternative therapies, including traditional Chinese Medicine (TCM) and Ayurveda, have antique origins in East or South Asia and are entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such as meditation and prayer, are based on mind-body interventions. Treatments considered alternative in one location may be considered conventional in another. Thus, chiropractic is not considered alternative in Denmark and likewise osteopathic medicine is no longer thought of as an alternative therapy in the United States.
One common feature of all definitions of alternative medicine is its designation as "other than" conventional medicine. For example, the widely referenced descriptive definition of complementary and alternative medicine devised by the US National Center for Complementary and Alternative Medicine (NCCAM) of the National Institutes of Health (NIH), states that it is "a group of diverse medical and health care systems, practices, and products that are not generally considered part of conventional medicine." This definition has been criticized as, if an alternative therapy, both effective and safe, is adopted by conventional medical practitioners, it does not necessarily follow that either it or its practitioners would no longer be considered alternative.
Some definitions seek to specify alternative medicine in terms of its social and political marginality to mainstream healthcare. This can refer to the lack of support that alternative therapies receive from the medical establishment and related bodies regarding access to research funding, sympathetic coverage in the medical press, or inclusion in the standard medical curriculum. In 1993, the British Medical Association (BMA), one among many professional organizations who have attempted to define alternative medicine, stated that it referred to "those forms of treatment which are not widely used by the conventional healthcare professions, and the skills of which are not taught as part of the undergraduate curriculum of conventional medical and paramedical healthcare courses". In a US context, an influential definition coined in 1993 by the Harvard-based physician, David M. Eisenberg, characterized alternative medicine "as interventions neither taught widely in medical schools nor generally available in US hospitals". These descriptive definitions are inadequate in the present-day when some conventional doctors offer alternative medical treatments and CAM introductory courses or modules can be offered as part of standard undergraduate medical training; alternative medicine is taught in more than 50 per cent of US medical schools and increasingly US health insurers are willing to provide reimbursement for CAM therapies. In 1999, 7.7% of US hospitals reported using some form of CAM therapy; this proportion had risen to 37.7% by 2008.
An expert panel at a conference hosted in 1995 by the US Office for Alternative Medicine (OAM), devised a theoretical definition of alternative medicine as "a broad domain of healing resourcesÂ ... other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period." This definition has been widely adopted by CAM researchers, cited by official government bodies such as the UK Department of Health, attributed as the definition used by the Cochrane Collaboration, and, with slight modification, was preferred in the 2005 consensus report of the US Institute of Medicine, "Complementary and Alternative Medicine in the United States".
The 1995 OAM conference definition, an expansion of Eisenberg's 1993 formulation, is silent regarding questions of the medical effectiveness of alternative therapies. Its proponents hold that it thus avoids relativism about differing forms of medical knowledge and, while it is an essentially political definition, this should not imply that the dominance of mainstream biomedicine is solely due to political forces. According to this definition, alternative and mainstream medicine can only be differentiated with reference to what is "intrinsic to the politically dominant health system of a particular society of culture". However, there is neither a reliable method to distinguish between cultures and subcultures, nor to attribute them as dominant or subordinate, nor any accepted criteria to determine the dominance of a cultural entity. If the culture of a politically dominant healthcare system is held to be equivalent to the perspectives of those charged with the medical management of leading healthcare institutions and programs, the definition fails to recognize the potential for division either within such an elite or between a healthcare elite and the wider population.
Normative definitions distinguish alternative medicine from the biomedical mainstream in its provision of therapies that are unproven, unvalidated or ineffective and support of theories which have no recognized scientific basis. These definitions characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but which are not based on evidence gathered with the scientific method. Exemplifying this perspective, a 1998 editorial co-authored by Marcia Angell, a former editor of the "New England Journal of Medicine", argued that:
This definition has been described by Robert L. Park as a logical Catch-22 which ensures that any CAM method which is proven to work "would no longer be CAM, it would simply be medicine."
Paul Offit, while criticizing alternative medicine, has provided a similar type of definition:
Joseph A. Schwarcz has stated: "There's a name for alternative medicines that work. It's called medicine." Tim Minchin states: "Alternative medicineÂ has either not been proved to work, or been proved not to work. You know what they call Âalternative medicineÂ that's been proved to work? Medicine."
"Complementary medicine" refers to use of alternative medicine alongside conventional science based medicine, in the belief that it "may help patients feel better and recover faster." In "Science and Technology: Public Attitudes and Public Understanding", chapter 7 of a report "Science and Engineering Indicators â 2002", issued by a US government agency (The National Science Foundation), it was stated that the term "alternative medicine" was there being used to refer to all treatments that had not been proven effective using scientific methods.
Regional definitions.
Public information websites maintained by the governments of the US and of the UK make a distinction between "alternative medicine" and "complementary medicine", but mention that these two overlap. The National Center for Complementary and Alternative Medicine (NCCAM) of the National Institutes of Health (NIH) (a part of the US Department of Health and Human Services) states that "alternative medicine" refers to using a non-mainstream approach in place of conventional medicine and that "complementary medicine" generally refers to using a non-mainstream approach together with conventional medicine, and comments that the boundaries between complementary and conventional medicine overlap and change with time.
The National Health Service (NHS) website "NHS Choices" (owned by the UK Department of Health), adopting the terminology of NCCAM, states that when a treatment is used alongside conventional treatments, to help a patient cope with a health condition, and not as an alternative to conventional treatment, this use of treatments can be called "complementary medicine"; but when a treatment is used instead of conventional medicine, with the intention of treating or curing a health condition, the use can be called "alternative medicine".
Similarly, the public information website maintained by the National Health and Medical Research Council (NHMRC) of the Commonwealth of Australia uses the acronym "CAM" for a wide range of health care practices, therapies, procedures and devices not within the domain of conventional medicine. In the Australian context this is stated to include acupuncture; aromatherapy; chiropractic; homeopathy; massage; meditation and relaxation therapies; naturopathy; osteopathy; reflexology, traditional Chinese medicine; and the use of vitamin supplements.
The Danish National Board of Health's "Council for Alternative Medicine" (Sundhedsstyrelsens RÃ¥d for Alternativ Behandling (SRAB)), an independent institution under the National Board of Health (Danish: "Sundhedsstyrelsen"), uses the term "alternative medicine" for:
Institutions.
In "General Guidelines for Methodologies on Research and Evaluation of Traditional Medicine", published in 2000 by the World Health Organization (WHO), complementary and alternative medicine were there defined as a broad set of health care practices that are not part of that country's own tradition and are not integrated into the dominant health care system.
Some herbal therapies are mainstream in Europe but are alternative in the US.
Special terminology used by selected individuals.
Two advocates of integrative medicine, writing in 2002 of the American healthcare system, claimed that it also addresses alleged problems with medicine based on science, which are not addressed by CAM; Ralph Snyderman and Andrew Weil stated that "integrative medicine is not synonymous with complementary and alternative medicine. It has a far larger meaning and mission in that it calls for restoration of the focus of medicine on health and healing and emphasizes the centrality of the patient-physician relationship." 
HistoryÂ â 19th century onwards.
"Further information: Rise of modern medicine"
Dating from the 1970s, medical professionals, sociologists, anthropologists and other commentators noted the increasing visibility of a wide variety of health practices that had neither derived directly from nor been verified by biomedical science. Since that time, those who have analyzed this trend have deliberated over the most apt language with which to describe this emergent health field. A variety of terms have been used, including heterodox, irregular, fringe and alternative medicine while others, particularly medical commentators, have been satisfied to label them as instances of quackery. The most persistent term has been alternative medicine but its use is problematic as it assumes a value-laden dichotomy between a medical fringe, implicitly of borderline acceptability at best, and a privileged medical orthodoxy, associated with validated medico-scientific norms. The use of the category of alternative medicine has also been criticized as it cannot be studied as an independent entity but must be understood in terms of a regionally and temporally specific medical orthodoxy. Its use can also be misleading as it may erroneously imply that a real medical alternative exists. As with near-synonymous expressions, such as unorthodox, complementary, marginal, or quackery, these linguistic devices have served, in the context of processes of professionalisation and market competition, to establish the authority of official medicine and police the boundary between it and its unconventional rivals.
An early instance of the influence of this modern, or western, scientific medicine outside Europe and North America is Peking Union Medical College.
From a historical perspective, the emergence of alternative medicine, if not the term itself, is typically dated to the 19th century. This is despite the fact that there are variants of Western non-conventional medicine that arose in the late-eighteenth century or earlier and some non-Western medical traditions, currently considered alternative in the West and elsewhere, which boast extended historical pedigrees. Alternative medical systems, however, can only be said to exist when there is an identifiable, regularized and authoritative standard medical practice, such as arose in the West during the nineteenth-century, to which they can function as an alternative.
During the late eighteenth and nineteenth centuries regular and irregular medical practitioners became more clearly differentiated throughout much of Europe and, as the nineteenth century progressed, most Western states converged in the creation of legally delimited and semi-protected medical markets. It is at this point that an "official" medicine, created in cooperation with the state and employing a scientific rhetoric of legitimacy, emerges as a recognizable entity and that the concept of alternative medicine as a historical category becomes tenable.
As part of this process, professional adherents of mainstream medicine in countries such as Germany, France, and Britain increasingly invoked the scientific basis of their discipline as a means of engendering internal professional unity and of external differentiation in the face of sustained market competition from homeopaths, naturopaths, mesmerists and other nonconventional medical practitioners, finally achieving a degree of imperfect dominance through alliance with the state and the passage of regulatory legislation. In the US the Johns Hopkins University School of Medicine, based in Baltimore, Maryland, opened in 1893,with William H. Welch and William Osler among the founding physicians, and was the first medical school devoted to teaching "German scientific medicine".
Buttressed by the increased authority consequent to the significant advances in the medical sciences of the late 19th century onwardsâincluding the development and application of the germ theory of disease by the chemist Louis Pasteur and the surgeon Joseph Lister, 1st Baron Lister, of microbiology co-founded by Robert Koch (in 1885 appointed professor of hygiene at the University of Berlin), and of the use of X-rays (RÃ¶ntgen rays)âthe 1910 Flexner Report called upon American medical schools to follow the model set by the Johns Hopkins School of Medicine and adhere to mainstream science in their teaching and research. This was in a belief, mentioned in the Report's introduction, that the preliminary and professional training then prevailing in medical schools should be reformed in view of the new means for diagnosing and combating disease being made available to physicians and surgeons by the sciences on which medicine depended.
Among putative medical practices available at the time which later became known as "alternative medicine" were homeopathy (founded in Germany in the early 19c.) and chiropractic (founded in North America in the late 19c.). These conflicted in principle with the developments in medical science upon which the Flexner reforms were based, and they have not become compatible with further advances of medical science such as listed in Timeline of medicine and medical technology, 1900â1999 and 2000âpresent, nor have Ayurveda, acupuncture or other kinds of alternative medicine.
At the same time "Tropical medicine" was being developed as a specialist branch of western medicine in research establishments such as Liverpool School of Tropical Medicine founded in 1898 by Alfred Lewis Jones, London School of Hygiene & Tropical Medicine, founded in 1899 by Patrick Manson and Tulane University School of Public Health and Tropical Medicine, instituted in 1912. A distinction was being made between western scientific medicine and indigenous systems. An example is given by an official report about indigenous systems of medicine in India, including Ayurveda, submitted by Mohammad Usman of Madras and others in 1923. This stated that the first question the Committee considered was "to decide whether the indigenous systems of medicine were scientific or not".
By the later twentieth century the term 'alternative medicine' had come into use for the purposes of public discussion, but it was not always being used with the same meaning by all parties. Arnold S. Relman remarked in 1998 that in the best kind of medical practice, all proposed treatments must be tested objectively, and that in the end there will only be treatments that pass and those that do not, those that are proven worthwhile and those that are not. He asked 'Can there be any reasonable "alternative"?' But also in 1998 the then Surgeon General of the United States, David Satcher, issued public information about eight common alternative treatments (including acupuncture, holistic and massage), together with information about common diseases and conditions, on nutrition, diet, and lifestyle changes, and about helping consumers to decipher fraud and quackery, and to find healthcare centers and doctors who practiced alternative medicine.
By 1990, approximately 60 million Americans had used one or more complementary or alternative therapies to address health issues, according to a nationwide survey in the US published in 1993 by David Eisenberg. A study published in the November 11, 1998 issue of the Journal of the American Medical Association reported that 42% of Americans had used complementary and alternative therapies, up from 34% in 1990. However, despite the growth in patient demand for complementary medicine, most of the early alternative/complementary medical centers failed.
Medical education since 1910.
Mainly as a result of reforms following the Flexner Report of 1910 medical education in established medical schools in the US has generally not included alternative medicine as a teaching topic. Typically, their teaching is based on current practice and scientific knowledge about: anatomy, physiology, histology, embryology, neuroanatomy, pathology, pharmacology, microbiology and immunology. Medical schools' teaching includes such topics as doctor-patient communication, ethics, the art of medicine, and engaging in complex clinical reasoning (medical decision-making). Writing in 2002, Snyderman and Weil remarked that by the early twentieth century the Flexner model had helped to create the 20th-century academic health center in which education, research and practice were inseparable. While this had much improved medical practice by defining with increasing certainty the pathophysiological basis of disease, a single-minded focus on the pathophysiological had diverted much of mainstream American medicine from clinical conditions which were not well understood in mechanistic terms and were not effectively treated by conventional therapies.
By 2001 some form of CAM training was being offered by at least 75 out of 125 medical schools in the US. Exceptionally, the School of Medicine of the University of Maryland, Baltimore includes a research institute for integrative medicine (a member entity of the Cochrane Collaboration). Medical schools are responsible for conferring medical degrees, but a physician typically may not legally practice medicine until licensed by the local government authority. Licensed physicians in the US who have attended one of the established medical schools there have usually graduated Doctor of Medicine (MD). All states require that applicants for MD licensure be graduates of an approved medical school and complete the United States Medical Licensing Exam (USMLE).
The British Medical Association, in its publication "Complementary Medicine, New Approach to Good Practice" (1993), gave as a working definition of non-conventional therapies (including acupuncture, chiropractic and homeopathy): "those forms of treatment which are not widely used by the orthodox health-care professions, and the skills of which are not part of the undergraduate curriculum of orthodox medical and paramedical health-care courses". By 2000 some medical schools in the UK were offering CAM familiarisation courses to undergraduate medical students while some were also offering modules specifically on CAM.
Proponents and opponents.
The Cochrane Collaboration Complementary Medicine Field explains its "Scope and Topics" by giving a broad and general definition for complementary medicine as including practices and ideas which are outside the domain of conventional medicine in several countries and defined by its users as preventing or treating illness, or promoting health and well being, and which complement mainstream medicine in three ways: by contributing to a common whole, by satisfying a demand not met by conventional practices, and by diversifying the conceptual framework of medicine.
Proponents of an evidence-base for medicine such as the Cochrane Collaboration (founded in 1993 and from 2011 providing input for WHO resolutions) take a position that "all" systematic reviews of treatments, whether "mainstream" or "alternative", ought to be held to the current standards of scientific method. In a study titled "Development and classification of an operational definition of complementary and alternative medicine for the Cochrane Collaboration" (2011) it was proposed that indicators that a therapy is accepted include government licensing of practitioners, coverage by health insurance, statements of approval by government agencies, and recommendation as part of a practice guideline; and that if something is currently a standard, accepted therapy, then it is not likely to be widely considered as CAM.
That alternative medicine has been on the rise "in countries where Western science and scientific method generally are accepted as the major foundations for healthcare, and 'evidence-based' practice is the dominant paradigm" was described as an "enigma" in the Medical Journal of Australia.
Critics in the US say the expression is deceptive because it implies there is an effective alternative to science-based medicine, and that "complementary" is deceptive because the word implies that the treatment increases the effectiveness of (complements) science-based medicine, while alternative medicines which have been tested nearly always have no measurable positive effect compared to a placebo.
Some opponents, focused upon health fraud, misinformation, and quackery as public health problems in the US, are highly critical of alternative medicine, notably Wallace Sampson and Paul Kurtz founders of Scientific Review of Alternative Medicine and Stephen Barrett, co-founder of The National Council Against Health Fraud and webmaster of Quackwatch. Grounds for opposing alternative medicine which have been stated in the US and elsewhere are:
Paul Offit has proposed four ways in which "alternative medicine becomes quackery":
The NCCAM classification system.
A United States government agency, the National Center on Complementary and Alternative Medicine (NCCAM), has created its own classification system for branches of complementary and alternative medicine. It classifies complementary and alternative therapies into five major groups, which have some overlap and two types of energy medicine are distinguished: one, "Veritable" involving scientifically observable energy, including magnet therapy, colorpuncture and light therapy; the other "Putative" which invoke physically undetectable or unverifiable energy.
Alternative medicine practices and beliefs are diverse in their foundations and methodologies. The wide range of treatments and practices referred to as alternative medicine includes some stemming from nineteenth century North America, such as chiropractic and naturopathy, others, mentioned by JÃ¼tte, that originated in eighteenth- and nineteenth-century Germany, such as homeopathy and hydropathy, and some that have originated in China or India, while African, Caribbean, Pacific Island, Native American, and other regional cultures have traditional medical systems as diverse as their diversity of cultures.
Examples of CAM as a broader term for unorthodox treatment and diagnosis of illnesses, disease, infections, etc., include yoga, acupuncture, aromatherapy, chiropractic, herbalism, homeopathy, hypnotherapy, massage, osteopathy, reflexology, relaxation therapies, spiritual healing and tai chi. CAM differs from conventional medicine. It is normally private medicine and not covered by health insurance. It is paid out of pocket by the patient and is an expensive treatment. CAM tends to be a treatment for upper class or more educated people.
The NCCAM classification system is - 
Examples classified under the NCCAM system.
Alternative therapies based on electricity or magnetism use verifiable electromagnetic fields, such as pulsed fields, alternating-current, or direct-current fields in an unconventional manner rather than claiming the existence of imponderable or supernatural energies.
Substance based practices use substances found in nature such as herbs, foods, non-vitamin supplements and megavitamins, and minerals, and includes traditional herbal remedies with herbs specific to regions in which the cultural practices arose. Nonvitamin supplements include fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil or pills, and ginseng, when used under a claim to have healing effects.
Mind-body interventions, working under the premise that the mind can affect "bodily functions and symptoms", include healing claims made in hypnotherapy, and in guided imagery, meditation, progressive relaxation, qi gong, tai chi and yoga. Meditation practices including mantra meditation, mindfulness meditation, yoga, tai chi, and qi gong have many uncertainties. According to an AHRQ review, the available evidence on meditation practices through September 2005 is of poor methodological quality and definite conclusions on the effects of meditation in healthcare cannot be made using existing research.
Naturopathy is based on a belief in vitalism, which posits that a special energy called vital energy or vital force guides bodily processes such as metabolism, reproduction, growth, and adaptation. The term was coined in 1895 by John Scheel and popularized by Benedict Lust, the "father of U.S. naturopathy". Today, naturopathy is primarily practiced in the United States and Canada. Naturopaths in unregulated jurisdictions may use the Naturopathic Doctor designation or other titles regardless of level of education.
Traditional Chinese medicine is based on a concept of vital energy, or Qi, flowing in the body along specific pathways. These purported pathways consist of 12 primary meridians. TCM has many branches including, acupuncture, massage, feng shui, herbs, as well as Chinese astrology. TCM diagnosis is primarily based on looking at the tongue, which is claimed to show the condition of the organs, as well as feeling the pulse of the radial artery, which is also claimed to show the condition of the organs.
Criticism.
Use of the terms "Complementary and alternative medicine (CAM)" and "alternative medicine" have been criticized.
CAM is not as well researched as conventional medicine which undergoes intense research before being released to the public. Funding for research is also sparse making it difficult to do further research for effectiveness of CAM. Most funding for CAM is funded by government agencies. Proposed research for CAM are rejected by most private funding agencies because the results of research are not reliable. The research for CAM has to meet certain standards from research ethics committees which most CAM researchers find almost impossible to meet. Because the results of CAM are not quantifiable, it is hard to prove its effectiveness and it appears to work in a more holistic sense. CAM is thought to help the patient in a mental or psychological sense since the research for CAM is hit and miss. Even with the little research done on it, CAM has not been proven to be effective. This creates an issue of whether the patient is receiving all the information about the treatment that is necessary for the patient to be well informed.
CAM is not as well regulated as conventional medicine. There are ethical concerns about whether people who perform CAM have the proper knowledge to perform the treatments they give to patients. CAM is often done by non-physicians and does not operate with the same medical licensing laws as conventional medicine. It is an issue of non-maleficence.
In USA.
A 2002 report on public attitudes and understanding issued by the US National Science Foundation defines the term "alternative medicine" as treatments that had not been proven effective using scientific methods, and described them as giving more weight to ancient traditions and anecdotes over biological science and clinical trials.
Criticisms have come from individuals such as Wallace Sampson in an article in Annals of the New York Academy of Sciences, June 1995. Sampson argued that proponents of alternative medicine often used terminology which was loose or ambiguous to create the appearance that a choice between "alternative" effective treatments existed when it did not, or that there was effectiveness or scientific validity when it did not exist, or to suggest that a dichotomy existed when it did not, or to suggest that consistency with science existed when it might not; that the term "alternative" was to suggest that a patient had a choice between effective treatments when there was not; that use of the word "conventional" or "mainstream" was to suggest that the difference between alternative medicine and science based medicine was the prevalence of use, rather than lack of a scientific basis of alternative medicine as compared to "conventional" or "mainstream" science based medicine; that use of the term "complementary" or "integrative" was to suggest that purported supernatural energies of alternative medicine could complement or be integrated into science based medicine. "Integrative medicine" or "integrated medicine" is used to refer to the belief that medicine based on science would be improved by "integration" with alternative medical treatments practices that are not, and is substantially similar in use to the term "complementary and alternative medicine".
Sampson has also written that CAM is the "propagation of the absurd", and argues that "alternative" and "complementary" have been substituted for "quackery", "dubious", and "implausible".
Another critic, with reference to government funding studies of integrating alternative medicine techniques into the mainstream, Steven Novella, a neurologist at Yale School of Medicine, wrote that it "is used to lend an appearance of legitimacy to treatments that are not legitimate." Another, Marcia Angell, argued that it was "a new name for snake oil." Angell considered that critics felt that healthcare practices should be classified based solely on scientific evidence, and if a treatment had been rigorously tested and found safe and effective, science based medicine will adopt it regardless of whether it was considered "alternative" to begin with. It was thus possible for a method to change categories (proven vs. unproven), based on increased knowledge of its effectiveness or lack thereof. Prominent supporters of this position include George D. Lundberg, former editor of the Journal of the American Medical Association (JAMA).
In an article first published in "CA: A Cancer Journal for Clinicians" in 1999, "Evaluating complementary and alternative therapies for cancer patients.", Barrie R. Cassileth mentioned that a 1997 letter to the US Senate Subcommittee on Public Health and Safety, which had deplored the lack of critical thinking and scientific rigor in OAM-supported research, had been signed by four Nobel Laureates and other prominent scientists. (This was supported by the National Institutes of Health (NIH).)
In March 2009 a "Washington Post" staff writer reported that the impending national discussion about broadening access to health care, improving medical practice and saving money was giving a group of scientists an opening to propose shutting down the National Center for Complementary and Alternative Medicine, quoting one of them, Steven Salzberg, a genome researcher and computational biologist at the University of Maryland, saying "One of our concerns is that NIH is funding pseudoscience." They argued that the vast majority of studies were based on fundamental misunderstandings of physiology and disease, and have shown little or no effect.
Stephen Barrett, founder and operator of Quackwatch, has argued that practices labeled "alternative" should be reclassified as either genuine, experimental, or questionable. Here he defines genuine as being methods that have sound evidence for safety and effectiveness, experimental as being unproven but with a plausible rationale for effectiveness, and questionable as groundless without a scientifically plausible rationale.
Sampson has also pointed out that CAM tolerated contradiction without thorough reason and experiment. Barrett has pointed out that there is a policy at the NIH of never saying something doesn't work only that a different version or dose might give different results. Barrett also expressed concern that, just because some "alternatives" have merit, there is the impression that the rest deserve equal consideration and respect even though most are worthless, since they are all classified under the one heading of alternative medicine.
Writers such as Carl Sagan (1934-1996), a noted astrophysicist, advocate of skeptical thinking (Scientific skepticism) and the author of "The DemonâHaunted World: Science as a Candle in the Dark" (1996), have described the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.
According to two writers, Wallace Sampson and K. Butler, marketing is part of the medical training required in chiropractic education, and propaganda methods in alternative medicine have been traced back to those used by Hitler and Goebels in their promotion of pseudoscience in medicine.
The NCCAM budget has been criticized because, despite the duration and intensity of studies to measure the efficacy of alternative medicine, there had been no effective CAM treatments supported by scientific evidence as of 2002, according to the QuackWatch website; the NCCAM budget has been on a sharp and sustained rise. Critics of the Center argue that the plausibility of interventions such as botanical remedies, diet, relaxation therapies and yoga should not be used to support research on implausible interventions based on superstition and belief in the supernatural, and that the plausible methods can be studied just as well in other parts of NIH, where they should be made to compete on an equal footing with other research projects.
In UK.
Richard Dawkins, an English evolutionary biologist and author, in an essay in his book "A Devil's Chaplain" (2003) (chapter 4.4), has defined alternative medicine as a "set of practices that cannot be tested, refuse to be tested, or consistently fail tests." Another essay in the same book (chapter 1.4) quoted from an article by John Diamond in "The Independent": "There is really no such thing as alternative medicine, just medicine that works and medicine that doesn't." 
Dawkins has argued that, if a technique is demonstrated effective in properly performed trials, it ceases to be alternative and simply becomes medicine.
As it relates to ethics, in November 2011 Edzard Ernst stated that the "level of misinformation about alternative medicine has now reached the point where it has become dangerous and unethical. So far, alternative medicine has remained an ethics-free zone. It is time to change this." Ernst requested that Prince Charles recall two guides to alternative medicine published by the Foundation for Integrated Health, on the grounds that "[t]hey both contain numerous misleading and inaccurate claims concerning the supposed benefits of alternative medicine" and that "[t]he nation cannot be served by promoting ineffective and sometimes dangerous alternative treatments." In general, he believes that CAM can and should be subjected to scientific testing.
Placebo effect.
A research methods expert and author of "Snake Oil Science", R. Barker Bausell, has stated that "it's become politically correct to investigate nonsense." There are concerns that just having NIH support is being used to give unfounded "legitimacy to treatments that are not legitimate."
Use of placebos in order to achieve a placebo effect in integrative medicine has been criticized as "diverting research time, money, and other resources from more fruitful lines of investigation in order to pursue a theory that has no basis in biology".
Another critic has argued that academic proponents of integrative medicine sometimes recommend misleading patients by using known placebo treatments in order to achieve a placebo effect. However, a 2010 survey of family physicians found that 56% of respondents said they had used a placebo in clinical practice as well. Eighty-five percent of respondents believed placebos can have both psychological and physical benefits.
Integrative medicine has been criticized in that its practitioners, trained in science based medicine, deliberately mislead patients by pretending placebos are not. "Quackademic medicine" is a pejorative term used for "integrative medicine", which is considered to be an infiltration of quackery into academic science-based medicine.
An analysis of trends in the criticism of complementary and alternative medicine (CAM) in five prestigious American medical journals during the period of reorganization within medicine (1965â1999) was reported as showing that the medical profession had responded to the growth of CAM in three phases, and that in each phase there had been changes in the medical marketplace which influenced the type of response in the journals. Changes included relaxed medical licensing, the development of managed care, rising consumerism, and the establishment of the USA Office of Alternative Medicine (now National Center for Complementary and Alternative Medicine). In the "condemnation" phase, from the late 1960s to the early 1970s, authors had ridiculed, exaggerated the risks, and petitioned the state to contain CAM; in the "reassessment" phase (mid-1970s through early 1990s), when increased consumer utilization of CAM was prompting concern, authors had pondered whether patient dissatisfaction and shortcomings in conventional care contributed to the trend; in the "integration" phase of the 1990s physicians began learning to work around or administer CAM, and the subjugation of CAM to scientific scrutiny had become the primary means of control.
Use and regulation.
Prevalence of use.
Complementary and alternative medicine (CAM) has been described as a broad domain of healing resources that encompasses all health systems, modalities, and practices and their accompanying theories and beliefs, other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period. CAM includes all such practices and ideas self-defined by their users as preventing or treating illness or promoting health and well-being. Boundaries within CAM and between the CAM domain and that of the dominant system are not always sharp or fixed.
About 50% of people in developed countries use some kind of complementary and alternative medicine other than prayer for health. A British telephone survey by the BBC of 1209 adults in 1998 shows that around 20% of adults in Britain had used alternative medicine in the past 12 months. About 40% of cancer patients use some form of CAM.
In developing nations, access to essential medicines is severely restricted by lack of resources and poverty. Traditional remedies, often closely resembling or forming the basis for alternative remedies, may comprise primary healthcare or be integrated into the healthcare system. In Africa, traditional medicine is used for 80% of primary healthcare, and in developing nations as a whole over one-third of the population lack access to essential medicines.
In USA.
In the United States, the 1974 Child Abuse Prevention and Treatment Act (CAPTA) required states to grant religious exemptions to child neglect and abuse laws, regarding religion-based healing practices, in order to receive federal money. Thirty-one states have child-abuse religious exemptions.
In respect of taxation in the USA, the Internal Revenue Service has discriminated in favour of medical expenses for acupuncture and chiropractic (and others including Christian Science practitioners) but against homeopathy and the use of non-prescription required medicine.
The use of alternative medicine in the US has increased, with a 50 percent increase in expenditures and a 25 percent increase in the use of alternative therapies between 1990 and 1997 in America. Americans spend many billions on the therapies annually. Most Americans used CAM to treat and/or prevent musculoskeletal conditions or other conditions associated with chronic or recurring pain. In America, women were more likely than men to use CAM, with the biggest difference in use of mind-body therapies including prayer specifically for health reasons". In 2008, more than 37% of American hospitals offered alternative therapies, up from 26.5 percent in 2005, and 25% in 2004. More than 70% of the hospitals offering CAM were in urban areas.
A survey of Americans found that 88 percent agreed that "there are some good ways of treating sickness that medical science does not recognize". Use of magnets was the most common tool in energy medicine in America, and among users of it, 58 percent described it as at least "sort of scientific", when it is not at all scientific. In 2002, at least 60 percent of US medical schools have at least some class time spent teaching alternative therapies. "Therapeutic touch", was taught at more than 100 colleges and universities in 75 countries before the practice was debunked by a nine-year-old child for a school science project.
A 1997 survey found that 13.7% of respondents in the US had sought the services of both a medical doctor and an alternative medicine practitioner. The same survey found that 96% of respondents who sought the services of an alternative medicine practitioner also sought the services of a medical doctor in the past 12 months. Medical doctors are often unaware of their patient's use of alternative medical treatments as only 38.5% of the patients alternative therapies were discussed with their medical doctor.
According to Michael H. Cohen, US regulation of alternative includes state licensure of healthcare providers and scope of practice limits on practice by non-MD healthcare professionals; state-law malpractice rules (standard of care limits on professional negligence); discipline of practitioners by state regulatory boards; and federal regulation such as food and drug law. He argues that US regulation of alternative medicine "seeks to integrate biomedical, holistic, and social models of health care in ways that maximize patientsâ well-being [w]hile still protecting patients from fraud."
Prevalence of use of specific therapies.
The most common CAM therapies used in the US in 2002 were prayer (45.2%), herbalism (18.9%), breathing meditation (11.6%), meditation (7.6%), chiropractic medicine (7.5%), yoga (5.1%-6.1%), body work (5.0%), diet-based therapy (3.5%), progressive relaxation (3.0%), mega-vitamin therapy (2.8%) and Visualization (2.1%)
In Britain, the most often used alternative therapies were Alexander technique, Aromatherapy, Bach and other flower remedies, Body work therapies including massage, Counseling stress therapies, hypnotherapy, Meditation, Reflexology, Shiatsu, Ayurvedic medicine, Nutritional medicine, and Yoga. Ayurvedic medicine remedies are mainly plant based with some use of animal materials. Safety concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities.
According to the National Health Service (England), the most commonly used complementary and alternative medicines (CAM) supported by the NHS in the UK are: acupuncture, aromatherapy, chiropractic, homeopathy, massage, osteopathy and clinical hypnotherapy.
"Complementary medicine treatments used for pain include: acupuncture, low-level laser therapy, meditation, aroma therapy, Chinese medicine, dance therapy, music therapy, massage, herbalism, therapeutic touch, yoga, osteopathy, chiropractic, naturopathy, and homeopathy."
In palliative care.
Complementary therapies are often used in palliative care or by practitioners attempting to manage chronic pain in patients. Integrative medicine is considered more acceptable in the interdisciplinary approach used in palliative care than in other areas of medicine. "From its early experiences of care for the dying, palliative care took for granted the necessity of placing patient values and lifestyle habits at the core of any design and delivery of quality care at the end of life. If the patient desired complementary therapies, and as long as such treatments provided additional support and did not endanger the patient, they were considered acceptable." The non-pharmacologic interventions of complementary medicine can employ mind-body interventions designed to "reduce pain and concomitant mood disturbance and increase quality of life."
Regulation.
In Austria and Germany complementary and alternative medicine is mainly in the hands of doctors with MDs, and half or more of the American alternative practitioners are licensed MDs. In Germany herbs are tightly regulated: half are prescribed by doctors and covered by health insurance.
Some professions of complementary/traditional/alternative medicine, such as chiropractic, have achieved full regulation in North America and other parts of the world and are regulated in a manner similar to that governing science-based medicine. In contrast, other approaches may be partially recognized and others have no regulation at all. Regulation and licensing of alternative medicine ranges widely from country to country, and state to state.
Government bodies in the USA and elsewhere have published information or guidance about alternative medicine. One of those is the U.S. Food and Drug Administration (FDA), which mentions specifically homeopathic products, traditional Chinese medicine and Ayurvedic products. A document which the FDA has issued for comment is headed "Guidance for Industry: Complementary and Alternative Medicine Products and Their Regulation by the Food and Drug Administration", last updated on March 2, 2007. The document opens with three preliminary paragraphs which explain that "in the document":
The FDA has also issued online warnings for consumers about medication health fraud. This includes a section on Alternative Medicine Fraud, such as a warning that Ayurvedic products generally have not been approved by the FDA before marketing.
Efficacy.
Alternative therapies lack the requisite scientific validation, and their effectiveness is either unproved or disproved. Many of the claims regarding the efficacy of alternative medicines are controversial, since research on them is frequently of low quality and methodologically flawed. Selective publication of results (misleading results from only publishing positive results, and not all results), marked differences in product quality and standardisation, and some companies making unsubstantiated claims, call into question the claims of efficacy of isolated examples where herbs may have some evidence of containing chemicals that may affect health. "The Scientific Review of Alternative Medicine" points to confusions in the general population - a person may attribute symptomatic relief to an otherwise-ineffective therapy just because they are taking something (the placebo effect); the natural recovery from or the cyclical nature of an illness (the regression fallacy) gets misattributed to an alternative medicine being taken; a person not diagnosed with science based medicine may never originally have had a true illness diagnosed as an alternative disease category.
Edzard Ernst characterized the evidence for many alternative techniques as weak, nonexistent, or negative and in 2011 published his estimate that about 7.4% were based on "sound evidence", although he believes that may be an overestimate due to various reasons. Ernst has concluded that 95% of the alternative treatments he and his team studied, including acupuncture, herbal medicine, homeopathy, and reflexology, are "statistically indistinguishable from placebo treatments", but he also believes there is something that conventional doctors can usefully learn from the chiropractors and homeopath: this is the therapeutic value of the placebo effect, one of the strangest phenomena in medicine.
In 2003, a project funded by the CDC identified 208 condition-treatment pairs, of which 58% had been studied by at least one randomized controlled trial (RCT), and 23% had been assessed with a meta-analysis. According to a 2005 book by a US Institute of Medicine panel, the number of RCTs focused on CAM has risen dramatically. The book cites Vickers (1998), who found that many of the CAM-related RCTs are in the Cochrane register, but 19% of these trials were not in MEDLINE, and 84% were in conventional medical journals.
As of 2005, the Cochrane Library had 145 CAM-related Cochrane systematic reviews and 340 non-Cochrane systematic reviews. An analysis of the conclusions of only the 145 Cochrane reviews was done by two readers. In 83% of the cases, the readers agreed. In the 17% in which they disagreed, a third reader agreed with one of the initial readers to set a rating. These studies found that, for CAM, 38.4% concluded positive effect or possibly positive (12.4%), 4.8% concluded no effect, 0.69% concluded harmful effect, and 56.6% concluded insufficient evidence. An assessment of conventional treatments found that 41.3% concluded positive or possibly positive effect, 20% concluded no effect, 8.1% concluded net harmful effects, and 21.3% concluded insufficient evidence. However, the CAM review used the more developed 2004 Cochrane database, while the conventional review used the initial 1998 Cochrane database.
Most alternative medical treatments are not patentable, which may lead to less research funding from the private sector. In addition, in most countries, alternative treatments (in contrast to pharmaceuticals) can be marketed without any proof of efficacyâalso a disincentive for manufacturers to fund scientific research. Some have proposed adopting a prize system to reward medical research. However, public funding for research exists. Increasing the funding for research on alternative medicine techniques is the purpose of the US National Center for Complementary and Alternative Medicine. NCCAM and its predecessor, the Office of Alternative Medicine, have spent more than $2.5 billion on such research since 1992; this research has largely not demonstrated the efficacy of alternative treatments.
In the same way as for conventional therapies, drugs, and interventions, it can be difficult to test the efficacy of alternative medicine in clinical trials. In instances where an established, effective, treatment for a condition is already available, the Helsinki Declaration states that withholding such treatment is unethical in most circumstances. Use of standard-of-care treatment in addition to an alternative technique being tested may produce confounded or difficult-to-interpret results.
Cancer researcher Andrew J. Vickers has stated:
Homeopathy is based on the belief that a disease can be cured by a very low dose of substance that creates similar symptoms in a healthy person. This conflicts with fundamental concepts of physics and chemistry and there is no good evidence from reviews of research to support its use.
Safety.
Adequacy of regulation and CAM safety.
Many of the claims regarding the safety and efficacy of alternative medicine are controversial. Some alternative treatments have been associated with unexpected side effects, which can be fatal.
One of the commonly voiced concerns about complementary alternative medicine (CAM) is the manner in which is regulated. There have been significant developments in how CAMs should be assessed prior to re-sale in the United Kingdom and the European Union (EU) in the last 2 years. Despite this, it has been suggested that current regulatory bodies have been ineffective in preventing deception of patients as many companies have re-labelled their drugs to avoid the new laws. There is no general consensus about how to balance consumer protection (from false claims, toxicity, and advertising) with freedom to choose remedies.
Advocates of CAM suggest that regulation of the industry will adversely affect patients looking for alternative ways to manage their symptoms, even if many of the benefits may represent the placebo affect. Some contend that alternative medicines should not require any more regulation than over-the-counter medicines that can also be toxic in overdose (such as paracetamol).
Interactions with conventional pharmaceuticals.
Forms of alternative medicine that are biologically active can be dangerous even when used in conjunction with conventional medicine. Examples include immuno-augmentation therapy, shark cartilage, bioresonance therapy, oxygen and ozone therapies, insulin potentiation therapy. Some herbal remedies can cause dangerous interactions with chemotherapy drugs, radiation therapy, or anesthetics during surgery, among other problems. An anecdotal example of these dangers was reported by Associate Professor Alastair MacLennan of Adelaide University, Australia regarding a patient who almost bled to death on the operating table after neglecting to mention that she had been taking "natural" potions to "build up her strength" before the operation, including a powerful anticoagulant that nearly caused her death.
To "ABC Online", MacLennan also gives another possible mechanism:
Potential side-effects.
Conventional treatments are subjected to testing for undesired side-effects, whereas alternative treatments, in general, are not subjected to such testing at all. Any treatment â whether conventional or alternative â that has a biological or psychological effect on a patient may also have potential to possess dangerous biological or psychological side-effects. Attempts to refute this fact with regard to alternative treatments sometimes use the "appeal to nature" fallacy, i.e., "that which is natural cannot be harmful".
An exception to the normal thinking regarding side-effects is Homeopathy. Since 1938, the U.S. Food and Drug Administration (FDA) has regulated homeopathic products in "several significantly different ways from other drugs." Homeopathic preparations, termed "remedies", are extremely dilute, often far beyond the point where a single molecule of the original active (and possibly toxic) ingredient is likely to remain. They are, thus, considered safe on that count, but "their products are exempt from good manufacturing practice requirements related to expiration dating and from finished product testing for identity and strength", and their alcohol concentration may be much higher than allowed in conventional drugs.
Treatment delay.
Those having experienced or perceived success with one alternative therapy for a minor ailment may be convinced of its efficacy and persuaded to extrapolate that success to some other alternative therapy for a more serious, possibly life-threatening illness. For this reason, critics argue that therapies that rely on the placebo effect to define success are very dangerous. According to mental health journalist Scott Lilienfeld in 2002, "unvalidated or scientifically unsupported mental health practices can lead individuals to forgo effective treatments" and refers to this as "opportunity cost". Individuals who spend large amounts of time and money on ineffective treatments may be left with precious little of either, and may forfeit the opportunity to obtain treatments that could be more helpful. In short, even innocuous treatments can indirectly produce negative outcomes.
Between 2001 and 2003, four children died in Australia because their parents chose ineffective naturopathic, homeopathic, or other alternative medicines and diets rather than conventional therapies.
Unconventional cancer "cures".
There have always been "many therapies offered outside of conventional cancer treatment centers and based on theories not found in biomedicine. These alternative cancer cures have often been described as 'unproven,' suggesting that appropriate clinical trials have not been conducted and that the therapeutic value of the treatment is unknown." However, "many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective...The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'."
Edzard Ernst has stated:
Research funding.
Funding for research into effectiveness of alternative treatments comes from a variety of public and private sources. In the USA, one conduit for funding and information is the National Center for Complementary and Alternative Medicine (NCCAM), a division of DHS's National Institutes of Health (NIH). Other governments have various levels of funding; the Dutch government funded CAM research between 1986 and 2003, but formally ended it in 2006.
Appeal.
Physicians who practice complementary medicine usually discuss and advise patients as to available complementary therapies. Patients often express interest in mind-body complementary therapies because they offer a non-drug approach to treating some health conditions. Some mind-body techniques, such as cognitive-behavioral therapy, were once considered complementary medicine, but are now a part of conventional medicine in the United States.
Against alternative medicine it has been argued that in addition to the social-cultural underpinnings of the popularity of alternative medicine, there are several psychological issues that are critical to its growth. One of the most critical is the placebo effect, which is a well-established observation in medicine. Related to it are similar psychological effects such as the will to believe, cognitive biases that help maintain self-esteem and promote harmonious social functioning, and the "post hoc, ergo propter hoc" fallacy.
In UK.
CAM's popularity may be related to other factors which Edzard Ernst mentioned in an interview in "The Independent":
Why is it so popular, then? Ernst blames the providers, customers and the doctors whose neglect, he says, has created the opening into which alternative therapists have stepped. "People are told lies. There are 40 million websites and 39.9 million tell lies, sometimes outrageous lies. They mislead cancer patients, who are encouraged not only to pay their last penny but to be treated with something that shortens their lives. "At the same time, people are gullible. It needs gullibility for the industry to succeed. It doesn't make me popular with the public, but it's the truth.
In a paper published in October 2010 entitled "The public's enthusiasm for complementary and alternative medicine amounts to a critique of mainstream medicine", Ernst described these views in greater detail and concluded:
[CAM] is popular. An analysis of the reasons why this is so points towards the therapeutic relationship as a key factor. Providers of CAM tend to build better therapeutic relationships than mainstream healthcare professionals. In turn, this implies that much of the popularity of CAM is a poignant criticism of the failure of mainstream healthcare. We should consider it seriously with a view of improving our service to patients.
In USA and Canada.
A study published in 1998 indicates that a majority of alternative medicine use was in conjunction with standard medical treatments. Approximately 4.4 percent of those studied used alternative medicine as a replacement for conventional medicine. The research found that those having used alternative medicine tended to have higher education or report poorer health status. Dissatisfaction with conventional medicine was not a meaningful factor in the choice, but rather the majority of alternative medicine users appear to be doing so largely because "they find these healthcare alternatives to be more congruent with their own values, beliefs, and philosophical orientations toward health and life." In particular, subjects reported a holistic orientation to health, a transformational experience that changed their worldview, identification with a number of groups committed to environmentalism, feminism, psychology, and/or spirituality and personal growth, or that they were suffering from a variety of common and minor ailments â notable ones being anxiety, back problems, and chronic pain.
Authors have speculated on the socio-cultural and psychological reasons for the appeal of alternative medicines among that minority using them "in lieu" of conventional medicine. There are several socio-cultural reasons for the interest in these treatments centered on the low level of scientific literacy among the public at large and a concomitant increase in antiscientific attitudes and new age mysticism. Related to this are vigorous marketing of extravagant claims by the alternative medical community combined with inadequate media scrutiny and attacks on critics.
There is also an increase in conspiracy theories toward conventional medicine and pharmaceutical companies, mistrust of traditional authority figures, such as the physician, and a dislike of the current delivery methods of scientific biomedicine, all of which have led patients to seek out alternative medicine to treat a variety of ailments. Many patients lack access to contemporary medicine, due to a lack of private or public health insurance, which leads them to seek out lower-cost alternative medicine. Medical doctors are also aggressively marketing alternative medicine to profit from this market.
Patients can also be averse to the painful, unpleasant, and sometimes-dangerous side effects of biomedical treatments. Treatments for severe diseases such as cancer and HIV infection have well-known, significant side-effects. Even low-risk medications such as antibiotics can have potential to cause life-threatening anaphylactic reactions in a very few individuals. Also, many medications may cause minor but bothersome symptoms such as cough or upset stomach. In all of these cases, patients may be seeking out alternative treatments to avoid the adverse effects of conventional treatments.
Schofield and others, in a systematic review published in 2011, make ten recommendations which they think may increase the effectiveness of consultations in a conventional (here: oncology) setting, such as "Ask questions about CAM use at critical points in the illness trajectory"; "Respond to the person's emotional state"; and "Provide balanced, evidence-based advice". They suggest that this approach may address "... concerns surrounding CAM use [and] encourage informed decision-making about CAM and ultimately, improve outcomes for patients".

</doc>
<doc id="1847" url="http://en.wikipedia.org/wiki?curid=1847" title="Archimedean solid">
Archimedean solid

In geometry, an Archimedean solid is a highly symmetric, semi-regular convex polyhedron composed of two or more types of regular polygons meeting in identical vertices. They are distinct from the Platonic solids, which are composed of only one type of polygon meeting in identical vertices, and from the Johnson solids, whose regular polygonal faces do not meet in identical vertices.
"Identical vertices" are usually taken to mean that for any two vertices, there must be an isometry of the entire solid that takes one vertex to the other. Sometimes it is instead only required that the faces that meet at one vertex are related isometrically to the faces that meet at the other. This difference in definitions controls whether the elongated square gyrobicupola (pseudo-rhombicuboctahedron) is considered an Archimedean solid or a Johnson solid: it is the unique convex polyhedron that has regular polygons meeting in the same way at each vertex, but that does not have a global symmetry taking every vertex to every other vertex. Based on its existence, has suggested a terminological distinction in which an Archimedean solid is defined as having the same vertex figure at each vertex (including the elongated square gyrobicupola) while a uniform polyhedron is defined as having each vertex symmetric to each other vertex (excluding the gyrobicupola).
Prisms and antiprisms, whose symmetry groups are the dihedral groups, are generally not considered to be Archimedean solids, despite meeting the above definition. With this restriction, there are only finitely many Archimedean solids. All but the elongated square gyrobicupola can be made via Wythoff constructions from the Platonic solids with tetrahedral, octahedral and icosahedral symmetry.
Origin of name.
The Archimedean solids take their name from Archimedes, who discussed them in a now-lost work. Pappus refers to it, stating that Archimedes listed 13 polyhedra. During the Renaissance, artists and mathematicians valued "pure forms" and rediscovered all of these forms. This search was almost entirely completed around 1620 by Johannes Kepler, who defined prisms, antiprisms, and the non-convex solids known as the Kepler-Poinsot polyhedra.
Kepler may have also found the elongated square gyrobicupola: at least, he once stated that there were 14 Archimedean solids. However, his published enumeration only includes the 13 uniform polyhedra, and the first clear statement of the gyrobicupola's existence was made in 1905, by Duncan Sommerville.
Classification.
There are 13 Archimedean solids (not counting the elongated square gyrobicupola; 15 if the mirror images of two enantiomorphs, see below, are counted separately).
Here the "vertex configuration" refers to the type of regular polygons that meet at any given vertex. For example, a vertex configuration of (4,6,8) means that a square, hexagon, and octagon meet at a vertex (with the order taken to be clockwise around the vertex).
Some definitions of semiregular polyhedron include one more figure, the elongated square gyrobicupola or "pseudo-rhombicuboctahedron".
Properties.
The number of vertices is 720Â° divided by the vertex angle defect.
The cuboctahedron and icosidodecahedron are edge-uniform and are called quasi-regular.
The duals of the Archimedean solids are called the Catalan solids. Together with the bipyramids and trapezohedra, these are the face-uniform solids with regular vertices.
Chirality.
The snub cube and snub dodecahedron are known as "chiral", as they come in a left-handed (Latin: levomorph or laevomorph) form and right-handed (Latin: dextromorph) form. When something comes in multiple forms which are each other's three-dimensional mirror image, these forms may be called enantiomorphs. (This nomenclature is also used for the forms of certain chemical compounds).
Construction of Archimedean solids.
The different Archimedean and Platonic solids can be related to each other using just several general constructions. Starting with a Platonic solid, truncation involves cutting away of corners. To preserve symmetry, the cut is in a plane perpendicular to the line joining a corner to the center of the polyhedron and is the same for all corners. Depending on how much is truncated (see table below), different Platonic and Archimedean (and other) solids can be created. Expansion involves moving each face away from the center (by the same distance so as to preserve the symmetry of the Platonic solid) and taking the convex hull. Expansion with twisting also involves rotating the faces, thus breaking the rectangles corresponding to edges into triangles. The last construction we use here is truncation of both corners and edges. Ignoring scaling, expansion can also be viewed as truncation of corners and edges but with a particular ratio between corner and edge truncation.
Note the duality between cube and octahedron, and between dodecahedron and icosahedron. Also, in part due to self-duality of tetrahedron, only one Archimedean solid has just tetrahedral symmetry.

</doc>
<doc id="1851" url="http://en.wikipedia.org/wiki?curid=1851" title="Antiprism">
Antiprism

In geometry, an "n"-sided antiprism is a polyhedron composed of two parallel copies of some particular "n"-sided polygon, connected by an alternating band of triangles. Antiprisms are a subclass of the prismatoids.
Antiprisms are similar to prisms except the bases are twisted relative to each other, and that the side faces are triangles, rather than quadrilaterals.
In the case of a regular "n"-sided base, one usually considers the case where its copy is twisted by an angle 180Â°/"n". Extra regularity is obtained by the line connecting the base centers being perpendicular to the base planes, making it a right antiprism. As faces, it has the two "n"-gonal bases and, connecting those bases, 2"n" isosceles triangles.
Uniform antiprism.
A uniform antiprism has, apart from the base faces, 2"n" equilateral triangles as faces. As a class, the uniform antiprisms form an infinite series of vertex-uniform polyhedra, as do the uniform prisms. For we have as degenerate case the regular tetrahedron as a "digonal antiprism", and for the non-degenerate regular octahedron as a "triangular antiprism".
The dual polyhedra of the antiprisms are the trapezohedra. Their existence was first discussed and their name was coined by Johannes Kepler.
Cartesian coordinates.
Cartesian coordinates for the vertices of a right antiprism with "n"-gonal bases and isosceles triangles are
with "k" ranging from 0 to 2"n"â1; if the triangles are equilateral,
Volume and surface area.
Let "a" be the edge-length of a uniform antiprism. Then the volume is
and the surface area is
Related polyhedra.
There are an infinite set of truncated antiprisms, including a lower-symmetry form of the truncated octahedron (truncated triangular antiprism). These can be alternated to create snub antiprisms, two of which are Johnson solids, and the "snub triangular antiprism" is a lower symmetry form of the icosahedron.
Symmetry.
The symmetry group of a right "n"-sided antiprism with regular base and isosceles side faces is D"n"d of order 4"n", except in the case of a tetrahedron, which has the larger symmetry group Td of order 24, which has three versions of D2d as subgroups, and the octahedron, which has the larger symmetry group Oh of order 48, which has four versions of D3d as subgroups.
The symmetry group contains inversion if and only if "n" is odd.
The rotation group is D"n" of order 2"n", except in the case of a tetrahedron, which has the larger rotation group T of order 12, which has three versions of D2 as subgroups, and the octahedron, which has the larger rotation group O of order 24, which has four versions of D3 as subgroups.
Star antiprism.
Uniform star antiprisms are named by their star polygon bases, {p/q}, and exist in prograde and retrograde (crossed) solutions. Crossed forms have intersecting vertex figures, and are denoted by inverted fractions, p/(p-q) instead of p/q, like 5/3 versus 5/2.
In the retrograde forms but not in the prograde forms, the triangles joining the star bases intersect the axis of rotational symmetry.
Some retrograde star antiprisms with regular star polygon bases cannot be constructed with equal edge lengths, so are not uniform polyhedra. Star antiprism compounds also can be constructed where p and q have common factors; thus a 10/4 antiprism is the compound of two 5/2 star antiprisms. 

</doc>
<doc id="1853" url="http://en.wikipedia.org/wiki?curid=1853" title="Natural history of Africa">
Natural history of Africa

The natural history of Africa encompasses some of the well known megafauna of that continent.
Natural history is the study and description of organisms and natural objects, especially their origins, evolution, and interrelationships.
Flora.
The vegetation of Africa follows very closely the distribution of heat and moisture. The northern and southern temperate zones have a flora distinct from that of the continent generally, which is tropical. In the countries bordering the Mediterranean, there are groves of orange and olive trees, evergreen oaks, cork trees and pines, intermixed with cypresses, myrtles, arbutus and fragrant tree-heaths.
South of the Atlas Range the conditions alter. The zones of minimum rainfall have a very scanty flora, consisting of plants adapted to resist the great dryness. Characteristic of the Sahara is the date palm, which flourishes where other vegetation can scarcely maintain existence, while in the semidesert regions the acacia, from which gum arabic is obtained, is abundant.
The more humid regions have a richer vegetation; dense forest where the rainfall is greatest and variations of temperature least, conditions found chiefly on the tropical coasts, and in the west African equatorial basin with its extension towards the upper Nile; and savanna interspersed with trees on the greater part of the plateaus, passing as the desert regions are approached into a scrub vegetation consisting of thorny acacias, etc. Forests also occur on the humid slopes of mountain ranges up to a certain elevation. In the coast regions the typical tree is the mangrove, which flourishes wherever the soil is of a swamp character.
The dense forests of West Africa contain, in addition to a great variety of hardwoods, two palms, "Elaeis guineensis" (oil palm) and "Raphia vinifera" (bamboo palm), not found, generally speaking, in the savanna regions. "Bombax" or silk-cotton trees attain gigantic proportions in the forests, which are the home of the India rubber-producing plants and of many valuable kinds of timber trees, such as odum ("Chlorophora excelsa"), ebony, mahogany ("Khaya senegalensis"), Oldfieldia ("Oldfieldia africana") and camwood ("Baphia nitida"). The climbing plants in the tropical forests are exceedingly luxuriant and the undergrowth or "bush" is extremely dense.
In the savannas the most characteristic trees are the monkey bread tree or baobab ("Adanisonia digitata"), doum palm ("Hyphaene") and euphorbias. The coffee plant grows wild in such widely separated places as Liberia and southern Ethiopia. The higher mountains have a special flora showing close agreement over wide intervals of space, as well as affinities with the mountain flora of the eastern Mediterranean, the Himalaya and Indo-China.
In the swamp regions of north-east Africa papyrus and associated plants, including the soft-wooded ambach, flourish in immense quantities, and little else is found in the way of vegetation. South Africa is largely destitute of forest save in the lower valleys and coast regions. Tropical flora disappears, and in the semi-desert plains the fleshy, leafless, contorted species of kapsias, mesembryanthemums, aloes and other succulent plants make their appearance. There are, too, valuable timber trees, such as the Yellow-wood ("Podocarpus elongatus"), stinkwood ("Ocotea"), sneezewood or Cape ebony ("Pteroxylon utile") and ironwood. Extensive miniature woods of heaths are found in almost endless variety and covered throughout the greater part of the year with innumerable blossoms in which red is very prevalent. Of the grasses of Africa alfa is very abundant in the plateaus of the Atlas range.
Fauna.
The fauna again shows the effect of the characteristics of the vegetation. The open savannas are the home of large ungulates, especially antelopes, the giraffe (peculiar to Africa), zebra, buffalo, wild donkey and four species of rhinoceros; and of carnivores, such as the lion, leopard, hyena, etc. The okapi (a genus restricted to Africa) is found only in the dense forests of the Congo basin. Bears are confined to the Atlas region, wolves and foxes to North Africa. The elephant (though its range has become restricted through the attacks of hunters) is found both in the savannas and forest regions, the latter being otherwise poor in large game, though the special habitat of the chimpanzee and gorilla. Baboons and mandrills, with few exceptions, are peculiar to Africa. The single-humped camel, as a domestic animal, is
especially characteristic of the northern deserts and steppes.
The rivers in the tropical zone abound with hippopotami and crocodiles, the former entirely confined to Africa. The vast herds of game, formerly so characteristic of many parts of Africa, have much diminished with the increase of intercourse with the interior. Game reserves have, however, been established in South Africa, British Central Africa, British East Africa, Somaliland, etc., while measures for the protection of wild animals were laid down in an international convention signed in May 1900.
The ornithology of northern Africa presents a close resemblance to that of southern Europe, scarcely a species being found which does not also occur in the other countries bordering the Mediterranean. Among the birds most characteristic of Africa are the ostrich and the secretary-bird. The ostrich is widely dispersed, but is found chiefly in the desert and steppe regions. The secretary-bird is common in the south. The weaver birds and their allies, including the long-tailed whydahs, are abundant, as are, among game-birds, the francolin and guineafowl. Many of the smaller birds, such as the sunbirds, bee-eaters, the parrots and kingfishers, as well as the larger plantain-eaters, are noted for the brilliance of their plumage.
Of reptiles the lizard and chameleon are common, and there are a number of venomous snakes, though these are not so numerous as in other tropical countries.
The scorpion is abundant. Of insects Africa has many thousand different kinds; of these the locust is the proverbial scourge of the continent, and the ravages of the termites are almost incredible. The spread of malaria by means of mosquitoes is common. The tsetse fly, whose bite is fatal to all domestic animals, is common in many districts of South and East Africa. It is found nowhere outside Africa.

</doc>
<doc id="1854" url="http://en.wikipedia.org/wiki?curid=1854" title="Geography of Africa">
Geography of Africa

Africa is a continent comprising 62 political territories, representing the largest of the great southward projections from the main mass of Earth's surface. Within its regular outline, it comprises an area of , including adjacent islands. Its highest mountain is Mount Kilimanjaro, its largest lake is Lake Victoria and its longest river is the Nile. 
Separated from Europe by the Mediterranean Sea and from much of Asia by the Red Sea, Africa is joined to Asia at its northeast extremity by the Isthmus of Suez (which is transected by the Suez Canal), wide. For geopolitical purposes, the Sinai Peninsula of EgyptÂ â east of the Suez CanalÂ â is often considered part of Africa. From the most northerly point, Ras ben Sakka in Tunisia, in 37Â°21â² N, to the most southerly point, Cape Agulhas in South Africa, 34Â°51â²15â³ S, is a distance approximately of ; from Cap-Vert, 17Â°31â²13â³W, the westernmost point, to Ras Hafun in Somalia, 51Â°27â²52â³ E, the most easterly projection, is a distance (also approximately) of . The length of coastline is and the absence of deep indentations of the shore is shown by the fact that Europe, which covers only , has a coastline of .
The main structural lines of the continent show both the east-to-west direction characteristic, at least in the eastern hemisphere, of the more northern parts of the world, and the north-to-south direction seen in the southern peninsulas. Africa is thus mainly composed of two segments at right angles, the northern running from east to west, and the southern from north to south.
Main features.
The average elevation of the continent approximates closely to above sea level, roughly near to the mean elevation of both North and South America, but considerably less than that of Asia, . In contrast with other continents, it is marked by the comparatively small area of either very high or very low ground, lands under occupying an unusually small part of the surface; while not only are the highest elevations inferior to those of Asia or South America, but the area of land over is also quite insignificant, being represented almost entirely by individual peaks and mountain ranges. Moderately elevated tablelands are thus the characteristic feature of the continent, though the surface of these is broken by higher peaks and ridges. (So prevalent are these isolated peaks and ridges that a specialised term ["Inselberg-landschaft"] has been adopted in Germany to describe this kind of country, thought to be in great part the result of wind action.)
As a general rule, the higher tablelands lie to the east and south, while a progressive diminution in altitude towards the west and north is observable. Apart from the lowlands and the Atlas mountain range, the continent may be divided into two regions of higher and lower plateaus, the dividing line (somewhat concave to the north-west) running from the middle of the Red Sea to about 6 deg. S. on the west coast...
Africa can be divided into a number of geographic zones:
Plateau region.
The high southern and eastern plateaus, rarely falling below , have a mean elevation of about . The South African Plateau, as far as about 12Â° S, is bounded east, west and south by bands of high ground which fall steeply to the coasts. On this account South Africa has a general resemblance to an inverted saucer. Due south the plateau rim is formed by three parallel steps with level ground between them. The largest of these level areas, the Great Karoo, is a dry, barren region, and a large tract of the plateau proper is of a still more arid character and is known as the Kalahari Desert.
The South African Plateau is connected towards East African plateau, with probably a slightly greater average elevation, and marked by some distinct features. It is formed by a widening out of the eastern axis of high ground, which becomes subdivided into a number of zones running north and south and consisting in turn of ranges, tablelands and depressions. The most striking feature is the existence of two great lines of depression, due largely to the subsidence of whole segments of the Earth's crust, the lowest parts of which are occupied by vast lakes. Towards the south the two lines converge and give place to one great valley (occupied by Lake Nyasa), the southern part of which is less distinctly due to rifting and subsidence than the rest of the system.
Farther north the western depression, known as the Albertine Rift is occupied for more than half its length by water, forming the Great Lakes of Tanganyika, Kivu, Lake Edward and Lake Albert, the first-named over 400 miles (600Â km) long and the longest freshwater lake in the world. Associated with these great valleys are a number of volcanic peaks, the greatest of which occur on a meridional line east of the eastern trough. The eastern branch of the East African Rift, contains much smaller lakes, many of them brackish and without outlet, the only one comparable to those of the western trough being Lake Turkana or Basso Norok.
At no great distance east of this rift-valley is Mount Kilimanjaro - with its two peaks Kibo and Mawenzi, the latter being , and the culminating point of the whole continentÂ â and Mount Kenya, which is . Hardly less important is the Ruwenzori Range, over , which lies east of the western trough. Other volcanic peaks rise from the floor of the valleys, some of the Kirunga (Mfumbiro) group, north of Lake Kivu, being still partially active. This could cause most of the citys and states to be flooded with lava and ash.
The third division of the higher region of Africa is formed by the Ethiopian Highlands, a rugged mass of mountains forming the largest continuous area of its altitude in the whole continent, little of its surface falling below , while the summits reach heights of 4600 m to 4900 m (15,000 to 16,000Â ft). This block of country lies just west of the line of the great East African Trough, the northern continuation of which passes along its eastern escarpment as it runs up to join the Red Sea. There is, however, in the centre a circular basin occupied by Lake Tsana.
Both in the east and west of the continent the bordering highlands are continued as strips of plateau parallel to the coast, the Ethiopian mountains being continued northwards along the Red Sea coast by a series of ridges reaching in places a height of . In the west the zone of high land is broader but somewhat lower. The most mountainous districts lie inland from the head of the Gulf of Guinea (Adamawa, etc.), where heights of 1800 m to 2400 m (6000 to 8000Â ft) are reached. Exactly at the head of the gulf the great peak of the Cameroon, on a line of volcanic action continued by the islands to the south-west, has a height of , while Clarence Peak, in Fernando Po, the first of the line of islands, rises to over . Towards the extreme west the Futa Jallon highlands form an important diverging point of rivers, but beyond this, as far as the Atlas chain, the elevated rim of the continent is almost wanting.
Plains.
The area between the east and west coast highlands, which north of 17Â° N is mainly desert, is divided into separate basins by other bands of high ground, one of which runs nearly centrally through North Africa in a line corresponding roughly with the curved axis of the continent as a whole. The best marked of the basins so formed (the Congo basin) occupies a circular area bisected by the equator, once probably the site of an inland sea.
Running along the south of desert is the plains region known as the Sahel.
The arid region, the SaharaÂ â the largest desert in the world, covering Â â extends from the Atlantic to the Red Sea. Though generally of slight elevation, it contains mountain ranges with peaks rising to Bordered N.W. by the Atlas range, to the northeast a rocky plateau separates it from the Mediterranean; this plateau gives place at the extreme east to the delta of the Nile. That river (see below) pierces the desert without modifying its character. The Atlas range, the north-westerly part of the continent, between its seaward and landward heights encloses elevated steppes in places broad. From the inner slopes of the plateau numerous wadis take a direction towards the Sahara. The greater part of that now desert region is, indeed, furrowed by old water-channels.
Mountains.
The following table gives the approximate altitudes of the chief mountains of the continent:
Rivers.
From the outer margin of the African plateaus, a large number of streams run to the sea with comparatively short courses, while the larger rivers flow for long distances on the interior highlands, before breaking through the outer ranges. The main drainage of the continent is to the north and west, or towards the basin of the Atlantic Ocean.
To the main African rivers belong: Nile (the longest river of Africa), Congo (river with the highest water discharge on the continent) and the Niger, which flows half of its length through the arid areas. The largest lakes are the following: Lake Victoria (Lake Ukerewe), Lake Chad, in the centre of the continent, Lake Tanganika, lying between the Democratic Republic of Congo, Burundi, Tanzania and Zambia. There is also the considerably large Lake Malawi stretching along the eastern border of one of the poorest countries in the world -Malawi. There are also numerous water dams throughout the continent: Kariba on the river of Zambezi, Asuan in Egypt on the river of Nile and the biggest dam of the continent lying completely in The republic of Ghana is called Akosombo on the Volta river (Fobil 2003).
The high lake plateau of the African Great Lakes region contains the headwaters of both the Nile and the Congo.
The upper Nile receives its chief supplies from the mountainous region adjoining the Central African trough in the neighbourhood of the equator. From there, streams pour eastward into Lake Victoria, the largest lake in Africa (covering over 26,000 square m.), and to the west and north into Lake Edward and Lake Albert. To the latter of these, the effluents of the other two lakes add their waters. Issuing from there, the Nile flows northward, and between the latitudes of 7 and 10 degrees north it traverses a vast marshy level, where its course is liable to being blocked by floating vegetation. After receiving the Bahr-el-Ghazal from the west and the Sobat, Blue Nile and Atbara from the Ethiopian Highlands (the chief gathering ground of the flood-water), it separates the great desert with its fertile watershed, and enters the Mediterranean at a vast delta.
The most remote head-stream of the Congo is the Chambezi, which flows southwest into the marshy Lake Bangweulu. From this lake issues the Congo, known in its upper course by various names. Flowing first south, it afterwards turns north through Lake Mweru and descends to the forest-clad basin of west equatorial Africa. Traversing this in a majestic northward curve, and receiving vast supplies of water from many great tributaries, it finally turns southwest and cuts a way to the Atlantic Ocean through the western highlands.
North of the Congo basin, and separated from it by a broad undulation of the surface, is the basin of Lake Chad â a flat-shored, shallow lake filled principally by the Chari coming from the southeast.
West of this is the basin of the Niger, the third major river of Africa. With its principal source in the far west, it reverses the direction of flow exhibited by the Nile and Congo, and ultimately flows into the AtlanticÂ â a fact that eluded European geographers for many centuries. An important branch, howeverÂ â the Benueâflows from the southeast.
These four river basins occupy the greater part of the lower plateaus of North and West AfricaÂ â the remainder consists of arid regions watered only by intermittent streams that do not reach the sea.
Of the remaining rivers of the Atlantic basin, the Orange, in the extreme south, brings the drainage from the Drakensberg on the opposite side of the continent, while the Kunene, Kwanza, Ogowe and Sanaga drain the west coastal highlands of the southern limb; the Volta, Komoe, Bandama, Gambia and Senegal the highlands of the western limb. North of the Senegal, for over 1000 miles (1600Â km) of coast, the arid region reaches to the Atlantic. Farther north are the streams, with comparatively short courses, reaching the Atlantic and Mediterranean from the Atlas mountains.
Of the rivers flowing to the Indian Ocean, the only one draining any large part of the interior plateaus is the Zambezi, whose western branches rise in the western coastal highlands. The main stream has its rise in 11Â°21â²3â³ S 24Â°22â² E, at an elevation of . It flows to the west and south for a considerable distance before turning eastward. All the largest tributaries, including the Shire, the outflow of Lake Nyasa, flow down the southern slopes of the band of high ground stretching across the continent from 10Â° to 12Â° S. In the southwest, the Zambezi system interlaces with that of the Taukhe (or Tioghe), from which it at times receives surplus water. The rest of the water of the Taukhe, known in its middle course as the Okavango, is lost in a system of swamps and saltpans that was formerly centred in Lake Ngami, now dried up.
Farther south, the Limpopo drains a portion of the interior plateau, but breaks through the bounding highlands on the side of the continent nearest its source. The Rovuma, Rufiji and Tana principally drain the outer slopes of the African Great Lakes highlands.
In the Horn region to the north, the Jubba and the Shebelle rivers begin in the Ethiopian Highlands. These rivers mainly flow southwards, with the Jubba emptying in the Indian Ocean. The Shebelle River reaches a point to the southwest. After that, it consists of swamps and dry reaches before finally disappearing in the desert terrain near the Jubba River. Another large stream, the Hawash, rising in the Ethiopian mountains, is lost in a saline depression near the Gulf of Aden.
Between the basins of the Atlantic and Indian Oceans, there is an area of inland drainage along the centre of the Ethiopian plateau, directed chiefly into the lakes in the Great Rift Valley. The largest river is the Omo, which, fed by the rains of the Ethiopian highlands, carries down a large body of water into Lake Rudolf. The rivers of Africa are generally obstructed either by bars at their mouths, or by cataracts at no great distance upstream. But when these obstacles have been overcome, the rivers and lakes afford a vast network of navigable waters.
The area of the Congo basin is greater than that of any other river except the Amazon, while the African inland drainage area is greater than that of any continent but Asia, where the corresponding area is 4,000,000 square miles (10 MmÂ²).
Lakes.
The principal lakes of Africa are situated in the African Great Lakes plateau. As a rule, the lakes found within the Great Rift Valley have steep sides and are very deep. This is the case with the two largest of the type, Tanganyika and Nyasa, the latter with depths of 430 fathoms (790 m).
Others, however, are shallow, and hardly reach the steep sides of the valleys in the dry season. Such are Lake Rukwa, in a subsidiary depression north of Nyasa, and Eiassi and Manyara in the system of the Great Rift Valley. Lakes of the broad type are of moderate depth, the deepest sounding in Lake Victoria being under 50 fathoms (90 m).
Besides the African Great Lakes, the principal lakes on the continent are: Lake Chad, in the northern inland watershed; Bangweulu and Mweru, traversed by the head-stream of the Congo; and Lake Mai-Ndombe and Ntomba (Mantumba), within the great bend of that river. All, except possibly Mweru, are more or less shallow, and Lake Chad appears to be drying up.
Divergent opinions have been held as to the mode of origin of the African Great Lakes, especially Tanganyika, which some geologists have considered to represent an old arm of the sea, dating from a time when the whole central Congo basin was under water; others holding that the lake water has accumulated in a depression caused by subsidence. The former view is based on the existence in the lake of organisms of a decidedly marine type. They include jellyfish, molluscs, prawns, crabs, etc.
Islands.
With the exception of Madagascar the African islands are small. Madagascar, with an area of , is, after Greenland, New Guinea and Borneo, the fourth largest island on the Earth. It lies in the Indian Ocean, off the S.E. coast of the continent, from which it is separated by the deep Mozambique channel, wide at its narrowest point. Madagascar in its general structure, as in flora and fauna, forms a connecting link between Africa and southern Asia. East of Madagascar are the small islands of Mauritius and RÃ©union. There are also islands in the Gulf of Guinea on which lies the Republic of Sao TomÃ© and PrÃ­ncipe (islands of SÃ£o TomÃ© and PrÃ­ncipe). Part of the Republic of Equatorial Guinea is lying on the island of Bioko (with the capital Malabo and the town of Lubu) and the island of AnnobÃ³n. Socotra lies E.N.E. of Cape Guardafui. Off the north-west coast are the Canary and Cape Verde archipelagoes. which, like some small islands in the Gulf of Guinea, are of volcanic origin. The South Atlantic Islands of Saint Helena and Ascension are classed as Africa but are situated on the Mid-Atlantic Ridge half way to South America.
Climate.
Lying almost entirely within the tropics, and equally to north and south of the equator, Africa does not show excessive variations of temperature.
Great heat is experienced in the lower plains and desert regions of North Africa, removed by the great width of the continent from the influence of the ocean, and here, too, the contrast between day and night, and between summer and winter, is greatest. (The rarity of the air and the great radiation during the night cause the temperature in the Sahara to fall occasionally to freezing point.)
Farther south, the heat is to some extent modified by the moisture brought from the ocean, and by the greater elevation of a large part of the surface, especially in East Africa, where the range of temperature is wider than in the Congo basin or on the Guinea coast.
In the extreme north and south the climate is a warm temperate one, the northern countries being on the whole hotter and drier than those in the southern zone; the south of the continent being narrower than the north, the influence of the surrounding ocean is more felt.
The most important climatic differences are due to variations in the amount of rainfall. The wide heated plains of the Sahara, and in a lesser degree the corresponding zone of the Kalahari in the south, have an exceedingly scanty rainfall, the winds which blow over them from the ocean losing part of their moisture as they pass over the outer highlands, and becoming constantly drier owing to the heating effects of the burning soil of the interior; while the scarcity of mountain ranges in the more central parts likewise tends to prevent condensation. In the inter-tropical zone of summer precipitation, the rainfall is greatest when the sun is vertical or soon after. It is therefore greatest of all near the equator, where the sun is twice vertical, and less in the direction of both tropics.
The rainfall zones are, however, somewhat deflected from a due west-to-east direction, the drier northern conditions extending southwards along the east coast, and those of the south northwards along the west. Within the equatorial zone certain areas, especially on the shores of the Gulf of Guinea and in the upper Nile basin, have an intensified rainfall, but this rarely approaches that of the rainiest regions of the world. The rainiest district in all Africa is a strip of coastland west of Mount Cameroon, where there is a mean annual rainfall of about as compared with a mean of at Cherrapunji, in Meghalaya, India.
The two distinct rainy seasons of the equatorial zone, where the sun is vertical at half-yearly intervals, become gradually merged into one in the direction of the tropics, where the sun is overhead but once. Snow falls on all the higher mountain ranges, and on the highest the climate is thoroughly Alpine.
The countries bordering the Sahara are much exposed to a very dry wind, full of fine particles of sand, blowing from the desert towards the sea. Known in Egypt as the khamsin, on the Mediterranean as the sirocco, it is called on the Guinea coast the harmattan. This wind is not invariably hot; its great dryness causes so much evaporation that cold is not infrequently the result. Similar dry winds blow from the Kalahari Desert in the south. On the eastern coast the monsoons of the Indian Ocean are regularly felt, and on the southeast hurricanes are occasionally experienced.
Health.
The climate of Africa lends itself to certain environmental diseases, the most serious of which are: malaria, sleeping sickness and yellow fever. Malaria is the most deadly environmental disease in Africa. It is transmitted by a genus of mosquito (anopheles mosquito) native to Africa, and can be contracted over and over again. There is not yet a vaccine for malaria, which makes it difficult to prevent the disease from spreading in Africa. Recently, the dissemination of mosquito netting has helped lower the rate of malaria.
Yellow fever is a disease also transmitted by mosquitoes native to Africa. Unlike malaria, it cannot be contracted more than once. Like chicken pox, it is a disease that tends to be severe the later in life a person contracts the disease.
Sleeping sickness, or African trypanosomiasis, is a disease that usually affects animals, but has been known to be fatal to some humans as well. It is transmitted by the tse tse fly, and is found almost exclusively in Sub-Saharan Africa. This disease has had a significant impact on African development not because of its deadly nature, like Malaria, but because it has prevented Africans from pursuing agriculture (as the sleeping sickness would kill their livestock).
Extreme points.
These are the points that are farther north, south, east or west than any other location on the continent.
The highest point in Africa is Mount Kilimanjaro, in Tanzania. The lowest point is Lake Asal, below sea level, in Djibouti.
See also.
Richard Grant 2014. Africa. Geographies of Change. New York: Oxford University Press.

</doc>
<doc id="1857" url="http://en.wikipedia.org/wiki?curid=1857" title="Approval voting">
Approval voting

Approval voting is a single-winner voting method used for elections. Each voter may 'approve' of (i.e. select) as many candidates as he or she wishes. The winner is the most-approved candidate.
The system was described in 1977 by Guy Ottewell and also by Robert J. Weber, who coined the term "Approval Voting." It was more fully published in 1978 by political scientist Steven Brams and mathematician Peter Fishburn.
Description.
Approval voting can be considered a form of Score voting, with the range restricted to two values, 0 and 1, or a form of majority judgment, with the grades restricted to "Good" and "Poor". Approval Voting can also be compared to plurality voting, without the rule that discards ballots which vote for more than one candidate.
By treating each candidate as a separate question, "Do you approve of this person for the job?" approval voting allows each voter to indicate which candidates he or she supports. All votes count equally, and everyone gets the same number of votes: one vote per candidate, either for or against. The final tallies show how many voters support each candidate, and the winner is the candidate whom the most voters support.
Approval voting ballots show, for each office being contested, a list of the candidates running for that seat. Next to each name is a checkbox, or another similar way to mark 'Yes' or 'No' for that candidate. This "check yes or no" approach means approval voting provides one of the simplest ballots for a voter to understand.
Ballots which mark every candidate the same (whether yes or no) have no effect on the outcome of the election. Each ballot can therefore be viewed as a small "delta" which separates two groups of candidates, those which are supported and those which are not. Each candidate approved is considered preferred to any candidate not approved, while the voter's preferences among approved candidates is unspecified, and likewise the voter's preferences among unapproved candidates is also unspecified.
Uses.
Approval voting has been adopted by the Mathematical Association of America (1986), the Institute of Management Sciences (1987) (now the Institute for Operations Research and the Management Sciences), the American Statistical Association (1987), and the Institute of Electrical and Electronics Engineers (1987). According to Steven J. Brams and Peter C. Fishburn, the IEEE board in 2002 rescinded its decision to use approval voting. IEEE Executive Director Daniel J. Senese stated that approval voting was abandoned because "few of our members were using it and it was felt that it was no longer needed."
Approval voting was used for Dartmouth Alumni Association elections for seats on the College Board of Trustees, but after some controversy it was replaced with traditional runoff elections by an alumni vote of 82% to 18% in 2009. Dartmouth students started to use approval voting to elect their student body president in 2011. In the first election, the winner secured the support of 41% of voters against several write-in candidates. In 2012, Suril Kantaria won with the support of 32% of the voters. In 2013, the winner earned the support of just under 40% of the voters.
Historically, several voting methods which incorporate aspects of approval voting have been used:
Effect on elections.
Approval voting advocates Steven Brams and Dudley R. Herschbach predict that approval voting should increase voter participation, prevent minor-party candidates from being spoilers, and reduce negative campaigning. The effect of this system as an electoral reform measure is not without critics, however. FairVote has a position paper arguing that approval voting has three flaws that undercut it as a method of voting and political vehicle. They argue that it can result in the defeat of a candidate who would win an absolute majority in a plurality system, can allow a candidate to win who might not win any support in a plurality elections, and has incentives for tactical voting.
One study showed that approval voting would not have chosen the same two winners as plurality voting (Chirac and Le Pen) in France's presidential election of 2002 (first round) â it instead would have chosen Chirac and Jospin as the top two to proceed to a runoff. Le Pen lost by a very high margin in the runoff, 82.2% to 17.8%, a sign that the true top two had not been found. Straight approval voting without a runoff, from the study, still would have selected Chirac, but with an approval percentage of only 36.7%, compared to Jospin at 32.9%. Le Pen, in that study, would have received 25.1%. In the real primary election, the top three were Chirac, 19.9%, Le Pen, 16.9%, and Jospin, 16.2%.
A generalized version of the Burr dilemma applies to approval voting when two candidates are appealing to the same subset of voters. Although approval voting differs from the voting system used in the Burr dilemma, approval voting can still leave candidates and voters with the generalized dilemma of whether to compete or cooperate.
While in the modern era there have been relatively few competitive approval voting elections where tactical voting is more likely, Brams argues that approval voting usually elects Condorcet winners in practice. Critics of the use of approval voting in the alumni elections for the Dartmouth Board of Trustees in 2009 placed its ultimately successful repeal before alumni voters, arguing that the system has not been electing the most centrist candidates. "The Dartmouth" editorialized that "When the alumni electorate fails to take advantage of the approval voting process, the three required Alumni Council candidates tend to split the majority vote, giving petition candidates an advantage. By reducing the number of Alumni Council candidates, and instituting a more traditional one-person, one-vote system, trustee elections will become more democratic and will more accurately reflect the desires of our alumni base."
Strategic voting.
Overview.
Approval voting is vulnerable to Bullet Voting and Compromising, while it is immune to Push-Over and Burying.
Bullet Voting occurs when a voter approves "only" candidate 'a' instead of "both" 'a' and 'b' for the reason that voting for 'b' can cause 'a' to lose.
Compromising occurs when a voter approves an "additional" candidate who is otherwise considered unacceptable to the voter, in order to prevent an even worse alternative from winning.
Strategic Approval voting differs from ranked choice voting methods where voters might reverse the preference order of two options. Strategic Approval voting, with more than two options, involves the voter changing their approval threshold. The voter decides which of the options to give the same rating, despite having a strict preference order between them.
Sincere voting.
Approval voting experts describe sincere votes as those "... that directly reflect the true preferences of a voter, i.e., that do not report preferences 'falsely.'" They also give a specific definition of a sincere approval vote in terms of the voter's ordinal preferences as being any vote that, if it votes for one candidate, it also votes for any more preferred candidate. This definition allows a sincere vote to treat strictly preferred candidates the same, ensuring that every voter has at least one sincere vote. The definition also allows a sincere vote to treat equally preferred candidates differently. When there are two or more candidates, every voter has at least three sincere approval votes to choose from. Two of those sincere approval votes do not distinguish between any of the candidates: vote for none of the candidates and vote for all of the candidates. When there are three or more candidates, every voter has more than one sincere approval vote that distinguishes between the candidates.
Examples.
Based on the definition above, if there are four candidates, A, B, C, and D, and a voter has a strict preference order, preferring A to B to C to D, then the following are the voter's possible sincere approval votes:
If the voter instead equally prefers B and C, while A is still the most preferred candidate and D is the least preferred candidate, then all of the above votes are sincere and the following combination is also a sincere vote:
The decision between the above ballots is equivalent to deciding an arbitrary "approval cutoff." All candidates preferred to the cutoff are approved, all candidates less preferred are not approved, and any candidates equal to the cutoff may be approved or not arbitrarily.
Sincere strategy with ordinal preferences.
A sincere voter with multiple options for voting sincerely still has to choose which sincere vote to use. Voting strategy is a way to make that choice, in which case strategic approval voting includes sincere voting, rather than being an alternative to it. This differs from other voting systems that typically have a unique sincere vote for a voter.
When there are three or more candidates, the winner of an approval voting election can change, depending on which sincere votes are used. In some cases, approval voting can sincerely elect any one of the candidates, including a Condorcet winner and a Condorcet loser, without the voter preferences changing. To the extent that electing a Condorcet winner and not electing a Condorcet loser is considered desirable outcomes for a voting system, approval voting can be considered vulnerable to sincere, strategic voting. In one sense, conditions where this can happen are robust and are not isolated cases. On the other hand, the variety of possible outcomes has also been portrayed as a virtue of approval voting, representing the flexibility and responsiveness of approval voting, not just to voter ordinal preferences, but cardinal utilities as well.
Dichotomous preferences.
Approval voting avoids the issue of multiple sincere votes in special cases when voters have dichotomous preferences. For a voter with dichotomous preferences, approval voting is strategy-proof (also known as strategy-free). When all voters have dichotomous preferences and vote the sincere, strategy-proof vote, approval voting is guaranteed to elect the Condorcet winner, if one exists. However, having dichotomous preferences when there are three or more candidates is not typical. It is an unlikely situation for all voters to have dichotomous preferences when there are more than a few voters.
Having dichotomous preferences means that a voter has bi-level preferences for the candidates. All of the candidates are divided into two groups such that the voter is indifferent between any two candidates in the same group and any candidate in the top-level group is preferred to any candidate in the bottom-level group. A voter that has strict preferences between three candidatesâprefers A to B and B to Câdoes not have dichotomous preferences.
Being strategy-proof for a voter means that there is a unique way for the voter to vote that is a strategically best way to vote, regardless of how others vote. In approval voting, the strategy-proof vote, if it exists, is a sincere vote.
Approval threshold.
Another way to deal with multiple sincere votes is to augment the ordinal preference model with an approval or acceptance threshold. An approval threshold divides all of the candidates into two sets, those the voter approves of and those the voter does not approve of. A voter can approve of more than one candidate and still prefer one approved candidate to another approved candidate. Acceptance thresholds are similar. With such a threshold, a voter simply votes for every candidate that meets or exceeds the threshold.
With threshold voting, it is still possible to not elect the Condorcet winner and instead elect the Condorcet loser when they both exist. However, according to Steven Brams, this represents a strength rather than a weakness of approval voting. Without providing specifics, he argues that the pragmatic judgements of voters about which candidates are acceptable should take precedence over the Condorcet criterion and other social choice criteria.
Strategy with cardinal utilities.
Voting strategy under approval is guided by two competing features of approval voting. On the one hand, approval voting fails the later-no-harm criterion, so voting for a candidate can cause that candidate to win instead of a more preferred candidate. On the other hand, approval voting satisfies the monotonicity criterion, so not voting for a candidate can never help that candidate win, but can cause that candidate to lose to a less preferred candidate. Either way, the voter can risk getting a less preferred election winner. A voter can balance the risk-benefit trade-offs by considering the voter's cardinal utilities, particularly via the von NeumannâMorgenstern utility theorem, and the probabilities of how others will vote.
A rational voter model described by Myerson and Weber specifies an approval voting strategy that votes for those candidates that have a positive prospective rating. This strategy is optimal in the sense that it maximizes the voter's expected utility, subject to the constraints of the model and provided the number of other voters is sufficiently large.
An optimal approval vote will always vote for the most preferred candidate and not vote for the least preferred candidate. However, an optimal vote can require voting for a candidate and not voting for a more preferred candidate if there 4 candidates or more.
Other strategies are also available and will coincide with the optimal strategy in special situations. For example:
Another strategy is to vote for the top half of the candidates, the candidates that have an above-median utility. When the voter thinks that the others will balance their votes randomly and evenly, the strategy will maximize the voter's power or efficacy, meaning that it will maximize the probability that the voter will make a difference in deciding which candidate wins.
Optimal strategic approval voting fails to satisfy the Condorcet criterion and can elect a Condorcet loser. Strategic approval voting can guarantee electing the Condorcet winner in some special circumstances. For example, if all voters are rational and cast a strategically optimal vote based on a common knowledge of how all the other voters vote except for small-probability, statistically independent errors in recording the votes, then the winner will be the Condorcet winner, if one exists.
Strategy examples.
In the example election described here, assume that the voters in each faction share the following von Neumann-Morgenstern utilities, fitted to the interval between 0 and 100. The utilities are consistent with the rankings given earlier and reflect a strong preference each faction has for choosing its city, compared to weaker preferences for other factors such as the distance to the other cities.
Using these utilities, voters will choose their optimal strategic votes based on what they think the various pivot probabilities are for pairwise ties. In each of the scenarios summarized below, all voters share a common set of pivot probabilities.
In the first scenario, voters all choose their votes based on the assumption that all pairwise ties are equally likely. As a result, they vote for any candidate with an above-average utility. Most voters vote for only their first choice. Only the Knoxville faction also votes for its second choice, Chattanooga. As a result, the winner is Memphis, the Condorcet loser, with Chattanooga coming in second place.
In the second scenario, all of the voters expect that Memphis is the likely winner, that Chattanooga is the likely runner-up, and that the pivot probability for a Memphis-Chattanooga tie is much larger than the pivot probabilities of any other pair-wise ties. As a result, each voter will vote for any candidate that is more preferred than the leading candidate and will also vote for the leading candidate if that candidate is more preferred than the expected runner-up. Each of the remaining scenarios follows a similar pattern of expectations and voting strategies.
In the second scenario, there is a three-way tie for first place. This happens because the expected winner, Memphis, was the Condorcet loser and was also ranked last by any voter that did not rank it first.
Only in the last scenario does the actual winner and runner-up match the expected winner and runner-up. As a result, this can be considered a stable strategic voting scenario. In the language of game theory, this is an "equilibrium." In this scenario, the winner is also the Condorcet winner.
Compliance with voting system criteria.
Most of the mathematical criteria by which voting systems are compared were formulated for voters with ordinal preferences. In this case, approval voting requires voters to make an additional decision of where to put their approval cutoff (see examples above). Depending on how this decision is made, approval voting satisfies different sets of criteria.
There is no ultimate authority on which criteria should be considered, but the following are some criteria that are accepted and considered to be desirable by many voting theorists:
Multiple winners.
Approval voting can be extended to multiple winner elections. The naive way to do so is as "block approval voting", a simple variant on block voting where each voter can select an unlimited number of candidates and the candidates with the most approval votes win. This does not provide proportional representation and is subject to the Burr dilemma, among other problems.
Other ways of extending Approval voting to multiple winner elections have been devised. Among these are proportional approval voting for determining a proportional assembly, and Minimax Approval for determining a consensus assembly where the least satisfied voter is satisfied the most.
Ballot types.
Approval ballots can be of at least four semi-distinct forms. The simplest form is a blank ballot where the names of supported candidates are written in by hand. A more structured ballot will list all the candidates and allow a mark or word to be made by each supported candidate. A more explicit structured ballot can list the candidates and give two choices by each. (Candidate list ballots can include spaces for write-in candidates as well.)
All four ballots are theoretically equivalent. The more structured ballots may aid voters in offering clear votes so they explicitly know all their choices. The Yes/No format can help to detect an "undervote" when a candidate is left unmarked and allow the voter a second chance to confirm the ballot markings are correct. The "single bubble" format is incapable of producing invalid ballots (which might otherwise be rejected in counting).
Unless the second or fourth format is used, fraudulently adding votes to an approval voting ballot will not invalidate the ballot, (that is, it will not make it appear inconsistent). Thus use of Approval voting raises the importance of ensuring that the "chain of custody" of ballots is secure.

</doc>
<doc id="1859" url="http://en.wikipedia.org/wiki?curid=1859" title="Arizona State University">
Arizona State University

Arizona State University (commonly referred to as ASU or Arizona State) is a national space-grant institution and public metropolitan research university located on several campuses spread across the Phoenix, Arizona, Metropolitan Area. It is the largest public university in the United States by enrollment. Founded in 1885 as the Territorial Normal School at Tempe, the school underwent a series of changes in name and curriculum, and in 1945 it came under control of the Arizona Board of Regents and was renamed Arizona State College. A 1958 statewide ballot measure gave the university its present name. In 1994 ASU was classified as a Research I institute; thus, making it one of the newest major research universities (public or private) in the nation. Arizona State's mission is to create a model of the âNew American Universityâ whose efficacy is measured âby those it includes and how they succeed, not by those it excludesâ. Currently, the university is ranked among the Top 25 research institutes in the U.S. in terms of research output, innovation, development, research expenditures, number of awarded patents, and awarded research grant proposals.
ASU awards bachelors, masters, and doctoral degrees, and is broadly organized into 16 colleges and schools spread across four campuses: the original Tempe campus, the West campus in northwest Phoenix, the Polytechnic campus in eastern Mesa, and the Downtown Phoenix campus. All four campuses are accredited as a single institution by the Higher Learning Commission. The University is categorized as a Research University with very high research activity (RU/VH) as reported by the Carnegie Classification of Institutions of Higher Education, with a research expenditure of $385Â million in 2012. It is one of the appointed members of the Universities Research Association, a consortium of 86 leading research-oriented universities.
ASU's athletic teams compete in Division I of the NCAA and are collectively known as the Arizona State Sun Devils. They are members of the Pacific-12 Conference and have won 23 national championships. Along with multiple athletic clubs and recreational facilities, ASU is also home to over 1,000 registered student organizations across its campuses, reflecting the diversity of the student body. With the continued growth of the student population, ASU has recently undergone numerous renovations across each of its campuses, including expansion of athletic facilities, student recreational centers and dormitories. The demand for improved facilities and more student housing on campus is being addressed with public/private investment. Currently, ASU's campus housing accommodates one of the largest residential populations in the nation with over 13,000 residents: a figure that is expected to increase each year as the university continues to build more on-campus housing.
History.
1885â1929.
Arizona State University was established as the Territorial Normal School at Tempe on March 12, 1885, when the 13th Arizona Territorial Legislature passed an act to create a normal school to train teachers for the Arizona Territory. The campus comprised a single, four-room schoolhouse on a 20-acre plot largely donated by Tempe residents George and Martha Wilson. Classes began with 33 students on February 8, 1886. The curriculum evolved over the years and the name was changed several times; the institution was also known as Arizona Territorial Normal School (1889â1896), Arizona Normal School (1896â1899), Normal School of Arizona (1899â1901), and Tempe Normal School (1901â1925). The school accepted both high school students and graduates, and awarded high school diplomas and teaching certificates to those who completed the requirements.
In 1923 the school stopped offering high school courses and added a high school diploma to the admissions requirements. In 1925 the school became the Tempe State Teachers College and offered four-year Bachelor of Education degrees as well as two-year teaching certificates. In 1929, the legislature authorized Bachelor of Arts in Education degrees as well, and the school was renamed the "'Arizona State Teachers College." Under the 30-year tenure of president Arthur John Matthews the school was given all-college student status. The first dormitories built in the state were constructed under his supervision. Of the 18 buildings constructed while Matthews was president, six are still currently in use. Matthews envisioned an "evergreen campus," with many shrubs brought to the campus, and implemented the planting of Palm Walk, now a landmark of the Tempe campus. His legacy is being continued to this day with the main campus having tbeen declared a nationally recognized arboretum.
During the Great Depression, Ralph W. Swetman was hired as president for a three-year term. Although enrollment increased by almost 100% during his tenure due to the depression, many faculty were terminated and faculty salaries were cut.
1930â1989.
In 1933, Grady Gammage, then president of Arizona State Teachers College at Flagstaff, became president of, a tenure that would last for nearly 28 years. Like his predecessor, Gammage oversaw construction of a number of buildings on the Tempe campus. He also oversaw the development of the university, graduate programs. The school's name was changed to Arizona State College in 1945, and finally to Arizona State University in 1958.
By the 1960s, with the presidency of Dr. G. Homer Durham, the University began to expand its academic curriculum by establishing several new colleges and beginning to award Doctor of Philosophy and other doctoral degrees.
The next three presidentsâHarry K. Newburn, 1969â71, John W. Schwada, 1971â81, and J. Russell Nelson, 1981â89âand Interim President Richard Peck, 1989, led the university to increased academic stature, creation of the West Campus, and rising enrollment.
1990âpresent.
Under the leadership of Dr. Lattie F. Coor, from 1990 to June 2002, ASU grew through the creation of the Polytechnic campus and extended education sites. Increased commitment to diversity, quality in undergraduate education, research, and economic development occurred over his 12-year tenure. Part of his legacy to the university was a successful fund-raising campaign: through private donations, more than $500Â million was invested in areas that would significantly impact the future of ASU. Among the campaign's achievements were the naming and endowing of Barrett, The Honors College, and the Katherine K. Herberger College of Fine Arts; the creation of many new endowed faculty positions; and hundreds of new scholarships and fellowships.
On July 1, 2002, Michael M. Crow became the university's 16th president. At his inauguration, he outlined his vision for transforming ASU into a "New American University"âone that would be open and inclusive, with a goal set for the university to meet Association of American Universities (AAU) criteria and to become a member. Furthermore, he initiated the idea of transforming ASU into "One University in Many Places" by merging ASU's several campuses into a single institution, sharing students, faculty, staff and accreditation. Aided by hundreds of millions of dollars in donations, ASU embarked on a years-long research facility capital building effort, resulting in the establishment of the Biodesign Institute and several large interdisciplinary research buildings. Along with the research facilities, the university faculty was expanded, including the addition of three Nobel Laureates. In addition, ASU's Downtown Phoenix campus was vastly expanded with several of the University's colleges and schools relocated to the downtown campus. Since fiscal year 2002 the university's research expenditures have tripled and more than 1.5Â million sq. ft. of new research space has been added to the university's research facilities.
The economic downturn that began in 2008 took a particularly hard toll on Arizona, resulting in large cuts to ASU's budget. In response to these cuts, ASU underwent several rounds of reorganizations, combining academic departments, consolidating colleges and schools, and reducing university staff and administrators; however, with an economic recovery underway in 2011, the university continued its campaign to expand the West and Polytechnic Campuses, and establishing a set of low-cost, teaching-focused extension campuses in Lake Havasu City and Payson, Arizona. The university has announced that a new building for the Sandra Day OâConnor College of Law will be built on the Downtown Phoenix Campus, relocating faculty and students from the Tempe Campus. The university plans to establish the Arizona Center for Law and Society in 2016.
Organization and Administration.
The Arizona Board of Regents governs Arizona State University as well as the other state's public universities; University of Arizona and Northern Arizona University. The Board of Regents is composed of twelve members including eleven voting, and one non-voting member. Members of the board include the Governor and the Superintendent of Public Instruction acting as ex-officio members, eight volunteer Regent members with eight years term that are appointed by the Governor, and two Student Regents with two years term, serving a one-year term as non-voting apprentices. ABOR provides policy guidance to the state universities of Arizona. ASU has multiple campus locations, covering the Phoenix metropolitan areas including the Main Tempe campus in Tempe, the West campus and Downtown Phoenix campus both in Phoenix, and the Polytechnic campus in Mesa. It also offers courses and degrees through ASU online to advance the mission of the university.
The Arizona Board of Regents appoints and elects the president of the university, who is considered the chief executive officer and the chief budget officer of the institution. The president is responsible for the execution of measures enacted by the Board of Regents, controls the property of the university, and acts as the official representative of the university to the Board of Regents. The chief executive officer is assisted through the administration of the institution by the provost, vice presidents, deans, faculty, directors, department chairs, and other officers. The president also selects and appoints administrative officers and general counsels. The 16th and current ASU president is Michael M. Crow, and has served since July 1, 2002.
Campuses and locations.
ASU's academic programs are spread across campuses in the Phoenix Metropolitan Area; however, unlike most multi-campus institutions, ASU describes itself as "one university in many places," explaining that it is "not a system with separate campuses, and not one main campus with branch campuses." The university considers each campus "distinctive" and academically focused on certain aspects of the overall university mission. The Tempe Campus is the university's research and graduate school center. Undergraduate studies on the Tempe campus are research-based programs designed to prepare students for graduate school, professional school, or employment. The Polytechnic campus is designed with an emphasis on professional and technological programs for direct workforce preparation. The Polytechnic campus is the location of many of the university's simulators and laboratories dedicated for project-based learning. The West campus is focused on interdisciplinary degrees and the liberal arts, while maintaining professional programs with a direct impact on the community and society. The Downtown Campus focuses on direct urban and public programs such as nursing, public policy, mass communication, and journalism. ASU recently relocated some nursing and health related programs to its new ASU-Mayo Medical School Campus. Inter-campus shuttles and light rail allow students and faculty to easily travel between the campuses. In addition to the physical campuses, ASU's "virtual campus", housed at the university's SkySong Innovation Center, provides online and extended education.
Tempe campus.
ASU's Tempe campus is located in downtown Tempe, Arizona, about eight miles (13Â km) east of downtown Phoenix. The campus is considered urban, and is approximately in size. The campus is arranged around broad pedestrian malls and is completely encompassed by an arboretum. The Tempe campus is also the largest of ASU's campuses, with 59,794 students enrolled in at least one class on campus.
The Tempe campus is ASU's original campus, and Old Main, the first building constructed, still stands today. There are many notable landmarks on campus, including Grady Gammage Memorial Auditorium, designed by Frank Lloyd Wright, Palm Walk, which is lined by 111 palm trees, Charles Trumbull Hayden Library, the University Club Building, Margaret Gisolo Dance Theatre, and University Bridge. In addition, the campus has an extensive public art collection, considered one of the ten best among university public art collections in America according to "Public Art Review". Against the northwest edge of campus is the Mill Avenue district (part of downtown Tempe) which has a college atmosphere that attracts many students to its restaurants and bars. The Tempe campus is also home to all of the university's athletic facilities.
West campus.
The West campus was established in 1984 by the Arizona Legislature and sits on in a suburban area of northwest Phoenix. The West campus lies about northwest of downtown Phoenix, and about northwest of the Tempe campus. The West campus is designated as a Phoenix Point of Pride, and is nearly completely powered by a 4.6MW solar array. This campus is home to the New College of Interdisciplinary Arts and Sciences, the Mary Lou Fulton Teachers College, and selected programs of the W.P. Carey School of Business.
The campus, patterned after the University of Oxfordâs architecture, has recently opened a new residence hall, dining facility and recreation center.
Polytechnic campus.
Founded in 1996 as "ASU East," the ASU Polytechnic campus serves 10,521 students and is home to more than 40 bachelorâs, masterâs and doctoral degrees in professional and technical programs through the College of Technology and Innovation, and selected programs of the W.P. Carey School of Business/Morrison School of Management and Agribusiness, Mary Lou Fulton Teachers College, the School of Letters and Sciences, and focuses on professional and technological programs including simulators and lab space in various fields of study. The campus is located in southeast Mesa, Arizona, approximately southeast of the Tempe campus, and southeast of downtown Phoenix. The Polytechnic campus sits on the former Williams Air Force Base.
Downtown Phoenix campus.
The newest of ASU's four campuses, the Downtown Phoenix campus was established in 2006 on the north side of Downtown Phoenix. The campus has an urban design, with several large modern academic buildings intermingled with commercial and retail office buildings. In addition to the new buildings, the campus included the adaptive reuse of several existing structures, including a 1930s era Post Office that is on the National Register of Historic Places. Serving 17,151 students, the campus houses the College of Health Solutions, College of Nursing and Health Innovation, College of Public Programs, Walter Cronkite School of Journalism and Mass Communication, and School of Criminology and Criminal Justice. In the summer of 2013, the campus added the Sun Devil Fitness Center in conjunction with the original YMCA building.
Colleges @ ASU.
In response to demands for lower-cost public higher education in Arizona, ASU is developing a number of small, undergraduate-only college locations throughout Arizona. Colleges @ ASU locations will be teaching-focused, and will provide a selection of popular undergraduate majors. The first is being planned for Lake Havasu City with programs available as early as fall 2012.
Online degree programs.
ASU offers more than 50 undergraduate and graduate degree programs through an entirely online platform, known as ASU Online. The degree programs delivered online hold the same accreditation as the university's traditional face-to-face programs, and students earn many of the same degrees as those who attended courses in person. Online students are taught by the same faculty and receive the same diploma as on-campus students. As of spring 2012, more than 5,000 students were enrolled at ASU Online. ASU Online is headquartered at ASU's SkySong campus in Scottsdale, Arizona. ASU Online was ranked #1 in online Student Services and Technology by U.S. News & World Report.
ASU-Mayo Medical School Campus.
In late 2011, ASU launched a collaboration with the Mayo Clinic to create a medical school. As part of the collaboration with Mayo, ASU moved some academic departments onto the Mayo Clinic campus in Scottsdale. Mayo Medical and ASU have instituted an undergraduate "Barrett-Mayo Pre-medical Scholars Program" to further the ASU Clinical partnering program mission. A partnership with organizations and hospitals throughout the region has been created. The partnership will help establish a network for knowledge sharing and testing of innovation. Real-world training for students researching medical issues affecting the community will be a priority of the school. ASU-Mayo Medical School plans to begin enrolling the first students in 2014. In preparation for the medical school opening, ASU began offering health and nursing degree programs on the Mayo Clinic Campus. The program at the ASU-Mayo Clinic Campus began in the Fall of 2012 and provides a hands-on education in world-class medical facilities to its students. After the construction of the school, unique MD degrees, believed to be the first in the nation, will be granted under the governance and oversight of Mayo Medical School and Arizona State University with a specialized masterâs degree in the Science of Health Care Management.
Academics.
Admissions.
Fall Freshman Statistics
Admission to any of the public universities in Arizona is ensured to residents in the top 25% of their high-school class with a GPA of 3.0 in core competencies. For fall 2013, ASU admitted 80.2% of all applicants and is considered a "more selective" university by U.S. News & World Report. Average GPA of high school graduates enrolling full-time is 3.42. All freshman are required to live on campus.
Barrett, The Honors College is ranked 1st in the nation among peer institutions (1300â1400 minimum SAT), 4th in Honors Factors, and 5th in Overall Excellence among all universities. Like most of ASU's colleges and schools (e.g. Walter Cronkite School of Journalism and Mass Communication, W.P. Carey School of Business, Sandra Day O'Connor College of Law, Mary Lou Fulton Teachers College, College of Nursing and Health Innovation, etc.), Barrett College maintains much more strict admissions standards. Furthermore, Barrett College provides a more rigorous curriculum with smaller classes and increased faculty interaction. Although there are no set minimum admissions criteria for Barrett College, the average GPA of incoming freshmen was 3.84, with average SAT scores of 1314/1600 and ACT scores of 29. The Honors college enrolls 4,803 undergraduate students, 613 of whom are National Merit Scholars.
ASU currently enrolls 6,474 international students, 8.4% of the total student population. The international student body represents 118 nations and more than 60 student clubs and organizations exist at ASU to serve the growing number of students from abroad. The growth in the number of international students in 2013 at ASU is a 28.5% increase over the 2012 figure.
Academic programs.
ASU offers over 250 majors to undergraduate students, and more than 100 graduate programs leading to numerous masters and doctoral degrees in the liberal arts and sciences, design and arts, engineering, journalism, education, business, law, nursing, public policy, technology, and sustainability. These programs are divided into 16 colleges and schools which are spread across ASU's four campuses. ASU uses a plus-minus grading system with highest cumulative GPA awarded of 4.0 (at time of graduation).
Rankings.
The 2014 "U.S. News & World Report" ranking of US colleges and universities ranked ASU's undergraduate program 63rd among public universities and 129th of 280 "national universities." ASU also ranked 2nd in the "Up and Coming" category of universities for making the most promising and innovative changes in the areas of academics, faculty and student life. In addition, ASU is ranked 73rd in the world and 46th in the US by the Center for World University Rankings. "Forbes" magazine named ASU one of "America's Best College Buys".
In 2012, "ASU students ranked fifth among all public universities in National Science Foundation grants for graduate study and 11th among all universities, including the schools of the Ivy League. Among other things, the high achievement in this area of excellence points to consistently strong advising and support, a logical outcome of Barrett (Arizona State University's honor college) investing more in honors staff than any other honors program that" Public University Honors reviewed.
Arizona State is ranked 5th in the nation by The Wall Street Journal for best qualified graduates. For its efforts to be a national leader in campus sustainability, ASU was named one of the top 20 "cool schools" by the Sierra Club in 2009, was named to the "Green Honor Roll" by the Princeton Review, and earned an "A-" grade on the 2010 College Sustainability Report Card.
Several of ASU's colleges and schools also appear among the top of the U.S. News & World Report rankings, including the 27th-ranked W. P. Carey School of Business (along with its 3rd-ranked program in Supply Chain Management and the 15th-ranked program in Information Systems), the 22nd-ranked Herberger Institute for Design and the Arts (along with its 7th-ranked program in Ceramics, 11th-ranked program in Photography and 5th-ranked program in Printmaking), the 12th-ranked School of Criminology and Criminal Justice, the 31st-ranked Sandra Day O'Connor College of Law (along with its 8th-ranked program in Legal Writing and 10th-ranked program in Dispute Resolution), the 43rd-ranked Ira A. Fulton School of Engineering (including five individual programs ranked in the top 30), the 16th-ranked School of Public Affairs (along with its 2nd-ranked program in City Management and Urban Policy, 10th-ranked program in Environmental Policy and Management, 16th-ranked program in Public Finance and Budgeting and 19th-ranked program in Public Management and Administration), the 18th-ranked Mary Lou Fulton Teachers College, the 21st-ranked , and 25th-ranked Healthcare Management. In addition, the individual Ph.D. programs in Materials Science and Engineering (20th), Physics (46th), Psychology (38th), Earth Science (17th), and Economics (36th) earned high rankings.
ASU's Walter Cronkite School of Journalism and Mass Communication has been ranked in the top 10 for journalism Schools by various publications and organizations over the last decade. The most recent rankings (2012) include: NewsPro (6th), Quality Education and Jobs (6th), and International Student (1st). In 2011, ASU was included in the Quacquarelli Symonds (QS) list as the 21st best school in the world for biological sciences.
Research and Institutes.
ASU consistently ranks among the top 20 universitiesâwithout a traditional medical schoolâfor research expenditures. It shares this designation with schools such as: Caltech, Georgia Tech, MIT, Purdue, Rockefeller, UC-Berkeley, and University of Texas at Austin. ASU is classified as a âRU/VH: Research University (very high research activity)â by the Carnegie Classification of Institutions of Higher Education. The university has tripled research expenditures since 2002 and now receives more than $385Â million annually. Like its research budget, the university's endowment continues to grow and now exceeds $500Â million (2013). ASU is a NASA designated national space-grant institute and a member of the Universities Research Association.
ASU is one of the nation's most successful universities in terms of creating start-up companies through research. The university attracted over $200Â million in financing during 2012, aiding in the creation of more than 55 companies. ASU ranks #2 in the nation for proprietary start-ups âcreated for every $10Â million in research expenditures.â In 2013, ASU researchers were issued 47 patents, a significant increase over 2012 when 26 patents were granted. ASU ranks 1st for Arizona Technology Transfers/Start-ups (AzTE) in fiscal year 2013: 14 AzTE Start-ups were created by all three state universities (which include Northern Arizona University and University of Arizona) and ASU accounted for 11 of those technology firms. According to the Switzerland based University Business Incubator (UBI) Index for 2013, ASU is one of the top universities in the world for business incubation, ranking 17th out of the top 25. ASU is one of only 14 universities and institutes to make the list from the United States and the only university representing Arizona. UBI reviewed 550 universities and associated business incubators from around the world using an assessment framework that takes more than 50 performance indicators into consideration. As an example, one of ASU's spin-offs (Heliae Development, LLC) raised more than $28Â million in venture capital in 2013 alone.
The university's push to create various institutes has led to greater funding and an increase in the number of researchers in multiple fields. Among the most notable and famed institutes at ASU are: The Biodesign Institute, Institute of Human Origins, L. William Seidman Research Institute (W.P. Carey School of Business), the Julie Ann Wrigley Global Institute of Sustainability, Learning Sciences Institute, Herberger Research Institute, Hispanic Research Center, and the International Institute for Species Exploration. Much of the research conducted at ASU is considered cutting edge with its focus on interdisciplinarity. The Biodesign Institute for instance, conducts research on issues such as biomedical and healthcare outcomes as part of a collaboration with the Mayo Clinic to diagnose and treat rare diseases, including cancer. Biodesign Institute researchers have also developed various techniques for reading and detecting biosignatures which expanded in 2006 with an $18Â million grant from the National Human Genome Research Institute of the National Institutes of Health. The institute also is heavily involved in sustainability research, primarily through reuse of CO2 via biological feedback and various biomasses (e.g. algae) to synthesize clean biofuels. Heliae is a Biodesign Institute spin-off and much of its business centers on Algal-derived, high value products. Furthermore, the institute is heavily involved in security research including technology that can detect biological and chemical changes in the air and water. The university has received more than $30.7Â million in funding from the Department of Defense for adapting this technology for use in detecting the presence of biological and chemical weapons.
World renowned scholars have been integral to the successes of the various institutes associated with the university. ASU students and researchers have been selected as Marshall, Truman, Rhodes, and Fulbright Scholars with the university ranking 4th for total recipients of the prestigious Fulbright Scholarship in the 2012â2013 academic year. ASU faculty includes Nobel Laureates, Royal Society members, National Academy members, and members of the National Institutes of Health, to name a few. ASU Professor Donald Johanson, who discovered the 3.18Â million year old fossil hominid Lucy (Australopithecus) in Ethiopia, established the Institute of Human Origins (IHO) in 1981. The institute was first established in Berkeley, California and later moved to ASU in 1997. As one of the leading research organization in the United States devoted to the science of human origins, IHO pursues a transdisciplinary strategy for field and analytical paleoanthropological research. The Herberger Institute Research Center supports the scholarly inquiry, applied research and creative activity of more than 400 faculty and 4,373 students. The renowned ASU Art Museum, Herberger Institute Community Programs, urban design, and other outreach and initiatives in the arts community round out the research and creative activities of the Herberger Institute. Among well known professors within the Herberger Institute is Johnny SaldaÃ±a of the School of Theatre and Film. SaldaÃ±a received the 1996 Distinguished Book Award and the prestigious Judith Kase Cooper Honorary Research Award, both from the American Alliance for Theatre Education (AATE). The Julie Ann Wrigley Global Institute of Sustainability is the center of ASU's initiatives focusing on practical solutions to environmental, economic, and social challenges. The institute has partnered with various cities, universities, and organizations from around the world to address issues affecting the global community.
ASU is also involved with NASA in the field of space exploration. In order to meet the needs of NASA programs, ASU built the LEED Gold Certified, 298,000-square-foot Interdisciplinary Science and Technology Building IV (ISTB 4) at a cost of $110Â million in 2012. The building includes space for the School of Earth and Space Exploration (SESE) and includes labs and other facilities for the Ira A. Fulton Schools of Engineering. One of the main projects at ISTB 4 includes the OSIRIS-REx Thermal Emission Spectrometer (OTES). Although ASU built the spectrometers aboard the Martian rovers Spirit and Opportunity, OTES will be the first major scientific instrument completely designed and built at ASU for a NASA space mission. Phil Christensen, the principal investigator for the Mars Global Surveyor Thermal Emission Spectrometer (TES), is a Regents' Professor at ASU. He also serves as the principal investigator for the Mars Odyssey THEMIS instruments, as well as co-investigator for the Mars Exploration Rovers. ASU scientists are responsible for the Mini-TES instruments aboard the Mars Exploration Rovers. The Center for Meteorite Studies, which is home to rare Martian meteorites and exotic fragments from space, and the Mars Space Flight Facility are both located on ASU's Tempe campus.
The Army Research Laboratory extended funding for the Arizona State University Flexible Display Center (FDC) in 2009 with a $50Â million grant. The university has partnered with the Pentagon on such endeavors since 2004 with an initial $43.7Â million grant. The universityâs FDC built the worldâs largest flexible screen color organic light emitting display (OLED) prototype using advanced mixed oxide thin-film transistors (TFTs). The technology delivers high-performance while remaining cost-effective during the manufacturing process. Vibrant colors, high switching speeds for video and reduced power consumption are some of the features the center has been able to successfully integrate into the technology. In 2012, ASU successfully eliminated the need for specialized equipment and processing, thereby reducing costs compared to competitive approaches.
Libraries.
ASU's faculty and students are served by two dedicated general-topic libraries: Hayden Library, which is the largest of the ASU libraries and is located on the Tempe campus, and Fletcher Library, located on the West campus. In addition, the Ross-Blakley Law Library and the Noble Science Library are housed in dedicated facilities on the Tempe campus. Music and Architecture collections are housed in facilities within the schools of Music and Architecture, respectively. Smaller library facilities are also located on the Polytechnic and Downtown campuses.
As of 2013, ASU's libraries held 4.5Â million volumes. In total, there are 7 libraries that service the university community. The Arizona State University library system is ranked the 34th largest research library in the United States and Canada, according to criteria established by the Association of Research Libraries that measures various aspects of quality and size of the collection. The University continues to grow its rare special collections, such as the recent addition of a privately held collection of manuscripts by poet RubÃ©n DarÃ­o.
Hayden Library is located on Cady Mall in the center of the Tempe campus. It opened in 1966 and serves as the library system's reference, periodical, and administrative center and houses the most extensive special collections in ASUâs library system. An expansion in 1989 created the subterranean entrance underneath Hayden Lawn and is attached to the above ground portion of the original library. There are two floors underneath Hayden Lawn with a landmark known as the "Beacon of Knowledge" rising form the center. The beacon is lit at night by the underground libraryâs lights.
The 2013 Capital Improvement Plan, approved by the Arizona Board of Regents, incorporates a $35Â million repurposing and renovation project for Hayden Library. The moat area that is currently open air and serves as an outdoor study space will be enclosed in order to increase indoor space for the library. Along with increasing space and renovating the facility, the front entrance of Hayden Library will be rebuilt.
Sustainability.
As of April 2013, ASU is the only institution of higher education in the United States to generate over 20 megawatts (MW) of electricity from solar arrays; an increase over the June 2012 total of 15.3 MW. ASU has 72 solar photovoltaic (PV) installations across all four campuses. The largest concentration of solar PV installations are on the Tempe campus, producing over 12.8 MW.
Additionally, there are six wind turbines installed on the roof of the Julie Ann Wrigley Global Institute of Sustainability building on the Tempe campus that have been in operation since October 2008. Under normal conditions, the six turbines produce enough electricity to power approximately 36 computers.
ASU's School of Sustainability is the first school in the United States dedicated to exploring the principles of sustainability. ASU's School of Sustainability is part of the Wrigley Global Institute of Sustainability.The School was established in spring 2007 and began enrolling undergraduates in fall 2008. ASU is also home to the Sustainability Consortium which was founded by Jay Golden in 2009.
The School of Sustainability has been essential in establishing the university as "a leader in the academics of sustainable business." The university is widely considered to be one of the most ambitious and principled organizations when it comes to embedding sustainable practices into its operating model. The university has embraced several challenging sustainability benchmarks. Among the numerous benchmarks outlined in the university's prospectus, is the creation of a large recycling and composting operation that by 2015, will eliminate 90% of the solid waste generated by all on-campus activities. This endeavor will be aided by educating students about the benefits of avoiding overconsumption that contributes to excessive waste. Sustainability courses have been expanded to attain this goal and many of the university's individual colleges and schools have integrated such material into their lectures and courses. Second, ASU is on track to reduce its rate of water consumption by 50%. The university's most aggressive benchmark is to be the first, large research university to achieve carbon neutrality as it pertains to its Scope 1, 2 and non-transportation Scope 3 greenhouse gas (GHG) emissions.
Traditions.
Maroon and Gold.
Gold is the oldest color associated with Arizona State University and dates back to 1896 when the school was named the Tempe Normal School. Maroon and white were later added to the color scheme in 1898. Gold signifies the âgolden promiseâ of ASU. The promise includes every student receiving a valuable educational experience. Gold also signifies the sunshine Arizona is famous for; including the power of the sun and its influence on the climate and the economy. The first uniforms worn by athletes associated with the university were black and white when the âNormalsâ were the name of the athletic teams. The student section, known as The Inferno, wears gold on game days.
Mascot and Spirit Squad.
Sparky the Sun Devil is the mascot of Arizona State University and was named by vote of the student body on November 8, 1946. Sparky often travels with the team across the country and has been at every football bowl game in which the university has participated in. The university's mascot is not to be confused with the universityâs new emblem and logo, The Trident, colloquially referred to as the "fork" or the "pitchfork" which is a hand gesture used by those associated with the university. The new logo and emblem are used on various university property, sport facilities, uniforms and documents. Arizona State Teacherâs College had a different mascot and the sports teams were known as the Owls and later, the Bulldogs. When the school was first established, the Tempe Normal Schoolâs teams were simply known as the Normals. Sparky is visible on the sidelines of every home game played in Sun Devil Stadium or other ASU athletic facilities. His routine at football games includes pushups after every touchdown scored by the Sun Devils. He is aided by Sparky's Crew, male yell leaders that must meet physical requirements in order to participate as members. The female members are known as the Spirit Squad and are categorized into a dance line and spirit line. They are the official squad that represents ASU. The spirit squad competes every year at the ESPN Universal Dance Association (UDA) College Nationals in the Jazz and Hip-Hop categories. They were chosen by the UDA to represent the USA at the World Dance Championship 2013 in the Jazz category. Currently, ASU's varsity intercollegiate cheerleading team is not allowed to participate at athletic events (e.g. football and basketball games) due to dismissal regarding prior misconduct. ASU Cheerleading has since become a club sport, through the Student Recreation Center, competing locally and nationally as a Collegiate Co-Ed Division IA-Level VI team. They have reestablished their commitment to excellence, winning various championships. The team has a strict code of conduct and is seeking reinstatement from the university to participate at athletic events.
âAâ Mountain.
A letter has existed on the slope of the mountain since 1918. A "T" followed by an "N" were the first letters to grace the landmark. Tempe Butte, home to "A" Mountain, has had the "A" installed on the slope of its south face since 1938 and is visible from campus just to the south. The original "A" was destroyed by vandals in 1952 with pipe bombs and a new "A", constructed of reinforced concrete, was built in 1955. The vandals were never identified but many speculate that the conspirators were students from the rival in-state university (University of Arizona). Many ancient Hohokam petroglyphs were destroyed by the bomb; nevertheless, many of these archeological sites around the mountain remain. There are many traditions surrounding "A" Mountain, including a revived "guarding of the 'A'" in which students camp on the mountainside before games with rival schools. "Whitewashing" of the "A" is a tradition in which incoming freshmen paint the letter white during orientation week. After the painting of the "A", new students learn the history of ASU and its other traditions.
Lantern Walk and Homecoming.
The Lantern Walk is one of the oldest traditions at ASU and dates back to 1917. It is considered one of ASUâs âmost cherishedâ traditions and is an occasion used to mark the work of those associated with ASU throughout history. Anyone associated with ASU is free to participate in the event, including students, alumni, faculty, employees, and friends. This differs slightly from the original tradition in which the seniors would carry lanterns up "A" Mountain followed by the freshman. The senior class president would describe ASU's traditions and the freshman would repeat an oath of allegiance to the university. It was described as a tradition of "good will between the classes" and a way of ensuring new students would continue the university's traditions with honor. In modern times, the participants walk through campus and follow a path up to âAâ Mountain in order to âlight upâ Tempe. Keynote speakers, performances, and other events are used to mark the occasion. The night is culminated with a fireworks display. The Lantern Walk was held after the Spring Semester (June) but is now held the week before Homecoming, a tradition that dates back to 1924 at ASU. It is held in the fall and in conjunction with a football game.
Victory Bell.
Arizona State University reintroduced the tradition of ringing a bell after each win for the football team in 2012. The ROTC cadets associated with the university are responsible for the transportation of the bell to various events and for ringing the bell after games are won by the Sun Devils. The first Victory Bell, in various forms, was used in the 1930s but the tradition faded in the 1970s when the bell in use was removed from Memorial Union for renovations. The bell cracked and was no longer capable of ringing. That bell is located on the southeast corner of Sun Devil Stadium near the entrance to the student section. That bell, given to the university in the late 1960s, is painted gold and is a campus landmark today.
Sun Devil Marching Band, Devil Walk and Songs of the University.
The Arizona State University Sun Devil Marching Band, created in 1915 and known as the "Pride of the Southwest", was the first of only two marching bands in the Pac-12 to be awarded the prestigious Sudler Trophy. The John Philip Sousa Foundation awarded the band the trophy in 1991. The Sun Devil Marching Band remains one of only 28 bands in the nation to have earned the designation. The band performs at every football game played in Sun Devil Stadium. Smaller ensembles of band members perform at other sport venues including basketball games at Wells Fargo Arena and baseball games at Packard Stadium. The Devil Walk is held in Wells Fargo Arena by the football team and involves a more formal introduction of the players to the community; a new approach to the tradition added in 2012 with the arrival of head coach Todd Graham. It begins 2 hours and 15 minutes prior to the game and allows the players to establish rapport with the fans. The walk ends as the team passes the band and fans lined along the path to Sun Devil Stadium. The most recognizable songs played by the band are "Alma Mater" and ASUâs fight songs titled "Maroon and Gold" and the "Al Davis Fight Song". "Alma Mater" was composed by former Music Professor and Director of Sun Devil Marching Band (then known as Bulldog Marching Band), Miles A. Dresskell, in 1937. "Maroon and Gold" was authored by former Director of Sun Devil Marching Band, Felix E. McKernan, in 1948. The "Al Davis Fight Song" (also known as "Go, Go Sun Devils" and "Arizona State University Fight Song") was composed by ASU alumnus Albert Oliver Davis in the 1940s without any lyrics. Recently lyrics were added to the song.
Student life.
Extracurricular programs.
Arizona State University has an active extracurricular involvement program (Sun Devil Involvement Center). Located on the 3rd floor of the Memorial Union, the Sun Devil Involvement Center (SDIC) provides opportunities for student involvement through clubs, sororities, fraternities, community service, leadership, student government, and co-curricular programming.
Changemaker Central is student-run centralized resource hub for student involvement in social entrepreneurship, civic engagement, service learning and community service that catalyzes student-driven social change. Changemaker Central locations have opened on all campuses in Fall 2011, providing flexible, creative workspaces for everyone in the ASU community. The project is entirely student run and advances ASUâs institutional commitments to social embeddedness and entrepreneurship. The space allows students to meet, work and join new networks and collaborative enterprises while taking advantage of ASUâs many resources and opportunities for engagement. Changemaker Central has signature programs, including Innovation Challenge and 10,000 Solutions, that support students in their journey to become changemakers by creating communities of support around new solutions/ideas and increasing access to early stage seed funding. The Innovation Challenge seeks undergraduate and graduate students from across the university who are dedicated to making a difference in our local and global communities through innovation. Students can win up to $10,000 to make their innovative project, prototype, venture or community partnership ideas happen. The 10,000 Solutions Project leverages the power of collaborative imagination and innovation to create a solutions bank. As an experimental problem solving platform, the project showcases and collects ideas at scale with local and global impact. The 10,000 Solutions Project aims to see what can be accomplished when passionate people join a collaborative community that builds upon each otherâs innovative ideas.
In addition to Changemaker Central, the Freshman Year Residential Experience (FYRE) and the Greek community (Greek Life) at Arizona State University have been important in binding students to the university, and providing social outlets. The Freshman Year Residential Experience at Arizona State University was developed to improve the freshman experience at Arizona State University and increase student retention figures. FYRE provides advising, computer labs, free walk-in tutoring, workshops, and classes for students. In 2003, "U.S. News & World Report" ranked FYRE as the 23rd best first year program in the nation. ASU is also home to one of the nation's first and fastest growing gay fraternities, Sigma Phi Beta, founded in 2003; considered a sign of the growing university's commitment to supporting diversity and inclusion.
The second Eta chapter of Phrateres, a non-exclusive, non-profit social-service club, was installed here in 1958. Between 1924 and 1967, 23 chapters of Phrateres were installed in universities across North America.
Student media.
"The State Press" is a daily paper published on Monday through Friday during the fall and spring semesters, and weekly during the summer sessions. The State Press covers news and events on all four ASU campuses. Student editors and managers are solely responsible for the content of the State Press newspaper and its associated website. These publications are overseen by an independent board and guided by a professional adviser employed by the University.
"The Downtown Devil" is another student-run newspaper with an associated website for the Downtown Phoenix Campus, produced by students at the Walter Cronkite School of Journalism and Mass Communication.
ASU has two radio stations. KASC The Blaze 1330Â AM, is a broadcast station that is owned and funded by the Cronkite School of Journalism, and is completely student-run save for a faculty and professional adviser. The Blaze broadcasts local, alternative and independent music 24 hours a day, and also features news and sports updates at the top and bottom of every hour. W7ASU is an amateur radio station that was first organized in 1935. W7ASU has about 30 members that enjoy amateur radio, and is primarily a contesting club.
Student government.
Associated Students of Arizona State University (ASASU) is the student government at Arizona State University. It is composed of the Undergraduate Student Government and the Graduate & Professional Student Association (GPSA). Members and officers of ASASU are elected annually by the student body.
The Residence Hall Association (RHA) of Arizona State University is the student government for every ASU student living on-campus. Each ASU campus has an RHA that operates independently of each other. The purpose of RHA is to enhance the quality of residence hall life and provide a cohesive voice for the residents by addressing the concerns of the on-campus populations to university administrators and other campus organizations; providing cultural, diversity, educational, and social programming; establishing and working with individual community councils.
Athletics.
Arizona State University's Division I athletic teams are called the Sun Devils, which is also the nickname used to refer to students and alumni of the university. They compete in the Pac-12 Conference in 20 varsity sports. Historically, the university has highly performed in men's, women's, and mixed archery; men's, women's, and mixed badminton; women's golf; women's swimming and diving; baseball; and football. Arizona State University's NCAA Division I-A program competes in 9 varsity sports for men and 11 for women. ASU's current athletic director is Steve Patterson, who was appointed to the position in 2012 after Lisa Love, the former Senior Associate Athletic Director at the University of Southern California, was relieved of her duties. Love was responsible for the hiring of coaches Herb Sendek, the men's basketball coach, and Dennis Erickson, the men's football coach. Erickson was fired in 2011 and replaced by Todd Graham.
ASU has won 23 national collegiate team championships in the following sports: baseball (5), men's golf (2), women's golf (7), men's gymnastics (1), softball (2), men's indoor track (1), women's indoor track (2), men's outdoor track (1), women's outdoor track (1), and wrestling (1).
In September 2009 criticism over the seven-figure salaries earned by various coaches at Arizona's public universities (including ASU) prompted the Arizona Board of Regents to re-evaluate the salary and benefit policy for athletic staff. With the 2011 expansion of the Pacific-12 Conference, a new $3Â billion contract for revenue sharing among all the schools in the conference was established. With the infusion of funds, the salary issue and various athletic department budgeting issues at ASU were addressed. The Pac-12's new media contract with ESPN allowed ASU to hire a new coach in 2012. A new salary and bonus package (maximum bonus of $2.05Â million) was instituted and is one of the most lucrative in the conference. ASU also plans to expand its athletic facilities with a public-private investment strategy to create an amateur sports district that can accommodate the Pan American Games and operate as an Olympic Training Center. The athletic district will include a $300Â million renovation of Sun Devil Stadium that will include new football facilities. The press box and football offices in Sun Devil Stadium were remodeled in 2012.
Arizona State Sun Devils football was founded in 1897 under coach Fred Irish. Currently, the team has played in the 2012 Fight Hunger Bowl, the 2011 Las Vegas bowl, and the 2007 Holiday Bowl. The Sun Devils played in the 1997 Rose Bowl and won the Rose Bowl in 1987. The team has appeared in the Fiesta Bowl in 1983, 1977, 1975, 1973, 1972, and 1971 winning 5 of 6. In 1970 and 1975 they were champions of the NCAA Division I FBS National Football Championship. The Sun Devils were Pac-12 Champions in 1986, 1996, and 2007. Altogether, the football team has 17 Conference Championships and has participated in a total of 26 bowl games as of 2012.
The university also participates in the American Collegiate Hockey Association (ACHA) and is billed as the top program within that league. Beginning in 2013, ASU will be a founding member of the new Western Collegiate Hockey League (WCHL). ASU Sun Devils Hockey will compete with NCAA Division 1 schools for the first time in 2012, largely due to the success of the program.
People.
Alumni.
Arizona State University has produced over 300,000 alumni worldwide. The university has produced many notable figures over its 125-year history, including: U.S. Senator Carl Hayden (who was instrumental in the growth of Central Arizona), former Congressman Barry Goldwater, Jr., and Silver Star recipient Pat Tillman who left his professional football career to enlist in the United States Army in the aftermath of the September 11, 2001 terrorist attacks. Other notable alumni include current U.S. Congressional Representatives Ed Pastor, Harry Mitchell, Kyrsten Sinema (the first openly bisexual person elected to Congress), among others; Eric Crown, CEO and co-founder of Insight Enterprises, Inc.; Ira A. Fulton, philanthropist and founder of Fulton Homes; Craig Weatherup, former Chairman of PepsiCo; Kate Spade, namesake and co-Founder of Kate Spade New York; Larry Carter, CFO of Cisco Systems; Doug Ducey, former partner and CEO of Coldstone Creamery and the 32nd Treasurer of Arizona; and Scott Smith, the current mayor of the City of Mesa.
In addition to Pat Tillman (football), ASU has had many famous athletes attend the school. Those athletes include: Phil Mickelson (golf), Reggie Jackson (baseball), Barry Bonds (baseball), James Harden (basketball), and Terrell Suggs (football). ASU alumni that are enshrined in the Pro Football Hall of Fame include: Curley Culp, Mike Haynes, John Henry Johnson, Randall McDaniel, and Charley Taylor. Other notable athletes that attended ASU are Dustin Pedroia (baseball), Jake Plummer (football), Danny White (football), Lionel Hollins (basketball), Fat Lever (basketball), and Byron Scott (basketball).
Famous celebrities include: television host and comedian Jimmy Kimmel; comedian and the first host of The Tonight Show, Steve Allen; filmmaker and screenwriter John Hughes (National Lampoon's Vacation, Ferris Bueller's Day Off, The Breakfast Club, Planes, Trains and Automobiles, Uncle Buck, and Home Alone); actor David Spade; actress and singer Lynda Carter; and actor Tyler Hoechlin from MTV's Teen Wolf. Influential writers and novelists include: Allison Dubois, whose novels and work with various law enforcement agencies inspired the TV miniseries "Medium"; novelist Amanda Brown; author, speaker and spiritual teacher Howard Falco; and best-selling author and Doctor of Animal Science Temple Grandin, whose work inspired the film, "Temple Grandin" starring Claire Danes. Journalists and commentators include: Al Michaels, NBC Sports' play-by-play commentator including the National Football League's broadcast of "Monday Night Football" and Jerry Dumas, writer, essayist, cartoonist, and a columnist for the "Greenwich Time" and best known for his "Sam and Silo" comic strip. Radio host and author Michael Reagan, the son of President Ronald Reagan and actress Jan Wyman, also attended ASU.
Among American research universities, Arizona State is ranked 4th for total recipients of the prestigious Fulbright Scholarship in the 2012â2013 academic year. ASU has made this list for more than 9 consecutive years. ASU alumni and students are also noted for their service to the community and have officially been recognized as a top university for contributing to the public good. The Arizona State University Alumni Association is located on the Tempe campus in Old Main. The Alumni Association is responsible for continuing many of the traditions of the university.
Faculty.
ASU faculty have included former CNN host Aaron Brown, meta-analysis developer Gene V. Glass, feminist and author Gloria Feldt, physicist Paul Davies, and Pulitzer Prize winner and "The Ants" coauthor Bert HÃ¶lldobler. Donald Johanson, who discovered the 3.18Â million year old fossil hominid Lucy (Australopithecus) in Ethiopia, is also a professor at ASU, as well as George Poste, Chief Scientist for the Complex Adaptive Systems Initiative. Current Nobel laureate faculty include Leland Hartwell, and Edward C. Prescott. On June 12, 2012 Elinor Ostrom, ASU's third Nobel laureate, died at the age of 78.
ASU faculty's achievements as of 2012 include:
Sexual assault investigation.
On May 1, 2014, ASU was listed as one of fifty five higher education institutions under investigation by the Office of Civil Rights "for possible violations of federal law over the handling of sexual violence and harassment complaints" by Barack Obama's White House Task Force To Protect Students from Sexual Assault. The publicly announced investigation comes after two previous Title IX suits. In July 2014, a group of at least nine current and former students who alleged that they were harassed or assaulted asked that the federal investigation be expanded.

</doc>
<doc id="1862" url="http://en.wikipedia.org/wiki?curid=1862" title="April 14">
April 14


</doc>
<doc id="1864" url="http://en.wikipedia.org/wiki?curid=1864" title="Astoria, Oregon">
Astoria, Oregon

Astoria is the seat of Clatsop County, Oregon, United States. Situated near the mouth of the Columbia River, the city was named after the American investor John Jacob Astor. His American Fur Company founded Fort Astoria at the site in 1811. Astoria was incorporated by the Oregon Legislative Assembly on October 20, 1876.
Located on the south shore of the Columbia, the city is served by the Port of Astoria with a deep-water port. Transportation includes the Astoria Regional Airport with U.S. Route 30 and U.S. Route 101 as the main highways, and the AstoriaâMegler Bridge connecting to neighboring Washington across the river. The population was 9,477 at the 2010 census.
History.
The Lewis and Clark Expedition spent the winter of 1805â1806 at Fort Clatsop, a small log structure south and west of modern-day Astoria. The expedition had hoped a ship would come by to take them back east, but instead endured a torturous winter of rain and cold, then returned east the way they came. Today the fort has been recreated and is now a historical park.
In 1810, John Jacob Astor's Pacific Fur Company sent the Astor Expedition that founded Fort Astoria as its primary fur-trading post in the Northwest, and in fact the first permanent U.S. settlement on the Pacific coast. It was an extremely important post for American exploration of the continent and was influential in establishing American claims to the land. Fort Astoria was constructed in 1811.
British explorer David Thompson was the first European to navigate the entire length of the Columbia River in 1811. Thompson reached the partially constructed Fort Astoria at the mouth of the Columbia, arriving two months after the Pacific Fur Company's ship, the "Tonquin".
The Pacific Fur Company failed, however, and the fort and fur trade were sold to the British in 1813. The house was restored to the U.S. in 1818, though the fur trade would remain under British control until American pioneers following the Oregon Trail began filtering into the port town in the mid-1840s. The Treaty of 1818 established joint U.S. â British occupancy of territory west of the continental divide to the Pacific Ocean. In 1846 the Oregon Treaty ended the Oregon Boundary Dispute; with Britain ceding all right to the mainland south of the 49th parallel north.
Washington Irving, a prominent American writer with a European reputation, was approached by John Jacob Astor to mythologize the three-year reign of his Pacific Fur Company. "Astoria" (1835), written while Irving was Astor's guest, cemented the importance of the region in the American psyche. In Irving's words, the fur traders were "Sinbads of the wilderness", and their venture was a staging point for the spread of American economic power into both the continental interior and into the Pacific.
As the Oregon Territory grew and became increasingly more settled, Astoria likewise grew as a port city at the mouth of the great river that provided the easiest access to the interior. The first U.S. Post Office west of the Rocky Mountains was established in Astoria in 1847. In 1876, the community was incorporated by the state.
Astoria attracted a host of immigrants beginning in the late 19th century: Nordic settlers, primarily Finns, and Chinese soon became significant parts of the population. The Finns mostly lived in Uniontown, near the present-day end of the AstoriaâMegler Bridge, and took fishing jobs; the Chinese tended to do cannery work, and usually lived either downtown or in bunkhouses near the canneries. In 1883, and again in 1922, downtown Astoria was devastated by fire, partly because it was mostly wood and entirely raised off the marshy ground on pilings. Even after the first fire, the same format was used, and the second time around the flames spread quickly again, as collapsing streets took out the water system. Frantic citizens resorted to dynamite, blowing up entire buildings to stop the fire from going further.
Astoria has served as a port of entry for over a century and remains the trading center for the lower Columbia basin, although it has long since been eclipsed by Portland, Oregon, and Seattle, Washington, as an economic hub on the coast of the Pacific Northwest. Astoria's economy centered on fishing, fish processing, and lumber. In 1945, about 30 canneries could be found along the Columbia; however, in 1974 Bumblebee Seafood moved its headquarters out of Astoria, and gradually reduced its presence until 1980 when the company closed its last Astoria cannery. The timber industry likewise declined; Astoria Plywood Mill, the city's largest employer, closed in 1989, and the Burlington Northern and Santa Fe Railway discontinued service in 1996.
From 1921 to 1966, a ferry route across the Columbia River connected Astoria with Pacific County, Washington. In 1966 the AstoriaâMegler Bridge was opened; it completed U.S. Route 101 and linked Astoria with Washington on the opposite shore of the Columbia, and replaced the ferries.
Today, tourism, Astoria's growing art scene, and light manufacturing are the main economic activities of the city. It is a port of call for cruise ships since 1982, after $10 million in pier improvements to accommodate cruise ships. To avoid Mexican ports of call during the Swine Flu outbreak of 2009, many cruises were re-routed to include Astoria. The residential community "The World" visited Astoria in June 2009.
In addition to the replicated Fort Clatsop, a popular point of interest is the Astoria Column, a tower high, built atop Coxcomb Hill above the town, with an inner circular staircase allowing visitors to climb to see a panoramic view of the town, the surrounding lands, and the Columbia flowing into the Pacific. The column was built by the Astor family in 1926 to commemorate the region's early history.
Since 1998, artistically-inclined fishermen and women from Alaska and the Pacific Northwest have traveled to Astoria for the Fisher Poets Gathering, where poets and singers tell their tales to honor the fishing industry and lifestyle.
Astoria is also the western terminus of the TransAmerica Trail, a bicycle touring route created by the American Cycling Association.
Astoria is home to three United States Coast Guard ships: the "Steadfast", "Alert", and "Fir".
Geography and climate.
According to the United States Census Bureau, the city has a total area of , of which, is land and is water.
Climate.
Astoria lies within the Marine west coast climate zone (KÃ¶ppen "Cfb"), with very mild temperatures year-round, some of the most consistent in the contiguous United States; winters are mild for this latitude (it usually remains above freezing at night) and wet. Summers are cool, although short heat waves can occur. Rainfall is most abundant in late fall and winter and is lightest in July and August. Snowfall is relatively rare, occurring in only three-fifths of years. Nevertheless, when conditions are ripe, significant snowfalls can occur.
Astoria is tied with Lake Charles, Louisiana, and Port Arthur, Texas, as the most humid city in the contiguous United States. The average relative humidity in Astoria is 89% in the morning and 73% in the afternoon.
Annually, there is an average of only 4.2 days with temperatures reaching or higher, and readings are rare. Normally there are only one or two nights per year when the temperature remains at or above . There are an average of 31 days with minimum temperatures at or below the freezing mark. The record high temperature was on July 1, 1942. The record low temperature was on December 8, 1972, and on December 21, 1990.
There are an average of 191 days with measurable precipitation. The wettest year was 1950 with and the driest year was 1985 with . The most rainfall in one month was in December 1933, and the most in 24 hours was on November 25, 1998. The most snowfall in one month was in January 1950, and the most snow in 24 hours was on December 11, 1922.
Demographics.
2010 census.
As of the census of 2010, there were 9,477 people, 4,288 households, and 2,274 families residing in the city. The population density was . There were 4,980 housing units at an average density of . The racial makeup of the city was 89.2% White, 0.6% African American, 1.1% Native American, 1.8% Asian, 0.1% Pacific Islander, 3.9% from other races, and 3.3% from two or more races. Hispanic or Latino of any race were 9.8% of the population.
There were 4,288 households of which 24.6% had children under the age of 18 living with them, 37.9% were married couples living together, 10.8% had a female householder with no husband present, 4.3% had a male householder with no wife present, and 47.0% were non-families. 38.8% of all households were made up of individuals and 15.1% had someone living alone who was 65 years of age or older. The average household size was 2.15 and the average family size was 2.86.
The median age in the city was 41.9 years. 20.3% of residents were under the age of 18; 8.6% were between the ages of 18 and 24; 24.3% were from 25 to 44; 29.9% were from 45 to 64; and 17.1% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.
2000 census.
As of the census of 2000, there were 9,813 people, 4,235 households, and 2,469 families residing in the city. The population density was 1,597.6 people per square mile (617.1 per kmÂ²). There were 4,858 housing units at an average density of 790.9 per square mile (305.5 per kmÂ²). The racial makeup of the city was:
5.98% of the population were Hispanic American or Latino of any race.
14.2% were of German, 11.4% Irish, 10.2% English, 8.3% United States or American, 6.1% Finnish, 5.6% Norwegian, and 5.4% Scottish ancestry according to Census 2000.
There were 4,235 households out of which 28.8% had children under the age of 18 living with them, 43.5% were married couples living together, 11.2% had a female householder with no husband present, and 41.7% were non-families. 35.4% of all households were made up of individuals and 13.6% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 2.93.
In the city the population was spread out with:
The median age was 38 years. For every 100 females there were 92.3 males. For every 100 females age 18 and over, there were 89.9 males.
The median income for a household in the city was $33,011, and the median income for a family was $41,446. Males had a median income of $29,813 versus $22,121 for females. The per capita income for the city was $18,759. About 11.6% of families and 15.9% of the population were below the poverty line, including 22.0% of those under age 18 and 9.6% of those age 65 or over.
Education.
The Astoria School District has four primary and secondary schools, including Astoria High School. Clatsop Community College is the city's two-year college. It also has a library and many parks with historical significance. As well as the second oldest Job Corps facility, Tongue Point Job Corps.
Media.
"The Daily Astorian" is a newspaper serving Astoria, the local NPR station is KMUN 91.9, and KAST 1370 is a local news-talk radio station. The "Coast River Business Journal" is a monthly business magazine covering Astoria, Clatsop County, and the Northwest Oregon coast.
In popular culture.
"Shanghaied in Astoria" is a musical about Astoria's history that has been performed in Astoria every year since 1984.
Astoria was the setting of the 1985 movie "The Goonies", which was filmed on location. Other movies filmed in Astoria include "Short Circuit", "The Black Stallion", "Kindergarten Cop", "Free Willy", "", "Teenage Mutant Ninja Turtles III", "Benji the Hunted", "The Ring Two", "Into the Wild", "The Guardian" and "Cthulhu".
The early 1960s television series "Route 66" filmed the episode entitled "One Tiger to a Hill" in Astoria; it was broadcast on September 21, 1962.
The fourth full-length album by the pop punk band The Ataris was named "So Long, Astoria" as an allusion to "The Goonies". "So Long, Astoria" is also the first track on the album. The rear album art also features news clippings such as a picture of the port's water tower from an article from 2002 of the water tower being pulled down.
The television series Dexter finale has the serial killer leave his son and partner and move to Astoria to work as a lumberjack.
Sister cities.
Astoria has one sister city, as designated by Sister Cities International:
Warships named "Astoria".
Two US Navy Cruisers were named USS "Astoria": a heavy cruiser (CA-34) and a light cruiser (CL-90). The former was lost in combat in August 1942 at the World War II Pacific ocean Battle of Savo Island, and the latter was scrapped in 1971 after being removed from active duty in 1949.

</doc>
<doc id="1866" url="http://en.wikipedia.org/wiki?curid=1866" title="Alarums and Excursions">
Alarums and Excursions

Alarums and Excursions (A&E) is an amateur press association started in June 1975 by Lee Gold (at the request of Bruce Pelz, who felt that discussion of "Dungeons & Dragons" was taking up too much space in Apa-L, the APA of the Los Angeles Science Fantasy Society). It was the first publication to focus solely on role-playing games. 
Each issue is a collection of contributions from different authors, often featuring game design discussions, rules variants, write-ups of game sessions, reviews, and comments on others contributions. It was a four-time winner of the Charles Roberts/Origins Award, winning "Best Amateur Adventure Gaming Magazine" in 1984, "Best Amateur Game Magazine" in 1999, and "Best Amateur Game Periodical" in 2000 and 2001.
Although game reports and social reactions are common parts of many "A&E" contributions, it has also, over the years, become a testing ground for new ideas on the development of the RPG as a genre and an art form. The idea that role-playing games "are" an art form took strong root in this zine, and left a lasting impression on many of the RPG professionals who contributed.
The July 2013 collation of "Alarums and Excursions" was #466.
Over the years, contributors have included: 
The role-playing game "Over the Edge" was inspired by discussions in "A&E".
"Alarums and excursions" is a stage direction for the moving of soldiers across a stage, used in Elizabethan drama.

</doc>
<doc id="1869" url="http://en.wikipedia.org/wiki?curid=1869" title="Alfred Jarry">
Alfred Jarry

Alfred Jarry (; 8 September 1873 â 1 November 1907) was a French symbolist writer best known for coining the term and philosophical concept of 'pataphysics.
Born in Laval, Mayenne, France, not far from the border of Brittany, he was of Breton descent on his mother's side. In his lifetime, though associated with the Symbolist movement, Jarry was best known for his play "Ubu Roi" (1896), which is often cited as a forerunner to the Surrealist and Futurist movements of the 1920s and 1930s. Jarry wrote in a variety of hybrid genres and styles, prefiguring the Postmodern. He wrote plays, novels, poetry, essays and speculative journalism. His texts present us with pioneering work in the fields of absurdist literature and postmodern philosophy.
Biography and works.
A precociously brilliant student, Jarry enthralled his classmates with a gift for pranks and troublemaking.
At the lycÃ©e in Rennes when he was 15, he led a group of boys who devoted much time and energy to poking fun at their well-meaning, but obese and incompetent physics teacher, a man named HÃ©bert. Jarry and classmate Henri Morin wrote a play they called "Les Polonais" and performed it with marionettes in the home of one of their friends. The main character, "PÃ¨re Heb", was a blunderer with a huge belly; three teeth (one of stone, one of iron, and one of wood); a single, retractable ear; and a misshapen body. In Jarry's later work "Ubu Roi", PÃ¨re Heb would develop into Ubu, one of the most monstrous and astonishing characters in French literature.
At 17 Jarry passed his baccalaurÃ©at and moved to Paris to prepare for admission to the Ãcole Normale SupÃ©rieure. Though he was not admitted, he soon gained attention for his original poems and prose-poems. A collection of his work, "Les minutes de sable mÃ©morial", was published in 1893.
That same year, both his parents died, leaving him a small inheritance which he quickly spent.
Jarry had meantime discovered the pleasures of alcohol, which he called "my sacred herb" or, when referring to absinthe, the "green goddess". A story is told that he once painted his face green and rode through town on his bicycle in its honour (and possibly under its influence).
When he was drafted into the army in 1894, his gift for turning notions upside down defeated attempts to instill military discipline. The sight of the small man in a uniform much too large for his less than 5-foot frameâthe army did not issue uniforms small enoughâwas so disruptively funny that he was excused from parades and marching drills. Eventually the army discharged him for medical reasons. His military experience eventually inspired the novel "Days and Nights".
Jarry returned to Paris and applied himself to writing, drinking, and the company of friends who appreciated his witty, sweet-tempered, and unpredictable conversation. This period is marked by his intense involvement with Remy de Gourmont in the publication of "L'Ymagier", a luxuriously produced "art" magazine devoted to the symbolic analysis of medieval and popular prints. Symbolism as an art movement was in full swing at this time and "L'Ymagier" provided a nexus for many of its key contributors. Jarry's play "Caesar Antichrist" (1895) drew on this movement for material. This is a work that bridges the gap between serious symbolic meaning and the type of critical absurdity with which Jarry would soon become associated. Using the biblical Book of Revelation as a point of departure, "Caesar Antichrist" presents a parallel world of extreme formal symbolism in which Christ is resurrected not as an agent of spirituality but as an agent of the Roman Empire that seeks to dominate spirituality. It is a unique narrative that effectively links the domination of the soul to contemporaneous advances in the field of Egyptology such as the 1894 excavation of the Narmer Palette, an ancient artifact used for situating the rebus within hermeneutics.
The spring of 1896 saw the publication, in Paul Fort's review "Le Livre d'art", of Jarry's 5-act play "Ubu Roi"âthe rewritten and expanded "Les Polonais" of his school days. "Ubu Roi"'s savage humor and monstrous absurdity, unlike anything thus far performed in French theater, seemed unlikely to ever actually be performed on stage. However, impetuous theater director AurÃ©lien-Marie LugnÃ©-Poe took the risk, producing the play at his ThÃ©Ã¢tre de l'Oeuvre.
On opening night (10 December 1896), with traditionalists and the avant-garde in the audience, King Ubu (played by Firmin GÃ©mier) stepped forward and intoned the opening word, "Merdre!" (often translated as "Pshit" or "Shittr!" in English). A quarter of an hour of pandemonium ensued: outraged cries, booing, and whistling by the offended parties, countered by cheers and applause by the more degenerate contingent. Such interruptions continued through the evening. At the time, only the dress rehearsal and opening night performance were held, and the play was not revived until after Jarry's death.
The play brought fame to the 23-year-old Jarry, and he immersed himself in the fiction he had created. GÃ©mier had modeled his portrayal of Ubu on Jarry's own staccato, nasal vocal delivery, which emphasized each syllable (even the silent ones). From then on, Jarry would always speak in this style. He adopted Ubu's ridiculous and pedantic figures of speech; for example, he referred to himself using the royal "we", and called the wind "that which blows" and the bicycle he rode everywhere "that which rolls".
Jarry moved into a flat which the landlord had created through the unusual expedient of subdividing a larger flat by means of a horizontal rather than a vertical partition. The diminutive Jarry could just manage to stand up in the place, but guests had to bend or crouch. Jarry also took to carrying a loaded revolver. In response to a neighbor's complaint that his target shooting endangered her children, he replied, "If that should ever happen, ma-da-me, we should ourselves be happy to get new ones with you".
With Franc-Nohain and Claude Terrasse, he co-founded the ThÃ©atre des Pantins, which in 1898 was the site of marionette performances of "Ubu Roi".
Living in worsening poverty, neglecting his health, and drinking excessively, Jarry went on to write what is often cited "" as the first cyborg sex novel, "Le SurmÃ¢le" ("The Supermale"), which is partly a satire on the Symbolist ideal of self-transcendence.
Unpublished until after his death, his fiction "Exploits and Opinions of Dr. Faustroll, Pataphysician" ("Gestes et opinions du docteur Faustroll, pataphysicien") describes the exploits and teachings of a sort of antiphilosopher who, born at age 63, travels through a hallucinatory Paris in a sieve and subscribes to the tenets of "'pataphysics". 'Pataphysics deals with "the laws which govern exceptions and will explain the universe supplementary to this one". In 'pataphysics, every event in the universe is accepted as an extraordinary event.
Jarry once wrote, expressing some of the bizarre logic of 'pataphysics, "If you let a coin fall and it falls, the next time it is just by an infinite coincidence that it will fall again the same way; hundreds of other coins on other hands will follow this pattern in an infinitely unimaginable fashion".
In his final years, he was a legendary and heroic figure to some of the young writers and artists in Paris. Guillaume Apollinaire, AndrÃ© Salmon, and Max Jacob sought him out in his truncated apartment. After his death, Pablo Picasso, fascinated with Jarry, acquired his revolver and wore it on his nocturnal expeditions in Paris, and later bought many of his manuscripts as well as executing a fine drawing of him.
Jarry lived in his 'pataphysical world' until his death in Paris on 1 November 1907 of tuberculosis, aggravated by drug and alcohol use. It is recorded that his last request was for a toothpick. He was interred in the CimetiÃ¨re de Bagneux, near Paris.
The complete works of Alfred Jarry are published in three volumes by Gallimard in the collection "BibliothÃ¨que de la PlÃ©iade".

</doc>
<doc id="1870" url="http://en.wikipedia.org/wiki?curid=1870" title="Amalric">
Amalric

Amalric or Amalaric (also Americ, Almerich, Emeric, Emerick and other variations) is a personal name derived from the tribal name "Amal" (referring to the Gothic Amali) and "ric" (Gothic "reiks") meaning "ruler, prince". 
Equivalents in different languages include:

</doc>
<doc id="1871" url="http://en.wikipedia.org/wiki?curid=1871" title="Amalric I of Jerusalem">
Amalric I of Jerusalem

Amalric of Jerusalem (also Amaury) (1136 â 11 July 1174) was King of Jerusalem 1163â1174, and Count of Jaffa and Ascalon before his accession. Amalric was the second son of Melisende of Jerusalem and Fulk of Jerusalem, and succeeded his older brother Baldwin III. During his reign, Jerusalem became more closely allied with the Byzantine Empire, and the two states launched an unsuccessful invasion of Egypt. Meanwhile, the Muslim territories surrounding Jerusalem began to be united under Nur ad-Din and later Saladin. He was the father of three future rulers of Jerusalem, Sibylla, Baldwin IV, and Isabella I.
Note: Older scholarship mistook the two names Amalric and Aimery as variant spellings of the same name, so these historians erroneously added numbers: Amalric I, r. 1163-1174 and Amalric II, r. 1197-1205. Now scholars recognize that the two names were not the same and no longer add the number for either king: Amalric, r. 1163-1174 and Aimery, r. 1197-1205.
Youth.
Amalric was born in 1136 to King Fulk, the former count of Anjou who had married the heiress of the kingdom, Melisende, daughter of King Baldwin II. After the death of Fulk in a hunting accident in 1143, the throne passed jointly to Melisende and Amalric's older brother Baldwin III, who was still only 13 years old. Melisende did not step down when Baldwin came of age two years later, and by 1150 the two were becoming increasingly hostile towards each other. In 1152 Baldwin had himself crowned sole king, and civil war broke out, with Melisende retaining Jerusalem while Baldwin held territory further north. Amalric, who had been given the County of Jaffa as an apanage when he reached the age of majority in 1151, remained loyal to Melisende in Jerusalem, and when Baldwin invaded the south, Amalric was besieged in the Tower of David with his mother. Melisende was defeated in this struggle and Baldwin ruled alone thereafter. In 1153 Baldwin captured the Egyptian fortress of Ascalon, which was then added to Amalric's fief of Jaffa (see Battle of Ascalon).
Amalric married Agnes of Courtenay in 1157. Agnes, daughter of Joscelin II of Edessa, had lived in Jerusalem since the western regions of the former crusader County of Edessa were lost in 1150. Patriarch Fulcher objected to the marriage on grounds of consanguinity, as the two shared a great-great-grandfather, Guy I of MontlhÃ©ry, and it seems that they waited until Fulcher's death to marry. Agnes bore Amalric three children: Sibylla, the future Baldwin IV (both of whom would come to rule the kingdom in their own right), and Alix, who died in childhood.
Succession.
Baldwin III died on 10 February 1163 and the kingdom passed to Amalric, although there was some opposition among the nobility to Agnes; they were willing to accept the marriage in 1157 when Baldwin III was still capable of siring an heir, but now the "Haute Cour" refused to endorse Amalric as king unless his marriage to Agnes was annulled. The hostility to Agnes, it must be admitted, may be exaggerated by the chronicler William of Tyre, whom she prevented from becoming Latin Patriarch of Jerusalem decades later, as well as from William's continuators like Ernoul, who hints at a slight on her moral character: "car telle n'est que roine doie iestre di si haute cite comme de Jherusalem" ("there should not be such a queen for so holy a city as Jerusalem"). Nevertheless, consanguinity was enough for the opposition. Amalric agreed and ascended the throne without a wife, although Agnes continued to hold the title Countess of Jaffa and Ascalon and received a pension from that fief's income. Agnes soon thereafter married Hugh of Ibelin, to whom she had been engaged before her marriage with Amalric. The church ruled that Amalric and Agnes' children were legitimate and preserved their place in the order of succession. Through her children Agnes would exert much influence in Jerusalem for almost 20 years.
Conflicts with the Muslim states.
During Baldwin III's reign, the County of Edessa, the first crusader state established during the First Crusade, was conquered by Zengi, the Turkic emir of Aleppo. Zengi united Aleppo, Mosul, and other cities of northern Syria, and intended to impose his control on Damascus in the south. The Second Crusade in 1148 had failed to conquer Damascus, which soon fell to Zengi's son Nur ad-Din. Jerusalem also lost influence to Byzantium in northern Syria when the Empire imposed its suzerainty over the Principality of Antioch. Jerusalem thus turned its attention to Egypt, where the Fatimid dynasty was suffering from a series of young caliphs and civil wars. The crusaders had wanted to conquer Egypt since the days of Baldwin I, who died during an expedition there. The capture of Ascalon by Baldwin III made the conquest of Egypt more feasible.
Invasions of Egypt.
Amalric led his first expedition into Egypt in 1163, claiming that the Fatimids had not paid the yearly tribute that had begun during the reign of Baldwin III. The vizier, Dirgham, had recently overthrown the vizier Shawar, and marched out to meet Amalric at Pelusium, but was defeated and forced to retreat to Bilbeis. The Egyptians then opened up the Nile dams and let the river flood, hoping to prevent Amalric from invading any further. Amalric returned home but Shawar fled to the court of Nur ad-Din, who sent his general Shirkuh to settle the dispute in 1164. In response Dirgham sought help from Amalric, but Shirkuh and Shawar arrived before Amalric could intervene and Dirgham was killed. Shawar, however, feared that Shirkuh would seize power for himself, and he too looked to Amalric for assistance. Amalric returned to Egypt in 1164 and besieged Shirkuh in Bilbeis until Shirkuh retreated to Damascus.
Amalric could not follow up on his success in Egypt because Nur ad-Din was active in Syria, having taken Bohemund III of Antioch and Raymond III of Tripoli prisoner at the Battle of Harim during Amalric's absence. Amalric rushed to take up the regency of Antioch and Tripoli and secured Bohemund's ransom in 1165 (Raymond remained in captivity until 1173). The year 1166 was relatively quiet, but Amalric sent envoys to the Byzantine Empire seeking an alliance and a Byzantine wife, and throughout the year had to deal with raids by Nur ad-Din, who captured Banias.
In 1167, Nur ad-Din sent Shirkuh back to Egypt and Amalric once again followed him, establishing a camp near Cairo; Shawar again allied with Amalric and a treaty was signed with the caliph al-Adid himself. Shirkuh encamped on the opposite side of the Nile. After an indecisive battle, Amalric retreated to Cairo and Shirkuh marched north to capture Alexandria; Amalric followed and besieged Shirkuh there, aided by a Pisan fleet from Jerusalem. Shirkuh negotiated for peace and Alexandria was handed over to Amalric. However, Amalric could not remain there indefinitely, and returned to Jerusalem after exacting an enormous tribute.
Byzantine alliance.
After his return to Jerusalem in 1167, Amalric married Maria Comnena, a great-grandniece of Byzantine emperor Manuel I Comnenus. The negotiations had taken two years, mostly because Amalric insisted that Manuel return Antioch to Jerusalem. Once Amalric gave up on this point he was able to marry Maria in Tyre on August 29, 1167. During this time the queen dowager, Baldwin III's widow Theodora, eloped with her cousin Andronicus to Damascus, and Acre, which had been in her possession, reverted into the royal domain of Jerusalem. It was also around this time that William of Tyre was promoted to archdeacon of Tyre, and was recruited by Amalric to write a history of the kingdom.
In 1168 Amalric and Manuel negotiated an alliance against Egypt, and William of Tyre was among the ambassadors sent to Constantinople to finalize the treaty. Although Amalric still had a peace treaty with Shawar, Shawar was accused of attempting to ally with Nur ad-Din, and Amalric invaded. The Knights Hospitaller eagerly supported this invasion, while the Knights Templar refused to have any part in it. In October, without waiting for any Byzantine assistance (and in fact without even waiting for the ambassadors to return), Amalric invaded and seized Bilbeis. The inhabitants were either massacred or enslaved. Amalric then marched to Cairo, where Shawar offered Amalric two million pieces of gold. Meanwhile Nur ad-Din sent Shirkuh back to Egypt as well, and upon his arrival Amalric retreated.
Rise of Saladin.
In January 1169 Shirkuh had Shawar assassinated. Shirkuh became vizier, although he himself died in March, and was succeeded by his nephew Saladin. Amalric became alarmed and sent Frederick de la Roche, Archbishop of Tyre, to seek help from the kings and nobles of Europe, but no assistance was forthcoming. Later that year however a Byzantine fleet arrived, and in October Amalric launched yet another invasion and besieged Damietta by sea and by land. The siege was long and famine broke out in the Christian camp; the Byzantines and crusaders blamed each other for the failure, and a truce was signed with Saladin. Amalric returned home.
Now Jerusalem was surrounded by hostile enemies. In 1170 Saladin invaded Jerusalem and took the city of Eilat, severing Jerusalem's connection with the Red Sea. Saladin, who was set up as Vizier of Egypt, was declared Sultan in 1171 upon the death of the last Fatimid caliph. Saladin's rise to Sultan was an unexpected reprieve for Jerusalem, as Nur ad-Din was now preoccupied with reining in his powerful vassal. Nevertheless, in 1171 Amalric visited Constantinople himself and envoys were sent to the kings of Europe for a second time, but again no help was received. Over the next few years the kingdom was threatened not only by Saladin and Nur ad-Din, but also by the Hashshashin; in one episode, the Knights Templar murdered some Hashshashin envoys, leading to further disputes between Amalric and the Templars.
Death.
Nur ad-Din died in 1174, upon which Amalric immediately besieged Banias. On the way back after giving up the siege he fell ill from dysentery, which was ameliorated by doctors but turned into a fever in Jerusalem. William of Tyre explains that "after suffering intolerably from the fever for several days, he ordered physicians of the Greek, Syrian, and other nations noted for skill in diseases to be called and insisted that they give him some purgative remedy." Neither they nor Latin doctors could help, and he died on July 11, 1174.
Maria Comnena had borne Amalric two daughters: Isabella, who would eventually marry four husbands in turn and succeed as queen, was born in 1172; and a stillborn child some time later. On his deathbed Amalric bequeathed Nablus to Maria and Isabella, both of whom would retire there. The leprous child Baldwin IV succeeded his father and brought his mother Agnes of Courtenay (now married to her fourth husband) back to court.
Physical characteristics.
William was a good friend of Amalric and described him in great detail. "He had a slight impediment in his speech, not serious enough to be considered as a defect but sufficient to render him incapable of ready eloquence. He was far better in counsel than in fluent or ornate speech." Like his brother Baldwin III, he was more of an academic than a warrior, who studied law and languages in his leisure time: "He was well skilled in the customary law by which the kingdom was governed â in fact, he was second to no one in this respect." He was probably responsible for an assize making all rear-vassals directly subject to the king and eligible to appear at the Haute Cour. Amalric had an enormous curiosity, and William was reportedly astonished to find Amalric questioning, during an illness, the resurrection of the body. He especially enjoyed reading and being read to, spending long hours listening to William read early drafts of his history. He did not enjoy games or spectacles, although he liked to hunt. He was trusting of his officials, perhaps too trusting, and it seems that there were many among the population who despised him, although he refused to take any action against those who insulted him publicly.
He was tall and fairly handsome; "he had sparkling eyes of medium size; his nose, like that of his brother, was becomingly aquiline; his hair was blond and grew back somewhat from his forehead. A comely and very full beard covered his cheeks and chin. He had a way of laughing immoderately so that his entire body shook." He did not overeat or drink to excess, but his corpulence grew in his later years, decreasing his interest in military operations; according to William, he "was excessively fat, with breasts like those of a woman hanging down to his waist."
Amalric was pious and attended mass every day, although he also "is said to have absconded himself without restraint to the sins of the flesh and to have seduced married womenâ¦" Despite his piety he taxed the clergy, which they naturally opposed.
As William says, "he was a man of wisdom and discretion, fully competent to hold the reins of government in the kingdom." He is considered the last of the "early" kings of Jerusalem, after whom there was no king able to save Jerusalem from its eventual collapse. Within a few years, Emperor Manuel died as well, and Saladin remained the only strong leader in the east.
References.
 

</doc>
<doc id="1872" url="http://en.wikipedia.org/wiki?curid=1872" title="Amalric II of Jerusalem">
Amalric II of Jerusalem

Aimery of Jerusalem or Aimery of Cyprus, born Aimery of Lusignan (1145 â 1 April 1205), King of Jerusalem 1197â1205, was an older brother of Guy of Lusignan.
Note: Older scholarship mistook the two names Amalric and Aimery as variant spellings of the same name, so these historians erroneously added numbers: Amalric I, r. 1163â1174 and AmalricÂ II, r. 1197â1205. Now scholars recognize that the two names were not the same and no longer add the number for either king: Amalric, r. 1163â1174 and Aimery, r. 1197â1205.
The Lusignan family was noted for its many Crusaders. Aimery and Guy were sons of Hugh VIII of Lusignan, who had himself campaigned in the Holy Land in the 1160s. After being expelled from Poitou by their overlord, Richard the Lion-hearted, for the murder of Patrick of Salisbury, 1st Earl of Salisbury, Aimery arrived in Palestine c. 1174, Guy possibly later. Aimery married Eschiva, daughter of Baldwin of Ibelin. He then took service with Agnes of Courtenay, wife of Reginald of Sidon and mother of Baldwin IV of Jerusalem. The pro-Ibelin "Chronicle of Ernoul" later claimed that he was her lover, but it is likely that she and Baldwin IV were attempting to separate him from the political influence of his wife's family. He was appointed Constable of Jerusalem soon after 22 April 1179. Guy married the king's widowed older sister, Sibylla of Jerusalem in 1180, and so gained a claim to the kingdom of Jerusalem.
Aimery was among those captured with his brother after the disastrous Battle of Hattin in 1187. In 1194, on the death of Guy, he became King of Cyprus. By his first wife, Eschiva of Ibelin, he was the father of Hugh I of Cyprus and was crowned in Nicosia on 22 September 1197. After Eschiva's death in October 1197 he married Isabella, the daughter of Amalric of Jerusalem by his second marriage, and became King of Jerusalem in right of his wife and was crowned at Acre in January 1198. This was only possible, because the candidacy for the crown of Aimery, who was a vassal of Roman-German Emperor Henry VI., was supported by the German crusaders.
In 1198, at the end of the Crusade of 1197, he was able to procure a five years' truce with the Muslims, owing to the struggle between Saladin's brothers and his sons for the inheritance of his territories. The truce was disturbed by raids on both sides, but in 1204 it was renewed for six years.
Many members of the royal family died in rapid succession in early 1205, including Aimery himself. Aimery's two older sons, Guy and John, boys of about eight years of age, died early in 1205. Aimery died of dysentery (allegedly brought on by "a surfeit of white mullet") or even poisoned at Saint Jean d'Acre on 1 April 1205, just after his son Aimery and four days before his wife, and was buried at Saint Sophia, Nicosia. The kingdom of Cyprus passed to Hugh, his only surviving son, while the Kingdom of Jerusalem passed to Maria, the daughter of Isabella by her previous marriage with Conrad of Montferrat.
Wives and children.
His first wife, married before 29 October 1174, was Eschiva of Ibelin (c. 1160 â Cyprus in Winter 1196â1197), daughter of Baldwin of Ibelin and first wife Richilde de Bethsan or Bessan. They had six children:
His second wife was Queen Isabella of Jerusalem, married January 1198 in Acre. They had three children:
References.
Other sources.
 

</doc>
<doc id="1873" url="http://en.wikipedia.org/wiki?curid=1873" title="Anthemius of Tralles">
Anthemius of Tralles

Anthemius of Tralles (c. 474 â before 558; ) was a Greek professor of Geometry in Constantinople and architect, who collaborated with Isidore of Miletus to build the church of Hagia Sophia by the order of Justinian I. Anthemius came from an educated family, one of five sons of Stephanus of Tralles, a physician. Of his brothers, Dioscorus followed his father's profession in Tralles; Alexander became at Rome one of the most celebrated medical men of his time; Olympius was deeply versed in Roman jurisprudence; and Metrodorus was a distinguished grammarian in Constantinople.
As an architect he is best known for replacing the old church of Hagia Sophia at Constantinople in 532; his daring plans for the church strikingly displayed his knowledge. His skills seem also to have extended to engineering for he repaired the flood defences at Daras.
Anthemius was also a capable mathematician. He described the string construction of the ellipse and he wrote a book on conic sections, which was excellent preparation for designing the elaborate vaulting of Hagia Sophia. He compiled a survey of mirror configurations in his work on remarkable mechanical devices which was known to Arab mathematicians such as Ibn al-Haytham.
A fragment of his treatise "On burning-glasses" was published as ' ("Concerning wondrous machines") by L. Dupuy in 1777, and also appeared in 1786 in the forty-second volume of the "Histoire de l'Academie des Instrumentistes". A. Westermann gave a revised edition of it in his ' ("Scriptores rerum mirabilium Graeci", "Greek marvel-writers") in 1839. In the course of the constructions for surfaces to reflect to one and the same point
Anthemius assumes a property of an ellipse not found in Apollonius's work, that the equality of the angles subtended at a focus by two tangents drawn from a point, and having given the focus and a double ordinate he goes on to use the focus and directrix to obtain any number of points on a parabolaâthe first instance on record of the practical use of the directrix.

</doc>
<doc id="1874" url="http://en.wikipedia.org/wiki?curid=1874" title="Absalon">
Absalon

Absalon or Axel ( â 21 March 1201) was a Danish archbishop and statesman, who was the Bishop of Roskilde from 1158 to 1192 and Archbishop of Lund from 1178 until his death. He was the foremost politician and churchfather of Denmark in the second half of the 12th century, and was the closest advisor of King Valdemar I of Denmark. He was a key figure in the Danish policies of territorial expansion in the Baltic Sea, Europeanization in close relationship with the Holy See, and reform in the relation between the Church and the public. He combined the ideals of Gregorian Reform ideals with loyal support of a strong monarchical power.
Absalon was born into the powerful "Hvide" clan, and owned great land possessions. He endowed several church institutions, most prominently his family's SorÃ¸ Abbey. He was granted lands by the crown, and built the first fortification of the city that evolved into modern-day Copenhagen. His titles were passed on to his nephews Anders Sunesen and Peder Sunesen. He died in 1201, and was interred at SorÃ¸ Abbey.
Early life.
Absalon was born around 1128 near SorÃ¸, Zealand. Due to a name which is unusual in Denmark, it is speculated that he was christened on the Danish "Absalon" name day, October 30. He was the son of Asser Rig, a magnate of the "Hvide" clan from Fjenneslev on Zealand. He was also a kinsman of Archbishop Eskil of Lund. He grew up at the castle of his father, and was brought up alongside his older brother Esbern Snare and the young prince Valdemar, who later became King Valdemar I of Denmark. During the civil war following the death of Eric III of Denmark in 1146, Absalon travelled abroad to study theology in Paris, while Esbern fought for Valdemar's ascension to the throne. At Paris, he was influenced by the Gregorian Reform ideals of churchly independence from Monarchical rule. He also befriended the canon William of Ãbelholt at the Abbey of St Genevieve, whom he later made abbott of EskilsÃ¸ Abbey.
Absalon first appears in Saxo Grammaticus's contemporary chronicle "Gesta Danorum" at the end of the civil war, at the brokering of the peace agreement between Sweyn III and Valdemar at St. Alban's Priory, Odense. He was a guest at following Roskilde banquet given in 1157 by Sweyn to his rivals Canute V and Valdemar. Both Absalon and Valdemar narrowly escaped assassination at the hands of Sweyn on this occasion, and escaped to Jutland, whither Sweyn followed them. Absalon probably did not take part in the following battle of Grathe Heath in 1157, in which Sweyn was defeated and slain and led to Valdemar ascending the Danish throne. On Good Friday 1158, bishop Asser of Roskilde died, and Absalon was eventually elected bishop of Roskilde on Zealand with the help of Valdemar, as the king's reward for the "Hvide" family support.
Bishop and advisor.
Absalon was a close counsellor of Valdemar, and chief promoter of the Danish crusades against the Wends. During the Danish civil war, Denmark had been open to coastal raids by the Wends. It was Absalon's intention to clear the Baltic Sea of the Wendish pirates who inhabited its southern littoral zone which was later called Pomerania. The pirates had raided the Danish coasts during the civil war of Sweyn III, Canute V, and Valdemar, to the point where at the accession of Valdemar one-third of Denmark lay wasted and depopulated. Absalon formed a guardian fleet, built coastal defenses, and led several campaigns against the Wends. He even advocated forgiving the earlier enemies of Valdemar, which helped stabilize Denmark internally.
Wendish campaigns.
The first expedition against the Wends that was conducted by Absalon in person, set out in 1160. These expeditions were successful, but brought no lasting victories. What started out as mere retribution, eventually evolved into full-fledged campaigns of expansion with religious crusader motives. In 1164 began twenty years of crusades against the Wends, sometimes with the help of German duke Henry the Lion, sometimes in opposition to him.
In 1168 the chief Wendish fortress at Arkona in RÃ¼gen, containing the sanctuary of their god Svantevit, was conquered. The Wends agreed to accept Danish suzerainty and the Christian religion at the same time. From Arkona, Absalon proceeded by sea to Charenza, in the midst of RÃ¼gen, the political capital of the Wends and an all but impregnable stronghold. But the unexpected fall of Arkona had terrified the garrison, which surrendered unconditionally at the first appearance of the Danish ships. Absalon, with only Bishop Sweyn of Aarhus, and twelve "housecarls" thereupon disembarked, passed between a double row of Wendish warriors, 6000 strong, along the narrow path winding among the morasses, to the gates of the fortress, and, proceeding to the temple of the seven-headed god Rugievit, caused the idol to be hewn down, dragged forth and burnt. The whole population of Garz was then baptized, and Absalon laid the foundations of twelve churches in the isle of RÃ¼gen. RÃ¼gen was then subjected to Absalon's Bishopric of Roskilde.
The destruction of this chief sally-port of the Wendish pirates enabled Absalon considerably to reduce the Danish fleet. But he continued to keep a watchful eye over the Baltic, and in 1170 destroyed another pirate stronghold, farther eastward, at DziwnÃ³w on the isle of Wolin. Absalon's last military exploit came in 1184, off Stralsund at Whitsun, when he soundly defeated a Pomeranian fleet that had attacked Denmark's vassal, Jaromar of RÃ¼gen.
Policies.
Absalon's main political goal was to free Denmark from entanglements with the Holy Roman Empire. Absalon reformed the Danish church organisation to closer match Holy See praxis, and worked to keep Denmark a close ally of the Holy See. However, during the schism between Pope Alexander III and Antipope Victor IV, Absalon stayed loyal to Valdemar even as he joined the Holy Roman Emperor Frederick Barberossa in supporting Victor IV. This caused a split within the Danish church, as it possibly forced Eskil into exile around 1161, despite Abaslon's attempts to keep the Danish church united. It was contrary to Absalon's advice and warnings that Valdemar I rendered fealty to the emperor Frederick Barbarossa at Dole in 1162. When Valdemar returned to Denmark, he was convinced into strengthening the Danevirke fortifications at the German border, with the support of Absalon.
Absalon built churches and monasteries, supporting international religious orders like the Cistercians and Augustinians, founding schools and doing his utmost to promote civilization and enlightenment. In 1162, Absalon transformed the SorÃ¸ Abbey of his family from Benedictine to Cistercian, granting it lands from his personal holdings. In 1167, Absalon was granted the land around the city of "Havn" (), and built there a castle in the coastal defense against the Wends. Havn quickly expanded as one of Scandinavia's most important centers of trade, and eventually evolved into modern-day Copenhagen. It was also Absalon who held the first Danish Synod at Lund in 1167. He was also interested in history and culture, and commissioned Saxo Grammaticus to write "Gesta Danorum", a comprehensive chronicle of the history of the Danes. In 1171, Absalon issued the "Zealand church law" (), which reduced the number of Canonical Law offenses for which the church could fine the public, while instituting the tithe payment system. Eventual violation of the law was specified as subject to a secular legal process.
Archbishop of Lund.
Archbishop Eskil returned from exile in 1167. Eskil agreed on canonizing Valdemar's father Knud Lavard in 1170, with Absalon assisting him at the feast. When Eskil stepped down as Archbishop of Lund in 1177, he chose Absalon as his successor. Absalon initially resisted the new position, as he did not want to lose his power position on Zealand, but complied with Papal orders to do so in 1178. By a unique Papal dispensation, Absalon was allowed to simultaneously maintain his post as Bishop of Roskilde. As the Archbishop of Lund, Absalon utilized ombudsmen from Zealand, demanded unfree labour from the peasantry, and instituted tithes. He was a harsh and effective ruler, who cleared all Orthodox Christian liturgic remnants in favour of Papal standards. A rebellion in the Scanian peasantry forced him to flee to Zealand in 1180, from where he returned and subdued the Scanians with the help of Valdemar.
When Valdemar died in 1182, his son succeeded him as Canute VI, and Absalon served as Canute VI's counsellor. Under Canute VI, Absalon was the chief policymaker in Danish politics. Absalon kept his hostile attitude to the Holy Roman Empire. On the accession of Canute VI in 1182, an imperial ambassador arrived at Roskilde to get the new king to swear fealty to Frederick Barbarossa, but Absalon resolutely withstood him. This represented the final Danish rejection of German supremacy.
Death.
When Absalon retired from military service in 1184 at the age of fifty-seven, he resigned the command of fleets and armies to younger men, like Duke Valdemar, the later king Valdemar II. He instead confined himself to the administration of the Danish empire. In 1192, Absalon made his nephew Peder Sunesen his successor as Bishop of Roskilde, while his other nephew Anders Sunesen was named the chancellor of Canute VI. Absalon died at SorÃ¸ Abbey on March 21, 1201, 73 years old, with his last will granting his personal holdings to the Abbey, apart from Fjenneslev which went to Esbern Snarre. He had already given Copenhagen to the Bishopric of Roskilde. Absalon was interred at SorÃ¸ Abbey, and was succeeded as Archbishop of Lund by Anders Sunesen.
Legacy.
Saxo Grammaticus' "Gesta Danorum" was not finished until after the death of Absalon, but Absalon was one of the chief heroic figures of the chronicle, which was to be the main source of knowledge about early Danish history. Absalon left a legacy as the foremost politician and churchfather of Denmark in the 12th century. Absalon was equally great as churchman, statesman and warrior. His policy of expansion was to give Denmark the dominion of the Baltic for three generations. That he enjoyed warfare there can be no doubt; yet he was not like the ordinary fighting bishops of the Middle Ages, whose sole indication of their religious role was to avoid the "shedding of blood" by using a mace in battle instead of a sword. Absalon never neglected his ecclesiastical duties, and even his wars were of the nature of crusades.
In the 2000s, "Absalon" was adopted as the name for a class of Royal Danish Navy vessels, and the lead vessel of the class. HDMS Absalon (L16) and "Esbern Snare" (L17) were launched and commissioned by Denmark in 2004 and 2005. In December 2008, "HDMS Absalon" was involved in the rescue of putative Somali pirates 90Â miles off Yemen in the Gulf of Aden. The craft from Somalia was reported to hold rocket-propelled grenades and AK-47 assault rifles, and to have been adrift for several days. Also per the report, the "Absalon" took the sailors and weapons aboard, sunk the craft, and turned the sailors over to the Yemen coast guard. The "Absalon", according to "The New York Times" report, "was deployed in the Gulf of Aden [in] September ['08] as part of an international effort to curb piracy," part of Combined Task Force 150.

</doc>
<doc id="1875" url="http://en.wikipedia.org/wiki?curid=1875" title="Adhemar of Le Puy">
Adhemar of Le Puy

 
Adhemar (also known as AdÃ©mar, Aimar, or Aelarz) de Monteil (died 1 August 1098), one of the principal figures of the First Crusade, was bishop of Puy-en-Velay from before 1087. 
Life.
At the Council of Clermont in 1095, Adhemar showed great zeal for the crusade (there is evidence Urban II had conferred with Adhemar before the council) and having been named apostolic legate and appointed to lead the crusade by Pope Urban II, he accompanied Raymond IV, Count of Toulouse, to the east. 
Whilst Raymond and the other leaders often quarrelled with each other over the leadership of the crusade, Adhemar was always recognized as the spiritual leader of the crusade.
Adhemar negotiated with Alexius I Comnenus at Constantinople, reestablished at Nicaea some discipline among the crusaders, fought a crucial role at the Battle of Dorylaeum and was largely responsible for sustaining morale during the siege of Antioch through various religious rites including fasting and special observances of holy days. After the capture of the city in June 1098, and the subsequent siege led by Kerbogha, Adhemar organized a procession through the streets, and had the gates locked so that the Crusaders, many of whom had begun to panic, would be unable to desert the city. He was extremely skeptical of Peter Bartholomew's discovery in Antioch of the Holy Lance, especially because he knew such a relic already existed in Constantinople; however, he was willing to let the Crusader army believe it was real if it raised their morale. 
When Kerbogha was defeated, Adhemar organized a council in an attempt to settle the leadership disputes, but he died on 1 August 1098, probably of typhus. 
The disputes among the higher nobles went unsolved, and the march to Jerusalem was delayed for months. However, the lower-class foot soldiers continued to think of Adhemar as a leader; some of them claimed to have been visited by his ghost during the siege of Jerusalem, and reported that Adhemar instructed them to hold another procession around the walls. This was done, and Jerusalem was taken by the Crusaders in 1099.

</doc>
<doc id="1878" url="http://en.wikipedia.org/wiki?curid=1878" title="Alphonse, Count of Poitiers">
Alphonse, Count of Poitiers

Alphonse or Alfonso (11 November 1220 â 21 August 1271) was the Count of Poitou from 1225 and Count of Toulouse (as Alphonse II) from 1249.
Born at Poissy, Alphonse was a son of Louis VIII, King of France and Blanche of Castile. He was a younger brother of Louis IX of France and an older brother of Charles I of Sicily. In 1229, his mother, who was regent of France, forced the Treaty of Paris on Raymond VII of Toulouse after his rebellion. It stipulated that a brother of King Louis was to marry Joan of Toulouse, daughter of Raymond VII of Toulouse, and so in 1237 Alphonse married her. Since she was Raymond's only child, they became rulers of Toulouse at Raymond's death in 1249.
By the terms of his father's will he received an "appanage" of Poitou and Auvergne. He won the battle of Taillebourg in the Saintonge War with his brother Louis IX, against a revolt allied with king Henry III of England.
Alphonse took part in two crusades with his brother, St Louis, in 1248 (the Seventh Crusade) and in 1270 (the Eighth Crusade). For the first of these, he raised a large sum and a substantial force, arriving in Damietta on 24 October 1249, after the town had already been captured. He sailed for home on 10 August 1250. His father-in-law had died while he was away, and he went directly to Toulouse to take possession. There was some resistance to his accession as count, which was suppressed with the help of his mother Blanche of Castile who was acting as regent in the absence of Louis IX. The county of Toulouse, since then, was joined to the Alphonse's "appanage".
In 1252, on the death of his mother, Blanche of Castile, Alphonse was joint regent with Charles of Anjou until the return of Louis IX. During that time he took a great part in the campaigns and negotiations which led to the Treaty of Paris in 1259, under which King Henry III of England recognized his loss of continental territory to France (including Normandy, Maine, Anjou, and Poitou) in exchange for France withdrawing support from English rebels.
Aside from the crusades, Alphonse stayed primarily in Paris, governing his estates by officials, inspectors who reviewed the officials work, and a constant stream of messages. His main work was on his own estates. There he repaired the evils of the Albigensian war and made a first attempt at administrative centralization, thus preparing the way for union with the crown. The charter known as "Alphonsine," granted to the town of Riom, became the code of public law for Auvergne. Honest and moderate, protecting the middle classes against exactions of the nobles, he exercised a happy influence upon the south, in spite of his naturally despotic character and his continual and pressing need of money. He is noted for ordering the first recorded local expulsion of Jews, when he did so in Poitou in 1249.
When Louis IX again engaged in a crusade (the Eighth Crusade), Alphonse again raised a large sum of money and accompanied his brother. This time, however, he did not return to France, dying while on his way back, probably at Savona in Italy, on 21 August 1271. He had been appointed a Knight of the Order of the Ship by his brother.
Death and Legacy.
Alphonse's death without heirs raised some questions as to the succession to his lands. One possibility was that they should revert to the crown, another that they should be redistributed to his family. The latter was claimed by Charles of Anjou, but in 1283 Parlement decided that the County of Toulouse should revert to the crown, if there were no male heirs. Alphonse's wife Joan (who died four days after Alphonse) had attempted to dispose of some of her inherited lands in her will. Joan was the only surviving child and heiress of Raymond VII, Count of Toulouse, Duke of Narbonne, and Marquis of Provence, so under ProvenÃ§al and French law, the lands should have gone to her nearest male relative. But, her will was invalidated by Parlement in 1274. One specific bequest in Alphonse's will, giving his wife's lands in the Comtat Venaissin to the Holy See, was allowed, and it became a Papal territory, a status that it retained until 1791.

</doc>
<doc id="1879" url="http://en.wikipedia.org/wiki?curid=1879" title="Alfonso Jordan">
Alfonso Jordan

Alfonso Jordan (; ) (1103â1148) was the Count of Tripoli (1105â09), Count of Rouergue (1109â48) and Count of Toulouse, Margrave of Provence and Duke of Narbonne (1112â48, as Alfonso I). 
Life.
He was the son of Raymond IV of Toulouse by his third wife, Elvira of Castile. He was born in the castle of Mont PÃ¨lerin in Tripoli while his father was on the First Crusade. He was given the name "Jordan" after being baptised in the Jordan River.
Alfonso's father died when he was two years old and he remained under the guardianship of his cousin, William Jordan, Count of Cerdagne, until he was five. He was then taken to Europe, where his half-brother Bertrand had given him the county of Rouergue. Upon Bertrand's death in 1112, Alfonso succeeded to the county of Toulouse and marquisate of Provence. In 1114, Duke William IX of Aquitaine, who claimed Toulouse by right of his wife Philippa, daughter of Count William IV, invaded the county and conquered it. Alfonso recovered a part in 1119, but he was not in full control until 1123. When at last successful, he was excommunicated by Pope Callixtus II for having expelled the monks of Saint-Gilles, who had aided his enemies.
Alfonso next had to fight for his rights in Provence against Count Raymond Berengar III of Barcelona. Not until September 1125 did their war end in "peace and concord" ("pax et concordia"). At this stage, Alfonso was master of the regions lying between the Pyrenees and the Alps, the Auvergne and the sea. His ascendancy was, according to one commentator, an unmixed good to the country, for during a period of fourteen years art and industry flourished. 
In March 1126, Alfonso was at the court of Alfonso VII of LeÃ³n when he acceded to the throne. 
According to the "Chronica Adefonsi imperatoris", Alfonso and Suero VermÃºdez took the city of LeÃ³n from opposition magnates and handed it over to Alfonso VII. Among those who may have accompanied Alfonso on one of his many extended stays in Spain was the troubadour Marcabru.
About 1134 Alfonso seized the viscounty of Narbonne and ruled it during the minority of the Viscountess Ermengarde, only restoring it to her in 1143. 
In 1141 King Louis VII pressed the claim of Philippa on behalf of his wife, Eleanor of Aquitaine, even besieging Toulouse, but without result. 
That same year Alfonso Jordan was again in Spain, making a pilgrimage to Saint James of Compostela, when he proposed a peace between the king of LeÃ³n and GarcÃ­a VI of Navarre, which became the basis for subsequent negotiations.
In 1144, Alfonso again incurred the displeasure of the church by siding with the citizens of Montpellier against their lord. 
In 1145, Bernard of Clairvaux addressed a letter to him full of concern about a heretic named Henry in the diocese of Toulouse. 
Bernard even went there to preach against the heresy, an early expression of Catharism. 
A second time he was excommunicated; but in 1146 he took the cross (i.e., vowed to go on crusade) at a meeting in VÃ©zelay called by Louis VII. 
In August 1147, he embarked for the near east on the Second Crusade.
He lingered on the way in Italy and probably in Constantinople, where he may have met the Emperor Manuel I.
Alfonso finally arrived at Acre in 1148. 
Among his companions he had made enemies and he was destined to take no share in the crusade he had joined. 
He died at Caesarea, and there were accusations of poisoning, usually levelled against either by Eleanor of Aquitaine, the wife of Louis, or Melisende, the mother of King Baldwin III of Jerusalem. 
By his wife since 1125, Faydiva d'UzÃ¨s, he left two legitimate sons: Raymond, who succeeded him, and Alfonso. His daughter Faydiva (died 1154) married Count Humbert III of Savoy. He left two other daughters: the legitimate Agnes (died 1187) and the illegitimate Laurentia, who married Count Bernard III of Comminges.
Notes.
 
 

</doc>
<doc id="1880" url="http://en.wikipedia.org/wiki?curid=1880" title="Ambroise">
Ambroise

Ambroise, sometimes Ambroise of Normandy, (flourished c. 1190) was a Norman poet and chronicler of the Third Crusade, author of a work called "L'Estoire de la guerre sainte", which describes in rhyming Old French verse the adventures of Richard Coeur de Lion as a crusader. The poem is known to us only through one Vatican manuscript, and long escaped the notice of historians.
The credit for detecting its value belongs to Gaston Paris, although his edition (1897) was partially anticipated by the editors of the "Monumenta Germaniae Historica", who published some selections in the twenty-seventh volume of their Scriptores (1885). Ambroise followed Richard I as a noncombatant, and not improbably as a court-minstrel. He speaks as an eye-witness of the king's doings at Messina, in Cyprus, at the siege of Acre, and in the abortive campaign which followed the capture of that city.
Ambroise is surprisingly accurate in his chronology; though he did not complete his work before 1195, it is evidently founded upon notes which he had taken in the course of his pilgrimage. He shows no greater political insight than we should expect from his position; but relates what he had seen and heard with a naÃ¯ve vivacity which compels attention. He is by no means an impartial source: he is prejudiced against the Saracens, against the French, and against all the rivals or enemies of his master, including the "Polein" party which supported Conrad of Montferrat against Guy of Lusignan. He is rather to be treated as a biographer than as a historian of the Crusade in its broader aspects. Nonetheless he is an interesting primary source for the events of the years 1190â1192 in the Kingdom of Jerusalem.
Books 2â6 of the "Itinerarium Regis Ricardi", a Latin prose narrative of the same events apparently compiled by Richard, a canon of Holy Trinity, London, are closely related to Ambroise's poem. They were formerly sometimes regarded as the first-hand narrative on which Ambroise based his work, but that can no longer be maintained.

</doc>
<doc id="1881" url="http://en.wikipedia.org/wiki?curid=1881" title="Art Deco">
Art Deco

Art Deco (), or Deco, is an influential visual arts design style that first appeared in France after World War I and began flourishing internationally in the 1920s, 1930s and 1940s before its popularity waned after World War II. It is an eclectic style that combines traditional craft motifs with Machine Age imagery and materials. The style is often characterized by rich colours, bold geometric shapes and lavish ornamentation.
Deco emerged from the interwar period when rapid industrialisation was transforming culture. One of its major attributes is an embrace of technology. This distinguishes Deco from the organic motifs favoured by its predecessor Art Nouveau.
Historian Bevis Hillier defined Art Deco as "an assertively modern style [that] ran to symmetry rather than asymmetry, and to the rectilinear rather than the curvilinear; it responded to the demands of the machine and of new material [and] the requirements of mass production".
During its heyday, Art Deco represented luxury, glamour, exuberance and faith in social and technological progress.
Etymology.
The first use of the term "Art Deco" has been attributed to architect Le Corbusier, who penned a series of articles in his journal "L'Esprit nouveau" under the headline "1925 Expo: Arts DÃ©co". He was referring to the 1925 Exposition Internationale des Arts DÃ©coratifs et Industriels Modernes (International Exposition of Modern Decorative and Industrial Arts).
The term came into more general use in 1966, when a French exhibition celebrating the 1925 event was held under the title "Les AnnÃ©es 25: Art DÃ©co/Bauhaus/Stijl/Esprit Nouveau". Here the term was used to distinguish the new styles of French decorative crafts that had emerged since the Belle Epoque. The term Art Deco has since been applied to a wide variety of works produced during the Interwar period ("L'Entre Deux Guerres"), and even to those of the Bauhaus in Germany. However, Art Deco originated in France. It has been argued that the term should be applied to French works and those produced in countries directly influenced by France.
Art Deco gained currency as a broadly applied stylistic label in 1968 when historian Bevis Hillier published the first book on the subject: "Art Deco of the '20s and '30s". Hillier noted that the term was already being used by art dealers and cites "The Times" (2 November 1966) and an essay named "Les Arts DÃ©co" in "Elle" magazine (November 1967) as examples of prior usage. In 1971, Hillier organised an exhibition at the Minneapolis Institute of Arts, which he details in his book about it, "The World of Art Deco".
Origins.
Some historians trace Deco's roots to the Universal Exposition of 1900. After this show a group of artists established an informal collective known as "La SociÃ©tÃ© des artistes dÃ©corateurs" (Society of Decorator Artists) to promote French crafts. Among them were Hector Guimard, EugÃ¨ne Grasset, Raoul Lachenal, Paul Bellot, Maurice DufrÃªne and Emile Decoeur. These artists are said to have influenced the principles of Art Deco.
The Art Deco era is often anecdotally dated from 1925 when the Exposition Internationale des Arts DÃ©coratifs et Industriels Modernes was organized to showcase new ideas in applied arts, although the style had been in full force in France for several years before that date. Deco was heavily influenced by pre-modern art from around the world and observable at the MusÃ©e du Louvre, MusÃ©e de l'Homme and the MusÃ©e national des Arts d'Afrique et d'OcÃ©anie. During the 1920s, affordable travel permitted "in situ" exposure to other cultures. There was also popular interest in archeology due to excavations at Pompeii, Troy, the tomb of Tutankhamun, etc. Artists and designers integrated motifs from ancient Egypt, Mesopotamia, Greece, Rome, Asia, Mesoamerica and Oceania with Machine Age elements.
Deco was also influenced by Cubism, Constructivism, Functionalism, Modernism, and Futurism.
In 1905, before the onset of Cubism, EugÃ¨ne Grasset wrote and published "MÃ©thode de Composition Ornementale, ÃlÃ©ments Rectilignes," within which he systematically explored the decorative (ornamental) aspects of geometric elements, forms, motifs and their variations, in contrast with (and as a departure from) the undulating Art Nouveau style of Hector Guimard, so popular in Paris a few years earlier. Grasset stresses the principle that various simple geometric shapes like triangles and squares are the basis of all compositional arrangements.
At the 1907 Salon d'Automne in Paris, Georges Braque exhibited "Viaduc Ã  l'Estaque" (a proto-Cubist work), now at the Minneapolis Institute of Arts. Simultaneously, there was a retrospective exhibition of 56 works by Paul CÃ©zanne, as a tribute to the artist who died in 1906. CÃ©zanne was interested in the simplification of forms to their geometric essentials: the cylinder, the sphere, the cone.
Paul Iribe created for the couturier Paul Poiret esthetic designs that shocked the Parisian milieu with its novelty. These illustrations were compiled into an album, "Les Robes de Paul Poiret racontÃ©e par Paul Iribe", published in 1908.
At the 1910 Salon des IndÃ©pendants, Jean Metzinger, Henri Le Fauconnier and Robert Delaunay, shown together in Room 18, elaborated upon CÃ©zannian syntax, revealing to the general public for the first time a "mobile perspective" in their art, soon to become known as Cubism. Several months later, the Salon d'Automne saw the invitation of Munich artists who for several years had been working with simple geometric shapes. Leading up to 1910 and culminating in 1912, the French designers AndrÃ© Mare and Louis Sue turned towards the quasi-mystical Golden ratio, in accord with Pythagorean and Platonic traditions, giving their works a Cubist sensibility.
Between 1910 and 1913, Paris saw the construction of the ThÃ©Ã¢tre des Champs-ÃlysÃ©es, 15 avenue Montaigne, another sign of the radical aesthetic change experienced by the Parisian milieu of the time. The rigorous composition of its facade, designed by Auguste Perret, is a major example of early Art Deco. The building includes an exterior bas relief by Antoine Bourdelle, a dome by Maurice Denis, paintings by Ãdouard Vuillard and Jacqueline Marval, and a stage curtain design by Ker-Xavier Roussel.
The artists of the Section d'Or exhibited (in 1912) works considerably more accessible to the general public than the analytical Cubism of Picasso and Braque. The Cubist vocabulary was poised to attract fashion, furniture and interior designers.
These revolutionary changes occurring at the outset of the 20th century are summarized in the 1912 writings of AndrÃ© Vera. "Le Nouveau style", published in the journal "L'Art dÃ©coratif", expressed the rejection of Art Nouveau forms (asymmetric, polychrome and picturesque) and called for "simplicitÃ© volontaire, symÃ©trie manifeste, l'ordre et l'harmonie", themes that would eventually become ubiquitous within the context of Art Deco.
Order, color and geometry: the essence of Art Deco vocabulary was made manifest before 1914.
Several years after World War I, in 1927, Cubists Joseph Csaky, Jacques Lipchitz, Louis Marcoussis, Henri Laurens, the sculptor Gustave Miklos, and others collaborated in the decoration of a Studio House, rue Saint-James, Neuilly-sur-Seine, designed by the architect Paul Ruaud and owned by the French fashion designer Jacques Doucet, also a collector of Post-Impressionist and Cubist paintings (including Les Demoiselles d'Avignon, which he bought directly from Picasso's studio). Laurens designed the fountain, Csaky designed Doucet's staircase, Lipchitz made the fireplace mantel, and Marcoussis made a Cubist rug.
"La Maison Cubiste" (The Cubist House).
In the Art DÃ©coratif section of the 1912 Salon d'Automne, an architectural installation was exhibited that quickly became known as "La Maison Cubiste" ("The Cubist House"). The facade was designed by Raymond Duchamp-Villon and the interior by AndrÃ© Mare along with a group of collaborators. "Mare's ensembles were accepted as frames for Cubist works because they allowed paintings and sculptures their independence", writes Christopher Green, "creating a play of contrasts, hence the involvement not only of Gleizes and Metiznger themselves, but of Marie Laurencin, the Duchamp brothers (Raymond Duchamp-Villon designed the facade) and Mare's old friends LÃ©ger and Roger de La Fresnaye".
"La Maison Cubiste" was a fully furnished house, with a staircase, wrought iron banisters, a living roomâthe "Salon Bourgeois", where paintings by Marcel Duchamp, Jean Metzinger, Albert Gleizes, Marie Laurencin and Fernand LÃ©ger were hungâand a bedroom. It was an early example of "L'art dÃ©coratif", a home within which Cubist art could be displayed in the comfort and style of modern, bourgeois life. Spectators at the Salon d'Automne passed through the full-scale 10-by-3-meter plaster model of the ground floor of the facade. This architectural installation was subsequently exhibited at the 1913 Armory Show, New York, Chicago and Boston, listed in the catalogue of the New York exhibit as Raymond Duchamp-Villon, number 609, and entitled "Facade architectural, plaster" ("FaÃ§ade architecturale").
Attributes.
Deco emphasizes geometric forms: spheres, polygons, rectangles, trapezoids, zigzags, chevrons, and sunburst motifs. Elements are often arranged in symmetrical patterns. Modern materials such as aluminum, stainless steel, Bakelite, chrome, and plastics are frequently used. Stained glass, inlays, and lacquer are also common. Colors tend to be vivid and high contrast.
Influence.
Art Deco was a globally popular style and affected many areas of design. It was used widely in consumer products such as automobiles, furniture, cookware, china, textiles, jewelry, clocks, and electronic items such as radios, telephones, and jukeboxes. It also influenced architecture, interior design, industrial design, fashion, graphic arts, and cinema.
During the 1930s, Art Deco was used extensively for public works projects, railway stations, ocean liners (including the "Ãle de France", "Queen Mary", and "Normandie"), movie palaces, and amusement parks.
The austerities imposed by World War II caused Art Deco to decline in popularity: it was perceived by some as gaudy and inappropriately luxurious. A resurgence of interest began during the 1960s. Deco continues to inspire designers and is often used in contemporary fashion, jewelry, and toiletries.
Streamline Moderne.
A style related to Art Deco is Streamline Moderne (or Streamline) which emerged during the mid-1930s. Streamline was influenced by modern aerodynamic principles developed for aviation and ballistics to reduce air friction at high velocities. Designers applied these principles to cars, trains, ships, and even objects not intended to move, such as refrigerators, gas pumps, and buildings.
One of the first production vehicles in this style was the Chrysler Airflow of 1933. It was unsuccessful commercially, but the beauty and functionality of its design set a precedent.
Streamlining quickly influenced automotive design and evolved the rectangular "horseless carriage" into sleek vehicles with aerodynamic lines, symmetry, and V-shapes. These designs continued to be popular after World War II.
Surviving examples.
United States.
The U.S. has many examples of Art Deco architecture. New York, Chicago, and Los Angeles have many Art Deco buildings: The famous skyscrapers are the best-known, but notable Art Deco buildings can be found in various neighborhoods. Detroit's many examples of Art Deco architecture include the Fisher Building, Guardian Building, and Fox Theatre, all of which are now National Historic Landmarks, as well as the Penobscot and the David Stott Buildings. Los Angeles's Art Deco architecture is particular along Wilshire Boulevard, a main thoroughfare that experienced a period of intense construction activity during the 1920s. Notable examples include the Bullocks Wilshire building and the Pellissier Building and Wiltern Theatre, built in 1929 and 1931 respectively. Both buildings experienced recent restoration.
Miami Beach, Florida, has a large collection of Art Deco buildings, with some thirty blocks of hotels and apartment houses dating from the 1920s to the 1940s. In 1979, the Miami Beach Architectural District was listed on the National Register of Historic Places. Nearly all the buildings have been restored and painted in their original pastel colors.
Art deco was popular during the later years of the movie palace era of theatre construction. Excellent examples of Art Deco theatres, such as the Fargo Theatre in Fargo, North Dakota, and The Campus Theatre in Lewisburg, Pennsylvania, still exist throughout the United States.
Fair Park, located in Dallas, Texas, is a large collection of Art Deco structures. Much of the Art Deco heritage of Tulsa, Oklahoma, remains from that city's . Houston, Texas, has some buildings surviving, such as the Houston City Hall, the JPMorgan Chase Building, Ezekiel W. Cullen Building, and the 1940 Air Terminal Museum. In Beaumont, the Jefferson County Courthouse, Kyle Building and the First National Bank Building are some of the few Art Deco buildings still in the city.
Hoover Dam is a somewhat unusual example of Art Deco design. Many dam guides state that the design was to be Gothic Revival, including the installation of gargoyles with water shooting out of their mouths. The recently opened Smith Center in Downtown Las Vegas incorporates many design elements from Hoover Dam and, therefore, is a contemporary example of the use of Art Deco design elements.
Kansas City is home to the Kansas City Power and Light Building, which was completed in 1931. This building is a good example of the Great Depression and its effect on Art Deco construction. Original plans were for a twin tower to be built next to it on its west side. However, it was never built due to financial constraints. As a result, the tower has a bare west side, with no windows. Other examples of Art Deco buildings in Kansas City include Municipal Auditorium, the Jackson County Courthouse, Kansas City City Hall, and 909 Walnut.
Minneapolis has the Foshay Tower, which was finished in 1929. The building was built immediately before the Great Depression and is the only obelisk-shaped office building in the world. Minneapolis also has the Rand Tower, the CenturyLink Building, the Minneapolis Post Office, and the Wells Fargo Center, an example of modern Art Deco architecture. Neighboring St. Paul is home to the First National Bank Building and the Saint Paul City Hall. The city of Rochester, Minnesota, houses the Plummer Building, the original building for the world-famous Mayo Clinic, which was built in 1927.
Cincinnati, Ohio, houses the Cincinnati Union Terminal, an Art Deco-style passenger railroad station that began operation in 1933. After the decline of railroad travel, most of the building was converted to other uses. It now serves as the Cincinnati Museum Center, which serves more than one million visitors per year and is the 17th most visited museum in the United States. Cincinnati is also home to the Carew Tower, a 49-story Art Deco skyscraper built in 1931.
In 2005, the largest residential restoration project in the country and the largest collection of Art Deco buildings in New Jersey began at the site of the former Jersey City Medical Center. The conversion of the national historic site to a residential enclave had, as of 2009, been completed on three of the several buildings on the site.
Flint, Michigan, is also home to The Paterson Building. The Paterson Building has extensive Art Deco throughout the interior and exterior.
Syracuse, New York, is home to the Niagara Mohawk Building, completed in 1932 and listed as a National Historic Landmark. Niagara Mohawk was considered in the 1930s to be the nation's most powerful electricity supplier, thus the building emphasiszed a vast futuristic look with an electric style embedded into it.
Canada.
In Canada Art Deco structures that survive are mainly in urban centres like Toronto, Montreal, Vancouver and Hamilton, Ontario. They range from public buildings like Vancouver City Hall to commercial buildings (College Park) to public works (R. C. Harris Water Treatment Plant). Toronto hosts the vast majority of Art Deco buildings built and surviving. In MontrÃ©al, the Salle Ernest Cormier at UniversitÃ© de MontrÃ©al is considered an example of the Art Deco style. The beautiful style ended just as soon as World War 2 started. The age of cheaper construction began because the main concern at the time was the war. Hamilton boasts a large collection of Art Deco buildings as well. Hamilton GO Centre is the only example of Art Deco railway station architecture in Canada. Other buildings include: The Pigott Building, an 18-storey condominium (1929), The Sunlife Building, The Bell Telephone Baker Exchange (first telephone exchange in the British Empire, 1929), Dominion Public Building refurbished into the John Sopinka Courthouse (1936), and The Hamilton Port Authority (1953).
Latin America.
Some of the finest surviving examples of Art Deco art and architecture are found in Cuba, especially in Havana. The Bacardi Building is noted for its particular Art Deco style. The style is expressed by the architecture of residences, businesses, hotels, and many pieces of decorative art, furniture, and utensils in public buildings, as well as in private homes.
In Mexico City, art deco residential buildings abound in the chic Condesa neighborhood, many designed by Francisco J. Serrano. Edificio El Moro and the interior of the Palacio de Bellas Artes are other examples.
Another country with many examples of Art Deco architecture is Brazil, especially in Porto Alegre, GoiÃ¢nia and cities like CipÃ³ (Bahia), IraÃ­ (Rio Grande do Sul) and Rio de Janeiro, especially in Copacabana. Also in the Brazil's north-east â notably in cities such as Campina Grande in the state of ParaÃ­ba â there are Art Deco buildings which have been termed "Sertanejo Art Deco" because of their peculiar architectural features. The reason for the style being so widespread in Brazil is its coincidence with the fast growth and radical economic changes of the country during the 1930s.
In Santiago, Chile, the Hotel Carrera (no longer a hotel) is a very fine example of Art Deco architecture. Art deco buildings are also numerous in Montevideo, Uruguay, including the Palacio Salvo, which was South America's tallest building when it was built in the late 1920s. Another example of Art Deco in Latin America is the Edificio El Moro in Mexico which has the Loteria Nacional nowadays, it was also the biggest building of Mexico City at the time it was completed
In Argentina, architect Alejandro Virasoro introduced Art Deco in 1926 and developed the use of reinforced concrete, with the Banco El Hogar Argentino and the Casa del Teatro (both in Buenos Aires) being his most important works. The Kavanagh building (1934), by SÃ¡nchez, Lagos and de la Torre, was the tallest reinforced concrete structure at its time, and a notable example of late Art Deco style. In the Buenos Aires Province, architect Francisco Salamone designed cemetery portals, city halls and slaughterhouses commissioned by the provincial government in the 1930s; his designs combined Art DSeco with futurism. In Rosario, Santa Fe, the Palacio Minetti is the most representative Art Deco piece.
Europe.
United Kingdom.
During the 1930s, Art Deco had a noticeable effect on house design in the United Kingdom, as well as the design of various public buildings. Straight, white-rendered house frontages rising to flat roofs, sharply geometric door surrounds and tall windows, as well as convex-curved metal corner windows, were all characteristic of that period.
In London, the former Arsenal Stadium has the famous East Stand facade. It remains at the Arsenal football club's old home at Highbury, London Borough of Islington, which was vacated in the summer of 2006. Opened in October 1936, the structure now has Grade II listed status and has been converted into apartments. William Bennie, the organizer of the project, famously used the Art Deco style in the final design which was considered one of the most opulent and impressive stands of world football. The London Underground is also famous for many examples of Art Deco architecture.
Du Cane Court, in Balham, south-west London, is a good example of the Art Deco style. It was thought to be possibly the largest block of privately owned apartments under one roof in Britain at the time it was built, and the first to employ pre-stressed concrete. It has a grand reception area and is surrounded by Japanese-style gardens; and it has had many famous residents, especially from the performing arts. Elsewhere in south-west London, is the famous Battersea Power Station, which has appeared in films and artwork including the cover of Pink Floyd's 1977 album "Animals". Partially built in the 1930s, the building retains its powerful Art Deco facade.
In North West England, the Midland Hotel, Morecambe is considered one of the finest surviving examples from this period, with famous sculptures by artist Eric Gill. The buildings and structures related to the Queensway Tunnel which connects Liverpool and Birkenhead are also distinctly Art Deco. Other notable Art Deco buildings in Liverpool include Philharmonic Hall and the former terminal building at Liverpool Airportânow the Crowne Plaza LJLA.
In North East England, the Wills Building, an old cigarette factory, is a fine Art Deco building built in the late 1940s in Newcastle upon Tyne.
Spain.
Valencia was built profusely in Art Deco style during the period of economic bounty between wars in which Spain remained neutral. Particularly remarkable are the famous bath house Las Arenas, the building hosting the rectorship of the University of Valencia and the cinemas Rialto (currently the Filmoteca de la Generalitat Valenciana), Capitol (reconverted into an office building) and Naruto.
Germany.
In Germany two variations of Art Deco flourished in the 1920s and 30s: The Neue Sachlichkeit (New Objectivity) employed the same curving horizontal lines and nautical motifs that are known as Streamline Moderne in the Anglophone world. While Neue Sachlichkeit was rather austere and reduced (eventually merging with the Bauhaus style), Expressionist architecture came up with a more emotional use of shapes, colors and textures, partly reinterpreting shapes from the German and Baltic Brick Gothic style. Notable examples are Erich Mendelsohn's "Mossehaus" and "SchaubÃ¼hne" theater in Berlin, Fritz HÃ¶ger's "Chilehaus" in Hamburg and his "Kirche am Hohenzollernplatz" in Berlin, the "Anzeiger Tower" in Hannover and the "Borsig Tower" in Berlin. Art deco architecture was revived in the late-20th century by architects like Hans Kollhoff (see his tower on Potsdamer Platz), Jan Kleihues and Tobias NÃ¶fer.
The 1921 Mossehaus in Berlin by Erich Mendelsohn was a pioneering design in Art Deco and Streamline Moderne, that displays how the Deco style spread and evolved in Europe.
Norway.
An example of Art Deco in Norway is found in the Student Society in Trondheim (built 1927â29). Its interior is based on an abandoned circus, so that the exterior exhibits a characteristic round shape.
Romania.
As a result of the inter-war period of rapid development, cities in Romania have numerous Art Deco buildings, including government buildings, hotels, and private houses. The best representative in this regard is the capital, Bucharest, which, despite the widespread destruction of its architecture during Communist times, still has many Art Deco examples, both on its main boulevards and in the lesser known parts of the city. 
Constanta has the second number of art deco buildings after Bucharest.
PloieÅti also has many Art Deco houses.
Lithuania.
Like Romania, Lithuania too experienced booming industrial growth during the Interwar period. This resulted in the rapid modernization of the city of Kaunas in particular. At this time it became the temporary capital of Lithuania. Vytautas the Great War Museum, built in 1936 and located in downtown Kaunas, along with the Central Post Building and the Pienocentras HQ Building (1934) are the three most prominent Art Deco structures in the city. Today many of these buildings still stand, and apartment complexes and large government buildings alike survive from this time, even through the Nazi and Soviet occupations of Kaunas. Many other buildings around the city were built in the Bauhaus style.
Belgium.
One of the largest Art Deco buildings in Western Europe is the Basilica of the Sacred Heart in Koekelberg, Brussels. In 1925, architect Albert van Huffel won the Grand Prize for Architecture with his scale model of the basilica at the "Exposition Internationale des Arts DÃ©coratifs et Industriels Modernes" in Paris.
Asia.
Mumbai has the second largest number of Art Deco buildings after Miami. The Art Deco style was also adopted in Chennai between the 1920s and 1940s though it was utilized to a lesser extent.
In Indonesia, the largest stock of Dutch East Indies-era buildings is found in the large cities of Java. Bandung has one of the largest remaining collections of 1920s Art Deco buildings in the world, including those by several Dutch architects and planners, notably Albert Aalbers's DENIS bank (1936) in Braga Street and the renovated Savoy Homann Hotel (1939). Others were Thomas Karsten, Henri Maclaine Pont, J Gerber and C.P.W. Schoemaker. The SociÃ«teit Concordia (now Merdeka Building) is a historic building in Bandung designed by Van Galen Last and C.P. Wolff Schoemaker. In Jakarta, surviving Art Deco buildings include the Nederlandsche Handel Maatschappij building (1929), now the Museum Bank Mandiri, by J. de Bruyn, A. P. Smiths, and C. Van de Linde; the Jakarta Kota Station (1929) designed by Frans Johan Louwrens Ghijsels, and the Metropole Cinema in Menteng.
In China, at least sixty buildings, of which many are Art Deco, designed by Hungarian architect LÃ¡szlÃ³ Hudec survive in downtown Shanghai.
In Japan, the 1933 residence of Prince Asaka in Tokyo is an Art Deco house turned museum.
In the Philippines, Art Deco buildings are found mostly in Manila, Iloilo City, and Sariaya. The best examples of these are the older buildings of the Far Eastern University and the Manila Metropolitan Theater, both in Manila.
Examples of Art Deco architecture in Malaysia include the Central Market, the Coliseum Theatre, the Odeon Cinema and the Lee Rubber Building in Kuala Lumpur, and the Standard Chartered Building, India House, and the OCBC Bank Building in George Town, Penang.
In Bangladesh, a number of Art Deco structures are found in Chittagong and Rajshahi. Built during the 1950s, they include the University of Rajshahi, the Chittagong Customs House and the Jamuna Bhaban among others. 
There are a few Art Deco survivors in Hong Kong, (The Peninsula Hong Kong 1928, Bank of China Building (Hong Kong) 1952) with high profile buildings demolished to make way for the modern skylines (HSBC Building 1935, demolished 1978). Residential buildings in Hong Kong Island and Kowloon used basic Art Deco theme at a much smaller scale. The Star Ferry Pier, Central and Tsim Sha Tsui Ferry Pier (both built 1957 and former demolished 2006) were both Streamline Moderne with some Art Deco elements.
Oceania.
New Zealand.
The town of Napier, New Zealand, was rebuilt in the Art Deco style after being largely razed by the Hawke's Bay earthquake of 3 February 1931 and is the world's most consistently Art Deco city. Although a few Art Deco buildings were replaced with contemporary structures during the 1960s, 1970s and 1980s, most of the centre remained intact long enough to become recognized as architecturally unique, and from the 1990s onwards had been protected and restored. As of 2007, Napier has been nominated for UNESCO World Heritage Site status, the first cultural site in New Zealand to be nominated. According to the World Heritage Trust, when Napier is compared to the other cites noted for their Art Deco architecture, such as Miami Beach, Santa Barbara, Bandung in Indonesia (planned originally as the future capital of Java), and Asmara in Eritrea (built by the Italians as a model colonial city), "noneÂ ... surpass Napier in style and coherence.
Hastings was also rebuilt in Art Deco style after the 1931 Hawke's Bay earthquake, and many fine Art Deco buildings survive.
Wellington has retained a sizeable number of Art Deco buildings, in spite of constant post-World War II development.
Australia.
Australia also has many surviving examples of Art Deco architecture. Among the most notable are Sydney's ANZAC War Memorial, 'mini-skyscrapers', such as the Grace Building (Sydney) and the Manchester Unity Building (Melbourne) featuring purely decorative towers to circumvent the height restriction laws of the time; the AWA Tower in Sydney, consists of a radio transmission tower atop a 15-story building; and the former Russell Street Police Headquarters in Melbourne, with its main multi-storey brick building designed by architect Percy Edgar Everett, reminiscent of the design of the Empire State Building.
In St Kilda, Victoria, the Palais and the Astor theatres are considered some of the finest surviving Art Deco buildings in Australia, while many rural towns such as Wagga Wagga, Innisfail, Albury and Griffith also have significant amounts of Art Deco buildings and homes.
Africa.
Africa's most celebrated examples of Art Deco were built in Eritrea during Italian rule. Many buildings survive in Asmara, the capital, and elsewhere.
Also there are many buildings in downtown Casablanca, Morocco's economic capital. During Portuguese colonial rule in Angola and Mozambique, a large number of buildings were erected especially in the capital cities of Luanda and Maputo. Cities in South Africa also contain examples of Art Deco design such as the City Hall, in Benoni, Gauteng, constructed in 1937. There are a few Art Deco buildings in Egypt, one of the most famous being the former Cadillac dealership in downtown Cairo and Casa d'Italia in Port Said (1936)â designed by the Italian architect Clemente Busiri Vici.

</doc>
<doc id="1884" url="http://en.wikipedia.org/wiki?curid=1884" title="ASCII art">
ASCII art

ASCII art is a graphic design technique that uses computers for presentation and consists of pictures pieced together from the 95 printable (from a total of 128) characters defined by the ASCII Standard from 1963 and ASCII compliant character sets with proprietary extended characters (beyond the 128 characters of standard 7-bit ASCII). The term is also loosely used to refer to text based visual art in general. ASCII art can be created with any text editor, and is often used with free-form languages. Most examples of ASCII art require a fixed-width font (non-proportional fonts, as on a traditional typewriter) such as Courier for presentation.
Among the oldest known examples of ASCII art are the
creations by computer-art pioneer Kenneth Knowlton from around 1966, who was working for Bell Labs at the time. "Studies in Perception I" by Ken Knowlton and Leon Harmon from 1966 shows some examples of their early ASCII art.
One of the main reasons ASCII art was born was because early printers often lacked graphics ability and thus characters were used in place of graphic marks. Also, to mark divisions between different print jobs from different users, bulk printers often used ASCII art to print large banners, making the division easier to spot so that the results could be more easily separated by a computer operator or clerk. ASCII art was also used in early e-mail when images could not be embedded. ASCII art can also be used for typesetting initials.
History.
Typewriter art.
Since 1867 typewriters have been used for creating visual art. The oldest known preserved example of typewriter art is a picture of a butterfly made in 1898 by Flora Stacey. 
Typewriter portraits by Hobart Reese gained attention in 1922.
Typewriter art was also called keyboard art. 
In the 1954 short film "Stamp Day for Superman", typewriter art was a feature of the plot.
TTY and RTTY.
TTY stands for "TeleTYpe" or "TeleTYpewriter" and is also known as Teleprinter or Teletype.
RTTY stands for Radioteletype; character sets such as Baudot code, which predated ASCII, were used. According to a chapter in the "RTTY Handbook", text images have been sent via teletypewriter as early as 1923. However, none of the "old" RTTY art has been discovered yet. What is known is that text images appeared frequently on radioteletype in the 1960s and the 1970s.
Line-printer art.
In the 1960s, Andries van Dam published a representation of an electronic circuit produced on an IBM 1403 line printer. At the same time, Kenneth Knowlton was producing realistic images, also on line printers, by overprinting several characters on top of one another.
ASCII art.
The widespread usage of ASCII art can be traced to the computer bulletin board systems of the late 1970s and early 1980s. The limitations of computers of that time period necessitated the use of text characters to represent images. Along with ASCII's use in communication, however, it also began to appear in the underground online art groups of the period. An ASCII comic is a form of webcomic which uses ASCII text to create images. In place of images in a regular comic, ASCII art is used, with the text or dialog usually placed underneath.
During the 1990s, graphical browsing and variable-width fonts became increasingly popular, leading to a decline in ASCII art. Despite this, ASCII art continued to survive through online MUDs, an acronym for "Multi-User Dungeon", (which are textual multiplayer role-playing video games), Internet Relay Chat, E-mail, message boards and other forms of online communication which commonly employ the needed fixed-width.
ANSI.
ASCII and more importantly, ANSI were staples of the early technological era; terminal systems relied on coherent presentation using color and control signals standard in the terminal protocols.
Over the years, warez groups began to enter the ASCII art scene. Warez groups usually release .nfo files with their software, cracks or other general software reverse-engineering releases. The ASCII art will usually include the warez group's name and maybe some ASCII borders on the outsides of the release notes, etc.
BBS systems were based on ASCII and ANSI art, as were most DOS and similar console applications, and the procursor to AOL.
Uses.
ASCII art is used wherever text can be more readily printed or transmitted than graphics, or in some cases, where the transmission of pictures is not possible. This includes typewriters, teleprinters, non-graphic computer terminals, printer separators, in early computer networking (e.g., BBSes), e-mail, and Usenet news messages. ASCII art is also used within the source code of computer programs for representation of company or product logos, and flow control or other diagrams. In some cases, the entire source code of a program is a piece of ASCII art â for instance, an entry to one of the earlier International Obfuscated C Code Contest is a program that adds numbers, but visually looks like a binary adder drawn in logic ports.
Examples of ASCII-style art predating the modern computer era can be found in the June 1939, July 1948 and October 1948 editions of Popular Mechanics.
"0verkill" is a 2D platform multiplayer shooter game designed entirely in colour ASCII art. MPlayer and VLC media player can display videos as ASCII art. ASCII art is used in the making of DOS-based ZZT games.
Many game walkthrough guides come as part of a basic .txt file; this file often contains the name of the game in ASCII art. Such as below, word art is created using backslashes and other ASCII values in order to create the illusion of 3D.
Types and styles.
Different techniques could be used in ASCII art to obtain different artistic effects. Electronic circuits and diagrams were implemented by typewriter or teletype and provided the pretense for ASCII.
Line art, for creating shapes: 
 .--. /\ ____
 '--' /__\ (^._.^)~ <(o.o )>
Solid art, for creating filled objects:
 .g@8g. db
 'Y8@P' d88b
Shading, using symbols with various intensities for creating gradients or contrasts:
 :$#$: "4b. ':.
 :$#$: "4b. ':.
Combinations of the above, often used as signatures, for example, at the end of an email:
 |\_/| **************************** (\_/)
 / @ @ \ * "Purrrfectly pleasant" * (='.'=)
 ( > Âº < ) * Poppy Prinz * (")_(")
 `Â»xÂ«Â´ * (pprinz@example.com) *
 / O \ ****************************
As-Pixel Characters, use combinations of â , â , â and â to make pictures:<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
 ââââââââââââââââââââââââââââ<br>
Emoticons and verticons.
The simplest forms of ASCII art are combinations of two or three characters for expressing emotion in text. They are commonly referred to as 'emoticon', 'smilie', or 'smiley'.
There is another type of one-line ASCII art that does not require the mental rotation of pictures, which is widely known in Japan as kaomoji (literally "face characters".) Traditionally, they are referred to as "ASCII face".
More complex examples use several lines of text to draw large symbols or more complex figures.
Popular smileys.
Hundreds of different text smileys were developed over time, but only a few were generally accepted, used and understood.
ASCII comic.
An ASCII comic is a form of webcomic.
The Adventures of Nerd Boy.
The Adventures of Nerd Boy, or just Nerd Boy is an ASCII comic by Joaquim GÃ¢ndara between 6 August 2001 and 17 July 2007, consisting of 600 strips. They were posted to ASCII art newsgroup alt.ascii-art and on the website. Some strips have been translated to Polish and French.
Styles of the computer underground text art scene.
Atari 400/800 ATASCII.
The Atari 400/800 which were released in 1979 did not follow the ASCII standard and had its own character set, called ATASCII. The emergence of ATASCII art coincided with the growing popularity of BBS Systems caused by availability of the acoustic couplers that were compatible with the 8-bit home computers. ATASCII text animations are also referred to as "break animations" by the Atari sceners.
C-64 PETSCII.
The Commodore 64, which was released in 1982, also did not follow the ASCII standard. The C-64 character set is called PETSCII, an extended form of ASCII-1963. As with the Atari's ATASCII art, C-64 fans developed a similar scene that used PETSCII for their creations.
"Block ASCII" / "High ASCII" style ASCII art on the IBM PC.
So-called "block ASCII" or "high ASCII" uses the extended characters of the 8-bit code page 437, which is a proprietary standard introduced by IBM in 1979 (ANSI Standard x3.16) for the IBM PC DOS and MS-DOS operating systems. "Block ASCIIs" were widely used on the PC during the 1990s until the Internet replaced BBSes as the main communication platform. Until then, "block ASCIIs" dominated the PC Text Art Scene.
The first art scene group that focused on the extended character set of the PC in their art work was called "Aces of ANSI Art," or "AAA." Some members left in 1990, and formed a group called ACiD, "ANSI Creators in Demand." In that same year the second major underground art scene group was founded, ICE, "Insane Creators Enterprise".
There is some debate between ASCII and block ASCII artist, with "Hardcore" ASCII artists maintaining that block ASCII art is in fact not ANSI art, because it does not use the 128 characters of the original ASCII standard. On the other hand, block ASCII artists argue that if their art uses only characters of the computers character set, then it is to be called ASCII, regardless if the character set is proprietary or not.
Microsoft Windows does not support the ANSI Standard x3.16. One can view block ASCIIs with a text editor using the font "Terminal", but it will not look exactly as it was intended by the artist. With a special ASCII/ANSI viewer, such as ACiDView for Windows (see ASCII and ANSI art viewers), one can see block ASCII and ANSI files properly. An example that illustrates the difference in appearance is part of this article. Alternatively, one could look at the file using the Type command in the command prompt.
"Amiga"/"Oldskool" style ASCII art.
In the art scene one popular ASCII style that used the 7-bit standard ASCII character set was the so-called "Oldskool" Style. It is also called "Amiga style", due to its origin and widespread use on the Commodore Amiga Computers. The style uses primarily the characters: _/\-+=.()<>:. The "oldskool" art looks more like the outlined drawings of shapes than real pictures.
This is an example of "Amiga style" (also referred to as "old school" or "oldskool" style) scene ASCII art.
The Amiga ASCII Scene surfaced in 1992, seven years after the introduction of the Commodore Amiga 1000. The Commodore 64 PETSCII scene did not make the transition to the Commodore Amiga as the C64 demo and warez scenes did. Among the first Amiga ASCII art groups were ART, Epsilon Design, Upper Class, Unreal (later known as "DeZign"). This means that the text art scene on the Amiga was actually younger than the text art scene on the PC. The Amiga artists also did not call their ASCII art style "Oldskool". That term was introduced on the PC. When and by whom is unknown and lost in history.
The Amiga style ASCII artwork was most often released in the form of a single text file, which included all the artwork (usually requested), with some design parts in between, as opposed to the PC art scene where the art work was released as a ZIP archive with separate text files for each piece. Furthermore, the releases were usually called "ASCII collections" and not "art packs" like on the IBM PC.
In text editors.
This kind of ASCII art is handmade in a text editor. Popular editors used to make this kind of ASCII art include CygnusEditor aka CED (Amiga) and EditPlus2 (PC).
Oldskool font example from the PC, which was taken from the ASCII Editor FIGlet.
Newskool style ASCII art.
"Newskool" is a popular form of ASCII art which capitalizes on character strings like "$#Xxo". In spite of its name, the style is not "new"; on the contrary, it was very old but fell out of favor and was replaced by "Oldskool" and "Block" style ASCII art. It was dubbed "Newskool" upon its comeback and renewed popularity at the end of the 1990s.
Newskool changed significantly as the result of the introduction of extended proprietary characters. The classic 7-bit standard ASCII characters remain predominant, but the extended characters are often used for "fine tuning" and "tweaking". The style developed further after the introduction and adaptation of Unicode.
Methods for generating ASCII art.
While some prefer to use a simple text editor to produce ASCII art, specialized programs, such as JavE have been developed that often simulate the features and tools in bitmap image editors. For Block ASCII art and ANSI art the artist almost always uses a special text editor, because the required characters are not available on a standard keyboard.
The special text editors have sets of special characters assigned to existing keys on the keyboard. Popular MS DOS based editors, such as TheDraw and ACiDDraw had multiple sets of different special characters mapped to the F-Keys to make the use of those characters easier for the artist who can switch between individual sets of characters via basic keyboard shortcuts. PabloDraw is one of the very few special ASCII/ANSI art editors that were developed for MS Windows XP.
Image to text conversion.
Other programs allow one to automatically convert an image to text characters, which is a special case of vector quantization. A method is to sample the image down to grayscale with less than 8-bit precision, and then assign a character for each value. Such ASCII art generators often allow users to choose the intensity and contrast of the generated image.
Examples of converted images are given below.
This is one of the earliest forms of ASCII art, dating back to the early days of the 1960s minicomputers and teletypes. During the 1970s it was popular in malls to get a t-shirt with a photograph printed in ASCII art on it from an automated kiosk manned by a computer. With the advent of the web and HTML and CSS, many ASCII conversion programs will now quantize to a full RGB colorspace, enabling colorized ASCII images.
Still images or movies can also be converted to ASCII on various Linux and UNIX computers using the or graphics device driver, or the VLC media player under Windows, Linux or OS X; all of which render the screen using ASCII symbols instead of pixels. See also .
There are also a number of smartphone applications, such as ASCII cam for Android, that generate ASCII art in real-time using input from the phone's camera. These applications typically allow the ASCII art to be saved as either a text file or as an image made up of ASCII text.
Non fixed-width ASCII.
Most ASCII art is created using a monospace font, where all characters are identical in width (Courier is a popular monospace font). Early computers in use when ASCII art came into vogue had monospace fonts for screen and printer displays. Today most of the more commonly used fonts in word processors, web browsers and other programs are proportional fonts, such as Helvetica or Times Roman, where different widths are used for different characters. ASCII art drawn for a fixed width font will usually appear distorted, or even unrecognizable when displayed in a proportional font.
Some ASCII artists have produced art for display in proportional fonts. These ASCIIs, rather than using a purely shade-based correspondence, use characters for slopes and borders and use block shading. These ASCIIs generally offer greater precision and attention to detail than fixed-width ASCIIs for a lower character count, although they are not as universally accessible since they are usually relatively font-specific.
Animated ASCII art.
Animated ASCII art started in 1970 from so-called VT100 animations produced on vt100 terminals. These animations were simply text with cursor movement instructions, deleting and erasing the characters necessary to appear animated. Usually, they represented a long hand-crafted process undertaken by a single person to tell a story.
Contemporary web browser revitalized animated ASCII art again. It became possible to display animated ASCII art via JavaScript or Java applets. Static ASCII art pictures are loaded and displayed one after another, creating the animation, very similar to how movie projectors unreel film reel and project the individual pictures on the big screen at movie theaters. A new term was born: "ASCIImation" â another name of Animated ASCII Art. A seminal work in this arena is the Star Wars ASCIImation. More complicated routines in JavaScript generate more elaborate ASCIImations showing effects like Morphing effects, star field emulations, fading effects and calculated images, such as mandelbrot fractal animations.
There are now many tools and programs that can transform raster images into text symbols; some of these tools can operate on streaming video. For example, the music video for pop singer Beck Hansen's song "Black Tambourine" is made up entirely of ASCII characters that approximate the original footage.
Other text-based visual art.
There are a variety of other types of art using text symbols from character sets other than ASCII and/or some form of color coding. Despite not being pure ASCII, these are still often referred to as "ASCII art". The character set portion designed specifically for drawing is known as the line drawing characters or pseudo-graphics.
ANSI art.
The IBM PC graphics hardware in text mode uses 16 bits per character. It supports a variety of configurations, but in its default mode under DOS they are used to give 256 glyphs from one of the IBM PC code pages (Code page 437 by default), 16 foreground colors, eight background colors, and a flash option. Such art can be loaded into screen memory directly. ANSI.SYS, if loaded, also allows such art to be placed on screen by outputting escape sequences that indicate movements of the screen cursor and color/flash changes. If this method is used then the art becomes known as ANSI art. The IBM PC code pages also include characters intended for simple drawing which often made this art appear much cleaner than that made with more traditional character sets. Plain text files are also seen with these characters, though they have become far less common since Windows GUI text editors (using the Windows ANSI code page) have largely replaced DOS based ones.
Shift_JIS.
A large character selection and the availability of fixed-width characters allow Japanese users to use Shift JIS as a text-based art on Japanese websites.
Special circumstances of Japan.
Japanese mainly refer to ASCII-art (AA) as Shift-JIS Art in Japan.
In this background,Independently generation author of the Japanese original 
in Japan, the ASCII-NET(People with disabilities of related Bulletin board system.) Mr.," the author eastern emoticons. The PC communications in June 20, 1986 00:28:26(JST) From the "face mark" was published 
Art by the derived has an eastern emoticon, has been recognized as a Character of actors, that a reason.
In other words, the "ASCII", does not refer to the American Standard Code, refers to ASCII-NET's art as ASCII Corporation.
"See List of common emoticons#Eastern".
Unicode.
Unicode would seem to offer the ultimate flexibility in producing text based art with its huge variety of characters. However, finding a suitable fixed-width font is likely to be difficult if a significant subset of Unicode is desired. (Modern UNIX-style operating systems do provide complete fixed-width Unicode fonts, e.g. for xterm. Windows has the Courier New font which includes characters like ââ¥ââ¨ââ¥âºÆ¸ÌµÌ¡ÓÌµÌ¨ÌÆ·) Also, the common practice of rendering Unicode with a mixture of variable width fonts is likely to make predictable display hard if more than a tiny subset of Unicode is used. â½Êâ±·Òá´¥â±·Êâ¼ is an adequate representation of a cat's face in a font with varying character widths.
Overprinting (surprint).
In the 1970s and early 1980s it was popular to produce a kind of text art that relied on overprinting â the overall darkness of a particular character space dependent on how many characters, as well as the choice of character, printed in a particular place. Thanks to the increased granularity of tone, photographs were often converted to this type of printout. Even manual typewriters or daisy wheel printers could be used. The technique has fallen from popularity since all cheap printers can easily print photographs, and a normal text file (or an e-mail message or Usenet posting) cannot represent overprinted text. However, something similar has emerged to replace it: shaded or colored ASCII art, using ANSI video terminal markup or color codes (such as those found in HTML, IRC, and many internet message boards) to add a bit more tone variation. In this way, it is possible to create ASCII art where the characters only differ in color.
Creation.
ASCII art text editors are used to create ASCII art from scratch, or to edit existing ASCII art files.
ASCII art may be created from an existing digital image using an ASCII art converter, an online tool or a software application that automatically converts an image into ASCII art, using vector quantization. Typically, this is done by sampling the image down to grayscale with less than 8-bit precision, so that each value corresponds to different ASCII character.

</doc>
<doc id="1887" url="http://en.wikipedia.org/wiki?curid=1887" title="Alexius">
Alexius

Alexius is the Latinized form of the given name Alexios (, polytonic , "defender", cf. Alexander), especially common in the later Byzantine Empire. Variants include Alexis with the Russian Aleksey and its Ukrainian counterpart Oleksa/Oleksiy deriving from this form. The female form is Alexia () and its variants such as Alessia. 
It may refer to:

</doc>
<doc id="1890" url="http://en.wikipedia.org/wiki?curid=1890" title="American English">
American English

American English is a set of dialects of the English language used mostly in the United States. Approximately two-thirds of the world's native speakers of English live in the United States. The predominant accent of American English that is most free from regional, ethnic, or cultural distinctions is the accent known as General American.
English is the most widely-spoken language in the United States. English is the common language used by the federal government and is considered the de facto language of the United States due to its widespread use. English has been given official status by 30 of the 50 state governments. As an example, while both Spanish and English have equivalent status in the local courts of the Commonwealth of Puerto Rico, under federal law, English is the official language for any matters being referred to the United States District Court for the territory.
The use of English in the United States is a result of British colonization. The first wave of English-speaking settlers arrived in North America during the 17th century, followed by further migrations in the 18th and 19th centuries. Since then, American English has been influenced by the languages of West Africa, the Native American population, German, Irish, Spanish, and other languages of successive waves of immigrants to the U.S.
Phonology.
Compared with English as spoken in England, North American English is more homogeneous. Some distinctive accents can be found on the East Coast (for example, in eastern New England, New York City, Philadelphia, and Baltimore) partly because these areas were in close contact with England and imitated prestigious varieties of English at a time when these were undergoing changes. In addition, many speech communities on the East Coast have existed in their present locations for centuries, while the interior of the country was settled by people from all regions of the existing United States and developed a far more general linguistic pattern.
Studies on historical usage of English in the United States and the United Kingdom suggest that spoken American English did not simply evolve from British English, but rather retained many archaic features British English has since lost. Most North American speech is rhotic, as English was in most places in the 17th century. Rhoticity was further supported by Hiberno-English, West Country English and Scottish English as well as the fact most regions of England at this time also had rhotic accents. In most varieties of North American English, the sound corresponding to the letter "r" is an alveolar approximant or retroflex rather than a trill or a tap. The loss of syllable-final "r" in North America is confined mostly to the accents of eastern New England, New York City and surrounding areas and the coastal portions of the South, and African American Vernacular English.
In rural tidewater Virginia and eastern New England, 'r' is non-rhotic in accented (such as "bird", "work", "first", "birthday") as well as unaccented syllables, although this is declining among the younger generation of speakers. Dropping of syllable-final "r" sometimes happens in natively rhotic dialects if "r" is located in unaccented syllables or words and the next syllable or word begins in a consonant (for example, many North Americans drop the first 'r' in "particular"). In England, the lost "r" was often changed into (schwa), giving rise to a new class of falling diphthongs. Furthermore, the "er" sound of "fur or "butter, is realized in AmE as a monophthongal r-colored vowel (stressed or unstressed as represented in the IPA). This does not happen in the non-rhotic varieties of North American speech.
Some other English changes in which most North American dialects do not participate:
On the other hand, North American English has undergone some sound changes not found in other varieties of English speech:
Some mergers found in most varieties of both American and British English include:
Vocabulary.
North America has given the English lexicon many thousands of words, meanings, and phrases. Several thousand are now used in English as spoken internationally.
Creation of an American lexicon.
The process of coining new lexical items started as soon as the colonists began borrowing names for unfamiliar flora, fauna, and topography from the Native American languages. Examples of such names are "opossum, raccoon, squash" and "moose" (from Algonquian). Other Native American loanwords, such as "wigwam" or "moccasin", describe articles in common use among Native Americans. The languages of the other colonising nations also added to the American vocabulary; for instance, "cookie", "cruller", "stoop", and "pit" (of a fruit) from Dutch; "angst, kindergarten, sauerkraut" from German, "levee, portage" ("carrying of boats or goods") and (probably) "gopher" from French; "barbecue, stevedore, and rodeo" from Spanish.
Among the earliest and most notable regular "English" additions to the American vocabulary, dating from the early days of colonization through the early 19th century, are terms describing the features of the North American landscape; for instance, "run, branch, fork, snag, bluff, gulch, neck" (of the woods), "barrens, bottomland, notch, knob, riffle, rapids, watergap, cutoff, trail, timberline" and "divide". Already existing words such as "creek, slough, sleet" and (in later use) "watershed" received new meanings that were unknown in England.
Other noteworthy American toponyms are found among loanwords; for example, "prairie, butte" (French); "bayou" (Choctaw via Louisiana French); "coulee" (Canadian French, but used also in Louisiana with a different meaning); "canyon, mesa, arroyo" (Spanish); "vlei, skate, kill" (Dutch, Hudson Valley).
The word "corn", used in England to refer to wheat (or any cereal), came to denote the plant "Zea mays", the most important crop in the U.S., originally named "Indian corn" by the earliest settlers; wheat, rye, barley, oats, etc. came to be collectively referred to as "grain". Other notable farm related vocabulary additions were the new meanings assumed by "barn" (not only a building for hay and grain storage, but also for housing livestock) and "team" (not just the horses, but also the vehicle along with them), as well as, in various periods, the terms "range, (corn) crib, truck, elevator, sharecropping" and "feedlot."
"Ranch," later applied to a house style, derives from Mexican Spanish; most Spanish contributions came after the War of 1812, with the opening of the West. Among these are, other than toponyms, "chaps" (from "chaparreras), plaza, lasso, bronco, , rodeo;" examples of "English" additions from the cowboy era are "bad man, maverick, chuck" ("food") and "Boot Hill;" from the California Gold Rush came such idioms as "hit pay dirt" or "strike it rich." The word "blizzard" probably originated in the West. A couple of notable late 18th century additions are the verb "belittle" and the noun "bid," both first used in writing by Thomas Jefferson.
With the new continent developed new forms of dwelling, and hence a large inventory of words designating real estate concepts "(land office, lot, outlands, waterfront," the verbs "locate" and "relocate, betterment, addition, subdivision)," types of property "(log cabin, adobe" in the 18th century; "frame house, apartment, tenement house, shack, " in the 19th century; "project, condominium, townhouse, split-level, mobile home, multi-family" in the 20th century), and parts thereof "(driveway, breezeway, backyard, dooryard; clapboard, siding, trim, baseboard; stoop" (from Dutch), "family room, den;" and, in recent years, "HVAC, central air, walkout basement)."
Ever since the American Revolution, a great number of terms connected with the U.S. political institutions have entered the language; examples are "run (i.e, for office), gubernatorial, primary election, carpetbagger" (after the Civil War), "repeater", "lame duck" (a British term used originally in Banking) and "pork barrel." Some of these are internationally used (for example, "caucus, gerrymander, filibuster, exit poll)."
19th century onwards.
The development of industry and material innovations throughout the 19th and 20th centuries were the source of a massive stock of distinctive new words, phrases and idioms. Typical examples are the vocabulary of "railroading" (see further at rail terminology) and "transportation" terminology, ranging from names of roads (from "dirt roads" and "back roads" to "freeways" and "parkways)" to road infrastructure "(parking lot, overpass, rest area)," and from automotive terminology to "public transit" (for example, in the sentence ""riding" the "subway downtown""); such American introductions as "commuter" (from "commutation ticket), concourse, to board" (a vehicle), "to park, double-park" and "parallel park" (a car), "" or the noun "terminal" have long been used in all dialects of English.
Trades of various kinds have endowed (American) English with household words describing jobs and occupations "(bartender, longshoreman, patrolman, hobo, bouncer, bellhop, roustabout, white collar, blue collar, employee, boss" [from Dutch], "intern, busboy, mortician, senior citizen)," businesses and workplaces "(department store, supermarket, thrift store, gift shop, drugstore, motel, main street, gas station, hardware store, savings and loan, hock" [also from Dutch]), as well as general concepts and innovations "(automated teller machine, smart card, cash register, dishwasher, reservation" [as at hotels], "pay envelope, movie, mileage, shortage, outage, blood bank)."
Already existing English wordsâsuch as "store, shop, dry goods, haberdashery, lumber"âunderwent shifts in meaning; someâsuch as "mason, student, clerk", the verbs "can" (as in "canned goods"), "ship, fix, carry, enroll" (as in school), "run" (as in "run a business"), "release" and "haul"âwere given new significations, while others (such as "tradesman)" have retained meanings that disappeared in England. From the world of business and finance came "breakeven, merger, , downsize, disintermediation, bottom line;" from sports terminology came, jargon aside, "Monday-morning quarterback, cheap shot, game plan" (football); "in the ballpark, out of left field, off base, hit and run," and many other idioms from baseball; gamblers coined "bluff, , ante, bottom dollar, raw deal, pass the buck, ace in the hole, freeze-out, showdown;" miners coined "bedrock, bonanza, peter out, pan out" and the verb "prospect" from the noun; and railroadmen are to be credited with "make the grade, sidetrack, head-on," and the verb "railroad." A number of Americanisms describing material innovations remained largely confined to North America: "elevator, ground, gasoline;" many automotive terms fall in this category, although many do not "(hatchback, sport utility vehicle, station wagon, tailgate, motorhome, truck, pickup truck, to exhaust)."
In addition to the above-mentioned loans from French, Spanish, Mexican Spanish, Dutch, and Native American languages, other accretions from foreign languages came with 19th and early 20th century immigration; notably, from Yiddish "(chutzpah, schmooze, tush") and Germanâ"hamburger" and culinary terms like "frankfurter/franks, liverwurst, sauerkraut, wiener, deli(catessen); scram, kindergarten, gesundheit;" musical terminology "(whole note, half note," etc.); and apparently "cookbook, fresh" ("impudent") and "what gives?" Such constructions as "Are you coming with?" and "I like to dance" (for "I like dancing") may also be the result of German or Yiddish influence.
Finally, a large number of English colloquialisms from various periods are American in origin; some have lost their American flavor (from "OK" and "cool" to "nerd" and "24/7)," while others have not "(have a nice day, for sure);" many are now distinctly old-fashioned "(swell, groovy)." Some English words now in general use, such as "hijacking, disc jockey, boost, bulldoze" and "jazz," originated as American slang. Among the many English idioms of U.S. origin are "get the hang of, bark up the wrong tree, keep tabs, run scared, take a backseat, have an edge over, stake a claim, take a shine to, in on the ground floor, bite off more than one can chew, off/on the wagon, stay put, inside track, stiff upper lip, bad hair day, throw a monkey wrench, under the weather, jump bail, come clean, come again?, it ain't over till it's over, what goes around comes around," and "will the real x please stand up?"
Morphology.
American English has always shown a marked tendency to use nouns as verbs. Examples of verbed nouns are "interview, advocate, vacuum, lobby, pressure, rear-end, transition, feature, profile, spearhead, skyrocket, showcase, service" (as a car), "corner, torch, exit" (as in "exit the lobby"), "factor" (in mathematics), "gun" ("shoot"), "author" (which disappeared in English around 1630 and was revived in the U.S. three centuries later) and, out of American material, "proposition, graft" (bribery), "bad-mouth, vacation, major, backpack, backtrack, intern, ticket" (traffic violations), "hassle, blacktop, peer-review, dope" and "OD", and, of course "verbed" as used at the start of this sentence.
Compounds coined in the U.S. are for instance "foothill, flatlands, badlands, landslide" (in all senses), "overview" (the noun), ", teenager, brainstorm, , hitchhike, smalltime, , frontman, lowbrow" and "highbrow, hell-bent, foolproof, nitpick, about-face" (later verbed), "upfront" (in all senses), "fixer-upper, no-show;" many of these are phrases used as adverbs or (often) hyphenated attributive adjectives: "non-profit, for-profit, free-for-all, ready-to-wear, catchall, low-down, down-and-out, down and dirty, in-your-face, nip and tuck;" many compound nouns and adjectives are open: "happy hour, fall guy, capital gain, road trip, wheat pit, head start, plea bargain;" some of these are colorful "(empty nester, loan shark, , buzz saw, ghetto blaster, dust bunny)," others are euphemistic "(differently abled (physically challenged), human resources, affirmative action, correctional facility)."
Many compound nouns have the form verb plus preposition: ", stopover, lineup, , tryout, spin-off, rundown" ("summary"), "shootout, holdup, hideout, comeback, cookout, kickback, makeover, takeover, rollback" ("decrease"), "rip-off, come-on, shoo-in, fix-up, tie-in, tie-up" ("stoppage"), "stand-in." These essentially are nouned phrasal verbs; some prepositional and phrasal verbs are in fact of American origin "(spell out, figure out, hold up, brace up, size up, rope in, back up/off/down/out, step down, miss out, kick around, cash in, rain out, check in" and "check out" (in all senses), "fill in" ("inform"), "kick in" or "throw in" ("contribute"), "square off, sock in, sock away, factor in/out, come down with, give up on, lay off" (from employment), "run into" and "across" ("meet"), "stop by, pass up, put up" (money), "set up" ("frame"), "trade in, pick up on, pick up after, lose out)."
Noun endings such as "-ee (retiree), -ery (bakery), -ster (gangster)" and "-cian (beautician)" are also particularly productive. Some verbs ending in "-ize" are of U.S. origin; for example, "fetishize, prioritize, burglarize, accessorize, itemize, editorialize, customize, notarize, weatherize, winterize, Mirandize;" and so are some back-formations "(locate, fine-tune, evolute, curate, donate, emote, upholster, peeve" and "enthuse)." Among syntactical constructions that arose in the U.S. are "as of" (with dates and times), "outside of, headed for, meet up with, back of, convince someone to, not about to" and "lack for."
Americanisms formed by alteration of some existing words include notably "pesky, phony, rambunctious, pry" (as in "pry open", from "prize), putter" (verb), "buddy, sundae, skeeter, sashay" and "kitty-corner." Adjectives that arose in the U.S. are for example, "lengthy, bossy, cute" and "cutesy, grounded" (of a child), "punk" (in all senses), "sticky" (of the weather), "through" (as in "through train", or meaning "finished"), and many colloquial forms such as "peppy" or "wacky". American blends include "motel, guesstimate, infomercial" and "televangelist."
English words that survived in the United States and not in the United Kingdom.
A number of words and meanings that originated in Middle English or Early Modern English and that have been in everyday use in the United States dropped out in most varieties of British English; some of these have cognates in Lowland Scots. Terms such as "fall" ("autumn"), "faucet" ("tap"), "diaper" ("nappy"), "candy" ("sweets"), "skillet", "eyeglasses" and "obligate" are often regarded as Americanisms. "Fall" for example came to denote the season in 16th century England, a contraction of Middle English expressions like "fall of the leaf" and "fall of the year".
During the 17th century, English immigration to the British colonies in North America was at its peak and the new settlers took the English language with them. While the term "fall" gradually became obsolete in Britain, it became the more common term in North America. "Gotten" (past participle of "get") is often considered to be an Americanism, although there are some areas of Britain, such as Lancashire and North East England, that still continue to use it and sometimes also use "putten" as the past participle for "put" (which is not done by most speakers of American English).
Other words and meanings, to various extents, were brought back to Britain, especially in the second half of the 20th century; these include "hire" ("to employ"), "quit" ("to stop", which spawned "quitter" in the U.S.), "I guess" (famously criticized by H. W. Fowler), "baggage", "hit" (a place), and the adverbs "overly" and "presently" ("currently"). Some of these, for example "monkey wrench" and "wastebasket", originated in 19th century Britain.
The mandative subjunctive (as in "the City Attorney suggested that the case "not be closed"") is livelier in American English than it is in British English. It appears in some areas as a spoken usage and is considered obligatory in contexts that are more formal. The adjectives "mad" meaning "angry", "smart" meaning "intelligent", and "sick" meaning "ill" are also more frequent in American (these meanings are also frequent in Hiberno-English) than British English.
Regional differences.
While written American English is standardized across the country, there are several recognizable variations in the spoken language, both in pronunciation and in vernacular vocabulary. "General American" is the name given to any American accent that is relatively free of noticeable regional influences.
Eastern seaboard.
After the Civil War, the settlement of the western territories by migrants from the Eastern U.S. led to dialect mixing and leveling, (koineization) so that regional dialects are most strongly differentiated along the Eastern seaboard. The Connecticut River and Long Island Sound is usually regarded as the southern/western extent of New England speech, which has its roots in the speech of the Puritans from East Anglia who settled in the Massachusetts Bay Colony.
The Potomac River generally divides a group of Northern coastal dialects from the beginning of the Coastal Southern dialect area; in between these two rivers several local variations exist, chief among them the one that prevails in and around New York City and northern New Jersey, which developed on a Dutch substratum after the English conquered New Amsterdam. The main features of Coastal Southern speech can be traced to the speech of the English from the West Country who settled in Virginia after leaving England at the time of the English Civil War.
Midwest.
A distinctive speech pattern also appears near the border between Canada and the United States, centered on the Great Lakes region (but only on the American side). This is the Inland North Dialectâthe "standard Midwestern" speech that was the basis for General American in the mid-20th century (although it has been recently modified by the northern cities vowel shift). Those not from this area frequently confuse it with the North Midland dialect treated below, referring to both collectively as "Midwestern" in the Mid-Atlantic region or "Northern" in the Southern US. The so-called '"Minnesotan" dialect is also prevalent in the cultural Upper Midwest, and is characterized by influences from the German and Scandinavian settlers of the region (like "yah" for yes, pronounced similarly to "ja" in German, Norwegian and Swedish). In parts of Pennsylvania and Ohio, another dialect known as Pennsylvania Dutch English is also spoken.
Interior.
In the interior, the situation is very different. West of the Appalachian Mountains begins the broad zone of what is generally called "Midland" speech. This is divided into two discrete subdivisions, the North Midland that begins north of the Ohio River valley area, and the South Midland speech; sometimes the former is designated simply "Midland" and the latter is reckoned as "Highland Southern". The North Midland speech continues to expand westward until it becomes the closely related Western dialect which contains Pacific Northwest English as well as the well-known California English, although in the immediate San Francisco area some older speakers do not possess the cotâcaught merger and thus retain the distinction between words such as cot and caught which reflects a historical Mid-Atlantic heritage.
The South Midland or Highland Southern dialect follows the Ohio River in a generally southwesterly direction, moves across Arkansas and Oklahoma west of the Mississippi, and peters out in West Texas. It is a version of the Midland speech that has assimilated some coastal Southern forms (outsiders often mistakenly believe South Midland speech and coastal South speech to be the same).
Although no longer region-specific, African American Vernacular English, which remains prevalent among African Americans, has a close relationship to Southern varieties of AmE and has greatly influenced everyday speech of many Americans.
The island state of Hawaii has a distinctive Hawaiian Pidgin.
Finally, dialect development in the United States has been notably influenced by the distinctive speech of such important cultural centers as Baltimore, Boston, Buffalo, Charleston, Cleveland, Chicago, Detroit, Miami, New Orleans, New York City, Philadelphia and Pittsburgh, which imposed their marks on the surrounding areas.
Differences between British and American English.
American English and British English (BrE) differ at the levels of phonology, phonetics, vocabulary, and, to a much lesser extent, grammar and orthography.
The first large American dictionary, "An American Dictionary of the English Language", was written by Noah Webster in 1828; Webster intended to show that the United States, which was a relatively new country at the time, spoke a different dialect from that of Britain.
Differences in grammar are relatively minor, and normally do not affect mutual intelligibility; these include: different use of some verbal auxiliaries; formal (rather than notional) agreement with collective nouns; different preferences for the past forms of a few verbs (for example, AmE/BrE: "learned"/"learnt", "burned"/"burnt", "snuck/sneaked", "dove/dived"); different prepositions and adverbs in certain contexts (for example, AmE "in school," BrE "at school"); and whether or not a definite article is used, in very few cases (AmE "to the hospital", BrE "to hospital"; contrast, however, AmE "actress Elizabeth Taylor", BrE "the actress Elizabeth Taylor"). Often, these differences are a matter of relative preferences rather than absolute rules; and most are not stable, since the two varieties are constantly influencing each other.
Differences in orthography are also minor. The main differences are that American English uses spellings such as "flavor" for British "flavour", "fiber" for "fibre", "defense" for "defence", "analyze" for "analyse", "catalog" for "catalogue" and "traveling" for "travelling". Noah Webster popularized such spellings in America, but he did not invent most of them. Rather, "he chose already existing options [...] on such grounds as simplicity, analogy or etymology". Other differences are due to the francophile tastes of 19th century Victorian England (for example they preferred "programme" for "program", "manoeuvre" for "maneuver", "cheque" for "check", etc.). AmE uses "-ize" in words like "realize". BrE prefers "-ise", but also uses "-ize" (see Oxford spelling).
AmE sometimes favors words that are morphologically more complex, whereas BrE uses clipped forms, such as AmE "transportation" and BrE "transport" or where the British form is a back-formation, such as AmE "burglarize" and BrE "burgle" (from "burglar"). It should, however, be noted that while individuals usually use one or the other, both forms will be widely understood and mostly used alongside each other within the two systems.

</doc>
<doc id="1893" url="http://en.wikipedia.org/wiki?curid=1893" title="Albert Spalding">
Albert Spalding

Albert Goodwill Spalding (September 2, 1850 â September 9, 1915) was an American pitcher, manager and executive in the early years of professional baseball, and the co-founder of A.G. Spalding sporting goods company. He played major league baseball between 1871 and 1878. In 1877, he became the first well-known player to use a fielding glove; such gloves were among the items sold at his sporting goods store.
After his retirement as a player, Spalding remained active with the Chicago White Stockings as president and part-owner. In the 1880s, he took players on the first world tour of baseball. With William Hulbert, Spalding organized the National League. He later called for the commission that investigated the origins of baseball and credited Abner Doubleday with creating the game. He also wrote the first set of official baseball rules.
Baseball career.
Player.
Having played baseball throughout his youth, Spalding first played competitively with the Rockford Pioneers, a youth team, which he joined in 1865. After pitching his team to a 26â2 victory over a local men's amateur team (the Mercantiles), he was approached at the age of 15 by another, the Forest Citys, for whom he played for two years. In the autumn of 1867 he accepted a $40 per week contract, nominally as a clerk, but really to play professionally for the Chicago Excelsiors, not an uncommon arrangement used to circumvent the rules of the time, which forbade the hiring of professional players. Following the formation of baseball's first professional organization, the National Association of Professional Base Ball Players (which became known as the National Association, the Association, or NA) in 1871, Spalding joined the Boston Red Stockings (precursor club to the modern Atlanta Braves) and was highly successful; winning 206 games (and losing only 53) as a pitcher and batting .323 as a hitter.
William Hulbert, principal owner of the Chicago White Stockings, did not like the loose organization of the National Association and the gambling element that influenced it, so he decided to create a new organization, which he dubbed the National League of Baseball Clubs. To aid him in this venture, Hulbert enlisted the help of Spalding. Playing to the pitcher's desire to return to his Midwestern roots and challenging Spalding's integrity, Hulbert convinced Spalding to sign a contract to play for the White Stockings (now known as the Chicago Cubs) in 1876. Spalding then coaxed teammates Deacon White, Ross Barnes and Cal McVey, as well as Philadelphia Athletics players Cap Anson and Bob Addy, to sign with Chicago. This was all done under complete secrecy during the playing season because players were all free agents in those days and they did not want their current club and especially the fans to know they were leaving to play elsewhere the next year. News of the signings by the Boston and Philadelphia players leaked to the press before the season ended and all of them faced verbal abuse and physical threats from the fans of those cities.
He was "the premier pitcher of the 1870s", leading the league in victories for each of his six full seasons as a professional. During each of those years he was his team's only pitcher. In 1876, Spalding won 47 games as the prime pitcher for the White Stockings and led them to win the first-ever National League pennant by a wide margin.
In 1877, Spalding began to use a glove to protect his catching hand. People had used gloves previously, but never had a star like Spalding used one. Spalding had an ulterior motive for doing so: he now owned a sporting goods store which sold baseball gloves and wearing one himself was good advertising for his business.
Spalding retired from playing baseball in 1878 at the age of 27, although he continued as president and part owner of the White Stockings and a major influence on the National League. Spalding's .796 career winning percentage (from an era when teams played about once or twice a week) is the highest ever achieved by a baseball pitcher.
Organizer and executive.
In the months after signing for Chicago, Hulbert and Spalding organized the National League by enlisting the four major teams in the East and the three other top teams in what was then considered to be the West. Joining Chicago initially were the leading teams from Cincinnati, Louisville, and St. Louis. The owners of these western clubs accompanied Hulbert and Spalding to New York where they secretly met with owners from New York, Philadelphia, Hartford, and Boston. Each signed the league's constitution, and the National League was officially born. "Spalding was thus involved in the transformation of baseball from a game of gentlemen athletes into a business and a professional sport." Although the National Association held on for a few more seasons, it was no longer recognized as the premier organization for professional baseball. Gradually, it faded out of existence and was replaced by myriad minor leagues and associations around the country.
In 1905, after Henry Chadwick wrote an article saying that baseball grew from the British sports of cricket and rounders, Spalding called for a commission to find out the real source of baseball. The commission called for citizens who knew anything about the founding of baseball to send in letters. After three years of searching, on December 30, 1907, Spalding received a letter that (erroneously) declared baseball to be the invention of Abner Doubleday. The commission, though, was biased, as Spalding would not appoint anyone to the commission if they believed the sport was somewhat related to the English sport of rounders. Just before the commission, in a letter to sportswriter Tim Murnane, Spalding noted, "Our good old American game of baseball must have an American Dad." The project, later called the Mills Commission, concluded that "Base Ball had its origins in the United States" and "the first scheme for playing baseball, according to the best evidence available to date, was devised by Abner Doubleday at Cooperstown, N.Y., in 1839."
Receiving the archives of Henry Chadwick in 1908, Spalding combined these records with his own memories (and biases) to write "America's National Game" (published 1911) which, despite its flaws, was probably the first scholarly account of the history of baseball.
Businessman.
In 1874 while Spalding was playing and organizing the league, Spalding and his brother Walter began a sporting goods store in Chicago, which grew rapidly (14 stores by 1901) and expanded into a manufacturer and distributor of all kinds of sporting equipment. The company became "synonymous with sporting goods" and is still a going concern.
Spalding published the first official rules guide for baseball. In it he stated that only Spalding balls could be used (previously, the quality of the balls used had been subpar). Spalding also founded the "Baseball Guide," which at the time was the most widely read baseball publication.
In 1888â1889, Spalding took a group of major league players around the world to promote baseball and Spalding sporting goods. This was the first-ever world baseball tour. Playing across the western U.S., the tour made stops in Hawaii (although no game was played), New Zealand, Australia, Ceylon, Egypt, Italy, France, and England. The tour returned to grand receptions in New York, Philadelphia, and Chicago. The tour included future Hall of Famers Cap Anson and John Montgomery Ward. While the players were on the tour, the National League instituted new rules regarding player pay that led to a revolt of players, led by Ward, who started the Players League the following season (1890). The league lasted one year, partially due to the anti-competitive tactics of Spalding to limit its success.
In 1900 Spalding was appointed by President McKinley as the USA's Commissioner at that year's Summer Olympic Games.
Other activities.
Spalding had been a prominent member of the Theosophical Society under William Quan Judge. In 1900, Spalding moved to San Diego with his newly acquired second wife, Elizabeth and became a prominent member and supporter of the Theosophical community Lomaland, which was being developed on Point Loma by Katherine Tingley. He built an estate in the Sunset Cliffs area of Point Loma where he lived with Elizabeth for the rest of his life. The Spaldings raised race horses and collected Chinese fine furniture and art.
The Spaldings had an extensive library which included many volumes on Theosophy, art, and literature. In 1907-1909 he was the driving force behind the development of a paved road, known as the "Point Loma boulevard", from downtown San Diego to Point Loma and Ocean Beach; the road also provided good access to Lomaland. It later provided the basis for California State Route 209. He proposed the project, supervised it on behalf of the city, and paid a portion of the cost out of his own pocket. He joined with George Marston and other civic-minded businessmen to purchase the site of the original Presidio of San Diego, which they developed as a historic park and eventually donated to the city of San Diego. He ran unsuccessfully for the United States Senate in 1910. He helped to organize the 1915 Panama-California Exposition, serving as second vice-president.
Death.
He died on September 9, 1915 in San Diego, and his ashes were scattered at his request.
Legacy.
He was elected to the Baseball Hall of Fame by the Veterans Committee in 1939, as one of the first inductees from the 19th century at that summer's opening ceremonies. His plaque in the Hall of Fame reads "Albert Goodwill Spalding. Organizational genius of baseball's pioneer days. Star pitcher of Forest City Club in late 1860s, 4-year champion Bostons 1871-1875 and manager-pitcher of champion Chicagos in National League's first year. Chicago president for 10 years. Organizer of baseball's first round-the-world tour in 1888."
His nephew, also named Albert Spalding, was a renowned violinist.

</doc>
<doc id="1894" url="http://en.wikipedia.org/wiki?curid=1894" title="Africa Alphabet">
Africa Alphabet

The Africa Alphabet (also International African Alphabet or IAI alphabet) was developed in 1928 under the lead of Diedrich Westermann. He developed it with a group of Africanists at the International Institute of African Languages and Cultures (later the IAI) in London. Its aim was to enable people to write all the African languages for practical and scientific purposes without diacritics. It is based on the International Phonetic Alphabet with a few differences, such as "j" and "y", which instead have the same (consonant) sound values as in English.
This alphabet has influenced development of orthographies of many African languages (serving "as the basis for the transcription" of about 60, by one count), but not all, and discussions of harmonization of systems of transcription that led to, among other things, adoption of the African reference alphabet.
The African Alphabet was used, with the International Phonetic Alphabet, as a basis for the World Orthography.

</doc>
<doc id="1896" url="http://en.wikipedia.org/wiki?curid=1896" title="Acquire">
Acquire

Acquire is a board game designed by Sid Sackson.
The game was originally published in 1962 by 3M as a part of their bookshelf games series. In most versions, the theme of the game is investing in hotel chains. In the 1990s Hasbro edition, the hotel chains were replaced by generic corporations, though the actual gameplay was unchanged. The game is currently published by Hasbro under the Avalon Hill brand, and the companies are once again hotel chains.
The object of the game is to earn the most money by developing and merging hotel chains. When a chain in which a player owns stock is acquired by a larger chain, players earn money based on the size of the acquired chain. At the end of the game, all players liquidate their stock in order to determine which player has the most money.
Components.
The components of the game have varied over the years. In particular, the tiles have been made from wood, plastic, and cardboard in various editions of the game. In the current 2008 version, the tiles are cardboard. The following components are included in all versions:
The array on the game board is arranged with lettered rows (A through I) and numbered columns (1 through 12). The 108 tiles correspond to each of the squares: 5E, 10B, and so forth.
Rules.
"Acquire" is a game for three to six players, though earlier editions included special rules for two players. Standard tournament games are played with four players.
Setup.
At the beginning of the game, each player receives $6000 in cash. Each player draws a tile and places it on the board. The player whose tile is in the topmost row (closest to row A) goes first. If more than one player selects a tile in that row, then the player whose tile is in the leftmost column (closest to 1) goes first. All players place these tiles on the board. Then, starting with the first player, each player draws six tiles.
Play of the game.
A turn consists of three steps:
Tile placement falls in one of four categories. The tile placed could be an orphan, adjacent to no other tile on the board. The tile could create a new chain of tiles, and the player who placed it on the board would have the opportunity to found a new chain. The tile could increase the length of an existing chain already on the board. Or the tile could link two chains, causing a merger of two or more chains. Since there are only seven hotel chains in the game, placing a tile that would create an eighth chain is not permitted.
When a player founds a chain, he receives one free share of stock in that chain. If, however, there are no shares left when the chain is founded, then the founding player does not receive the free share.
Chains are deemed "safe" if they have 11 or more links; placing a tile that would cause such a chain to be acquired by a larger chain is also not permitted.
After a player places a tile, and the results of that placement have been handled, he may purchase up to three shares of stock. A player may only purchase shares of stock in chains that have already been founded. The price of a share depends on the size of the chain, according to a chart that lists prices according to size. A player may purchase shares in one, two, or three existing chains (assuming at least three chains are currently in play), in any combination up to a total of three shares.
Finally, the player replaces the tile he played, ensuring that he has six tiles at the end of his turn.
Growing and merging chains.
A chain is a conglomeration of tiles that are linked to each other either horizontally or vertically but not diagonally. For example, adjacent to square 5F are squares 4F, 6F, 5E, and 5G, but not 6E or 4G. If there is a tile in 5F, then placing either tile 4F or 5G would result in founding a new hotel chain. A chain grows when a player increases the length of a chain. Suppose a chain consists of squares 8D, 8E, and 8F. Playing tile 9F would add to the length of the chain. Playing tile 9C would not.
Chains merge when a player places the tile that eliminates the empty space between them. Suppose there is a chain at 1A, 2A, 3A, and 4A, along with another chain at 6A and 7A. Placing tile 5A would cause these two chains to merge. When a merger occurs, the larger hotel chain always acquires the smaller hotel chain. That is, the hotel chain with more tiles will continue to exist and now grows to include the smaller hotel chain (after bonuses have been calculated according to the steps outlined below). If a tile is placed between two hotel chains of the same size, the individual player who places the tile decides which hotel chain remains on the board and which is acquired. In this situation, there are a number of strategic reasons why an individual player might select one hotel chain over another to be the one that remains on the board. However, often it is most advantageous for the player selecting to choose to let the more expensive chains remain on the board (and trade in their stock of the less expensive chain at the 2-to-1 ratio described below).
Mergers.
The merger is the mechanism by which the players compete. Mergers yield bonuses for the two shareholders who hold, respectively, the largest and second-largest interests in a chain. Mergers also give each player who holds any interest at all in a chain a chance to sell his stock or to trade it in for shares of the acquiring chain. A merger takes place in three steps:
If placing a tile causes three or four chains to merge, then the merger steps are handled between the largest and second-largest chain, then with the third-largest chain, and finally with the smallest chain.
Rules issues.
The rules allow any player to count the number of shares available in the bank. However, the rules do not specify whether a player should hold his shares of stock face up or face down. That is, the rules do not say whether one player may ask another how many shares of stock he or she owns in a particular chain. Whether this is public or private information should be agreed upon between players before the game begins.
The current rules do not provide for a two-player game. However, the stock market was used as a "third shareholder" in previous versions of the game. By this rule, a tile is drawn whenever a merger is declared. The number on the tile indicates how many shares the stock market owns in the company that is being acquired. The players must compete with the market as well as with each other in order to receive bonuses.
Ending the game.
Any player may declare the game over at any time during his turn if either of two conditions is true: one chain has 41 or more tiles, or there is at least one chain on the board and every chain on the board has 11 or more tiles. Upon declaring the game over, the player is allowed to complete his turn (including buying stock). Ending the game is optional - if he believes it is to his advantage not to end the game, he may refrain from doing so. Once the game ends, the minority and majority bonuses are paid to the minority and majority holders in each of the remaining chains; each player sells his or her shares of stock in each of the remaining chains; and the player with the most money wins. Because ending the game is optional, and a player may not realize that he can end it, it is unethical to say "good game", or in any other way indicate that the game could be ended, until after a player actually has ended the game.
Corner Cases.
These conditions can be used to handle unusual situations.
Awards.
The game was short-listed for the first Spiel des Jahres board game awards in 1979.
"GAMES" magazine has inducted "Acquire" into their buyers' guide Hall of Fame. The magazine's stated criteria for the Hall of Fame encompasses "games that have met or exceeded the highest standards of quality and play value and have been continuously in production for at least 10 years; i.e., classics."
It was inducted into the Academy of Adventure Gaming Arts & Design's Hall of Fame, along with Sackson, in 2011.
Online play.
"Acquire" is playable online through the GameTable Online site.
Acquire is also playable for free as NetAcquire. Various NetAcquire clients are available including Visual Basic (for Windows) by Kensit, JAVA (for all) by Tim Styer, and a Mac version by Nolan Waite. Details can be found at http://www.netacquire.ca.

</doc>
<doc id="1897" url="http://en.wikipedia.org/wiki?curid=1897" title="Australian English">
Australian English

Australian English (AusE, AuE, AusEng, en-AU) is a major variety of the English language and is used throughout Australia. Although English has no official status in the Constitution, Australian English is Australia's "de facto" official language and is the first language of the majority of the population.
Australian English began to diverge from British English after the founding of the colony of New South Wales in 1788 and was recognised as being different from British English by 1820. It arose from the intermingling of early settlers from a great variety of mutually intelligible dialectal regions of the British Isles and quickly developed into a distinct variety of English.
Australian English differs from other varieties of English in vocabulary, accent, pronunciation, register, grammar and spelling.
History.
The earliest form of Australian English was first spoken by the children of the colonists born into the colony of New South Wales. This very first generation of children created a new dialect that was to become the language of the nation. The Australian-born children in the new colony were exposed to a wide range of different dialects from all over the British Isles, in particular from Ireland and South East England.
The native-born children of the colony created the new dialect from factors present in the speech they heard around them, and provided an avenue for the expression of peer solidarity. Even when new settlers arrived, this new dialect was strong enough to deflect the influence of other patterns of speech.
A large part of the convict body were of Irish origin, 25% of the total convict population. Many had been arrested in Ireland, and some in Great Britain. Many, if not most of the Irish convicts spoke either no English at all, or spoke it poorly and rarely. There were other significant populations of convicts from non-English speaking areas of Britain, such as the Scottish Highlands and Wales.
Records from the early 19th century indicate the distinct dialect that had surfaced in the colonies since first settlement in 1788, with Peter Miller Cunningham's 1827 book "Two Years in New South Wales", describing the distinctive accent and vocabulary of the native born colonists, different from that of their parents and with a strong London influence. Anthony Burgess writes that "Australian English may be thought of as a kind of fossilised Cockney of the Dickensian era". 
The first of the Australian gold rushes, in the 1850s, began a large wave of immigration, during which about two per cent of the population of the United Kingdom emigrated to the colonies of New South Wales and Victoria. According to linguist Bruce Moore, "the major input of the various sounds that went into constructing the Australian accent was from south-east England".
Some elements of Aboriginal languages have been adopted by Australian Englishâmainly as names for places, flora and fauna (for example dingo) and local culture. Many such are localised, and do not form part of general Australian use, while others, such as "kangaroo", "boomerang", "budgerigar", "wallaby" and so on have become international. Other examples are "cooee" and "hard yakka". The former is used as a high-pitched call, for attracting attention, (pronounced ) which travels long distances. "Cooee" is also a notional distance: "if he's within cooee, we'll spot him". "Hard yakka" means "hard work" and is derived from "yakka", from the Jagera/Yagara language once spoken in the Brisbane region.
Also from there is the word "bung", from the Sydney pidgin English (and ultimately from the Sydney Aboriginal language), meaning "dead", with some extension to "broken" or "useless". Many towns or suburbs of Australia have also been influenced or named after Aboriginal words. The most well known example is the capital, Canberra, named after a local language word meaning "meeting place".
Among the changes starting in the 19th century was the introduction of words, spellings, terms and usages from North American English. The words imported included some later considered to be typically Australian, such as "bushwhacker" and "squatter".
This American influence continued with the popularity of American films, and with the influx of American military personnel in World War II; seen in the enduring persistence of such terms as "okay", "you guys" and "gee".
Phonology and pronunciation.
The primary way in which Australian English is distinctive from other varieties of English is through its unique pronunciation. It shares most similarity with other Southern Hemisphere accents, in particular New Zealand English. Like most dialects of English it is distinguished primarily by its vowel phonology.
Vowels.
The vowels of Australian English can be divided according to length. The long vowels, which include monophthongs and diphthongs, mostly correspond to the tense vowels used in analyses of Received Pronunciation (RP) as well as its centring diphthongs. The short vowels, consisting only of monophthongs, correspond to the RP lax vowels. There exist pairs of long and short vowels with overlapping vowel quality giving Australian English phonemic length distinction, which is unusual amongst the various dialects of English, though not unknown elsewhere, such as in regional south-eastern dialects of the UK and eastern seaboard dialects in the US. As with General American and New Zealand English, the weak-vowel merger is complete in Australian English: unstressed (sometimes written as or ) is merged into (schwa), unless it is followed by a velar consonant.
Consonants.
There is little variation with respect to the sets of consonants used in various English dialects. There are, however, variations in how these consonants are used. Australian English is no exception.
Australian English is non-rhotic; in other words, the sound does not appear at the end of a syllable or immediately before a consonant. However, a linking can occur when a word that has a final <r> in the spelling comes before another word that starts with a vowel. An intrusive may similarly be inserted before a vowel in words that do not have <r> in the spelling in certain environments, namely after the long vowel and after word final .
There is some degree of allophonic variation in the alveolar stops. As with North American English, Intervocalic alveolar flapping is a feature of Australian English: prevocalic and surface as the alveolar tap after sonorants other than , /m/as well as at the end of a word or morpheme before any vowel in the same breath group. For many speakers, and in the combinations - and - are also palatalised, thus - and -, as Australian is only very slightly retroflex, the tip remaining below the level of the bottom teeth in the same position as for ; it is also somewhat rounded ("to say 'r' the way Australians do you need to say 'w' at the same time"), where older English and have fallen together as . The wineâwhine merger is complete in Australian English.
"Yod"-dropping occurs after , , , , , , , and , . Other cases of and , along with and , have coalesced to , , and respectively for many speakers. is generally retained in other consonant clusters.
Pronunciation.
Differences in stress, weak forms and standard pronunciation of isolated words occur between Australian English and other forms of English, which while noticeable do not impair intelligibility.
The affixes "-ary", "-ery", "-ory", "-bury", "-berry" and "-mony" (seen in words such as "necessary, mulberry" and "matrimony") can be pronounced either with a full vowel or a schwa. Although some words like "necessary" are almost universally pronounced with the full vowel, older generations of Australians are relatively likely to pronounce these affixes with a schwa while younger generations are relatively likely to use a full vowel.
Words ending in unstressed "-ile" derived from Latin adjectives ending in "-ilis" are pronounced with a full vowel (), so that "fertile" rhymes with "fur tile" rather than "turtle".
In addition, miscellaneous pronunciation differences exist when compared with other varieties of English in relation to seemingly random words. For example, the vowel in "yoghurt" is pronounced as ("long 'O'") rather than ("short o"). Similarly, "vitamin" is pronounced with ("long 'I'") in the first syllable, rather than ("short 'I'"). Despite this, "advertisement" is pronounced with . "Brooch" is pronounced with as opposed to , and "Anthony" with rather than .
Variation.
Academic research has shown that the most notable variation within Australian English is largely sociocultural. This is mostly evident in phonology, which is divided into three sociocultural varieties: "broad", "general" and "cultivated".
A limited range of word choices is strongly regional in nature. Consequently, the geographical background of individuals can be inferred, if they use words that are peculiar to particular Australian states or territories and, in some cases, even smaller regions.
In addition, some Australians speak creole languages derived from Australian English, such as Australian Kriol, Torres Strait Creole and Norfuk.
Sociocultural.
The "broad", "general" and "cultivated" accents form a continuum that reflects minute variations in the Australian accent. They can reflect the social class, education and urban or rural background of speakers, though such indicators are not always reliable. According to linguists, the general Australian variant emerged some time before 1900. Recent generations have seen a comparatively smaller proportion of the population speaking with the broad variant, along with the near extinction of the cultivated Australian accent. The growth and dominance of general Australian accents perhaps reflects its prominence on radio and television during the late 20th century.
Australian Aboriginal English is made up of a range of forms which developed differently in different parts of Australia, and are said to vary along a continuum, from forms close to Standard Australian English to more non-standard forms. There are distinctive features of accent, grammar, words and meanings, as well as language use.
The ethnocultural dialects are diverse accents in Australian English that are spoken by the minority groups, which are of non-English speaking background. A massive immigration from Asia has made a large increase in diversity and the will for people to show their cultural identity within the Australian context. These ethnocultural varieties contain features of General Australian English as adopted by the children of immigrants blended with some non-English language features, such as the Afro-Asiatic and Asian languages.
Regional variation.
Although Australian English is relatively homogeneous, some regional variations are notable. The dialects of English spoken in South Australia, Western Australia, New South Wales, Victoria, Tasmania, Queensland and the Torres Strait Islands differ slightly from each other. Differences exist both in terms of vocabulary and phonology.
Most regional differences come down to word usage. For example, swimming clothes are known as "cossies" or "swimmers" in New South Wales, "togs" in Queensland, and "bathers" in Victoria, Western Australia and South Australia; what is referred to as a "stroller" in most of Australia is called a "pusher" in Victoria and usually a "pram" in Western Australia. Preference for synonymous words also differs between states. For example, "garbage" (i.e. garbage bin, garbage truck) dominates over "rubbish" in New South Wales and Queensland, while "rubbish" is more popular in Victoria, Western Australia and South Australia. The word "footy" generally refers to the most popular football code in the particular state or territory; that is, rugby league in New South Wales and Queensland, and Australian rules football elsewhere. Beer glasses are also named differently in different states. Distinctive grammatical patterns exist such as the use of the interrogative "eh?".
There are some notable regional variations in the pronunciations of certain words. The extent to which the trapâbath split has taken hold is one example. This phonological development is more advanced in South Australia, which had a different settlement chronology and type than other parts of the country. Words such as "dance", "advance", "plant", "graph", "example" and "answer" are pronounced far more frequently with the older (as in "mad") outside of South Australia, but with the British-influenced (as in "father") within South Australia. "L"-vocalisation is also more common in South Australia than other states. In Western Australian English the vowels in "near" and "square" are typically realised as centring diphthongs, whereas in the eastern states they may also be realised as monophthongs. A feature common in Victorian English is salaryâcelery merger. There is also regional variation in before .
Vocabulary.
Australian English has many words and idioms which are unique to the dialect and have been written on extensively, with the Macquarie Dictionary, widely regarded as the national standard, incorporating numerous Australian terms.
Internationally well-known examples of Australian terminology include "outback", meaning a remote, sparsely populated area, "the bush", meaning either a native forest or a country area in general, and "g'day", a greeting. "Dinkum", or "fair dinkum" means "true", or "is that true?", among other things, depending on context and inflection. The derivative "dinky-di" means "true" or devoted: a "dinky-di Aussie" is a "true Australian".
Australian poetry, such as "The Man from Snowy River" as well as folk songs such as "Waltzing Matilda", contain many historical Australian words and phrases that are understood by Australians even though some are not in common usage today.
Australian English, in common with several British English dialects (for example, Cockney, Scouse, Glaswegian and Geordie), uses the word "mate". Many words used by Australians were at one time used in England but have since fallen out of usage or changed in meaning there.
For example, "creek" in Australia, as in North America, means a stream or small river, whereas in the UK it means a small watercourse flowing into the sea; "paddock" in Australia means field, whereas in the UK it means a small enclosure for livestock; "bush" or "scrub" in Australia, as in North America, means a wooded area, whereas in England they are commonly used only in proper names (such as Shepherd's Bush and Wormwood Scrubs).
Litotes, such as "not bad", "not much" and "you're not wrong", are also used, as are diminutives, which are commonly used and are often used to indicate familiarity. Some common examples are "arvo" (afternoon), "barbie" (barbecue), "smoko" (cigarette break), "Aussie" (Australian) and "pressie" (present/gift). This may also be done with people's names to create nicknames (other English speaking countries create similar diminutives). For example, "Gazza" from Gary, or "Smitty" from John Smith. The use of the suffix "-o" originates in Irish Gaelic (Irish "Ã³"), which is both a postclitic and a suffix with much the same meaning as in Australian English.
In informal speech, incomplete comparisons are sometimes used, such as "sweet as" (as in "That car is sweet as."). "Full", "fully" or "heaps" may precede a word to act as an intensifier (as in "The waves at the beach were heaps good."). This was more common in regional Australia and South Australia but has been in common usage in urban Australia for decades. The suffix "-ly" is sometimes omitted in broader Australian English. For instance "real good" instead of "really good".
Australia's switch to the metric system in the 1970s changed the country's vocabulary of measurement from Imperial towards metric measures.
Comparison with other varieties.
Where British and American vocabulary differs, Australians sometimes favour a usage different from both varieties, as with footpath (for US sidewalk, UK pavement) or capsicum (for US bell pepper, UK green/red pepper). In other instances, it either shares a term with American English, as with truck (UK: lorry) or eggplant (UK: aubergine), or with British English, as with mobile phone (US: cell phone) or bonnet (US: hood).
A non-exhaustive selection of common British English terms not commonly used in Australian English include (Australian usage in brackets): artic/articulated lorry (semi-trailer); aubergine (eggplant); bank holiday (public holiday); bedsit (one-bedroom apartment); bespoke (custom); black pudding (blood sausage); cagoule (raincoat); candy floss (fairy floss); cash machine (automatic teller machine/ATM); child-minder (babysitter); clingfilm (glad wrap/cling-wrap); courgette (zucchini); crisps (chips/potato chips); doddle (bludge); dungarees (overalls); dustbin (garbage/rubbish bin); dustcart (garbage/rubbish truck); duvet (doona); elastoplast/plaster (band-aid); estate car (station wagon); fairy cake (cupcake/patty cake); flannel ((face) washer/wash cloth); free phone (toll-free); football (soccer); high street (main street); hoover (vacuum cleaner); ice lolly (ice block/icy pole); kitchen roll (paper towel); lavatory (toilet); lorry (truck); off-licence (bottle shop); pavement (footpath); red/green pepper (capsicum); pillar box (mail box); plimsoll (sandshoe); pushchair (pram/stroller); saloon (sedan); sweets (lollies); utility room (laundry); Wellington boots (gumboots).
A non-exhaustive list of American English terms not commonly found in Australian English include: acclimate (acclimatise); aluminum (aluminium); bangs (fringe); bell pepper (capsicum); bellhop (hotel porter); broil (grill); burglarize (rob); busboy (included under the broader term of waiter/waitress; rarely, table clearer); candy (lolly); cell phone (mobile phone); cilantro (coriander); cookie (biscuit); counter-clockwise (anticlockwise); diaper (nappy); emergency brake (handbrake); faucet (tap); flashlight (torch); gasoline (petrol); hood (bonnet); jell-o (jelly); jelly (jam); math (maths); pacifier (dummy); parking lot (car park); popsicle (ice block/icy pole); railway ties (sleepers); row house (terrace house); scallion (spring onion); stickshift (manual transmission); streetcar (tram); takeout (takeaway); trash can (garbage/rubbish bin); trunk (boot); turn signal (indicator/blinker); vacation (holiday).
Terms shared by British and American English but not so commonly found in Australian English include: abroad (overseas); cooler/ice box (esky); pickup truck (ute); wildfire (bushfire).
In addition, a number of words in Australian English have different meanings to those ascribed in other varieties of English. Clothing-related examples are notable. "Pants" in Australian English refer to British English "trousers" but in British English refer to Australian English "underpants"; "vest" in Australian English refers to British English "waistcoat" but in British English refers to Australian English "singlet"; "thong" in both American and British English refers to underwear otherwise known as a "G-string" while in Australian English it refers to British and American English "flip-flop".
Grammar.
As with American English, but unlike British English, collective nouns are almost always singular in construction, e.g. "the government was unable to decide" as opposed to "the government were unable to decide".
In common with British English, the past tense and past participles of the verbs "learn", "spell" and "smell" are often irregular ("learnt", "spelt", "smelt").
"Shan't" and the use of "should" as in "I should be happy if...", common in upper-register British English, are almost never encountered in Australian (or North American) English.
While prepositions before days may be omitted in American English, i.e. "She resigned Thursday", they must be retained in Australian English, as in British English: "She resigned on Thursday". Ranges of dates use "to", i.e. "Monday to Friday", as with British English, rather than "Monday through Friday" in American English.
"River" generally follows the name of the river in question as in North America, i.e. "Darling River", rather than the British convention of coming before the name, e.g. "River Thames". Note in South Australia however, the British convention appliesâfor example, the "River Murray" or the "River Torrens".
When saying or writing out numbers, "and" is inserted before the tens and units, i.e. "one hundred and sixty-two", as with British practice. However Australians, like Americans, are more likely to pronounce numbers such as 1200 as "twelve hundred", rather than "one thousand two hundred".
As with American English, "on the weekend" and "studied medicine" are used rather than the British "at the weekend" and "read medicine".
Spelling, style and keyboards.
As in most English-speaking countries, there is no official governmental regulator or overseer of correct spelling and grammar. The "Macquarie Dictionary" is used by universities and other organisations as a standard for Australian English spelling. The "Style Manual: For Authors, Editors and Printers", the "Cambridge Guide to Australian English Usage" and the "Australian Guide to Legal Citation" are prominent style guides.
Australian spelling is closer to British than American spelling. As with British spelling, the "u" is retained in words such as "colour", "honour", "labour" and "favour". While the Macquarie Dictionary lists the "-our" ending and follows it with the "-or" ending as an acceptable variant, the latter are rarely found in usage today. Australian print media, including digital media, today strongly favour "-our" endings. A notable exception to this rule is the Australian Labor Party, which adopted the American spelling in 1912 as a result of "-or" spellings' comparative popularity at that time and American influence. Consistent with British spellings, "-re", rather than "-er", is the only listed variant in Australian dictionaries in words such as "theatre", "centre" and "manoeuvre". Unlike British English, which is split between "-ise" and "-ize" in words such as "organise" and "realise", with "-ize" favoured by the Oxford English Dictionary and "-ise" listed as a variant, "-ize" is rare in Australian English and designated as a variant by the Macquarie Dictionary. "Ae" and "oe" are often maintained in words such as "manoeuvre", "paedophilia" and "foetus" (excepting those listed below), however the Macquarie dictionary lists forms with "e" (e.g. pedophilia, fetus) as acceptable variants and notes a tendency within Australian English towards using only "e". Individual words spelt differently from British spelling, according to the "Macquarie Dictionary", include "program" (in all contexts) as opposed to "programme", "inquire" (for all meanings) and derivatives "inquired", "inquiring", "inquiry", "inquirer", etc. as opposed to "enquire" and derivatives, "analog" as opposed to "analogue" (as with American English, "analog" is used in the context of information transmission and "analogue" in the sense of "something analogous to"), "livable" as opposed to "liveable", "guerilla" as opposed to "guerrilla", "yoghurt" as opposed to "yogurt", "verandah" as opposed to "veranda", "sulfur" and derivatives "sulfide", "sulfidic" and "sulfuric" as opposed to "sulphur" and derivatives, "burqa" as opposed to "burka", "pastie" (food) as opposed to "pasty", "onto" "or" "on to" as opposed to "on to", "anytime" as opposed to "any time", "alright" "or" "all right" as opposed to "all right", and "anymore" as opposed to "any more". Both "acknowledgement" and "acknowledgment", as well as "abridgement" and "abridgment" are used, with the shorter forms being endorsed by Australian governments. "Okay", rather than "OK", is listed as the preferred variant.
Different spellings have existed throughout Australia's history. A pamphlet entitled "The So-Called "American Spelling"", published in Sydney some time in the 19th century, argued that "there is no valid etymological reason for the preservation of the "u" in such words as "honor", "labor", etc." The pamphlet also claimed that "the tendency of people in Australasia is to excise the u, and one of the Sydney morning papers habitually does this, while the other generally follows the older form." What are today regarded as American spellings were popular in Australia throughout the late 19th and early 20th centuries, with the Victorian Department of Education endorsing them into the 1970s and The Age newspaper until the 1990s. This influence can be seen in the spelling of the Australian Labor Party and also in some place names such as Victor Harbor. The "Concise Oxford English Dictionary" has been attributed with re-establishing the dominance of the British spellings in the 1920s and 1930s. For a short time during the late 20th century, Harry Lindgren's 1969 spelling reform proposal ("Spelling Reform 1" or "SR1") was popular in Australia and was adopted by the Australian government. SR1 calls for the short sound (as in "bet") to be spelt with E (for example "friendâfrend, headâhed"). Many general interest paperbacks were printed in SR1.
Both single and double quotation marks are in use (with double quotation marks being far more common in print media), with logical (as opposed to typesetter's) punctuation. Spaced and unspaced em-dashes remain in mainstream use, as with American and Canadian English. The DD/MM/YYYY date format is used with Monday as the first day of the week (as with British practice), however the 12-hour clock is used almost universally (as in the United States).
There are two major English language keyboard layouts, the United States layout and the United Kingdom layout. Australia universally uses the United States keyboard. As such, Pound Sterling and Euro currency symbols do not appear on Australian keyboards, which also lack negation symbols and have punctuation symbols placed differently from the way they are placed on British keyboards.

</doc>
<doc id="1902" url="http://en.wikipedia.org/wiki?curid=1902" title="American Airlines Flight 77">
American Airlines Flight 77

American Airlines Flight 77 was a scheduled domestic passenger flight that was hijacked by five men affiliated to al-Qaeda on September 11, 2001, as part of the September 11 attacks. They deliberately crashed it into the Pentagon in Arlington County, Virginia near Washington, D.C., killing all 64 people on board including the five hijackers and six crew as well as 125 people in the building. The Boeing 757-223 aircraft was flying American Airlines' daily scheduled morning transcontinental service from Washington Dulles International Airport, in Dulles, Virginia to Los Angeles International Airport in Los Angeles, California.
Less than 35Â minutes into the flight, the hijackers stormed the cockpit. They forced the passengers, crew, and pilots to the rear of the aircraft. Hani Hanjour, one of the hijackers who was trained as a pilot, assumed control of the flight. Unknown to the hijackers, passengers aboard made telephone calls to loved ones and relayed information on the hijacking.
The hijackers crashed the aircraft into the western side of the Pentagon at 9:37Â a.m. (EDT). Dozens of people witnessed the crash, and news sources began reporting on the incident within minutes. The impact severely damaged an area of the Pentagon and caused a large fire. At 10:10Â a.m., a portion of the Pentagon collapsed; firefighters spent days trying to fully extinguish the blaze. The damaged sections of the Pentagon were rebuilt in 2002, with occupants moving back into the completed areas on August 15, 2002.
The 184 victims of the attack are memorialized in the Pentagon Memorial adjacent to the Pentagon. The park contains a bench for each of the victims, arranged according to their year of birth, ranging from 1930 (age 71) to 1998 (age 3).
Hijackers.
The hijackers on American Airlines FlightÂ 77 were led by Hani Hanjour, who piloted the aircraft into the Pentagon. Hanjour first came to the United States in 1990.
Hanjour trained at the CRM Airline Training Center in Scottsdale, Arizona, earning his FAA commercial pilot's certificate in April 1999. He had wanted to be a commercial pilot for the Saudi national airline but was rejected when he applied to the civil aviation school in Jeddah in 1999. Hanjour's brother later explained that, frustrated at not finding a job, Hanjour "increasingly turned his attention toward religious texts and cassette tapes of militant Islamic preachers". Hanjour returned to Saudi Arabia after being certified as a pilot, but left again in late 1999, telling his family that he was going to the United Arab Emirates to work for an airline. Hanjour likely went to Afghanistan, where Al Qaeda recruits were screened for special skills they may have. Already having selected the Hamburg Cell members, Al Qaeda leaders selected Hanjour to lead the fourth team of hijackers.
Alec Station, the CIA's unit dedicated to tracking Osama bin Laden, had discovered that two of the other hijackers, al-Hazmi and al-Mihdhar, had multiple entry visas to the United States well before 9/11. Two FBI agents inside the unit tried to alert FBI headquarters, but CIA officers rebuffed them.
In December 2000, Hanjour arrived in San Diego, joining "muscle" hijackers Nawaf al-Hazmi and Khalid al-Mihdhar, who had been there since January 2000. Soon after arriving, Hanjour and Hazmi left for Mesa, Arizona, where Hanjour began refresher training at Arizona Aviation.
In April 2001, they relocated to Falls Church, Virginia, where they awaited the arrival of the remaining "muscle" hijackers. One of these men, Majed Moqed, arrived on May 2, 2001, with Flight 175 hijacker Ahmed al-Ghamdi from Dubai at Dulles International Airport. They moved into an apartment with Hazmi and Hanjour. While living in Falls Church, Hazmi attended the mosque in the community.
On May 21, 2001, Hanjour rented a room in Paterson, New Jersey, where he stayed with other hijackers through the end of August. The last FlightÂ 77 "muscle" hijacker, Salem al-Hazmi, arrived on June 29, 2001, with Abdulaziz al-Omari (a hijacker of FlightÂ 11) at John F. Kennedy International Airport from the United Arab Emirates. They stayed with Hanjour.
Hanjour received ground instruction and did practice flights at Air Fleet Training Systems in Teterboro, New Jersey, and at Caldwell Flight Academy in Fairfield, New Jersey. Hanjour moved out of the room in Paterson and arrived at the Valencia Motel in Laurel, Maryland, on September 2, 2001. While in Maryland, Hanjour and fellow hijackers trained at Gold's Gym in Greenbelt. On September 10, he completed a certification flight, using a terrain recognition system for navigation, at Congressional Air Charters in Gaithersburg, Maryland.
On September 10, Nawaf al-Hazmi, accompanied by other hijackers, checked into the Marriott in Herndon, Virginia, near Dulles Airport.
Suspected accomplices.
According to a U.S. State Department cable leaked in the Wikileaks dump in February 2010, the FBI has investigated another suspect, Mohammed al-Mansoori. He had associated with three Qatari citizens who flew from Los Angeles to London (via Washington) and Qatar on the eve of the attacks, after allegedly surveying the World Trade Center and the White House. U.S. law enforcement officials said that the data about the four men was "just one of many leads that were thoroughly investigated at the time and never led to terrorism charges". An official added that the three Qatari citizens have never been questioned by the FBI. Eleanor Hill, the former staff director for the congressional joint inquiry on the September 11 attacks, said the cable reinforces questions about the thoroughness of the FBI's investigation. She also said that the inquiry concluded that the hijackers had a support network that helped them in different ways.
The three Qatari men were booked to fly from Los Angeles to Washington on September 10, 2001, on the same plane that was hijacked and piloted into the Pentagon on the following day. Instead, they flew from Los Angeles to Qatar, via Washington and London. While the cable said that Mansoori was currently under investigation, U.S. law enforcement officials said that there was no active investigation of him or of the Qatari citizens mentioned in the cable.
Flight.
The American Airlines FlightÂ 77 aircraft was a Boeing 757-223 (registration number N644AA). The flight crew included pilot Charles Burlingame, First Officer David Charlebois, and flight attendants Michele Heidenberger, Jennifer Lewis, Kenneth Lewis, and Renee May. The capacity of the aircraft was 188 passengers, but with 58 passengers on September 11, the load factor was 33 percent. American Airlines said that Tuesdays were the least-traveled day of the week, with the same load factor seen on Tuesdays in the previous three months for Flight 77.
Boarding and departure.
On the morning of September 11, 2001, the five hijackers arrived at Washington Dulles International Airport. At 07:15, Khalid al-Mihdhar and Majed Moqed checked in at the American Airlines ticket counter for Flight 77, arriving at the passenger security checkpoint a few minutes later at 07:18. Both men set off the metal detector and were put through secondary screening. Moqed continued to set off the alarm, so he was searched with a hand wand. The Hazmi brothers checked in together at the ticket counter at 07:29. Hani Hanjour checked in separately and arrived at the passenger security checkpoint at 07:35. Hanjour was followed minutes later at the checkpoint by Salem and Nawaf al-Hazmi, who also set off the metal detector's alarm. The screener at the checkpoint never resolved what set off the alarm. As seen in security footage later released, Nawaf Hazmi appeared to have an unidentified item in his back pocket. Utility knives up to four inches were permitted at the time by the Federal Aviation Administration (FAA) as carry-on items. The passenger security checkpoint at Dulles International Airport was operated by Argenbright Security, under contract with United Airlines.
The hijackers were all selected for extra screening of their checked bags. Hanjour, al-Mihdhar, and Moqed were chosen by the Computer Assisted Passenger Prescreening System criteria, while the brothers Nawaf and Salem al-Hazmi were selected because they did not provide adequate identification and were deemed suspicious by the airline check-in agent. Hanjour, Mihdhar, and Nawaf al-Hazmi did not check any bags for the flight. Checked bags belonging to Moqed and Salem al-Hazmi were held until they boarded the aircraft. By 07:50, the five hijackers, carrying knives and box cutters, had made it through the airport security checkpoint.
Flight 77 was scheduled to depart for Los Angeles at 08:10; 58 passengers boarded through Gate D26, including the five hijackers. Excluding the hijackers, of the 59 other passengers and crew on board, there were 26 men, 22 women, and five children ranging in age from three to eleven. On the flight, Hani Hanjour was seated up front in 1B, while Salem and Nawaf al-Hazmi were seated in first class in seats 5E and 5F. Majed Moqed and Khalid al-Mihdhar were seated further back in 12A and 12B, in economy class. Flight 77 left the gate on time and took off from Runway 30 at Dulles at .
Hijacking.
The 9/11 Commission estimated that the flight was hijacked between 08:51 and 08:54, shortly after American Airlines Flight 11 struck the World Trade Center and not too long after United Airlines Flight 175 had been hijacked. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. Unlike the other three flights, there were no reports of anyone being stabbed or a bomb threat. At 08:54, the plane began to deviate from its normal, assigned flight path and turned south. The hijackers set the flight's autopilot heading for Washington, D.C. By 08:56, the flight was turned around, and the transponder had been disabled.
The FAA was aware at this point that there was an emergency on board the airplane. By this time, American Airlines Flight 11 had already crashed into the North Tower of the World Trade Center, and United Airlines Flight 175 was known to have been hijacked and was within minutes of striking the South Tower. After learning of this second hijacking involving an American Airlines aircraft and the hijacking involving United Airlines, American Airlines' Executive Vice President Gerard Arpey ordered a nationwide ground stop for the airline. The Indianapolis Air Traffic Control Center, as well as American Airlines dispatchers, made several failed attempts to contact the aircraft. At the time the airplane was hijacked, it was flying over an area of limited radar coverage. With air controllers unable to contact the flight by radio, an Indianapolis official declared that the Boeing 757 had possibly crashed at 09:09.
Two people on the aircraft made phone calls to contacts on the ground. At 09:12, flight attendant Renee May called her mother, Nancy May, in Las Vegas. During the call, which lasted nearly two minutes, May said her flight was being hijacked by six persons, and staff and passengers had been moved to the rear of the airplane. May asked her mother to contact American Airlines, which she and her husband promptly did; American Airlines was already aware of the hijacking. Between 09:16 and 09:26, passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the airplane had been hijacked and that the assailants had box cutters and knives. She reported that the passengers, including the pilots, had been moved to the back of the cabin and that the hijackers were unaware of her call. A minute into the conversation, the call was cut off. Theodore Olson contacted the command center at the Department of Justice, and tried unsuccessfully to contact Attorney General John Ashcroft. About five minutes later, Barbara Olson called again, told her husband that the "pilot" (possibly Hanjour on the cabin intercom) had announced the flight was hijacked, and asked "what do I tell the pilot to do?" Ted Olson asked her location and she reported the plane was flying low over a residential area. He told her of the attacks on the World Trade Center. Soon afterward, the call cut off again.
An airplane was detected again by Dulles controllers on radar screens as it approached Washington, turning and descending rapidly. Controllers initially thought this was a military fighter, due to its high speed and maneuvering. Reagan Airport controllers asked a passing Air National Guard Lockheed C-130 Hercules to identify and follow the aircraft. The pilot, Lt. Col. Steven O'Brien, told them it was a BoeingÂ 757 or 767, and its silver fuselage meant it was probably an American Airlines jet. He had difficulty picking out the airplane in the "East Coast haze", but then saw a "huge" fireball, and initially assumed it had hit the ground. Approaching the Pentagon, he saw the impact site on the building's west side and reported to Reagan control, "Looks like that aircraft crashed into the Pentagon, sir."
Crash.
According to the 9/11 Commission Report, as Flight 77 was west-southwest of the Pentagon, it made a 330-degree turn. At the end of the turn, it was descending through , pointed toward the Pentagon and downtown Washington. Hani Hanjour advanced the throttles to maximum power and dived toward the Pentagon. While level above the ground and seconds from the crash, the wings knocked over five street lampposts and the right wing struck a portable generator, creating a smoke trail seconds before smashing into the Pentagon. FlightÂ 77, flying at 530Â mph (853Â km/h, 237Â m/s, or 460 knots) over the Navy Annex Building adjacent to Arlington National Cemetery, crashed into the western side of the Pentagon in Arlington County, Virginia, just south of Washington, D.C., at 09:37:46, killing all 64 people on board: 53 passengers, five hijackers, and six crew. The plane hit the Pentagon at the first-floor level, and at the moment of impact, the airplane was rolled slightly to the left, with the right wing elevated. The front part of the fuselage disintegrated on impact, while the mid and tail sections moved for another fraction of a second, with tail section debris penetrating furthest into the building. In all, the airplane took eight-tenths of a second to fully penetrate into the three outermost of the building's five rings and unleashed a fireball that rose above the building.
At the time of the attacks, approximately 18,000Â people worked in the Pentagon, which was 4,000Â fewer than before renovations began in 1998. The section of the Pentagon, which had recently been renovated at a cost of $250Â million, housed the Naval Command Center and other Pentagon offices, as well as some unoccupied offices. The crash and subsequent fire penetrated three outer ring sections of the western side. The outermost ring section was largely destroyed, and a large section collapsed. 125 people in the Pentagon died in the attack.
In all, there were 189Â deaths at the Pentagon site, including the 125 in the Pentagon building in addition to the 64 on board the aircraft. Passenger Barbara Olson was en route to a taping of "Politically Incorrect". A group of children, their chaperones, and National Geographic Society staff members were also on board, embarking on an educational trip west to the Channel Islands National Marine Sanctuary near Santa Barbara, California. The fatalities at the Pentagon included 55Â military personnel and 70Â civilians. Of those 125 killed, 92 were on the first floor, 31 were on the second floor, and two were on the third. The Army suffered 75 fatalitiesâfar more than any other branch. Another 106 injured were treated at area hospitals. Lieutenant General Timothy Maude, an Army Deputy Chief of Staff, was the highest-ranking military officer killed at the Pentagon; also killed was retired Rear Admiral Wilson Flagg, a passenger on the plane. LT Mari-Rae Sopper, JAGC, USNR, was also on board the flight, and was the first Navy Judge Advocate ever killed in action.
On the side where the plane hit, the Pentagon is bordered by Interstate 395 and Washington Boulevard. Motorist Mary Lyman, who was on I-395, saw the airplane pass over at a "steep angle toward the ground and going fast" and then saw the cloud of smoke from the Pentagon. Omar Campo, another witness, was cutting the grass on the other side of the road when the airplane flew over his head.
"I was cutting the grass and it came in screaming over my head. I felt the impact. The whole ground shook and the whole area was full of fire. I could never imagine I would see anything like that here".Afework Hagos, a computer programmer, was on his way to work and stuck in a traffic jam near the Pentagon when the airplane flew over. "There was a huge screaming noise and I got out of the car as the plane came over. Everybody was running away in different directions. It was tilting its wings up and down like it was trying to balance. It hit some lampposts on the way in." Daryl Donley witnessed the crash and took some of the first photographs of the site.
"USA Today" reporter Mike Walter was driving on Washington Boulevard when he witnessed the crash, which he recounted,
"I looked out my window and I saw this plane, this jet, an American Airlines jet, coming. And I thought, 'This doesn't add up, it's really low.' And I saw it. I mean it was like a cruise missile with wings. It went right there and slammed right into the Pentagon".Terrance Kean, who lived in a nearby apartment building, heard the noise of loud jet engines, glanced out his window, and saw a "very, very large passenger jet". He watched "it just plow right into the side of the Pentagon. The nose penetrated into the portico. And then it sort of disappeared, and there was fire and smoke everywhere." Tim Timmerman, who is a pilot himself, noticed American Airlines markings on the aircraft as he saw it hit the Pentagon. Other drivers on Washington Boulevard, Interstate 395, and Columbia Pike witnessed the crash, as did people in Pentagon City, Crystal City, and other nearby locations.
Former Georgetown University basketball coach John Thompson had originally booked a ticket on Flight 77. As he would tell the story many times in the following years, including a September 12, 2011 interview on Jim Rome's radio show, he had been scheduled to appear on that show on September 12, 2001. Thompson was planning to be in Las Vegas for a friend's birthday on September 13, and initially insisted on traveling to Rome's Los Angeles studio on the 11th. However, this did not work for the show, which wanted him to travel on the day of the show. After a Rome staffer personally assured Thompson that he would be able to travel from Los Angeles to Las Vegas immediately after the show, Thompson changed his travel plans. He felt the impact from the crash at his home near the Pentagon.
Rescue and recovery.
Rescue efforts began immediately after the crash. Almost all the successful rescues of survivors occurred within half an hour of the impact. Initially, rescue efforts were led by the military and civilian employees within the building. Within minutes, the first fire companies arrived and found these volunteers searching near the impact site. The firemen ordered them to leave as they were not properly equipped or trained to deal with the hazards. The Arlington County Fire Department (ACFD) assumed command of the immediate rescue operation within 10Â minutes of the crash. ACFD Assistant Chief James Schwartz implemented an incident command system (ICS) to coordinate response efforts among multiple agencies. It took about an hour for the ICS structure to become fully operational. Firefighters from Fort Myer and Reagan National Airport arrived within minutes. Rescue and firefighting efforts were impeded by rumors of additional incoming planes. Chief Schwartz ordered two evacuations during the day in response to these rumors.
As firefighters attempted to extinguish the fires, they watched the building in fear of a structural collapse. One firefighter remarked that they "pretty much knew the building was going to collapse because it started making weird sounds and creaking". Officials saw a cornice of the building move and ordered an evacuation. Minutes later, at 10:10, the upper floors of the damaged area of the Pentagon collapsed. The collapsed area was about at its widest point and at its deepest. The amount of time between impact and collapse allowed everyone on the fourth and fifth levels to evacuate safely before the structure collapsed. After the collapse, the interior fires intensified, spreading through all five floors. After 11:00, firefighters mounted a two-pronged attack against the fires. Officials estimated temperatures of up to . While progress was made against the interior fires by late afternoon, firefighters realized a flammable layer of wood under the Pentagon's slate roof had caught fire and begun to spread. Typical firefighting tactics were rendered useless by the reinforced structure as firefighters were unable to reach the fire to extinguish it. Firefighters instead made firebreaks in the roof on September 12 to prevent further spreading. At 18:00 on the 12th, Arlington County issued a press release stating the fire was "controlled" but not fully "extinguished". Firefighters continued to put out smaller fires that ignited in the succeeding days.
Various pieces of aircraft debris were found within the wreckage at the Pentagon. While on fire and escaping from the Navy Command Center, Lt. Kevin Shaeffer observed a chunk of the aircraft's nose cone and the nose landing gear in the service road between rings B and C. Early in the morning on Friday, September 14, Fairfax County Urban Search and Rescue Team members Carlton Burkhammer and Brian Moravitz came across an "intact seat from the plane's cockpit", while paramedics and firefighters located the two black boxes near the punch out hole in the A-E drive, nearly into the building. The cockpit voice recorder was to retrieve any information, though the flight data recorder yielded useful information. Investigators also found a part of Nawaf al-Hazmi's driver's license in the North Parking Lot rubble pile. Personal effects belonging to victims were found and taken to Fort Myer.
Remains.
Army engineers determined by 5:30Â p.m. on the first day that no one remained alive in the damaged section of the building. In the days after the crash, news reports emerged that up to 800 people had died. Army troops from Fort Belvoir were the first teams to survey the interior of the crash site and noted the presence of human remains. Federal Emergency Management Agency (FEMA) Urban Search and Rescue teams, including Fairfax County Urban Search and Rescue assisted the search for remains, working through the National Interagency Incident Management System (NIIMS). Kevin Rimrodt, a Navy photographer surveying the Navy Command Center after the attacks, remarked that "there were so many bodies, I'd almost step on them. So I'd have to really take care to look backwards as I'm backing up in the dark, looking with a flashlight, making sure I'm not stepping on somebody". Debris from the Pentagon was taken to the Pentagon's north parking lot for more detailed search for remains and evidence.
Remains that were recovered from the Pentagon were photographed, and turned over to the Armed Forces Medical Examiner office, located at Dover Air Force Base in Delaware. The medical examiner's office was able to identify remains belonging to 179 of the victims. Investigators eventually identified 184 of the 189 people who died in the attack. The remains of the five hijackers were identified through a process of elimination, and were turned over as evidence to the Federal Bureau of Investigation (FBI). On September 21, the ACFD relinquished control of the crime scene to the FBI. The Washington Field Office, National Capital Response Squad (NCRS), and the Joint Terrorism Task Force (JTTF) led the crime scene investigation at the Pentagon.
By October 2, 2001, the search for evidence and remains was complete and the site was turned over to Pentagon officials. In 2002, the remains of 25 victims were buried collectively at Arlington National Cemetery, with a five-sided granite marker inscribed with the names of all the victims in the Pentagon. The ceremony also honored the five victims whose remains were never found.
Flight Data Recorder and Cockpit Voice Recorder.
At around 3:40 a.m on September 14, a paramedic and a firefighter who were searching through the debris of the impact site found two dark boxes, about by long. They called for an FBI agent, who in turn called for someone from the National Transportation Safety Board (NTSB). The NTSB employee confirmed that these were the flight recorders ("black boxes") from American Airlines Flight 77. Dick Bridges, deputy manager for Arlington County, Virginia, said the cockpit voice recorder that used magnetic tape was damaged on the outside and the flight data recorder that used a solid-state drive was charred. Bridges said the recorders were found "right where the plane came into the building."
The cockpit voice recorder that used magnetic tape was transported to the NTSB lab in Washington, D.C., to see what data was salvageable. In its report on the cockpit voice recorder, the NTSB identified the unit as an L-3 Communications, Fairchild Aviation Recorders model A-100A cockpit voice recorder; a device which records on magnetic tape. The NTSB reported that "The majority of the recording tape was fused into a solid block of charred plastic." No usable segments of tape were found inside the recorder. All the data from the flight data recorder that used a solid-state drive was recovered.
Continuity of operations.
At the moment of impact, Secretary of Defense Donald Rumsfeld was in his office on the other side of the Pentagon, away from the crash site. He ran to the site and assisted the injured. Rumsfeld returned to his office, and went to a conference room in the Executive Support Center where he joined a secure videoteleconference with Vice President Dick Cheney and other officials. On the day of the attacks, DoD officials considered moving their command operations to Site R, a backup facility in Pennsylvania. Secretary of Defense Rumsfeld insisted he remain at the Pentagon, and sent Deputy Secretary Paul Wolfowitz to Site R. The National Military Command Center (NMCC) continued to operate at the Pentagon, even as smoke entered the facility. Engineers and building managers manipulated the ventilation and other building systems that still functioned to draw smoke out of the NMCC and bring in fresh air.
During a press conference held inside the Pentagon at 18:42, Rumsfeld announced, "The Pentagon's functioning. It will be in business tomorrow." Pentagon employees returned the next day to offices in mostly unaffected areas of the building. By the end of September, more workers returned to the lightly damaged areas of the Pentagon.
Aftermath.
Early estimates on rebuilding the damaged section of the Pentagon were that it would take three years to complete. However, the project moved forward at an accelerated pace and was completed by the one-year anniversary of the attack. The rebuilt section of the Pentagon includes a small indoor memorial and chapel at the point of impact. An outdoor memorial, commissioned by the Pentagon and designed by Julie Beckman and Keith Kaseman, was completed on schedule for its dedication on September 11, 2008.
Security camera video.
On May 16, 2006, the Department of Defense released filmed footage that was recorded by a security camera of American Airlines FlightÂ 77 crashing into the Pentagon, with a plane visible in one frame, as a "thin white blur" and an explosion following. The images were made public in response to a December 2004 Freedom of Information Act request by Judicial Watch. Some still images from the video had previously been released and publicly circulated, but this was the first official release of the edited video of the crash.
A nearby Citgo service station also had security cameras, but a video released on September 15, 2006 did not show the crash because the camera was pointed away from the crash site.
The Doubletree Hotel, located nearby in Crystal City, Virginia, also had a security camera video. On December 4, 2006, the FBI released the video in response to a FOIA lawsuit filed by Scott Bingham. The footage is "grainy and the focus is soft, but a rapidly growing tower of smoke is visible in the distance on the upper edge of the frame as the plane crashes into the building".
Memorials.
On September 12, 2002, Defense Secretary Donald Rumsfeld and General Richard Myers, Chairman of the Joint Chiefs of Staff, dedicated the Victims of Terrorist Attack on the Pentagon Memorial at Arlington National Cemetery. The memorial specifically honors the five individuals for whom no identifiable remains were found. This included Dana Falkenberg, age three, who was aboard American Airlines Flight 77 with her parents and older sister. A portion of the remains of 25 other victims are also buried at the site. The memorial is a pentagonal granite marker high. On five sides of the memorial along the top are inscribed the words "Victims of Terrorist Attack on the Pentagon September 11, 2001". Aluminum plaques, painted black, are inscribed with the names of the 184 victims of the terrorist attack. The site is located in Section 64, on a slight rise, which gives it a view of the Pentagon.
At the National September 11 Memorial, the names of the Pentagon victims are inscribed on the South Pool, on Panels S-1 and S-72 â S-76.

</doc>
<doc id="1905" url="http://en.wikipedia.org/wiki?curid=1905" title="Ambush">
Ambush

An ambush is a long-established military tactic, in which combatants take advantage of concealment and the element of surprise to attack unsuspecting enemy combatants from concealed positions, such as among dense underbrush or behind hilltops. Ambushes have been used consistently throughout history, from ancient to modern warfare.
History.
The use by early humans of the ambush may date as far back as two million years when anthropologists have recently suggested that ambush techniques were used to hunt large game. 
More recently, an ambush often might involve thousands of soldiers on a large scale, such as over a mountain pass. Ambushes appear many times in military history. One outstanding example from ancient times is the Battle of the Trebia river. Hannibal encamped within striking distance of the Romans with the Trebia River between them, and placed a strong force of cavalry and infantry in concealment, near the battle zone. He had noticed, says Polybius, a "âplace between the two camps, flat indeed and treeless, but well adapted for an ambuscade, as it was traversed by a water-course with steep banks, densely overgrown with brambles and other thorny plants, and here he proposed to lay a stratagem to surprise the enemyâ". 
When the Roman infantry became entangled in combat with his army, the hidden ambush force attacked the legionnaires in the rear. The result was slaughter and defeat for the Romans. Nevertheless the battle also displays the effects of good tactical discipline on the part of the ambushed force. Although most of the legions were lost, about 10,000 Romans cut their way through to safety, maintaining unit cohesion. This ability to maintain discipline and break out or maneuver away from a killing zone is a hallmark of good troops and training in any ambush situation. See Ranger reference below.
Another famous ambush was that sprung by Germanic warchief Arminius against the Romans at Battle of the Teutoburg Forest. This particular ambush was to have an impact on the course of Western history. The Germanic forces demonstrated several principles needed for a successful ambush. They took cover in difficult forested terrain, allowing the warriors time and space to mass without detection. They had the element of surprise, and this was also aided by the defection of Arminius from Roman ranks prior to the battle. They sprung the attack when the Romans were most vulnerable- when they had left their fortified camp, and were on the march in a pounding rainstorm. 
They did not dawdle at the hour of decision but attacked quickly, using a massive series of short, rapid, vicious charges against the length of the whole Roman line, with charging units sometimes withdrawing to the forest to regroup while others took their place. The Germans also made use of blocking obstacles, erecting a trench and earthen wall to hinder Roman movement along the route of the killing zone. The result was mass slaughter of the Romans, and the destruction of 3 legions. The Germanic victory caused a limit on Roman expansion in the West. Ultimately, it established the Rhine as the boundary of the Roman Empire for the next four hundred years, until the decline of the Roman influence in the West. The Roman Empire made no further concerted attempts to conquer Germania beyond the Rhine.
Procedure.
In modern warfare, an ambush is most often employed by ground troops up to platoon size against enemy targets, which may be other ground troops, or possibly vehicles. However, in some situations, especially when deep behind enemy lines, the actual attack will be carried out by a platoon, a company-sized unit will be deployed to support the attack group, setting up and maintaining a forward patrol harbour from which the attacking force will deploy, and to which they will retire after the attack.
Planning.
Ambushes are complex, multi-phase operations, and are, therefore, usually planned in some detail. First, a suitable killing zone is identified. This is the place where the ambush will be laid. It's generally a place where enemy units are expected to pass, and which gives reasonable cover for the deployment, execution, and extraction phases of the ambush patrol. A path along a wooded valley floor would be a typical example.
Ambush can be described geometrically as:

</doc>
<doc id="1908" url="http://en.wikipedia.org/wiki?curid=1908" title="Abzyme">
Abzyme

An abzyme (from antibody and enzyme), also called "catmab" (from "catalytic monoclonal antibody"), and most often called "catalytic antibody", is a monoclonal antibody with catalytic activity. Abzymes are usually raised in lab animals immunized against synthetic haptans, but some natural abzymes can be found in normal humans (anti-vasoactive intestinal peptide autoantibodies) and in patients with autoimmune diseases such as systemic lupus erythematosus, where they can bind to and hydrolyze DNA. To date abzymes display only weak, modest catalytic activity and have not proved to be of any practical use. They are, however, subjects of considerable academic interest. Studying them has yielded important insights into reaction mechanisms, enzyme structure and function, catalysis, and the immune system itself.
Enzymes function by lowering the activation energy of the transition state of a chemical reaction, thereby enabling the formation of an otherwise less-favorable molecular intermediate between the reactant(s) and the product(s). If an antibody is developed to bind to a molecule that's structurally and electronically similar to the transition state of a given chemical reaction, the developed antibody will bind to, and stabilize, the transition state, just like a natural enzyme, lowering the activation energy of the reaction, and thus catalyzing the reaction. By raising an antibody to bind to a stable transition-state analog, a new and unique type of enzyme is produced. 
So far, all catalytic antibodies produced have displayed only modest, weak catalytic activity. The reasons for low catalytic activity for these molecules have been widely discussed. Possibilities indicate that factors beyond the binding site may play an important, in particular through protein dynamics. Some abzymes have been engineered to use metal ions and other cofactors to improve their catalytic activity.
History.
The possibility of catalyzing a reaction by means of an antibody which binds the transition state was first suggested by William P. Jencks in 1969. In 1994, Peter G. Schultz and Richard A. Lerner received the prestigious Wolf Prize in Chemistry for developing catalytic antibodies for many reactions and popularizing their study into a significant sub-field of enzymology.
Potential HIV treatment.
In a June 2008 issue of the journal Autoimmunity Review, researchers S Planque, Sudhir Paul, Ph.D, and Yasuhiro Nishiyama, Ph.D of the University Of Texas Medical School at Houston announced that they have engineered an abzyme that degrades the superantigenic region of the gp120 CD4 binding site. This is the one part of the HIV virus outer coating that does not change, because it is the attachment point to T lymphocytes, the key cell in cell-mediated immunity. Once infected by HIV, patients produce antibodies to the more changeable parts of the viral coat. The antibodies are ineffective because of the virus' ability to change their coats rapidly. Because this protein gp120 is necessary for HIV to attach, it does not change across different strains and is a point of vulnerability across the entire range of the HIV variant population.
The abzyme does more than bind to the site, it catalytically destroys the site, rendering the virus inert, and then can attack other HIV viruses. A single abzyme molecule can destroy thousands of HIV viruses.

</doc>
<doc id="1909" url="http://en.wikipedia.org/wiki?curid=1909" title="Adaptive radiation">
Adaptive radiation

In evolutionary biology, adaptive radiation is a process in which organisms diversify rapidly into a multitude of new forms, particularly when a change in the environment makes new resources available, creates new challenges and opens environmental niches. Starting with a recent single ancestor, this process results in the speciation and phenotypic adaptation of an array of species exhibiting different morphological and physiological traits with which they can exploit a range of divergent environments.
Adaptive radiation, a characteristic example of cladogenesis, can be graphically illustrated as a "bush", or clade, of coexisting species (on the tree of life).
 Caribbean anoline lizards are a particularly interesting example of an adaptive radiation. The Hawaiian islands are very isolated and contribute numerous examples of adaptive radiation. An exceptional example of adaptive radiation would be the avian species of the Hawaiian honeycreepers. Via natural selection, these birds adapted rapidly and converged based on the different environments of the Hawaiian islands. 
Much research has been done on adaptive radiation due to its dramatic effects on the diversity of a population. However, more research is needed, especially to fully understand the many factors affecting adaptive radiation. Both empirical and theoretical approaches are helpful, though each has its disadvantages. In order to procure the largest amount of data, empirical and theoretical approaches must be united. 
Identification.
Four features can be used to identify an adaptive radiation:
Causes.
Innovation.
The evolution of a novel feature may permit a clade to diversify by making new areas of morphospace accessible. A classic example is the evolution of a fourth cusp in the mammalian tooth. This trait permits a vast increase in the range of foodstuffs which can be fed on. Evolution of this character has thus increased the number of ecological niches available to mammals. The trait arose a number of times in different groups during the Cenozoic, and in each instance was immediately followed by an adaptive radiation. Birds find other ways to provide for each other, i.e. the evolution of flight opened new avenues for evolution to explore, initiating an adaptive radiation.
Other examples include placental gestation (for eutherian mammals), or bipedal locomotion (in hominins).
Opportunity.
Adaptive radiations often occur as a result of an organism arising in an environment with unoccupied niches, such as a newly formed lake or isolated island chain. The colonizing population may diversify rapidly to take advantage of all possible niches.
In Lake Victoria, an isolated lake which formed recently in the African rift valley, over 300 species of cichlid fish adaptively radiated from one parent species in just 15,000 years.
Adaptive radiations commonly follow mass extinctions: following an extinction, many niches are left vacant. A classic example of this is the replacement of the non-avian dinosaurs with mammals at the end of the Cretaceous, and of brachiopods by bivalves at the Permo-Triassic boundary.

</doc>
<doc id="1910" url="http://en.wikipedia.org/wiki?curid=1910" title="Agarose gel electrophoresis">
Agarose gel electrophoresis

Agarose gel electrophoresis is a method of gel electrophoresis used in biochemistry, molecular biology, and clinical chemistry to separate a mixed population of DNA or proteins in a matrix of agarose. The proteins may be separated by charge and/or size (IEF agarose, essentially size independent), and the DNA and RNA fragments by length. Biomolecules are separated by applying an electric field to move the negatively charged molecules through an agarose matrix, and the biomolecules are separated by size in the agarose gel matrix.
Agarose gels are easy to cast and are particularly suitable for separating larger DNA of size range most often encountered in laboratories, which accounts for the popularity of its use. The separated DNA may be viewed with stain, most commonly under UV light, and the DNA fragments can be extracted from the gel with relative ease. Most agarose gels used are between 0.7 - 2% dissolved in a suitable electrophoresis buffer.
Properties of agarose gel.
Agarose gel is a three-dimensional matrix formed of helical agarose molecules in supercoiled bundles that are aggregated into three-dimensional structures with channels and pores through which biomolecules can pass. The 3-D structure is held together with hydrogen bonds and can therefore be disrupted by heating back to a liquid state. The melting temperature is different from the gelling temperature, depending on the sources, agarose gel has a gelling temperature of 35-42Â°C and a melting temperature of 85-95Â°C. Low-melting and low-gelling agaroses made through chemical modifications are also available.
Agarose gel has large pore size and good gel strength that made it particularly suitable as an anticonvection medium for the electrophoresis of DNA and large protein molecules. The pore size of a 1% gel has been estimated from 100Â nm to 200-500Â nm, and its gel strength allows gels as dilute as 0.15% to form slab for gel electrophoresis. Agarose gel has lower resolving power than polyacrylamide gel for DNA but has a greater range of separation, and is therefore used for DNA fragments of usually 50-20,000 bp in size. The limit of resolution for standard agarose gel electrophoresis is around 750 kb, but resolution of over 6 Mb is possible with pulsed field gel electrophoresis (PFGE). It can also be used to separate large protein, and it is the preferred matrix for the gel electrophoresis of particles with effective radii larger than 5-10Â nm. A 0.9% agarose gel has pores large enough for the entry of bacteriophage T4.
The agarose polymer contains charged groups, in particular pyruvate and sulphate. These negatively-charged groups create a flow of water in the opposite direction to the movement of DNA in a process called electroendosmosis (EEO), and can therefore retard the movement of DNA and cause blurring of bands. Higher concentration gel would have higher electroosmotic flow. Low EEO agarose is therefore generally preferred for use in agarose gel electrophoresis of nucleic acids, but high EEO agarose may be used for other purposes. The lower sulphate content of low EEO agarose, particularly low-melting point (LMP) agarose, is also beneficial in cases where the DNA extracted from gel is to be used for further manipulation as the presence of contaminating sulphate may affect some subsequent procedures, such as ligation and PCR. Zero EEO agaroses however are undesirable for some applications as they may be made by adding positively-charged group and such groups can affect subsequent enzyme reactions.
Migration of nucleic acids in agarose gel.
Factors affect migration of nucleic acid in gel.
A number of factors can affect the migration of nucleic acids: the dimension of the gel pores (gel concentration), size of DNA being electrophoresed, the voltage used, the ionic strength of the buffer, and the concentration intercalating dye such as ethidium bromide if used during electrophoresis.
Smaller molecules travel faster than larger molecules in gel, and double-stranded DNA moves at a rate that is inversely proportional to the log10 of the number of base pairs. This relationship however breaks down with very large DNA fragments, and separation of very large DNA fragments requires the use of pulsed field gel electrophoresis (PFGE).
For standard agarose gel electrophoresis, larger molecules are resolved better using a low concentration gel while smaller molecules separate better at high concentration gel. High concentrations gel however requires longer run times (sometimes days).
The movement of the DNA may be affected by the conformation of the DNA molecule, for example, supercoiled DNA usually moves faster than relaxed DNA because it is tightly coiled and hence more compact. In a normal plasmid DNA preparation, multiple forms of DNA may be present. Gel electrophoresis of the plasmids would normally show the negatively supercoiled form as the main band, while nicked DNA (open circular form) and the relaxed closed circular form appears as minor bands. The rate at which the various forms move however can change using different electrophoresis conditions, and the mobility of larger circular DNA may be more strongly affected than linear DNA by the pore size of the gel.
Ethidium bromide which intercalates into circular DNA can change the charge, length, as well as the superhelicity of the DNA molecule, therefore its presence in gel during electrophoresis can affect its movement. Agarose gel electrophoresis can be used to resolve circular DNA with different supercoiling topology.
DNA damage due to increased cross-linking will also reduce electrophoretic DNA migration in a dose-dependent way.
The rate of migration of the DNA is proportional to the voltage applied, i.e. the higher the voltage, the faster the DNA moves. The resolution of large DNA fragments however is lower at high voltage. The mobility of DNA may also change in an unsteady field - in a field that is periodically reversed, the mobility of DNA of a particular size may drop significantly at a particular cycling frequency. This phenomenon can result in band inversion in field inversion gel electrophoresis (FIGE), whereby larger DNA fragments move faster than smaller ones.
Mechanism of migration and separation.
The negative charge of its phosphate backbone moves the DNA towards the positively-charged anode during electrophoresis. However, the migration of DNA molecules in solution, in the absence of a gel matrix, is independent of molecular weight during electrophoresis. The gel matrix is therefore responsible for the separation of DNA by size during electrophoresis, and a number of models exist to explain the mechanism of separation of biomolecules in gel matrix. A widely accepted one is the Ogston model which treats the polymer matrix as a sieve. A globular protein or a random coil DNA moves through the interconnected pores, and the movement of larger molecules is more likely to be impeded and slowed down by collisions with the gel matrix, and the molecules of different sizes can therefore be separated in this sieving process.
The Ogston model however breaks down for large molecules whereby the pores are significantly smaller than size of the molecule. For DNA molecules of size greater than 1 kb, a reptation model (or its variants) is most commonly used. This model assumes that the DNA can crawl in a "snake-like" fashion (hence "reptation") through the pores as an elongated molecule. At higher electric field strength, this turned into a biased reptation model, whereby the leading end of the molecule become strongly biased in the forward direction and pulls the rest of the molecule along. Real-time fluorescence microscopy of stained molecules, however, showed more subtle dynamics during electrophoresis, with the DNA showing considerable elasticity as it alternately stretching in the direction of the applied field and then contracting into a ball, or becoming hooked into a U-shape when it gets caught on the polymer fibres.
General procedure.
The details of an agarose gel electrophoresis experiment may vary depending on methods, but most follow a general procedure.
Casting of gel.
The gel is prepared by dissolving the agarose powder in an appropriate buffer, such as TAE or TBE, to be used in electrophoresis. The agarose is dispersed in the buffer before heating it to near-boiling point, but avoid boiling. The melted agarose is allowed to cool sufficiently before pouring the solution into a cast as the cast may warp or crack if the agarose solution is too hot. A comb is placed in the cast to create wells for loading sample, and the gel should be completely set before use.
The concentration of gel affects the resolution of DNA separation. For a standard agarose gel electrophoresis, a 0.8% gives good separation or resolution of large 5â10kb DNA fragments, while 2% gel gives good resolution for small 0.2â1kb fragments. 1% gels are common for many applications. The concentration is measured in weight of agarose over volume of buffer used. High percentage gels are often brittle and may not set evenly, while low percentage gels (01.-0.2%) are fragile and not easy to handle. Low-melting-point (LMP) agarose gels are also more fragile than normal agarose gel. PFGE and FIGE are often done with high percentage agarose gels.
Loading of samples.
Once the gel has set, the comb is removed, leaving wells where DNA samples can be loaded. Loading buffer is mixed with the DNA sample before the mixture is loaded into the wells. The loading buffer contains a dense compound, which may be glycerol, sucrose, or Ficoll, that raises the density of the sample so that the DNA sample may sink to the bottom of the well. If the DNA sample contains residual ethanol after its preparation, it may float out of the well. The loading buffer also include colored dyes such as xylene cyanol and bromophenol blue used to monitor the progress of the electrophoresis. The DNA samples are loaded using a pipette.
Electrophoresis.
Agarose gel electrophoresis is most commonly done horizontally in a submarine mode whereby the slab gel is completely submerged in buffer during electrophoresis. It is also possible, but less common, to perform the electrophoresis vertically, as well as horizontally with the gel raised on agarose legs using the appropriate apparatus. The buffer used in the gel is the same as the running buffer in the electrophoresis tank, which is why electrophoresis in the submarine mode is possible with agarose gel.
For optimal resolution of DNA greater than 2kb in size in standard gel electrophoresis, 5 to 8 V/cm is recommended (the distance in cm refers to the distance between electrodes, therefore this recommended voltage would be 5 to 8 multiplied by the distance between the electrodes in cm). Voltage may also be limited by the fact that it heats the gel and may cause the gel to melt if it is run at high voltage for a prolonged period, especially if the gel used is LMP agarose gel. Too high a voltage may also reduce resolution, as well as causing band streaking for large DNA molecules. Too low a voltage may lead to broadening of band for small DNA fragments due to dispersion and diffusion.
Since DNA is not visible in natural light, the progress of the electrophoresis is monitored using colored dyes. Xylene cyanol (light blue color) comigrates large DNA fragments, while Bromophenol blue (dark blue) comigrates with the smaller fragments. Less commonly-used dyes include Cresol Red and Orange G which migrate ahead of bromophenol blue. A DNA marker is also run together for the estimation of the molecular weight of the DNA fragments. Note however that the size of a circular DNA like plasmids cannot be accurately gauged using standard markers unless it has been linearized by restriction digest, alternatively a supercoiled DNA marker may be used.
Staining and visualization.
DNA as well as RNA are normally visualized by staining with ethidium bromide, which intercalates into the major grooves of the DNA and fluoresces under UV light. The ethidium bromide may be added to the agarose solution before it gels, or the DNA gel may be stained later after electrophoresis. Destaining of the gel is not necessary but may produce better images. Other methods of staining are available; examples are SYBR Green, GelRed, methylene blue, brilliant cresyl blue, Nile blue sulphate, and crystal violet. SYBR Green, GelRed and other similar commercial products are sold as safer alternatives to ethidium bromide as it has been shown to be mutagenic in Ames test, although the carcinogenicity of ethidium bromide has not actually been established. SYBR Green requires the use of a blue-light transilluminator. DNA stained with crystal violet can be viewed under natural light without the use of a UV transilluminator which is an advantage, however it may not produce a strong band.
When stained with ethidium bromide, the gel is viewed with an ultraviolet (UV) transilluminator. Standard transilluminators use wavelengths of 302/312-nm (UV-B), however exposure of DNA to UV radiation for as little as 45 seconds can produce damage to DNA and affect subsequent procedures, for example reducing the efficiency of transformation, "in vitro" transcription, and PCR. Exposure of the DNA to UV radiation therefore should be limited. Using a higher wavelength of 365Â nm (UV-A range) causes less damage to the DNA but also produces much weaker fluorescence with ethidium bromide. Where multiple wavelengths can be selected in the transillumintor, the shorter wavelength would be used to capture images, while the longer wavelength should be used when it is necessary to work on the gel for any extended period of time.
The transilluminator apparatus may also contain image capture devices, such as a digital or polaroid camera, that allow an image of the gel to be taken or printed.
Downstream procedures.
The separated DNA bands are often used for further procedures, and a DNA band may be cut out of the gel as a slice, dissolved and purified. The gels may also be used for blotting techniques.
Buffers.
In general, the ideal buffer should have good conductivity, produce less heat and have a long life. There are a number of buffers used for agarose electrophoresis. The most common being, for nucleic acids Tris/Acetate/EDTA (TAE), Tris/Borate/EDTA (TBE). Many other buffers have been proposed, e.g. lithium borate (LB), which is almost never used, based on Pubmed citations, iso electric histidine, pK matched goods buffers, etc.; in most cases the purported rationale is lower current (less heat) and or matched ion mobilities, which leads to longer buffer life. Tris-phosphate buffer has high buffering capacity but cannot be used if DNA extracted is to be used in phosphate sensitive reaction. Borate is problematic; Borate can polymerize, and/or interact with cis diols such as those found in RNA. TAE has the lowest buffering capacity but provides the best resolution for larger DNA. This means a lower voltage and more time, but a better product. LB is relatively new and is ineffective in resolving fragments larger than 5 kbp; However, with its low conductivity, a much higher voltage could be used (up to 35 V/cm), which means a shorter analysis time for routine electrophoresis. As low as one base pair size difference could be resolved in 3% agarose gel with an extremely low conductivity medium (1 mM lithium borate). The buffers used contain EDTA to inactivate many nucleases which require divalent cation for their function.
Applications.
Agarose gels are easily cast and handled compared to other matrices and nucleic acids are not chemically altered during electrophoresis. Samples are also easily recovered. After the experiment is finished, the resulting gel can be stored in a plastic bag in a refrigerator.
Electrophoresis is performed in buffer solutions to reduce pH changes due to the electric field, which is important because the charge of DNA and RNA depends on pH, but running for too long can exhaust the buffering capacity of the solution. Further, different preparations of genetic material may not migrate consistently with each other, for morphological or other reasons.

</doc>
<doc id="1911" url="http://en.wikipedia.org/wiki?curid=1911" title="Allele">
Allele

An allele ( or ), or allel, is one of a number of alternative forms of the same gene or same genetic locus. Sometimes, different alleles can result in different observable phenotypic traits, such as different pigmentation. However, most genetic variations result in little or no observable variation.
Most multicellular organisms have two sets of chromosomes; that is, they are diploid. These chromosomes are referred to as homologous chromosomes. Diploid organisms have one copy of each gene (and, therefore, one allele) on each chromosome. If both alleles are the same, they and the organism are homozygous with respect to that gene. If the alleles are different, they and the organism are heterozygous with respect to that gene.
The word "allele" is a short form of allelomorph ("other form"), which was used in the early days of genetics to describe variant forms of a gene detected as different phenotypes. It derives from the Greek prefix "á¼Î»Î»Î®Î»", "allel", meaning "reciprocal" or "each other", which itself is related to the Greek adjective á¼Î»Î»Î¿Ï (allos; cognate with Latin "alius"), meaning "other".
Dominant and recessive alleles.
In many cases, genotypic interactions between the two alleles at a locus can be described as dominant or recessive, according to which of the two homozygous phenotypes the heterozygote most resembles. Where the heterozygote is indistinguishable from one of the homozygotes, the allele involved is said to be dominant to the other, which is said to be recessive to the former. The degree and pattern of dominance varies among loci. This type of interaction was first formally described by Gregor Mendel. However, many traits defy this simple categorization and the phenotypes are modeled by co-dominance and polygenic inheritance.
The term "wild type" allele is sometimes used to describe an allele that is thought to contribute to the typical phenotypic character as seen in "wild" populations of organisms, such as fruit flies ("Drosophila melanogaster"). Such a "wild type" allele was historically regarded as dominant, common, and normal, in contrast to "mutant" alleles regarded as recessive, rare, and frequently deleterious. It was formerly thought that most individuals were homozygous for the "wild type" allele at most gene loci, and that any alternative "mutant" allele was found in homozygous form in a small minority of "affected" individuals, often as genetic diseases, and more frequently in heterozygous form in "carriers" for the mutant allele. It is now appreciated that most or all gene loci are highly polymorphic, with multiple alleles, whose frequencies vary from population to population, and that a great deal of genetic variation is hidden in the form of alleles that do not produce obvious phenotypic differences.
Multiple alleles.
A population or species of organisms typically includes multiple alleles at each locus among various individuals. Allelic variation at a locus is measurable as the number of alleles (polymorphism) present, or the proportion of heterozygotes in the population.
For example, at the gene locus for the ABO blood type carbohydrate antigens in humans, classical genetics recognizes three alleles, IA, IB, and i, that determine compatibility of blood transfusions. Any individual has one of six possible genotypes (IAIA, IAi, IBIB, IBi, IAIB, and ii) that produce one of four possible phenotypes: "Type A" (produced by IAIA homozygous and IAi heterozygous genotypes), "Type B" (produced by IBIB homozygous and IBi heterozygous genotypes), "Type AB" produced by IAIB heterozygous genotype, and "Type O" produced by ii homozygous genotype. It is now known that each of the A, B, and O alleles is actually a class of multiple alleles with different DNA sequences that produce proteins with identical properties: more than 70 alleles are known at the ABO locus. An individual with "Type A" blood may be an AO heterozygote, an AA homozygote, or an AA heterozygote with two different "A" alleles.
Allele and genotype frequencies.
The frequency of alleles in a diploid population can be used to predict the frequencies of the corresponding genotypes (see Hardy-Weinberg principle). For a simple model, with two alleles:
where "p" is the frequency of one allele and "q" is the frequency of the alternative allele, which necessarily sum to unity. Then, "p"2 is the fraction of the population homozygous for the first allele, 2"pq" is the fraction of heterozygotes, and "q"2 is the fraction homozygous for the alternative allele. If the first allele is dominant to the second then the fraction of the population that will show the dominant phenotype is "p"2 + 2"pq", and the fraction with the recessive phenotype is "q"2.
With three alleles:
In the case of multiple alleles at a diploid locus, the number of possible genotypes (G) with a number of alleles (a) is given by the expression:
Allelic dominance in genetic disorders.
A number of genetic disorders are caused when an individual inherits two recessive alleles for a single-gene trait. Recessive genetic disorders include Albinism, Cystic Fibrosis, Galactosemia, Phenylketonuria (PKU), and Tay-Sachs Disease. Other disorders are also due to recessive alleles, but because the gene locus is located on the X chromosome, so that males have only one copy (that is, they are hemizygous), they are more frequent in males than in females. Examples include red-green color blindness and Fragile X syndrome.
Other disorders, such as Huntington disease, occur when an individual inherits only one dominant allele.

</doc>
<doc id="1912" url="http://en.wikipedia.org/wiki?curid=1912" title="Ampicillin">
Ampicillin

Ampicillin is an antibiotic useful for the treatment of a number of bacterial infections. It is a beta-lactam antibiotic that is part of the aminopenicillin family and is roughly equivalent to amoxicillin in terms of activity. It is taken by mouth. It is active against many Gram-(+) and Gram-(-) bacteria.
It is effective for ear infections and respiratory infections such as sinusitis caused by bacteria, acute exacerbations of COPD, and epiglottis. It is also sometimes used for the treatment of urinary tract infections, meningitis, and salmonella infections, but resistance to ampicillin is increasingly common among the bacteria responsible for these infections. 
Common side effects include rash, diarrhea, nausea and vomiting.<Ref name =FDA></ref> It is not useful for the treatment of viral infections. 
It is on the World Health Organization's List of Essential Medicines, a list of the most important medication needed in a basic health system.
Medical uses.
Ampicillin is active against Gram-(+) bacteria including "Streptococcus pneumoniae", "Streptococcus pyogenes", "Staphylococcus aureus" (but not methicillin-resistant strains), and some "Enterococci". Activity against Gram-(-) bacteria includes "Neisseria meningitidis", some "Haemophilus influenzae", and some Enterobacteriaceae. Its spectrum of activity is enhanced by co-administration of sulbactam, a drug that inhibits beta lactamase, an enzyme produced by bacteria to inactivate ampicillin and related antibiotics.
It is used for the treatment of infections known to be or highly likely to be caused by these bacteria. These include common respiratory infections including sinusitis, bronchitis, and pharyngitis, as well as otitis media. In combination with vancomycin (which provides coverage of ampicillin-resistant pneumococci), it is effective for the treatment of bacterial meningitis. It is also used for gastrointestinal infections caused by consuming contaminated water or food, such as "Salmonella", "Shigella", and "Listeriosis".
Ampicillin is a first-line agent for the treatment of infections caused by "Enterococci". The bacteria are an important cause of healthcare-associated infections such as endocarditis, meningitis, and catheter-associated urinary tract infections that are typically resistant to other antibiotics.
Adverse Effects.
Ampicillin is relatively non-toxic. Its most common side effects include rash, diarrhea, nausea and vomiting.<Ref name =FDA></ref> In very rare cases it causes severe side effects such as anaphylaxis and "Clostridium difficile" diarrhea.
Mechanism of action.
Belonging to the penicillin group of beta-lactam antibiotics, ampicillin is able to penetrate Gram-positive and some Gram-negative bacteria. It differs from penicillin G, or benzylpenicillin, only by the presence of an amino group. That amino group helps the drug penetrate the outer membrane of Gram-negative bacteria.
Ampicillin acts as an irreversible inhibitor of the enzyme transpeptidase, which is needed by bacteria to make their cell walls. It inhibits the third and final stage of bacterial cell wall synthesis in binary fission, which ultimately leads to cell lysis. Ampicillin is bacteriocidal.
History.
Ampicillin has been used extensively to treat bacterial infections since 1961. Until the introduction of ampicillin by the British company Beecham, penicillin therapies had only been effective against Gram-positive organisms such as staphylococci and streptococci. Ampicillin (originally branded as 'Penbritin') also demonstrated activity against Gram-negative organisms such as "H. influenzae", coliforms and "Proteus" spp. 

</doc>
<doc id="1913" url="http://en.wikipedia.org/wiki?curid=1913" title="Annealing">
Annealing

Annealing may refer to:

</doc>
<doc id="1914" url="http://en.wikipedia.org/wiki?curid=1914" title="Antibiotic resistance">
Antibiotic resistance

Antibiotic resistance is a form of drug resistance whereby some (or, less commonly, all) sub-populations of a microorganism, usually a bacterial species, are able to survive after exposure to one or more antibiotics; pathogens resistant to multiple antibiotics are considered "multidrug resistant" (MDR) or, more colloquially, superbugs.
Antibiotic resistance is a serious and growing phenomenon in contemporary medicine and has emerged as one of the pre-eminent public health concerns of the 21st century, in particular as it pertains to pathogenic organisms (the term is especially relevant to organisms that cause disease in humans). A World Health Organization report released April 30, 2014 states, "this serious threat is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country. Antibiotic resistanceâwhen bacteria change so antibiotics no longer work in people who need them to treat infectionsâis now a major threat to public health."
In the simplest cases, drug-resistant organisms may have acquired resistance to first-line antibiotics, thereby necessitating the use of second-line agents. Typically, a first-line agent is selected on the basis of several factors including safety, availability, and cost; a second-line agent is usually broader in spectrum, has a less favourable risk-benefit profile, and is more expensive or, in dire circumstances, may be locally unavailable. In the case of some MDR pathogens, resistance to second- and even third-line antibiotics is, thus, sequentially acquired, a case quintessentially illustrated by "Staphylococcus aureus" in some nosocomial settings. Some pathogens, such as "Pseudomonas aeruginosa", also possess a high level of intrinsic resistance.
It may take the form of a spontaneous or induced genetic mutation, or the acquisition of resistance genes from other bacterial species by horizontal gene transfer via conjugation, transduction, or transformation. Many antibiotic resistance genes reside on transmissible plasmids, facilitating their transfer. Exposure to an antibiotic naturally selects for the survival of the organisms with the genes for resistance. In this way, a gene for antibiotic resistance may readily spread through an ecosystem of bacteria. Antibiotic-resistance plasmids frequently contain genes conferring resistance to several different antibiotics. This is not the case for "Mycobacterium tuberculosis", the bacteria that causes Tuberculosis, since evidence is lacking for whether these bacteria have plasmids. Also "M. tuberculosis" lack the opportunity to interact with other bacteria in order to share plasmids.
Genes for resistance to antibiotics, like the antibiotics themselves, are ancient. However, the increasing prevalence of antibiotic-resistant bacterial infections seen in clinical practice stems from antibiotic use both within human medicine and veterinary medicine. Any use of antibiotics can increase selective pressure in a population of bacteria to allow the resistant bacteria to thrive and the susceptible bacteria to die off. As resistance towards antibiotics becomes more common, a greater need for alternative treatments arises. However, despite a push for new antibiotic therapies, there has been a continued decline in the number of newly approved drugs. Antibiotic resistance therefore poses a significant problem.
The growing prevalence and incidence of infections due to MDR pathogens is epitomised by the increasing number of familiar acronyms used to describe the causative agent and sometimes the infection; of these, MRSA is probably the most well-known, but others including VISA (vancomycin-intermediate "S. aureus"), VRSA (vancomycin-resistant "S. aureus"), ESBL (Extended spectrum beta-lactamase), VRE (Vancomycin-resistant "Enterococcus") and MRAB (Multidrug-resistant "A. baumannii") are prominent examples. Nosocomial infections overwhelmingly dominate cases where MDR pathogens are implicated, but multidrug-resistant infections are also becoming increasingly common in the community.
Cause.
Although there were low levels of preexisting antibiotic-resistant bacteria before the widespread use of antibiotics, evolutionary pressure from their use has played a role in the development of multidrug-resistant varieties and the spread of resistance between bacterial species. The widespread use of antibiotics both inside and outside medicine is playing a significant role in the emergence of resistant bacteria.
In some countries, antibiotics are sold over the counter without a prescription, which also leads to the creation of resistant strains. Other practices contributing to resistance include antibiotic use in livestock feed to promote faster growth. Household use of antibacterials in soaps and other products, although not clearly contributing to resistance, is also discouraged (as not being effective at infection control). Unsound practices in the pharmaceutical manufacturing industry can also contribute towards the likelihood of creating antibiotic-resistant strains. The procedures and clinical practice during the period of drug treatment are frequently flawedÂ â usually no steps are taken to isolate the patient to prevent re-infection or infection by a new pathogen, negating the goal of complete destruction by the end of the course (see Healthcare-associated infections and Infection control).
Certain antibiotic classes are more highly associated with colonisation with "superbugs" compared to other antibiotic classes. A superbug, also called multiresistant, is a bacterium that carries several resistance genes. The risk for colonisation increases if there is a lack of susceptibility (resistance) of the superbugs to the antibiotic used and high tissue penetration, as well as broad-spectrum activity against "good bacteria". In the case of MRSA, increased rates of MRSA infections are seen with glycopeptides, cephalosporins, and especially quinolones. In the case of colonisation with "Clostridium difficile", the high-risk antibiotics include cephalosporins and in particular quinolones and clindamycin.
Of antibiotics used in the United States in 1997, half were used in humans and half in animals; in 2013, 80% were used in animals.
Natural occurrence.
There is evidence that naturally occurring antibiotic resistance is common. The genes that confer this resistance are known as the environmental resistome. These genes may be transferred from non-disease-causing bacteria to those that do cause disease, leading to clinically significant antibiotic resistance.
In 1952, an experiment conducted by Joshua and Esther Lederberg showed that penicillin-resistant bacteria existed before penicillin treatment. While experimenting at the University of Wisconsin-Madison, Joshua Lederberg and his graduate student Norton Zinder also demonstrated preexistent bacterial resistance to streptomycin. In 1962, the presence of penicillinase was detected in dormant "Bacillus licheniformis" endospores, revived from dried soil on the roots of plants, preserved since 1689 in the British Museum. Six strains of "Clostridium", found in the bowels of William Braine and John Hartnell (members of the Franklin Expedition) showed resistance to cefoxitin and clindamycin. It was suggested that penicillinase may have emerged as a defense mechanism for bacteria in their habitats, such as the case of penicillinase-rich "Staphylococcus aureus", living with penicillin-producing "Trichophyton", however this was deemed circumstantial. Search for a penicillinase ancestor has focused on the class of proteins that must be "a priori" capable of specific combination with penicillin. The resistance to cefoxitin and clindamycin in turn was attributed to Braine's and Hartnell's contact with microorganisms that naturally produce them or random mutation in the chromosomes of "Clostridium" strains. Nonetheless there is evidence that heavy metals and some pollutants may select for antibiotic-resistant bacteria, generating a constant source of them in small numbers.
Medicine.
The volume of antibiotic prescribed is the major factor in increasing rates or bacterial resistance rather than compliance with antibiotics. Inappropriate prescribing of antibiotics has been attributed to a number of causes, including people insisting on antibiotics, physicians prescribing them as they feel they do not have time to explain why they are not necessary, and physicians not knowing when to prescribe antibiotics or being overly cautious for medical and/or legal reasons. For example, a third of people believe that antibiotics are effective for the common cold, and the common cold is the most common reason antibiotics are prescribed even though antibiotics are useless against viruses. A single regimen of antibiotics even in compliant patients leads to a greater risk of resistant organisms to that antibiotic in the person for a month to possibly a year.
Antibiotic resistance has been shown to increase with duration of treatment; therefore, as long as an effective lower limit is observed, the use by the medical community of shorter courses of antibiotics is likely to decrease rates of resistance, reduce cost, and have better outcomes due to fewer complications such as C. difficile infection and diarrhea. In some situations a short course is inferior to a long course.
A BMJ editorial recommended that antibiotics can often be safely stopped 72Â hours after symptoms resolve. Because patients may feel better before the infection is eradicated, doctors must provide instructions to patients so they know when it is safe to stop taking a prescription. Some researchers advocate doctors' using a very short course of antibiotics, reevaluating the patient after a few days, and stopping treatment if there are no longer clinical signs of infection.
Patients taking less than the required dosage or failing to take their doses within the prescribed timing results in decreased concentration of antibiotics in the bloodstream and tissues, and, in turn, exposure of bacteria to suboptimal antibiotic concentrations increases the frequency of antibiotic resistant organisms, however factors within the intensive care unit setting such as mechanical ventilation and multiple underlying diseases also appeared to contribute to bacterial resistance. These nosocomial pneumonia patients represented a situation where there was relatively little contribution of host defense to outcome, and therefore may not be applicable to otherwise healthy individuals taking antibiotics.
Poor hand hygiene by hospital staff has been associated with the spread of resistant organisms, and an increase in hand washing compliance results in decreased rates of these organisms.
The improper use of antibiotics and therapeutic treatments can often be attributed to the presence of structural violence in particular regions. Socioeconomic factors such as race and poverty affect the accessibility of and adherence to drug therapy. The efficacy of treatment programs for these drug-resistant strains depends on whether or not programmatic improvements take into account the effects of structural violence.
Veterinary medicine.
The emergence of antibiotic-resistant microorganisms in human medicine is primarily the result of the use of antibiotics in humans, although the use of antibiotics in animals is also partly responsible.
Since the last third of the 20th century, there has been extensive use of antibiotics in animal husbandry. Some of these drugs are not considered significant for use in humans, because of their lack of either efficacy or purpose in humans (such as the use of ionopores in ruminants), or because that drug has mostly gone out of use in humans. Others however are used in both animals and humans, including penicillin and some forms of tetracycline. Historically, regulation of antibiotic use in food animals has been limited to limiting drug residues in meat, egg, and milk products, rather than by direct concern over the development of antibiotic resistance. This mirrors the primary concerns in human medicine, where, in general, researchers and doctors were more concerned about effective but non-toxic doses of drugs rather than antibiotic resistance.
The resistant bacteria which antibiotic exposure selects in animals can be transmitted to humans via three pathways, those being through the consumption of animal products (milk, meat, eggs, etc.), from close or direct contact with animals or other humans, or through the environment. In the first pathway, food preservation methods can help eliminate, decrease, or prevent the growth of bacteria in some food classes. Evidence for the transfer of antibiotic- resistant microorganisms from animals to humans has been scant, and most evidence shows that pathogens of concern in human populations originated in humans and are maintained there, with rare cases of transference to humans. The use of antibiotics in animals can be classified into different use patterns according to its purpose. The most accepted classification discriminates therapeutic, prophylactic, metaphylactic, and growth promotion uses of antibiotics. All four patterns select for bacterial resistance, since antibiotic resistance is a natural evolutionary process, but the non-therapeutic uses expose larger number of animals, and therefore of bacteria, for more extended periods, and at lower doses. They therefore greatly increase the cross-section for the evolution of resistance.
The World Health Organization concluded that inappropriate use of antibiotics in animal husbandry is an underlying contributor to the emergence and spread of antibiotic-resistant germs, and that the use of antibiotics as growth promoters in animal feeds should be prohibited, in the absence of risk assessments. Regarding this matter, the OIE has added to the Terrestrial Animal Health Code a series of guidelines with recommendations to its members for the creation and harmonization of national antimicrobial resistance surveillance and monitoring programs, monitoring of the quantities of antibiotics used in animal husbandry, and recommendations to ensure the proper and prudent use of antibiotic substances. Another guideline introduced in the terrestrial Animal Health Code is the implementation of methodologies that help to establish risk factors associated with this worldwide concern. The OIE concluded that risk assessments should be performed in order to assess, manage and define the possible health risks of antibiotic resistance in human and animal populations.
In the world, antibiotics are widely used on animals. As in human medicine, antibiotics can often be bought without prescription and veterinary supervision for use on pets and livestock. Bacteria remaining in these animals are likely to be resistant to the antibiotics used, and may be passed into the environment by the excretion and secretion of materials such as milk, feces, urine, saliva, semen, lochia, etc. The actual impact of these resistant germs depends on their specific type and on the animal or organism they henceforth infect. Some germs, such as tetanus, are toxic regardless of their antibiotic-resistant status. (It is useful to remember that antibiotics are not used in treatment of all diseases caused by bacteria. Tetanus, as an example, is prevented by vaccine and is extremely difficult to treat once symptoms appear.)
In 1998, European Union health ministers voted to ban four antibiotics widely used to promote animal growth (despite their scientific panel's recommendations). Regulation banning the use of antibiotics in European feed, with the exception of two antibiotics in poultry feeds, became effective in 2006. In Scandinavia, there is evidence that the ban has led to a lower prevalence of antimicrobial resistance in (nonhazardous) animal bacterial populations. However, a corresponding change in antibiotic-resistance cases among humans has not yet been demonstrated.
United States.
In the United States, the United States Department of Agriculture (USDA) and the Food and Drug Administration (FDA) collect data on antibiotic use in animals and humans. In research studies, occasional animal-to-human spread of drug-resistant organisms has been demonstrated. Antibiotics and other drugs are used in U.S. animal feed to promote animal productivity. In particular, poultry feed and water is a common route of administration of drugs, due to higher overall costs when drugs are administered by handling animals individually. In general, animals that appear ill are not permitted to be slaughtered for human consumption in the United States.
Growing U.S. consumer concern about using antibiotics in animal feed has led to a niche market of "antibiotic-free" animal products, but this small market is unlikely to change entrenched, industry-wide practices.
In 2001, the Union of Concerned Scientists estimated that greater than 70% of the antibiotics used in the U.S. are given to food animals (for example, chickens, pigs, and cattle), in the absence of disease. The amounts given are termed "sub-therapeutic", i.e., insufficient to combat disease. Despite no diagnosis of disease, the administration of these drugs (most of which are not significant to human medicine) results in decreased mortality and morbidity and increased growth in the animals so treated. It is theorized that sub-therapeutic dosages kills some, but not all, of the bacterial organisms in the animal â likely leaving those that are naturally antibiotic-resistant. Studies have shown, however, that, in essence, the overall population levels of bacteria are unchanged; only the mix of bacteria is affected.
The actual mechanism by which sub-therapeutic antibiotic feed additives serve as growth promoters is thus unclear. Some people have speculated that animals and fowl may have sub-clinical infections, which would be cured by low levels of antibiotics in feed, thereby allowing the creatures to thrive. No convincing evidence has been advanced for this theory, and the bacterial load in an animal is essentially unchanged by use of antibiotic feed additives. The mechanism of growth promotion is therefore probably something other than "killing off the bad bugs."
In 2000, the FDA announced their intention to revoke approval of fluoroquinolone use in poultry production because of substantial evidence linking it to the emergence of fluoroquinolone-resistant "Campylobacter" infections in humans. Legal challenges from the food animal and pharmaceutical industries delayed the final decision to do so until 2006. Fluroquinolones have been banned from extra-label use in food animals in the USA since 2007. However, they remain widely used in companion and exotic animals.
In 2001, "National Hog Farmer" magazine warned U.S. producers that "C. difficile" "is sweeping the industry, killing many piglets" (Neutkens D; "New Clostridium Claiming Baby Pigs"). In 2006, a study by the USDA's National Animal Health Monitoring System further investigated the prevalence of "C. difficile" in hog farms. The study, which covered hog farms of a size typical of those producing 94% of US swine, found the prevalence of "C. difficile" "relatively low (11.4%)" and that there was no difference in region or in size of farm. Human infection with "C. difficile" (either drug-resistant or not) is most commonly associated with the use of strong antibiotics in hospitalized humans, and is not associated with humans in contact with farm animals.
During 2007, two federal bills (S. 549 and H.R. 962) aimed at phasing out "nontherapeutic" antibiotics in U.S. food animal production. The Senate bill, introduced by Sen. Edward "Ted" Kennedy, died. The House bill, introduced by Rep. Louise Slaughter, died after being referred to Committee.
In the United States, the FDA first determined in 1977 that there is evidence of emergence of antibiotic-resistant bacterial strains in livestock. The long-established practice of permitting OTC sales of antibiotics (including penicillin and other drugs) to lay animal owners for administration to their own animals nonetheless continued in all states. In March 2012, the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock that violated FDA regulations. On April 11, 2012 the FDA announced a voluntary program to phase out unsupervised use of drugs as feed additives and convert approved over-the-counter uses for antibiotics to prescription use only, requiring veterinarian supervision of their use and a prescription. In December 2013, the FDA announced the commencement of these steps to phase out the use of antibiotics for the purposes of promoting livestock growth.
Environmental impact.
Antibiotics have been polluting the environment since their introduction through human waste (medication, farming), animals, and the pharmaceutical industry. Along with antibiotic waste, resistant bacteria follow, thus introducing antibiotic-resistant bacteria into the environment. As bacteria replicate quickly, the resistant bacteria that enter the environment replicate their resistance genes as they continue to divide. In addition, bacteria carrying resistance genes have the ability to spread those genes to other species via horizontal gene transfer. Therefore, even if the specific antibiotic is no longer introduced into the environment, antibiotic-resistance genes will persist through the bacteria that have since replicated without continuous exposure.
A study the Poudre River (Colorado, United States) implicated wastewater treatment plants, as well as animal-feeding operations in the dispersal of antibiotic-resistance genes into the environment. This research was done using molecular signatures in order to determine the sources, and the location at the Poudre River was chosen due to lack of other anthropogenic influences upstream. The study indicates that monitoring of antibiotic-resistance genes may be useful in determining not only the point of origin of their release but also how these genes persist in the environment. In addition, studying physical and chemical methods of treatment may alleviate pressure of antibiotic-resistance genes in the environment, and thus their entry back into human contact.
Mechanisms.
Antibiotic resistance can be a result of horizontal gene transfer, and also of unlinked point mutations in the pathogen genome at a rate of about 1 in 108 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure. Those bacteria with a mutation that allows them to survive live to reproduce. They then pass this trait to their offspring, which leads to the evolution of a fully resistant colony.
The four main mechanisms by which microorganisms exhibit resistance to antimicrobials are:
There are three known mechanisms of fluoroquinolone resistance. Some types of efflux pumps can act to decrease intracellular quinolone concentration. In Gram-negative bacteria, plasmid-mediated resistance genes produce proteins that can bind to DNA gyrase, protecting it from the action of quinolones. Finally, mutations at key sites in DNA gyrase or topoisomerase IV can decrease their binding affinity to quinolones, decreasing the drug's effectiveness. Research has shown the bacterial protein LexA may play a key role in the acquisition of bacterial mutations giving resistance to quinolones and rifampicin. DNA damage induces the SOS gene repressor LexA to undergo autoproteolytic activity. This includes the transcription of genes encoding Pol II, Pol IV, and Pol V, which are three nonessential DNA polymerases that are required for mutation in response to DNA damage.
Antibiotic resistance can also be introduced artificially into a microorganism through laboratory protocols, sometimes used as a selectable marker to examine the mechanisms of gene transfer or to identify individuals that absorbed a piece of DNA that included the resistance gene and another gene of interest. A recent study demonstrated that the extent of horizontal gene transfer among "Staphylococcus" is much greater than previously expectedâand encompasses genes with functions beyond antibiotic resistance and virulence, and beyond genes residing within the mobile genetic elements.
For a long time, it has been thought that, for a microorganism to become resistant to an antibiotic, it must be in a large population. However, recent findings show that there is no necessity of large populations of bacteria for the appearance of antibiotic resistance. We know now that small populations of E.coli in an antibiotic gradient can become resistant. Any heterogeneous environment with respect to nutrient and antibiotic gradients may facilitate the development of antibiotic resistance in small bacterial populations and this is also true for the human body. Researchers hypothesize that the mechanism of resistance development is based on four SNP mutations in the genome of E.coli produced by the gradient of antibiotic. These mutations confer the bacteria emergence of antibiotic resistance.
A common misconception is that a person can become resistant to certain antibiotics. It is a strain of microorganism that can become resistant, not a person's body.
Resistant pathogens.
"Staphylococcus aureus".
"Staphylococcus aureus" (colloquially known as "Staph aureus" or a "Staph infection") is one of the major resistant pathogens. Found on the mucous membranes and the human skin of around a third of the population, it is extremely adaptable to antibiotic pressure. It was one of the earlier bacteria in which penicillin resistance was foundâin 1947, just four years after the drug started being mass-produced. Methicillin was then the antibiotic of choice, but has since been replaced by oxacillin due to significant kidney toxicity. Methicillin-resistant "Staphylococcus aureus" (MRSA) was first detected in Britain in 1961, and is now "quite common" in hospitals. MRSA was responsible for 37% of fatal cases of sepsis in the UK in 1999, up from 4% in 1991. Half of all "S. aureus" infections in the US are resistant to penicillin, methicillin, tetracycline and erythromycin.
This left vancomycin as the only effective agent available at the time. However, strains with intermediate (4-8Â Î¼g/ml) levels of resistance, termed glycopeptide-intermediate "Staphylococcus aureus" (GISA) or vancomycin-intermediate "Staphylococcus aureus" (VISA), began appearing in the late 1990s. The first identified case was in Japan in 1996, and strains have since been found in hospitals in England, France and the US. The first documented strain with complete (>16Â Î¼g/ml) resistance to vancomycin, termed vancomycin-resistant "Staphylococcus aureus" (VRSA) appeared in the United States in 2002. However, in 2011, a variant of vancomycin has been tested that binds to the lactate variation and also binds well to the original target, thus reinstating potent antimicrobial activity.
A new class of antibiotics, oxazolidinones, became available in the 1990s, and the first commercially available oxazolidinone, linezolid, is comparable to vancomycin in effectiveness against MRSA. Linezolid-resistance in "S. aureus" was reported in 2001.
Community-acquired MRSA (CA-MRSA) has now emerged as an epidemic that is responsible for rapidly progressive, fatal diseases, including necrotizing pneumonia, severe sepsis, and necrotizing fasciitis. MRSA is the most frequently identified antimicrobial drug-resistant pathogen in US hospitals. The epidemiology of infections caused by MRSA is rapidly changing. In the past 10Â years, infections caused by this organism have emerged in the community. The two MRSA clones in the United States most closely associated with community outbreaks, USA400 (MW2 strain, ST1 lineage) and USA300, often contain Panton-Valentine leukocidin (PVL) genes and, more frequently, have been associated with skin and soft tissue infections. Outbreaks of CA-MRSA infections have been reported in correctional facilities, among athletic teams, among military recruits, in newborn nurseries, and among men that have sex with men. CA-MRSA infections now appear endemic in many urban regions and cause most CA-"S. aureus" infections.
"Streptococcus" and "Enterococcus".
"Streptococcus pyogenes" (Group A "Streptococcus": GAS) infections can usually be treated with many different antibiotics. Early treatment may reduce the risk of death from invasive group A streptococcal disease. However, even the best medical care does not prevent death in every case. For those with very severe illness, supportive care in an intensive-care unit may be needed. For persons with necrotizing fasciitis, surgery often is needed to remove damaged tissue. Strains of "S. pyogenes" resistant to macrolide antibiotics have emerged; however, all strains remain uniformly susceptible to penicillin.
Resistance of "Streptococcus pneumoniae" to penicillin and other beta-lactams is increasing worldwide. The major mechanism of resistance involves the introduction of mutations in genes encoding penicillin-binding proteins. Selective pressure is thought to play an important role, and use of beta-lactam antibiotics has been implicated as a risk factor for infection and colonization. "S. pneumoniae" is responsible for pneumonia, bacteremia, otitis media, meningitis, sinusitis, peritonitis and arthritis.
Multidrug-resistant "Enterococcus faecalis" and "Enterococcus faecium" are associated with nosocomial infections. Among these strains, penicillin-resistant "Enterococcus" was seen in 1983, vancomycin-resistant "Enterococcus" in 1987, and linezolid-resistant "Enterococcus" in the late 1990s.
"Pseudomonas aeruginosa".
"Pseudomonas aeruginosa" is a highly prevalent opportunistic pathogen. One of the most worrisome characteristics of "P. aeruginosa" is its low antibiotic susceptibility, which is attributable to a concerted action of multidrug efflux pumps with chromosomally encoded antibiotic resistance genes (for example, "mexAB-oprM", "mexXY", etc.) and the low permeability of the bacterial cellular envelopes. "Pseudomonas aeruginosa" has the ability to produce 4-hydroxy-2-alkylquinolines (HAQs) and it has been found that HAQs have prooxidant effects, and overexpressing modestly increased susceptibility to antibiotics.
The study experimented with the "Pseudomonas aeruginosa" biofilms and found that a disruption of relA and spoT genes produced an inactivation of the Stringent response (SR) in cells with nutrient limitation, which provides cells be more susceptible to antibiotics.
"Clostridium difficile".
"Clostridium difficile" is a nosocomial pathogen that causes diarrheal disease in hospitals world wide.
"C. difficile" colitis is most strongly associated with fluoroquinolones, cephalosporins, carbapenems, and clindamycin. The European Center for Disease Prevention and Control recommend that fluoroquinolones and the antibiotic clindamycin be avoided in clinical practice due to their high association with CDI.
Some research suggests the overuse of antibiotics in the raising of livestock is contributing to outbreaks of bacterial infections such as C. difficile.[16]
Antibiotics, especially those with a broad activity spectrum (such as clindamycin) disrupt normal intestinal flora. This can lead to an overgrowth of C. difficile, which flourishes under these conditions. Pseudomembranous colitis can follow, creating generalized inflammation of the colon and the development of "pseudomembrane", a viscous collection of inflammatory cells, fibrin, and necrotic cells.[4] Clindamycin-resistant "C. difficile" was reported as the causative agent of large outbreaks of diarrheal disease in hospitals in New York, Arizona, Florida and Massachusetts between 1989 and 1992. Geographically dispersed outbreaks of "C. difficile" strains resistant to fluoroquinolone antibiotics, such as ciprofloxacin and levofloxacin, were also reported in North America in 2005.
"Salmonella" and "E. coli".
Infection with "Escherichia coli" and "Salmonella" can result from the consumption of contaminated food and water. Both of these bacteria are well known for causing nosocomial (hospital-linked) infections, and often, these strains found in hospitals are antibiotic resistant due to adaptations to wide spread antibiotic use. When both bacteria are spread, serious health conditions arise. Many people are hospitalized each year after becoming infected, with some dying as a result. Since 1993, some strains of "E. coli" have become resistant to multiple types of fluoroquinolone antibiotics.
"Acinetobacter baumannii".
On November 5, 2004, the Centers for Disease Control and Prevention (CDC) reported an increasing number of "Acinetobacter baumannii" bloodstream infections in patients at military medical facilities in which service members injured in the Iraq/Kuwait region during Operation Iraqi Freedom and in Afghanistan during Operation Enduring Freedom were treated. Most of these showed multidrug resistance (MRAB), with a few isolates resistant to all drugs tested.
"Klebsiella pneumoniae".
Klebsiella pneumoniae carbapenemase (KPC)-producing bacteria are a group of emerging highly drug-resistant Gram-negative bacilli causing infections associated with significant morbidity and mortality whose incidence is rapidly increasing in a variety of clinical settings around the world. "Klebsiella pneumoniae" includes numerous mechanisms for antibiotic resistance, many of which are located on highly mobile genetic elements. Carbapenem antibiotics (heretofore often the treatment of last resort for resistant infections) are generally not effective against KPC-producing organisms.
"Mycobacterium tuberculosis".
Tuberculosis is increasing across the globe, especially in developing countries, over the past few years. TB resistant to antibiotics is called MDR TB (Multidrug Resistant TB). Globally, MDR TB causes 150,000 deaths annually. The rise of the HIV/AIDS epidemic has contributed to this.
TB was considered one of the most prevalent diseases, and did not have a cure until the discovery of Streptomycin by Selman Waksman in 1943. However, the bacteria soon developed resistance. Since then, drugs such as isoniazid and rifampin have been used. M. tuberculosis develops resistance to drugs by spontaneous mutations in its genomes. Resistance to one drug is common, and this is why treatment is usually done with more than one drug. Extensively Drug-Resistant TB (XDR TB) is TB that is also resistant to the second line of drugs.
Resistance of "Mycobacterium tuberculosis" to isoniazid, rifampin, and other common treatments has become an increasingly relevant clinical challenge. (For more on Drug-Resistant TB, visit the Multi-drug-resistant tuberculosis page.)
"Neisseria gonorrhoeae".
Neisseria gonorrhoeae is a sexually transmitted pathogen that can cause pelvic pain, pain on urination, penile, and vaginal discharge, as well as systemic symptoms in human infection. The bacteria was first identified in 1879, although some Biblical scholars believe that references to the disease can be found as early as Parshat Metzora of the Old Testament.
Treatment with penicillin in the 1940s proved helpful, but by the 1970s resistant strains predominated. Resistance to penicillin has developed through two mechanisms: chomasomally mediated resistance (CMRNG) and penicillinase-mediated resistance (PPNG). CMRNG involves stepwise mutation of penA, which codes for the penicilin-binding protein (PBP-2); mtr, which encodes an efflux pump to remove penicilin from the cell; and penB, which encodes the bacterial cell wall porins. PPNG involves the acquisition of a plasmid-borne beta-lactamase.
Fluoroquinolones were a useful next-line treatment until resistance was achieved through efflux pumps and mutations to the gyrA gene, which encodes DNA gyrase. Third-generation cephalosporins have been used to treat gonorrhoea since 2007, although resistant strains have emerged. Strains of Neisseria gonorrhoea have also been found to be resistant to tetracyclines and aminoglycosides. Neisseria gonorrheoea has a high affinity for horizontal gene transfer, and as a result, the existence of any strain resistant to a given drug could spread easily across strains.
Today, injectable ceftriaxone is used, sometimes in combination with azithromycin or doxycycline.
Prevention.
World Health Organization recommendations.
An April 30, 2014, report by the WHO makes the following recommendations for how to tackle antibiotic resistance:
Decreasing antibiotic use.
Rational use of antibiotics may reduce the chances of development of opportunistic infection by antibiotic-resistant bacteria due to dysbacteriosis. The immune systems will cure minor bacterial infections on its own. It is also important to note that antibiotics will not cure viral infections such as colds and the flu, and taking an antibiotic unnecessarily to treat a viral infection can lead to increased resistance. 
It is unclear if rapid viral testing affects antibiotic use in children.
Vaccines do not have the problem of resistance because a vaccine enhances the body's immune system, whereas an antibiotic operates separately from the body's normal defenses. Nevertheless, new strains that escape immunity induced by vaccines may evolve; for example, an updated influenza vaccine is needed each year.
Phage therapy.
Phage therapy, an approach that has been extensively researched and used as a therapeutic agent for over 60Â years, especially in the Soviet Union, represents a potentially significant but currently underdeveloped approach to the treatment of bacterial disease. Phage therapy was widely used in the United States until the discovery of antibiotics, in the early 1940s. Bacteriophages or "phages" are viruses that invade bacterial cells and, in the case of lytic phages, disrupt bacterial metabolism and cause the bacterium to lyse. Phage therapy is the therapeutic use of lytic bacteriophages to treat pathogenic bacterial infections.
Bacteriophage therapy is a potentially important alternative to antibiotics in the current era of multidrug-resistant pathogens. A review of studies that dealt with the therapeutic use of phages from 1966 to 1996 and few latest ongoing phage therapy projects via internet showed: Phages were used topically, orally or systemically in Polish and Soviet studies. The success rate found in these studies was 80â95%, with few gastrointestinal or allergic side-effects. British studies also demonstrated significant efficacy of phages against "Escherichia coli", "Acinetobacter" spp., "Pseudomonas" spp., and "Staphylococcus aureus". US studies dealt with improving the bioavailability of phage. Phage therapy may prove as an important alternative to antibiotics for treating multidrug-resistant pathogens.
Research.
Decreasing antibiotic use.
While theoretically promising, antistaphylococcal vaccines have shown limited efficacy, because of immunological variation between "Staphylococcus" species, and the limited duration of effectiveness of the antibodies produced. Development and testing of more effective vaccines is underway.
The Australian Commonwealth Scientific and Industrial Research Organisation (CSIRO), realizing the need for the reduction of antibiotic use, has been working on two alternatives. One alternative is to prevent diseases by adding cytokines instead of antibiotics to animal feed. These proteins are made in the animal body "naturally" after a disease and are not antibiotics, so they do not contribute to the problem of antibiotic resistance. Furthermore, studies on using cytokines have shown they also enhance the growth of animals like the antibiotics now used, but without the drawbacks of antibiotic use. Cytokines have the potential to achieve the animal growth rates traditionally sought by the use of antibiotics without the contribution of antibiotic resistance associated with the widespread nontherapeutic uses of antibiotics currently used in the food animal production industries. In addition, CSIRO is working on vaccines for diseases.
Development pipeline.
Since the discovery of antibiotics, research and development (R&D) efforts have provided new drugs in time to treat bacteria that became resistant to older antibiotics, but in the 2000s there has been concern that development has slowed enough that seriously ill patients may run out of treatment options. Another concern is that doctors may become reluctant to perform routine surgeries due to the increased risk of harmful infection. Backup treatments can have serious side-effects; for example, treatment of multi-drug-resistant tuberculosis can cause deafness and insanity. The potential crisis at hand is the result of a marked decrease in industry R&D. Poor financial investment in antibiotic research has exacerbated the situation. In 2011, Pfizer, one of the last major pharmaceutical companies developing new antibiotics, shut down its primary research effort, citing poor shareholder returns relative to drugs for chronic illnesses.
In the United States, drug companies and the administration of President Barack Obama have been proposing changing the standards by which the FDA approves antibiotics targeted at resistant organisms. On 12 December 2013, the Antibiotic Development to Advance Patient Treatment (ADAPT) Act of 2013 was introduced in the U.S. Congress. The ADAPT Act aims to fast-track the drug development in order to combat the growing public health threat of 'superbugs'. Under this Act, the FDA can approve antibiotics and antifungals needed for life-threatening infections based on data from smaller clinical trials. The CDC will reinforce the monitoring of the use of antibiotics that treat serious and life-threatening infections and the emerging resistance, and make the data publicly available. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints' is also streamlined to allow the most up-to-date and cutting-edge data available to healthcare professionals under the new Act.
On 18 September 2014 Obama signed an executive order to implement the recommendations proposed in a report by the President's Council of Advisors on Science and Technology (PCAST) which outlines strategies to stream-line clinical trials and speed up the R&D of new antibiotics. Among the proposals:
The executive order also included a $20 million prize to encourage the development of diagnostic tests to identify highly resistant bacterial infections.
The U.S. National Institutes of Health plans to fund a new research network on the issue up to $62Â million from 2013 to 2019. Using authority created by the Pandemic and All Hazards Preparedness Act of 2006, the Biomedical Advanced Research and Development Authority in the U.S. Department of Health and Human Services announced that it will spend between $40Â million and $200Â million in funding for R&D on new antibiotic drugs under development by GlaxoSmithKline. 
Mechanism.
In research published on October 17, 2008 in "Cell", a team of scientists pinpointed the place on bacteria where the antibiotic myxopyronin launches its attack, and why that attack is successful. The myxopyronin binds to and inhibits the crucial bacterial enzyme RNA polymerase. The myxopyronin changes the structure of the switch-2 segment of the enzyme, inhibiting its function of reading and transmitting DNA code. This prevents RNA polymerase from delivering genetic information to the ribosomes, causing the bacteria to die.
In 2012, a team of the University of Leipzig modified a peptide found in honeybees. It is effective against 37 types of bacteria.
One major cause of antibiotic resistance is the increased pumping activity of microbial ABC transporters, which diminishes the effective drug concentration inside the microbial cell. ABC transporter inhibitors that can be used in combination with current antimicrobials are being tested in clinical trials and are available for therapeutic regimens.
Applications.
Antibiotic resistance is an important tool for genetic engineering. By constructing a plasmid that contains an antibiotic-resistance gene as well as the gene being engineered or expressed, a researcher can ensure that, when bacteria replicate, only the copies that carry the plasmid survive. This ensures that the gene being manipulated passes along when the bacteria replicates.
In general, the most commonly used antibiotics in genetic engineering are "older" antibiotics that have largely fallen out of use in clinical practice. These include:
In industry, the use of antibiotic resistance is disfavored, since maintaining bacterial cultures would require feeding them large quantities of antibiotics. Instead, the use of auxotrophic bacterial strains (and function-replacement plasmids) is preferred.

</doc>
<doc id="1915" url="http://en.wikipedia.org/wiki?curid=1915" title="Antigen">
Antigen

In immunology, an antigen (Ag), or antibody generator, is any substance which provokes an adaptive immune response. That is to say, an antigen is a molecule that also induces an immune response in the body. An antigen is often foreign or toxic to the body (for example, a bacterium) which, once in the body, attracts and is bound to a respective and specific antibody. Each antibody is specifically designed to deal with certain antigens because of variation in the antibody's complementarity determining regions (a common analogy used to describe this is the fit between a lock and a key). Paul Ehrlich coined the term antibody (in German "AntikÃ¶rper") in his side-chain theory at the end of 19th century. The term antigen originally came from ANTIbody GENerator (see section History).
The antigen may originate from within the body ("self") or from the external environment ("non-self"). The immune system is usually non-reactive against "self" antigens under normal conditions and is supposed to identify and attack only "non-self" invaders from the outside world or modified/harmful substances present in the body under distressed conditions.
Cells present their antigenic structures to the immune system via a histocompatibility molecule. Depending on the antigen presented and the type of the histocompatibility molecule, several types of immune cells can become activated. Antigen was originally a structural molecule that binds specifically to the antibody, but the term now also refers to any molecule or molecular fragment that can be recognized by highly variable antigen receptors (B-cell receptor or T-cell receptor) of the adaptive immune system. For T-Cell Receptor (TCR) recognition, it must be processed into small fragments inside the cell and presented to a T-cell receptor by major histocompatibility complex (MHC). Antigen by itself is not capable to elicit the immune response without the help of an Immunologic adjuvant. The essential role of the adjuvant component of vaccines in the activation of innate immune system is so-called immunologist's dirty little secret as originally described by Charles Janeway.
An immunogen is in analogy to the antigen a substance (or a mixture of substances) that is able to provoke an immune response if injected to the body. An immunogen is able to initiate an indispensable innate immune response first, later leading to the activation of the adaptive immune response, whereas an antigen is able to bind the highly variable immunoreceptor products (B-cell receptor or T-cell receptor) once these have been produced. The overlapping concepts of immunogenicity and antigenicity are, therefore, subtly different. According to current textbook notions:
Immunogenicity is the ability to induce a humoral and/or cell-mediated immune response
Antigenicity is the ability to combine specifically with the final products of the immune response (i.e. secreted antibodies and/or surface receptors on T-cells). Although all immunogenic molecules are also antigenic, the reverse is not true.
At the molecular level, an antigen can be characterized by its ability to be bound by the variable Fab region of an antibody. Note also that different antibodies have the potential to discriminate between specific epitopes present on the surface of the antigen (as illustrated in the Figure). Hapten is a small molecule that changes the structure of an antigenic epitope. In order to induce an immune response, it has to be attached to a large carrier molecule such as protein. 
Antigens are usually proteins and polysaccharides, less frequently also lipids. This includes parts (coats, capsules, cell walls, flagella, fimbrae, and toxins) of bacteria, viruses, and other microorganisms. Lipids and nucleic acids are antigenic only when combined with proteins and polysaccharides. Non-microbial exogenous (non-self) antigens can include pollen, egg white, and proteins from transplanted tissues and organs or on the surface of transfused blood cells. Vaccines are examples of antigens in an immunogenic form, which are to be intentionally administered to induce the memory function of adaptive immune system toward the antigens of the pathogen invading the recipient.
Origin of the term antigen.
In 1899, Ladislas Deutsch (Laszlo Detre) (1874â1939) named the hypothetical substances halfway between bacterial constituents and antibodies "substances immunogenes ou antigenes" (antigenic or immunogenic substances). He originally believed those substances to be precursors of antibodies, just as zymogen is a precursor of an enzyme. But, by 1903, he understood that an antigen induces the production of immune bodies (antibodies) and wrote that the word "antigen" is a contraction of Antisomatogen(= "ImmunkÃ¶rperbildner"). The Oxford English Dictionary indicates that the logical construction should be "anti(body)-gen".
Origin of antigens.
Antigens can be classified in order of their class.
Exogenous antigens.
Exogenous antigens are antigens that have entered the body from the outside, for example by inhalation, ingestion, or injection. The immune system's response to exogenous antigens is often subclinical. By endocytosis or phagocytosis, exogenous antigens are taken into the antigen-presenting cells (APCs) and processed into fragments. APCs then present the fragments to T helper cells (CD4+) by the use of class II histocompatibility molecules on their surface. Some T cells are specific for the peptide:MHC complex. They become activated and start to secrete cytokines. Cytokines are substances that can activate cytotoxic T lymphocytes (CTL), antibody-secreting B cells, macrophages, and other particles.
Some antigens start out as exogenontigens, and later become endogenous (for example, intracellular viruses). Intracellular antigens can again be released back into circulation upon the destruction of the infected cell.
Endogenous antigens.
Endogenous antigens are antigens that have been generated within previously normal cells as a result of normal cell metabolism, or because of viral or intracellular bacterial infection. The fragments are then presented on the cell surface in the complex with MHC class I molecules. If activated cytotoxic CD8+ T cells recognize them, the T cells begin to secrete various toxins that cause the lysis or apoptosis of the infected cell. In order to keep the cytotoxic cells from killing cells just for presenting self-proteins, self-reactive T cells are deleted from the repertoire as a result of tolerance (also known as negative selection). Endogenous antigens include xenogenic (heterologous), autologous and idiotypic or allogenic (homologous) antigens.
Autoantigens.
An autoantigen is usually a normal protein or complex of proteins (and sometimes DNA or RNA) that is recognized by the immune system of patients suffering from a specific autoimmune disease. These antigens should not be, under normal conditions, the target of the immune system, but, due mainly to genetic and environmental factors, the normal immunological tolerance for such an antigen has been lost in these patients.
Tumor antigens.
"Tumor antigens" or "neoantigens" are those antigens that are presented by MHC I or MHC II molecules on the surface of tumor cells. These antigens can sometimes be presented by tumor cells and never by the normal ones. In this case, they are called tumor-specific antigens (TSAs) and, in general, result from a tumor-specific mutation. More common are antigens that are presented by tumor cells and normal cells, and they are called tumor-associated antigens (TAAs). Cytotoxic T lymphocytes that recognize these antigens may be able to destroy the tumor cells before they proliferate or metastasize.
Tumor antigens can also be on the surface of the tumor in the form of, for example, a mutated receptor, in which case they will be recognized by B cells.
Nativity.
A native antigen is an antigen that is not yet processed by an APC to smaller parts. T cells cannot bind native antigens, but require that they be processed by APCs, whereas B cells can be activated by native ones.
Antigenic specificity.
Antigen(ic) specificity is the ability of the host cells to recognize an antigen specifically as a unique molecular entity and distinguish it from another with exquisite precision. Antigen specificity is due primarily to the side-chain conformations of the antigen. It is a measurement, although the degree of specificity may not be easy to measure, and need not be linear or of the nature of a rate-limited step or equation.

</doc>
<doc id="1916" url="http://en.wikipedia.org/wiki?curid=1916" title="Autosome">
Autosome

An autosome is a chromosome that is not an allosome (i.e., not a sex chromosome). Autosomes appear in pairs whose members have the same form but differ from other pairs in a diploid cell, whereas members of an allosome pair may differ from one another and thereby determine sex. The DNA in autosomes is collectively known as atDNA or auDNA.
For example, humans have a diploid genome that usually contains 22 pairs of autosomes and one allosome pair (46 chromosomes total). The autosome pairs are labeled with numbers (1-22 in humans) roughly in order of their sizes in base pairs, while allosomes are labeled with their letters. By contrast, the allosome pair consists of two X chromosomes in females or one X and one Y chromosome in males. (Unusual combinations of XYY, XXY, XXX, XXXX, XXXXX or XXYY, among other allosome combinations, are known to occur and usually cause developmental abnormalities.) 

</doc>
<doc id="1919" url="http://en.wikipedia.org/wiki?curid=1919" title="Antwerp (disambiguation)">
Antwerp (disambiguation)

Antwerp is the name of a city, a district and a province in Flanders, Belgium:
Antwerp is also the name of a number of places 

</doc>
<doc id="1920" url="http://en.wikipedia.org/wiki?curid=1920" title="Aquila">
Aquila

Aquila is the Latin and Romance languages word for "eagle". Specifically, it may refer to:

</doc>
<doc id="1921" url="http://en.wikipedia.org/wiki?curid=1921" title="Al-Qaeda">
Al-Qaeda

al-Qaeda ( ; ', , translation: "The Base" and alternatively spelled al-Qaida and sometimes al-Qa'ida"') is a global militant Islamist organization founded by Osama bin Laden, Abdullah Azzam, and several other militants, at some point between August 1988 and late 1989, with origins traceable to the Soviet war in Afghanistan. It operates as a network comprising both a multinational, stateless army and a radical Wahhabi Muslim movement calling for a strict interpretation of sharia law and jihad, the struggle towards Islamic ideals, at a global scale. It has been designated as a terrorist organization by the United Nations Security Council, NATO, the European Union, the United States, Russia, India and various other countries (see below). Al-Qaeda has carried out many attacks on targets it considers "kafir". Amidst the Syrian civil war, al-Qaeda factions started fighting each other, as well as the Kurds and government.
al-Qaeda has attacked civilian and military targets in various countries, including the September 11 attacks, 1998 US embassy bombings and the 2002 Bali bombings. The US government responded to the September 11 attacks by launching the War on Terror. With the loss of key leaders, culminating in the death of Osama bin Laden, al-Qaeda's operations have devolved from actions that were controlled from the top down, to actions by franchise associated groups, to actions of lone wolf operators.
Characteristic techniques employed by al-Qaeda include suicide attacks and simultaneous bombings of different targets. Activities ascribed to it may involve members of the movement, who have taken a pledge of loyalty to Osama bin Laden, or the much more numerous "al-Qaeda-linked" individuals who have undergone training in one of its camps in Afghanistan, Pakistan, Iraq or Sudan, but who have not taken any pledge. al-Qaeda ideologues envision a complete break from all foreign influences in Muslim countries, and the creation of a new world-wide Islamic caliphate. Among the beliefs ascribed to al-Qaeda members is the conviction that a ChristianâJewish alliance is conspiring to destroy Islam. As Salafist jihadists, they believe that the killing of civilians is religiously sanctioned, and they ignore any aspect of religious scripture which might be interpreted as forbidding the murder of civilians and internecine fighting. Al-Qaeda also opposes man-made laws, and wants to replace them with a strict form of sharia law.
al-Qaeda is also responsible for instigating sectarian violence among Muslims. al-Qaeda leaders regard liberal Muslims, Shias, Sufis and other sects as heretics and have attacked their mosques and gatherings. Examples of sectarian attacks include the Yazidi community bombings, the Sadr City bombings, the Ashoura Massacre and the April 2007 Baghdad bombings. The group is led by the Egyptian theologian Ayman al-Zawahiri.
Organization.
al-Qaeda's management philosophy has been described as "centralization of decision and decentralization of execution." It is thought that al-Qaeda's leadership, following the War on Terror, has "become geographically isolated," leading to the "emergence of decentralized leadership" of regional groups using the al-Qaeda "brand".
Many terrorism experts do not believe that the global jihadist movement is driven at every level by al-Qaeda's leadership. Although bin Laden still held considerable ideological sway over some Muslim extremists before his death, experts argue that al-Qaeda has fragmented over the years into a variety of regional movements that have little connection with one another. Marc Sageman, a psychiatrist and former Central Intelligence Agency (CIA) officer, said that al-Qaeda is now just a "loose label for a movement that seems to target the West." "There is no umbrella organisation. We like to create a mythical entity called [al-Qaeda] in our minds, but that is not the reality we are dealing with."
This view mirrors the account given by Osama bin Laden in his October 2001 interview with Tayseer Allouni:
"... this matter isn't about any specific person and... is not about the al-Qa'idah Organization. We are the children of an Islamic Nation, with Prophet Muhammad as its leader, our Lord is one... and all the true believers [mu'mineen] are brothers. So the situation isn't like the West portrays it, that there is an 'organization' with a specific name (such as 'al-Qa'idah') and so on. That particular name is very old. It was born without any intention from us. Brother Abu Ubaida... created a military base to train the young men to fight against the vicious, arrogant, brutal, terrorizing Soviet empire... So this place was called 'The Base' ['Al-Qa'idah'], as in a training base, so this name grew and became. We aren't separated from this nation. We are the children of a nation, and we are an inseparable part of it, and from those public *** which spread from the far east, from the Philippines, to Indonesia, to Malaysia, to India, to Pakistan, reaching Mauritania... and so we discuss the conscience of this nation."
Others, however, see al-Qaeda as an integrated network that is strongly led from the Pakistani tribal areas and has a powerful strategic purpose. Bruce Hoffman, a terrorism expert at Georgetown University, said "It amazes me that people don't think there is a clear adversary out there, and that our adversary does not have a strategic approach."
al-Qaeda has the following direct affiliates:
al-Qaeda has the following indirect affiliates:
Leadership.
Information mostly acquired from Jamal al-Fadl provided American authorities with a rough picture of how the group was organized. While the veracity of the information provided by al-Fadl and the motivation for his cooperation are both disputed, American authorities base much of their current knowledge of al-Qaeda on his testimony.
Osama bin Laden was the most historically notable emir, or commander, and Senior Operations Chief of al-Qaeda prior to his assassination on May 1, 2011 by US forces. Ayman al-Zawahiri, al-Qaeda's Deputy Operations Chief prior to bin Laden's death, assumed the role of commander, according to an announcement by al-Qaeda on June 16, 2011. He replaced Saif al-Adel, who had served as interim commander.
Bin Laden was advised by a Shura Council, which consists of senior al-Qaeda members, estimated by Western officials to consist of 20â30 people.
Atiyah Abd al-Rahman was alleged to be second in command prior to his death on August 22, 2011.
On June 5, 2012, Pakistan intelligence officials announced that al-Rahman's alleged successor Abu Yahya al-Libi had been killed in Pakistan.
al-Qaeda's network was built from scratch as a conspiratorial network that draws on leaders of all its regional nodes "as and when necessary to serve as an integral part of its high command."
Command structure.
When asked about the possibility of al-Qaeda's connection to the July 7, 2005 London bombings in 2005, Metropolitan Police Commissioner Sir Ian Blair said: "Al-Qaeda is not an organization. Al-Qaeda is a way of working... but this has the hallmark of that approach... al-Qaeda clearly has the ability to provide training... to provide expertise... and I think that is what has occurred here."
On August 13, 2005, however, "The Independent" newspaper, quoting police and MI5 investigations, reported that the July 7 bombers had acted independently of an al-Qaeda terror mastermind someplace abroad.
What exactly al-Qaeda is, or was, remains in dispute. Certainly, it has been obliged to evolve and adapt in the aftermath of 9/11 and the launch of the 'war on terror'.
Nasser al-Bahri, who was Osama bin Laden's bodyguard for four years in the run-up to 9/11 gives a highly detailed description of how the organization functioned at that time in his memoir. He describes its formal administrative structure and vast arsenal, as well as day-to-day life as a member.
However, author and journalist Adam Curtis argues that the idea of al-Qaeda as a formal organization is primarily an American invention. Curtis contends the name "al-Qaeda" was first brought to the attention of the public in the 2001 trial of bin Laden and the four men accused of the 1998 US embassy bombings in East Africa:
The reality was that bin Laden and Ayman al-Zawahiri had become the focus of a loose association of disillusioned Islamist militants who were attracted by the new strategy. But there was no organization. These were militants who mostly planned their own operations and looked to bin Laden for funding and assistance. He was not their commander. There is also no evidence that bin Laden used the term "al-Qaeda" to refer to the name of a group until after September 11 attacks, when he realized that this was the term the Americans had given it.
As a matter of law, the US Department of Justice needed to show that bin Laden was the leader of a criminal organization in order to charge him "in absentia" under the Racketeer Influenced and Corrupt Organizations Act, also known as the RICO statutes. The name of the organization and details of its structure were provided in the testimony of Jamal al-Fadl, who said he was a founding member of the organization and a former employee of bin Laden. Questions about the reliability of al-Fadl's testimony have been raised by a number of sources because of his history of dishonesty, and because he was delivering it as part of a plea bargain agreement after being convicted of conspiring to attack U.S. military establishments. Sam Schmidt, one of his defense lawyers, said:
There were selective portions of al-Fadl's testimony that I believe was false, to help support the picture that he helped the Americans join together. I think he lied in a number of specific testimony about a unified image of what this organization was. It made al-Qaeda the new Mafia or the new Communists. It made them identifiable as a group and therefore made it easier to prosecute any person associated with al-Qaeda for any acts or statements made by bin Laden.
Field operatives.
The number of individuals in the organization who have undergone proper military training, and are capable of commanding insurgent forces, is largely unknown. Documents captured in the raid on bin Laden compound in 2011, show that the core al-Qaeda membership in 2002 was 170. In 2006, it was estimated that al-Qaeda had several thousand commanders embedded in 40 different countries. As of 2009, it was believed that no more than 200â300 members were still active commanders.
According to the award-winning 2004 BBC documentary "The Power of Nightmares", al-Qaeda was so weakly linked together that it was hard to say it existed apart from bin Laden and a small clique of close associates. The lack of any significant numbers of convicted al-Qaeda members, despite a large number of arrests on terrorism charges, was cited by the documentary as a reason to doubt whether a widespread entity that met the description of al-Qaeda existed.
Insurgent forces.
According to Robert Cassidy, al-Qaeda controls two separate forces deployed alongside insurgents in Iraq and Pakistan. The first, numbering in the tens of thousands, was "organized, trained, and equipped as insurgent combat forces" in the Soviet-Afghan war. It was made up primarily of foreign "mujahideen" from Saudi Arabia and Yemen. Many went on to fight in Bosnia and Somalia for global "jihad". Another group, approximately 10,000 strong, live in Western states and have received rudimentary combat training.
Other analysts have described al-Qaeda's rank and file as being "predominantly Arab," in its first years of operation, and now also includes "other peoples" as of 2007. It has been estimated that 62% of al-Qaeda members have university education.
Financing.
Some financing for al-Qaeda in the 1990s came from the personal wealth of Osama bin Laden. By 2001 Afghanistan had become politically complex and mired. With many financial sources for al-Qaeda, bin Laden's financing role may have become comparatively minor. Sources in 2001 could also have included Jamaa Al-Islamiyya and Islamic Jihad, both associated with Afghan-based Egyptians. Other sources of income in 2001 included the heroin trade and donations from supporters in Kuwait, Saudi Arabia and other Islamic countries. A WikiLeaks released memo from the United States Secretary of State sent in 2009 asserted that the primary source of funding of Sunni terrorist groups worldwide was Saudi Arabia.
Strategy.
On March 11, 2005, "Al-Quds Al-Arabi" published extracts from Saif al-Adel's document "Al Qaeda's Strategy to the Year 2020". Abdel Bari Atwan summarizes this strategy as comprising five stages to rid the Ummah from all forms of oppression:
Atwan also noted, regarding the collapse of the U.S., "If this sounds far-fetched, it is sobering to consider that this virtually describes the downfall of the Soviet Union."
Name.
In Arabic, "al-Qaeda" has four syllables ("", or ). However, since two of the Arabic consonants in the name (the voiceless uvular plosive and the voiced pharyngeal fricative ) are not phones found in the English language, the closest naturalized English pronunciations include , and . al-Qaeda's name can also be transliterated as al-Qaida, al-Qa'ida, el-Qaida, or al-Qaeda.
The name comes from the Arabic noun "qÄ'idah", which means "foundation" or "basis", and can also refer to a military base. The initial "al-" is the Arabic definite article "the", hence "the base".
Bin Laden explained the origin of the term in a videotaped interview with Al Jazeera journalist Tayseer Alouni in October 2001:
The name 'al-Qaeda' was established a long time ago by mere chance. The late Abu Ebeida El-Banashiri established the training camps for our "mujahedeen" against Russia's terrorism. We used to call the training camp al-Qaeda. The name stayed.
It has been argued that two documents seized from the Sarajevo office of the Benevolence International Foundation prove that the name was not simply adopted by the "mujahid" movement and that a group called al-Qaeda was established in August 1988. Both of these documents contain minutes of meetings held to establish a new military group, and contain the term "al-Qaeda".
Former British Foreign Secretary Robin Cook wrote that the word al-Qaeda should be translated as "the database", and originally referred to the computer file of the thousands of "mujahideen" militants who were recruited and trained with CIA help to defeat the Russians. In April 2002, the group assumed the name "Qa'idat al-Jihad", which means "the base of Jihad". According to Diaa Rashwan, this was "apparently as a result of the merger of the overseas branch of Egypt's al-Jihad (Egyptian Islamist Jihad, or EIJ) group, led by Ayman al-Zawahiri, with the groups Bin Laden brought under his control after his return to Afghanistan in the mid-1990s."
Ideology.
The radical Islamist movement in general and al-Qaeda in particular developed during the Islamic revival and Islamist movement of the last three decades of the 20th century, along with less extreme movements.
Some have argued that "without the writings" of Islamic author and thinker Sayyid Qutb, "al-Qaeda would not have existed." Qutb preached that because of the lack of "sharia" law, the Muslim world was no longer Muslim, having reverted to pre-Islamic ignorance known as "jahiliyyah".
To restore Islam, he said a vanguard movement of righteous Muslims was needed to establish "true Islamic states", implement "sharia", and rid the Muslim world of any non-Muslim influences, such as concepts like socialism and nationalism. Enemies of Islam in Qutb's view included "treacherous Orientalists" and "world Jewry," who plotted "conspiracies" and "wicked[ly]" opposed Islam.
In the words of Mohammed Jamal Khalifa, a close college friend of bin Laden: Islam is different from any other religion; it's a way of life. We [Khalifa and bin Laden] were trying to understand what Islam has to say about how we eat, who we marry, how we talk. We read Sayyid Qutb. He was the one who most affected our generation.
Qutb had an even greater influence on bin Laden's mentor and another leading member of al-Qaeda, Ayman al-Zawahiri. Zawahiri's uncle and maternal family patriarch, Mafouz Azzam, was Qutb's student, then protÃ©gÃ©, then personal lawyer, and finally executor of his estateâone of the last people to see Qutb before his execution. "Young Ayman al-Zawahiri heard again and again from his beloved uncle Mahfouz about the purity of Qutb's character and the torment he had endured in prison." Zawahiri paid homage to Qutb in his work "Knights under the Prophet's Banner."
One of the most powerful of Qutb's ideas was that many who said they were Muslims were not. Rather, they were apostates. That not only gave jihadists "a legal loophole around the prohibition of killing another Muslim," but made "it a religious obligation to execute" these self-professed Muslims. These alleged apostates included leaders of Muslim countries, since they failed to enforce "sharia" law.
Religious compatibility.
Abdel Bari Atwan writes that:
History.
"The Guardian" has described five distinct phases in the development of al-Qaeda: beginnings in the late 1980s, a "wilderness" period in 1990â96, its "heyday" in 1996â2001, a network period from 2001 to 2005, and a period of fragmentation from 2005 to today.
Jihad in Afghanistan.
The origins of al-Qaeda as a network inspiring terrorism around the world and training operatives can be traced to the Soviet War in Afghanistan (December 1979 â February 1989). The US viewed the conflict in Afghanistan, with the Afghan Marxists and allied Soviet troops on one side and the native Afghan "mujahideen", some of whom were radical Islamic militants, on the other, as a blatant case of Soviet expansionism and aggression. A CIA program called Operation Cyclone channeled funds through Pakistan's Inter-Services Intelligence agency to the Afghan Mujahideen who were fighting the Soviet occupation.
At the same time, a growing number of Arab "mujahideen" joined the "jihad" against the Afghan Marxist regime, facilitated by international Muslim organizations, particularly the Maktab al-Khidamat, which was funded by the Saudi Arabia government as well as by individual Muslims (particularly Saudi businessmen who were approached by bin Laden). Together, these sources donated some $600Â million a year to jihad.
In 1984, Maktab al-Khidamat (MAK), or the "Services Office", a Muslim organization founded to raise and channel funds and recruit foreign "mujahideen" for the war against the Soviets in Afghanistan, was established in Peshawar, Pakistan, by bin Laden and Abdullah Yusuf Azzam, a Palestinian Islamic scholar and member of the Muslim Brotherhood. MAK organized guest houses in Peshawar, near the Afghan border, and gathered supplies for the construction of paramilitary training camps to prepare foreign recruits for the Afghan war front. Bin Laden became a "major financier" of the "mujahideen", spending his own money and using his connections with "the Saudi royal family and the petro-billionaires of the Gulf" to influence public opinion about the war and raise additional funds.
From 1986, MAK began to set up a network of recruiting offices in the US, the hub of which was the Al Kifah Refugee Center at the Farouq Mosque on Brooklyn's Atlantic Avenue. Among notable figures at the Brooklyn center were "double agent" Ali Mohamed, whom FBI special agent Jack Cloonan called "bin Laden's first trainer," and "Blind Sheikh" Omar Abdel-Rahman, a leading recruiter of "mujahideen" for Afghanistan. Al-Qaeda evolved from MAK.
Azzam and bin Laden began to establish camps in Afghanistan in 1987.
US government financial support for the Afghan Islamic militants was substantial. Aid to Gulbuddin Hekmatyar, an Afghan "mujahideen" leader and founder and leader of the Hezb-e Islami radical Islamic militant faction, alone amounted "by the most conservative estimates" to $600Â million. Later, in the early 1990s, after the US had withdrawn support, Hekmatyar "worked closely" with bin Laden. In addition to receiving hundreds of millions of dollars in American aid, Hekmatyar was the recipient of the lion's share of Saudi aid. There is evidence that the CIA supported Hekmatyar's drug trade activities by giving him immunity for his opium trafficking, which financed the operation of his militant faction.
MAK and foreign "mujahideen" volunteers, or "Afghan Arabs," did not play a major role in the war. While over 250,000 Afghan "mujahideen" fought the Soviets and the communist Afghan government, it is estimated that were never more than 2,000 foreign "mujahideen" in the field at any one time. Nonetheless, foreign "mujahideen" volunteers came from 43 countries, and the total number that participated in the Afghan movement between 1982 and 1992 is reported to have been 35,000. Bin Laden played a central role in organizing training camps for the foreign Muslim volunteers.
The Soviet Union finally withdrew from Afghanistan in 1989. To the surprise of many, Mohammad Najibullah's communist Afghan government hung on for three more years, before being overrun by elements of the "mujahideen". With "mujahideen" leaders unable to agree on a structure for governance, chaos ensued, with constantly reorganizing alliances fighting for control of ill-defined territories, leaving the country devastated.
Expanding operations.
Toward the end of the Soviet military mission in Afghanistan, some "mujahideen" wanted to expand their operations to include Islamist struggles in other parts of the world, such as Israel and Kashmir. A number of overlapping and interrelated organizations were formed, to further those aspirations.
One of these was the organization that would eventually be called al-Qaeda, formed by bin Laden with an initial meeting held on August 11, 1988.
Notes of a meeting of bin Laden and others on August 20, 1988, indicate al-Qaeda was a formal group by that time: "basically an organized Islamic faction, its goal is to lift the word of God, to make His religion victorious." A list of requirements for membership itemized the following: listening ability, good manners, obedience, and making a pledge ("bayat") to follow one's superiors.
In his memoir, bin Laden's former bodyguard, Nasser al-Bahri, gives the only publicly available description of the ritual of giving "bayat" when he swore his allegiance to the al-Qaeda chief.
According to Wright, the group's real name wasn't used in public pronouncements because "its existence was still a closely held secret." His research suggests that al-Qaeda was formed at an August 11, 1988, meeting between "several senior leaders" of Egyptian Islamic Jihad, Abdullah Azzam, and bin Laden, where it was agreed to join bin Laden's money with the expertise of the Islamic Jihad organization and take up the jihadist cause elsewhere after the Soviets withdrew from Afghanistan.
Bin Laden wished to establish non-military operations in other parts of the world; Azzam, in contrast, wanted to remain focused on military campaigns. After Azzam was assassinated in 1989, the MAK split, with a significant number joining bin Laden's organization.
In November 1989, Ali Mohamed, a former special forces Sergeant stationed at Fort Bragg, North Carolina, left military service and moved to California. He traveled to Afghanistan and Pakistan and became "deeply involved with bin Laden's plans."
A year later, on November 8, 1990, the FBI raided the New Jersey home of Ali Mohammed's associate El Sayyid Nosair, discovering a great deal of evidence of terrorist plots, including plans to blow up New York City skyscrapers. Nosair was eventually convicted in connection to the 1993 World Trade Center bombing. In 1991, Ali Mohammed is said to have helped orchestrate bin Laden's relocation to Sudan.
Gulf War and the start of US enmity.
Following the Soviet Union's withdrawal from Afghanistan in February 1989, bin Laden returned to Saudi Arabia. The Iraqi invasion of Kuwait in August 1990 had put the Kingdom and its ruling House of Saud at risk. The world's most valuable oil fields were within easy striking distance of Iraqi forces in Kuwait, and Saddam's call to pan-Arab/Islamism could potentially rally internal dissent.
In the face of a seemingly massive Iraqi military presence, Saudi Arabia's own forces were well armed but far outnumbered. Bin Laden offered the services of his "mujahideen" to King Fahd to protect Saudi Arabia from the Iraqi army. The Saudi monarch refused bin Laden's offer, opting instead to allow US and allied forces to deploy troops into Saudi territory.
The deployment angered bin Laden, as he believed the presence of foreign troops in the "land of the two mosques" (Mecca and Medina) profaned sacred soil. After speaking publicly against the Saudi government for harboring American troops, he was banished and forced to live in exile in Sudan.
Sudan.
From around 1992 to 1996, al-Qaeda and bin Laden based themselves in Sudan at the invitation of Islamist theoretician Hassan al-Turabi. The move followed an Islamist coup d'Ã©tat in Sudan, led by Colonel Omar al-Bashir, who professed a commitment to reordering Muslim political values. During this time, bin Laden assisted the Sudanese government, bought or set up various business enterprises, and established camps where insurgents trained.
A key turning point for bin Laden, further pitting him against the Sauds, occurred in 1993 when Saudi Arabia gave support for the Oslo Accords, which set a path for peace between Israel and Palestinians.
Zawahiri and the EIJ, who served as the core of al-Qaeda but also engaged in separate operations against the Egyptian government, had bad luck in Sudan. In 1993, a young schoolgirl was killed in an unsuccessful EIJ attempt on the life of the Egyptian prime minister, Atef Sedki. Egyptian public opinion turned against Islamist bombings, and the police arrested 280 of al-Jihad's members and executed 6.
Due to bin Laden's continuous verbal assault on King Fahd of Saudi Arabia, on March 5, 1994 Fahd sent an emissary to Sudan demanding bin Laden's passport; bin Laden's Saudi citizenship was also revoked. His family was persuaded to cut off his monthly stipend, $7Â million ($ today) a year, and his Saudi assets were frozen. His family publicly disowned him. There is controversy over whether and to what extent he continued to garner support from members of his family and/or the Saudi government.
In June 1995, an even more ill-fated attempt to assassinate Egyptian president Mubarak led to the expulsion of EIJ, and in May 1996, of bin Laden, by the Sudanese government.
According to Pakistani-American businessman Mansoor Ijaz, the Sudanese government offered the Clinton Administration numerous opportunities to arrest bin Laden. Those opportunities were met positively by Secretary of State Madeleine Albright, but spurned when Susan Rice and counter-terrorism czar Richard Clarke persuaded National Security Advisor Sandy Berger to overrule Albright. Ijaz's claims appeared in numerous Op-Ed pieces, including one in the "Los Angeles Times" and one in "The Washington Post" co-written with former Ambassador to Sudan Timothy M. Carney. Similar allegations have been made by "Vanity Fair" contributing editor David Rose, and Richard Miniter, author of "Losing bin Laden", in a November 2003 interview with "World".
Several sources dispute Ijaz's claim, including the National Commission on Terrorist Attacks on the US (the 9â11 Commission), which concluded in part: Sudan's minister of defense, Fatih Erwa, has claimed that Sudan offered to hand Bin Ladin over to the US The Commission has found no credible evidence that this was so. Ambassador Carney had instructions only to push the Sudanese to expel Bin Ladin. Ambassador Carney had no legal basis to ask for more from the Sudanese since, at the time, there was no indictment out-standing.
Refuge in Afghanistan.
After the Soviet withdrawal, Afghanistan was effectively ungoverned for seven years and plagued by constant infighting between former allies and various "mujahideen" groups.
Throughout the 1990s, a new force began to emerge. The origins of the Taliban (literally "students") lay in the children of Afghanistan, many of them orphaned by the war, and many of whom had been educated in the rapidly expanding network of Islamic schools (madrassas) either in Kandahar or in the refugee camps on the Afghan-Pakistani border.
According to Ahmed Rashid, five leaders of the Taliban were graduates of Darul Uloom Haqqania, a madrassa in the small town of Akora Khattak. The town is situated near Peshawar in Pakistan, but largely attended by Afghan refugees. This institution reflected Salafi beliefs in its teachings, and much of its funding came from private donations from wealthy Arabs. Bin Laden's contacts were still laundering most of these donations, using "unscrupulous" Islamic banks to transfer the money to an "array" of charities which serve as front groups for al-Qaeda, or transporting cash-filled suitcases straight into Pakistan. Another four of the Taliban's leaders attended a similarly funded and influenced madrassa in Kandahar.
Many of the "mujahideen" who later joined the Taliban fought alongside Afghan warlord Mohammad Nabi Mohammadi's Harkat i Inqilabi group at the time of the Russian invasion. This group also enjoyed the loyalty of most Afghan Arab fighters.
The continuing internecine strife between various factions, and accompanying lawlessness following the Soviet withdrawal, enabled the growing and well-disciplined Taliban to expand their control over territory in Afghanistan, and it came to establish an enclave which it called the Islamic Emirate of Afghanistan. In 1994, it captured the regional center of Kandahar, and after making rapid territorial gains thereafter, conquered the capital city Kabul in September 1996.
After the Sudanese made it clear, in May 1996, that bin Laden would never be welcome to return, Taliban-controlled Afghanistanâwith previously established connections between the groups, administered with a shared militancy, and largely isolated from American political influence and military powerâprovided a perfect location for al-Qaeda to relocate its headquarters. Al-Qaeda enjoyed the Taliban's protection and a measure of legitimacy as part of their Ministry of Defense, although only Pakistan, Saudi Arabia, and the United Arab Emirates recognized the Taliban as the legitimate government of Afghanistan.
While in Afghanistan, the Taliban government tasked al-Qaeda with the training of Brigade 055, an elite part of the Taliban's army from 1997â2001. The Brigade was made up of mostly foreign fighters, many veterans from the Soviet Invasion, and all under the same basic ideology of the mujahideen. In November 2001, as Operation Enduring Freedom had toppled the Taliban government, many Brigade 055 fighters were captured or killed, and those that survived were thought to head into Pakistan along with bin Laden.
By the end of 2008, some sources reported that the Taliban had severed any remaining ties with al-Qaeda, while others cast doubt on this. According to senior US military intelligence officials, there were fewer than 100 members of al-Qaeda remaining in Afghanistan in 2009.
Call for global jihad.
Around 1994, the Salafi groups waging "jihad" in Bosnia entered into a seemingly irreversible decline. As they grew less and less aggressive, groups such as EIJ began to drift away from the Salafi cause in Europe. Al-Qaeda decided to step in and assumed control of around 80% of the terrorist cells in Bosnia in late 1995.
At the same time, al-Qaeda ideologues instructed the network's recruiters to look for "Jihadi international", Muslims who believed that "jihad" must be fought on a global level. The concept of a "global Salafi "jihad"" had been around since at least the early 1980s. Several groups had formed for the explicit purpose of driving non-Muslims out of every Muslim land, at the same time, and with maximum carnage. This was, however, a fundamentally defensive strategy.
Al-Qaeda sought to open the "offensive phase" of the global Salafi "jihad". Bosnian Islamists in 2006 called for "solidarity with Islamic causes around the world", supporting the insurgents in Kashmir and Iraq as well as the groups fighting for a Palestinian state.
Fatwas.
In 1996, al-Qaeda announced its "jihad" to expel foreign troops and interests from what they considered Islamic lands. Bin Laden issued a "fatwa" (binding religious edict), which amounted to a public declaration of war against the US and its allies, and began to refocus al-Qaeda's resources on large-scale, propagandist strikes.
On February 23, 1998, bin Laden and Ayman al-Zawahiri, a leader of Egyptian Islamic Jihad, along with three other Islamist leaders, co-signed and issued a "fatwa" calling on Muslims to kill Americans and their allies where they can, when they can. Under the banner of the World Islamic Front for Combat Against the Jews and Crusaders, they declared:
[T]he ruling to kill the Americans and their alliesâcivilians and militaryâis an individual duty for every Muslim who can do it in any country in which it is possible to do it, in order to liberate the al-Aqsa Mosque [in Jerusalem] and the holy mosque [in Mecca] from their grip, and in order for their armies to move out of all the lands of Islam, defeated and unable to threaten any Muslim. This is in accordance with the words of Almighty Allah, 'and fight the pagans all together as they fight you all together,' and 'fight them until there is no more tumult or oppression, and there prevail justice and faith in Allah'.
Neither bin Laden nor al-Zawahiri possessed the traditional Islamic scholarly qualifications to issue a "fatwa". However, they rejected the authority of the contemporary "ulema" (which they saw as the paid servants of "jahiliyya" rulers), and took it upon themselves. Former Russian FSB agent Alexander Litvinenko, who was later killed, said that the FSB trained al-Zawahiri in a camp in Dagestan eight months before the 1998 "fatwa".
Iraq.
Al-Qaeda is Sunni, and often attacked the Iraqi Shia majority in an attempt to incite sectarian violence and greater chaos in the country. Al-Zarqawi purportedly declared an all-out war on Shiites while claiming responsibility for Shiite mosque bombings. The same month, a statement claiming to be by AQI rejected as "fake" a letter allegedly written by al-Zawahiri, in which he appears to question the insurgents' tactic of indiscriminately attacking Shiites in Iraq. In a December 2007 video, al-Zawahiri defended the Islamic State in Iraq, but distanced himself from the attacks against civilians committed by "hypocrites and traitors existing among the ranks".
US and Iraqi officials accused AQI of trying to slide Iraq into a full-scale civil war between Iraq's majority Shiites and minority Sunni Arabs, with an orchestrated campaign of civilian massacres and a number of provocative attacks against high-profile religious targets. With attacks such as the 2003 Imam Ali Mosque bombing, the 2004 Day of Ashura and Karbala and Najaf bombings, the 2006 first al-Askari Mosque bombing in Samarra, the deadly single-day series of bombings in which at least 215 people were killed in Baghdad's Shiite district of Sadr City, and the second al-Askari bombing in 2007, they provoked Shiite militias to unleash a wave of retaliatory attacks, resulting in death squad-style killings and spiraling further sectarian violence which escalated in 2006 and brought Iraq to the brink of violent anarchy in 2007. In 2008, sectarian bombings blamed on al-Qaeda in Iraq killed at least 42 people at the Imam Husayn Shrine in Karbala in March, and at least 51 people at a bus stop in Baghdad in June.
In February 2014, after a prolonged dispute with al-Qaeda in Iraq's successor organisation, the Islamic State of Iraq and the Levant (ISIS), al-Qaeda publicly announced it was cutting all ties with the group, reportedly for its brutality and "notorious intractability".
Somalia and Yemen.
In Somalia, al-Qaeda agents had been collaborating closely with its Somali wing, which was created from the al-Shabaab group. In February 2012, al-Shabaab officially joined al-Qaeda, declaring loyalty in a joint video. The Somalian al-Qaeda actively recruit children for suicide-bomber training, and export young people to participate in military actions against Americans at the AfPak border.
The percentage of terrorist attacks in the West originating from the Afghanistan-Pakistan (AfPak) border declined considerably from almost 100% to 75% in 2007, and to 50% in 2010, as al-Qaeda shifted to Somalia and Yemen. While al-Qaeda leaders are hiding in the tribal areas along the AfPak border, the middle-tier of the movement display heightened activity in Somalia and Yemen.
âWe know that South Asia is no longer their primary base," a US defense agency source said. "They are looking for a hide-out in other parts of the world, and continue to expand their organization.â
In January 2009, al-Qaeda's division in Saudi Arabia merged with its Yemeni wing to form al-Qaeda in the Arabian Peninsula. Centered in Yemen, the group takes advantage of the country's poor economy, demography and domestic security. In August 2009, they made the first assassination attempt against a member of the Saudi royal dynasty in decades. President Obama asked his Yemen counterpart Ali Abdullah Saleh to ensure closer cooperation with the US in the struggle against the growing activity of al-Qaeda in Yemen, and promised to send additional aid. Because of the wars in Iraq and Afghanistan, the US was unable to pay sufficient attention to Somalia and Yemen, which could cause problems in the near future. In December 2011, US Secretary of Defense Leon Panetta said that the US operations against al-Qaeda "are now concentrating on key groups in Yemen, Somalia and North Africa." Al-Qaeda in the Arabian Peninsula claimed responsibility for the 2009 bombing attack on Northwest Airlines Flight 253 by Umar Farouk Abdulmutallab. The group released photos of Abdulmutallab smiling in a white shirt and white Islamic skullcap, with the al-Qaeda in Arabian Peninsula banner in the background.
United States operations.
In December 1998, the Director of the CIA Counterterrorism Center reported to the president that al-Qaeda was preparing for attacks in the USA, including the training of personnel to hijack aircraft. On September 11, 2001, al-Qaeda attacked the United States, hijacking four airliners and deliberately crashing them. The attackers killed 2,977 people.
U.S. officials called Anwar al-Awlaki an "example of al-Qaeda reach into" the U.S. in 2008 after probes into his ties to the September 11 attacks hijackers. A former FBI agent identifies Awlaki as a known "senior recruiter for al-Qaeda", and a spiritual motivator. Awlaki's sermons in the U.S. were attended by three of the 9/11 hijackers, as well as accused Fort Hood shooter Nidal Malik Hasan. U.S. intelligence intercepted emails from Hasan to Awlaki between December 2008 and early 2009. On his website, Awlaki has praised Hasan's actions in the Fort Hood shooting.
An unnamed official claimed there was good reason to believe Awlaki "has been involved in very serious terrorist activities since leaving the U.S. [after 9/11], including plotting attacks against America and our allies." In addition, "Christmas Day bomber" Umar Farouk Abdulmutallab said al-Awlaki was one of his al-Qaeda trainers, meeting with him and involved in planning or preparing the attack, and provided religious justification for it, according to unnamed U.S. intelligence officials. In March 2010, alAwlaki said in a videotape delivered to CNN that jihad against America was binding upon himself and every other able Muslim.
US President Barack Obama approved the targeted killing of al-Awlaki by April 2010, making al-Awlaki the first US citizen ever placed on the CIA target list. That required the consent of the U.S. National Security Council, and officials said it was appropriate for an individual who posed an imminent danger to national security. In May 2010, Faisal Shahzad, who pleaded guilty to the 2010 Times Square car bombing attempt, told interrogators he was "inspired by" al-Awlaki, and sources said Shahzad had made contact with al-Awlaki over the internet. Representative Jane Harman called him "terrorist number one", and "Investor's Business Daily" called him "the world's most dangerous man". In July 2010, the US Treasury Department added him to its list of Specially Designated Global Terrorists, and the UN added him to its list of individuals associated with al-Qaeda. In August 2010, al-Awlaki's father initiated a lawsuit against the U.S. government with the American Civil Liberties Union, challenging its order to kill al-Awlaki. In October 2010, U.S. and U.K. officials linked al-Awlaki to the 2010 cargo plane bomb plot. In September 2011, he was killed in a targeted killing drone attack in Yemen. It was reported on March 16, 2012 that Osama bin Laden plotted to kill United States President Barack Obama.
Death of Osama bin Laden.
On May 1, 2011 in Washington, D.C. (May 2, Pakistan Standard Time), U.S. President Barack Obama announced that Osama bin Laden had been killed by "a small team of Americans" acting under Obama's direct orders, in a covert operation in Abbottabad, Pakistan, about north of Islamabad. According to U.S. officials a team of 20â25 US Navy SEALs under the command of the Joint Special Operations Command and working with the CIA stormed bin Laden's compound in two helicopters. Bin Laden and those with him were killed during a firefight in which U.S. forces experienced no injuries or casualties. According to one US official the attack was carried out without the knowledge or consent of the Pakistani authorities. In Pakistan some people were reported to be shocked at the unauthorized incursion by US armed forces. The site is a few miles from the Pakistan Military Academy in Kakul. In his broadcast announcement President Obama said that U.S. forces "took care to avoid civilian casualties."
Details soon emerged that three men and a woman were killed along with bin Laden, the woman being killed when she was "used as a shield by a male combatant". DNA from bin Laden's body, compared with DNA samples on record from his dead sister, confirmed bin Laden's identity. The body was recovered by the US military and was in its custody until, according to one US official, his body was buried at sea according to Islamic traditions. One U.S. official stated that "finding a country willing to accept the remains of the world's most wanted terrorist would have been difficult." U.S State Department issued a "Worldwide caution" for Americans following bin Laden's death and U.S Diplomatic facilities everywhere were placed on high alert, a senior U.S official said. Crowds gathered outside the White House and in New York City's Times Square to celebrate bin Laden's death.
Syria.
In 2003, President Bashar Al-Assad revealed in an interview with a Kuwaiti newspaper that he doubted the organization of Al-Qaeda even existed. He was quoted as saying, "Is there really an entity called al-Qaeda? Was it in Afghanistan? Does it exist now?" He went on further to remark about Bin Laden commenting, he "cannot talk on the phone or use the Internet, but he can direct communications to the four corners of the world? This is illogical."
Following the mass protests that took place later in 2011 demanding the resignation of Al-Assad, Al-Qaeda affiliated organizations and Sunni sympathizers soon began to constitute the most effective fighting force in the Syrian opposition. Until then, al-Qaeda's presence in Syria was not worth mentioning, but its growth thereafter was rapid. Groups such as the Al-Nusra Front and the Islamic State of Iraq and the Levant (ISIS; sometimes ISIL) have recruited many foreign Mujahideen to train and fight in what has gradually become a highly sectarian war. Ideologically, the Syrian Civil War has served the interests of Al-Qaeda as it pits a mainly Sunni opposition against a Shia backed Alawite regime. Viewing Shia Islam as heretical, Al-Qaeda and other fundamentalist Sunni militant groups have invested heavily in the civil conflict, actively backing and supporting the Syrian Opposition despite its clashes with moderate opposition groups such as the Free Syrian Army (FSA).
On February 2, 2014, Al-Qaeda distanced itself from ISIS and its actions in Syria.
India.
In September 2014 al-Zawahiri announced al-Qaeda was establishing a front in India to "wage jihad against its enemies, to liberate its land, to restore its sovereignty, and to revive its Caliphate." He nominated India as a beachhead for regional jihad taking in neighboring countries such as Myanmar and Bangladesh. The motivation for the video was questioned in some quarters where it was seen the militant group was struggling to remain relevant in light of the emerging prominence of ISIS. Reaction amongst Muslims in India to the formation of the new wing, to be known as "Qaedat al-Jihad fi'shibhi al-qarrat al-Hindiya" or al-Qaida in the Indian Subcontinent [AQIS], was one of fury. Leaders of several Indian Muslim organizations rejected al-Zawahiri's pronouncement, saying they could see no good coming from it, and viewed it as a threat to Muslim youth in the country.
US intelligence analyst accused the Pakistan military of 'stage-managing' the terror outfit's latest advance into India. Bruce Riedel, a former CIA analyst and National Security Council official for South Asia, also said that Pakistan should be warned that it will be placed on the list of states sponsoring terrorism. Riedel also said that "Zawahiri made the tape in his hideout in Pakistan, no doubt, and many Indians suspect the ISI (Inter Services Intelligence) is helping to protect him," he wrote.
Attacks.
Al-Qaeda has carried out a total of six major terrorist attacks, four of them in its jihad against America. In each case the leadership planned the attack years in advance, arranging for the shipment of weapons and explosives and using its privatized businesses to provide operatives with safehouses and false identities.
Al-Qaeda usually does not disburse funds for attacks, and very rarely makes wire transfers.
1992.
On December 29, 1992, al-Qaeda's first terrorist attack took place as two bombs were detonated in Aden, Yemen. The first target was the Movenpick Hotel and the second was the parking lot of the Goldmohur Hotel.
The bombings were an attempt to eliminate American soldiers on their way to Somalia to take part in the international famine relief effort, Operation Restore Hope. Internally, al-Qaeda considered the bombing a victory that frightened the Americans away, but in the US the attack was barely noticed.
No Americans were killed because the soldiers were staying in a different hotel altogether, and they went on to Somalia as scheduled. However little noticed, the attack was pivotal as it was the beginning of al-Qaeda's change in direction, from fighting armies to killing civilians. Two people were killed in the bombing, an Australian tourist and a Yemeni hotel worker. Seven others, mostly Yemenis, were severely injured.
Two fatwas are said to have been appointed by the most theologically knowledgeable of al-Qaeda's members, Mamdouh Mahmud Salim, to justify the killings according to Islamic law. Salim referred to a famous fatwa appointed by Ibn Taymiyyah, a 13th-century scholar much admired by Wahhabis, which sanctioned resistance by any means during the Mongol invasions.
1993 World Trade Center bombing.
In 1993, Ramzi Yousef used a truck bomb to attack the World Trade Center in New York City. The attack was intended to break the foundation of Tower One knocking it into Tower Two, bringing the entire complex down.
Yousef hoped this would kill 250,000 people. The towers shook and swayed but the foundation held and he succeeded in killing only six people (although he injured 1,042 others and caused nearly $300Â million in property damage).
After the attack, Yousef fled to Pakistan and later moved to Manila. There he began developing the Bojinka plot plans to implode a dozen American airliners simultaneously, to assassinate Pope John Paul II and President Bill Clinton, and to crash a private plane into CIA headquarters. He was later captured in Pakistan.
None of the US government's indictments against bin Laden have suggested that he had any connection with this bombing, but Ramzi Yousef is known to have attended a terrorist training camp in Afghanistan. After his capture, Yousef declared that his primary justification for the attack was to punish the US for its support for the Israeli occupation of Palestinian territories and made no mention of any religious motivations. A follow-up attack was planned by Omar Abdel-Rahman â the New York City landmark bomb plot. However, the plot was foiled by the authorities.
Late 1990s.
In 1996, bin Laden personally engineered a plot to assassinate Clinton while the president was in Manila for the Asia-Pacific Economic Cooperation. However, intelligence agents intercepted a message just minutes before the motorcade was to leave, and alerted the US Secret Service. Agents later discovered a bomb planted under a bridge.
The 1998 U.S. embassy bombings in East Africa resulted in upward of 300 deaths, mostly locals. A barrage of cruise missiles launched by the U.S. military in response devastated an al-Qaeda base in Khost, Afghanistan, but the network's capacity was unharmed. In late 1999/2000, Al-Qaeda planned attacks to coincide with the millennium, masterminded by Abu Zubaydah and involving Abu Qatada, which would include the bombing Christian holy sites in Jordan, the bombing of Los Angeles International Airport by Ahmed Ressam, and the bombing of the .
In October 2000, al-Qaeda militants in Yemen bombed the missile destroyer "U.S.S. Cole" in a suicide attack, killing 17 U.S. servicemen and damaging the vessel while it lay offshore. Inspired by the success of such a brazen attack, al-Qaeda's command core began to prepare for an attack on the U.S. itself.
September 11 attacks.
The September 11, 2001 attacks were the most devastating terrorist acts in American history, killing approximately 3,000 people. Two commercial airliners were deliberately flown into the World Trade Center towers, a third into The Pentagon, and a fourth, originally intended to target the United States Capitol, crashed in a field in Stonycreek Township near Shanksville, Pennsylvania.
The attacks were conducted by al-Qaeda, acting in accord with the 1998 "fatwa" issued against the U.S. and its allies by military forces under the command of bin Laden, al-Zawahiri, and others. Evidence points to suicide squads led by al-Qaeda military commander Mohamed Atta as the culprits of the attacks, with bin Laden, Ayman al-Zawahiri, Khalid Sheikh Mohammed, and Hambali as the key planners and part of the political and military command.
Messages issued by bin Laden after September 11, 2001 praised the attacks, and explained their motivation while denying any involvement. Bin Laden legitimized the attacks by identifying grievances felt by both mainstream and Islamist Muslims, such as the general perception that the U.S. was actively oppressing Muslims.
Bin Laden asserted that America was massacring Muslims in 'Palestine, Chechnya, Kashmir and Iraq' and that Muslims should retain the 'right to attack in reprisal'. He also claimed the 9/11 attacks were not targeted at women and children, but 'America's icons of military and economic power'.
Evidence has since come to light that the original targets for the attack may have been nuclear power stations on the east coast of the U.S. The targets were later altered by al-Qaeda, as it was feared that such an attack "might get out of hand".
Designation as terrorist organization.
Al-Qaeda has been designated a "terrorist organization" by the following countries and international organizations:
War on Terrorism.
In the immediate aftermath of the attacks, the US government decided to respond militarily, and began to prepare its armed forces to overthrow the Taliban regime it believed was harboring al-Qaeda. Before the US attacked, it offered Taliban leader Mullah Omar a chance to surrender bin Laden and his top associates. The first forces to be inserted into Afghanistan were Paramilitary Officers from the CIA's elite Special Activities Division (SAD).
The Taliban offered to turn over bin Laden to a neutral country for trial if the US would provide evidence of bin Laden's complicity in the attacks. US President George W. Bush responded by saying: "We know he's guilty. Turn him over", and British Prime Minister Tony Blair warned the Taliban regime: "Surrender bin Laden, or surrender power".
Soon thereafter the US and its allies invaded Afghanistan, and together with the Afghan Northern Alliance removed the Taliban government in the war in Afghanistan.
As a result of the US using its special forces and providing air support for the Northern Alliance ground forces, both Taliban and al-Qaeda training camps were destroyed, and much of the operating structure of al-Qaeda is believed to have been disrupted. After being driven from their key positions in the Tora Bora area of Afghanistan, many al-Qaeda fighters tried to regroup in the rugged Gardez region of the nation.
Again, under the cover of intense aerial bombardment, US infantry and local Afghan forces attacked, shattering the al-Qaeda position and killing or capturing many of the militants. By early 2002, al-Qaeda had been dealt a serious blow to its operational capacity, and the Afghan invasion appeared an initial success. Nevertheless, a significant Taliban insurgency remains in Afghanistan, and al-Qaeda's top two leaders, bin Laden and al-Zawahiri, evaded capture.
Debate raged about the exact nature of al-Qaeda's role in the 9/11 attacks, and after the US invasion began, the US State Department also released a videotape showing bin Laden speaking with a small group of associates somewhere in Afghanistan shortly before the Taliban was removed from power. Although its authenticity has been questioned by some, the tape appears to implicate bin Laden and al-Qaeda in the September 11 attacks and was aired on many television channels all over the world, with an accompanying provided by the U.S. Defense Department.
In September 2004, the US government 9/11 Commission investigating the September 11 attacks officially concluded that the attacks were conceived and implemented by al-Qaeda operatives. In October 2004, bin Laden appeared to claim responsibility for the attacks in a videotape released through Al Jazeera, saying he was inspired by Israeli attacks on high-rises in the 1982 invasion of Lebanon: "As I looked at those demolished towers in Lebanon, it entered my mind that we should punish the oppressor in kind and that we should destroy towers in America in order that they taste some of what we tasted and so that they be deterred from killing our women and children."
By the end of 2004, the U.S. government proclaimed that two-thirds of the most senior al-Qaeda figures from 2001 had been captured and interrogated by the CIA: Abu Zubaydah, Ramzi bin al-Shibh and Abd al-Rahim al-Nashiri in 2002; Khalid Sheikh Mohammed in 2003; and Saif al Islam el Masry in 2004. Mohammed Atef and several others were killed. The West was criticised for not being able to comprehend or deal with Al-Qaida despite more than a decade of the war. This also meant no progress has been made in global state security.
Activities.
Africa.
Al-Qaeda involvement in Africa has included a number of bombing attacks in North Africa, as well as supporting parties in civil wars in Eritrea and Somalia. From 1991 to 1996, bin Laden and other al-Qaeda leaders were based in Sudan.
Islamist rebels in the Sahara calling themselves al-Qaeda in the Islamic Maghreb have stepped up their violence in recent years. French officials say the rebels have no real links to the al-Qaeda leadership, but this is a matter of some dispute in the international press and amongst security analysts. It seems likely that bin Laden approved the group's name in late 2006, and the rebels "took on the al Qaeda franchise label", almost a year before the violence began to escalate.
In Mali, the Ansar Dine faction was also reported as an ally of Al-Qaeda in 2013. The Ansar al Dine faction aligned themselves with the AQIM.
Following the Libyan Civil War, the removal of Gaddafi and the ensuing period of post-civil war violence in Libya allowed various Islamist militant organizations affiliated with Al-Qaeda to expand their operations in the region. The 2012 Benghazi attack, which resulted in the death of US Ambassador J. Christopher Stevens and 3 other Americans, is suspected of having been carried out by various Jihadist networks, such as Al-Qaeda in the Islamic Maghreb, Ansar al-Sharia and several other Al-Qaeda affiliated groups. The capture of Nazih Abdul-Hamed al-Ruqai, a senior Al-Qaeda operative wanted by the United States for his involvement in the 1998 United States embassy bombings, on October 5, 2013 by US Navy Seals, FBI and CIA agents illustrates the importance the US and other Western allies have placed on North Africa.
Europe.
Before the 9/11 attacks and the US invasion of Afghanistan, recruits at Al-Qaeda training camps who had Western backgrounds were especially sought after by Al-Qaeda's military wing for conducting operations overseas. Language skills and knowledge of Western culture were generally found among recruits from Europe, such was the case with Mohamed Atta, an Egyptian national studying in Germany at the time of his training, and other members of the Hamburg Cell. Osama bin Laden and Mohammed Atef would later designate Atta as the ringleader of the 9/11 hijackers. Following the attacks, Western intelligence agencies determined that Al-Qaeda cells operating in Europe had aided the hijackers with financing and communications with the central leadership based in Afghanistan.
In 2003, Islamists carried out a series of bombings in Istanbul killing fifty-seven people and injuring seven hundred. Seventy-four people were charged by the Turkish authorities. Some had previously met bin Laden, and though they specifically declined to pledge allegiance to al-Qaeda they asked for its blessing and help.
In 2009, three Londoners, Tanvir Hussain, Assad Sarwar and Ahmed Abdullah Ali, were convicted of conspiring to detonate bombs disguised as soft drinks on seven airplanes bound for Canada and the U.S. The massively complex police and MI5 investigation of the plot involved more than a year of surveillance work conducted by over two hundred officers. British and U.S. officials said the planâunlike many recent homegrown European terrorist plotsâwas directly linked to al-Qaeda and guided by senior Islamic militants in Pakistan.
In 2012, Russian Intelligence indicated that al-Qaeda had given a call for "forest jihad" and has been starting massive forest fires as part of a strategy of "thousand cuts".
Arab world.
Following Yemeni unification in 1990, Wahhabi networks began moving missionaries into the country in an effort to subvert the capitalist north. Although it is unlikely bin Laden or Saudi al-Qaeda were directly involved, the personal connections they made would be established over the next decade and used in the "USS Cole" bombing. Concerns grow over Al Qaeda's group in Yemen.
In Iraq, al-Qaeda forces loosely associated with the leadership were embedded in the Jama'at al-Tawhid wal-Jihad organization commanded by Abu Musab al-Zarqawi. Specializing in suicide operations, they have been a "key driver" of the Sunni insurgency. Although they played a small part in the overall insurgency, between 30% and 42% of all suicide bombings which took place in the early years were claimed by Zarqawi's organization. Reports have indicated that oversights such as the failure to control access to the Qa'qaa munitions factory in Yusufiyah have allowed large quantities of munitions to fall into the hands of al-Qaida. In November 2010, the Islamic State of Iraq militant group, which is linked to al-Qaeda in Iraq, threatened to "exterminate Iraqi Christians".
Significantly, it was not until the late 1990s that al-Qaeda began training Palestinians. This is not to suggest that resistance fighters are underrepresented in the network as a number of Palestinians, mostly coming from Jordan, wanted to join and have risen to serve high-profile roles in Afghanistan. Rather, large groups such as Hamas and Palestinian Islamic Jihadâwhich cooperate with al-Qaeda in many respectsâhave had difficulties accepting a strategic alliance, fearing that al-Qaeda will co-opt their smaller cells. This may have changed recently, as Israeli security and intelligence services believe al-Qaeda has managed to infiltrate operatives from the Occupied Territories into Israel, and is waiting for the right time to mount an attack.
Kashmir.
Bin Laden and Ayman al-Zawahiri consider India to be a part of the 'Crusader-Zionist-Hindu' conspiracy against the Islamic world. According to the 2005 report 'Al Qaeda: Profile and Threat Assessment' by Congressional Research Service, bin Laden was involved in training militants for Jihad in Kashmir while living in Sudan in the early nineties. By 2001, Kashmiri militant group Harkat-ul-Mujahideen had become a part of the al-Qaeda coalition. According to the United Nations High Commissioner for Refugees al-Qaeda was thought to have established bases in Pakistan-administered Kashmir (in Azad Kashmir, and to some extent in GilgitâBaltistan) during the 1999 Kargil War and continued to operate there with tacit approval of Pakistan's Intelligence services.
Many of the militants active in Kashmir were trained in the same Madrasahs as Taliban and al-Qaeda. Fazlur Rehman Khalil of Kashmiri militant group Harkat-ul-Mujahideen was a signatory of al-Qaeda's 1998 declaration of Jihad against America and its allies. In a 'Letter to American People' written by bin Laden in 2002 he stated that one of the reasons he was fighting America is because of its support to India on the Kashmir issue. In November 2001, Kathmandu airport went on high alert after threats that bin Laden planned to hijack a plane from there and crash it into a target in New Delhi. In 2002, US Secretary of Defense Donald Rumsfeld, on a trip to Delhi, suggested that al-Qaeda was active in Kashmir though he did not have any hard evidence. He proposed hi tech ground sensors along the line of control to prevent militants from infiltrating into Indian administered Kashmir.
An investigation in 2002 unearthed evidence that al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence In 2002, a special team of Special Air Service and Delta Force was sent into Indian Administered Kashmir to hunt for bin Laden after reports that he was being sheltered by Kashmiri militant group Harkat-ul-Mujahideen which had previously been responsible for 1995 Kidnapping of western tourists in Kashmir. Britain's highest ranking al-Qaeda operative Rangzieb Ahmed had previously fought in Kashmir with the group Harkat-ul-Mujahideen and spent time in Indian prison after being captured in Kashmir.
US officials believe that al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Their strategy was to force Pakistan to move its troops to the border with India, thereby relieving pressure on al-Qaeda elements hiding in northwestern Pakistan. In 2006 al-Qaeda claimed they had established a wing in Kashmir; this has worried the Indian government. However the Indian Army Lt. Gen. H.S. Panag, GOC-in-C Northern Command, said to reporters that the army has ruled out the presence of al-Qaeda in Indian-administered Jammu and Kashmir; furthermore he said that there is nothing that can verify reports from the media of al-Qaeda presence in the state. He however stated that al-Qaeda had strong ties with Kashmiri militant groups Lashkar-e-Taiba and Jaish-e-Mohammed based in Pakistan. It has been noted that Waziristan has now become the new battlefield for Kashmiri militants fighting NATO in support of al-Qaeda and Taliban. Dhiren Barot, who wrote the "Army of Madinah in Kashmir" and was an al-Qaeda operative convicted for involvement in the 2004 financial buildings plot, had received training in weapons and explosives at a militant training camp in Kashmir.
Maulana Masood Azhar, the founder of another Kashmiri group Jaish-e-Mohammed, is believed to have met bin Laden several times and received funding from him. In 2002, Jaish-e-Mohammed organized the kidnapping and murder of Daniel Pearl in an operation run in conjunction with al-Qaeda and funded by bin Laden. According to American counter-terrorism expert Bruce Riedel, al-Qaeda and Taliban were closely involved in the 1999 hijacking of Indian Airlines Flight 814 to Kandahar which led to the release of Maulana Masood Azhar & Ahmed Omar Saeed Sheikh from an Indian prison in exchange for the passengers. This hijacking, Riedel stated, was rightly described by then Indian Foreign minister Jaswant Singh as a 'dress rehearsal' for September 11 attacks. Bin Laden personally welcomed Azhar and threw a lavish party in his honor after his release, according to Abu Jandal, bodyguard of bin Laden. Ahmed Omar Saeed Sheikh, who had been in Indian prison for his role in 1994 kidnappings of Western tourists in India, went on to murder Daniel Pearl and was sentenced to death by Pakistan. Al-Qaeda operative Rashid Rauf, who was one of the accused in 2006 transatlantic aircraft plot, was related to Maulana Masood Azhar by marriage.
Lashkar-e-Taiba, a Kashmiri militant group which is thought to be behind 2008 Mumbai attacks, is also known to have strong ties to senior al-Qaeda leaders living in Pakistan. In Late 2002, top al-Qaeda operative Abu Zubaydah was arrested while being sheltered by Lashkar-e-Taiba in a safe house in Faisalabad. The FBI believes that al-Qaeda and Lashkar have been 'intertwined' for a long time while the CIA has said that al-Qaeda funds Lashkar-e-Taiba. French investigating magistrate Jean-Louis BruguiÃ¨re, who was the top French counter-terrorism official, told Reuters in 2009 that 'Lashkar-e-Taiba is no longer a Pakistani movement with only a Kashmir political or military agenda. Lashkar-e-Taiba is a member of al-Qaeda.'
In a video released in 2008, senior al-Qaeda operative American-born Adam Yahiye Gadahn stated that "victory in Kashmir has been delayed for years; it is the liberation of the jihad there from this interference which, Allah willing, will be the first step towards victory over the Hindu occupiers of that Islam land."
In September 2009, a US drone strike reportedly killed Ilyas Kashmiri who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' al-Qaeda member while others have described him as head of military operations for al-Qaeda. Kashmiri was also charged by the US in a plot against Jyllands-Posten, the Danish newspaper which was at the center of Jyllands-Posten Muhammad cartoons controversy. US officials also believe that Kashmiri was involved in the Camp Chapman attack against the CIA. In January 2010, Indian authorities notified Britain of an al-Qaeda plot to hijack an Indian airlines or Air India plane and crash it into a British city. This information was uncovered from interrogation of Amjad Khwaja, an operative of Harkat-ul-Jihad al-Islami, who had been arrested in India.
In January 2010, US Defense secretary Robert Gates, while on a visit to Pakistan, stated that al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan.
Internet.
Timothy L. Thomas claims that in the wake of its evacuation from Afghanistan, al-Qaeda and its successors have migrated online to escape detection in an atmosphere of increased international vigilance. As a result, the organization's use of the Internet has grown more sophisticated, encompassing financing, recruitment, networking, mobilization, publicity, as well as information dissemination, gathering and sharing.
Abu Ayyub al-Masri's al-Qaeda movement in Iraq regularly releases short videos glorifying the activity of jihadist suicide bombers. In addition, both before and after the death of Abu Musab al-Zarqawi (the former leader of al-Qaeda in Iraq), the umbrella organization to which al-Qaeda in Iraq belongs, the Mujahideen Shura Council, has a regular presence on the Web.
The range of multimedia content includes guerrilla training clips, stills of victims about to be murdered, testimonials of suicide bombers, and videos that show participation in jihad through stylized portraits of mosques and musical scores. A website associated with al-Qaeda posted a video of captured American entrepreneur Nick Berg being decapitated in Iraq. Other decapitation videos and pictures, including those of Paul Johnson, Kim Sun-il, and Daniel Pearl, were first posted on jihadist websites.
In December 2004 an audio message claiming to be from bin Laden was posted directly to a website, rather than sending a copy to al Jazeera as he had done in the past.
Al-Qaeda turned to the Internet for release of its videos in order to be certain it would be available unedited, rather than risk the possibility of al Jazeera editors editing the videos and cutting out anything critical of the Saudi royal family. Bin Laden's December 2004 message was much more vehement than usual in this speech, lasting over an hour.
In the past, Alneda.com and Jehad.net were perhaps the most significant al-Qaeda websites. Alneda was initially taken down by American Jon Messner, but the operators resisted by shifting the site to various servers and strategically shifting content.
The US is currently attempting to extradite a British information technology specialist, Babar Ahmad, on charges of operating a network of English-language al-Qaeda websites, such as Azzam.com. Ahmad's extradition is opposed by various British Muslim organizations, such as the Muslim Association of Britain.
Online Communications.
In 2007, Al-Qaeda released Mujahedeen Secrets, encryption software used for online and cellular communications. A later version, Mujahideen Secrets 2, was released in 2008.
Aviation network.
Al-Qaeda is believed to be operating a clandestine aviation network including "several Boeing 727 aircraft", turboprops and executive jets, according to a Reuters story. Based on a U.S. Department of Homeland Security report, the story said that al-Qaeda is possibly using aircraft to transport drugs and weapons from South America to various unstable countries in West Africa. A Boeing 727 can carry up to 10 tons of cargo. The drugs eventually are smuggled to Europe for distribution and sale, and the weapons are used in conflicts in Africa and possibly elsewhere. Gunmen with links to al-Qaeda have been increasingly kidnapping some Europeans for ransom. The profits from the drug and weapon sales, and kidnappings can, in turn, fund more militant activities.
Involvement in military conflicts.
The following is a list of military conflicts in which Al-Qaeda and its direct affiliates have taken part militarily. 
Alleged CIA involvement.
 Experts debate whether or not the al-Qaeda attacks were blowback from the American CIA's "Operation Cyclone" program to help the Afghan mujahideen. Robin Cook, British Foreign Secretary from 1997 to 2001, has written that al-Qaeda and bin Laden were "a product of a monumental miscalculation by western security agencies", and that "Al-Qaida, literally 'the database', was originally the computer file of the thousands of mujahideen who were recruited and trained with help from the CIA to defeat the Russians."
Munir Akram, Permanent Representative of Pakistan to the United Nations from 2002 to 2008, wrote in a letter published in the New York Times on January 19, 2008:
The strategy to support the Afghans against Soviet military intervention was evolved by several intelligence agencies, including the C.I.A. and Inter-Services Intelligence, or ISI. After the Soviet withdrawal, the Western powers walked away from the region, leaving behind 40,000 militants imported from several countries to wage the anti-Soviet jihad. Pakistan was left to face the blowback of extremism, drugs and guns.
A variety of sourcesâCNN journalist Peter Bergen, Pakistani ISI Brigadier Mohammad Yousaf, and CIA operatives involved in the Afghan program, such as Vincent Cannistraroâdeny that the CIA or other American officials had contact with the foreign mujahideen or bin Laden, let alone armed, trained, coached or indoctrinated them.
Bergen and others argue that there was no need to recruit foreigners unfamiliar with the local language, customs or lay of the land since there were a quarter of a million local Afghans willing to fight; that foreign mujahideen themselves had no need for American funds since they received several hundred million dollars a year from non-American, Muslim sources; that Americans could not have trained mujahideen because Pakistani officials would not allow more than a handful of them to operate in Pakistan and none in Afghanistan; and that the Afghan Arabs were almost invariably militant Islamists reflexively hostile to Westerners whether or not the Westerners were helping the Muslim Afghans.
According to Bergen, known for conducting the first television interview with bin Laden in 1997, the idea that "the CIA funded bin Laden or trained bin Laden... [is] a folk myth. There's no evidence of this... Bin Laden had his own money, he was anti-American and he was operating secretly and independently... The real story here is the CIA didn't really have a clue about who this guy was until 1996 when they set up a unit to really start tracking him." But Bergen conceded that, in one "strange incident," the CIA appeared to give visa help to mujahideen-recruiter Omar Abdel-Rahman.
In his widely praised account of al-Qaeda, English journalist Jason Burke wrote:
Broader influence.
Anders Behring Breivik, the perpetrator of the 2011 Norway attacks, was inspired by al-Qaeda, calling it "the most successful revolutionary movement in the world." While admitting different aims, he sought to "create a European version of al-Qaida."
Criticism.
According to a number of sources there has been a "wave of revulsion" against al-Qaeda and its affiliates by "religious scholars, former fighters and militants" alarmed by al-Qaeda's takfir and killing of Muslims in Muslim countries, especially Iraq.
Noman Benotman, a former Afghan Arab and militant of the Libyan Islamic Fighting Group, went public with an open letter of criticism to Ayman al-Zawahiri in November 2007 after persuading imprisoned senior leadership of his former group to enter into peace negotiations with the Libyan regime. While Ayman al-Zawahiri announced the affiliation of the group with al-Qaeda in November 2007, the Libyan government released 90 members of the group from prison several months later after "they were said to have renounced violence."
In 2007, around the sixth anniversary of the September 11 attacks and a couple of months before "Rationalizing Jihad" first appeared in the newspapers, the Saudi sheikh Salman al-Ouda delivered a personal rebuke to bin Laden. Al-Ouda, a religious scholar and one of the fathers of the Sahwa, the fundamentalist awakening movement that swept through Saudi Arabia in the 1980s, is a widely respected critic of jihadism. Al-Ouda addressed al-Qaeda's leader on television asking him
My brother Osama, how much blood has been spilt? How many innocent people, children, elderly, and women have been killed... in the name of al-Qaeda? Will you be happy to meet God Almighty carrying the burden of these hundreds of thousands or millions [of victims] on your back?
According to Pew polls, support for al-Qaeda has been slightly dropped for parts of the Muslim world in the years before 2008. The numbers supporting suicide bombings in Indonesia, Lebanon, and Bangladesh, for instance, have dropped by half or more in the last five years. In Saudi Arabia, only 10Â percent now have a favorable view of al-Qaeda, according to a December poll by Terror Free Tomorrow, a Washington-based think tank.
In 2007, the imprisoned Sayyed Imam Al-Sharif, an influential Afghan Arab, "ideological godfather of al-Qaeda", and former supporter of takfir, sensationally withdrew his support from al-Qaeda with a book "Wathiqat Tarshid Al-'Aml Al-Jihadi fi Misr w'Al-'Alam" ("Rationalizing Jihad in Egypt and the World").
Although once associated with al-Qaeda, in September 2009 LIFG completed a new "code" for jihad, a 417-page religious document entitled "Corrective Studies". Given its credibility and the fact that several other prominent Jihadists in the Middle East have turned against al-Qaeda, the LIFG's about face may be an important step toward staunching al-Qaeda's recruitment.
See also.
Publications:

</doc>
<doc id="1923" url="http://en.wikipedia.org/wiki?curid=1923" title="Alessandro Volta">
Alessandro Volta

Alessandro Giuseppe Antonio Anastasio Volta (February 18, 1745 â March 5, 1827) was an Italian physicist known for the invention of the battery in the 1800s.
Early life and works.
Volta was born in Como, a town in present-day northern Italy (near the Swiss border) on February 18, 1745. In 1774, he became a professor of physics at the Royal School in Como. A year later, he improved and popularized the electrophorus, a device that produced static electricity. His promotion of it was so extensive that he is often credited with its invention, even though a machine operating on the same principle was described in 1762 by the Swedish experimenter Johan Wilcke.
In the years between 1776â78, Volta studied the chemistry of gases. He discovered methane after reading a paper by Benjamin Franklin of America on "flammable air", and Volta searched for it carefully in Italy. In November 1776, he found methane at Lake Maggiore, and by 1778 he managed to isolate methane. He devised experiments such as the ignition of methane by an electric spark in a closed vessel. Volta also studied what we now call electrical capacitance, developing separate means to study both electrical potential ("V" ) and charge ("Q" ), and discovering that for a given object, they are proportional. This may be called Volta's Law of capacitance, and it was for this work the unit of electrical potential has been named the volt.
In 1779 he became a professor of experimental physics at the University of Pavia, a chair that he occupied for almost 40 years. In 1794, Volta married an aristocratic lady also from Como, Teresa Peregrini, with whom he raised three sons: Giovanni, Flaminio and Zanino.
Volta and Galvani.
Luigi Galvani discovered something he named "animal electricity" when two different metals were connected in series with the frog's leg and to one another. Volta realized that the frog's leg served as both a conductor of electricity (what we would now call an electrolyte) and as a detector of electricity. He replaced the frog's leg with brine-soaked paper, and detected the flow of electricity by other means familiar to him from his previous studies. In this way he discovered the electrochemical series, and the law that the electromotive force (emf) of a galvanic cell, consisting of a pair of metal electrodes separated by electrolyte, is the difference between their two electrode potentials (thus, two identical electrodes and a common electrolyte give zero net emf). This may be called Volta's Law of the electrochemical series.
In 1800 as the result of a professional disagreement over the galvanic response advocated by Galvani, he invented the voltaic pile, an early electric battery, which produced a steady electric current. Volta had determined that the most effective pair of dissimilar metals to produce electricity was zinc and silver. Initially he experimented with individual cells in series, each cell being a wine goblet filled with brine into which the two dissimilar electrodes were dipped. The voltaic pile replaced the goblets with cardboard soaked in brine.
First battery.
In announcing his discovery of his voltaic pile, Volta paid tribute to the influences of William Nicholson, Tiberius Cavallo, and Abraham Bennet.
The battery made by Volta is credited as the first electrochemical cell. It consists of two electrodes: one made of zinc, the other of copper. The electrolyte is either sulfuric acid mixed with water or a form of saltwater brine. The electrolyte exists in the form 2H+ and SO42â. The zinc, which is higher than both copper and hydrogen in the electrochemical series, reacts with the negatively charged sulfate (SO42â). The positively charged hydrogen ions (protons) capture electrons from the copper, forming bubbles of hydrogen gas, H2. This makes the zinc rod the negative electrode and the copper rod the positive electrode.
Thus, there are two terminals, and an electric current will flow if they are connected. The chemical reactions in this voltaic cell are as follows:
The copper does not react, but rather it functions as an electrode for the electric current.
However, this cell also has some disadvantages. It is unsafe to handle, since sulfuric acid, even if diluted, can be hazardous. Also, the power of the cell diminishes over time because the hydrogen gas is not released. Instead, it accumulates on the surface of the zinc electrode and forms a barrier between the metal and the electrolyte solution.
Last years and retirement.
In honor of his work, Volta was made a count by Napoleon Bonaparte in 1810. His image was depicted on the Italian 10,000 lira note (no longer in circulation, since the euro has replaced the lira) along with a sketch of his voltaic pile.
Volta retired in 1819 to his estate in Camnago, a frazione of Como, Italy, now named "Camnago Volta" in his honor. He died there on March 5, 1827. Volta's remains were buried in Camnago Volta.
Volta's legacy is celebrated by the Tempio Voltiano memorial located in the public gardens by the lake. There is also a museum which has been built in his honor, which exhibits some of the equipment that Volta used to conduct experiments. Nearby stands the Villa Olmo, which houses the Voltian Foundation, an organization promoting scientific activities. Volta carried out his experimental studies and produced his first inventions near Como.
Religious beliefs.
Volta was raised as a Catholic and for all of his life continued to maintain his belief. Because he was not ordained a clergyman, like his family expected, he was sometimes accused of being irreligious and some people have speculated about his possible unbelief, stressing that "he did not join the Church", or that he virtually "ignored the church's call". Nevertheless, he casted out doubts in a declaration of faith in which he said:
I do not understand how anyone can doubt the sincerity and constancy of my attachment to the religion which I profess, the Roman, Catholic and Apostolic religion in which I was born and brought up, and of which I have always made confession, externally and internally... I have, indeed, and only too often, failed in the performance of those good works which are the mark of a Catholic Christian, and I have been guilty of many sins: but through the special mercy of God I have never, as far as I know, wavered in my faith. In this faith I recognise a pure gift of God, a supernatural grace ; but I have not neglected those human means which confirm belief, and overthrow the doubts which at times arise. I studied attentively the grounds and basis of religion, the works of apologists and assailants, the reasons for and against, and I can say that the result of such study is to clothe religion with such a degree of probability, even for the merely natural reason, that every spirit unperverted by sin and passion, every naturally noble spirit must love and accept it. May this confession which has been asked from me and which I willingly give, written and subscribed by my own hand, with authority to show it to whomsoever you will, for I am not ashamed of the Gospel, may it produce some good fruit.

</doc>
<doc id="1924" url="http://en.wikipedia.org/wiki?curid=1924" title="Argo Navis">
Argo Navis

Argo Navis (or simply Argo) was a large constellation in the southern sky that has since been divided into three constellations. It represented the "Argo", the ship used by Jason and the Argonauts in Greek mythology. The abbreviation was "Arg" and the genitive was "Argus Navis".
Due to the phenomenon of precession, there are far fewer southern stars visible today than there were in the times of ancient sailors of the Mediterranean. It is more difficult to make out what remains of the constellation. 
Stars that make up constellations rotate around the galactic center of the Milky Way at different speeds. Over time constellations appear to deform and then cease to exist. In 20,000 years, none of the currently observed constellations will be found in the night skies.
When the original Argo Navis constellation still existed, it was found low on the horizon of the night sky. Starting in springtime, there appeared the apparition of a great ship. This ship sailed ever westward skimming along the southern horizon. The ancient Greeks said it was Argo Navis, the ship sailed by Jason and his Argonauts in search of the Golden Fleece.
Argo Navis is the only one of the 48 constellations listed by the 2nd century astronomer Ptolemy that is no longer officially recognized as a constellation. It was unwieldy due to its enormous size: were it still considered a single constellation, it would be the largest of all. In 1752, the French astronomer Nicolas Louis de Lacaille subdivided it into Carina (the keel, or the hull, of the ship), Puppis (the poop deck, or stern), Vela (the sails), and, according to some, Pyxis (the compass, formerly the mast). When Argo Navis was split, its Bayer designations were also split. Carina has the Î±, Î² and Îµ, Vela has Î³ and Î´, Puppis has Î¶, and so on.
The constellation Pyxis (the mariner's compass) occupies an area which in antiquity was considered part of Argo's mast (called Malus). While its Bayer designations are separate from those of Carina, Puppis and Vela, having its own Î±, Î² and Î³, for example, various modern authorities hold that Pyxis was in fact part of the Greek conception of Argo Navis.
The Maori had several names for what was the constellation Argo, including "Te Waka-o-Tamarereti", "Te Kohi-a-Autahi", and "Te Kohi".

</doc>
<doc id="1925" url="http://en.wikipedia.org/wiki?curid=1925" title="Andromeda (mythology)">
Andromeda (mythology)

In Greek mythology, Andromeda is the daughter of Cepheus, an Aethiopian king, and Cassiopeia. When Cassiopeia's hubris leads her to boast that Andromeda is more beautiful than the Nereids, Poseidon, influenced by Hades sends a sea monster, Cetus, to ravage Aethiopia as divine punishment. Andromeda is stripped and chained naked to a rock as a sacrifice to sate the monster, but is saved from death by Perseus.
Her name is the Latinized form of the Greek ("AndromÃ©da") or ("AndromÃ©dÄ"): "ruler of men", from ("anÄr, andrÃ³s") "man", and "medon", "ruler".
As a subject, Andromeda has been popular in art since classical times; it is one of several Greek myths of a Greek hero's rescue of the intended victim of an archaic Hieros gamos (sacred marriage), giving rise to the "princess and dragon" motif. From the Renaissance, interest revived in the original story, typically as derived from Ovid's account.
Mythology.
In Greek mythology, Andromeda was the daughter of Cepheus and Cassiopeia, king and queen of the North African kingdom of Aethiopia.
Her mother Cassiopeia boasted that her daughter was more beautiful than the Nereids, the nymph-daughters of the sea god Nereus and often seen accompanying Poseidon. To punish the queen for her arrogance, Poseidon, brother to Zeus and god of the sea, sent a sea monster named Cetus to ravage the coast of Aethiopia including the kingdom of the vain queen. The desperate king consulted the Oracle of Apollo, who announced that no respite would be found until the king sacrificed his daughter, Andromeda, to the monster. Stripped naked, she was chained to a rock on the coast.
Perseus was returning from having slain the Gorgon Medusa. After he happened upon the chained Andromeda, he approached Cetus while invisible (for he was wearing Hades's helm), and killed the sea monster. He set Andromeda free, and married her in spite of her having been previously promised to her uncle Phineus. At the wedding a quarrel took place between the rivals and Phineus was turned to stone by the sight of the Gorgon's head.
Andromeda followed her husband, first to his native island of Serifos, where he rescued his mother DanaÃ«, and then to Tiryns in Argos. Together, they became the ancestors of the family of the "Perseidae" through the line of their son Perses. Perseus and Andromeda had seven sons: Perses, Alcaeus, Heleus, Mestor, Sthenelus, Electryon, and Cynurus as well as two daughters, Autochthe and Gorgophone. Their descendants ruled Mycenae from Electryon down to Eurystheus, after whom Atreus attained the kingdom, and would also include the great hero Heracles. According to this mythology, Perseus is the ancestor of the Persians.
At the port city of Jaffa (today part of Tel Aviv) an outcrop of rocks near the harbour has been associated with the place of Andromeda's chaining and rescue by the traveler Pausanias, the geographer Strabo and the historian of the Jews Josephus.
After Andromeda's death, as Euripides had promised Athena at the end of his "Andromeda", produced in 412 BCE, the goddess placed her among the constellations in the northern sky, near Perseus and Cassiopeia; the constellation Andromeda, so known since antiquity, is named after her.
Constellations.
Andromeda is represented in the northern sky by the constellation Andromeda, which contains the Andromeda Galaxy.
Four constellations are associated with the myth. Viewing the fainter stars visible to the naked eye, the constellations are rendered as:
Other constellations related to the story are:
Portrayals of the myth.
Italian composer Salvatore Sciarrino composed an hour-long operatic drama called Perseo e Andromeda in 2000.
Sophocles and Euripides (and in more modern times, Corneille) made the story the subject of tragedies, and its incidents were represented in numerous ancient works of art. Jean-Baptiste Lully's opera, "PersÃ©e", also dramatizes the myth.
Andromeda has been the subject of numerous ancient and modern works of art, including, Rembrandt's "Andromeda Chained to the Rocks", one of Titian's "poesies" (Wallace Collection), and compositions by Joachim Wtewael (Louvre), Veronese (Rennes), Rubens, Ingres, and Gustave Moreau. From the Renaissance onward the chained nude figure of Andromeda typically was the centre of interest, and often she was shown alone, fearfully awaiting the monster.
In year 1973, animated film called "Perseus" (20 minutes) was made in the Soviet Union as part of Soviet Union animated film collection called "Legends and mÑths of Ancient Greece".
The 1981 film "Clash of the Titans" retells the story of Perseus, Andromeda, and Cassiopeia, but makes a few changes (notably Cassiopeia boasts that her daughter is more beautiful than Thetis as opposed to the Nereids as a group). Thetis was a Nereid, but also the future mother of Achilles. Andromeda and Perseus meet and fall in love after he saves her soul from the enslavement of Thetis' hideous son, Calibos, whereas in the myth, they simply meet as Perseus returns home from having slain Medusa. In the film, the monster is called a kraken, although it is depicted as a lizard-like creature rather than a squid; and combining two elements of the myth, Perseus defeats the sea monster by showing it Medusa's face, turning the monster into stone. Andromeda is depicted as being strong-willed and independent, whereas in the stories she is only really mentioned as being the princess whom Perseus saves from the sea monster. Andromeda was portrayed by Judi Bowker in this film.
Andromeda also features in the 2010 film "Clash of the Titans", a remake of the 1981 version. Several changes were made in regard to the myth, most notably that Perseus did not marry Andromeda after he rescued her from the sea monster. Andromeda was portrayed by Alexa Davalos. The character was played by Rosamund Pike in the sequel "Wrath of the Titans", the second of a planned trilogy. In the end of the sequel, Perseus and Andromeda begin a relationship.
In the Japanese anime "Saint Seiya" the character, Shun, represents the Andromeda constellation using chains as his main weapons, reminiscent of Andromeda being chained before she was saved by Perseus. In order to attain the Andromeda Cloth, he was chained between two large pillars of rock and he had to overcome the chains before the tide came in and killed him, also reminiscent of this myth.
Andromeda appears in Disney's "" as a new student of "Prometheus Academy" which Hercules and other characters from Greek mythology attend.
In "The Sea of Monsters", the second book in the "Percy Jackson & the Olympians" series, a cruise ship which serves as living space for Kronos's army called "The Princess Andromeda" is named after her.

</doc>
<doc id="1926" url="http://en.wikipedia.org/wiki?curid=1926" title="Antlia">
Antlia

Antlia (; from Ancient Greek "á¼Î½ÏÎ»Î¯Î±") is a constellation in the southern sky. Its name means "pump" and it specifically represents an air pump. The constellation was created in the 18th century from an undesignated region of sky, so the stars comprising Antlia are faint. Antlia is bordered by Hydra the sea snake, Pyxis the compass, Vela the sails, and Centaurus the centaur. This group of constellations is prominent in the southern sky in late winter and spring. NGC 2997, a spiral galaxy, and the Antlia Dwarf Galaxy lie within Antlia's borders.
History.
Antlia was created in 1756 by the French astronomer AbbÃ© Nicolas Louis de Lacaille, who created fourteen constellations for the southern sky to fill some faint regions. Though Antlia was technically visible to ancient Greek astronomers, its stars were too faint to have been included in any constellations. Because of this, its main stars have no particular pattern and it is devoid of bright deep-sky objects. It was originally named "Antlia pneumatica" ("Machine Pneumatique" in French) to commemorate the air pump invented by the French physicist Denis Papin. 
Lacaille and Johann Bode each depicted Antlia differently, as either the single-cylinder vacuum pump used in Papin's initial experiments, or the more advanced double-cylinder version. The International Astronomical Union subsequently adopted it as one of the 88 modern constellations. There is no mythology attached to Antlia as Lacaille discontinued the tradition of giving names from mythology to constellations and instead chose names mostly from scientific instruments.
According to some, the most prominent stars that now comprise Antlia were once included within the ancient constellation Argo Navis, the Ship of the Argonauts, which due to its immense size was split into several smaller constellations by Lacaille in 1763. However, given the faintness and obscurity of its stars, most authorities do not believe that the ancient Greeks included Antlia as part of their classical depiction of Argo Navis.
In non-Western astronomy.
Chinese astronomers were able to view what is modern Antlia from their latitudes, and incorporated its stars into two different constellations. Several stars in the southern part of Antlia were a portion of "Dong'ou", which represented an area in southern China. Furthermore, Epsilon, Eta, and theta Antliae were incorporated into the celestial temple, which also contained stars from modern Pyxis.
Notable features.
Stars.
Lacaille gave nine stars Bayer designations, labelling them Alpha through to Theta, including two stars next to each other as Zeta. Gould later added a tenth, Iota Antliae. Beta and Gamma Antliae (now HR 4339 and HD 90156) ended up in the neighbouring constellation Hydra once the constellation boundaries were delineated in 1930. 
The constellation's brightest star, Alpha Antliae is an orange giant of spectral type K4III that is a suspected variable star, ranging between apparent magnitudes 4.22 and 4.29. It is 
located 366 light-years away from Earth. Estimated to be shining with around 480 to 555 the luminosity of the Sun, it is most likely an ageing star that is brightening and on its way to becoming a Mira variable star, having converted all its core fuel into carbon. Located near Alpha is Delta Antliae, a binary star around 481 light years distant. The primary is a blue-white main sequence star of spectral type B9.5V and magnitude 5.6 and the secondary is an yellow-white main sequence star of spectral type F9Ve and magnitude 9.6. 
Zeta Antliae is a wide double star 373 light years away. The primary (Zeta1 Antliae) is of magnitude 5.8, though it is a double star with a primary of magnitude 6.2 and a secondary of magnitude 7.0. The secondary (Zeta2 Antliae) is of magnitude 5.9. Eta Antliae is another double composed of an F-type main sequence star of spectral type and magnitude 5.22, with a companion of magnitude 11.3. Theta Antliae is likewise double, composed of an A-type main sequence star and yellow giant.
Epsilon Antliae is an evolved orange giant star of spectral type K3 IIIa, with a diameter around 69 times that of the Sun. It is slightly variable. At the other end of Antlia, Iota Antliae is likewise an orange giant of spectral type K1 III. 
T Antliae is a yellow-white supergiant of spectral type F6Iab and Cepheid variable ranging between magnitude 8.88 and 9.82. U Antliae is a red C-type carbon star and is an irregular variable that ranges between magnitudes 5.27 and 6.04.
HR 4049, also known as AG Antliae, is an unusual hot variable ageing star of spectral type B9.5Ib-II. UX Antliae is an R Coronae Borealis variable with a baseline apparent magnitude of around 11.85, with irregular dimmings down to below magnitude 18.0. A luminous and remote star, it is a supergiant with a spectrum resembling that of a yellow-white F-type star but it has almost no hydrogen. 
DEN 1048-3956 is a brown dwarf located around 13 light-years distant from Earth. At magnitude 17 it is much too faint to be seen with the unaided eye.
Deep-sky objects.
Because it occupies a part of the celestial sphere that faces away from the Milky Way, Antlia contains very few deep-sky objects. It contains no globular clusters, no planetary nebulae, and no open clusters. However, it does contain several galaxies. 
NGC 2997 is a loose face-on spiral galaxy of type Sc. It is the brightest galaxy in Antlia at an integrated magnitude of 10.6. Though nondescript in most amateur telescopes, it presents bright clusters of young stars and many dark dust lanes in photographs.
The Antlia Dwarf, a 14.8m dwarf spheroidal galaxy that belongs to our Local Group of galaxies. It was discovered only as recently as 1997.
The Antlia Cluster is a cluster of galaxies located in the Hydra-Centaurus Supercluster. It is the third nearest to our Local Group after the Virgo Cluster and Fornax Cluster.

</doc>
<doc id="1927" url="http://en.wikipedia.org/wiki?curid=1927" title="Ara (constellation)">
Ara (constellation)

Ara is a southern constellation situated between Scorpius and Triangulum Australe. Its name is Latin for "altar". Ara was one of the 48 Greek constellations described by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union.
Notable features.
Stars.
Ara contains part of the Milky Way to the south of Scorpius and thus has rich star fields.
The constellation's stars have no names in Western culture, but the Chinese call Î± Arae "Choo" ("club" or "staff"), and Îµ Arae "Tso Kang", meaning 'left guard'.
Deep-sky objects.
The northwest corner of Ara is crossed by the Milky Way and contains several open clusters (notably NGC 6200) and diffuse nebulae (including the bright cluster/nebula pair NGC 6188 and NGC 6193). The brightest of the globular clusters, sixth magnitude NGC 6397, lies at a distance of just , making it one of the closest globular cluster to the solar system.
Although Ara lies close to the heart of the Milky Way, two spiral galaxies (NGC 6215 and NGC 6221) are visible near star Î· Arae.
Planetary Nebulae.
The Stingray Nebula (Hen 3-1357), the youngest known planetary nebula as of 2010, formed in Ara; the light from its formation was first observable around 1987.
Last, but not least; there is also NGC 6326. A planetary nebula that might have a binary system at its center.
Illustrations.
In illustrations, Ara is usually depicted as an altar with its smoke 'rising' southward. However, depictions of Ara often vary in their details. In the early days of printing, a 1482 woodcut of Gaius Julius Hyginus's classic "Poeticon Astronomicon" depicts the altar as surrounded by demons. Johann Bayer in 1603 depicted Ara as an altar with burning incense; the flames rise southward as in most atlases. Hyginus also depicted Ara as an altar with burning incense, though his Ara featured devils on either side of the flames. However, Willem Blaeu, a Dutch uranographer active in the 16th and 17th centuries, drew Ara as an altar designed for sacrifice, with a burning animal offering. Unlike most depictions, the smoke from Blaeu's Ara rises northward, represented by Alpha Arae. A more unusual depiction of Ara comes from Aratus, a Greek uranographer, in 270 BCE. He drew Ara as a lighthouse, where Alpha. Beta, Epsilon, and Zeta Arae represent the base, and Eta Arae represents the flames at the lighthouse's light.
Mythology.
In ancient Greek mythology, Ara was identified as the altar where the gods first made offerings and formed an alliance before defeating the Titans. The nearby Milky Way represents the smoke rising from the offerings on the altar.
Equivalents.
In Chinese astronomy, the stars of the constellation Ara lie within "The Azure Dragon of the East" (æ±æ¹éé¾, "DÅng FÄng QÄ«ng LÃ³ng").
Namesakes.
USS Ara (AK-136) was a United States Navy Crater class cargo ship named after the constellation.
Bibliography.
Online sources

</doc>
<doc id="1928" url="http://en.wikipedia.org/wiki?curid=1928" title="Auriga">
Auriga

Auriga can refer to:

</doc>
<doc id="1930" url="http://en.wikipedia.org/wiki?curid=1930" title="Arkansas">
Arkansas

Arkansas ( ) is a state located in the Southern region of the United States. Its name is of Siouan derivation, denoting the Quapaw Indians. The state's diverse geography ranges from the mountainous regions of the Ozark and the Ouachita Mountains, which make up the U.S. Interior Highlands, to the densely forested land in the south known as the Arkansas Timberlands, to the eastern lowlands along the Mississippi River and the Arkansas Delta. Known as "the Natural State", the diverse regions of Arkansas offer residents and tourists a variety of opportunities for outdoor recreation.
Arkansas is the 29th largest in square miles and the 32nd most populous of the 50 United States. The capital and most populous city is Little Rock, located in the central portion of the state, a hub for transportation, business, culture, and government. The northwestern corner of the state, including the FayettevilleâSpringdaleâRogers Metropolitan Area and Fort Smith metropolitan area, is also an important population, education, and economic center. The largest city in the eastern part of the state is Jonesboro.
The Territory of Arkansas was admitted to the Union as the 25th state on June 15, 1836. Arkansas withdrew from the United States and joined the Confederate States of America during the Civil War. Upon returning to the Union, the state would continue to suffer due to its earlier reliance on slavery and the plantation economy, causing the state to fall behind economically and socially. White rural interests continued to dominate the state's politics until the Civil Rights movement in the mid-20th century. Arkansas began to diversify its economy following World War II and now relies on its service industry as well as aircraft, poultry, steel and tourism in addition to cotton and rice.
The culture of Arkansas is observable in museums, theaters, novels, television shows, restaurants and athletic venues across the state. Despite a plethora of cultural, economic, and recreational opportunities, Arkansas is often stereotyped as a "poor, banjo-picking hillbilly" state, a reputation dating back to early accounts of the territory by frontiersmen in the early 1800s. Arkansas's enduring image has earned the state "a special place in the American consciousness", but it has in reality produced such prominent figures as politician and educational advocate William Fulbright, former President Bill Clinton, former NATO Supreme Allied Commander, General Wesley Clark, Walmart magnate Sam Walton and singer-songwriter Johnny Cash.
Etymology.
The name Arkansas derives from the same root as the name for the state of Kansas. The Kansa tribe of Native Americans are closely associated with the Sioux tribes of the Great Plains. The word "Arkansas" itself is a French pronunciation ("Arcansas") of a Quapaw (a related "Kaw" tribe) word, "akakaze", meaning "land of downriver people" or the Sioux word "akakaze" meaning "people of the south wind".
In 1881, the pronunciation of Arkansas with the final "s" being silent was made official by an act of the state legislature after a dispute arose between Arkansas's then-two U.S. senators as one favored the pronunciation as while the other favored .
In 2007, the state legislature passed a non-binding resolution declaring the possessive form of the state's name to be "Arkansas's" which has been followed increasingly by the state government.
Geography.
Boundaries.
Arkansas borders Louisiana to the south, Texas to the southwest, Oklahoma to the west, Missouri to the north, and Tennessee and Mississippi on the east. The United States Census Bureau classifies Arkansas as a southern state, sub-categorized among the West South Central States. The Mississippi River forms most of Arkansas's eastern border, except in Clay and Greene, counties where the St. Francis River forms the western boundary of the Missouri Bootheel, and in many places where the current channel of the Mississippi has meandered from where its original legal designation. The state line along the Mississippi River is indeterminate along much of the eastern border with Mississippi due to these meanders.
Terrain.
Arkansas can generally be split into two halves, the highlands in the northwest half and the lowlands of the southeastern half. The highlands are part of the Southern Interior Highlands, including The Ozarks and the Ouachita Mountains. The southern lowlands include the Gulf Coastal Plain and the Arkansas Delta. This dual split is somewhat simplistic, however, and thus usually yields to general regions named northwest, southwest, northeast, southeast, or central Arkansas. These directionally named regions are also not defined along county lines and are also broad. Arkansas has seven distinct natural regions: the Ozark Mountains, Ouachita Mountains, Arkansas River Valley, Gulf Coastal Plain, Crowley's Ridge, and the Arkansas Delta, with Central Arkansas sometimes included as a blend of multiple regions.
The southeastern part of Arkansas along the Mississippi Alluvial Plain is sometimes called the Arkansas Delta. This region is a flat landscape of rich alluvial soils formed by repeated flooding of the adjacent Mississippi. Farther away from the river, in the southeast portion of the state, the Grand Prairie consists of a more undulating landscape. Both are fertile agricultural areas. The Delta region is bisected by an unusual geological formation known as Crowley's Ridge. A narrow band of rolling hills, Crowley's Ridge rises from above the surrounding alluvial plain and underlies many of the major towns of eastern Arkansas.
Northwest Arkansas is part of the Ozark Plateau including the Ozark Mountains, to the south are the Ouachita Mountains, and these regions are divided by the Arkansas River; the southern and eastern parts of Arkansas are called the Lowlands. These mountain ranges are part of the U.S. Interior Highlands region, the only major mountainous region between the Rocky Mountains and the Appalachian Mountains. The highest point in the state is Mount Magazine in the Ouachita Mountains; it rises to above sea level.
Hydrology.
Arkansas has many rivers, lakes, and reservoirs within or along its borders. Major tributaries of the Mississippi River include the Arkansas River, White River, and St. Francis River. The Arkansas is fed by the Mulberry River, and Fourche LaFave River in the Arkansas River Valley, which is also home to Lake Dardanelle. The Buffalo River, Little Red River, Black River and Cache River all serve as tributaries to the White River, which also empties into the Mississippi. The Saline River, Little Missouri River, Bayou Bartholomew, and the Caddo River all serve as tributaries to the Ouachita River in south Arkansas, which eventually empties into the Mississippi in Louisiana. The Red River briefly serves as the state's boundary with Texas. Arkansas has few natural lakes but many major reservoirs, including Bull Shoals Lake, Lake Ouachita, Greers Ferry Lake, Millwood Lake, Beaver Lake, Norfork Lake, DeGray Lake, and Lake Conway.
Arkansas is home to many caves, such as Blanchard Springs Caverns. More than 43,000 Native American living, hunting and tool making sites, many of them Pre-Columbian burial mounds and rock shelters, have been cataloged by the State Archeologist. Crater of Diamonds State Park near Murfreesboro is the world's only diamond-bearing site accessible to the public for digging. Arkansas is home to a dozen Wilderness Areas totaling . These areas are set aside for outdoor recreation and are open to hunting, fishing, hiking, and primitive camping. No mechanized vehicles nor developed campgrounds are allowed in these areas.
Flora and fauna.
Arkansas is divided into three broad ecoregions, the "Ozark, Ouachita-Appalachian Forests", "Mississippi Alluvial and Southeast USA Coastal Plains", and the "Southeastern USA Plains". The state is further divided into seven subregions: the Arkansas Valley, Boston Mountains, Mississippi Alluvial Plain, Mississippi Valley Loess Plain, Ozark Highlands, Ouachita Mountains, and the South Central Plains. A 2010 United States Forest Service survey determined of Arkansas's land is forestland, or 56% of the state's total area. Dominant species in Arkansas's forests include "Quercus" (oak), "Carya" (hickory), "Pinus echinata" (shortleaf pine) and "Pinus taeda" (Loblolly pine).
Arkansas's plant life varies with its climate and elevation. The pine belt stretching from the Arkansas delta to Texas consists of dense oak-hickory-pine growth. Lumbering and paper milling activity is active throughout the region. In eastern Arkansas, one can find "Taxodium " (cypress), "Quercus nigra" (water oaks), and hickories with their roots submerged in the Mississippi Valley bayous indicative of the deep south. Nearby Crowley's Ridge is only home of the tulip tree in the state, and generally hosts more northeastern plant life such as the beech tree. The northwestern highlands are covered in an oak-hickory mixture, with Ozark white cedars, "cornus" (dogwoods), and "Cercis canadensis" (redbuds) also present. The higher peaks in the Arkansas River Valley play host to scores of ferns, including the "Woodsia scopulina" and "Adiantum" (maidenhair fern) on Mount Magazine.
Climate.
Arkansas generally has a humid subtropical climate, which borders on humid continental in some northern highland areas. While not bordering the Gulf of Mexico, Arkansas is still close enough to this warm, large body of water for it to influence the weather in the state. Generally, Arkansas has hot, humid summers and cold, slightly drier winters. In Little Rock, the daily high temperatures average around with lows around in July. In January highs average around and lows around . In Siloam Springs in the northwest part of the state, the average high and low temperatures in July are and in January the average high and lows are . Annual precipitation throughout the state averages between about ; somewhat wetter in the south and drier in the northern part of the state. Snowfall is infrequent but most common in the northern half of the state. The half of the state south of Little Rock is more apt to see ice storms. Arkansas' all-time record high is at Ozark on August 10, 1936; the all-time record low is at Gravette, on February 13, 1905.
Arkansas is known for extreme weather and many storms. A typical year will see thunderstorms, tornadoes, hail, snow and ice storms. Between both the Great Plains and the Gulf States, Arkansas receives around 60 days of thunderstorms. A few of the most destructive tornadoes in U.S. history have struck the state. While being sufficiently away from the coast to be safe from a direct hit from a hurricane, Arkansas can often get the remnants of a tropical system which dumps tremendous amounts of rain in a short time and often spawns smaller tornadoes.
History.
Early Arkansas through territorial period.
Prior to European settlement of North America, Arkansas was inhabited by the Caddo, Osage, and Quapaw people. Explorers to visit the state include Hernando de Soto in 1541, Jacques Marquette and Louis Jolliet in 1673, and Robert La Salle and Henri de Tonti in 1681. Originally a Quapaw village, Arkansas Post was the first European settlement upon its establishment by de Tonti in 1686, in the name of King Louis XIV of France. As Europeans settled the east coast, many other Native American tribes were relocated to Arkansas. Settlers, including fur trappers, moved to Arkansas in the early 18th century. These people used Arkansas Post as a home base and entrepÃ´t. During the colonial period, Arkansas changed hands between France and Spain following the Seven Years' War, although neither showed interest in the remote settlement of Arkansas Post. In April 1783, Arkansas saw its only battle of the American Revolutionary War, a brief siege of the post by British Captain James Colbert with the assistance of the Choctaw and Chickasaw. The early Spanish or French explorers of the state gave it its name, which is probably a phonetic spelling of the Illinois tribe's name for the Quapaw people, who lived downriver from them.
Napoleon Bonaparte sold French Louisiana to the United States in 1803, including all of Arkansas, in a transaction known today as the Louisiana Purchase, although French soldiers remained at Arkansas Post. Following the purchase, the balanced give-and-take relationship between settlers and Native Americans began to change all along the frontier, including in Arkansas. Following a controversy over allowing slavery in the territory, the Territory of Arkansas was organized on July 4, 1819. Gradual emancipation in Arkansas was struck down by one vote, the Speaker of the House Henry Clay, allowing Arkansas to organize as a slave territory. Slavery became a wedge issue in Arkansas, forming a geographic divide that remained for decades. The owners and operators of the cotton plantation economy in southeast Arkansas firmly supported slavery, as slave labor was perceived by them to be the best or "only" economically viable method of harvesting their crop. The "hill country" of northwest Arkansas was unable to grow cotton and relied on a cash-scarce, subsistence farming economy. Native American removals began in earnest during the territorial period, with final Quapaw removal complete by 1833. The capital was relocated from Arkansas Post to Little Rock in 1821, during the territorial period.
Statehood, Civil War and Reconstruction.
When Arkansas applied for statehood, the slavery issue was again raised in Washington DC. Congress eventually approved the Arkansas Constitution after a 25-hour session, admitting Arkansas on June 15, 1836 as the 25th state and the 13th slave state, having a population of about 60,000. Arkansas struggled with taxation to support its new state government, a problem made worse by a state banking scandal and worse yet by the Panic of 1837. In early antebellum Arkansas, the southeast Arkansas economy grew rapidly on the backs of slaves. On the eve of the Civil War in 1860, enslaved African Americans numbered 111,115 people, just over 25% of the state's population. However, plantation agriculture would ultimately set the state and region behind the nation for decades. The growth of southeast Arkansas also caused a rift to form between the northwest and southeast.
Many politicians were elected to office from the Family, the Southern rights political force in antebellum Arkansas, but the populace generally wanted to avoid a civil war. When the Gulf states seceded in early 1861, Arkansas voted to remain in the Union. Arkansas did not secede until Abraham Lincoln demanded Arkansas troops be sent to Fort Sumter to quell the rebellion there. The following month a state convention voted to terminate Arkansas's membership in the Union and join the Confederate States of America. Arkansas held a very important position for the Rebels, maintaining control of the Mississippi River and surrounding Southern states. The bloody Battle of Wilson's Creek just across the border in Missouri shocked many Arkansans who thought the war would be a quick and decisive Southern victory. Battles early in the war took place in northwest Arkansas, including the Battle of Cane Hill, Battle of Pea Ridge, and Battle of Prairie Grove. Union General Samuel Curtis swept across the state to Helena in 1862. Little Rock was captured the following year, forcing the Confederate capital to move to Hot Springs, and then again to Washington from 1863-1865, for the remainder of the war. Throughout the state, guerrilla warfare ravaged the countryside and destroyed cities. Passion for the Confederate cause waned after implementation of unpopular programs like a draft, high taxes, and martial law.
Under the Military Reconstruction Act, Congress declared Arkansas restored to the Union in June 1868. The Republican-controlled reconstruction legislature established universal male suffrage (though temporarily disfranchising all former Confederates, who were mostly Democrats), a public education system, and passed general issues to improve the state and help more of the population. The state soon came under almost exclusive control of the Radical Republicans, (those who moved in from the North being derided as "carpetbaggers" based on allegations of corruption), and led by Governor Powell Clayton, they presided over a time of great upheaval and racial violence in the state between Republican state militia and the Ku Klux Klan.
In 1874, the Brooks-Baxter War, a political struggle between factions of the Republican Party shook Little Rock and the state governorship. It was settled only when President Ulysses S. Grant ordered Joseph Brooks to disperse his militant supporters.
Following the Brooks-Baxter War, a new state constitution was ratified, re-enfranchising former Confederates.
In 1881, the Arkansas state legislature enacted a bill that adopted an official pronunciation of the state's name, to combat a controversy then simmering. (See Law and Government below.)
After Reconstruction, the state began to receive more immigrants and migrants. Chinese, Italian, and Syrian men were recruited for farm labor in the developing Delta region. None of these nationalities stayed long at farm labor; the Chinese especially quickly became small merchants in towns around the Delta. Some early 20th-century immigration included people from eastern Europe. Together, these immigrants made the Delta more diverse than the rest of the state. In the same years, some black migrants moved into the area because of opportunities to develop the bottomlands and own their own property. Many Chinese became such successful merchants in small towns that they were able to educate their children at college.
Construction of railroads enabled more farmers to get their products to market. It also brought new development into different parts of the state, including the Ozarks, where some areas were developed as resorts. In a few years at the end of the 19th century, for instance, Eureka Springs in Carroll County grew to 10,000 people, rapidly becoming a tourist destination and the fourth-largest city of the state. It featured newly constructed, elegant resort hotels and spas planned around its natural springs, considered to have healthful properties. The town's attractions included horse racing and other entertainment. It appealed to a wide variety of classes, becoming almost as popular as Hot Springs.
In the late 1880s, the worsening agricultural depression catalyzed Populist and third party movements, leading to interracial coalitions. Struggling to stay in power, in the 1890s the Democrats in Arkansas followed other Southern states in passing legislation and constitutional amendments that disfranchised blacks and poor whites. Democrats wanted to prevent their alliance. In 1891 state legislators passed a requirement for a literacy test, knowing that many blacks and whites would be excluded, at a time when more than 25% of the population could neither read nor write. In 1892 they amended the state constitution to include a poll tax and more complex residency requirements, both of which adversely affected poor people and sharecroppers, and forced them from voter rolls.
By 1900 the Democratic Party expanded use of the white primary in county and state elections, further denying blacks a part in the political process. Only in the primary was there any competition among candidates, as Democrats held all the power. The state was a Democratic one-party state for decades, until after the federal Civil Rights Act of 1964 and Voting Rights Act of 1965 were passed by Congress.
Between 1905 and 1911, Arkansas began to receive a small immigration of German, Slovak, and Scots-Irish from Europe. The German and Slovak peoples settled in the eastern part of the state known as the Prairie, and the Irish founded small communities in the southeast part of the state. The Germans were mostly Lutheran and the Slovaks were primarily Catholic. The Irish were mostly Protestant from Ulster, of Scots and Northern Borders descent.
After the Supreme Court's decision in "Brown "v." Board of Education of Topeka, Kansas" in 1954, the Little Rock Nine brought Arkansas to national attention when the Federal government intervened to protect African-American students trying to integrate a high school in the Arkansas capital. Governor Orval Faubus ordered the Arkansas National Guard to aid segregationists in preventing nine African-American students from enrolling at Little Rock's Central High School. After attempting three times to contact Faubus, President Dwight D. Eisenhower sent 1000 troops from the active-duty 101st Airborne Division to escort and protect the African-American students as they entered school on September 25, 1957. In defiance of federal court orders to integrate, the governor and city of Little Rock decided to close the high schools for the remainder of the school year. By the fall of 1959, the Little Rock high schools were completely integrated.
Bill Clinton, the 42nd President of the United States, was born in Hope, Arkansas. Before his presidency, Clinton served as the 40th and 42nd Governor of Arkansas, a total of nearly 12 years.
Cities and towns.
Little Rock has been Arkansas's capital city since 1821 when it replaced Arkansas Post as the capitol of the Territory of Arkansas. The state capitol was moved to Hot Springs and later Washington during the Civil War when the Union armies threatened the city in 1862, and state government did not return to Little Rock until after the war ended. Today, the Little RockâNorth Little RockâConway metropolitan area is the largest in the state, with a population of 724,385 in 2013.
The FayettevilleâSpringdaleâRogers Metropolitan Area is the second-largest metropolitan area in Arkansas, growing at the fastest rate due to the influx of businesses and the growth of the University of Arkansas and Walmart.
The state has nine cities with populations above 50,000 (based on 2010 census). In descending order of size they are Little Rock, Fort Smith, Fayetteville, Springdale, Jonesboro, North Little Rock, Conway, Rogers, and Pine Bluff. Of these, only Fort Smith and Jonesboro are outside the two largest metropolitan areas. Other notable cities include Hot Springs, Bentonville, Texarkana, Sherwood, Jacksonville, Russellville, Bella Vista, West Memphis, Paragould, Cabot, Searcy, Van Buren, El Dorado, Blytheville, Harrison, and Mountain Home.
Demographics.
Population.
The United States Census Bureau estimates that the population of Arkansas was 2,949,132 on July 1, 2012, a 1.1% increase since the 2010 United States Census.
As of 2012, Arkansas has an estimated population of 2,949,132. From fewer than 15,000 in 1820, Arkansas's population grew to 52,240 during a special census in 1835, far exceeding the 40,000 required to apply for statehood. Following statehood in 1836, the population doubled each decade until the 1870 Census conducted following the Civil War. The state recorded growth in each successive decade, although slowing until recording losses in the 1950 and 1960 Censuses. This outmigration was a result of multiple factors, including mechanization on the farm reducing the number of laborers needed and young educated people leaving the state due to a lack of non-farming industry in the state. Arkansas again began to grow, recording positive growth rates ever since and exceeding the 2 million mark during the 1980 Census. Arkansas's current rate of change, age distributions, and gender distributions mirror national averages. Minority group data also approximates national averages, with the exception of persons of Hispanic or Latino origin approximately 10% below the national percentage in Arkansas. The center of population of Arkansas for 2000 was located in Perry County, near Nogal.
Race and ancestry.
In terms of race and ethnicity, the state was 80.1% White (74.2% non-Hispanic White), 15.6% Black or African American, 0.9% American Indian and Alaska Native, 1.3% Asian, and 1.8% from Two or More Races. Hispanics or Latinos of any race made up 6.6% of the population.
As of 2011, 39.0% of Arkansas's population younger than age 1 were minorities.
European Americans have a strong presence in the northwestern Ozarks and the central part of the state. African Americans live mainly in the southern and eastern parts of the state. Arkansans of Irish, English and German ancestry are mostly found in the far northwestern Ozarks near the Missouri border. Ancestors of the Irish in the Ozarks were chiefly Scots-Irish, Protestants from Northern Ireland, the Scottish lowlands and northern England part of the largest group of immigrants from Great Britain and Ireland before the American Revolution. English and Scots-Irish immigrants settled throughout the backcountry of the South and in the more mountainous areas. Americans of English stock are found throughout the state.
The principal ancestries of Arkansas's residents in 2010 were surveyed to be the following:
Many of the people of American ancestry have some English descent and some have Scots-Irish descent. Their families have been in the state so long, in many cases since before statehood, that they choose to identify simply as having American ancestry or do not in fact know their own ancestry. Their ancestry primarily goes back to the original 13 colonies and for this reason many of them today simply claim American ancestry. Many people who identify themselves as Irish descent are in fact of Scots-Irish descent.
According to the 2006â2008 American Community Survey, 93.8% of Arkansas' population (over the age of five) spoke only English at home. About 4.5% of the state's population spoke Spanish at home. About 0.7% of the state's population spoke any other Indo-European languages. About 0.8% of the state's population spoke an Asian language, and 0.2% spoke other languages.
Religion.
Arkansas, like most other Southern states, is part of the Bible Belt and is predominantly Protestant. The largest denominations by number of adherents in 2010 were the Southern Baptist Convention with 661,382; the United Methodist Church with 158,574; non-denominational Evangelical Protestants with 129,638; and the Catholic Church with 122,662. However, there are some residents of the state who live by other religions such as Wiccan, Pagan, Islam, Hinduism, Buddhism and still others who prefer no religious denomination.
Economy.
Once a state with a cashless society in the uplands and plantation agriculture in the lowlands, Arkansas's economy has evolved and diversified to meet the needs of today's consumer. The state's gross domestic product (GDP) was $105 billion in 2010. Six Fortune 500 companies are based in Arkansas, including the world's #1 retailer, Walmart. The per capita personal income in 2010 was $36,027, ranking forty-fifth in the nation. The three-year median household income from 2009-11 was $39,806, ranking forty-ninth in the nation. The state's agriculture outputs are poultry and eggs, soybeans, sorghum, cattle, cotton, rice, hogs, and milk. Its industrial outputs are food processing, electric equipment, fabricated metal products, machinery, and paper products. Mines in Arkansas produce natural gas, oil, crushed stone, bromine, and vanadium. According to CNBC, Arkansas currently ranks as the 20th best state for business, with the 2nd-lowest cost of doing business, 5th-lowest cost of living, 11th best workforce, 20th-best economic climate, 28th-best educated workforce, 31st-best infrastructure and the 32nd-friendliest regulatory environment. Arkansas gained twelve spots in the best state for business rankings since 2011. As of 2014, Arkansas was found to be the most affordable US state to live in.
As of April 2013 the state's unemployment rate is 7.5%
Industry and commerce.
Arkansas's earliest industries were fur trading and agriculture, with development of cotton plantations in the areas near the Mississippi River. They were dependent on slave labor through the American Civil War.
Today only approximately 3% of the population is employed in the agricultural sector, it remains a major part of the state's economy, ranking 13th in the nation in the value of products sold. The state is the U.S.'s largest producer of rice, broilers, and turkeys, and ranks in the top three for cotton, pullets, and aquaculture (catfish). Forestry remains strong in the Arkansas Timberlands, and the state ranks fourth nationally and first in the South in softwood lumber production. In recent years, automobile parts manufacturers have opened factories in eastern Arkansas to support auto plants in other states. Bauxite was formerly a large part of the state's economy, mined mostly around Saline County.
Tourism is also very important to the Arkansas economy; the official state nickname "The Natural State" was created for state tourism advertising in the 1970s, and is still used to this day. The state maintains 52 state parks and the National Park Service maintains seven properties in Arkansas. The completion of the William Jefferson Clinton Presidential Library in Little Rock has drawn many visitors to the city and revitalized the nearby River Market District. Many cities also hold festivals which draw tourists to the culture of Arkansas, such as King Biscuit Blues Festival, Ozark Folk Festival, Toad Suck Daze, and Tontitown Grape Festival.
Culture.
The culture of Arkansas is available to all in various forms, whether it be architecture, literature, or fine and performing arts. The state's culture also includes distinct cuisine, dialect, and traditional festivals. Sports are also very important to the culture of Arkansas, ranging from football, baseball, and basketball to hunting and fishing. Perhaps the best-known piece of Arkansas's culture is the stereotype of its citizens as shiftless hillbillies. The reputation began when the state was characterized by early explorers as a savage wilderness full of outlaws and thieves. The most enduring icon of Arkansas's hillbilly reputation is "The Arkansas Traveller", a painted depiction of a folk tale from the 1840s. Although intended to represent the divide between rich southeastern plantation Arkansas planters and the poor northwestern hill country, the meaning was twisted to represent a Northerner lost in the Ozarks on a white horse asking a backwoods Arkansan for directions. The state also suffers from the racial stigma common to former Confederate states, with historical events such as the Little Rock Nine adding to Arkansas's enduring image.
Art and history museums display pieces of cultural value for Arkansans and tourists to enjoy. Crystal Bridges Museum of American Art in Bentonville is the most popular with 604,000 visitors in 2012, its first year. The museum includes walking trails and educational opportunities in addition to displaying over 450 works covering five centuries of American art. Several historic town sites have been restored as Arkansas state parks, including Historic Washington State Park, Powhatan Historic State Park, and Davidsonville Historic State Park.
Arkansas features a variety of native music across the state, ranging from the blues heritage of West Memphis and Helena-West Helena to rockabilly, bluegrass, and folk music from the Ozarks. Festivals such as the King Biscuit Blues Festival and Bikes, Blues, and BBQ pay homage to the history of blues in the state. The Ozark Folk Festival in Mountain View is a celebration of Ozark culture and often features folk and bluegrass musicians. Literature set in Arkansas such as "I Know Why the Caged Bird Sings" by Maya Angelou and "A Painted House" by John Grisham describe the culture at various time periods.
Sports and recreation.
Sports are an integral part of the culture of Arkansas, and her residents enjoy participating in and spectating various events throughout the year. One of the oldest sports in Arkansas is hunting. The state created the Arkansas Game and Fish Commission in 1915 to regulate and enforce hunting. Today a significant portion of Arkansas's population participates in hunting duck in the Mississippi flyway and deer across the state. Millions of acres of public land are available for both bow and modern gun hunters.
Fishing has always been popular in Arkansas, and the sport and the state have benefited from the creation of reservoirs across the state. Following the completion of Norfork Dam, the Norfork Tailwater and the White River have become a destination for trout fishers. Several smaller retirement communities such as Bull Shoals, Hot Springs Village, and Fairfield Bay have flourished due to their position on a fishing lake. The Buffalo National River has been preserved in its natural state by the National Park Service and is frequented by fly fishers annually.
Football, especially collegiate football, has always been important to Arkansans. College football in Arkansas began from humble beginnings. The University of Arkansas first fielded a team in 1894 when football was a very dangerous game. Calling the Hogs is a cheer that shows support for the Razorbacks, one of the two FBS teams in the state. High school football also began to grow in Arkansas in the early 20th century. Over the years, many Arkansans have looked to the Razorbacks football team as the public image of the state. Following the Little Rock Nine integration crisis at Little Rock Central High School, Arkansans looked to the successful Razorback teams in the following years to repair the state's reputation. Although the University of Arkansas is based in Fayetteville, the Razorbacks have always played at least two games per season at War Memorial Stadium in Little Rock in an effort to keep fan support in central and south Arkansas. Arkansas State University joined the University of Arkansas in the Football Bowl Subdivision in 1992 after playing in lower divisions for nearly two decades. However, the two schools have never played each other, due to the University of Arkansas' policy of not playing intrastate games. Six of Arkansas' smaller colleges play in the Great American Conference, with University of Arkansas at Pine Bluff playing in the Southwestern Athletic Conference and University of Central Arkansas competing in the Southland Conference.
Baseball runs deep in Arkansas and has been popular since before the state hosted Major League Baseball (MLB) spring training in Hot Springs from 1886-1920s. Today, two minor league teams are based in the state. The Arkansas Travelers play at Dickey-Stephens Park in North Little Rock, and the Northwest Arkansas Naturals play in Arvest Ballpark in Springdale. Both teams compete in the Texas League.
Health.
Arkansans, as with many Southern states, have a high incidence of premature death, infant mortality, cardiovascular deaths, and occupational fatalities compared to the rest of the United States. The state is tied for 43rd with New York in percentage of adults who regularly exercise. Arkansas is usually ranked as one of the least healthy states due to high obesity, smoking, and sedentary lifestyle rates. The state also has an uninsured rate of 18%, ranking it 37th in the nation. Uninsured individuals often obtain care that is more expensive and less effective, increasing the cost of health care across the state and compounding the problem.
The Arkansas Clean Indoor Air Act went into effect in 2006, a statewide smoking ban excluding bars and some restaurants.
Healthcare in Arkansas is provided by a network of hospitals as members of the Arkansas Hospital Association. Major institutions with multiple branches include Baptist Health, Community Health Systems, and HealthSouth. The University of Arkansas for Medical Sciences (UAMS) in Little Rock operates the UAMS Medical Center, a teaching hospital ranked as high performing nationally in cancer and nephrology. The pediatric division of UAMS Medical Center is known as Arkansas Children's Hospital, nationally ranked in pediatric cardiology and heart surgery. Together, these two institutions are the state's only Level I trauma centers.
A Gallup poll demonstrates that Arkansas made the most immediate progress in reducing its number of uninsured residents following the passage of the Affordable Care Act. The percentage of uninsured in Arkansas dropped from 22.5 percent in 2013 to 12.4 percent in August 2014.
Education.
Arkansas ranks as the 32nd smartest state on the Morgan Quitno Smartest State Award, 44th in percentage of residents with at least a high school diploma, and 48th in percentage of bachelor's degree attainment. However, Arkansas has been making major strides recently in education reform. "Education Week" has praised the state, ranking Arkansas in the top 10 of their Quality Counts Education Rankings every year since 2009 while scoring it in the top 5 during 2012 and 2013. Arkansas specifically received an A in Transition and Policy Making for progress in this area consisting of early-childhood education, college readiness, and career readiness. Governor Mike Beebe has made improving education a major issue through his attempts to spend more on education. Through reforms, the state is now a leader in requiring curricula designed to prepare students for postsecondary education, rewarding teachers for student achievement, and providing incentives for principals who work in lower-tier schools.
In 2010 Arkansas students earned an average score of 20.3 on the ACT exam, just below the national average of 21. These results were expected due to the large increase in the number of students taking the exam since the establishment of the Academic Challenge Scholarship. Top high schools receiving recognition from the U.S. News & World Report are spread across the state, including Haas Hall Academy in Fayetteville, KIPP Delta Collegiate in Helena-West Helena, Bentonville, Rogers, Rogers Heritage, Valley Springs, Searcy, and McCrory. A total of 81 Arkansas high schools were ranked by the U.S. News & World Report in 2012.
The state supports a network of public universities and colleges, including two major university systems: Arkansas State University System and University of Arkansas System. The University of Arkansas, flagship campus of the University of Arkansas System in Fayetteville was ranked #63 among public schools in the nation by "U.S. News & World Report". Other public institutions include Arkansas Tech University, Henderson State University, Southern Arkansas University, and University of Central Arkansas across the state. It is also home to 11 private colleges and universities including Hendrix College, one of the nation's top 100 liberal arts colleges, according to U.S. News & World Report.
Transportation.
Transportation in Arkansas is overseen by the Arkansas State Highway and Transportation Department (AHTD), headquartered in Little Rock. Several main corridors pass through Little Rock, including InterstateÂ 30 (I-30) and I-40 (the nationâs 3rd-busiest trucking corridor). In northeast Arkansas, I-55 travels north from Memphis to Missouri, with a new spur to Jonesboro (I-555). Northwest Arkansas is served by I-540 from Fort Smith to Bella Vista, which is a segment of future I-49. The state also has the 13th largest state highway system in the nation.
Arkansas is served by of railroad track divided among twenty-six railroad companies including three Class I railroads. Freight railroads are concentrated in southeast Arkansas to serve the industries in the region. The Texas Eagle, an Amtrak passenger train, serves five stations in the state Walnut Ridge, Little Rock, Malvern, Arkadelphia, and Texarkana.
Arkansas also benefits from the use of its rivers for commerce. The Mississippi River and Arkansas River are both major rivers. The United States Army Corps of Engineers maintains the McClellan-Kerr Arkansas River Navigation System, allowing barge traffic up the Arkansas River to the Port of Catoosa in Tulsa, Oklahoma.
There are four airports with commercial service: Little Rock National Airport, Northwest Arkansas Regional Airport, Fort Smith Regional Airport, and Texarkana Regional Airport, with dozens of smaller airports in the state.
Public transit and community transport services for the elderly or those with developmental disabilities are provided by agencies such as the Central Arkansas Transit Authority and the Ozark Regional Transit, organizations that are part of the Arkansas Transit Association.
Law and government.
As with the federal government of the United States, political power in Arkansas is divided into three branches: executive, legislative, and judicial. Each officer's term is four years long. Office holders are term-limited to two full terms plus any partial terms before the first full term.
Executive.
The current Governor of Arkansas is Mike Beebe, a Democrat, who was elected on November 7, 2006. Beebe was reelected to his second and final term in 2010 which will expire January 13, 2015. The six other elected executive positions in Arkansas are lieutenant governor, secretary of state, attorney general, treasurer, auditor, and land commissioner. The governor also appoints qualified individuals to lead various state boards, committees, and departments. Arkansas governors served two-year terms until a referendum lengthened the term to four years, effective with the 1986 general election.
In Arkansas, the lieutenant governor is elected separately from the governor and thus can be from a different political party.
Legislative.
The Arkansas General Assembly is the state's bicameral bodies of legislators, composed of the Senate and House of Representatives. The Senate contains 35 members from districts of approximately equal population. These districts are redrawn decennially with each US census, and in election years ending in "2", the entire body is put up for reelection. Following the election, half of the seats are designated as two-year seats and will be up for reelection again in two years, these "half-terms" do not count against a legislator's term limits. The remaining half serve a full four-year term. This staggers elections such that half the body is up for re-election every two years and allows for complete body turnover following redistricting. Arkansas voters selected a 21-14 Republican majority in the Senate in 2012. Arkansas House members can serve a maximum of three two-year terms. House districts are redistricted by the Arkansas Board of Apportionment. Following the 2012 elections, Republicans gained a 51-49 majority in the House of Representatives.
The Republican Party majority status in the Arkansas State House of Representatives following the 2012 elections is the party's first since 1874. Arkansas was the last state of the old Confederacy to never have Republicans control either chamber of its house since the Civil War.
Following the term limits changes, studies have show that lobbyist have become less influential in state politics, but legislative staff, not subject to term limits, have acquired additional power and influence due to the high rate of elected official turnover.
Judicial.
Arkansas's judicial branch has five court systems: Arkansas Supreme Court, Arkansas Court of Appeals, Circuit Courts, District Courts and City Courts.
Most cases begin in district court, which is subdivided into state district court and local district court. State district courts exercise district-wide jurisdiction over the districts created by the General Assembly, and local district courts are presided over by part-time judges who may privately practice law. There are currently 25 state district court judges presiding over 15 districts, with more districts to be created in 2013 and 2017. There are 28 judicial circuits of Circuit Court, with each contains five subdivisions: criminal, civil, probate, domestic relations, and juvenile court. The jurisdiction of the Arkansas Court of Appeals is determined by the Arkansas Supreme Court, and there is no right of appeal from the Court of Appeals to the high court. However, the Arkansas Supreme Court can review Court of Appeals cases upon application by either a party to the litigation, upon request by the Court of Appeals, or if the Arkansas Supreme Court feels the case should have been initially assigned to it. The twelve judges of the Arkansas Court of Appeals are elected from judicial districts to renewable six-year terms.
The Arkansas Supreme Court is the court of last resort in the state, composed of seven justices elected to eight-year terms. Established by the Arkansas Constitution in 1836, the court's decisions can be appealed to only the Supreme Court of the United States.
Federal.
One of Arkansas's U.S. Senators is Democrat Mark Pryor, and the other one is Republican John Boozman. The state has four seats in U.S. House of Representatives. All four seats are held by Republicans: Rick Crawford (1st district), Tim Griffin (2nd district), Steve Womack (3rd district), and Tom Cotton (4th district).
Politics.
Arkansas Governor Bill Clinton brought national attention to the state with a long speech at the 1988 Democratic National Convention endorsing Michael Dukakis. Pundits suggested the speech would ruin Clinton's political career, but instead, Clinton won the Democratic nomination for President the following cycle. Presenting himself as a "New Democrat" and using incumbent George H. W. Bush's against him, Clinton won the 1992 presidential election (43.0% of the vote) against Republican Bush (37.4% of the vote) and billionaire populist Ross Perot, who ran as an independent (18.9% of the vote).
Most Republican strength lies mainly in the northwestern part of the state, particularly Fort Smith and Bentonville, as well as North Central Arkansas around the Mountain Home area. In the latter area, Republicans have been known to get 90 percent or more of the vote. The rest of the state is more Democratic. Arkansas has only elected two Republicans to the U.S. Senate since Reconstruction, Tim Hutchinson, who was defeated after one term by Mark Pryor and John Boozman, who defeated incumbent Blanche Lincoln. Prior to 2013, the General Assembly had not been controlled by the Republican Party since Reconstruction with the GOP holding a 51-seat majority in the state House and a 21-seat (of 35) in the state Senate following victories in 2012. Arkansas was one of just three states among the states of the former Confederacy that sent two Democrats to the U.S. Senate (the others being Florida and Virginia) during the first decade of the 21st century.
In 2010, Republicans captured three of the state's four seats in the U.S. House of Representatives. In 2012, Republicans won election for all four House seats. Arkansas holds the distinction of having a U.S. House delegation composed of military veterans (Rick Crawford - Army; Tim Griffin - Army Reserve; Steve Womack - Army National Guard, Tom Cotton- Army).
Reflecting the state's large evangelical population, the state has a strong social conservative bent. Under the Arkansas Constitution Arkansas is a right to work state, its voters passed a ban on same-sex marriage with 75% voting yes, and the state is one of a handful with legislation on its books banning abortion in the event "Roe v. Wade" is ever overturned.
Attractions.
Arkansas is home to many areas protected by the National Park System. These include:

</doc>
<doc id="1931" url="http://en.wikipedia.org/wiki?curid=1931" title="Atmosphere (disambiguation)">
Atmosphere (disambiguation)

An atmosphere is a gas layer around a celestial body.
Atmosphere may also refer to:

</doc>
<doc id="1933" url="http://en.wikipedia.org/wiki?curid=1933" title="Apus">
Apus

Apus is a faint constellation in the southern sky, first defined in the late 16th century. Its name means "no feet" in Greek, and it represents a bird-of-paradise (which were once believed to lack feet). It is bordered by Triangulum Australe, Circinus, Musca, Chamaeleon, Octans, Pavo and Ara. Its genitive is "Apodis".
History.
Apus was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman and it first appeared on a 35Â cm diameter celestial globe published in 1597 (or 1598) in Amsterdam by Plancius with Jodocus Hondius. Plancius called the constellation "Paradysvogel Apis Indica"; the first word is Dutch for 'bird of paradise,' of genus Pteridophora, but the others are Latin for "Indian Bee," although "apis" (Latin for "bee") is presumably an error for "avis" or "bird".
The name "Apus" is derived from the Greek "apous", meaning "without feet", which referred to the Western conception of a bird-of-paradise as one without feet, a misconception perpetuated by the fact that the only specimens available in the West had both feet and wings removed. These specimens began to arrive in Europe in 1522, when the survivors of Ferdinand Magellan's expedition brought them home.
After its introduction on Plancius's globe, the first known depiction of the constellation in a celestial atlas was in Johann Bayer's "Uranometria" of 1603, where it was called "Apis Indica".
It was suggested by Richard Allen that Houtmann, who observed the southern constellations from the island of Sumatra, took his ideas for the formation of Apus (as well as Phoenix and Indus) from the Chinese, who knew these stars as the "Little Wonder Bird.". Ridpath, however, disputes this possibility, arguing that the southern constellations were introduced later than Allen believed, and by different people altogether.
Notable features.
The most prominent deep-sky objects in Apus include the globular clusters NGC 6101 and IC 4499 as well as the spiral galaxy IC 4633.
Equivalents.
When the Ming Dynasty Chinese astronomer Xu Guangqi adapted the European southern hemisphere constellations to the Chinese system in "The Southern Asterisms", he combined Apus with some of the stars in Octans to form the "Exotic Bird" (ç°é, "YÃ¬quÃ¨").

</doc>
<doc id="1934" url="http://en.wikipedia.org/wiki?curid=1934" title="Abadan, Iran">
Abadan, Iran

ÃbÃ¢dÃ¢n () is a city in and the capital of Abadan County, Khuzestan Provincewhich in located in central west of Iran. It lies on Abadan Island ( long, 3â19Â km or 2â12Â miles wide, the island is bounded in the west by the Arvand waterway and to the east by the Bahmanshir outlet of the Karun River otherwise known as Shatt al-Arab), from the Persian Gulf, near the Iraqi-Iran border.
Etymology.
The earliest mention of the island of Abadan, if not the port itself is found in works of the geographer Marcian, who renders the name "Apphadana". Earlier, the classical geographer, Ptolemy notes "Apphana" as an island off the mouth of the Tigris (which is, where the modern Island of Abadan is located). An etymology for this name is presented by 'B. Farahvashi" to be derived from the Persian word "ab" (water) and the root "pÄ" (guard, watch) thus "coastguard station").
In the Islamic times, a pseudo-etymology was produced by the historian Ahmad ibn Yahya al-Baladhuri (d.892) quoting a folk story that the town was presumably founded by one ""Abbad bin Hosayn" from the Arabian Tribe of Banu Tamim", who established a garrison there during the governorship of "Hajjaj" in the Ummayad period.
In the subsequent centuries, the Persian version of the name had begun to come into general use before it was adopted by official decree in 1935.
Population.
The civilian population of the city dropped close to zero during the eight years of the IranâIraq War (1980â88). The 1986 census recorded only 6 people. In 1991, 84,774 had returned to live in the city. By 2001, the population had jumped to 206,073, and it was 217,988, in 48,061 families, according to 2006 census. Abadan Refinery is one of the largest in the world.
Only 9% of managers (of the oil company) were from Khuzestan. The proportion of natives of Tehran, the Caspian, Azarbaijan and Kurdistan rose from 4% of blue collar workers to 22% of white collar workers to 45% of managers. Thus while Persian-speakers were concentrated on the lower rungs of the work force, managers tended to be brought in from some distance.
History.
Abadan is thought to have been further developed into a major port city under the Abbasids' rule. In this time period, it was a commercial source of salt and woven mats. The siltation of the river delta forced the town further away from water; In the 14th century, however, Ibn Battutah described Abadan just as a small port in a flat salty plain. Politically, Abadan was often the subject of dispute between the nearby states; in 1847, Persia acquired it from Turkey, in which state Abadan has remained since. From the 17th century onward, the island of Abadan was part of the lands of the Arab "Ka'ab" (Bani Kaab) tribe. One section of this tribe, "Mohaysen", had its headquarters at "Mohammara"(present-day Khorramshahr), until the removal of Shaikh Khaz'al Khan in 1924.
It was not until the 20th century that rich oil fields were discovered in the area. On 16 July 1909, after secret negotiation with the British consul, Percy Cox assisted by Arnold Wilson, Sheik Khaz'al agreed to a rental agreement for the island including Abadan. The Sheik continued to administer the island until 1924. The Anglo-Persian Oil Company built their first pipeline terminus oil refinery in Abadan, starting in 1909 and completing it in 1912, with oil flowing by August 1912 (see Abadan Refinery). Refinery throughput numbers rose from 33,000 tons in 1912-1913 to 4,338,000 tons in 1931. By 1938, it was the largest in the world.
During World War II, Abadan was a major logistics center for Lend-Lease aircraft being sent to the Soviet Union by the United States.
In 1951, Iran nationalized all oil properties and refining ground to a stop on the island. Rioting broke out in Abadan, after the government had decided to nationalize the oil facilities, and 3 British workers were killed. It wasn't until 1954, that a settlement was reached, which allows a group of international oil companies to manage the production and refining on the island. This continued until 1973, when the NIOC took over all facilities. After total nationalization, Iran focused on supplying oil domestically and built a pipeline from Abadan to Tehran.
Whereas Abadan was not a major cultural or religious center, it did play an important role in the Islamic Revolution. On 19 August 1978âthe anniversary of the US backed pro-Shah coup d'Ã©tat which overthrew the nationalists and popular Iranian prime minister, Dr. Mohammed Mossadeghâ the Cinema Rex, a movie theatre in Abadan, Iran, was set ablaze. The Cinema Rex Fire was the site of 430 deaths, but more importantly, it was another event that kept the Islamic Revolution moving ahead. At the time there was a lot of confusion and misinformation about the incident; however the public blamed the local police chief and also the Shah and SAVAK. The reformist Sobhe Emrooz newspaper in one of its editorials revealed that the Cinema Rex was burned down by the radical Islamists. The newspaper was shut down immediately after. Over time, the true culprits, radical Islamists, were apprehended and the logic behind this act was revealed, as they were trying both to foment the general public to distrust the government even more, and also as they perceived cinema as a link to the Americans. This fire was one of four during a short period in August, with other fires in Mashhad, Rizaiya, and Shiraz.
In September 1980, Abadan was almost overrun during a surprise attack on Khuzestan by Iraq, marking the beginning of the IranâIraq War. For 12 months Abadan was besieged, but never captured, by Iraqi forces, and in September 1981, the Iranians broke the siege of Abadan. Much of the city, including the oil refinery which was the world's largest refinery with capacity of 628,000 barrels per day, was badly damaged or destroyed by the siege and by bombing. Previous to the war, the city's civilian population was about 300,000, but before it was over, most of the populace had sought refuge elsewhere in Iran.
After the war, the biggest concern was the rebuilding of Abadan's oil refinery, as they were operating at 10% of capacity due to damage. In 1993, the refinery began limited operation & and the port reopened. By 1997, the refinery reached the same rate of production it was at before the war. Recently, Abandan has been the site of major labor activity as workers at the oil refineries in the city have staged walkouts and strikes to protest non-payment of wages and the political situation in the country.
Recent events.
To honor the 100th anniversary of the refining of oil in Abadan, city officials are planning an oil museum. The Abadan oil refinery was featured on the reverse side of Iran's 100-rial banknotes printed in 1965 and from 1971 to 1973.
Places of interest.
The Abadan Institute of Technology was established in Abadan in 1939. The school specialized in engineering and petroleum chemistry, and was designed to train staff for the refinery in town. The school's name has since changed several times, but since 1989 has been considered a branch campus of the Petroleum University of Technology, centered in Tehran.
There is an international airport in Abadan. It is represented by the IATA airport code ABD.
Climate.
The climate in Abadan is arid (KÃ¶ppen climate classification "BWh") and similar to Baghdad's, but slightly hotter due to Abadan's lower latitude. Summers are dry and extremely hot, with temperatures above 45 degrees almost daily. Winters are mildly wet and spring-like, though subject to cold spells. Winter temperatures are around 16â20 degrees. The world's highest unconfirmed temperature was a temperature flare up during a heat burst in June 1967, with a temperature of . Reliable measurements in the city range from .
References.
 

</doc>
<doc id="1935" url="http://en.wikipedia.org/wiki?curid=1935" title="Attorney">
Attorney

An attorney is a person who takes the place of another in legal contexts. Accordingly, it may refer to:

</doc>
<doc id="1937" url="http://en.wikipedia.org/wiki?curid=1937" title="Alexander Fleming">
Alexander Fleming

Sir Alexander Fleming, FRSE, FRS, FRCS(Eng) (6 August 188111 March 1955) was a Scottish biologist, pharmacologist and botanist. He wrote many articles on bacteriology, immunology, and chemotherapy. His best-known discoveries are the enzyme lysozyme in 1923 and the antibiotic substance penicillin from the mould "Penicillium notatum" in 1928, for which he shared the Nobel Prize in Physiology or Medicine in 1945 with Howard Florey and Ernst Boris Chain.
Biography.
Early life and education.
Fleming was born on 6 August 1881 at Lochfield farm near Darvel, in Ayrshire, Scotland. He was the third of the four children of farmer Hugh Fleming (1816â1888) from his second marriage to Grace Stirling Morton (1848â1928), the daughter of a neighbouring farmer. Hugh Fleming had four surviving children from his first marriage. He was 59 at the time of his second marriage, and died when Alexander (known as Alec) was seven.
Fleming went to Loudoun Moor School and Darvel School, and earned a two-year scholarship to Kilmarnock Academy before moving to London, where he attended the Royal Polytechnic Institution. After working in a shipping office for four years, the twenty-year-old Fleming inherited some money from an uncle, John Fleming. His elder brother, Tom, was already a physician and suggested to his younger sibling that he should follow the same career, and so in 1903, the younger Alexander enrolled at St Mary's Hospital Medical School in Paddington; he qualified with an MBBS degree from the school with distinction in 1906.
Fleming had been a private in the London Scottish Regiment of the Volunteer Force since 1900, and had been a member of the rifle club at the medical school. The captain of the club, wishing to retain Fleming in the team suggested that he join the research department at St Mary's, where he became assistant bacteriologist to Sir Almroth Wright, a pioneer in vaccine therapy and immunology. In 1908, he gained a BSc degree with Gold Medal in Bacteriology, and became a lecturer at St Mary's until 1914. 
Fleming served throughout World War I as a captain in the Royal Army Medical Corps, and was Mentioned in Dispatches. He and many of his colleagues worked in battlefield hospitals at the Western Front in France. In 1918 he returned to St Mary's Hospital, where he was elected Professor of Bacteriology of the University of London in 1928.
Research.
Work before penicillin.
Following World War I, Fleming actively searched for anti-bacterial agents, having witnessed the death of many soldiers from sepsis resulting from infected wounds. Antiseptics killed the patients' immunological defences more effectively than they killed the invading bacteria. In an article he submitted for the medical journal "The Lancet" during World War I, Fleming described an ingenious experiment, which he was able to conduct as a result of his own glass blowing skills, in which he explained why antiseptics were killing more soldiers than infection itself during World War I. Antiseptics worked well on the surface, but deep wounds tended to shelter anaerobic bacteria from the antiseptic agent, and antiseptics seemed to remove beneficial agents produced that protected the patients in these cases at least as well as they removed bacteria, and did nothing to remove the bacteria that were out of reach. Sir Almroth Wright strongly supported Fleming's findings, but despite this, most army physicians over the course of the war continued to use antiseptics even in cases where this worsened the condition of the patients.
In 1921, Fleming discovered "lysozyme", an enzyme that had an antibacterial effect.
Accidental discovery.
"When I woke up just after dawn on September 28, 1928, I certainly didn't plan to revolutionise all medicine by discovering the world's first antibiotic, or bacteria killer," Fleming would later say, "But I suppose that was exactly what I did."
By 1927, Fleming had been investigating the properties of staphylococci. He was already well-known from his earlier work, and had developed a reputation as a brilliant researcher, but his laboratory was often untidy. On 3 September 1928, Fleming returned to his laboratory having spent August on holiday with his family. Before leaving, he had stacked all his cultures of staphylococci on a bench in a corner of his laboratory. On returning, Fleming noticed that one culture was contaminated with a fungus, and that the colonies of staphylococci immediately surrounding the fungus had been destroyed, whereas other staphylococci colonies farther away were normal, famously remarking "That's funny". Fleming showed the contaminated culture to his former assistant Merlin Price, who reminded him, "That's how you discovered lysozyme." Fleming grew the mould in a pure culture and found that it produced a substance that killed a number of disease-causing bacteria. He identified the mould as being from the "Penicillium" genus, and, after some months of calling it "mould juice", named the substance it released "penicillin" on 7 March 1929. The laboratory in which Fleming discovered and tested penicillin is preserved as the Alexander Fleming Laboratory Museum in St. Mary's Hospital, Paddington.
He investigated its positive anti-bacterial effect on many organisms, and noticed that it affected bacteria such as staphylococci and many other Gram-positive pathogens that cause scarlet fever, pneumonia, meningitis and diphtheria, but not typhoid fever or paratyphoid fever, which are caused by Gram-negative bacteria, for which he was seeking a cure at the time. It also affected "Neisseria gonorrhoeae," which causes gonorrhoea although this bacterium is Gram-negative.
Fleming published his discovery in 1929, in the British "Journal of Experimental Pathology," but little attention was paid to his article. Fleming continued his investigations, but found that cultivating "penicillium" was quite difficult, and that after having grown the mould, it was even more difficult to isolate the antibiotic agent. Fleming's impression was that because of the problem of producing it in quantity, and because its action appeared to be rather slow, penicillin would not be important in treating infection. Fleming also became convinced that penicillin would not last long enough in the human body ("in vivo") to kill bacteria effectively. Many clinical tests were inconclusive, probably because it had been used as a surface antiseptic. In the 1930s, Flemingâs trials occasionally showed more promise, and he continued, until 1940, to try to interest a chemist skilled enough to further refine usable penicillin. Fleming finally abandoned penicillin, and not long after he did, Howard Florey and Ernst Boris Chain at the Radcliffe Infirmary in Oxford took up researching and mass-producing it, with funds from the U.S. and British governments. They started mass production after the bombing of Pearl Harbor. By D-Day in 1944, enough penicillin had been produced to treat all the wounded in the Allied forces.
Purification and stabilisation.
In Oxford, Ernst Boris Chain and Edward Abraham discovered how to isolate and concentrate penicillin. Abraham was the first to propose the correct structure of penicillin. Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. When Chain heard that he was coming, he remarked "Good God! I thought he was dead."
Norman Heatley suggested transferring the active ingredient of penicillin back into water by changing its acidity. This produced enough of the drug to begin testing on animals. There were many more people involved in the Oxford team, and at one point the entire Dunn School was involved in its production.
After the team had developed a method of purifying penicillin to an effective first stable form in 1940, several clinical trials ensued, and their amazing success inspired the team to develop methods for mass production and mass distribution in 1945.
Fleming was modest about his part in the development of penicillin, describing his fame as the "Fleming Myth" and he praised Florey and Chain for transforming the laboratory curiosity into a practical drug. Fleming was the first to discover the properties of the active substance, giving him the privilege of naming it: penicillin. He also kept, grew, and distributed the original mould for twelve years, and continued until 1940 to try to get help from any chemist who had enough skill to make penicillin. But Sir Henry Harris said in 1998: "Without Fleming, no Chain; without Chain, no Florey; without Florey, no Heatley; without Heatley, no penicillin."
Antibiotics.
Fleming's accidental discovery and isolation of penicillin in September 1928 marks the start of modern antibiotics. Before that, several scientists had published or pointed out that mould or "penicillium sp." were able to inhibit bacterial growth, and even to cure bacterial infections in animals. Ernest Duchesne in 1897 in his thesis "Contribution to the study of vital competition in micro-organisms: antagonism between moulds and microbes", or also Clodomiro Picado Twight whose work at Institut Pasteur in 1923 on the inhibiting action of fungi of the "Penicillin sp" genre in the growth of staphylococci drew little interest from the direction of the Institut at the time. Fleming was the first to push these studies further by isolating the penicillin, and by being motivated enough to promote his discovery at a larger scale. Fleming also discovered very early that bacteria developed antibiotic resistance whenever too little penicillin was used or when it was used for too short a period. Almroth Wright had predicted antibiotic resistance even before it was noticed during experiments. Fleming cautioned about the use of penicillin in his many speeches around the world. He cautioned not to use penicillin unless there was a properly diagnosed reason for it to be used, and that if it were used, never to use too little, or for too short a period, since these are the circumstances under which bacterial resistance to antibiotics develops.
Myths.
The popular story of Winston Churchill's father paying for Fleming's education after Fleming's father saved young Winston from death is false. According to the biography, "Penicillin Man: Alexander Fleming and the Antibiotic Revolution" by Kevin Brown, Alexander Fleming, in a letter to his friend and colleague Andre Gratia, described this as "A wondrous fable." Nor did he save Winston Churchill himself during World War II. Churchill was saved by Lord Moran, using sulphonamides, since he had no experience with penicillin, when Churchill fell ill in Carthage in Tunisia in 1943. The "Daily Telegraph" and the "Morning Post" on 21 December 1943 wrote that he had been saved by penicillin. He was saved by the new sulphonamide drug, Sulphapyridine, known at the time under the research code M&B 693, discovered and produced by May & Baker Ltd, Dagenham, Essex â a subsidiary of the French group RhÃ´ne-Poulenc. In a subsequent radio broadcast, Churchill referred to the new drug as "This admirable M&B." It is highly probable that the correct information about the sulphonamide did not reach the newspapers because, since the original sulphonamide antibacterial, Prontosil, had been a discovery by the German laboratory Bayer, and as Britain was at war with Germany at the time, it was thought better to raise British morale by associating Churchill's cure with the British discovery, penicillin.
Personal life.
On 23 December 1915, Fleming married a trained nurse, Sarah Marion McElroy of Killala, County Mayo, Ireland. Their only child, Robert Fleming, (b. 1924) became a general medical practitioner. After Sarah's death in 1949, Fleming married Dr. Amalia Koutsouri-Vourekas, a Greek colleague at St. Mary's, on 9 April 1953; she died in 1986.
Death.
On 11 March 1955, Fleming died at his home in London of a heart attack. He was buried in St Paul's Cathedral.
Honours, awards and achievements.
His discovery of penicillin had changed the world of modern medicine by introducing the age of useful antibiotics; penicillin has saved, and is still saving, millions of people around the world.
The laboratory at St Mary's Hospital where Fleming discovered penicillin is home to the Fleming Museum, a popular London attraction. His alma mater, St Mary's Hospital Medical School, merged with Imperial College London in 1988. The Sir Alexander Fleming Building on the South Kensington campus was opened in 1998 and is now one of the main preclinical teaching sites of the Imperial College School of Medicine.
His other alma mater, the Royal Polytechnic Institution (now the University of Westminster) has named one of its student halls of residence "Alexander Fleming House", which is near to Old Street.

</doc>
<doc id="1938" url="http://en.wikipedia.org/wiki?curid=1938" title="Andrew Carnegie">
Andrew Carnegie

Andrew Carnegie ( , but commonly or ; November 25, 1835Â â August 11, 1919) was a Scottish American industrialist who led the enormous expansion of the American steel industry in the late 19th century. He was also one of the highest profile philanthropists of his era and had given away almost 90 percent â amounting to, in 1919, $350 million (in 2015, $) â of his fortune to charities and foundations by the time of his death. His 1889 article proclaiming "The Gospel of Wealth" called on the rich to use their wealth to improve society, and stimulated a wave of philanthropy.
Carnegie was born in Dunfermline, Scotland, and emigrated to the United States with his very poor parents in 1848. Carnegie started as a telegrapher and by the 1860s had investments in railroads, railroad sleeping cars, bridges and oil derricks. He accumulated further wealth as a bond salesman raising money for American enterprise in Europe. He built Pittsburgh's Carnegie Steel Company, which he sold to J.P. Morgan in 1901 for $480 million (in 2015, $), creating the U.S. Steel Corporation. Carnegie devoted the remainder of his life to large-scale philanthropy, with special emphasis on local libraries, world peace, education and scientific research. With the fortune he made from business, he built Carnegie Hall, and founded the Carnegie Corporation of New York, Carnegie Endowment for International Peace, Carnegie Institution for Science, Carnegie Trust for the Universities of Scotland, Carnegie Hero Fund, Carnegie Mellon University and the Carnegie Museums of Pittsburgh, among others. His life has often been referred to as a true "rags to riches" story.
Biography.
Early life.
Andrew Carnegie was born in Dunfermline, Scotland, in a typical weaver's cottage with only one main room, consisting of half the ground floor which was shared with the neighboring weaver's family. The main room served as a living room, dining room and bedroom. He was named after his legal grandfather. In 1836, the family moved to a larger house in Edgar Street (opposite Reid's Park), following the demand for more heavy damask from which his father, William Carnegie, benefited. His uncle, George Lauder, whom he referred to as "Dod", introduced him to the writings of Robert Burns and historical Scottish heroes such as Robert the Bruce, William Wallace, and Rob Roy. Falling on very hard times as a handloom weaver and with the country in starvation, William Carnegie decided to move with his family to Allegheny, Pennsylvania, in the United States in 1848 for the prospect of a better life. Andrew's family had to borrow money in order to migrate. Allegheny was a very poor area. His first job at age 13 in 1848 was as a bobbin boy, changing spools of thread in a cotton mill 12 hours a day, 6 days a week in a Pittsburgh cotton factory. His starting wage was $1.20 per week. Andrew's father, William Carnegie, started off working in a cotton mill but then would earn money weaving and peddling linens. His mother, Margaret Morrison Carnegie, earned money by binding shoes.
Railroads.
In 1850, Carnegie became a telegraph messenger boy in the Pittsburgh Office of the Ohio Telegraph Company, at $2.50 per week, following the recommendation of his uncle. His new job gave him many benefits including free admission to the local theater. This made him appreciate Shakespeare's work.
He was a very hard worker and would memorize all of the locations of Pittsburgh's businesses and the faces of important men. He made many connections this way. He also paid close attention to his work, and quickly learned to distinguish the differing sounds the incoming telegraph signals produced. He developed the ability to translate signals by ear, without using the paper slip, and within a year was promoted as an operator. Carnegie's education and passion for reading was given a great boost by Colonel James Anderson, who opened his personal library of 400 volumes to working boys each Saturday night. Carnegie was a consistent borrower and a "self-made man" in both his economic development and his intellectual and cultural development. His capacity, his willingness for hard work, his perseverance, and his alertness soon brought forth opportunities.
Starting in 1853, Thomas A. Scott of the Pennsylvania Railroad Company employed Carnegie as a secretary/telegraph operator at a salary of $4.00 per week. At age 18, the precocious youth began a rapid advance through the company, becoming the superintendent of the Pittsburgh Division. His employment by the Pennsylvania Railroad Company would be vital to his later success. The railroads were the first big businesses in America, and the Pennsylvania was one of the largest of them all. Carnegie learned much about management and cost control during these years, and from Scott in particular.
Scott also helped him with his first investments. Many of these were part of the corruption indulged in by Scott and the Pennsylvania's president, J. Edgar Thomson, which consisted of inside trading in companies that the railroad did business with, or payoffs made by contracting parties "as part of a quid pro quo". In 1855, Scott made it possible for Carnegie to invest $500 in the Adams Express, which contracted with the Pennsylvania to carry its messengers. The money was secured by his mother's placing a $500 mortgage on the family's $700 home, but the opportunity was available only because of Carnegie's close relationship with Scott. A few years later, he received a few shares in T.T. Woodruff's sleeping car company, as a reward for holding shares that Woodruff had given to Scott and Thomson, as a payoff. Reinvesting his returns in such inside investments in railroad-related industries: (iron, bridges, and rails), Carnegie slowly accumulated capital, the basis for his later success. Throughout his later career, he made use of his close connections to Thomson and Scott, as he established businesses that supplied rails and bridges to the railroad, offering the two men a stake in his enterprises.
1860â1865: The Civil War.
Before the Civil War, Carnegie arranged a merger between Woodruff's company and that of George Pullman, the inventor of a sleeping car for first class travel which facilitated business travel at distances over . The investment proved a great success and a source of profit for Woodruff and Carnegie. The young Carnegie continued to work for the Pennsylvania's Tom Scott, and introduced several improvements in the service.
In spring 1861, Carnegie was appointed by Scott, who was now Assistant Secretary of War in charge of military transportation, as Superintendent of the Military Railways and the Union Government's telegraph lines in the East. Carnegie helped open the rail lines into Washington D.C. that the rebels had cut; he rode the locomotive pulling the first brigade of Union troops to reach Washington D.C. Following the defeat of Union forces at Bull Run, he personally supervised the transportation of the defeated forces. Under his organization, the telegraph service rendered efficient service to the Union cause and significantly assisted in the eventual victory. Carnegie later joked that he was "the first casualty of the war" when he gained a scar on his cheek from freeing a trapped telegraph wire.
Defeat of the Confederacy required vast supplies of munitions, as well as railroads (and telegraph lines) to deliver the goods. The war demonstrated how integral the industries were to American success.
Keystone Bridge Company.
In 1864, Carnegie invested $40,000 in Story Farm on Oil Creek in Venango County, Pennsylvania. In one year, the farm yielded over $1,000,000 in cash dividends, and petroleum from oil wells on the property sold profitably. The demand for iron products, such as armor for gunboats, cannon, and shells, as well as a hundred other industrial products, made Pittsburgh a center of wartime production. Carnegie worked with others in establishing a steel rolling mill, and steel production and control of industry became the source of his fortune. Carnegie had some investments in the iron industry before the war.
After the war, Carnegie left the railroads to devote all his energies to the ironworks trade. Carnegie worked to develop several iron works, eventually forming The Keystone Bridge Works and the Union Ironworks, in Pittsburgh. Although he had left the Pennsylvania Railroad Company, he remained closely connected to its management, namely Thomas A. Scott and J. Edgar Thomson. He used his connection to the two men to acquire contracts for his Keystone Bridge Company and the rails produced by his ironworks. He also gave stock to Scott and Thomson in his businesses, and the Pennsylvania was his best customer. When he built his first steel plant, he made a point of naming it after Thomson. As well as having good business sense, Carnegie possessed charm and literary knowledge. He was invited to many important social functionsâfunctions that Carnegie exploited to his own advantage.
Carnegie believed in using his fortune for others and doing more than making money. He wrote: 
Industrialist.
1885â1900: Steel empire.
Carnegie did not want to marry during his mother's lifetime, instead choosing to take care of her in her illness towards the end of her life. After she died in 1886, Carnegie married Louise Whitfield, who was more than 20 years his junior. In 1897, the couple had their only child, a daughter, whom they named after Carnegie's mother, Margaret.
Carnegie made his fortune in the steel industry, controlling the most extensive integrated iron and steel operations ever owned by an individual in the United States. One of his two great innovations was in the cheap and efficient mass production of steel by adopting and adapting the Bessemer process for steel making. Sir Henry Bessemer had invented the furnace which allowed the high carbon content of pig iron to be burnt away in a controlled and rapid way. The steel price dropped as a direct result, and Bessemer steel was rapidly adopted for railway lines and girders for buildings and bridges.
The second was in his vertical integration of all suppliers of raw materials. In the late 1880s, Carnegie Steel was the largest manufacturer of pig iron, steel rails, and coke in the world, with a capacity to produce approximately 2,000 tons of pig metal per day. In 1888, Carnegie bought the rival Homestead Steel Works, which included an extensive plant served by tributary coal and iron fields, a 425-mile (685Â km) long railway, and a line of lake steamships. Carnegie combined his assets and those of his associates in 1892 with the launching of the Carnegie Steel Company.
By 1889, the U.S. output of steel exceeded that of the UK, and Carnegie owned a large part of it. Carnegie's empire grew to include the J. Edgar Thomson Steel Works, (named for John Edgar Thomson, Carnegie's former boss and president of the Pennsylvania Railroad), Pittsburgh Bessemer Steel Works, the Lucy Furnaces, the Union Iron Mills, the Union Mill (Wilson, Walker & County), the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines. Carnegie, through Keystone, supplied the steel for and owned shares in the landmark Eads Bridge project across the Mississippi River at St. Louis, Missouri (completed 1874). This project was an important proof-of-concept for steel technology, which marked the opening of a new steel market.
1901: U.S. Steel.
In 1901, Carnegie was 66 years of age and considering retirement. He reformed his enterprises into conventional joint stock corporations as preparation to this end. John Pierpont Morgan was a banker and perhaps America's most important financial deal maker. He had observed how efficiently Carnegie produced profit. He envisioned an integrated steel industry that would cut costs, lower prices to consumers, produce in greater quantities and raise wages to workers. To this end, he needed to buy out Carnegie and several other major producers and integrate them into one company, thereby eliminating duplication and waste. He concluded negotiations on March 2, 1901, and formed the United States Steel Corporation. It was the first corporation in the world with a market capitalization over $1 billion.
The buyout, secretly negotiated by Charles M. Schwab (no relation to Charles R. Schwab), was the largest such industrial takeover in United States history to date. The holdings were incorporated in the United States Steel Corporation, a trust organized by Morgan, and Carnegie retired from business. His steel enterprises were bought out at a figure equivalent to 12 times their annual earningsâ$480 million (in 2015, $) which at the time was the largest ever personal commercial transaction.
Carnegie's share of this amounted to $225,639,000 (in 2015, $), which was paid to Carnegie in the form of 5%, 50-year gold bonds. The letter agreeing to sell his share was signed on February 26, 1901. On March 2, the circular formally filing the organization and capitalization (at $1,400,000,000â4% of U.S. national wealth at the time) of the United States Steel Corporation actually completed the contract. The bonds were to be delivered within two weeks to the Hudson Trust Company of Hoboken, New Jersey, in trust to Robert A. Franks, Carnegie's business secretary. There, a special vault was built to house the physical bulk of nearly $230,000,000 worth of bonds. It was said that "...Carnegie never wanted to see or touch these bonds that represented the fruition of his business career. It was as if he feared that if he looked upon them they might vanish like the gossamer gold of the leprechaun. Let them lie safe in a vault in New Jersey, safe from the New York tax assessors, until he was ready to dispose of them..."
Scholar and activist.
1880â1900.
Carnegie continued his business career; some of his literary intentions were fulfilled. He befriended English poet Matthew Arnold, English philosopher Herbert Spencer, and American humorist Mark Twain, as well as being in correspondence and acquaintance with most of the U.S. Presidents, statesmen, and notable writers.
Carnegie constructed commodious swimming-baths for the people of his hometown in Dunfermline in 1879. In the following year, Carnegie gave $40,000 for the establishment of a free library in Dunfermline. In 1884, he gave $50,000 to Bellevue Hospital Medical College (now part of New York University Medical Center) to found a histological laboratory, now called the Carnegie Laboratory.
In 1881, Carnegie took his family, including his 70 year-old mother, on a trip to the United Kingdom. They toured Scotland by coach, and enjoyed several receptions en route. The highlight for them all was a triumphal return to Dunfermline, where Carnegie's mother laid the foundation stone of a Carnegie library for which he donated the money. Carnegie's criticism of British society did not mean dislike; on the contrary, one of Carnegie's ambitions was to act as a catalyst for a close association between the English-speaking peoples. To this end, in the early 1880s in partnership with Samuel Storey, he purchased numerous newspapers in England, all of which were to advocate the abolition of the monarchy and the establishment of "the British Republic". Carnegie's charm aided by his great wealth meant that he had many British friends, including Prime Minister William Ewart Gladstone.
In 1886, Carnegie's younger brother Thomas died at age 43. Success in the business continued, however. While owning steel works, Carnegie had purchased at low cost the most valuable of the iron ore fields around Lake Superior. The same year Carnegie became a figure of controversy. Following his tour of the UK, he wrote about his experiences in a book entitled "An American Four-in-hand in Britain". Although still actively involved in running his many businesses, Carnegie had become a regular contributor to numerous magazines, most notably "The Nineteenth Century", under the editorship of James Knowles, and the influential "North American Review", led by editor Lloyd Bryce.
In 1886, Carnegie wrote his most radical work to date, entitled "Triumphant Democracy". Liberal in its use of statistics to make its arguments, the book argued his view that the American republican system of government was superior to the British monarchical system. It gave a highly favorable and idealized view of American progress and criticized the British royal family. The cover depicted an upended royal crown and a broken scepter. The book created considerable controversy in the UK. The book made many Americans appreciate their country's economic progress and sold over 40,000 copies, mostly in the U.S.
In 1889, Carnegie published in the June issue of the "North American Review". After reading it, Gladstone requested its publication in England, where it appeared as "The Gospel of Wealth" in the "Pall Mall Gazette". The article was the subject of much discussion. Carnegie argued that the life of a wealthy industrialist should comprise two parts. The first part was the gathering and the accumulation of wealth. The second part was for the subsequent distribution of this wealth to benevolent causes. The philanthropy was key to making the life worthwhile.
Carnegie was a well-regarded writer. He published three books on travel.
Anti-imperialism.
While Carnegie did not comment on British imperialism, he very strongly opposed the idea of American colonies. He strongly opposed the annexation of the Philippines, almost to the point of supporting William Jennings Bryan against McKinley in 1900. In 1898, Carnegie tried to arrange for independence for the Philippines. As the end of the Spanish American War neared, the United States bought the Philippines from Spain for $20 million. To counter what he perceived as imperialism on the part of the United States, Carnegie personally offered $20 million to the Philippines so that the Filipino people could buy their independence from the United States. However, nothing came of the offer. In 1898 Carnegie joined the American Anti-Imperialist League, in opposition to the U.S. annexation of the Philippines. Its membership included former presidents of the United States Grover Cleveland and Benjamin Harrison and literary figures like Mark Twain.
1901â1919: Philanthropist.
Carnegie spent his last years as a philanthropist. From 1901 forward, public attention was turned from the shrewd business acumen which had enabled Carnegie to accumulate such a fortune, to the public-spirited way in which he devoted himself to utilizing it on philanthropic projects. He had written about his views on social subjects and the responsibilities of great wealth in "Triumphant Democracy" (1886) and "Gospel of Wealth" (1889). Carnegie bought Skibo Castle in Scotland, and made his home partly there and partly in New York. He then devoted his life to providing the capital for purposes of public interest and social and educational advancement.
He was a powerful supporter of the movement for spelling reform as a means of promoting the spread of the English language.
Among his many philanthropic efforts, the establishment of public libraries throughout the United States, Britain, Canada and other English-speaking countries was especially prominent. In this special driving interest and project of his he was inspired by a visit and tour he made with Mr. Enoch Pratt (1808-1896), formerly of Massachusetts but who made his fortune in Baltimore and ran his various mercantile and financial businesses very thriftily. Pratt in turn had been inspired and helped by his friend and fellow Bay Stater, George Peabody, (1795-1869) who also had made his fortune in the "Monumental City" of Baltimore before moving to New York and London to expand his empire as the richest man in America before the Civil War. Later he too endowed several institutions, schools, libraries and foundations in his home commonwealth, and also in Baltimore with his Peabody Institute in 1857, completed in 1866, with added library wings a decade later and several educational foundations throughout the Old South. Several decades later, Carnegie's visit with Mr. Pratt for several days; resting and dining in his city mansion, then touring, visiting and talking with staff and ordinary citizen patrons of the newly established Enoch Pratt Free Library (1886) impressed the Scotsman deeply and years later he was always heard to proclaim that "Pratt was my guide and inspiration". The first Carnegie library opened in 1883 in Dunfermline. His method was to build and equip, but only on condition that the local authority matched that by providing the land and a budget for operation and maintenance. To secure local interest, in 1885, he gave $500,000 to Pittsburgh for a public library, and in 1886, he gave $250,000 to Allegheny City for a music hall and library; and $250,000 to Edinburgh for a free library. In total Carnegie funded some 3,000 libraries, located in 47 US states, and also in Canada, the United Kingdom, what is now the Republic of Ireland, Australia, New Zealand, the West Indies, and Fiji. He also donated Â£50,000 to help set up the University of Birmingham in 1899. In the early 20th Century, a decade after Mr. Pratt's death, when expansion and city revenues grew tight, Carnegie returned the favor and endowed a large sum to permit the building of many Carnegie Libraries in the Enoch Pratt system in Baltimore and enabled EPFL to expand through the next quarter-century to meet the needs of the growing city and supply neighborhood branches for its annexed suburbs.
As Van Slyck (1991) showed, the last years of the 19th century saw acceptance of the idea that free libraries should be available to the American public. But the design of the idealized free library was the subject of prolonged and heated debate. On one hand, the library profession called for designs that supported efficiency in administration and operation; on the other, wealthy philanthropists favored buildings that reinforced the paternalistic metaphor and enhanced civic pride. Between 1886 and 1917, Carnegie reformed both library philanthropy and library design, encouraging a closer correspondence between the two.
He gave $2 million in 1900 to start the Carnegie Institute of Technology (CIT) at Pittsburgh and the same amount in 1902 to found the Carnegie Institution at Washington, D.C. He later contributed more to these and other schools. CIT is now known as Carnegie Mellon University after it merged with the Mellon Institute of Industrial Research. Carnegie also served on the Board of Cornell University.
In 1911, Carnegie became a sympathetic benefactor to George Ellery Hale, who was trying to build the Hooker Telescope at Mount Wilson, and donated an additional ten million dollars to the Carnegie Institution with the following suggestion to expedite the construction of the telescope: "I hope the work at Mount Wilson will be vigorously pushed, because I am so anxious to hear the expected results from it. I should like to be satisfied before I depart, that we are going to repay to the old land some part of the debt we owe them by revealing more clearly than ever to them the new heavens." The telescope saw first light on November 2, 1917, with Carnegie still alive.
In Scotland, he gave $10 million in 1901 to establish the Carnegie Trust for the Universities of Scotland. It was created by a deed which he signed on June 7, 1901, and it was incorporated by Royal Charter on August 21, 1902. The Trust was funded by a gift of $10 million (a then unprecedented sum: at the time, total government assistance to all four Scottish universities was about Â£50,000 a year) and its aim was to improve and extend the opportunities for scientific research in the Scottish universities and to enable the deserving and qualified youth of Scotland to attend a university. He was subsequently elected Lord Rector of University of St. Andrews in December 1901. He also donated large sums of money to Dunfermline, the place of his birth. In addition to a library, Carnegie also bought the private estate which became Pittencrieff Park and opened it to all members of the public, establishing the Carnegie Dunfermline Trust to benefit the people of Dunfermline. A statue of him stands there today. He gave a further $10 million in 1913 to endow the Carnegie United Kingdom Trust, a grant-making foundation.
Carnegie also established large pension funds in 1901 for his former employees at Homestead and, in 1905, for American college professors. The latter fund evolved into TIAA-CREF. One critical requirement was that church-related schools had to sever their religious connections to get his money.
His interest in music led him to fund construction of 7,000 church organs. He built and owned Carnegie Hall in New York City.
Carnegie was a large benefactor of the Tuskegee Institute under Booker T. Washington for African-American education. He helped Washington create the National Negro Business League.
He founded the Carnegie Hero Fund for the United States and Canada in 1904 (a few years later also established in the United Kingdom, Switzerland, Norway, Sweden, France, Italy, the Netherlands, Belgium, Denmark, and Germany) for the recognition of deeds of heroism. Carnegie contributed $1,500,000 in 1903 for the erection of the Peace Palace at The Hague; and he donated $150,000 for a Pan-American Palace in Washington as a home for the International Bureau of American Republics.
Carnegie was honored for his philanthropy and support of the arts by initiation as an honorary member of Phi Mu Alpha Sinfonia Fraternity on October 14, 1917, at the New England Conservatory of Music in Boston, Massachusetts. The fraternity's mission reflects Carnegie's values by developing young men to share their talents to create harmony in the world.
By the standards of 19th century tycoons, Carnegie was not a particularly ruthless man but a humanitarian with enough acquisitiveness to go in the ruthless pursuit of money; on the other hand, the contrast between his life and the lives of many of his own workers and of the poor, in general, was stark. "Maybe with the giving away of his money," commented biographer Joseph Wall, "he would justify what he had done to get that money."
To some, Carnegie represents the idea of the American dream. He was an immigrant from Scotland who came to America and became successful. He is not only known for his successes but his enormous amounts of philanthropist works, not only to charities but also to promote democracy and independence to colonized countries.
Death.
Carnegie died on August 11, 1919, in Lenox, Massachusetts, of bronchial pneumonia. He had already given away $350,695,653 (approximately $4.8 billion, adjusted to 2010 figures) of his wealth. After his death, his last $30,000,000 was given to foundations, charities, and to pensioners. He was buried at the Sleepy Hollow Cemetery in North Tarrytown, New York. The grave site is located on the Arcadia Hebron plot of land at the corner of Summit Avenue and Dingle Road. Carnegie is buried only a few yards away from union organizer Samuel Gompers, another important figure of industry in the Gilded Age.
Controversies.
1889: Johnstown Flood.
Carnegie was one of more than 50 members of the South Fork Fishing and Hunting Club, which has been blamed for the Johnstown Flood that killed 2,209 people in 1889.
At the suggestion of his friend Benjamin Ruff, Carnegie's partner Henry Clay Frick had formed the exclusive South Fork Fishing and Hunting Club high above Johnstown, Pennsylvania. The sixty-odd club members were the leading business tycoons of Western Pennsylvania and included among their number Frick's best friend, Andrew Mellon, his attorneys Philander Knox and James Hay Reed, as well as Frick's business partner, Carnegie. High above the city, near the small town of South Fork, the South Fork Dam was originally built between 1838 and 1853 by the Commonwealth of Pennsylvania as part of a canal system to be used as a reservoir for a canal basin in Johnstown. With the coming-of-age of railroads superseding canal barge transport, the lake was abandoned by the Commonwealth, sold to the Pennsylvania Railroad, and sold again to private interests and eventually came to be owned by the South Fork Fishing and Hunting Club in 1881. Prior to the flood, speculators had purchased the abandoned reservoir, made less than well-engineered repairs to the old dam, raised the lake level, built cottages and a clubhouse, and created the South Fork Fishing and Hunting Club. Less than 20 miles downstream from the dam sat the city of Johnstown.
The dam was high and long. Between 1881 when the club was opened, and 1889, the dam frequently sprang leaks and was patched, mostly with mud and straw. Additionally, a previous owner removed and sold for scrap the 3 cast iron discharge pipes that previously allowed a controlled release of water. There had been some speculation as to the dam's integrity, and concerns had been raised by the head of the Cambria Iron Works downstream in Johnstown. Such repair work, a reduction in height, and unusually high snowmelt and heavy spring rains combined to cause the dam to give way on May 31, 1889 resulting in twenty million tons of water sweeping down the valley causing the Johnstown Flood. When word of the dam's failure was telegraphed to Pittsburgh, Frick and other members of the South Fork Fishing and Hunting Club gathered to form the Pittsburgh Relief Committee for assistance to the flood victims as well as determining never to speak publicly about the club or the flood. This strategy was a success, and Knox and Reed were able to fend off all lawsuits that would have placed blame upon the club's members.
Although Cambria Iron and Steel's facilities were heavily damaged by the flood, they returned to full production within a year. After the flood, Carnegie built Johnstown a new library to replace the one built by Cambria's chief legal counsel Cyrus Elder, which was destroyed in the flood. The Carnegie-donated library is now owned by the Johnstown Area Heritage Association, and houses the Flood Museum.
1892: Homestead Strike.
The Homestead Strike was a bloody labor confrontation lasting 143 days in 1892, one of the most serious in U.S. history. The conflict was centered on Carnegie Steel's main plant in Homestead, Pennsylvania, and grew out of a dispute between the National Amalgamated Association of Iron and Steel Workers of the United States and the Carnegie Steel Company.
Carnegie left on a trip to Scotland before the unrest peaked. In doing so, Carnegie left mediation of the dispute in the hands of his associate and partner Henry Clay Frick. Frick was well known in industrial circles for maintaining staunch anti-union sensibilities.
After a recent increase in profits by 60%, the company refused to raise workers' pay by more than 30%. When some of the workers demanded the full 60%, management locked the union out. Workers considered the stoppage a "lockout" by management and not a "strike" by workers. As such, the workers would have been well within their rights to protest, and subsequent government action would have been a set of criminal procedures designed to crush what was seen as a pivotal demonstration of the growing labor rights movement, strongly opposed by management. Frick brought in thousands of strikebreakers to work the steel mills and Pinkerton agents to safeguard them.
On July 6, the arrival of a force of 300 Pinkerton agents from New York City and Chicago resulted in a fight in which 10 menâseven strikers and three Pinkertonsâwere killed and hundreds were injured. Pennsylvania Governor Robert Pattison ordered two brigades of state militia to the strike site. Then, allegedly in response to the fight between the striking workers and the Pinkertons, anarchist Alexander Berkman shot at Frick in an attempted assassination, wounding Frick. While not directly connected to the strike, Berkman was tied in for the assassination attempt. According to Berkman, "...with the elimination of Frick, responsibility for Homestead conditions would rest with Carnegie." Afterwards, the company successfully resumed operations with non-union immigrant employees in place of the Homestead plant workers, and Carnegie returned to the United States. However, Carnegie's reputation was permanently damaged by the Homestead events.
Philosophy.
Andrew Carnegie Dictum.
In his final days, Carnegie suffered from bronchial pneumonia. Before his death on August 11, 1919, Carnegie had donated $350,695,654 for various causes. The "Andrew Carnegie Dictum" was:
Carnegie was involved in philanthropic causes, but he kept himself away from religious circles. He wanted to be identified by the world as a "positivist". He was highly influenced in public life by John Bright.
On wealth.
As early as 1868, at age 33, he drafted a memo to himself. He wrote: "...The amassing of wealth is one of the worse species of idolatry. No idol more debasing than the worship of money."
In order to avoid degrading himself, he wrote in the same memo he would retire at age 35 to pursue the practice of philanthropic giving for "...the man who dies thus rich dies disgraced." However, he did not begin his philanthropic work in all earnest until 1881, with the gift of a library to his hometown of Dunfermline, Scotland.
Carnegie wrote "The Gospel of Wealth", an article in which he stated his belief that the rich should use their wealth to help enrich society.
The following is taken from one of Carnegie's memos to himself: 
Intellectual influences.
Carnegie claimed to be a champion of evolutionary thought particularly the work of Herbert Spencer, even declaring Spencer his teacher. Though Carnegie claims to be a disciple of Spencer many of his actions went against the ideas espoused by Spencer.
Spencerian evolution was for individual rights and against government interference. Furthermore, Spencerian evolution held that those unfit to sustain themselves must be allowed to perish. Spencer believed that just as there were many varieties of beetles, respectively modified to existence in a particular place in nature, so too had human society âspontaneously fallen into division of labourâ. Individuals who survived to this, the latest and highest stage of evolutionary progress would be âthose in whom the power of self-preservation is the greatestâare the select of their generation.â Moreover, Spencer perceived governmental authority as borrowed from the people to perform the transitory aims of establishing social cohesion, insurance of rights, and security. Spencerian âsurvival of the fittestâ firmly credits any provisions made to assist the weak, unskilled, poor and distressed to be an imprudent disservice to evolution. Spencer insisted people should resist for the benefit of collective humanity as these severe fate singles out the weak, debauched, and disabled.
Andrew Carnegieâs political and economic focus of during the late nineteenth and early twentieth century was the defense of laissez faire economics. Carnegie emphatically resisted government intrusion in commerce, as well as government-sponsored charities. Carnegie believed the concentration of capital was essential for societal progress and should be encouraged. Carnegie was an ardent supporter of commercial âsurvival of the fittestâ and sought to attain immunity from business challenges by dominating all phases of the steel manufacturing procedure. Carnegieâs determination to lower costs included cutting labor expenses as well. In a notably Spencerian manner, Carnegie argued that unions impeded the natural reduction of prices by pushing up costs, which blocked evolutionary progress. Carnegie felt that unions represented the narrow interest of the few while his actions benefited the entire community.
On the surface, Andrew Carnegie appears to be a strict laissez-faire capitalist and follower of Herbert Spencer, often referring to himself as a disciple of Spencer. Conversely, Carnegie a titan of industry seems to embody all of the qualities of Spencerian survival of the fittest. The two men enjoyed a mutual respect for one another and maintained correspondence until Spencerâs death in 1903. There are however, some major discrepancies between Spencerâs capitalist evolutionary conceptions and Andrew Carnegieâs capitalist practices.
Spencer wrote that in production the advantages of the superior individual is comparatively minor, and thus acceptable, yet the benefit that dominance provides those who control a large segment of production might be hazardous to competition. Spencer feared that an absence of âsympathetic self-restraintâ of those with too much power could lead to the ruin of his competitors. He did not think free market competition necessitated competitive warfare. Furthermore, Spencer argued that individuals with superior resources who deliberately used investment schemes to put competitor out of business were committing acts of âcommercial murderâ. Carnegie built his wealth in the steel industry by maintaining an extensively integrated operating system. Carnegie also bought out some regional competitors, and merged with others, usually maintaining the majority shares in the companies. Over the course of twenty years, Carnegieâs steel properties grew to include the Edgar Thomson Steel Works, the Lucy Furnace Works, the Union Iron Mills, the Homestead Works, the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines among many other industry related assets. Furthermore, Carnegieâs success was due to his convenient relationship with the railroad industries, which not only relied on steel for track, but were also making money from steel transport. The steel and railroad barons worked closely to negotiate prices instead of free market competition determinations.
Besides Carnegieâs market manipulation, United States trade tariffs were also working in favor of the steel industry. Carnegie spent energy and resources lobbying congress for a continuation of favorable tariffs from which he earned millions of dollars a year. Carnegie tried to keep this information concealed, but legal document released in 1900, during proceeding with the ex-chairman of Carnegie Steel Henry Clay Frick revealed how favorable the tariffs had been. Herbert Spencer absolutely was against government interference in business in the form of regulatory limitation, taxes, and tariffs as well. Spencer saw tariffs as a form of taxation that levied against the majority in service to âthe benefit of a small minority of manufacturers and artisansâ.
Despite Carnegie's personal dedication to Herbert Spencer as a friend, his adherence to Spencerâs political and economic ideas is more contentious. In particular, it appears Carnegie either misunderstood or intentionally misrepresented some of Spencer's principal arguments. Spencer remarked upon his first visit to Carnegie's steel mills in Pittsburgh, which Carnegie saw as the manifestation of Spencer's philosophy, "Six months' residence here would justify suicide."
On the subject of charity Andrew Carnegie's actions diverged in the most significant and complex manner from Herbert Spencer's philosophies. In his 1854 essay Manners and Fashion, Spencer referred to public education as âOld schemesâ. He went on to declare that public schools and colleges, fill the heads of students with inept useless knowledge, which excludes useful knowledge. Spencer stated that he trusted no organization of any kind, âpolitical, religious, literary, philanthropicâ, and believed that as they expanded in influence so too did its regulations expand. In addition Spencer thought that as all institutions grow they become evermore corrupted by the influence of power and money. The institution eventually loses its âoriginal spirit, and sinks into a lifeless mechanismâ. Spencer insisted that all forms of philanthropy uplift the poor and downtrodden were reckless and incompetent. Spencer thought any attempt to prevent âthe really salutary sufferingsâ of the less fortunate âbequeath to posterity a continually increasing curseâ. Carnegie, a self-proclaimed devotee of Spencer, testified to Congress on February 5, 1915: "My business is to do as much good in the world as I can; I have retired from all other business."
Carnegie held that societal progress relied on individuals who maintained moral obligations to themselves and to society. Furthermore, he believed that charity supplied the means for those who wish to improve themselves to achieve their goals. Carnegie urged other wealthy people to contribute to society in the form of parks, works of art, libraries and other endeavors that improve the community and contribute to the âlasting good.â Carnegie also held a strong opinion against inherited wealth. Carnegie believed that the sons of prosperous businesspersons were rarely as talented as their fathers. By leaving large sums of money to their children, wealthy business leaders were wasting resources that could be used to benefit society. Most notably, Carnegie believed that the future leaders of society would rise from the ranks the poor. Carnegie strongly believed in this because he had risen from the bottom. He believed the poor possessed an advantage over the wealthy because they receive greater attention from their parents and are taught better work ethics.
Religion and world view.
Witnessing sectarianism and strife in 19th century Scotland regarding religion and philosophy, Carnegie kept his distance from organized religion and theism. Carnegie instead preferred to see things through naturalistic and scientific terms stating, "Not only had I got rid of the theology and the supernatural, but I had found the truth of evolution."
Later in life, Carnegie's firm opposition to religion softened. For many years he was a member of Madison Avenue Presbyterian Church, pastored from 1905 to 1926 by Social Gospel exponent Henry Sloane Coffin, while his wife and daughter belonged to the Brick Presbyterian Church. He also prepared (but did not deliver) an address in which he professed a belief in "an Infinite and Eternal Energy from which all things proceed".
World peace.
Influenced by his "favorite living hero in public life", the British liberal, John Bright, Carnegie started his efforts in pursuit of world peace at a young age. His motto, "All is well since all grows better", served not only as a good rationalization of his successful business career but also in his view of international relations.
Despite his efforts towards international peace, Carnegie faced many dilemmas on his quest. These dilemmas are often regarded as conflicts between his view on international relations and his other loyalties. Throughout the 1880s and 1890s, for example, Carnegie allowed his steel works to fill large orders of armor plate for the building of an enlarged and modernized United States Navy; while he opposed American oversea expansion. 
On the matter of American colonial expansion, Carnegie had always thought it is an unwise gesture for the United States. He did not oppose the annexation of the Hawaiian islands or Puerto Rico, but he opposed the annexation of the Philippines. Carnegie believed that it involved a denial of the fundamental democratic principle, and he also urged William McKinley to withdraw American troops and allow the Filipinos to live with their independence. This act well impressed the other American anti-imperialists, who soon elected him vice-president of the Anti-Imperialist League.
After he sold his steel company in 1901, Carnegie was able to get fully involved into the acts for the peace cause, both financially and personally. He gave away much of his fortunes to various peace-keeping agencies in order to keep them growing. When his friend, the British writer William T. Stead, asked him to create a new organization for the goal of a peace and arbitration society, his reply was as such:
Carnegie believed that it is the effort and will of the people, that maintains the peace in international relations. Money is just a push for the act. If world peace depended solely on financial support, it would not seem a goal, but more like an act of pity.
Like Stead he believed that the United States and the British Empire would merge into one nation, telling him "We are heading straight to the Re-United States". Carnegie believed that the combined country's power would maintain world peace and disarmament. The creation of the Carnegie Endowment for International Peace in 1910 was regarded as a milestone on the road to the ultimate goal of abolition of war. Beyond a gift of $10 million for peace promotion, Carnegie also encouraged the "scientific" investigation of the various causes of war, and the adoption of judicial methods that should eventually eliminate them. He believed that the Endowment exists to promote information on the nations' rights and responsibilities under existing international law and to encourage other conferences to codify this law.
In 1914, on the eve of the First World War, Carnegie founded the Church Peace Union (CPU), a group of leaders in religion, academia, and politics. Through the CPU, Carnegie hoped to mobilize the world's churches, religious organizations, and other spiritual and moral resources to join in promoting moral leadership to put an end to war forever. For its inaugural international event, the CPU sponsored a conference to be held on August 1, 1914, on the shores of Lake Constance in southern Germany. As the delegates made their way to the conference by train, Germany was invading Belgium.
Despite its inauspicious beginning, the CPU thrived. Today its focus is on ethics and it is known as the Carnegie Council for Ethics in International Affairs, an independent, nonpartisan, nonprofit organization, whose mission is to be the voice for ethics in international affairs.
The outbreak of the First World War was clearly a shock to Carnegie and his optimistic view on world peace. Although his promotion of anti-imperialism and world peace had all failed, and the Carnegie Endowment had not fulfilled his expectations, his beliefs and ideas on international relations had helped build the foundation of the League of Nations after his death, which took world peace to another level.
Writings.
Carnegie was a frequent contributor to periodicals on labor issues. In addition to "Triumphant Democracy" (1886), and "The Gospel of Wealth" (1889), he also wrote "An American Four-in-hand in Britain" (1883), "Round the World" (1884), "The Empire of Business" (1902), "The Secret of Business is the Management of Men" (1903), "James Watt" (1905) in the Famous Scots Series, "Problems of Today" (1907), and his posthumously published autobiography "Autobiography of Andrew Carnegie" (1920).
Legacy and honors.
Carnegie received the honorary Doctor of Laws (DLL) from the University of Glasgow in June 1901, and received the Freedom of the City of Glasgow "in recognition of his munificence" later the same year.
Carnegie's personal papers reside at the Library of Congress Manuscript Division.
The Carnegie Collections of the Columbia University Rare Book and Manuscript Library consist of the archives of the following organizations founded by Carnegie: The Carnegie Corporation of New York (CCNY); The Carnegie Endowment for International Peace (CEIP); the Carnegie Foundation for the Advancement of Teaching (CFAT);The Carnegie Council on Ethics and International Affairs (CCEIA). These collections deal primarily with Carnegie philanthropy and have very little personal material related to Carnegie. Carnegie Mellon University and the Carnegie Library of Pittsburgh jointly administer the Andrew Carnegie Collection of digitized archives on Carnegie's life.
External links.
General interest.
 

</doc>
<doc id="1939" url="http://en.wikipedia.org/wiki?curid=1939" title="Approximant consonant">
Approximant consonant

Approximants are speech sounds that involve the articulators approaching each other but not narrowly enough nor with enough articulatory precision to create turbulent airflow. Therefore, approximants fall between fricatives, which do produce a turbulent airstream, and vowels, which produce no turbulence. This class of sounds includes lateral approximants like (as in "less"), non-lateral approximants like (as in "rest"), and semivowels like and (as in "yes" and "west", respectively).
Before Peter Ladefoged coined the term "approximant" in the 1960s the term "frictionless continuant" referred to non-lateral approximants.
Semivowels.
Some approximants resemble vowels in acoustic and articulatory properties and the terms "semivowel" and "glide" are often used for these non-syllabic vowel-like segments. The correlation between semivowels and vowels is strong enough that cross-language differences between semivowels correspond with the differences between their related vowels.
Vowels and their corresponding semivowels alternate in many languages depending on the phonological environment, or for grammatical reasons, as is the case with Indo-European ablaut. Similarly, languages often avoid configurations where a semivowel precedes its corresponding vowel. A number of phoneticians distinguish between semivowels and approximants by their location in a syllable. Although he uses the terms interchangeably, remarks that, for example, the final glides of English "par" and "buy" differ from French "par" ('through') and "baille" ('tub') in that, in the latter pair, the approximants appear in the syllable coda, whereas, in the former, they appear in the syllable nucleus. This means that opaque (if not minimal) contrasts can occur in languages like Italian (with the i-like sound of "piede" 'foot', appearing in the nucleus: , and that of "piano" 'slow', appearing in the syllable onset: ) and Spanish (with a near minimal pair being "abyecto" 'abject' and "abierto" 'opened').
In articulation and often diachronically, palatal approximants correspond to front vowels, velar approximants to back vowels, and labialized approximants to rounded vowels. In American English, the rhotic approximant corresponds to the rhotic vowel. This can create alternations (as shown in the above table).
In addition to alternations, glides can be inserted to the left or the right of their corresponding vowels when occurring next to a hiatus. For example, in Ukrainian, medial triggers the formation of an inserted that acts as a syllable onset so that when the affix is added to ÑÑÑÐ±Ð¾Ð» ('football') to make ÑÑÑÐ±Ð¾Ð»ÑÑÑ 'football player', it's pronounced but Ð¼Ð°Ð¾ÑÑÑ ('Maoist'), with the same affix, is pronounced with a glide. Dutch has a similar process that extends to mid vowels:
Similarly, vowels can be inserted next to their corresponding glide in certain phonetic environments. Sievers' law describes this behaviour for Germanic.
Non-high semivowels also occur. In colloquial Nepali speech, a process of glide-formation occurs, wherein one of two adjacent vowels becomes non-syllabic; this process includes mid vowels so that ('cause to wish') features a non-syllabic mid vowel. Spanish features a similar process and even nonsyllabic can occur so that "ahorita" ('right away') is pronounced . It is not often clear, however, whether such sequences involve a semivowel (a consonant) or a diphthong (a vowel), and in many cases that may not be a meaningful distinction.
Although many languages have central vowels , which lie between back/velar and front/palatal , there are few cases of a corresponding approximant . One is in the Korean diphthong or , though this is more frequently analyzed as velar (as in the table above), and Mapudungun may be another: It has three high vowel sounds, , , and three corresponding consonants, , and , and a third one is often described as a voiced unrounded velar fricative; some texts note a correspondence between this approximant and that is parallel to â and â. An example is "liq" (?) ('white').
Approximants versus fricatives.
In addition to less turbulence, approximants also differ from fricatives in the precision required to produce them. 
When emphasized, approximants may be slightly fricated (that is, the airstream may become slightly turbulent), which is reminiscent of fricatives. For example, the Spanish word "ayuda" ('help') features a palatal approximant that is pronounced as a fricative in emphatic speech. However, such frication is generally slight and intermittent, unlike the strong turbulence of fricative consonants. 
Because voicelessness has comparatively reduced resistance to air flow from the lungs, the increased air flow creates more turbulence, making acoustic distinctions between voiceless approximants (which are extremely rare cross-linguistically) and voiceless fricatives difficult. This is why, for example, the voiceless labialized velar approximant (also transcribed with the special letter ) has traditionally been labeled a fricative, and no language is known to contrast it with a voiceless labialized velar fricative . Similarly, Standard Tibetan has a voiceless lateral approximant, , and Welsh has a voiceless lateral fricative , but the distinction is not always clear from descriptions of these languages. Again, no language is known to contrast the two. Iaai is reported to have an unusually large number of voiceless approximants, with .
For places of articulation further back in the mouth, languages do not contrast voiced fricatives and approximants. Therefore the IPA allows the symbols for the voiced fricatives to double for the approximants, with or without a lowering diacritic. 
Occasionally, the glottal "fricatives" are called approximants, since typically has no more frication than voiceless approximants, but they are often phonations of the glottis without any accompanying manner or place of articulation.
Lateral approximants.
In lateral approximants, the center of tongue makes solid contact with the roof of the mouth. However, the defining location is the side of the tongue, which only approaches the teeth. 
Voiceless approximants.
Voiceless approximants are rarely distinguished from voiceless fricatives. Some of them are:
Nasal approximants.
Examples are:
In Portuguese, the nasal glides and historically became and in some words. In Bini, the nasalized allophones of the approximants and are nasal occlusives, and .
What are transcribed as nasal approximants may include non-syllabic elements of nasal vowels/diphthongs.

</doc>
<doc id="1940" url="http://en.wikipedia.org/wiki?curid=1940" title="Astronomer Royal">
Astronomer Royal

Astronomer Royal is a senior post in the Royal Households of the United Kingdom. There are two officers, the senior being the Astronomer Royal dating from 22 June 1675; the second is the Astronomer Royal for Scotland dating from 1834.
King Charles II, who founded the Royal Observatory Greenwich in 1675 instructed the first Astronomer Royal John Flamsteed "."
From that time until 1972, the Astronomer Royal was Director of the Royal Observatory Greenwich. The Astronomer Royal receives a stipend of 100 GBP per year and is a member of the Royal Household, under the general authority of the Lord Chamberlain. After the separation of the two offices, the position of Astronomer Royal has been largely honorary, though he remains available to advise the Sovereign on astronomical and related scientific matters, and the office is of great prestige.
There was also formerly a Royal Astronomer of Ireland.

</doc>
<doc id="1941" url="http://en.wikipedia.org/wiki?curid=1941" title="Aeon">
Aeon

The word aeon , also spelled eon, originally means "life" or "being", though it then tended to mean "age", "forever" or "for eternity". It is a Latin transliteration from the koine Greek word ("ho aion"), from the archaic ("aiwon"). In Homer it typically refers to life or lifespan. Its latest meaning is more or less similar to the Sanskrit word "kalpa" and Hebrew word "olam". A cognate Latin word "aevum" or "aeuum" (cf. ) for "age" is present in words such as "longevity" and "mediaeval".
Although the term aeon may be used in reference to a period of a billion years (especially in geology, cosmology or astronomy), its more common usage is for any long, indefinite, period. Aeon can also refer to the four aeons on the Geologic Time Scale that make up the Earth's history, the Hadean, Archean, Proterozoic, and the current aeon Phanerozoic.
Astronomy and cosmology.
In astronomy an aeon is defined as a billion years (109).
Roger Penrose uses the word "aeon" to describe the period between successive and cyclic big bangs within the context of conformal cyclic cosmology.
Eternity or age.
The Bible translation is a treatment of the Hebrew word "olam" and the Greek word "aion". Both these words have similar meaning, and Young's Literal Translation renders them and their derivatives as âageâ or âage-duringâ. Other English versions most often translate them to indicate eternity, being translated as eternal, everlasting, forever, etc. However, there are notable exceptions to this in all major translations, such as : ââ¦I am with you always, to the end of the ageâ (NRSV), the word âageâ being a translation of "aion". Rendering "aion" to indicate eternality in this verse would result in the contradictory phrase âend of eternityâ, so the question arises whether it should ever be so. Proponents of Universal Reconciliation point out that this has significant implications for the problem of hell. Contrast in well-known English translations with its rendering in Young's Literal Translation:
And these shall go away to punishment age-during, but the righteous to life age-during. (YLT)
Then they will go away to eternal punishment, but the righteous to eternal life. (NIV)
These will go away into eternal punishment, but the righteous into eternal life. (NASB)
And these shall go away into everlasting punishment, but the righteous into eternal life. (KJV)
And these will depart into everlasting cutting-off, but the righteous ones into everlasting life. (NWT)
Philosophy and mysticism.
Plato used the word "aeon" to denote the eternal world of ideas, which he conceived was "behind" the perceived world, as demonstrated in his famous allegory of the cave.
Christianity's idea of "eternal life" comes from the word for life, "zoe", and a form of "aeon", which could mean life in the next aeon, the Kingdom of God, or Heaven, just as much as immortality, as in .
According to the Christian doctrine of Universal Reconciliation, the Greek New Testament scriptures use the word "eon" to mean a long period (perhaps 1000 years) and the word "eonian" to mean "during a long period"; Thus there was a time before the eons, and the eonian period is finite. After each man's mortal life ends, he is judged worthy of eonian life or eonian punishment. That is, after the period of the eons, all punishment will cease and death is overcome and then God becomes the all in each one (). This contrasts with the conventional Christian belief in eternal life and eternal punishment.
Occultists of the Thelema and O.T.O. traditions sometimes speak of a "magical Aeon" that may last for far less time, perhaps as little as 2,000 years.
Aeon may also be an archaic name for omnipotent beings, such as gods.
Gnosticism.
In many Gnostic systems, the various emanations of God, who is also known by such names as the One, the Monad, "Aion teleos" ( "The Broadest Aeon"), Bythos ("depth or profundity", Greek ), "Proarkhe" ("before the beginning", Greek ), the "Arkhe" ("the beginning", Greek ), "Sophia" (wisdom), Christos (the Anointed One) are called "Aeons". In the different systems these emanations are differently named, classified, and described, but the emanation theory itself is common to all forms of Gnosticism. 
In the Basilidian Gnosis they are called sonships (Ïá¼±ÏÏÎ·ÏÎµÏ "huiotetes"; sing.: "huiotes"); according to Marcus, they are numbers and sounds; in Valentinianism they form male/female pairs called "syzygies" (Greek , from ÏÏÎ¶ÏÎ³Î¿Î¹ "syzygoi").
Similarly, in the Greek Magical Papyri, the term "Aion" is often used to denote the All, or the supreme aspect of God. 

</doc>
<doc id="1942" url="http://en.wikipedia.org/wiki?curid=1942" title="Airline">
Airline

An airline is a company that provides air transport services for traveling passengers and freight. Airlines lease or own their aircraft with which to supply these services and may form partnerships or alliances with other airlines for mutual benefit. Generally, airline companies are recognized with an air operating certificate or license issued by a governmental aviation body.
Airlines vary from those with a single aircraft carrying mail or cargo, through full-service international airlines operating hundreds of aircraft. Airline services can be categorized as being intercontinental, intra-continental, domestic, regional, or international, and may be operated as scheduled services or charters.
History.
The first airlines.
DELAG, "Deutsche Luftschiffahrts-Aktiengesellschaft" was the world's first airline. It was founded on November 16, 1909 with government assistance, and operated airships manufactured by The Zeppelin Corporation. Its headquarters were in Frankfurt. The four oldest non-dirigible airlines that still exist are Netherlands' KLM, Colombia's Avianca, Australia's Qantas, and the Czech Republic's Czech Airlines. KLM first flew in May 1920, while Qantas (which stands for "Queensland and Northern Territory Aerial Services Limited") was founded in Queensland, Australia, in late 1920.
European airline industry.
Beginnings.
The earliest fixed wing airline was the Aircraft Transport and Travel, formed by George Holt Thomas in 1916. Using a fleet of former military Airco DH.4A biplanes that had been modified to carry two passengers in the fuselage, it operated relief flights between Folkestone and Ghent. On 15 July 1919, the company flew a proving flight across the English Channel, despite a lack of support from the British government. Flown by Lt. H Shaw in an Airco DH.9 between RAF Hendon and Paris - Le Bourget Airport, the flight took 2 hours and 30 minutes at Â£21 per passenger.
On 25 August 1919, the company used DH.16s to pioneer a regular service from Hounslow Heath Aerodrome to Le Bourget, the first regular international service in the world. The airline soon gained a reputation for reliability, despite problems with bad weather and began to attract European competition. In November 1919, it won the first British civil airmail contract. Six Royal Air Force Airco DH.9A aircraft were lent to the company, to operate the airmail service between Hawkinge and Cologne. In 1920, they were returned to the Royal Air Force.
Other British competitors were quick to follow - Handley Page Transport was established in 1919 and used the company's converted wartime Type O/400 bombers with a capacity for 19 passengers, to run a London-Paris passenger service.
The first French airlines were also established and began to offer competition for the same route. The SociÃ©tÃ© GÃ©nÃ©rale des Transports AÃ©riens was created in late 1919, by the Farman brothers and the Farman F.60 Goliath plane flew scheduled services from Toussus-le-Noble to Kenley, near Croydon. Another early French airline was the Compagnie des Messageries AÃ©riennes, established in 1919 by Louis-Charles Breguet, offering a mail and freight service between Le Bourget Airport, Paris and Lesquin Airport, Lille.
The Dutch airline KLM made its first flight in 1920, and is the oldest continuously operating airline in the world. Established by aviator Albert Plesman, it was immediately awarded a "Royal" predicate from Queen Wilhelmina Its first flight was from Croydon Airport, London to Amsterdam, using a leased Aircraft Transport and Travel DH-16, and carrying two British journalists and a number of newspapers. In 1921 KLM started scheduled services.
In Finland, the charter establishing Aero O/Y (now Finnair) was signed in the city of Helsinki on September 12, 1923. Junkers F.13 D-335 became the first aircraft of the company, when Aero took delivery of it on March 14, 1924. The first flight was between Helsinki and Tallinn, capital of Estonia, and it took place on March 20, 1924, one week later.
In the Soviet Union, the Chief Administration of the Civil Air Fleet was established in 1921. One of its first acts was to help found Deutsch-Russische Luftverkehrs A.G. (Deruluft), a German-Russian joint venture to provide air transport from Russia to the West. Domestic air service began around the same time, when Dobrolyot started operations on 15 July 1923 between Moscow and Nizhni Novgorod. Since 1932 all operations had been carried under the name Aeroflot.
Early European airlines tended to favour comfort - the passenger cabins were often spacious with luxury interiors - over speed and efficiency. The relatively basic navigational capabilities of pilots at the time also meant that delays due to the weather, especially during the winter in the south of England, were commonplace.
Rationalization.
By the early 1920s, small airlines were struggling to compete, and there was a movement towards increased rationalization and consolidation. In 1924, Imperial Airways was formed from the merger of Instone Air Line Company, British Marine Air Navigation, Daimler Airway and Handley Page Transport Co Ltd., to allow British airlines to compete with stiff competition from French and German airlines that were enjoying heavy government subsidies. The airline was a pioneer in surveying and opening up air routes across the world to serve far-flung parts of the British Empire and to enhance trade and integration.
The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Two French airlines also merged to form Air Union on 1 January 1923. This later merged with four other French airlines to become Air France, the country's flagship carrier to this day, on 7 October 1933.
Germany's Deutsche Luft Hansa was created in 1926 by merger of two airlines, one of them Junkers Luftverkehr. Luft Hansa, due to the Junkers heritage and unlike most other airlines at the time, became a major investor in airlines outside of Europe, providing capital to Varig and Avianca. German airliners built by Junkers, Dornier, and Fokker were among the most advanced in the world at the time.
Global expansion.
In 1926, Alan Cobham surveyed a flight route from the UK to Cape Town, South Africa, following this up with another proving flight to Melbourne, Australia. Other routes to British India and the Far East were also charted and demonstrated at this time. Regular services to Cairo and Basra began in 1927 and was extended to Karachi in 1929. The London-Australia service was inaugurated in 1932 with the Handley Page HP 42 airliners. Further services were opened up to Calcutta, Rangoon, Singapore, Brisbane and Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
Imperial's aircraft were small, most seating fewer than twenty passengers, and catered for the rich - only about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research.
Like Imperial Airways, Air France and KLM's early growth depended heavily on the needs to service links with far-flung colonial possessions (North Africa and Indochina for the French and the East Indies for the Dutch). France began an air mail service to Morocco in 1919 that was bought out in 1927, renamed AÃ©ropostale, and injected with capital to become a major international carrier. In 1933, AÃ©ropostale went bankrupt, was nationalized and merged into Air France.
Although Germany lacked colonies, it also began expanding its services globally. In 1931, the airship Graf Zeppelin began offering regular scheduled passenger service between Germany and South America, usually every two weeks, which continued until 1937. In 1936, the airship Hindenburg entered passenger service and successfully crossed the Atlantic 36 times before crashing at Lakehurst, New Jersey on May 6, 1937.
By the end of the 1930s Aeroflot had become the world's largest airline, employing more than 4,000 pilots and 60,000 other service personnel and operating around 3,000 aircraft (of which 75% were considered obsolete by its own standards). During the Soviet era Aeroflot was synonymous with Russian civil aviation, as it was the only air carrier. It became the first airline in the world to operate sustained regular jet services on 15 September 1956 with the Tupolev Tu-104.
EU airline deregulation.
Deregulation of the European Union airspace in the early 1990s has had substantial effect on structure of the industry there. The shift towards 'budget' airlines on shorter routes has been significant. Airlines such as EasyJet and Ryanair have often grown at the expense of the traditional national airlines.
There has also been a trend for these national airlines themselves to be privatized such as has occurred for Aer Lingus and British Airways. Other national airlines, including Italy's Alitalia, have suffered - particularly with the rapid increase of oil prices in early 2008.
U.S. airline industry.
Early development.
Tony Jannus conducted the United States' first scheduled commercial airline flight on 1 January 1914 for the St. Petersburg-Tampa Airboat Line. The 23-minute flight traveled between St. Petersburg, Florida and Tampa, Florida, passing some above Tampa Bay in Jannus' Benoist XIV wood and muslin biplane flying boat. His passenger was a former mayor of St. Petersburg, who paid $400 for the privilege of sitting on a wooden bench in the open cockpit. The Airboat line operated for about four months, carrying more than 1,200 passengers who paid $5 each. Chalk's International Airlines began service between Miami and Bimini in the Bahamas in February 1919. Based in Ft. Lauderdale, Chalk's claimed to be the oldest continuously operating airline in the United States until its closure in 2008.
Following World War I, the United States found itself swamped with aviators. Many decided to take their war-surplus aircraft on barnstorming campaigns, performing aerobatic maneuvers to woo crowds. In 1918, the United States Postal Service won the financial backing of Congress to begin experimenting with air mail service, initially using Curtiss Jenny aircraft that had been procured by the United States Army Air Service. Private operators were the first to fly the mail but due to numerous accidents the US Army was tasked with mail delivery. During the Army's involvement they proved to be too unreliable and lost their air mail duties. By the mid-1920s, the Postal Service had developed its own air mail network, based on a transcontinental backbone between New York City and San Francisco. To supplant this service, they offered twelve contracts for spur routes to independent bidders. Some of the carriers that won these routes would, through time and mergers, evolve into Pan Am, Delta Air Lines, Braniff Airways, American Airlines, United Airlines (originally a division of Boeing), Trans World Airlines, Northwest Airlines, and Eastern Air Lines.
Service during the early 1920s was sporadic: most airlines at the time were focused on carrying bags of mail. In 1925, however, the Ford Motor Company bought out the Stout Aircraft Company and began construction of the all-metal Ford Trimotor, which became the first successful American airliner. With a 12-passenger capacity, the Trimotor made passenger service potentially profitable. Air service was seen as a supplement to rail service in the American transportation network.
At the same time, Juan Trippe began a crusade to create an air network that would link America to the world, and he achieved this goal through his airline, Pan American World Airways, with a fleet of flying boats that linked Los Angeles to Shanghai and Boston to London. Pan Am and Northwest Airways (which began flights to Canada in the 1920s) were the only U.S. airlines to go international before the 1940s.
With the introduction of the Boeing 247 and Douglas DC-3 in the 1930s, the U.S. airline industry was generally profitable, even during the Great Depression. This trend continued until the beginning of World War II.
Development since 1945.
As governments met to set the standards and scope for an emergent civil air industry toward the end of the war, the U.S. took a position of maximum operating freedom; U.S. airline companies were not as hard-hit as European and the few Asian ones had been. This preference for "open skies" operating regimes continues, with limitations, to this day.
World War II, like World War I, brought new life to the airline industry. Many airlines in the Allied countries were flush from lease contracts to the military, and foresaw a future explosive demand for civil air transport, for both passengers and cargo. They were eager to invest in the newly emerging flagships of air travel such as the Boeing Stratocruiser, Lockheed Constellation, and Douglas DC-6. Most of these new aircraft were based on American bombers such as the B-29, which had spearheaded research into new technologies such as pressurization. Most offered increased efficiency from both added speed and greater payload.
In the 1950s, the De Havilland Comet, Boeing 707, Douglas DC-8, and Sud Aviation Caravelle became the first flagships of the Jet Age in the West, while the Eastern bloc had Tupolev Tu-104 and Tupolev Tu-124 in the fleets of state-owned carriers such as Czechoslovak ÄSA, Soviet Aeroflot and East-German Interflug. The Vickers Viscount and Lockheed L-188 Electra inaugurated turboprop transport.
The next big boost for the airlines would come in the 1970s, when the Boeing 747, McDonnell Douglas DC-10, and Lockheed L-1011 inaugurated widebody ("jumbo jet") service, which is still the standard in international travel. The Tupolev Tu-144 and its Western counterpart, Concorde, made supersonic travel a reality. Concorde first flew in 1969 and operated through 2003. In 1972, Airbus began producing Europe's most commercially successful line of airliners to date. The added efficiencies for these aircraft were often not in speed, but in passenger capacity, payload, and range. Airbus also features modern electronic cockpits that were common across their aircraft to enable pilots to fly multiple models with minimal cross-training.
US airline deregulation.
1970 U.S. airline industry deregulation lowered federally controlled barriers for new airlines just as a downturn in the nation's economy occurred. New start-ups entered during the downturn, during which time they found aircraft and funding, contracted hangar and maintenance services, trained new employees, and recruited laid off staff from other airlines.
Major airlines dominated their routes through aggressive pricing and additional capacity offerings, often swamping new start-ups. In the place of high barriers to entry imposed by regulation, the major airlines implemented an equally high barrier called loss leader pricing. In this strategy an already established and dominant airline stomps out its competition by lowering airfares on specific routes, below the cost of operating on it, choking out any chance a start-up airline may have. The industry side effect is an overall drop in revenue and service quality. Since deregulation in 1978 the average domestic ticket price has dropped by 40%. So has airline employee pay. By incurring massive losses, the airlines of the USA now rely upon a scourge of cyclical Chapter 11 bankruptcy proceedings to continue doing business. America West Airlines (which has since merged with US Airways) remained a significant survivor from this new entrant era, as dozens, even hundreds, have gone under.
In many ways, the biggest winner in the deregulated environment was the air passenger. Although not exclusively attributable to deregulation, indeed the U.S. witnessed an explosive growth in demand for air travel. Many millions who had never or rarely flown before became regular fliers, even joining frequent flyer loyalty programs and receiving free flights and other benefits from their flying. New services and higher frequencies meant that business fliers could fly to another city, do business, and return the same day, from almost any point in the country. Air travel's advantages put long distance intercity railroad travel and bus lines under pressure, with most of the latter having withered away, whilst the former is still protected under nationalization through the continuing existence of Amtrak.
By the 1980s, almost half of the total flying in the world took place in the U.S., and today the domestic industry operates over 10,000 daily departures nationwide.
Toward the end of the century, a new style of low cost airline emerged, offering a no-frills product at a lower price. Southwest Airlines, JetBlue, AirTran Airways, Skybus Airlines and other low-cost carriers began to represent a serious challenge to the so-called "legacy airlines", as did their low-cost counterparts in many other countries. Their commercial viability represented a serious competitive threat to the legacy carriers. However, of these, ATA and Skybus have since ceased operations.
Increasingly since 1978, US airlines have been reincorporated and spun off by newly created and internally led manangement companies, and thus becoming nothing more than operating units and subsidiaries with limited financially decisive control. Among some of these holding companies and parent companies which are relatively well known, are the UAL Corporation, along with the AMR Corporation, among a long list of airline holding companies sometime recognized worldwide. Less recognized are the private equity firms which often seize managerial, financial, and board of directors control of distressed airline companies by temporarily investing large sums of capital in air carriers, to rescheme an airlines assets into a profitable organization or liquidating an air carrier of their profitable and worthwhile routes and business operations.
Thus the last 50 years of the airline industry have varied from reasonably profitable, to devastatingly depressed. As the first major market to deregulate the industry in 1978, U.S. airlines have experienced more turbulence than almost any other country or region. In fact, no U.S. legacy carrier survived bankruptcy-free. Amongst the outspoken critics of deregulation, former CEO of American Airlines, Robert Crandall has publicly stated:
"Chapter 11 bankruptcy protection filing shows airline industry deregulation was a mistake."
The airline industry bailout.
Congress passed the (P.L. 107-42) in response to a severe liquidity crisis facing the already-troubled airline industry in the aftermath of the September 11th terrorist attacks. Congress sought to provide cash infusions to carriers for both the cost of the four-day federal shutdown of the airlines and the incremental losses incurred through December 31, 2001 as a result of the terrorist attacks. This resulted in the first government bailout of the 21st century. Between 2000 and 2005 US airlines lost $30 billion with wage cuts of over $15 billion and 100,000 employees laid off.
In recognition of the essential national economic role of a healthy aviation system, Congress authorized partial compensation of up to $5 billion in cash subject to review by the Department of Transportation and up to $10 billion in loan guarantees subject to review by a newly created Air Transportation Stabilization Board (ATSB). The applications to DOT for reimbursements were subjected to rigorous multi-year reviews not only by DOT program personnel but also by the Government Accountability Office and the DOT Inspector General.
Ultimately, the federal government provided $4.6 billion in one-time, subject-to-income-tax cash payments to 427 U.S. air carriers, with no provision for repayment, essentially a gift from the taxpayers. (Passenger carriers operating scheduled service received approximately $4 billion, subject to tax.) In addition, the ATSB approved loan guarantees to six airlines totaling approximately $1.6 billion. Data from the US Treasury Department show that the government recouped the $1.6 billion and a profit of $339 million from the fees, interest and purchase of discounted airline stock associated with loan guarantees.
Asian airline industry.
Although Philippine Airlines (PAL) was officially founded on February 26, 1941, its license to operate as an airliner was derived from merged Philippine Aerial Taxi Company (PATCO) established by mining magnate Emmanuel N. Bachrach on December 3, 1930, making it Asia's oldest scheduled carrier still in operation. Commercial air service commenced three weeks later from Manila to Baguio, making it Asia's first airline route. Bachrach's death in 1937 paved the way for its eventual merger with Philippine Airlines in March 1941 and made it Asia's oldest airline. It is also the oldest airline in Asia still operating under its current name. Bachrach's majority share in PATCO was bought by beer magnate Andres R. Soriano in 1939 upon the advice of General Douglas MacArthur and later merged with newly formed Philippine Airlines with PAL as the surviving entity. Soriano has controlling interest in both airlines before the merger. PAL restarted service on March 15, 1941 with a single Beech Model 18 NPC-54 aircraft, which started its daily services between Manila (from Nielson Field) and Baguio, later to expand with larger aircraft such as the DC-3 and Vickers Viscount.
India was also one of the first countries to embrace civil aviation. One of the first West Asian airline companies was Air India, which had its beginning as Tata Airlines in 1932, a division of Tata Sons Ltd. (now Tata Group). The airline was founded by India's leading industrialist, JRD Tata. On October 15, 1932, J. R. D. Tata himself flew a single engined De Havilland Puss Moth carrying air mail (postal mail of Imperial Airways) from Karachi to Bombay via Ahmedabad. The aircraft continued to Madras via Bellary piloted by Royal Air Force pilot Nevill Vintcent. Tata Airlines was also one of the world's first major airlines which began its operations without any support from the Government.
With the outbreak of World War II, the airline presence in Asia came to a relative halt, with many new flag carriers donating their aircraft for military aid and other uses. Following the end of the war in 1945, regular commercial service was restored in India and Tata Airlines became a public limited company on July 29, 1946 under the name Air India. After the independence of India, 49% of the airline was acquired by the Government of India. In return, the airline was granted status to operate international services from India as the designated flag carrier under the name Air India International.
On July 31, 1946, a chartered Philippine Airlines (PAL) DC-4 ferried 40 American servicemen to Oakland, California, from Nielson Airport in Makati City with stops in Guam, Wake Island, Johnston Atoll and Honolulu, Hawaii, making PAL the first Asian airline to cross the Pacific Ocean. A regular service between Manila and San Francisco was started in December. It was during this year that the airline was designated as the flag carrier of Philippines.
During the era of decolonization, newly born Asian countries started to embrace air transport. Among the first Asian carriers during the era were Cathay Pacific of Hong Kong (founded in September 1946 ), Orient Airways (later Pakistan International Airlines; founded in October 1946), Malayan Airways Limited in 1947 (later Singapore and Malaysia Airlines), El Al in Israel in 1948, Garuda Indonesia in 1948, Japan Airlines in 1951, Thai Airways International in 1960, and Korean National Airlines in 1947.
Latin American airline industry.
Among the first countries to have regular airlines in Latin America were Bolivia with Lloyd AÃ©reo Boliviano, Cuba with Cubana de AviaciÃ³n, Colombia with Avianca, Argentina with Aerolineas Argentinas, Chile with LAN Chile (today LAN Airlines), Brazil with Varig, Dominican Republic with Dominicana de AviaciÃ³n, Mexico with Mexicana de AviaciÃ³n, Trinidad and Tobago with BWIA West Indies Airways (today Caribbean Airlines), Venezuela with Aeropostal, and TACA based in El Salvador and representing several airlines of Central America (Costa Rica, Guatemala, Honduras and Nicaragua). All the previous airlines started regular operations well before World War II.
The air travel market has evolved rapidly over recent years in Latin America. Some industry estimates indicate that over 2,000 new aircraft will begin service over the next five years in this region.
These airlines serve domestic flights within their countries, as well as connections within Latin America and also overseas flights to North America, Europe, Australia, and Asia.
Only three airlines: LAN, OceanAir and TAM Airlines have international subsidiaries and cover many destinations within the Americas as well as major hubs in other continents. LAN with Chile as the central operation along with Peru, Ecuador, Colombia and Argentina and some operations in the Dominican Republic. The recently formed AviancaTACA group has control of Avianca Brazil, VIP Ecuador and a strategic alliance with AeroGal. And TAM with its Mercosur base in Asuncion, Paraguay. As of 2010, talks of uniting LAN and TAM have strongly developed to create a joint airline named LATAM.
Regulatory considerations.
National.
Many countries have national airlines that the government owns and operates. Fully private airlines are subject to a great deal of government regulation for economic, political, and safety concerns. For instance, governments often intervene to halt airline labor actions to protect the free flow of people, communications, and goods between different regions without compromising safety.
The United States, Australia, and to a lesser extent Brazil, Mexico, India, the United Kingdom, and Japan have "deregulated" their airlines. In the past, these governments dictated airfares, route networks, and other operational requirements for each airline. Since deregulation, airlines have been largely free to negotiate their own operating arrangements with different airports, enter and exit routes easily, and to levy airfares and supply flights according to market demand.
The entry barriers for new airlines are lower in a deregulated market, and so the U.S. has seen hundreds of airlines start up (sometimes for only a brief operating period). This has produced far greater competition than before deregulation in most markets, and average fares tend to drop 20% or more. The added competition, together with pricing freedom, means that new entrants often take market share with highly reduced rates that, to a limited degree, full service airlines must match. This is a major constraint on profitability for established carriers, which tend to have a higher cost base.
As a result, profitability in a deregulated market is uneven for most airlines. These forces have caused some major airlines to go out of business, in addition to most of the poorly established new entrants.
International.
Groups such as the International Civil Aviation Organization establish worldwide standards for safety and other vital concerns. Most international air traffic is regulated by bilateral agreements between countries, which designate specific carriers to operate on specific routes. The model of such an agreement was the Bermuda Agreement between the US and UK following World War II, which designated airports to be used for transatlantic flights and gave each government the authority to nominate carriers to operate routes.
Bilateral agreements are based on the "freedoms of the air", a group of generalized traffic rights ranging from the freedom to overfly a country to the freedom to provide domestic flights within a country (a very rarely granted right known as cabotage). Most agreements permit airlines to fly from their home country to designated airports in the other country: some also extend the freedom to provide continuing service to a third country, or to another destination in the other country while carrying passengers from overseas.
In the 1990s, "open skies" agreements became more common. These agreements take many of these regulatory powers from state governments and open up international routes to further competition. Open skies agreements have met some criticism, particularly within the European Union, whose airlines would be at a comparative disadvantage with the United States' because of cabotage restrictions.
Economic considerations.
Historically, air travel has survived largely through state support, whether in the form of equity or subsidies. The airline industry as a whole has made a cumulative loss during its 100-year history, once the costs include subsidies for aircraft development and airport construction.
One argument is that positive externalities, such as higher growth due to global mobility, outweigh the microeconomic losses and justify continuing government intervention. A historically high level of government intervention in the airline industry can be seen as part of a wider political consensus on strategic forms of transport, such as highways and railways, both of which receive public funding in most parts of the world. Profitability is likely to improve in the future as privatization continues and more competitive low-cost carriers proliferate.
Although many countries continue to operate state-owned or parastatal airlines, many large airlines today are privately owned and are therefore governed by microeconomic principles to maximize shareholder profit.
Top airline groups by revenue.
for 2010, source : Airline Business August 2011, Flightglobal Data Research
Ticket revenue.
Airlines assign prices to their services in an attempt to maximize profitability. The pricing of airline tickets has become increasingly complicated over the years and is now largely determined by computerized yield management systems.
Because of the complications in scheduling flights and maintaining profitability, airlines have many loopholes that can be used by the knowledgeable traveler. Many of these airfare secrets are becoming more and more known to the general public, so airlines are forced to make constant adjustments.
Most airlines use differentiated pricing, a form of price discrimination, to sell air services at varying prices simultaneously to different segments. Factors influencing the price include the days remaining until departure, the booked load factor, the forecast of total demand by price point, competitive pricing in force, and variations by day of week of departure and by time of day. Carriers often accomplish this by dividing each cabin of the aircraft (first, business and economy) into a number of travel classes for pricing purposes.
A complicating factor is that of origin-destination control ("O&D control"). Someone purchasing a ticket from Melbourne to Sydney (as an example) for A$200 is competing with someone else who wants to fly Melbourne to Los Angeles through Sydney on the same flight, and who is willing to pay A$1400. Should the airline prefer the $1400 passenger, or the $200 passenger plus a possible Sydney-Los Angeles passenger willing to pay $1300? Airlines have to make hundreds of thousands of similar pricing decisions daily.
The advent of advanced computerized reservations systems in the late 1970s, most notably Sabre, allowed airlines to easily perform cost-benefit analyses on different pricing structures, leading to almost perfect price discrimination in some cases (that is, filling each seat on an aircraft at the highest price that can be charged without driving the consumer elsewhere).
The intense nature of airfare pricing has led to the term "fare war" to describe efforts by airlines to undercut other airlines on competitive routes. Through computers, new airfares can be published quickly and efficiently to the airlines' sales channels. For this purpose the airlines use the Airline Tariff Publishing Company (ATPCO), who distribute latest fares for more than 500 airlines to Computer Reservation Systems across the world.
The extent of these pricing phenomena is strongest in "legacy" carriers. In contrast, low fare carriers usually offer preannounced and simplified price structure, and sometimes quote prices for each leg of a trip separately.
Computers also allow airlines to predict, with some accuracy, how many passengers will actually fly after making a reservation to fly. This allows airlines to overbook their flights enough to fill the aircraft while accounting for "no-shows," but not enough (in most cases) to force paying passengers off the aircraft for lack of seats, stimulative pricing for low demand flights coupled with overbooking on high demand flights can help reduce this figure. This is especially crucial during tough economic times as airlines undertake massive cuts to ticket prices to retain demand.
Operating costs.
Full-service airlines have a high level of fixed and operating costs to establish and maintain air services: labor, fuel, airplanes, engines, spares and parts, IT services and networks, airport equipment, airport handling services, sales distribution, catering, training, aviation insurance and other costs. Thus all but a small percentage of the income from ticket sales is paid out to a wide variety of external providers or internal cost centers.
Moreover, the industry is structured so that airlines often act as tax collectors. Airline fuel is untaxed because of a series of treaties existing between countries. Ticket prices include a number of fees, taxes and surcharges beyond the control of airlines. Airlines are also responsible for enforcing government regulations. If airlines carry passengers without proper documentation on an international flight, they are responsible for returning them back to the original country.
Analysis of the 1992â1996 period shows that every player in the air transport chain is far more profitable than the airlines, who collect and pass through fees and revenues to them from ticket sales. While airlines as a whole earned 6% return on capital employed (2-3.5% less than the cost of capital), airports earned 10%, catering companies 10-13%, handling companies 11-14%, aircraft lessors 15%, aircraft manufacturers 16%, and global distribution companies more than 30%. (Source: Spinetta, 2000, quoted in Doganis, 2002)
The widespread entrance of a new breed of low cost airlines beginning at the turn of the century has accelerated the demand that full service carriers control costs. Many of these low cost companies emulate Southwest Airlines in various respects, and like Southwest, they can eke out a consistent profit throughout all phases of the business cycle.
As a result, a shakeout of airlines is occurring in the U.S. and elsewhere. American Airlines, United Airlines, Continental Airlines (twice), US Airways (twice), Delta Air Lines, and Northwest Airlines have all declared Chapter 11 bankruptcy. Some argue that it would be far better for the industry as a whole if a wave of actual closures were to reduce the number of "undead" airlines competing with healthy airlines while being artificially protected from creditors via bankruptcy law. On the other hand, some have pointed out that the reduction in capacity would be short lived given that there would be large quantities of relatively new aircraft that bankruptcies would want to get rid of and would re-enter the market either as increased fleets for the survivors or the basis of cheap planes for new startups.
Where an airline has established an engineering base at an airport, then there may be considerable economic advantages in using that same airport as a preferred focus (or "hub") for its scheduled flights.
Assets and financing.
Airline financing is quite complex, since airlines are highly leveraged operations. Not only must they purchase (or lease) new airliner bodies and engines regularly, they must make major long-term fleet decisions with the goal of meeting the demands of their markets while producing a fleet that is relatively economical to operate and maintain. Compare Southwest Airlines and their reliance on a single airplane type (the Boeing 737 and derivatives), with the now defunct Eastern Air Lines which operated 17 different aircraft types, each with varying pilot, engine, maintenance, and support needs.
A second financial issue is that of hedging oil and fuel purchases, which are usually second only to labor in its relative cost to the company. However, with the current high fuel prices it has become the largest cost to an airline. Legacy airlines, compared with new entrants, have been hit harder by rising fuel prices partly due to the running of older, less fuel efficient aircraft. While hedging instruments can be expensive, they can easily pay for themselves many times over in periods of increasing fuel costs, such as in the 2000â2005 period.
In view of the congestion apparent at many international airports, the ownership of slots at certain airports (the right to take-off or land an aircraft at a particular time of day or night) has become a significant tradable asset for many airlines. Clearly take-off slots at popular times of the day can be critical in attracting the more profitable business traveler to a given airline's flight and in establishing a competitive advantage against a competing airline.
If a particular city has two or more airports, market forces will tend to attract the less profitable routes, or those on which competition is weakest, to the less congested airport, where slots are likely to be more available and therefore cheaper. For example, Reagan National Airport attracts profitable routes due partly to its congestion, leaving less-profitable routes to Baltimore-Washington International Airport and Dulles International Airport.
Other factors, such as surface transport facilities and onward connections, will also affect the relative appeal of different airports and some long distance flights may need to operate from the one with the longest runway. For example, LaGuardia Airport is the preferred airport for most of Manhattan due to its proximity, while long-distance routes must use John F. Kennedy International Airport's longer runways.
Airline partnerships.
Codesharing is the most common type of airline partnership; it involves one airline selling tickets for another airline's flights under its own airline code. An early example of this was Japan Airlines' (JAL) codesharing partnership with Aeroflot in the 1960s on TokyoâMoscow flights; Aeroflot operated the flights using Aeroflot aircraft, but JAL sold tickets for the flights as if they were JAL flights. This practice allows airlines to expand their operations, at least on paper, into parts of the world where they cannot afford to establish bases or purchase aircraft. Another example was the Austrianâ Sabena partnership on the ViennaâBrusselsâNew York/JFK route during the late '60s, using a Sabena Boeing 707 with Austrian livery.
Since airline reservation requests are often made by city-pair (such as "show me flights from Chicago to DÃ¼sseldorf"), an airline that can codeshare with another airline for a variety of routes might be able to be listed as indeed offering a ChicagoâDÃ¼sseldorf flight. The passenger is advised however, that airline no. 1 operates the flight from say Chicago to Amsterdam, and airline no. 2 operates the continuing flight (on a different airplane, sometimes from another terminal) to DÃ¼sseldorf. Thus the primary rationale for code sharing is to expand one's service offerings in city-pair terms to increase sales.
A more recent development is the airline alliance, which became prevalent in late 1990s. These alliances can act as virtual mergers to get around government restrictions. Alliances of airlines such as Star Alliance, Oneworld, and SkyTeam coordinate their passenger service programs (such as lounges and frequent-flyer programs), offer special interline tickets, and often engage in extensive codesharing (sometimes systemwide). These are increasingly integrated business combinationsâsometimes including cross-equity arrangementsâin which products, service standards, schedules, and airport facilities are standardized and combined for higher efficiency. One of the first airlines to start an alliance with another airline was KLM, who partnered with Northwest Airlines. Both airlines later entered the SkyTeam alliance after the fusion of KLM and Air France in 2004.
Often the companies combine IT operations, or purchase fuel and aircraft as a bloc to achieve higher bargaining power. However, the alliances have been most successful at purchasing invisible supplies and services, such as fuel. Airlines usually prefer to purchase items visible to their passengers to differentiate themselves from local competitors. If an airline's main domestic competitor flies Boeing airliners, then the airline may prefer to use Airbus aircraft regardless of what the rest of the alliance chooses.
Fuel hedging.
Southwest is credited with maintaining strong business profits between 1999 and the early 2000s due to its fuel hedging policy. Looking at the annual reports, many other airlines are replicating Southwest's hedging policy to control their fuel costs.
Environmental impacts.
Aircraft engines emit noise pollution, gases and particulate emissions, and contribute to global dimming.
Growth of the industry in recent years raised a number of ecological questions.
Domestic air transport grew in China at 15.5 percent annually from 2001 to 2006. The rate of air travel globally increased at 3.7 percent per year over the same time. In the EU greenhouse gas emissions from aviation increased by 87% between 1990 and 2006. However it must be compared with the flights increase, only in UK, between 1990 and 2006 terminal passengers increased from 100 000 thousands to 250 000 thousands., according to AEA reports every year, 750 million passengers travel by European airlines, which also share 40% of merchandise value in and out of Europe. Without even pressure from "green activists", targeting lower ticket prices, generally, airlines do what is possible to cut the fuel consumption (and gas emissions connected therewith). Further, according to some reports, it can be concluded that the last piston-powered aircraft were as fuel-efficient as the average jet in 2005.
Despite continuing efficiency improvements from the major aircraft manufacturers, the expanding demand for global air travel has resulted in growing greenhouse gas (GHG) emissions. Currently, the aviation sector, including US domestic and global international travel, make approximately 1.6 percent of global anthropogenic GHG emissions per annum. North America accounts for nearly 40 percent of the world's GHG emissions from aviation fuel use.
CO2 emissions from the jet fuel burned per passenger on an average airline flight is about 353 kilograms (776Â pounds). Loss of natural habitat potential associated with the jet fuel burned per passenger on a airline flight is estimated to be 250 square meters (2700 square feet).
In the context of climate change and peak oil, there is a debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.
The airline industry is responsible for about 11 percent of greenhouse gases emitted by the U.S. transportation sector. Boeing estimates that biofuels could reduce flight-related greenhouse-gas emissions by 60 to 80 percent. The solution would be blending algae fuels with existing jet fuel:
There are Electric aircraft projects, where some of them are fully operational planes as of 2013.
Call signs.
Each operator of a scheduled or charter flight uses an airline call sign when communicating with airports or air traffic control centres. Most of these call-signs are derived from the airline's trade name, but for reasons of history, marketing, or the need to reduce ambiguity in spoken English (so that pilots do not mistakenly make navigational decisions based on instructions issued to a different aircraft), some airlines and air forces use call-signs less obviously connected with their trading name. For example, British Airways uses a "Speedbird" call-sign, named after the logo of its predecessor, BOAC, while SkyEurope used "Relax".
Airline personnel.
The various types of airline personnel include:
Flight operations personnel including flight safety personnel.
Airlines follow a corporate structure where each broad area of operations (such as maintenance, flight operations(including flight safety),
and passenger service) is supervised by a vice president. Larger airlines often appoint vice presidents to oversee each of the
airline's hubs as well. Airlines employ lawyers to deal with regulatory procedures and other administrative tasks.
Industry trends.
The pattern of ownership has been privatized in the recent years, that is, the ownership has gradually changed from governments to private and individual sectors or organizations. This occurs as regulators permit greater freedom and non-government ownership, in steps that are usually decades apart. This pattern is not seen for all airlines in all regions. 
The overall trend of demand has been consistently increasing. In the 1950s and 1960s, annual growth rates of 15% or more were common. Annual growth of 5-6% persisted through the 1980s and 1990s. Growth rates are not consistent in all regions, but countries with a de-regulated airline industry have more competition and greater pricing freedom. This results in lower fares and sometimes dramatic spurts in traffic growth. The U.S., Australia, Canada, Japan, Brazil, India and other markets exhibit this trend. The industry has been observed to be cyclical in its financial performance. Four or five years of poor earnings precede five or six years of improvement. But profitability even in the good years is generally low, in the range of 2-3% net profit after interest and tax. In times of profit, airlines lease new generations of airplanes and upgrade services in response to higher demand. Since 1980, the industry has not earned back the cost of capital during the best of times. Conversely, in bad times losses can be dramatically worse. Warren Buffett once said that despite all the money that has been invested in all airlines, the net profit is less than zero. He believes it is one of the hardest businesses to manage.
As in many mature industries, consolidation is a trend. Airline groupings may consist of limited bilateral partnerships, long-term, multi-faceted alliances between carriers, equity arrangements, mergers, or takeovers. Since governments often restrict ownership and merger between companies in different countries, most consolidation takes place within a country. In the U.S., over 200 airlines have merged, been taken over, or gone out of business since deregulation in 1978. Many international airline managers are lobbying their governments to permit greater consolidation to achieve higher economy and efficiency.

</doc>
<doc id="1943" url="http://en.wikipedia.org/wiki?curid=1943" title="Australian Democrats">
Australian Democrats

The Australian Democrats is a centrist political party in Australia with a social-liberal ideology. The party was formed in 1977, a merger of the Australia Party and the New Liberal Movement, with former Liberal minister Don Chipp as its high-profile leader. Though never achieving a seat in the House of Representatives, the party had considerable influence in the Senate for the following 30 years. Its representation in the Parliament of Australia ended on 30 June 2008, after loss of its four remaining Senate seats at the 2007 general election. Efforts to restore the party's parliamentary effectiveness have not been successful and, , control was the subject of a lengthy dispute by two factions reportedly associated with two former parliamentarians.
The party was founded on principles of honesty, tolerance, compassion and direct democracy through postal ballots of all members, so that "there should be no hierarchical structure ... by which a carefully engineered elite could make decisions for the members." From the outset, members' participation was fiercely protected in national and divisional constitutions prescribing internal elections, regular meeting protocols, annual conferencesâand monthly journals for open discussion and balloting. Dispute resolution procedures were established, with final recourse to a party ombudsman and membership ballot.
Policies determined by the unique participatory method promoted environmental awareness and sustainability, opposition to the primacy of economic rationalism (Australian neoliberalism), preventative approaches to human health and welfare, animal rights, rejection of nuclear technology and weapons.
The Australian Democrats were the first representatives of green politics at the federal level in Australia. They played a key role in the "cause cÃ©lÃ¨bre" of the Franklin River Dam.
The party's centrist role made it subject to criticism from both the right and left of the political spectrum. In particular, Chipp's former conservative affiliation was frequently recalled by opponents on the left. This problem was to torment later leaders and strategists who, by 1991, were proclaiming "the electoral objective" as a higher priority than the rigorous participatory democracy espoused by the party's founders.
Over three decades, the Australian Democrats achieved representation in the legislatures of the ACT, South Australia, New South Wales, Western Australia and Tasmania as well as Senate seats in all six states. However, at the 2004 and 2007 federal elections, all seven of its Senate seats were lost and the sole remaining State parliamentarian, David Winderlich, left the party and was defeated as an independent in 2010.
History.
1977â79.
On the evening of 29 April 1977, Don Chipp addressed an overflowing Perth Town Hall meeting which unanimously passed a resolution to form a Centre-Line Party, which Chipp was invited to leadâbut he firmly declined to reverse his avowed decision to quit politics, having resigned from the Liberal Party and been offered a lucrative position as a radio public affairs commentator. The Centre-Line Party was the provisional title of the Australian Democrats party. The occasion was a meeting at the Perth Town Hall to which Don Chipp had been invited in the hope that he would accept the position of leader of the new party, which would be an amalgamation of the Australia Party and the New Liberal Movement. On that occasion, Chipp declined to commit himself but did so at a corresponding public meeting in Melbourne on 9 May 1977. Chipp received a standing ovation from over 3,000 people, including former Prime Minister John Gorton, and decided to commit himself to leading the new party which was already being constructed by a national steering committee. The new party was eventually renamed the Australian Democrats by a ballot of its membership. "Fifty-six suggestions produced by members were listed on the ballot paper, including Uniting Australia Party, Australian Centre Line Party, Dinkum Democrats, Practical Idealists of Australia and People for Sanity Party!! After the ballot, the suggestion of the Steering Committee, 'Australian Democrats', was overwhelmingly accepted." The name "Australian Democrats" was already in informal currency before this decision.
The first Australian Democrats (AD) federal parliamentarian was Senator Janine Haines who filled Steele Hall's casual Senate vacancy for South Australia in 1977. Surprisingly, she was not a candidate when the party contested the 1977 federal elections after Don Chipp had agreed to be leader and figurehead. Members and candidates were not lacking in electoral experience, since the Australia Party had been contesting all federal elections since 1969 and the Liberal Movement, in 1974 and 1975. The party's broad aim was to achieve a balance of power in one or more parliaments and to exercise it responsibly in line with policies determined by membership.
The grassroot support attracted by Chipp's leadership was measurable at the party's first electoral test at the 1977 federal election on 10 December, when 9.38 per cent of the total Lower House vote was polled and 11.13 per cent of the Senate vote. At that time, with five Senate seats being contested in each state, the required quota was a daunting 16.66 per cent. However, the first 6-year-term seats were won by Don Chipp (Vic) and Colin Mason (NSW).
1980â82.
The Australian Democrats' first national conference, on 16â17 February 1980, was opened by the distinguished nuclear physicist and former governor of South Australia, Sir Mark Oliphant, who said: "I was privileged to be in the chair at the public meeting in Melbourne when [Don Chipp] announced formation of a new party, dedicated to preserve what freedoms we still retain, and to increase them. A party in which dictatorship from the top was replaced by consensus. A party not ordered about by big business and the rich, or by union bosses. A party where a man could retain freedom of conscience and not thereby be faced with expulsion. A party to which the intelligent individual could belong without having to subscribe to a dogmatic creed. In other words, a democratic party." 
At a Melbourne media conference on 19 September 1980, in the midst of the 1980 election campaign, Chipp described his party's aim as to "keep the bastards honest"âthe "bastards" being the major parties and/or politicians in general. This became a long-lived slogan for the Democrats.
At the October 1980 election, the Democrats polled 9.25 per cent of the Senate vote, electing Janine Haines (SA) and two new senators Michael Macklin (Qld) and John Siddons (Vic), bringing the party's strength to five Senate seats from 1 July 1981 .
A by-election in the South Australian state seat of Mitcham (now Waite) saw Heather Southcott retain the seat for the Democrats in 1982. Since 1955 it had been held by conservative lawyer Robin Millhouse whose New Liberal Movement merged into the Democrats in 1977, and who was resigning to take up a senior judicial appointment. Southcott was defeated later that year at the 1982 state election. Mitcham was the only single-member lower-house seat anywhere in Australia to be won by the Democrats.
1986â90.
Don Chipp resigned from the Senate on 18 August 1986, being succeeded as party leader by Janine Haines and replaced as a senator for Victoria by Janet Powell.
At the 1987 election following a double dissolution, the reduced quota of 7.7% necessary to win a seat assisted the election of three new senators. 6-year terms were won by Paul McLean (NSW) and incumbents Janine Haines (South Australia) and Janet Powell (Victoria). In South Australia, a second senator, John Coulter, was elected for a 3-year term, as were incumbent Michael Macklin (Queensland) and Jean Jenkins (Western Australia).
1990â91.
1990 saw the voluntary departure from the Senate of Janine Haines (a step with which not all Democrats agreed) and the failure of her strategic goal of winning the House of Representatives seat of Kingston.
The casual vacancy was filled by Meg Lees several months before the election of Cheryl Kernot in place of retired deputy leader Michael Macklin. The ambitious Kernot immediately contested the Senate leadership. Being unemployed at the time, she requested and obtained party funds to pay for her travel to address members in all seven divisions. In the event, Victorian Janet Powell was elected as leader and John Coulter was chosen as deputy leader.
Despite the loss of Haines and the WA Senate seat (through an inconsistent national preference agreement with the ALP), the 1990 federal election heralded something of a rebirth for the party, with a dramatic rise in primary vote. This was at the same time as an economic recession was building, and events such as the Gulf War in Kuwait were beginning to shepherd issues of globalisation and transnational trade on to national government agendas.
Virtually alone on the Australian political landscape, Janet Powell consistently attacked both the government and opposition which had closed ranks in support of the Gulf War. Whereas the House of Representatives was thus able to avoid any debate about the war and Australia's participation, the Democrats took full advantage of the opportunity to move for a debate in the Senate.
Possibly because of the party's opposition to the Gulf War, there was mass-media antipathy and negative publicity which some construed as poor media performance by Janet Powell, the party's standing having stalled at about 10%. Before 12 months of her leadership had passed, the South Australian and Queensland divisions were circulating the party's first-ever petition to criticise and oust the parliamentary leader. The explicit grounds related to Powell's alleged responsibility for poor AD ratings in Gallup and other media surveys of potential voting support. When this charge was deemed insufficient, interested party officers and senators reinforced it with negative media 'leaks' concerning her openly established relationship with Sid Spindler and exposure of administrative failings resulting in excessive overtime to a staff member. With National Executive blessing, the party room pre-empted the ballot by replacing the leader with deputy John Coulter. In the process, severe internal divisions were generated. One major collateral casualty was the party whip Paul McLean who resigned and quit the Senate in disgust at what he perceived as in-fighting between close friends. The casual NSW vacancy created by his resignation was filled by Karin Sowada. Powell duly left the party, along with many leading figures of the Victorian branch of the party, and unsuccessfully stood as an Independent candidate when her term expired. In later years, she campaigned for the Australian Greens.
Electoral fortunes.
The Australian Democrats' electoral fortunes have fluctuated throughout their history.
During the Hawke and Keating Labor Governments (1983â96), the Australian Democrats held a theoretical balance of power in the Senate: the numbers were such that they could team with Labor to pass legislation, or team with the Coalition to block legislation on occasions when the Coalition decided to oppose a government bill.
Their power was weakened in 1996 after the Howard Government was elected, and a Labor senator, Mal Colston, resigned from the Labor party. This meant that the Australian Democrats now shared the parliamentary balance of power with two Independent senators. As a result, the Coalition government could often bypass the Australian Democrats, and pass legislation by negotiating with Colston and Brian Harradine. Following the 1998 election the Australian Democrats again held the balance of power, until the Coalition gained a Senate majority at the 2004 election.
The Hawke and Keating governments pursued economic policies that drew on economic rationalist and neo-liberal thought, and the Australian Democrats positioned themselves to the left of the ALP government, and thus at the left end of mainstream Australian politics. Their appeal (and focus on issues beyond the usual "economic" ones that monopolised major party attention) was always greatest amongst tertiary-educated voters. However, the party's progressive politics also remained attractive to a sizeable section of mainly middle class ("wet") Liberal supporters â often female, and often disparagingly described on the right of the Liberal Party as "Soccer Mums" or "Doctor's Wives" â who were turned off by the Liberal party's social conservatism and "Reagonomic/Thatcherite" economic policies. Many Liberals saw their support of the Australian Democrats in the Senate as having "an each way bet", ameliorating the effect of their support for the Liberals in the House of Representatives â an attitude positively fostered, not unsurprisingly, by Democrat politicians and campaigners.
Cheryl Kernot became leader in 1993. She had strong media appeal, which increased media and public awareness of herself and the party. She was known to have interests in industrial relations and was able to cultivate solid relationships with Labor government frontbenchers, which also added to her credibility in the press gallery.
Lack of clear direction other than, possibly, senators' common ambition to play a more productive role in government manifested itself in tensions over Cheryl Kernot's policy on industrial relations (see the Workplace Relations Act 1996). Under Kernot, after negotiations and some compromises from the government, the Australian Democrats voted for the Howard Government's right-leaning industrial relations legislation which decreased union power and allowed a larger role for individual employer-employee contracts.
Kernot, however, remained broadly opposed to the Liberal government. This, together with her personal ambition for a role and contribution to strategy in government, led her to defect to the ALP in 1997. Her replacement as leader was by long-serving deputy, Meg Lees.
Under Lees' leadership, in the 1998 federal election, the Democrats' candidate John Schumann came within 2 per cent of taking Liberal Foreign Minister Alexander Downer's seat of Mayo in the Adelaide Hills under Australia's preferential voting system. The party's Senate representation increased to nine Senators.
Internal conflict and leadership tensions from 2000 to 2002, blamed on the party's support for the Government's Goods and Services Tax (GST), was damaging to the Democrats.
Opposed by the Labor Party, the Australian Greens and independent Senator Brian Harradine, the GST required Democrat support to pass. In an election fought on tax, the Democrats publicly stated that they liked neither the Liberal (GST) tax package or the Labor package, but pledged to work with whichever party was elected to make their tax package better. They campaigned with the slogan "No GST on food".
In 1999, after negotiations with Prime Minister Howard, Meg Lees, Andrew Murray and the party room Senators agreed to support the A New Tax System (ANTS) legislation with exemptions from GST for most food and some medicines, as well as many environmental and social concessions. Five Australian Democrats senators voted in favour. However, two dissident senators on the party's left Natasha Stott Despoja and Andrew Bartlett voted against the GST.
In 2001, a leadership spill saw Meg Lees replaced as leader by Natasha Stott Despoja after a very public and bitter leadership battle. Despite criticism of Stott Despoja's youth and lack of experience, the 2001 election saw the Democrats receive similar media coverage to the previous election. Despite the internal divisions, the Australian Democrats' election result in 2001 was quite good. However, it was not enough to prevent the loss of Vicki Bourne's Senate seat in NSW.
Resulting tensions between Stott Despoja and Lees led to Meg Lees leaving the party in 2002, becoming an independent and forming the Australian Progressive Alliance. Stott Despoja stood down from the leadership following a loss of confidence by her party room colleagues. It led to a protracted leadership battle in 2002, which eventually led to the election of Senator Andrew Bartlett as leader. While the public fighting stopped, the public support for the party remained at record lows.
On 6 December 2003, Bartlett stepped aside temporarily as leader of the party, after an incident in which he swore at Liberal Senator Jeannie Ferris on the floor of Parliament while intoxicated. The party issued a statement stating that Deputy Leader Lyn Allison would serve as the Acting Leader of the party. Bartlett apologised to the Democrats, Jeannie Ferris and the Australian public for his behaviour and assured all concerned that it would never happen again. On 29 January 2004, after seeking medical treatment, Bartlett returned to the Australian Democrats leadership, vowing to abstain from alcohol.
2004.
Support for the Australian Democrats fell significantly at the 2004 federal election in which they achieved only 2.4 per cent of the national vote. Nowhere was this more noticeable than in their key support base of suburban Adelaide in South Australia, where they received between 7 and 31 per cent of the Lower House vote in 2001, and between 1 and 4 per cent in 2004. Three incumbent senators were defeatedâAden Ridgeway (NSW), Brian Greig (WA) and John Cherry (Qld). Following the loss, the customary post-election leadership ballot installed Lyn Allison as leader and Andrew Bartlett as her deputy.
From 1 July 2005 the Australian Democrats lost official parliamentary party status, being represented by only four senators while the governing Liberal-National Coalition gained a majority and potential control of the Senateâthe first time this advantage had been enjoyed by any government since 1980.
2006.
On 5 January 2006, the ABC reported that the Tasmanian Electoral Commission had de-registered that division of the party for failing to provide a list containing the required number of members to be registered for Tasmanian state and local elections.
On 18 March 2006, at the 2006 South Australian state election, the Australian Democrats were reduced to 1.7 per cent of the Legislative Council (upper house) vote. Their sole councillor up for re-election, Kate Reynolds, was defeated.
After the election, South Australian senator Natasha Stott Despoja denied rumours that she was considering quitting the party.
In early July, Richard Pascoe, national and South Australian party president, resigned, citing slumping opinion polls and the poor result in the 2006 South Australian election as well as South Australian parliamentary leader Sandra Kanck's comments regarding the drug MDMA which he saw as damaging to the party.
On 5 July 2006, Australian Democrats senator for Western Australia Andrew Murray announced his intention not to contest the 2007 federal election, citing frustration arising from the Howard Government's control of both houses and his unwillingness to serve another six-year term. His term ended on 30 June 2008.
On 28 August 2006, the founder of the Australian Democrats, Don Chipp, died. Former prime minister Bob Hawke said: "... there is a coincidental timing almost between the passing of Don Chipp and what I think is the death throes of the Democrats. "
On 22 October 2006, Australian Democrats Senator Natasha Stott Despoja announced her intention not to seek re-election at the 2007 federal election due to health concerns. Her term ended on 30 June 2008.
In November 2006, the Australian Democrats fared very poorly in the Victorian state election, receiving a Legislative Council vote tally of only 0.83%, less than half of the party's result in 2002 (1.79 per cent).
2007.
In the New South Wales state election of March 2007, the Australian Democrats lost their last remaining NSW Upper House representative, Arthur Chesterfield-Evans. The party fared poorly, gaining only 1.8 per cent of the Legislative Council vote. A higher vote was achieved in some of the Legislative Assembly seats selectively contested as compared to 2003. However, the statewide vote share fell because the party was unable to field as many candidates as in 2003.
In the Victorian state by-election in Albert Park District the Australian Democrats stood candidate Paul Kavanagh, who polled a respectable 5.75 per cent of the primary vote, despite a large number of candidates, and all media attention focusing on the battle between Labor and Greens candidates.
On 13 September 2007, the ACT Democrats (Australian Capital Territory Division of the party) was deregistered by the ACT Electoral Commissioner, being unable to demonstrate a minimum membership of 100 electors.
The Democrats had no success at the 2007 federal election. Two incumbent senators, Lyn Allison (Victoria) and Andrew Bartlett (Queensland), were defeated, their seats both reverting to major parties. Their two remaining colleagues, Andrew Murray (WA) and Natasha Stott Despoja (SA), did not run for new terms. All four senators' terms expired on 30 June 2008âleaving the Australian Democrats with no federal representation for the first time since its founding in 1977. An ABC report noted that "on the Australian Electoral Commission (AEC) website the party is now referred to just as 'other'".
Post-2007.
Since losing Senate representation, the Australian Democrats have continued to contest state and federal elections without any success. The last of the party's state upper-house members, David Winderlich, resigned from the party in October 2009 and was defeated as an independent at the 2010 election. The South Australian electoral rules were recently tightened, to make it harder (and more expensive) for smaller parties and independents to field candidates. Consequently, the Democrats did not even have a candidate in the most recent (2014) South Australian election, in their state of origin.
In March 2012, the Australian Electoral Commission queried a Democrats submission of 550 names of purported members and proposed deregistering the party for having fewer than 500 members, the threshold needed for registration. The Commission later satisfied itself that the party had sufficient membership to continue its registration.
In the absence of a Liberal candidate, the Democrats polled over 10 percent of the vote at the 2012 Heffron by-election in New South Wales.
Policy.
The party's original support base consisted of voters alienated by perceived unproductive adversarial conflict between the two mainstream parties and an emerging new constituency of people with a desire to participate more effectively in government and to promote concerns for environmental protection and social justice. The party aimed to combine liberal social policies with centrist, particularly neo-Keynesian economics and a progressive environmental platform.
The original agenda included interventionist economic policies, commitment to environmental causes, support for reconciliation with Australia's indigenous population through such mechanisms as formal treaties, pacifist approaches to international relations, open government, constitutional reform, progressive approaches to social issues such as sexuality and drugs, and strong support for human rights and civil liberties. Its membership largely comprised tertiary-educated and middle-class constituents. The party also appealed to voters opposed to untrammeled government power and wishing to have alternative views aired in parliaments and media.
The party has a platform of participatory democracy, with policies supporting proportional representation and citizen-initiated referenda. Many important internal issues (such as electoral preselection and leadership) are decided by direct postal ballot of the membership. Although policies are theoretically set in a similar fashion, Australian Democrats parliamentarians generally had extensive freedom in interpreting them.
However, by 1980, the Australian Democrats had employed the postal-ballot method at both national at state levels to develop an extensive body of written policy covering not only the political agendas of the day but also innovative and far-sighted policies for environmental and economic sustainability, water and energy conservation, e.g., through development of alternative energy sources, expanded public transport, etc. To the community's growing concerns about human rights, the Australian Democrats added finely detailed policies on animal welfare and species preservation. The material is available in election manifestos and copies of the party's journals, obtainable in major public libraries.
In a 2009 "rebuild" process, the party announced creation of a new policy process, attempts to improve internal communication, and envisaged development of a new party constitution.
Prior to the 2013 federal election, the party, though factionally divided into two separate organisations, was able to publish a comprehensive package of member-balloted policies.
Support.
Support for the Democrats historically tended to fluctuate between about 5 and 10 per cent of the population and was geographically concentrated around the wealthy dense CBD and inner-suburban neighbourhoods of the capital cities (especially Adelaide). Therefore, they never managed to win a House of Representatives seat. During the 1980s, 1990s and early 2000s they typically held one or two Senate seats in each state, as well as having some representatives in state parliaments.
Following the internal conflict over GST (1998â2001) and resultant leadership changes, a dramatic decline occurred in the Democrats' membership and voting support in all states. Simultaneously, an increase was recorded in support for the Australian Greens who, by 2004, were supplanting the Democrats as a substantial third party. The trend was noted that year by political scientists Dean Jaensch et al. Elsewhere, Jaensch later suggested it was possible the Democrats could make a political comeback in the federal arena.
Following Tony Abbott's displacement of Malcolm Turnbull as federal leader of the Liberal Party in 2009, the Democrats sought to attract the support of "those Liberals who no longer feel they can support their party".
Federal parliamentary leaders.
Of the party's nine elected federal parliamentary leaders, six were women. Aboriginal senator Aden Ridgeway was deputy leader under Natasha Stott Despoja.

</doc>
<doc id="1944" url="http://en.wikipedia.org/wiki?curid=1944" title="Australian Capital Territory">
Australian Capital Territory

Australian Capital Territory (ACT) (formerly, "The Territory for the Seat of Government" and, later, the "Federal Capital Territory") is a territory in the south east of Australia, enclaved within New South Wales. It is the smaller of the two self-governing internal territories in Australia. The only city and by far the most populous community is Canberra, the capital city of Australia.
The need for a national territory was flagged by colonial delegates during the Federation conventions of the late 19th century. Section 125 of the Australian Constitution provided that, following Federation in 1901, land would be ceded freely to the new Federal Government. The territory was transferred to the Commonwealth by the state of New South Wales in 1911, two years prior to the naming of Canberra as the national capital in 1913. The floral emblem of the ACT is the Royal Bluebell and the bird emblem is the Gang-gang Cockatoo.
Geography.
The ACT is bounded by the Goulburn-Cooma railway line in the east, the watershed of Naas Creek in the south, the watershed of the Cotter River in the west, and the watershed of the Molonglo River in the north-east. The ACT also has a small strip of territory around the southern end of the Beecroft Peninsula, which is the northern headland of Jervis Bay.
Apart from the city of Canberra, the Australian Capital Territory also contains agricultural land (sheep, dairy cattle, vineyards and small amounts of crops) and a large area of national park (Namadgi National Park), much of it mountainous and forested. Small townships and communities located within the ACT include Williamsdale, Naas, Uriarra, Tharwa and Hall.
Tidbinbilla is a locality to the south-west of Canberra that features the Tidbinbilla Nature Reserve and the Canberra Deep Space Communication Complex, operated by the United States' National Aeronautics and Space Administration (NASA) as part of its Deep Space Network.
There are a large range of mountains, rivers and creeks in the Namadgi National Park. These include the Naas and Murrumbidgee Rivers.
Climate.
Because of its elevation and distance from the coast, the Australian Capital Territory experiences four distinct seasons, unlike many other Australian cities whose climates are moderated by the sea. Canberra is noted for its warm to hot, dry summers, and cold winters with occasional fog and frequent frosts. Many of the higher mountains in the territory's south-west are snow-covered for at least part of the winter. Thunderstorms can occur between October and March, and annual rainfall is , with rainfall highest in spring and summer and lowest in winter.
The highest maximum temperature recorded in the ACT was at Acton on 11 January 1939. The lowest minimum temperature was at Gudgenby on 11 July 1971.
Geology.
Notable geological formations in the Australian Capital Territory include the "Canberra Formation", the "Pittman Formation", "Black Mountain Sandstone" and "State Circle Shale".
In the 1840s fossils of brachiopods and trilobites from the Silurian period were discovered at Woolshed Creek near Duntroon. At the time, these were the oldest fossils discovered in Australia, though this record has now been far surpassed. Other specific geological places of interest include the State Circle cutting and the Deakin anticline.
The oldest rocks in the ACT date from the Ordovician around 480 million years ago. During this period the region along with most of Eastern Australia was part of the ocean floor; formations from this period include the "Black Mountain Sandstone" formation and the "Pittman Formation" consisting largely of quartz-rich sandstone, siltstone and shale. These formations became exposed when the ocean floor was raised by a major volcanic activity in the Devonian forming much of the east coast of Australia.
Governance.
The ACT has internal self-government, but Australia's Constitution does not afford the territory government the full legislative independence provided to Australian states. Laws are made in a 17-member Legislative Assembly that combines both state and local government functions.
Members of the Legislative Assembly are elected via the Hare Clarke system. The ACT Chief Minister (currently Katy Gallagher, Australian Labor Party) is elected by members of the ACT Assembly. The ACT Government Chief Minister is a member of the Council of Australian Governments.
Unlike other self-governing Australian territories (for example, the Northern Territory), the ACT does not have an Administrator. The Crown is represented by the Australian Governor-General in the government of the ACT. Until 4 December 2011, the decisions of the assembly could be overruled by the Governor-General (effectively by the national government) under section 35 of the Australian Capital Territory (Self-Government) Act 1988, although the federal parliament voted in 2011 to abolish this veto power, instead requiring a majority of both houses of the federal parliament to override an enactment of the ACT. The Chief Minister performs many of the roles that a state governor normally holds in the context of a state; however, the Speaker of the Legislative Assembly gazettes the laws and summons meetings of the Assembly.
In Australia's Federal Parliament, the ACT is represented by four federal members: two members of the House of Representatives; the Division of Fraser and the Division of Canberra and is one of only two territories to be represented in the Senate, with two Senators (the other being the Northern Territory). The Member for Fraser and the ACT Senators also represent the constituents of the Jervis Bay Territory.
In 1915 the "Jervis Bay Territory Acceptance Act 1915" created the Jervis Bay Territory as an annexe to the Australian Capital Territory. In 1988, when the ACT gained self-government, Jervis Bay became a separate territory administered by the Australian Government Minister responsible for Territories, presently the Minister for Home Affairs.
The ACT retains a small area of territory on the coast on the Beecroft Peninsula, consisting of a strip of coastline around the northern headland of Jervis Bay (not to be confused with the Jervis Bay Territory, which is on the southern headland of the Bay). The ACT's land on the Beecroft Peninsula is an "exclave", that is, an area of territory not physically connected to the main part of the ACT. Interestingly, this ACT exclave surrounds a small exclave of NSW territory, namely the Point Perpendicular lighthouse which is at the southern tip of the Beecroft Peninsula. The lighthouse and its grounds are New South Wales territory, but cut off from the rest of the state by the strip of ACT land. This is a geographic curiosity: an exclave of NSW land enclosed by an exclave of ACT land.
Administration.
ACT Ministers implement their executive powers through the following government directorates:
Demographics.
In the 2011 census the population of the ACT was 357,222 of whom most lived in Canberra. The ACT median weekly income for people aged over 15 was in the range $600â$699 while that for the population living outside Canberra was at the national average of $400â$499. The average level of degree qualification in the ACT is higher than the national average. Within the ACT 4.5% of the population have a postgraduate degree compared to 1.8% across the whole of Australia.
Urban structure.
Canberra is a planned city that was originally designed by Walter Burley Griffin, a major 20th century American architect. Major roads follow a wheel-and-spoke pattern rather than a grid. The city centre is laid out on two perpendicular axes: a water axis stretching along Lake Burley Griffin, and a ceremonial land axis stretching from Parliament House on Capital Hill north-eastward along ANZAC Parade to the Australian War Memorial at the foot of Mount Ainslie.
The area known as the Parliamentary Triangle is formed by three of Burley Griffin's axes, stretching from Capital Hill along Commonwealth Avenue to the Civic Centre around City Hill, along Constitution Avenue to the Defence precinct on Russell Hill, and along Kings Avenue back to Capital Hill.
The larger scheme of Canberra's layout is based on the three peaks surrounding the city, Mount Ainslie, Black Mountain, and Red Hill. The main symmetrical axis of the city is along ANZAC Parade and roughly on the line between Mount Ainslie and Bimberi Peak. Bimberi Peak being the highest mountain in the ACT approximately south west of Canberra . The precise alignment of ANZAC parade is between Mount Ainslie and Capital Hill (formally Kurrajong Hill).
The Griffins assigned spiritual values to Mount Ainslie, Black Mountain, and Red Hill and originally planned to cover each of these in flowers. That way each hill would be covered with a single, primary color which represented its spiritual value. This part of their plan never came to fruition. In fact, WWI interrupted the construction and some conflicts after the war made it a difficult process for the Griffins. Nevertheless, Canberra stands as an exemplary city design and is located halfway between the ski slopes and the beach. It enjoys a natural cooling from geophysical factors.
The urban areas of Canberra are organised into a hierarchy of districts, town centres, group centres, local suburbs as well as other industrial areas and villages. There are seven districts (with an eighth currently under construction), each of which is divided into smaller suburbs, and most of which have a town centre which is the focus of commercial and social activities. The districts were settled in the following chronological order:
The North and South Canberra districts are substantially based on Walter Burley Griffin's designs. In 1967 the then National Capital Development Commission adopted the "Y Plan" which laid out future urban development in Canberra around a series of central shopping and commercial area known as the 'town centres' linked by freeways, the layout of which roughly resembled the shape of the letter Y, with Tuggeranong at the base of the Y and Belconnen and Gungahlin located at the ends of the arms of the Y.
Development in Canberra has been closely regulated by government, both through the town planning process, but also through the use of crown lease terms that have tightly limited the use of parcels of land. All land in the ACT is held on 99 year leases from the national government, although most leases are now administered by the Territory government.
Most suburbs have their own local shops, and are located close to a larger shopping centre serving a group of suburbs. Community facilities and schools are often also located near local shops or group shopping centres. Many of Canberra's suburbs are named after former Prime Ministers, famous Australians, early settlers, or use Aboriginal words for their title.
Street names typically follow a particular theme; for example, the streets of Duffy are named after Australian dams and reservoirs, the streets of Dunlop are named after Australian inventions, inventors and artists and the streets of Page are named after biologists and naturalists. Most diplomatic missions are located in the suburbs of Yarralumla, Deakin and O'Malley. There are three light industrial areas: the suburbs of Fyshwick, Mitchell and Hume.
Education.
Almost all educational institutions in the Australian Capital Territory are located within Canberra. The ACT public education system schooling is normally split up into Pre-School, Primary School (K-6), High School (7â10) and College (11â12) followed by studies at university or CIT (Canberra Institute of Technology). Many private high schools include years 11 and 12 and are referred to as colleges. Children are required to attend school until they turn 17 under the ACT Government's "Learn or Earn" policy.
In February 2004 there were 140 public and non-governmental schools in Canberra; 96 were operated by the Government and 44 are non-Government. In 2005 there were 60,275 students in the ACT school system. 59.3% of the students were enrolled in government schools with the remaining 40.7% in non-government schools. There were 30,995 students in primary school, 19,211 in high school, 9,429 in college and a further 340 in special schools.
As of May 2004, 30% of people in the ACT aged 15â64 had a level of educational attainment equal to at least a bachelor's degree, significantly higher than the national average of 19%. The two main tertiary institutions are the Australian National University (ANU) in Acton and the University of Canberra (UC) in Bruce. There are also two religious university campuses in Canberra: Signadou is a campus of the Australian Catholic University and St Mark's Theological College is a campus of Charles Sturt University. Tertiary level vocational education is also available through the multi-campus Canberra Institute of Technology.
The Australian Defence Force Academy (ADFA) and the Royal Military College, Duntroon (RMC) are in the suburb of Campbell in Canberra's inner northeast. ADFA teaches military undergraduates and postgraduates and is officially a campus of the University of New South Wales while Duntroon provides Australian Army Officer training.
The Academy of Interactive Entertainment (AIE) offers courses in computer game development and 3D animation.

</doc>
<doc id="1946" url="http://en.wikipedia.org/wiki?curid=1946" title="Unit of alcohol">
Unit of alcohol

Units of alcohol are a measure of the volume of pure alcohol in an alcoholic beverage. They are used in some countries as a guideline for alcohol consumption. In some other countries a "standard drink", different from country to country, is defined for the same purpose.
One unit of alcohol is defined as 10 millilitres (7.9 grams) in the United Kingdom; typical drinks provide 1â3 units. In Australia "a 'standard drink' is the amount of a beverage that contains ten grams of alcohol at 20 degrees Celsius (12.7Â ml)". In the US a "standard" drink is one that contains about 0.6 US fluid ounces or 14 grams of alcohol, about 77% more than a UK unit.
Labelling is usually required to give an indication of alcoholic content of a serving. In the United Kingdom the number of units in a typical serving is printed; Australia requires that "the label on a package of an alcoholic beverage must include a statement of the number of standard drinks in the package".
A typical healthy adult can metabolise about one unit per hour, depending on many factors.
Formula.
The number of UK units of alcohol in a drink can be determined by multiplying the volume of the drink (in millilitres) by its percentage ABV, and dividing by 1000.
For example, one imperial pint (568 ml) of beer at 4% ABV contains:
formula_1
The formula uses . This results in exactly one unit per percentage point per litre, of any alcoholic beverage.
Since 4% can be expressed as .04, gives the amount of alcohol in terms of mlâwhich, when divided by , shows the number of units.
When the volume of an alcoholic drink is shown in centilitres, determining the number of units in a drink is as simple as volume Ã percentage (converted into a fraction of 1).
Thus, 750 millilitres of wine (the contents of a standard wine bottle) at 12% ABV contain:
formula_2
Quantities.
It is often stated that a unit of alcohol is supplied by a small glass of wine, half a pint of beer, or a single measure of spirits. Such statements may be misleading because they do not reflect differences in strength of the various kinds of wines, beers, and spirits.
The advent of smartphones led to the creation of apps which inform consumers of the number of units contained in an alcoholic drink.
Spirits.
Most spirits sold in the United Kingdom have 40% ABV or slightly less. In England a single pub measure (25 ml) of a spirit contains one unit. However, a larger 35ml measure is increasingly used (and in particular is standard in Northern Ireland ), which contains 1.4 units of alcohol at 40% ABV. Sellers of spirits by the glass must state the capacity of their standard measure in ml.
Time to metabolise.
On average, it takes about one hour for the body to metabolise (break down) one unit of alcohol. However, this can vary with body weight, sex, age, personal metabolic rate, recent food intake, the type and strength of the alcohol, and
medications taken. Alcohol may be metabolised more slowly if liver function is impaired.
Recommended maximum.
From 1992 to 1995 the UK government advised that men should drink no more than 21 units per week, and women no more than 14. (The difference between the sexes was due to the typically lower weight and water-to-body-mass ratio of women.) The Times reported in October 2007 that these limits had been "plucked out of the air" and had no scientific basis.
This was changed after a government study showed that many people were in effect "saving up" their units and using them at the end of the week, a phenomenon referred to as binge drinking. Since 1995 the advice was that regular consumption of 3â4 units a day for men, or 2â3 units a day for women, would not pose significant health risks, but that consistently drinking four or more units a day (men), or three or more units a day (women), is not advisable.
An international study of about 6,000 men and 11,000 women for a total of 75,000 person-years found that people who reported that they drank more than a threshold value of 2 units of alcohol a day had a higher risk of fractures than non-drinkers. For example, those who drank over 3 units a day had nearly twice the risk of a hip fracture.

</doc>
<doc id="1947" url="http://en.wikipedia.org/wiki?curid=1947" title="Aotus">
Aotus

Aotus may refer to:
The name is derived from the Ancient Greek words for "earless" in both cases: the monkey is missing external ears, and the pea is missing earlike bracteoles.

</doc>
<doc id="1948" url="http://en.wikipedia.org/wiki?curid=1948" title="Ally McBeal">
Ally McBeal

Ally McBeal is an American legal comedy-drama television series, originally aired on Fox from September 8, 1997 to May 20, 2002. Created by David E. Kelley, the series stars Calista Flockhart in the title role as a young lawyer working in the fictional Boston law firm Cage and Fish, with other young lawyers whose lives and loves were eccentric, humorous and dramatic. The series placed #48 on "Entertainment Weekly" 2007 "New TV Classics" list.
Overview.
The series, set in the fictional Boston law firm Cage and Fish, begins with main character Allison Marie "Ally" McBeal joining the firm (co-owned by her law school classmate Richard Fish) after leaving her previous job due to sexual harassment. On her first day Ally is horrified to find that she will be working alongside her ex-boyfriend Billy Thomasâwhom she has never gotten overâand to make things worse, Billy is now married to fellow lawyer Georgia, who also later joins Cage and Fish. The triangle among the three forms the basis for the main plot for the show's first three seasons.
Although ostensibly a legal drama, the main focus of the series was the romantic and personal lives of the main characters, often using legal proceedings as plot devices to contrast or reinforce a character's drama. For example, bitter divorce litigation of a client might provide a backdrop for Ally's decision to break up with a boyfriend. Legal arguments were also frequently used to explore multiple sides of various social issues.
Cage & Fish (which becomes Cage/Fish & McBeal or Cage, Fish, & Associates towards the end of the series), the fictional law firm where most of the characters work, is depicted as a highly sexualized environment, symbolized by its unisex restroom. Lawyers and secretaries in the firm routinely date, flirt with, or have a romantic history with each other, and frequently run into former or potential romantic interests in the courtroom or on the street outside.
The series had many offbeat and frequently surreal running gags and themes, such as Ally's tendency to immediately fall over whenever she met somebody she found attractive, or Richard Fish's wattle fetish and humorous mottos ("Fishisms" & "Bygones"), or John's gymnastic dismounts out of the office's unisex bathroom stalls, that ran through the series. The show used vivid, dramatic fantasy sequences for Ally's and other characters' wishful thinking; particularly notable is the dancing baby.
The series also featured regular visits to a local bar where singer Vonda Shepard regularly performed (though occasionally handing over the microphone to the characters). The series also took place in the same continuity as David E. Kelley's legal drama "The Practice" (which aired on ABC), as the two shows crossed over with one another on occasion, a very rare occurrence for two shows that aired on different networks.
Episodes.
In Australia, Ally McBeal was aired from the Seven Network from 1997 to 2002. In 2010, Ally McBeal was aired repeatedly with Network Ten.
Crossovers with "The Practice".
Seymore Walsh, a stern judge often exasperated by the eccentricities of the Cage & Fish lawyers and played by actor Albert Hall, was also a recurring character on "The Practice". In addition, Judge Jennifer 'Whipper' Cone appears on "The Practice" episode "Line of Duty" (S02E15), while Judge Roberta Kittelson, a recurring character on "The Practice", has a featured guest role in the "Ally McBeal" episode "Do you Wanna Dance?"
Most of the primary "Practice" cast members guest starred in the "Ally McBeal" episode "The Inmates" (S01E20), in a storyline that concluded with the "Practice" episode "Axe Murderer" (S02E26), featuring Calista Flockhart and Gil Bellows reprising their "Ally" characters; what's unique about this continuing storyline is that "Ally McBeal" and "The Practice" happened to air on different networks. Bobby Donnell, the main character of "The Practice" played by Dylan McDermott, was featured heavily in both this crossover and another "Ally McBeal" episode, "These are the Days."
Regular "Practice" cast members Lara Flynn Boyle and Michael Badalucco each had a cameo in "Ally McBeal" (Boyle as a woman who trades insults with Ally in the episode "Making Spirits Bright" and Badalucco as one of Ally's dates in the episode "I Know him by Heart") but it remains ambiguous whether they were playing the same characters they play on "The Practice".
Reception.
The show's ratings began to decline in the third season, but stabilized in the fourth season after Robert Downey, Jr. joined the regular cast as Ally's boyfriend Larry Paul, and a fresher aesthetic was created by new art director Matthew DeCoste. However, Downey's character was written out after the end of the season due to the actor's troubles with drug addiction.
Along with "Dharma & Greg", "Ally McBeal" was one of the last two surviving shows to debut during the 1997-98 season, one of the weakest in television history for new shows. (Only seven shows to debut would be picked up for a second season, and only "Dharma & Greg" and "Ally McBeal" would last longer than three seasons, each providing enough episodes for syndication.) Both shows ended at the end of the 2001-02 season, five years after their debut.
Feminist criticism.
Despite its success, "Ally McBeal" did receive some negative criticism from TV critics and feminists who found the title character annoying and demeaning to women (specifically professional women) because of her perceived flightiness, lack of demonstrated legal knowledge, short skirts, and extreme emotional instability. Perhaps the most notorious example of the debate sparked by the show was the June 29, 1998 cover story of "Time" magazine, which juxtaposed McBeal with three pioneering feminists (Susan B. Anthony, Betty Friedan, Gloria Steinem) and asked "Is Feminism Dead?" In episode 12 of the second season of the show, Ally talks to her co-worker John Cage about a dream she had, saying "You know, I had a dream that they put my face on the cover of "Time" magazine as 'the face of feminism'."
Music.
"Ally McBeal" was a heavily music-oriented show. Vonda Shepard, a virtually unknown musician at the time, was featured continually on the show. Her song "Searchin' My Soul" became the show's theme song. Many of the songs Shepard performed were established hits with lyrics that paralleled the events of the episode, including "Both Sides Now", "Hooked on a Feeling" and "Tell Him". Besides recording background music for the show, Shepard frequently appeared at the ends of episodes as a musician performing at a local piano bar frequented by the main characters. On rare occasions, her character would have conventional dialogue. A portion of "Searchin' My Soul" was played at the beginning of each episode, but remarkably the song was never played in its entirety.
Several of the characters had a musical leitmotif that played when they appeared. John Cage's was "My Everything", Ling Woo's was the Wicked Witch of the West theme from "The Wizard of Oz", and Ally McBeal herself picked "Tell Him", when told by a psychiatrist that she needed a theme.
Due to the popularity of the show and Shepard's music, a soundtrack titled "Songs from Ally McBeal" was released in 1998, as well as a successor soundtrack titled "Heart and Soul: New Songs From Ally McBeal" in 1999. Two compilation albums from the show featuring Shepard were also released in 2000 and 2001. A Christmas album was also released under the title "Ally McBeal: A Very Ally Christmas". The album received positive reviews, and Shephardâs version of Kay Starrâs Christmas song (Everybody's Waitin' For) The Man with the Bag, received considerable airplay during the holiday season.
Other artists featured on the show include Barry White, Al Green, Tina Turner, Anastacia, Elton John, Sting and Mariah Carey. Josh Groban played the role of Malcolm Wyatt in the May 2001 season finale, performing "You're Still You". The series creator, David E. Kelley, was impressed with Groban's performance at The Family Celebration event and based on the audience reaction to Groban's singing, Kelley created a character for him in that finale. The background score for the show was composed by Danny Lux.
DVD releases.
Due to music licensing issues, none of the seasons of "Ally McBeal" were available on DVD in the United States (only 6 random episodes can be found on the R1 edition) until 2009, though it has been available in Italy, Belgium, the Netherlands, Japan, Hong Kong, Portugal, Spain, France, Germany, the United Kingdom, Mexico, Taiwan, Australia, Brazil and the Czech Republic with all the show's music intact since 2005. In the UK, Ireland and Spain all seasons are available in a complete box set.
20th Century Fox released the complete first season on DVD in Region 1 on October 6, 2009. They also released a special complete series edition on the same day. Season 1 does not contain any special features, the complete series set however does contain several bonus features including featurettes, an all-new retrospective, the episode of The Practice in which Calista Flockhart guest starred and a bonus disc entitled "The Best of Ally McBeal Soundtrack". In addition, both releases contain all of the original music. Season 2 was released on April 6, 2010. Seasons 3, 4 and 5 were all released on October 5, 2010. Season 1 and 2 are also available on the US iTunes Store.
"Ally" (1999).
In 1999, at the height of the show's popularity, a half-hour version entitled "Ally" began airing in parallel with the main program. This version, designed in a sitcom format, used re-edited scenes from the main program, along with previously unseen footage. The intention was to further develop the plots in the comedy-drama in a sitcom style. It also focused only on Ally's personal life, cutting all the courtroom plots. The repackaged show was cancelled partway through its initial run. While 13 episodes of "Ally" were created, only 10 were actually broadcast.
In popular culture.
McBeal and 1990s young affluent professional women were parodied in the song "Ally McBeal" (tune of "Like a Rolling Stone" by Bob Dylan) by a cappella group Da Vinci's Notebook on their album "The Life and Times of Mike Fanning", released in 2000.
In episode 2, season 3 of the British comedy "The Adam and Joe Show", the show was parodied as 'Ally McSqeal' using soft toys.
The season 2 episode of "Futurama", "When Aliens Attack", featured a parody of the show entitled "Single Female Lawyer". The principal crux of the parody was that, effectively, "Single Female Lawyer" had no discernible plot other than the fact that the female lead was very attractive, wore a short skirt, and slept with her clients. The show has been broadcast into space for centuries, but the last episode was missing (due to Philip Fry's incompetence and time travel) and so a warlike alien race, who had become hooked on the show, demanded that Earth either play out the final episode for them or they would ignite the planet's atmosphere. Luckily, due to the nature of the show being little more than fan service, it was easy for Fry, Leela and the others to replicate it by simply putting Leela in a miniskirt and ad libbing the dialogue on the spot.

</doc>
<doc id="1949" url="http://en.wikipedia.org/wiki?curid=1949" title="Andreas Capellanus">
Andreas Capellanus

Andreas Capellanus ("Capellanus" meaning "chaplain"), also known as Andrew the Chaplain, and occasionally by a French translation of his name, AndrÃ© le Chapelain, was the 12th-century author of a treatise commonly known as "De amore" ("About Love"), and often known in English, somewhat misleadingly, as "The Art of Courtly Love", though its realistic, somewhat cynical tone suggests that it is in some measure an antidote to courtly love. Little is known of Andreas Capellanus's life, but he is presumed to have been a courtier of Marie of Troyes, and probably of French origin.
His work.
"De Amore" was written at the request of Marie de Champagne, daughter of King Louis VII of France and of Eleanor of Aquitaine. In it, the author informs a young pupil, Walter, of the pitfalls of love. A dismissive allusion in the text to the "wealth of Hungary" has suggested the hypothesis that it was written after 1184, at the time when Bela III of Hungary had sent to the French court a statement of his income and had proposed marriage to Marie's sister Marguerite of France, but before 1186, when his proposal was accepted. 
"De Amore" is made up of three books. The first book covers the etymology and definition of love and is written in the manner of an academic lecture. The second book consists of sample dialogues between members of different social classes; it outlines how the romantic process between the classes should work. Book three is made of stories from actual courts of love presided over by noble women. 
John Jay Parry, the editor of one modern edition of "De Amore", quotes critic Robert Bossuat as describing "De Amore" as "one of those capital works which reflect the thought of a great epoch, which explains the secret of a civilization". It may be viewed as didactic, mocking, or merely descriptive; in any event it preserves the attitudes and practices that were the foundation of a long and significant tradition in Western literature.
The social system of "courtly love", as gradually elaborated by the ProvenÃ§al troubadours from the mid twelfth century, soon spread. One of the circles in which this poetry and its ethic were cultivated was the court of Eleanor of Aquitaine (herself the granddaughter of an early troubadour poet, William IX of Aquitaine). It has been claimed that "De Amore" codifies the social and sexual life of Eleanor's court at Poitiers between 1170 and 1174, though it was evidently written at least ten years later and, apparently, at Troyes. It deals with several specific themes that were the subject of poetical debate among late twelfth century troubadours and trobairitz.
The meaning of "De Amore" has been debated over the centuries. In the years immediately following its release many people took Andreasâ opinions concerning Courtly Love seriously. In more recent times, however, scholars have come to view the priestâs work as satirical. Many scholars now agree that Andreas was commenting on the materialistic, superficial nature of the nobles of the Middle Ages. Andreas seems to have been warning young Walter, his protege, about love in the Middle Ages.

</doc>
<doc id="1950" url="http://en.wikipedia.org/wiki?curid=1950" title="American Civil Liberties Union">
American Civil Liberties Union

The American Civil Liberties Union (ACLU) is a nonpartisan non-profit organization whose stated mission is "to defend and preserve the individual rights and liberties guaranteed to every person in this country by the Constitution and laws of the United States." It works through litigation, lobbying, and community education. Founded in 1920 by Roger Baldwin, Crystal Eastman, George Kessler, Helen Keller and Walter Nelles, the ACLU has over 500,000 members and has an annual budget over $100Â million. Local affiliates of the ACLU are active in all 50 states and Puerto Rico. The ACLU provides legal assistance in cases when it considers civil liberties to be at risk. Legal support from the ACLU can take the form of direct legal representation, or preparation of "amicus curiae" briefs expressing legal arguments (when another law firm is already providing representation).
When the ACLU was founded in 1920, its focus was on freedom of speech, primarily for anti-war protesters. During the 1920s, the ACLU expanded its scope to include protecting the free speech rights of artists and striking workers, and working with the National Association for the Advancement of Colored People (NAACP) to combat racism and discrimination. During the 1930s, the ACLU started to engage in work combating police misconduct and for Native American rights. Most of the ACLU's cases came from the Communist party and Jehovah's Witnesses. In 1940, the ACLU leadership was caught up in the Red Scare, and voted to exclude Communists from its leadership positions. During World War II, the ACLU defended Japanese-American citizens, unsuccessfully trying to prevent their forcible relocation to internment camps. During the Cold War, the ACLU headquarters was dominated by anti-communists, but many local affiliates defended members of the Communist Party.
By 1964, membership had risen to 80,000, and the ACLU participated in efforts to expand civil liberties. In the 1960s, the ACLU continued its decades-long effort to enforce separation of church and state. It defended several anti-war activists during the Vietnam War. The ACLU was involved in the "Miranda" case, which addressed misconduct by police during interrogations; and in the "New York Times" case, which established new protections for newspapers reporting on government activities. In the 1970s and 1980s, the ACLU ventured into new legal areas, defending homosexuals, students, prisoners, and the poor. In the twenty-first century, the ACLU has fought the teaching of creationism in public schools and challenged some provisions of anti-terrorism legislation as infringing on privacy and civil liberties.
In addition to representing persons and organizations in lawsuits, the ACLU lobbies for policies that have been established by its board of directors. Current positions of the ACLU include: opposing the death penalty; supporting same-sex marriage and the right of gays to adopt; supporting birth control and abortion rights; eliminating discrimination against women, minorities, and LGBT people; supporting the rights of prisoners and opposing torture; supporting the right of religious persons to practice their faiths without government interference; and opposing government preference for religion over non-religion, or for particular faiths over others.
Organization.
Leadership.
The ACLU is led by an executive director and a president, Anthony Romero and Susan Herman, respectively, in 2011. The president acts as chairman of the ACLU's board of directors, leads fundraising, and facilitates policy-setting. The executive director manages the day-to-day operations of the organization. The board of directors consists of 80 persons, including representatives from each state affiliate, as well as at-large delegates.
The leadership of the ACLU does not always agree on policy decisions; differences of opinion within the ACLU leadership have sometimes grown into major debates. In 1937, an internal debate erupted over whether to defend Henry Ford's right to distribute anti-union literature. In 1939, a heated debate took place over whether to prohibit communists from serving in ACLU leadership roles. During the early 1950s the board was divided on whether to defend communists persecuted under McCarthyism. In 1968, a schism formed over whether to represent Dr. Spock's anti-war activism. In 1973, there was internal conflict over whether to call for the impeachment of Richard Nixon. In 2005, there was internal conflict about whether or not a gag rule should be imposed on ACLU employees to prevent publication of internal disputes.
Funding.
The ACLU consists of two separate non-profit organizations: the ACLU, and the ACLU Foundation. Both organizations engage in litigation, advocacy of civil rights, and education. The ACLU is a 501(c)(4) corporation which also engages in political lobbying, and donations to that component of the ACLU are not tax deductible. The ACLU Foundation is a 501(c)(3) non-profit corporation, which does not engage in lobbying, and donations to it are tax deductible.
In 2011, the ACLU and the ACLU Foundation had a combined income of $109Â million, originating from grants (60%), membership donations (23%), and bequests (17%). Membership dues account for $25Â million per year and are treated as donations; members choose the amount they pay annually, averaging $50 per member per year. In 2011, the combined expenses of the ACLU and ACLU Foundation were $106Â million, spent on Programs (88%), management (7%), and fundraising (5%). The ACLU Foundation accounts for about 75% of the combined budget, and the ACLU about 25%.
The ACLU solicits donations to its charitable foundation. The ACLU is accredited by the Better Business Bureau, and the Charity Navigator has ranked the ACLU with a four-star rating. The local affiliates also solicit their own funding, and some receive funds from the national ACLU. The distribution and amount of funding for state affiliates varies from state to state. Smaller affiliates with fewer resources, such as that in Nebraska, receive subsidies from the national ACLU.
In October 2004, the ACLU rejected $1.5Â million from both the Ford Foundation and Rockefeller Foundation because the Foundations had adopted language from the USA PATRIOT Act in their donation agreements, including a clause stipulating that none of the money would go to "underwriting terrorism or other unacceptable activities." The ACLU views this clause, both in Federal law and in the donors' agreements, as a threat to civil liberties, saying it is overly broad and ambiguous.
Due to the nature of its legal work, the ACLU is often involved in litigation against governmental bodies, which are generally protected from adverse monetary judgments; a town, state or federal agency may be required to change its laws or behave differently, but not to pay monetary damages except by an explicit statutory waiver. In some cases, the law permits plaintiffs who successfully sue government agencies to collect money damages or other monetary relief. In particular, the Civil Rights Attorney's Fees Award Act of 1976 leaves the government liable in some civil rights cases. Fee awards under this civil rights statute are considered "equitable relief" rather than damages, and government entities are not immune from equitable relief. Under laws such as this, the ACLU and its state affiliates sometimes share in monetary judgments against government agencies. In 2006, the Public Expressions of Religion Protection Act sought to prevent monetary judgments in the particular case of violations of church-state separation.
The ACLU has received court awarded fees from opponents, for example, the Georgia affiliate was awarded $150,000 in fees after suing a county demanding the removal of a Ten Commandments display from its courthouse; a second Ten Commandments case in the State, in a different county, led to a $74,462 judgment. The State of Tennessee was required to pay $50,000, the State of Alabama $175,000, and the State of Kentucky $121,500, in similar Ten Commandments cases.
State affiliates.
Most of the organization's workload is performed by the 53 local affiliates. There is an affiliate in each state and in Puerto Rico. California has three affiliates. The affiliates operate autonomously from the national organization; each affiliate has its own staff, executive director, board of directors, and budget. Each affiliate consists of two non-profit corporations: a 501(c)(3) corporation that does not perform lobbying, and a 501(c)(4) corporation which is entitled to lobby.
ACLU affiliates are the basic unit of the ACLU's organization and engage in litigation, lobbying, and public education. For example, in a twenty-month period beginning January 2004, the ACLU's New Jersey chapter was involved in fifty-one cases according to their annual reportâthirty-five cases in state courts, and sixteen in federal court. They provided legal representation in thirty-three of those cases, and served as amicus in the remaining eighteen. They listed forty-four volunteer attorneys who assisted them in those cases.
Positions.
The ACLU's official position statements, as of January 2012, included the following policies:
Support and opposition.
The ACLU is supported by a variety of persons and organizations. There were over 500,000 members in 2011, and the ACLU annually receives thousands of grants from hundreds of charitable foundations. Allies of the ACLU in legal actions have included the National Association for the Advancement of Colored People, the American Jewish Congress, People For the American Way, the National Rifle Association, the Electronic Frontier Foundation, Americans United for Separation of Church and State, and the National Organization for Women.
The ACLU has been criticized by liberals, such as when it excluded communists from its leadership ranks, when it defended Neo-Nazis, when it declined to defend Paul Robeson, or when it opposed the passage of the National Labor Relations Act. Conversely, it has been criticized by conservatives, such as when it argued against official prayer in public schools, or when it opposed the Patriot Act. The ACLU has supported conservative figures such as Rush Limbaugh, George Wallace, Henry Ford, and Oliver North; and it has supported liberal figures such as Dick Gregory, H. L. Mencken, Rockwell Kent, and Dr. Benjamin Spock.
A major source of criticism are legal cases in which the ACLU represents an individual or organization that promotes offensive or unpopular viewpoints, such as the Ku Klux Klan, Neo-Nazis, Nation of Islam, North American Man/Boy Love Association, or Westboro Baptist Church. The ACLU responded to these criticisms by stating "It is easy to defend freedom of speech when the message is something many people find at least reasonable. But the defense of freedom of speech is most critical when the message is one most people find repulsive."
Early years.
CLB era.
The ACLU developed from the National Civil Liberties Bureau (CLB), co-founded in 1917 during the Great War by Crystal Eastman, an attorney activist, and Roger Nash Baldwin. The focus of the CLB was on freedom of speech, primarily anti-war speech, and on supporting conscientious objectors who did not want to serve in World War I.
Three United States Supreme Court decisions in 1919 each upheld convictions under laws against certain kinds of anti-war speech. In 1919, the Court upheld the conviction of Socialist Party leader Charles Schenck for publishing anti-war literature. In "Debs v. United States," the court upheld the conviction of Eugene Debs. While the Court upheld a conviction a third time in "Abrams v. United States", Justice Oliver Wendell Holmes wrote an important dissent which has gradually been absorbed as an American principle: he urged the court to treat freedom of speech as a fundamental right, which should rarely be restricted.
In 1918 Crystal Eastman resigned from the organization due to health issues. After assuming sole leadership of the CLB, Baldwin insisted that the organization be reorganized. He wanted to change its focus from litigation to direct action and public education.
The CLB directors concurred, and on January 19, 1920, they formed an organization under a new name, the American Civil Liberties Union. Although a handful of other organizations in the United States at that time focused on civil rights, such as the National Association for the Advancement of Colored People (NAACP) and Anti-Defamation League (ADL), the ACLU was the first that did not represent a particular group of persons, or a single theme. Like the CLB, the NAACP pursued litigation to work on civil rights, including efforts to overturn the disfranchisement of African Americans in the South that had taken place since the turn of the century.
During the first decades of the ACLU, Baldwin continued as its leader. His charisma and energy attracted many supporters to the ACLU board and leadership ranks. Baldwin was ascetic, wearing hand-me-down clothes, pinching pennies, and living on a very small salary. The ACLU was directed by an executive committee, but it was not particularly democratic or egalitarian. The ACLU's base in New York resulted in its being dominated by people from the city and state. Most ACLU funding came from philanthropies, such as the Garland Fund.
Free speech era.
In the 1920s, government censorship was commonplace. Magazines were routinely confiscated under the anti-obscenity Comstock laws; permits for labor rallies were often denied; and virtually all anti-war or anti-government literature was outlawed. Right-wing conservatives wielded vast amounts of power, and activists that promoted unionization, socialism, or government reform were often denounced as un-American or unpatriotic. In one typical instance in 1923, author Upton Sinclair was arrested for trying to read the First Amendment during an Industrial Workers of the World rally.
ACLU leadership was divided on how to challenge the civil rights violations. One faction, including Baldwin, Arthur Garfield Hays and Norman Thomas, believed that direct, militant action was the best path. Hays was the first of many successful attorneys that relinquished their private practices to work for the ACLU. Another group, including Walter Nelles and Walter Pollak felt that lawsuits taken to the Supreme Court were the best way to achieve change. Both groups worked in tandem, but equally worshipped the Bill of Rights and the US Constitution.
During the 1920s, the ACLU's primary focus was on freedom of speech in general, and speech within the labor movement particularly. Because most of the ACLU's efforts were associated with the labor movement, the ACLU itself came under heavy attack from conservative groups, such as the American Legion, the National Civic Federation, and Industrial Defense Association and the Allied Patriotic Societies.
In addition to labor, the ACLU also led efforts in non-labor arenas, for example, promoting free speech in public schools. The ACLU itself was banned from speaking in New York public schools in 1921. The ACLU, working with the NAACP, also supported racial discrimination cases. The ACLU defended free speech regardless of the opinions being espoused. For example, the reactionary, anti-Catholic, anti-black Ku Klux Klan (KKK) was a frequent target of ACLU efforts, but the ACLU defended the KKK's right to hold meetings in 1923. There were some civil rights that the ACLU did not make an effort to defend in the 1920s, including censorship of the arts, government search and seizure issues, right to privacy, or wiretapping.
The Communist party of the United States was routinely harassed and oppressed by government officials, leading it to be the primary client of the ACLU. The Communists were very aggressive in their tactics, often engaging in illegal or unethical conduct, and this led to frequent conflicts between the Communists and ACLU. Communist leaders often attacked the ACLU, particularly when the ACLU defended the free speech rights of conservatives. This uneasy relationship between the two groups continued for decades.
Scopes trial.
When 1925 arrived â five years after the ACLU was formed â the organization had virtually no success to show for its efforts. That changed in 1925, when the ACLU persuaded John T. Scopes to defy Tennessee's anti-evolution law in a court test. Clarence Darrow, a member of the ACLU National Committee, headed Scopes' legal team. The prosecution, led by William Jennings Bryan, contended that the Bible should be interpreted literally in teaching creationism in school. The ACLU lost the case and Scopes was fined $100. The Tennessee Supreme Court later upheld the law but overturned the conviction on a technicality.
The Scopes trial was a phenomenal public relations success for the ACLU. The ACLU became well known across America, and the case led to the first endorsement of the ACLU by a major U.S. newspaper. The ACLU continued to fight for the separation of church and state in schoolrooms, decade after decade, including the 1982 case "McLean v. Arkansas" and the 2005 case "Kitzmiller v. Dover Area School District".
Baldwin himself was involved in an important free speech victory of the 1920s, after he was arrested for attempting to speak at a rally of striking mill workers in New Jersey. Although the decision was limited to the state of New Jersey, the appeals court's judgement in 1928 declared that constitutional guarantees of free speech must be given "liberal and comprehensive construction", and it marked a major turning point in the civil rights movement, signaling the shift of judicial opinion in favor of civil rights.
The most important ACLU case of the 1920s was "Gitlow v. New York", in which Benjamin Gitlow was arrested for violating a state law against inciting anarchy and violence, when he distributed literature promoting communism. Although the Supreme Court did not overturn Gitlow's conviction, it adopted the ACLU's stance (later termed the incorporation doctrine) that the First Amendment freedom of speech applied to state laws, as well as federal laws.
First victories.
Leaders of the ACLU were divided on the best tactics to use to promote civil liberties. Felix Frankfurter felt that legislation was the best long-term solution, because the Supreme Court could not (andin his opinionshould not) mandate liberal interpretations of the Bill of Rights. But Walter Pollack, Morris Ernst, and other leaders felt that Supreme Court decisions were the best path to guarantee civil liberties. A series of Supreme Court decisions in the 1920s foretold a changing national atmosphere; anti-radical emotions were diminishing, and there was a growing willingness to protect freedom of speech and assembly via court decisions.
Free speech.
Censorship was commonplace in the early 20th century. State laws and city ordinances routinely outlawed speech deemed to be obscene or offensive, and prohibited meetings or literature that promoted unions or labor organization. Starting in 1926, the ACLU began to expand its free speech activities to encompass censorship of art and literature. In that year, H. L. Mencken deliberately broke Boston law by distributing copies of his banned "American Mercury" magazine; the ACLU defended him and won an acquittal. The ACLU went on to win additional victories, including the landmark case "United States v. One Book Called Ulysses" in 1933, which reversed a ban by the Customs Department against the book "Ulysses" by James Joyce. The ACLU only achieved mixed results in the early years, and it was not until 1966 that the Supreme Court finally clarified the obscenity laws in the "Roth v. United States" and "Memoirs v. Massachusetts" cases.
The Comstock laws banned distribution of sex education information, based on the premise that it was obscene and led to promiscuous behavior Mary Ware Dennett was fined $300 in 1928, for distributing a pamphlet containing sex education material. The ACLU, led by Morris Ernst, appealed her conviction and won a reversal, in which judge Learned Hand ruled that the pamphlet's main purpose was to "promote understanding".
The success prompted the ACLU to broaden their freedom of speech efforts beyond labor and political speech, to encompass movies, press, radio and literature. The ACLU formed the National Committee on Freedom from Censorship in 1931 to coordinate this effort. By the early 1930s, censorship in the United States was diminishing.
Two major victories in the 1930s cemented the ACLUs campaign to promote free speech. In "Stromberg v. California", decided in 1931, the Supreme Court sided with the ACLU and affirmed the right of a communist party member to salute a communist flag. The result was the first time the Supreme Court used the Due Process Clause of the 14th amendment to subject states to the requirements of the First Amendment. In "Near v. Minnesota", also decided in 1931, the Supreme Court ruled that states may not exercise prior restraint and prevent a newspaper from publishing, simply because the newspaper had a reputation for being scandalous.
1930s.
The late 1930s saw the emergence of a new era of tolerance in the United States. National leaders hailed the Bill of Rights, particularly as it protected minorities, as the essence of democracy. The 1939 Supreme Court decision in "Hague v. Committee for Industrial Organization" affirmed the right of communists to promote their cause. Even conservative elements, such as the American Bar Association began to campaign for civil liberties, which were long considered to be the domain of left-leaning organizations. By 1940, the ACLU had achieved many of the goals it set in the 1920s, and many of its policies were the law of the land.
Expansion.
In 1929, after the Scopes and Dennett victories, Baldwin perceived that there was vast, untapped support for civil liberties in the United States. Baldwin proposed an expansion program for the ACLU, focusing on police brutality, Native American rights, African American rights, censorship in the arts, and international civil liberties. The board of directors approved Baldwin's expansion plan, except for the international efforts.
The ACLU played a major role in passing the 1932 Norris â La Guardia Act, a federal law which prohibited employers from preventing employees from joining unions, and stopped the practice of outlawing strikes, unions, and labor organizing activities with the use of injunctions. The ACLU also played a key role in initiating a nationwide effort to reduce misconduct (such as extracting false confessions) within police departments, by publishing the report "Lawlessness in Law Enforcement" in 1931, under the auspices of Herbert Hoover's Wickersham Commission. In 1934, the ACLU lobbied for the passage of the Indian Reorganization Act, which restored some autonomy to Native American tribes, and established penalties for kidnapping native American children.
Although the ACLU deferred to the NAACP for litigation promoting civil liberties for African Americans, the ACLU did engage in educational efforts, and published "Black Justice" in 1931, a report which documented institutional racism throughout the South, including lack of voting rights, segregation, and discrimination in the justice system. Funded by the Garland Fund, the ACLU also participated in producing the influential Margold Report, which outlined a strategy to fight for civil rights for blacks. The ACLU's plan was to demonstrate that the "separate but equal" policies governing the Southern discrimination were illegal because blacks were never, in fact, treated equally.
Depression era and the New Deal.
In 1932twelve years after the ACLU was foundedit had achieved significant success; the Supreme Court had embraced the free speech principles espoused by the ACLU, and the general public was becoming more supportive of civil rights in general. But the Great Depression brought new assaults on civil liberties; the year 1930 saw a large increase in the number of free speech prosecutions, a doubling of the number of lynchings, and all meetings of unemployed persons were banned in Philadelphia.
The Franklin D. Roosevelt administration proposed the New Deal to combat the depression. ACLU leaders were of mixed opinions about the New Deal, since many felt that it represented an increase in government intervention into personal affairs, and because the National Recovery Administration suspended anti-trust legislation. Roosevelt was not personally interested in civil rights, but did appoint many civil libertarians to key positions, including Interior Secretary Harold Ickes, a member of the ACLU.
The economic policies of the New Deal leaders were often aligned with ACLU goals, but social goals were not. In particular, movies were subject to a barrage of local ordinances banning screenings that were deemed immoral or obscene. Even public health films portraying pregnancy and birth were banned; as was "Life" magazine's April 11, 1938 issue which included photos of the birth process. The ACLU fought these bans, but did not prevail.
The Catholic Church attained increasing political influence in the 1930s, and used its influence to promote censorship of movies, and to discourage publication of birth control information. This conflict between the ACLU and the Catholic Church led to the resignation of the last Catholic priest from ACLU leadership in 1934; a Catholic priest would not be represented there again until the 1970s.
The ACLU took no official position on president Franklin Delano Roosevelt's 1937 court-packing plan, which threatened to increase the number of Supreme Court justices, unless the Supreme Court reversed its course and began approving New Deal legislation. The Supreme Court responded by making a major shift in policy, and no longer applied strict constitutional limits to government programs, and also began to take a more active role in protecting civil liberties.
The first decision that marked the court's new direction was "De Jonge v. Oregon", in which a communist labor organizer was arrested for calling a meeting to discuss unionization. The ACLU attorney Osmond Fraenkel, working with International Labor Defense, defended De Jonge in 1937, and won a major victory when the Supreme Court ruled that "peaceable assembly for lawful discussion cannot be made a crime." The De Jong case marked the start of an era lasting for a dozen years, during which Roosevelt appointees (led by Hugo Black, William O. Douglas, and Frank Murphy) established a body of civil liberties law. In 1938, Justice Harlan F. Stone wrote the famous "footnote four" in "United States v. Carolene Products Co." in which he suggested that state laws which impede civil liberties wouldhenceforthrequire compelling justification.
Senator Robert F. Wagner proposed the National Labor Relations Act in 1935, which empowered workers to unionize. Ironically, the ACLU, after 15 years of fighting for workers rights, initially opposed the act (it later took no stand on the legislation) because some ACLU leaders feared the increased power the bill gave to the government. The newly formed National Labor Relations Board (NLRB) posed a dilemma for the ACLU, because in 1937 it issued an order to Henry Ford, prohibiting Ford from disseminating anti-union literature. Part of the ACLU leadership habitually took the side of labor, and that faction supported the NLRB's action. But part of the ACLU supported Ford's right to free speech. ACLU leader Arthur Garfield Hays proposed a compromise (supporting the auto workers union, yet also endorsing Ford's right to express personal opinions), but the schism highlighted a deeper divide that would become more prominent in the years to come.
The ACLU's support of the NRLB was a major development for the ACLU, because it marked the first time it accepted that a government agency could be responsible for upholding civil liberties. Until 1937, the ACLU felt that civil rights were best upheld by citizens and private organizations.
Some factions in the ACLU proposed new directions for the organization. In the late 1930s, some local affiliates proposed shifting their emphasis from civil liberties appellate actions, to becoming a legal aid society, centered on store front offices in low income neighborhoods. The ACLU directors rejected that proposal. Other ACLU members wanted the ACLU to shift focus into the political arena, and to be more willing to compromise their ideals in order to strike deals with politicians. This initiative was also rejected by the ACLU leadership.
Jehovah's Witnesses.
The ACLU's support of defendants with unpopular, sometimes extreme, viewpoints have produced many landmark court cases and established new civil liberties. One such defendant was the Jehovah's Witnesses, who were involved in a large number of Supreme Court cases. Cases that the ACLU supported included "Lovell v. Griffin" (which struck down a city ordinance that required a permit before a person could distribute "literature of any kind"); "Martin v. Struthers" (which struck down an ordinance prohibiting door-to-door canvassing); and "Cantwell v. Connecticut" (which reversed the conviction of a Witness who was reciting offensive speech on a street corner).
The most important cases involved statutes requiring flag salutes. The Jehovah's Witnesses felt that saluting a flag was contrary to their religious beliefs. Two children were convicted in 1938 of not saluting the flag. The ACLU supported their appeal to the Supreme Court, but the court affirmed the conviction, in 1940. But three years later, in "West Virginia State Board of Education v. Barnette", the Supreme court reversed itself and wrote "If there is any fixed star in our constitutional constellation, it is that no official, high or petty, can prescribe what shall be orthodox in politics, nationalism, religion, or other matters of opinion or force citizens to confess by word or act their faith therein." To underscore its decision, the Supreme Court announced it on Flag Day.
Communism and totalitarianism.
The rise of totalitarianism in Germany, Russia, and Italy during World War II had a tremendous impact on the civil liberties movement. On the one hand, the oppression of the totalitarian states put into sharp relief the virtue of freedom of speech and association in the United States; on the other hand, they prompted an anti-communist hysteria in America which eroded many civil liberties.
The ACLU leadership was divided over whether or not to defend pro-Nazi speech in the United States; pro-labor elements within the ACLU were hostile towards Nazism and fascism, and objected when the ACLU defended Nazis. Several states passed laws outlawing the hate speech directed at ethnic groups. The first person arrested under New Jersey's 1935 hate speech law was a Jehovah's Witness who was charged with disseminating anti-Catholic literature. The ACLU defended the Jehovah's Witnesses, and the charges were dropped. The ACLU proceeded to defend numerous pro-Nazi groups, defending their rights to free speech and free association.
In the late 1930s, the ACLU allied itself with the Popular Front, a coalition of liberal organizations coordinated by the United States Communist Party. The ACLU benefited because affiliates from the Popular Front could often fight local civil rights battles much more effectively than the New York-based ACLU. The association with the Communist Party led to accusations that the ACLU was a "communist front", particularly because Harry F. Ward was both chairman of the ACLU and chairman of the American League Against War and Fascism, a communist organization.
The House Unamerican Activities Committee (HUAC) was created in 1938 to uncover sedition and treason within the United States. When witnesses testified at its hearings, the ACLU was mentioned several times, leading the HUAC to mention the ACLU prominently in its 1939 report. This damaged the ACLU's reputation severely, even though the report said that it could not "definitely state whether or not" the ACLU was a communist organization.
While the ACLU rushed to defend its image against allegations of being a communist front, it also worked to protect witnesses who were being harassed by the HUAC. The ACLU was one of the few organizations to protest (unsuccessfully) against passage of the Smith Act in 1940, which would later be used to imprison many persons who supported Communism. The ACLU defended many persons who were prosecuted under the Smith Act, including labor leader Harry Bridges.
ACLU leadership was split on whether to purge its leadership of communists. Norman Thomas, John Haynes Holmes, and Morris Ernst were anti-communists who wanted to distance the ACLU from communism; opposing them were Harry Ward, Corliss Lamont and Elizabeth Flynn who rejected any political test for ACLU leadership. A bitter struggle ensued throughout 1939, and the anti-communists prevailed in February 1940, when the board voted to prohibit anyone who supported totalitarianism from ACLU leadership roles. Chairman Harry Ward immediately resigned, andfollowing a contentious six-hour debatelegendary activist Elizabeth Flynn was voted off the ACLU's board. The 1940 resolution was a disaster for the ACLU, and considered by many to be a betrayal of its fundamental principles. The resolution was rescinded in 1968, and Flynn was posthumously reinstated to the ACLU in 1970.
Mid-century.
World War II.
When World War II engulfed the United States, the Bill of Rights was enshrined as a hallowed document, and numerous organizations defended civil liberties. Chicago and New York proclaimed "Civil Rights" weeks, and President Franklin Delano Roosevelt announced a national Bill of Rights day. Eleanor Roosevelt was the keynote speaker at the 1939 ACLU convention. In spite of this newfound respect for civil rights, Americans were becoming adamantly anti-communist, and believed that excluding communists from American society was an essential step to preserve democracy.
Contrasted with World War I, there was relatively little violation of civil liberties during World War II. President Roosevelt was a strong supporter of civil liberties, butmore importantlythere were few anti-war activists during World War II. The most significant exception was the internment of Japanese Americans. Two months after the Japanese attack on Pearl Harbor, Roosevelt authorized the creation of military "exclusion zones" with Executive Order 9066, paving the way for the detention of all West Coast Japanese Americans in inland camps. In addition to the non-citizen Issei (prohibited from naturalization as members of an "unassimilable" race), over two-thirds of those swept up were American-born citizens. The ACLU immediately protested to Roosevelt, comparing the evacuations to Nazi concentration camps. The ACLU was the only major organization to object to the internment plan, and their position was very unpopular, even within the organization. Not all ACLU leaders wanted to defend the Japanese Americans; Roosevelt loyalists such as Morris Ernst wanted to support Roosevelt's war effort, but pacifists such as Baldwin and Norman Thomas felt that Japanese Americans needed access to due process before they could be imprisoned. In a March 20, 1942 letter to Roosevelt, Baldwin called on the administration to allow Japanese Americans to prove their loyalty at individual hearings, describing the constitutionality of the planned removal "open to grave question." His suggestions went nowhere, and opinions within the organization became increasingly divided as the Army began the "evacuation" of the West Coast. In May, the two factions, one pushing to fight the exclusion orders then being issued, the other advocating support for the President's policy of removing citizens whose "presence may endanger national security," brought their opposing resolutions to a vote before the board and the ACLU's national leaders. They decided not to challenge the eviction of Japanese American citizens, and on June 22 instructions were sent to West Coast branches not to support cases that argued the government had no constitutional right to do so.
The ACLU offices on the West Coast had been more directly involved in addressing the tide of anti-Japanese prejudice from the start, as they were geographically closer to the issue, and were already working on cases challenging the exclusion by this time. The Seattle office, assisting in Gordon Hirabayashi's lawsuit, created an unaffiliated committee to continue the work the ACLU had started, while in Los Angeles, attorney A.L. Wirin continued to represent Ernest Kinzo Wakayama but without addressing the case's constitutional questions. (Wirin would lose private clients because of his defense of Wakayama and other Japanese Americans.) However, the San Francisco branch, led by Ernest Besig, refused to discontinue its support for Fred Korematsu, whose case had been taken on prior to the June 22 directive, and attorney Wayne Collins, with Besig's full support, centered his defense on the illegality of Korematsu's exclusion.
The West Coast offices had wanted a test case to take to court, but had a difficult time finding a Japanese American who was both willing to violate the internment orders and able to meet the ACLU's desired criteria of a sympathetic, Americanized plaintiff. Of the 120,000 Japanese Americans affected by the order, only 12 disobeyed, and Korematsu, Hirabayashi, and two others were the only resisters whose cases eventually made it to the Supreme Court. "Hirabayashi v. United States" came before the Court in May 1943, and the justices upheld the government's right to exclude Japanese Americans from the West Coast; although it had earlier forced its local office in L.A. to stop aiding Hirabayashi, the ACLU donated $1,000 to the case (over a third of the legal team's total budget) and submitted an "amicus" brief. Besig, dissatisfied with Osmond Fraenkel's tamer defense, filed an additional "amicus" brief that directly addressed Hirabayashi's constitutional rights. In the meantime, A.L. Wirin served as one of the attorneys in "Yasui v. United States" (decided the same day as the Hirabayashi case, and with the same results), but he kept his arguments within the perimeters established by the national office. The only case to receive a favorable ruling, "ex parte Endo", was also aided by two "amicus" briefs from the ACLU, one from the more conservative Fraenkel and another from the more putative Wayne Collins.
"Korematsu v. United States" proved to be the most controversial of these cases, as Besig and Collins refused to bow to national pressure to pursue the case without challenging the government's right to remove citizens from their homes. The ACLU board threatened to revoke the San Francisco branch's national affiliation, while Baldwin tried unsuccessfully to convince Collins to step down so he could replace him as lead attorney in the case. Eventually Collins agreed to present the case alongside Charles Horsky, although their arguments before the Supreme Court remained based in the unconstitutionality of the exclusion order Korematsu had disobeyed. The case was decided in December 1944, when the Court once again upheld the government's right to relocate Japanese Americans, although Korematsu's, Hirabayashi's and Yasui's convictions were later overturned in "coram nobis" proceedings in the 1980s.
Although the ACLU (somewhat unevenly) defended the Japanese Americans, it was more reluctant to defend anti-war protesters. A majority of the board passed a resolution in 1942 which declared the ACLU unwilling to defend anyone who interfered with the United States' war effort. Included in this group were the thousands of Nisei who renounced their U.S. citizenship during the war but later regretted the decision and tried to revoke their applications for "repatriation." (A significant number of those slated to "go back" to Japan had never actually been to the country and were in fact being deported rather than repatriated.) Ernest Besig had in 1944 visited the Tule Lake Segregation Center, where the majority of these "renunciants" were concentrated, and subsequently enlisted Wayne Collins' help to file a lawsuit on their behalf, arguing the renunciations had been given under duress. The national organization prohibited local branches from representing the renunciants, forcing Collins to pursue the case on his own, although Besig and the Northern California office provided some support.
When the war ended in 1945, the ACLU was 25 years old, and had accumulated an impressive set of legal victories. President Harry S. Truman sent a congratulatory telegram to the ACLU on the occasion of their 25th anniversary. American attitudes had changed since World War I, and dissent by minorities was tolerated with more willingness. The Bill of Rights was more respected, and minority rights were becoming more commonly championed. During their 1945 annual conference, the ACLU leaders composed a list of important civil rights issues to focus on in the future, and the list included racial discrimination and separation of church and state.
The ACLU supported the African-American defendants in "Shelley v. Kraemer", when they tried to occupy a house they had purchased in a neighborhood which had racially restrictive housing covenants. The African-American purchasers won the case in 1945.
Cold War era.
Anti-communist sentiment gripped the United States during the Cold War beginning in 1946. Federal investigations caused many persons with communist or left-leaning affiliations to lose their jobs, become blacklisted, or be jailed. During the Cold War, although the United States collectively ignored the civil rights of communists, other civil libertiesâsuch as due process in law and separation of church and stateâcontinued to be reinforced and even expanded.
The ACLU was internally divided when it purged communists from its leadership in 1940, and that ambivalence continued as it decided whether to defend alleged communists during the late 1940s. Some ACLU leaders were anti-communist, and felt that the ACLU should not defend any victims. Some ACLU leaders felt that communists were entitled to free speech protections, and the ACLU should defend them. Other ACLU leaders were uncertain about the threat posed by communists, and tried to establish a compromise between the two extremes. This ambivalent state of affairs would last until 1954, when the civil liberties faction prevailed, leading to the resignation of most of the anti-communist leaders.
In 1947, President Truman issued Executive Order 9835, which created the Federal Loyalty Program. This program authorized the Attorney General to create a list of organizations which were deemed to be subversive. Any association with these programs was ground for barring the person from employment. Listed organizations were not notified that they were being considered for the list, nor did they have an opportunity to present counterarguments; nor did the government divulge any factual basis for inclusion in the list. Although ACLU leadership was divided on whether to challenge the Federal Loyalty Program, some challenges were successfully made.
Also in 1947, the House Un-American Activities Committee (HUAC) subpoenaed ten Hollywood directors and writers, the "Hollywood Ten", intending to ask them to identify Communists, but the witnesses refused to testify. All were imprisoned for contempt of Congress. The ACLU supported the appeals of several of the artists, but lost on appeal. The Hollywood establishment panicked after the HUAC hearings, and created a blacklist which prohibited anyone with leftist associations from working. The ACLU supported legal challenges to the blacklist, but those challenges failed. The ACLU was more successful with an education effort; the 1952 report "The Judges and the Judged", prepared at the ACLU's direction in response to the blacklisting of actress Jean Muir, described the unfair and unethical actions behind the blacklisting process, and it helped gradually turn public opinion against McCarthyism.
The federal government took direct aim at the U.S. communist party in 1948 when it indicted its top twelve leaders in the Foley Square trial. The case hinged on whether or not mere membership in a totalitarian political party was sufficient to conclude that members advocated the overthrow of the United States government. The ACLU chose to not represent any of the defendants, and they were all found guilty and sentenced to three to five years in prison. Their defense attorneys were all cited for contempt, went to prison and were disbarred. When the government indicted additional party members, the defendants could not find attorneys to represent them. Communists protested outside the courthouse; a bill to outlaw picketing of courthouses was introduced in Congress, and the ACLU supported the anti-picketing law.
The ACLU, in a change of heart, supported the party leaders during their appeal process. The Supreme Court upheld the convictions in the "Dennis v. United States" decision by softening the free speech requirements from a "clear and present danger" test, to a "grave and probable" test. The ACLU issued a public condemnation of the "Dennis" decision, and resolved to fight it. One reason for the Supreme Court's support of cold war legislation was the 1949 deaths of Supreme Court justices Frank Murphy and Wiley Rutledge, leaving Hugo Black and William O. Douglas as the only remaining civil libertarians on the Court.
The "Dennis" decision paved the way for the prosecution of hundreds of other communist party members. The ACLU supported many of the communists during their appeals (although most of the initiative originated with local ACLU affiliates, not the national headquarters) but most convictions were upheld. The two California affiliates, in particular, felt the national ACLU headquarters was not supporting civil liberties strongly enough, and they initiated more cold war cases than the national headquarters did.
The ACLU also challenged many loyalty oath requirements across the country, but the courts upheld most of the loyalty oath laws. California ACLU affiliates successfully challenged the California state loyalty oath. The Supreme Court, until 1957, upheld nearly every law which restricted the liberties of communists.
The ACLU, even though it scaled back its defense of communists during the Cold War, still came under heavy criticism as a "front" for communism. Critics included the American Legion, Senator Joseph McCarthy, the HUAC, and the FBI. Several ACLU leaders were sympathetic to the FBI, and as a consequence, the ACLU rarely investigated any of the many complaints alleging abuse of power by the FBI during the Cold War.
Organizational change.
In 1950, the ACLU board of directors asked executive director Baldwin to resign, feeling that he lacked the organizational skills to lead the 9,000 (and growing) member organization. Baldwin objected, but a majority of the board elected to remove him from the position, and he was replaced by Patrick Murphy Malin. Under Malinâs guidance, membership tripled to 30,000 by 1955the start of a 24-year period of continual growth leading to 275,000 members in 1974. Malin also presided over an expansion of local ACLU affiliates.
The ACLU, which had been controlled by an elite of a few dozen New Yorkers, became more democratic in the 1950s. In 1951, the ACLU amended its bylaws to permit the local affiliates to participate directly in voting on ACLU policy decisions. A bi-annual conference, open to the entire membership, was instituted in the same year, and in later decades it became a pulpit for activist members, who suggested new directions for the ACLU, including abortion rights, death penalty, and rights of the poor.
McCarthyism era.
During the early 1950s, the ACLU continued to steer a moderate course through the Cold War. When leftist singer Paul Robeson was denied a passport in 1950, even though he was not a communist and not accused of any illegal acts, the ACLU chose to not defend him. The ACLU later reversed their stance, and supported William Worthy and Rockwell Kent in their passport confiscation cases, which resulted in legal victories in the late 1950s.
In response to communist witch-hunts, many witnesses and employees chose to use the fifth amendment protection against self-incrimination to avoid divulging information about their political beliefs. Government agencies and private organizations, in response, established polices which inferred communist party membership for anyone who invoked the fifth amendment. The national ACLU was divided on whether to defend employees who had been fired merely for pleading the fifth amendment, but the New York affiliate successfully assisted teacher Harry Slochower in his Supreme Court case which reversed his termination.
The fifth amendment issue became the catalyst for a watershed event in 1954, which finally resolved the ACLUâs ambivalence by ousting the anti-communists from ACLU leadership. In 1953, the anti-communists, led by Norman Thomas and James Fly, proposed a set of resolutions that inferred guilt of persons that invoked the fifth amendment. These resolutions were the first that fell under the ACLUâs new organizational rules permitting local affiliates to participate in the vote; the affiliates outvoted the national headquarters, and rejected the anti-communist resolutions. Anti-communists leaders refused to accept the results of the vote, and brought the issue up for discussion again at the 1954 bi-annual convention. ACLU member Frank Graham, president of the University of North Carolina, attacked the anti-communists with a counter-proposal, which stated that the ACLU âstand[s] against guilt by association, judgment by accusation, the invasion of privacy of personal opinions and beliefs, and the confusion of dissent with disloyalty.â The anti-communists continued to battle Grahamâs proposal, but were outnumbered by the affiliates. The anti-communists finally gave up and departed the board of directors in late 1954 and 1955, ending an eight-year reign of ambivalence within the ACLU leadership ranks. Thereafter, the ACLU proceeded with firmer resolve against Cold War anti-communist legislation. The period from the 1940 resolution (and the purge of Elizabeth Flynn) to the 1954 resignation of the anti-communist leaders is considered by many to be an era in which the ACLU abandoned its core principles.
McCarthyism declined in late 1954 after television journalist Edward R. Murrow and others publicly chastised McCarthy. The controversies over the Bill of Rights that were generated by the Cold War ushered in a new era in American Civil liberties. In 1954 in ââBrown v. Board of Educationââ, the Supreme Court unanimously overturned state-sanctioned school segregation, and thereafter a flood of civil rights victories dominated the legal landscape.
The Supreme Court handed the ACLU two key victories in 1957, in "Watkins v. United States" and "Yates v. United States", both of which undermined the Smith Act and marked the beginning of the end of communist party membership inquiries. In 1965, the Supreme Court produced some decisions, including "Lamont v. Postmaster General" (in which the plaintiff was Corliss Lamont, a former ACLU board member), which upheld fifth amendment protections and brought an end to restrictions on political activity.
1960s.
The decade from 1954 to 1964 was the most successful period in the ACLUâs history. Membership rose from 30,000 to 80,000, and by 1965 it had affiliates in seventeen states. During the ACLUâs bi-annual conference in Colorado in 1964, the Supreme Court issued rulings on eight cases in which the ACLU was involved; the ACLU prevailed on seven of the eight. The ACLU played a role in Supreme Court decisions reducing censorship of literature and arts, protecting freedom of association, prohibiting racial segregation, excluding religion from public schools, and providing due process protection to criminal suspects. The ACLU's success arose from changing public attitudes; the American populace was more educated, more tolerant, and more willing to accept unorthodox behavior.
Separation of church and state.
Legal battles concerning the separation of church and state originated in laws dating to 1938 which required religious instruction in school, or provided state funding for religious schools. The Catholic church was a leading proponent of such laws; and the primary opponents (the "separationists") were the ACLU, Americans United for Separation of Church and State, and the American Jewish Congress. The ACLU led the challenge in the 1947 "Everson v. Board of Education" case, in which Justice Hugo Black wrote "[t]he First Amendment has erected a wall between church and stateâ¦. That wall must be kept high and impregnable." It was not clear that the Bill of Rights forbid state governments from supporting religious education, and strong legal arguments were made by religious proponents, arguing that the Supreme Court should not act as a "national school board", and that the Constitution did not govern social issues. However, the ACLU and other advocates of church/state separation persuaded the Court to declare such activities unconstitutional. Historian Samuel Walker writes that the ACLUâs âgreatest impact on American lifeâ was its role in persuading the Supreme Court to âconstitutionalize" so many public controversies.
In 1948, the ACLU prevailed in the "McCollum v. Board of Education" case, which challenged public school religious classes taught by clergy paid for from private funds. The ACLU also won cases challenging schools in New Mexico which were taught by clergy and had crucifixes hanging in the classrooms. In the 1960s, the ACLU, in response to member insistence, turned its attention to in-class promotion of religion. In 1960, 42 percent of American schools included Bible reading. In 1962, the ACLU published a policy statement condemning in-school prayers, observation of religious holidays, and Bible reading. The Supreme Court concurred with the ACLUâs position, when it prohibited New Yorkâs in-school prayers in the 1962 "Engel v. Vitale" decision. Religious factions across the country rebelled against the anti-prayer decisions, leading them to propose the School Prayer Constitutional Amendment, which declared in-school prayer legal. The ACLU participated in a lobbying effort against the amendment, and the 1966 congressional vote on the amendment failed to obtain the required two-thirds majority.
However, not all cases were victories; ACLU lost cases in 1949 and 1961 which challenged state laws requiring commercial businesses to close on Sunday, the Christian Sabbath. The Supreme court has never overturned such laws, although some states subsequently revoked many of the laws under pressure from commercial interests.
Freedom of expression.
During the 1940s and 1950s, the ACLU continued its battle against censorship of art and literature. In 1948, the New York affiliate of the ACLU received mixed results from the Supreme Court, winning the appeal of Carl Jacob Kunz, who was convicted for speaking without a police permit, but losing the appeal of Irving Feiner who was arrested to prevent a breach of the peace, based on his oration denouncing president Truman and the American Legion. The ACLU lost the case of Joseph Beahharnais, who was arrested for group libel when he distributed literature impugning the character of African Americans.
Cities across America routinely banned movies because they were deemed to be "harmful", "offensive", or "immoral"censorship which was validated by the 1915 "Mutual v. Ohio" Supreme Court decision which held movies to be mere commerce, undeserving of first amendment protection. The film "The Miracle" was banned in New York in 1951, at the behest of the Catholic Church, but the ACLU supported the filmâs distributor in an appeal of the ban, and won a major victory in the 1952 decision "Joseph Burstyn, Inc v. Wilson". The Catholic Church led efforts throughout the 1950s attempting to persuade local prosecutors to ban various books and movies, leading to conflict with the ACLU when the ACLU published it statement condemning the churchâs tactics. Further legal actions by the ACLU successfully defended films such as "M" and "la Ronde", leading the eventual dismantling of movie censorship. Hollywood continued employing self-censorship with its own Production Code, but in 1956 the ACLU called on Hollywood to abolish the Code.
The ACLU defended beat generation artists, including Allen Ginsburg who was prosecuted for his poem "Howl"; andin an unorthodox case the ACLU helped a coffee house regain its restaurant license which was revoked because its Beat customers were allegedly disturbing the peace and quiet of the neighborhood.
The ACLU lost an important press censorship case when, in 1957, the Supreme Court upheld the obscenity conviction of publisher Samuel Roth for distributing adult magazines. As late as 1953, books such as "Tropic of Cancer" and "From Here to Eternity" were still banned. But public standards rapidly became more liberal though the 1960s, and obscenity was notoriously difficult to define, so by 1971 prosecutions for obscenity had halted.
Racial discrimination.
A major aspect of civil liberties progress after World War II was the undoing centuries of racism in federal, state, and local governments an effort generally known as the Civil Rights Movement. Several civil liberties organizations worked together for progress, including the National Association for the Advancement of Colored People (NAACP), the ACLU, and the American Jewish Congress. The NAACP took primary responsibility for Supreme Court cases (often led by lead NAACP attorney Thurgood Marshall), with the ACLU focusing on police misconduct, and supporting the NAACP with amicus briefs. The NAACP achieved a key victory in 1950 with the "Henderson v. United States" decision that ended segregation in interstate bus and rail transportation.
In 1954, the ACLU filed an amicus brief in the case of "Brown v. Board of Education", which led to the ban on racial segregation in U.S. public schools. Southern states instituted a McCarthyism-style witch-hunt against the NAACP, attempting it to disclose membership lists. The ACLU's fight against racism was not limited to segregation; in 1964 the ACLU provided key support to plaintiffs, primarily lower income urban residents, in "Reynolds v. Sims", which required states to establish the voting districts in accordance with the "one person, one vote" principle.
Police misconduct.
The ACLU regularly tackled police misconduct issues, starting with the 1932 case "Powell v. Alabama" (right to an attorney), and including 1942's "Betts v. Brady" (right to an attorney), and 1951's "Rochin v. California" (involuntary stomach pumping). In the late 1940s, several ACLU local affiliates established permanent committees to address policing issues. During the 1950s and 1960s, the ACLU was responsible for substantially advancing the legal protections against police misconduct. The Philadelphia affiliate was responsible for causing the City of Philadelphia, in 1958, to create the nation's first civilian police review board. In 1959, the Illinois affiliate published the first report in the nation, "Secret Detention by the Chicago Police", which documented unlawful detention by police.
Some of the most well known ACLU successes came in the 1960s, when the ACLU prevailed in a string of cases limiting the power of police to gather evidence; in 1961's "Mapp v. Ohio", the Supreme court required states to obtain a warrant before searching a person's home. The "Gideon v. Wainwright" decision in 1963 provided legal representation to indigents. In 1964, the ACLU persuaded the Court, in "Escobedo v. Illinois", to permit suspects to have an attorney present during questioning. And, in 1966, the "Miranda v. Arizona" decision required police to notify suspects of their constitutional rights. Although many law enforcement officials criticized the ACLU for expanding the rights of suspects, police officers themselves took advantage of the ACLU. For example when the ACLU represented New York policemen in their lawsuit which objected to searches of their workplace lockers. In the late 1960s, civilian review boards in New York and Philadelphia were abolished, over the ACLUâs objection.
Civil liberties revolution of the 1960s.
The 1960s was a tumultuous era in the United States, and public interest in civil liberties underwent an explosive growth. Civil liberties actions in the 1960s were often led by young people, and often employed tactics such as sit ins and marches. Protests were often peaceful, but sometimes employed militant tactics. The ACLU played a central role in all major civil liberties debates of the 1960s, including new fields such as gay rights, prisoner's rights, abortion, rights of the poor, and the death penalty. Membership in the ACLU increased from 52,000 at the beginning of the decade, to 104,000 in 1970. In 1960, there were affiliates in seven states, and by 1974 there were affiliates in 46 states. During the 1960s, the ACLU underwent a major transformation tactics; it shifted emphasis from legal appeals (generally involving amicus briefs submitted to the Supreme Court) to direct representation of defendants when they were initially arrested. At the same time, the ACLU transformed its style from "disengaged and elitist" to "emotionally engaged". The ACLU published a breakthrough document in 1963, titled "How Americans Protest", which was borne of frustration with the slow progress in battling racism, and which endorsed aggressive, even militant protest techniques.
African-American protests in the South accelerated in the early 1960s, and the ACLU assisted at every step. After four African-American college students staged a sit-in in a segregated North Carolina department store, the sit-in movement gained momentum across the United States. During 1960-61, the ACLU defended black students arrested for demonstrating in North Carolina, Florida, and Louisiana. The ACLU also provided legal help for the Freedom Rides in 1961, the integration of the University of Mississippi, the 1963 protests in Birmingham, Alabama, and the 1964 Freedom Summer.
The NAACP was responsible for managing most sit-in related cases that made it to the Supreme Court, winning nearly every decision. But it fell to the ACLU and other legal volunteer efforts to provide legal representation to hundreds of protestorswhite and blackwho were arrested while protesting in the South. The ACLU joined with other civil liberties groups to form the Lawyers Constitutional Defense Committee (LCDC) which subsequently provided legal representation to many of the protesters. The ACLU provided the majority of the funding for the LCDC.
In 1964, the ACLU opened up a major office in Atlanta, Georgia, dedicated to serving Southern issues. Much of the ACLU's progress in the South was due to Charles Morgan, Jr., the charismatic leader of the Atlanta office. He was responsible for desegregating juries (Whitus v. Georgia), desegregating prisons ("Lee v. Washington"), and reforming election laws. The ACLU's southern office also defended African-American congressman Julian Bond in "Bond v. Floyd", when the Georgia congress refused to formally induct Bond into the legislature. Another widely publicized case defended by Morgan was that of Army doctor Howard Levy, who was convicted of refusing to train Green Berets. Despite raising the defense that the Green Berets were committing war crimes in Vietnam, Levy lost on appeal in "Parker v. Levy", 417 U.S. 733 (1974).
In 1969, the ACLU won a major victory for free speech, when it defended Dick Gregory after he was arrested for peacefully protesting against the mayor of Chicago. The court ruled in "Gregory v. Chicago" that a speaker cannot be arrested for disturbing the peace when the hostility is initiated by someone in the audience, as that would amount to a "heckler's veto".
Vietnam war.
The ACLU was at the center of several legal aspects of the Vietnam war: defending draft resisters, challenging the constitutionality of the war, the potential impeachment of Richard Nixon, and the use of national security concerns to preemptively censor newspapers.
David J. Miller was the first person prosecuted for burning his draft card. The New York affiliate of the ACLU appealed his 1965 conviction (367 F.2d 72: "United States of America v. David J. Miller", 1966), but the Supreme Court refused to hear the appeal. Two years later, the Massachusetts affiliate took the card-burning case of David OâBrien to the Supreme court, arguing that the act of burning was a form of symbolic speech, but the Supreme Court upheld the conviction in "United States v. O'Brien", 391 US 367 (1968). Thirteen year old Junior High student Mary Tinker wore a black armband to school in 1965 to object to the war, and was suspended from school. The ACLU appealed her case to the supreme court and won a victory in "Tinker v. Des Moines". This critical case established that the government may not establish "enclaves" such as schools or prisons where all rights are forfeit.
The ACLU defended Sydney Street, who was arrested for burning an American flag to protest the reported assassination of civil rights leader James Meredith. In the "Street v. New York" decision, the court agreed with the ACLU that encouraging the country to abandon one of its national symbols was constitutionally protected form of expression. The ACLU successfully defended Paul Cohen, who was arrested for wearing a jacket with the words "fuck the draft" on its back, while he walked through the Los Angeles courthouse. The Supreme Court, in "Cohen v. California", held that the vulgarity of the wording was essential to convey the intensity of the message.
Non-war related free speech rights were also advanced during the Vietnam war era; in 1969, the ACLU defended a Ku Klux Klan member who advocated long-term violence against the government, and the Supreme Court concurred with the ACLU's argument in the landmark decision "Brandenburg v. Ohio", which held that only speech which advocated "imminent" violence could be outlawed.
A major crisis gripped the ACLU in 1968 when a debate erupted over whether to defend Benjamin Spock and the Boston Five against federal charges that they encouraged draftees to avoid the draft. The ACLU board was deeply split over whether to defend the activists; half the board harbored anti-war sentiments, and felt that the ACLU should lend its resources to the cause of the Boston Five. The other half of the board believed that civil liberties were not at stake, and the ACLU would be taking a political stance. Behind the debate was the longstanding ACLU tradition that it was politically impartial, and provided legal advice without regard to the political views of the defendants. The board finally agreed to a compromise solution that permitted the ACLU to defend the anti-war activists, without endorsing the activist's political views. Some critics of the ACLU suggest that the ACLU became a partisan political organization following the Spock case. After the Kent State shootings in 1970, ACLU leaders took another step towards politics by passing a resolution condemning the Vietnam war. The resolution was based in a variety of legal arguments, including civil liberties violations and a claim that the war was illegal.
Also in 1968, the ACLU held an internal symposium to discuss its dual roles: providing "direct" legal support (defense for accused in their initial trial, benefiting only the individual defendant), and appellate support (providing amicus briefs during the appeal process, to establish widespread legal precedent). Historically, the ACLU was known for its appellate work which led to landmark Supreme Court decisions, but by 1968, 90% of the ACLU's legal activities involved direct representation. The symposium concluded that both roles were valid for the ACLU.
1970s and 1980s.
Watergate era.
The ACLU supported "The New York Times" in its 1971 suit against the government, requesting permission to publish the Pentagon papers. The court upheld the "Times" and ACLU in the "New York Times Co. v. United States" ruling, which held that the government could not preemptively prohibit the publication of classified information and had to wait until after it was published to take action.
As the Watergate saga unfolded, the ACLU became the first national organization to call for Nixon's impeachment. This, following the resolution opposing the Vietnam war, was a second major decision that caused critics of the ACLU, particularly conservatives, to claim that the ACLU had evolved into a liberal political organization.
Enclaves and new civil liberties.
The decade from 1965 to 1975 saw an expansion of the field of civil liberties. Administratively, the ACLU responded by appointing Aryeh Neier to take over from Pemberton as Executive Director in 1970. Neier embarked on an ambitious program to expand the ACLU; he created the ACLU Foundation to raise funds, and he created several new programs to focus the ACLU's legal efforts. By 1974, ACLU membership had reached 275,000.
During those years, the ACLU led the way in expanding legal rights in three directions: new rights for persons within government-run "enclaves", new rights for victim groups, and privacy rights for mainstream citizens. At the same time, the organization grew substantially. The ACLU helped develop the field of constitutional law that governs "enclaves", which are groups of persons that live in conditions under government control. Enclaves include mental hospital patients, members of the military, and prisoners, and students (while at school). The term enclave originated with Supreme Court justice Abe Fortas's use of the phrase "schools may not be enclaves of totalitarianism" in the "Tinker v. Des Moines" decision.
The ACLU initiated the legal field of student's rights with the "Tinker v. Des Moines" case, and expanded it with cases such as "Goss v. Lopez" which required schools to provide students an opportunity to appeal suspensions.
As early as 1945, the ACLU had taken a stand to protect the rights of the mentally ill, when it drafted a model statute governing mental commitments. In the 1960s, the ACLU opposed involuntary commitments, unless it could be demonstrated that the person was a danger to himself or the community. In the landmark 1975 "O'Connor v. Donaldson" decision the ACLU represented a non-violent mental health patient who had been confined against his will for 15 years, and persuaded the Supreme Court to rule such involuntary confinements illegal. The ACLU has also defended the rights of mentally ill individuals who are not dangerous, but who create disturbances. The New York chapter of the ACLU defended Billie Boggs, a mentally ill woman who exposed herself and defecated and urinated in public.
Prior to 1960, prisoners had virtually no recourse to the court system, because courts considered prisoners to have no civil rights. That changed in the late 1950s, when the ACLU began representing prisoners that were subject to police brutality, or deprived of religious reading material. In 1968, the ACLU successfully sued to desegregate the Alabama prison system; and in 1969, the New York affiliate adopted a project to represent prisoners in New York prisons. Private attorney Phil Hirschkop discovered degrading conditions in Virginia prisons following the Virginia State Penitentiary strike, and won an important victory in 1971's "Landman v. Royster" which prohibited Virginia from treating prisoners in inhumane ways. In 1972, the ACLU consolidated several prison rights efforts across the nation and created the National Prison Project. The ACLU's efforts led to landmark cases such as "Ruiz v. Estelle" (requiring reform of the Texas prison system) and in 1996 U.S. Congress enacted the Prison Litigation Reform Act (PLRA) which codified prisoners' rights.
Victim groups.
The ACLU, during the 1960s and 1970s, expanded its scope to include what it referred to as "victim groups", namely women, the poor, and homosexuals. Heeding the call of female members, the ACLU endorsed the Equal Rights Amendment in 1970 and created the Women's Rights Project in 1971. The Women's Rights Project dominated the legal field, handling more than twice as many cases as the National Organization for Women, including breakthrough cases such as "Reed v. Reed", "Frontiero v. Richardson", and " Taylor v. Louisiana".
ACLU leader Harriet Pilpel raised the issue of the rights of homosexuals in 1964, and two years later the ACLU formally endorsed gay rights. In 1973 the ACLU created the Sexual Privacy Project (later the Gay and Lesbian Rights Project) which combated discrimination against homosexuals. This support continues even today. After then-Senator Larry Craig was arrested for soliciting sex in a public bathroom, the ACLU wrote an amicus brief for Craig, saying that sex between consenting adults in public places was protected under privacy rights.
Rights of the poor was another area that was expanded by the ACLU. In 1966 and again in 1968, activists within the ACLU encouraged the organization to adopt a policy overhauling the welfare system, and guaranteeing low-income families a baseline income; but the ACLU board did not approve the proposals. The ACLU played a key role in the 1968 "King v. Smith" decision, where the Supreme Court ruled that welfare benefits for children could not be denied by a state simply because the mother cohabited with a boyfriend.
Privacy.
The right to privacy is not explicitly identified in the U.S. Constitution, but the ACLU led the charge to establish such rights in the indecisive 1961 "Poe v. Ullman" case, which addressed a state statute outlawing contraception. The issue arose again in "Griswold v. Connecticut" (1965), and this time the Supreme Court adopted the ACLUs position, and formally declared a right to privacy. The New York affiliate of the ACLU pushed to eliminate anti-abortion laws starting in 1964, a year before "Griswold" was decided, and in 1967 the ACLU itself formally adopted the right to abortion as a policy. The ACLU led the defense in "United States v. Vuitch" which expanded the right of physicians to determine when abortions were necessary. These efforts culminated in one of the most controversial Supreme Court decisions of all time, "Roe v. Wade", which legalized abortion in the first three months of pregnancy. The ACLU successfully argued against state bans on interracial marriage, in the case of "Loving v. Virginia" (1967).
Related to privacy, the ACLU engaged in several battles to ensure that government records about individuals were kept private, and to give individuals the right to review their records. The ACLU supported several measures, including the 1970 Fair Credit Reporting Act required credit agencies to divulge credit information to individuals; the 1973 Family Educational Rights and Privacy Act, which provided students the right to access their records; and the 1974 Privacy Act which prevented the federal government from disclosing personal information without good cause.
Allegations of bias.
In the early 1970s, conservatives and libertarians began to criticize the ACLU for being too political and too liberal. Legal scholar Joseph W. Bishop wrote that the ACLU's trend to partisanship started with its defense of Dr. Spock's anti-war protests. Critics also blamed the ACLU for encouraging the Supreme Court to embrace judicial activism. Critics claimed that the ACLU's support of controversial decisions like "Roe v. Wade" and "Griswold v. Connecticut" violated the intention of the authors of the Bill of Rights. The ACLU became an issue in the 1988 presidential campaign, when Republican candidate George H. W. Bush accused Democratic candidate Michael Dukakis (a member of the ACLU) of being a "card carrying member of the ACLU".
The Skokie case.
It is the policy of the ACLU to support the civil liberties of defendants regardless of their ideological stance. The ACLU takes pride in defending individuals with unpopular or bigoted viewpoints, such as George Wallace, George Lincoln Rockwell, and KKK members. The ACLU has defended American Nazis many times, and their actions often brought protests, particularly from American Jews.
In 1977, a small group of American Nazis, led by Frank Collin, applied to the town of Skokie, Illinois for permission to hold a demonstration in the town park. Skokie at the time had a majority population of Jews, totaling 40,000 of 70,000 citizens, some of whom were survivors of Nazi concentration camps. Skokie refused to grant permission, and an Illinois judge supported Skokie and prohibited the demonstration. Skokie immediately passed three ordinances aimed at preventing the group from meeting in Skokie. The ACLU assisted Collin and appealed to federal court. The appeal dragged on for a year, and the ACLU eventually prevailed in "Smith v. Collin", 447 F.Supp. 676.
The Skokie case was heavily publicized across America, partially because Jewish groups such as the Jewish Defense League and Anti Defamation League strenuously objected to the demonstration, leading many members of the ACLU to cancel their memberships. The Illinois affiliate of the ACLU lost about 25% of its membership and nearly one-third of its budget. The financial strain from the controversy led to layoffs at local chapters. After the membership crisis died down, the ACLU sent out a fund-raising appeal which explained their rationale for the Skokie case, and raised over $500,000 ($ in 2015 dollars).
Reagan era.
The inauguration of Ronald Reagan as president in 1981, ushered in an eight-year period of conservative leadership in the U.S. government. Under his leadership, the government pushed a conservative social agenda, including outlawing abortion, inserting prayer in schools, banning pornography, and resisting gay rights.
Fifty years after the Scopes trial, the ACLU found itself fighting another classroom case, the Arkansas 1981 creationism statute, which required schools to teach the biblical account of creation as a scientific alternative to evolution. The ACLU won the case in the "McLean v. Arkansas" decision.
In 1982, the ACLU became involved in a case involving the distribution of child pornography ("New York v. Ferber"). In an amicus brief, the ACLU argued that child pornography that violates the three prong obscenity test should be outlawed, but that the law in question was overly restrictive because it outlawed artistic displays and otherwise non-obscene material. The court did not adopt the ACLU's position.
During the 1988 presidential election, Vice President George H. W. Bush noted that his opponent Massachusetts Governor Michael Dukakis had described himself as a "card-carrying member of the ACLU" and used that as evidence that Dukakis was "a strong, passionate liberal" and "out of the mainstream". The phrase subsequently was used by the organization in an advertising campaign.
In 1990 the ACLU defended Lieutenant Colonel Oliver North, whose conviction was tainted by coerced testimonya violation of his fifth amendment rightsduring the IranâContra affair, where Oliver North was involved in illegal weapons sales to Iran in order to illegally fund the Contra guerillas.
Modern era.
1990 to 2000.
In 1997, ruling unanimously in the case of "Reno v. American Civil Liberties Union", the Supreme Court voted down anti-indecency provisions of the Communications Decency Act (the CDA), finding they violated the freedom of speech provisions of the First Amendment. In their decision, the Supreme Court held that the CDA's "use of the undefined terms 'indecent' and 'patently offensive' will provoke uncertainty among speakers about how the two standards relate to each other and just what they mean."
The ACLU's position on spam is considered controversial by a broad cross-section of political points of view. In 2000, Marvin Johnson, a legislative counsel for the ACLU, stated that proposed anti-spam legislation infringed on free speech by denying anonymity and by forcing spam to be labeled as such, "Standardized labeling is compelled speech." He also stated, "It's relatively simple to click and delete." The debate found the ACLU joining with the Direct Marketing Association and the Center for Democracy and Technology in criticizing a bipartisan bill in the House of Representatives in 2000. As early as 1997 the ACLU had taken a strong position that nearly all spam legislation was improper, although it has supported "opt-out" requirements in some cases. The ACLU opposed the 2003 CAN-SPAM act suggesting that it could have a chilling effect on speech in cyberspace.
In November 2000, 15 African-American residents of Hearne, Texas, were indicted on drug charges after being arrested in a series of "drug sweeps". The ACLU filed a class action lawsuit, "Kelly v. Paschall", on their behalf, alleging that the arrests were unlawful. The ACLU contended that 15 percent of Hearne's male African American population aged 18 to 34 were arrested based on the "uncorroborated word of a single unreliable confidential informant coerced by police to make cases." On May 11, 2005, the ACLU and Robertson County announced a confidential settlement of the lawsuit, an outcome which "both sides stated that they were satisfied with." The District Attorney dismissed the charges against the plaintiffs of the suit. The 2009 film American Violet depicts this case.
In 2000, the ACLU's Massachusetts affiliate represented the North American Man Boy Love Association (NAMBLA), on first amendment grounds, in the "Curley v. NAMBLA" wrongful death civil suit that was based solely on the fact that a man who raped and murdered a child had visited the NAMBLA website. Also In 2000, the ACLU lost the "Boy Scouts of America v. Dale" case, which had asked the Supreme Court to require the Boy Scouts of America to drop their policy of prohibiting homosexuals from becoming Boy Scout leaders.
Twenty-first century.
In March 2004, the ACLU, along with Lambda Legal and the National Center for Lesbian Rights, sued the state of California on behalf of six same-sex couples who were denied marriage licenses. That case, "Woo v. Lockyer", was eventually consolidated into "In re Marriage Cases", the California Supreme Court case which led to same-sex marriage being available in that state from June 16, 2008 until Proposition 8 was passed on November 4, 2008.
During the 2004 trial regarding allegations of Rush Limbaugh's drug abuse, the ACLU argued that his privacy should not have been compromised by allowing law enforcement examination of his medical records. In June 2004, the school district in Dover, Pennsylvania, required that its high school biology students listen to a statement which asserted that the theory of evolution is not fact and mentioning intelligent design as an alternative theory. Several parents called the ACLU to complain, because they believed that the school was promoting a religious idea in the classroom and violating the Establishment Clause of the First Amendment. The ACLU, joined by Americans United for Separation of Church and State, represented the parents in a lawsuit against the school district. After a lengthy trial, Judge John E. Jones III ruled in favor of the parents in the "Kitzmiller v. Dover Area School District" decision, finding that intelligent design is not science and permanently forbidding the Dover school system from teaching intelligent design in science classes.
In April, 2006, Edward Jones and the ACLU sued the City of Los Angeles, on behalf of Robert Lee Purrie and five other homeless people, for the city's violation of the 8th and 14th Amendments to the U.S. Constitution, and Article I, sections 7 and 17 of the California Constitution (supporting due process and equal protection, and prohibiting cruel and unusual punishment). The Court ruled in favor of the ACLU, stating that, "the LAPD cannot arrest people for sitting, lying, or sleeping on public sidewalks in Skid Row." Enforcement of section 41.18(d) 24 hours a day against persons who have nowhere else to sit, lie, or sleep, other than on public streets and sidewalks, is breaking these amendments. The Court said that the anti-camping ordinance is "one of the most restrictive municipal laws regulating public spaces in the United States". Jones and the ACLU wanted a compromise in which the LAPD is barred from enforcing section 41.18(d) (arrest, seizure, and imprisonment) in Skid Row between the hours of 9:00Â p.m. and 6:30Â a.m. The compromise plan permits the homeless to sleep on the sidewalk, provided they are not "within 10 feet of any business or residential entrance" and only between these hours. One of the motivations for the compromise is the shortage of space in the prison system. Downtown development business interests and the Central City Association (CCA) were against the compromise. Police Chief William Bratton said the case had slowed the police effort to fight crime and clean up Skid Row, and that when he was allowed to clean up Skid Row, real estate profited. On September 20, 2006, the Los Angeles City Council voted to reject the compromise. On October 3, 2006, police arrested Skid Row's transients for sleeping on the streets for the first time in months.
In 2006, the ACLU of Washington State joined with a pro-gun rights organization, the Second Amendment Foundation, and prevailed in a lawsuit against the North Central Regional Library District (NCRL) in Washington for its policy of refusing to disable restrictions upon an adult patron's request. Library patrons attempting to access pro-gun web sites were blocked, and the library refused to remove the blocks. In 2012, the ACLU sued the same library system for refusing to temporarily, at the request of an adult patron, disable internet filters which blocked access to Google Images.
In 2006, the ACLU challenged a Missouri law that prohibited picketing outside of veterans' funerals. The suit was filed in support of the Westboro Baptist Church and Shirley Phelps-Roper, who were threatened with arrest. The Westboro Baptist Church is well known for their picket signs that contain messages such as, "God Hates Fags", "Thank God for Dead Soldiers" and "Thank God for 9/11". The ACLU issued a statement calling the legislation a "law that infringes on Shirley Phelps-Roper's rights to religious liberty and free speech". The ACLU prevailed in the lawsuit. In 2008, the ACLU was part of a consortium of legal advocates, including Lambda Legal and the National Center for Lesbian Rights, that challenged California's Proposition 8, which declared same-sex marriages illegal. The ACLU and its allies prevailed.
In light of the Supreme Court's "Heller" decision recognizing that the Constitution protects an individual right to bear arms, ACLU of Nevada took a position of supporting "the individual's right to bear arms subject to constitutionally permissible regulations" and pledged to "defend this right as it defends other constitutional rights". Since 2008, the ACLU has increasingly assisted gun owners recover firearms that have been seized illegally by law enforcement.
In 2009, the ACLU filed an amicus brief in "Citizens United v. Federal Election Commission", arguing that the Bipartisan Campaign Reform Act of 2002 violated the First Amendment right to free speech by curtailing political speech. This stance on the landmark "Citizens United" case caused considerable disagreement within the organization, resulting in a discussion about its future stance during a quarterly board meeting in 2010. On March 27, 2012, the ACLU reaffirmed its stance in support of the Supreme Court's "Citizens United" ruling, at the same time voicing support for expanded public financing of election campaigns and stating the organization would firmly oppose any future constitutional amendment limiting free speech.
On January 7, 2013, the ACLU reached a settlement with the federal government in "Collins v. United States" that provided for the payment of full separation pay to servicemembers discharged under "don't ask, don't tell" since November 10, 2004, who had previously been granted only half that. Some 181 were expected to receive about $13,000 each.
Anti-terrorism issues.
After the September 11, 2001 attacks, the federal government instituted a broad range of new measures to combat terrorism, including the passage of the USA PATRIOT Act. The ACLU challenged many of the measures, claiming that they violated rights regarding due process, privacy, illegal searches, and cruel and unusual punishment. An ACLU policy statement states:Our way forward lies in decisively turning our backs on the policies and practices that violate our greatest strength: our Constitution and the commitment it embodies to the rule of law. Liberty and security do not compete in a zero-sum game; our freedoms are the very foundation of our strength and security. The ACLU's National Security Project advocates for national security policies that are consistent with the Constitution, the rule of law, and fundamental human rights. The Project litigates cases relating to detention, torture, discrimination, surveillance, censorship, and secrecy.
During the ensuing debate regarding the proper balance of civil liberties and security, the membership of the ACLU increased by 20%, bringing the group's total enrollment to 330,000. The growth continued, and by August 2008 ACLU membership was greater than 500,000. It remained at that level through 2011.
The ACLU has been a vocal opponent of the USA PATRIOT Act of 2001, the PATRIOT 2 Act of 2003, and associated legislation made in response to the threat of domestic terrorism. In response to a requirement of the USA PATRIOT Act, the ACLU withdrew from the Combined Federal Campaign charity drive. The campaign imposed a requirement that ACLU employees must be checked against a federal anti-terrorism watch list. The ACLU has stated that it would "reject $500,000 in contributions from private individuals rather than submit to a government 'blacklist' policy."
In 2004, the ACLU sued the federal government in "American Civil Liberties Union v. Ashcroft (2004)" on behalf of Nicholas Merrill, owner of an Internet Service Provider. Under the provisions of the PATRIOT act, the government had issued National Security Letters to Merrill to compel him to provide private internet access information from some of his customers. In addition, the government placed a gag order on Merrill, forbidding him from discussing the matter with anyone.
In January 2006, the ACLU filed a lawsuit, "ACLU v. NSA", in a federal district court in Michigan, challenging government spying in the NSA warrantless surveillance controversy. On August 17, 2006, that court ruled that the warrantless wiretapping program is unconstitutional and ordered it ended immediately. However, the order was stayed pending an appeal. The Bush administration did suspend the program while the appeal was being heard. In February 2008, the U.S. Supreme Court turned down an appeal from the ACLU to let it pursue a lawsuit against the program that began shortly after the Sept. 11 terror attacks.
The ACLU and other organizations also filed separate lawsuits around the country against telecommunications companies. The ACLU filed a lawsuit in Illinois ("Terkel v. AT&T") which was dismissed because of the State Secrets Privilege and two others in California requesting injunctions against AT&T and Verizon. On August 10, 2006, the lawsuits against the telecommunications companies were transferred to a federal judge in San Francisco.
The ACLU represents a Muslim-American who was detained but never accused of a crime in "Al-Kidd v Ashcroft", a civil suit against former Attorney General John Ashcroft. In January 2010, the American military released the names of 645 detainees held at the Bagram Theater Internment Facility in Afghanistan, modifying its long-held position against publicizing such information. This list was prompted by a Freedom of Information Act lawsuit filed in September 2009 by the ACLU, whose lawyers had also requested detailed information about conditions, rules and regulations.
The ACLU has also criticized targeted killings of American citizens who fight against the United States. In 2011 the ACLU criticized the killing of radical Muslim cleric Anwar al-Awlaki.

</doc>
<doc id="1955" url="http://en.wikipedia.org/wiki?curid=1955" title="Adobe Systems">
Adobe Systems

Adobe Systems Incorporated is an American multinational computer software company headquartered in San Jose, California, United States. The company has historically focused upon the creation of multimedia and creativity software products, with a more-recent foray towards rich Internet application software development. It is best known for the Portable Document Format (PDF) and Adobe Creative Suite, later Adobe Creative Cloud.
Adobe was founded in February 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC in order to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution.
As of 2009, Adobe Systems has 9,117 employees, about 40% of whom work in San Jose. Adobe also has major development operations in Waltham, Massachusetts; New York City, New York; Orlando, Florida; Minneapolis, Minnesota; Lehi, Utah; Seattle, Washington; San Francisco and San Luis Obispo, California in the United States;
History.
The company name "Adobe" comes from Adobe Creek in Los Altos, California, which ran behind the houses of both of the company's founders. Adobe's corporate logo, featuring the stylized "A", was designed by Marva Warnock, wife of John Warnock, who is also a graphic designer.
Adobe's first products after PostScript were digital fonts, which they released in a proprietary format called Type 1. Apple subsequently developed a competing standard, TrueType, which provided full scalability and precise control of the pixel pattern created by the font's outlines, and licensed it to Microsoft. Adobe responded by publishing the Type 1 specification and releasing Adobe Type Manager, software that allowed WYSIWYG scaling of Type 1 fonts on screen, like TrueType, although without the precise pixel-level control. But these moves were too late to stop the rise of TrueType. Although Type 1 remained the standard in the graphics/publishing market, TrueType became the standard for business and the average Windows user. In 1996, Adobe and Microsoft announced the OpenType font format, and in 2003 Adobe completed converting its Type 1 font library to OpenType.
In the mid-1980s, Adobe entered the consumer software market with Adobe Illustrator, a vector-based drawing program for the Apple Macintosh. Illustrator, which grew from the firm's in-house font-development software, helped popularize PostScript-enabled laser printers. Unlike MacDraw, the then standard Macintosh vector drawing program, Illustrator described shapes with more flexible BÃ©zier curves, providing unprecedented accuracy. Font rendering in Illustrator, however, was left to the Macintosh's QuickDraw libraries and would not be superseded by a PostScript-like approach until Adobe released Adobe Type Manager.
Adobe Systems entered NASDAQ in 1986. Its revenue has grown from roughly $1 billion in 1999 to roughly $4 billion in 2012. Adobe's fiscal years run from December to November. For example, the 2007 fiscal year ended on November 30, 2007.
In 1989, Adobe introduced what was to become its flagship product, a graphics editing program for the Macintosh called Photoshop. Stable and full-featured, Photoshop 1.0 was ably marketed by Adobe and soon dominated the market.
In 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software. PDF is now an International Standard: ISO 32000-1:2008. The technology is adopted worldwide as a common medium for electronic documents.
In December 1991, Adobe released Adobe Premiere, which Adobe rebranded to Adobe Premiere Pro in 2003. In 1994, Adobe acquired Aldus and added Adobe PageMaker and Adobe After Effects to its product line later in the year; it also controls the TIFF file format. In 1995, Adobe added Adobe FrameMaker, the long-document DTP application, to its product line after Adobe acquired Frame Technology Corp. In 1996, Adobe Systems Inc added Ares Software Corp. In 1999, Adobe introduced Adobe InCopy as a direct competitor to QuarkCopyDesk.
In 1992, Adobe acquired OCR Systems, Inc.; in 1994, the company acquired Aldus Corporation. On May 30, 1997, Adobe reincorporated in Delaware by merging with and into Adobe Systems (Delaware), which had incorporated on May 9, 1997. Adobe Systems Incorporated (Delaware), the surviving corporation, changed its name to Adobe Systems Incorporated concurrently with the merger.
The company acquired GoLive Systems, Inc. and released Adobe GoLive in 1999 and began shipping Adobe InDesign as a direct competitor to QuarkXPress and as an eventual replacement for PageMaker. In May 2003, Adobe acquired Syntrillium Software, adding Adobe Audition to its product line. In December 2004, French company OKYZ S.A., makers of 3D collaboration software, was acquired. This acquisition added 3D technology and expertise to the Adobe Intelligent Document Platform.
On December 12, 2005 Adobe acquired its main rival Macromedia in a stock swap valued at about $3.4 billion, adding Adobe ColdFusion, Adobe Contribute, Adobe Captivate, Adobe Acrobat Connect (formerly Macromedia Breeze), Adobe Director, Adobe Dreamweaver, Adobe Fireworks, Adobe Flash, FlashPaper, Adobe Flex, Macromedia FreeHand, Macromedia HomeSite, Macromedia JRun, Adobe Presenter, and Macromedia Authorware to Adobe's product line.
On November 12, 2007, CEO, Bruce Chizen resigned. Effective December 1, he was replaced by Shantanu Narayen, Adobe's current president and Chief Operating Officer. Bruce Chizen served out his term on Adobe's Board of Directors, and then continued in a strategic advisory role until the end of Adobe's 2008 fiscal year.
Adobe released Adobe Media Player in April 2008. On April 27, Adobe discontinued development and sales of its older HTML/web development software, GoLive in favor of Dreamweaver. Adobe offered a discount on Dreamweaver for GoLive users and supports those who still use GoLive with online tutorials and migration assistance. On June 1, Adobe launched Acrobat.com, a series of web applications geared for collaborative work. Creative Suite 4, which includes Design, Web, Production Premium and Master Collection came out in October 2008 in six configurations at prices from about USD $1,700 to $2,500 or by individual application. The Windows version of Photoshop includes 64-bit processing. On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak economic environment.
Adobe announced two acquisitions in 2009: on August 29, it purchased Business Catalyst, and on September 15, it bought Omniture. On November 10, the company laid off 680 employees. Adobe announced it was investigating a "coordinated attack" against corporate network systems in China, managed by the company.
Adobe's 2010 was marked by continuing front-and-back arguments with Apple over the latter's non-support for Adobe Flash on its iPhone, iPad and other products. Apple CEO Steve Jobs claimed that Flash was not reliable or secure enough, while Adobe executives have argued that Apple wish to maintain control over the iOS platform. In April 2010, Steve Jobs published a post titled "Thoughts on Flash" where he outlined his thoughts on Adobe Flash and the rise of HTML 5.
In July 2010, Adobe bought Day Software integrating their line of CQ Products: WCM, DAM, SOCO, and Mobile
In January 2011, Adobe acquired Demdex, Inc with the intent of adding Demdex's audience-optimization software to its online marketing suite.
At Photoshop World 2011, Adobe unveiled a new mobile photo service. Carousel is a new application for iPhone, iPad and Mac that uses Photoshop Lightroom technology for users to adjust and fine-tune images on all platforms. Carousel will also allow users to automatically sync, share and browse photos. The service was later renamed to "Adobe Revel".
On November 9, 2011 Adobe announced that they would cease development of Flash for mobile devices following version 11.1. Instead they will be focusing on HTML 5 for mobile devices.
On December 1, 2011, Adobe announced that it has entered into a definitive agreement to acquire privately held Efficient Frontier.
In December 2012, Adobe opened a new 280,000 square foot corporate campus in Lehi, UT.
In 2013 Adobe Systems endured a major security breach. Vast portions of the source code for the company's software were stolen and posted online and over 150 million records of Adobe's customers have been made readily available for download.
A class-action lawsuit alleging that the company suppressed employee compensation was filed against Adobe, and three other Silicon Valley-based companies in a California federal district court in 2013. In May 2014 it was revealed the four companies, Adobe, Apple, Google, and Intel had reached agreement with the plaintiffs, 64,000 employees of the four companies, to pay a sum of $324.5 million to settle the suit.
Products.
Reception.
For years hackers have exploited vulnerabilities in Adobe programs, such as Adobe Reader, to gain unauthorized access to computers. Adobe's Flash Player has also been criticized for, among other things, suffering from performance, memory usage and security problems (see criticism of Flash Player). A report by security researchers from Kaspersky Lab criticized Adobe for producing the products having top 10 security vulnerabilities.
Observers noted that Adobe was spying on its customers by including spyware in the Creative Suite 3 software and quietly sending user data to a firm named Omniture. When users became aware, Adobe explained what the suspicious software did and admitted that they: "could and should do a better job taking security concerns into account". When a security flaw was later discovered in Photoshop CS5, Adobe sparked outrage by saying it would leave the flaw unpatched, so anyone who wanted to use the software securely would have to pay for an upgrade. Following a fierce backlash Adobe decided to provide the software patch.
Adobe has been criticized for pushing unwanted software - third-party browser toolbars and free virus scanners, usually as part of the Flash update process, and for pushing a third-party scareware program designed to scare users into paying for unneeded system repairs.
Awards.
Since 1995, "Fortune" has ranked Adobe as an outstanding place to work. Adobe was rated the 5th-best U.S. company to work for in 2003, 6th in 2004, 31st in 2007, 40th in 2008, 11th in 2009, 42nd in 2010, 65th in 2011, 41st in 2012, and 83rd in 2013.
In May 2008, Adobe Systems India was ranked 19th of great places to work in India. In June 2014, it was ranked 6th of great places to work in India. In October 2008, Adobe Systems Canada Inc. was named one of "Canada's Top 100 Employers" by Mediacorp Canada Inc., and was featured in "Maclean's" newsmagazine.
Pricing.
Adobe has been criticized for its pricing practices, with retail prices being as much as twice as high in non-US countries as in the US. As pointed out by many, it is significantly cheaper to pay for a return airfare ticket to the United States and purchase one particular collection of Adobe's software there than to buy it locally in Australia.
After Adobe revealed the pricing for the Creative Suite 3 Master Collection, which was Â£1,000 higher for European customers, a petition to protest over "unfair pricing" was published and signed by 10,000 users. In June 2009, Adobe further increased its prices in the UK by 10% in spite of weakening of the pound against the dollar, and UK users are not allowed to buy from the US store.
Source code and customer data breach.
On October 3, 2013, the company initially revealed that 2.9 million customers' sensitive and personal data was stolen in security breach which included encrypted credit card information. Adobe later admitted that 38 million active users have been affected and the attackers obtained access to their IDs and encrypted passwords, as well as to many inactive Adobe accounts. The company did not make it clear if all the personal information was encrypted, such as email addresses and physical addresses, though data privacy laws in 44 states require this information to be encrypted.
A 3.8 GB file stolen from Adobe and containing 152 million usernames, reversibly encrypted passwords and unencrypted password hints was posted on AnonNews.org. LastPass, a password security firm, said that Adobe failed to use best practices for securing the passwords and has not salted them. Another security firm, Sophos, showed that Adobe used a weak encryption method permitting the recovery of a lot of information with very little effort. According to an IT expert, Adobe has failed its customers and âshould hang their heads in shameâ.
Many of the credit cards were tied to the Creative Cloud software-by-subscription service. Adobe offered its affected US customers a free membership in a credit monitoring service, but no similar arrangements have been made for non-US customers. When a data breach occurs in the US, penalties depend on the state where the victim resides, not where the company is based.
After stealing the customers' data, cyber-thieves also accessed Adobe's source code repository, likely in mid-August 2013. Because hackers acquired copies of the source code of Adobe proprietary products, they could find and exploit any potential weaknesses in its security, computer experts warned. Security researcher Alex Holden, chief information security officer of Hold Security, characterized this Adobe breach, which affected Acrobat, ColdFusion and numerous other applications, as "one of the worst in US history". Adobe also announced that hackers stole parts of the source code of Photoshop, which according to commentators could allow programmers to copy its engineering techniques and would make it easier to pirate Adobe's expensive products.
On a server of a Russian-speaking hacker group, the "disclosure of encryption algorithms, other security schemes, and software vulnerabilities can be used to bypass protections for individual and corporate data" and may have opened the gateway to new generation zero-day attacks. Hackers already used ColdFusion exploits to make off with usernames and encrypted passwords of PR Newswire's customers, which has been tied to the Adobe security breach. They also used a ColdFusion exploit to breach Washington state court and expose up to 160,000 Social Security numbers.
Public relations.
Adobe ranked no. 5 on a list of "Internetâs 9 Most Hated Companies", based on a 2013 survey on Reddit.com. Adobe's Reader and Flash were listed on "The 10 most hated programs of all time" on TechRadar.com.

</doc>
<doc id="1957" url="http://en.wikipedia.org/wiki?curid=1957" title="Alexander technique">
Alexander technique

Lessons in the Alexander technique, named after Frederick Matthias Alexander, teach people how to stop using unnecessary levels of muscular and mental tension during their everyday activities. It is an educational process rather than a relaxation technique or form of exercise. Most other methods take it for granted that 'one's awareness of oneself' is accurate, whereas Alexander realized that a person who had been using himself wrongly for a long time could not trust his feelings (sensory appreciation) in carrying out any activity (Bloch, 221). Practitioners say that such problems are often caused by repeated misuse of the body over a long period of time, for example, by standing or sitting with one's weight unevenly distributed, holding one's head incorrectly, or walking or running inefficiently. The purpose of the Alexander technique is to help people unlearn maladaptive physical habits and return to a balanced state of rest and poise in which the body is well-aligned.
Alexander developed the technique's principles in the 1890s as a personal tool to alleviate breathing problems and hoarseness during public speaking. He credited the technique with allowing him to pursue his passion for Shakespearean acting.
History.
Frederick Matthias Alexander (1869-1955) was a Shakespearean orator who developed voice loss during his performances. After doctors found no physical cause, Alexander reasoned that he was doing something to himself while speaking to cause his problem. His self-observation in multiple mirrors revealed that he was contracting his whole body prior to phonation in preparation for all verbal response. He developed the hypothesis that this habitual pattern of pulling the head backwards and downwards needlessly disrupted the normal working of the total postural, breathing and vocal mechanisms. After experimenting to develop his ability to stop the unnecessary and habitual contracting in his neck, he found that his problem with recurrent voice loss was resolved. While on a recital tour in New Zealand (1895) he began to realise the wider significance of head carriage for overall physical functioning. Further, Alexander observed that many individuals commonly tightened the musculature of the upper torso as he had done, in anticipation of many other activities besides speech.
Alexander believed his work could be applied to improve individual health and well being. He further refined his technique of self-observation and re-training to teach his discoveries to others. He explained his reasoning in four books published in 1918, 1923, 1931 (1932 in the UK) and 1942. He also trained teachers to teach his work from 1930 until his death in 1955. Teacher training was interrupted during World War II between 1941 and 1943, when Alexander accompanied children and teachers of the Little School to Stow, Massachusetts to join his brother. A.R. Alexander also taught his brother's technique. In the 1960s, there was enough interest to start the first dedicated school, called The American Center for the Alexander Technique, in New York City.
Famous people who have studied the Alexander Technique include writers Aldous Huxley, Robertson Davies and Roald Dahl, playwright George Bernard Shaw, actors Judi Dench, Hilary Swank, Sir Ben Kingsley, Michael Caine, Jeremy Irons, Suzanna Hamilton, John Cleese, Kevin Kline, William Hurt, Jamie Lee Curtis, Paul Newman, Mary Steenburgen, Robin Williams and Patti Lupone, musicians Paul McCartney, Madonna, Yehudi Menuhin and Sting, and Nobel Prize winner for medicine and physiology Nikolaas Tinbergen.
Process.
Alexander's approach emphasises the use of freedom to choose beyond conditioning in every action. The technique is applied dynamically to everyday movements, as well as actions selected by students.
Because of a change in balance, actions such as sitting, squatting, lunging or walking are often selected by the teacher. Other actions may be selected by the student, tailored to their interests or work activities such as hobbies, computer use, lifting, driving or performance in acting, sports, speech or music. Alexander teachers often use themselves as examples. They demonstrate, explain, and analyse a student's moment to moment responses as well as using mirrors, video feedback or classmate observations. Guided modelling with light hand contact is the primary tool for detecting and guiding the way past unnecessary effort. Suggestions for improvements are often student-specific.
Exercise as a teaching tool is deliberately omitted because of a common mistaken assumption that there exists a "correct" position. There are only two specific exercises practised separately; the first is lying semi-supine; resting in this way uses "mechanical advantage" as a means of releasing cumulative muscular tension. It's also a specific time to practice Alexander's principle of conscious "directing" without "doing." The second exercise is the "Whispered Ah," which is used to co-ordinate and free breathing & vocal production.
Freedom, efficiency and patience are the prescribed values. Proscribed are unnecessary effort, self-limiting habits as well as mistaken perceptual assumptions. Students are led to change their largely automatic routines that are interpreted by the teacher to currently or cumulatively be physically limiting, inefficient or not in keeping with anatomical structure. The Alexander teacher provides verbal coaching while monitoring, guiding and preventing unnecessary habits at their source with a specialised hands-on assistance. This specialised hands-on requires Alexander teachers to demonstrate on themselves the improved physical co-ordination they are communicating to the student.
Alexander developed terminology to describe his methods, outlined in his four books that explain the sometimes paradoxical experience of learning and substituting new improvements.
Uses.
According to Alexander Technique instructor Michael J. Gelb, people tend to study the Alexander Technique either to rid themselves of pain, to increase their performance abilities, or for reasons of personal development and transformation.
As an example among performance-art applications, the Alexander technique is used and taught by classically trained vocal coaches and musicians. Its advocates claim that it allows for the free alignment of all aspects of the vocal tract by consciously increasing air-flow, allowing improved vocal technique and tone. Because the technique has allegedly been used to improve breathing and stamina in general, advocates also claim that athletes, people with asthma, tuberculosis, and panic attacks have also found improvements. The technique has been used by actors to reduce stage fright and to increase spontaneity. By improving stress-management, the technique can be an adjunct to psychotherapy for people with disabilities, Post-traumatic Stress Disorder, panic attacks, stuttering, and chronic pain.
Method.
The Alexander Technique is most commonly taught privately in a series of 10 to 40 private lessons which may last from 30 minutes to an hour. Students are often performers, such as actors, dancers, musicians, athletes and public speakers, or people who work on computers, or who are in frequent pain for other reasons. Instructors observe their students, then show them how to hold themselves and move with better poise and less strain. Sessions include chair work and table work, often in front of a mirror, during which the instructor and the student will stand, sit and lie down, moving efficiently while maintaining a correct relationship between the head, neck and spine.
To qualify as a teacher of Alexander Technique, instructors are required to complete at least 1,600 hours, spanning at least three years, of supervised teacher training. The result must be satisfactory to qualified peers to gain membership in professional societies.
Effectiveness.
The Alexander technique is cost-effective in the management of chronic pain.
There is no good evidence the technique helps people with their asthma.
There is weak evidence the Technique may help improve the quality of life of people with Parkinson's Disease.
Influence.
The American philosopher and educator John Dewey became impressed with the Alexander technique after his headaches, neck pains, blurred vision, and stress symptoms largely improved during the time he used Alexander's advice to change his posture. In 1923, Dewey wrote the introduction to Alexander's "Constructive Conscious Control of the Individual".
Aldous Huxley had transformative lessons with Alexander, and continued doing so with other teachers after moving to the US. He rated Alexander's work highly enough to base the character of the doctor who saves the protagonist in 'Eyeless in Gaza' (an experimental form of autobiographical work) on F.M. Alexander, putting many of his phrases into the character's mouth. Huxley's work 'The Art of Seeing' also discusses his views on the technique.
Sir Stafford Cripps, George Bernard Shaw, Henry Irving and other stage grandees, Lord Lytton and other eminent people of the era also wrote positive appreciations of his work after taking lessons with Alexander.
Since Alexander's work in the field came at the start of the 20th century, his ideas influenced many originators in the field of mind-body improvement. Fritz Perls, who originated GestaltÂ therapy, credited Alexander as an inspiration for his psychological work. The Feldenkrais Method and the Mitzvah Technique were both influenced by the Alexander technique, in the form of study previous to the originators founding their own disciplines.

</doc>
<doc id="1960" url="http://en.wikipedia.org/wiki?curid=1960" title="Andrea Alciato">
Andrea Alciato

Andrea Alciato (8 May 1492 â 12 January 1550), commonly known as Alciati (Andreas Alciatus), was an Italian jurist and writer. He is regarded as the founder of the French school of legal humanists.
Biography.
Alciati was born in Alzate Brianza, near Milan, and settled in France in the early 16th century. He displayed great literary skill in his exposition of the laws, and was one of the first to interpret the civil law by the history, languages and literature of antiquity, and to substitute original research for the servile interpretations of the glossators. He published many legal works, and some annotations on Tacitus and accumulated a sylloge of Roman inscriptions from Milan and its territories, as part of his preparation for his history of Milan, written in 1504-05. 
Alciati is most famous for his "Emblemata," published in dozens of editions from 1531 onward. This collection of short Latin verse texts and accompanying woodcuts created an entire European genre, the emblem book, which attained enormous popularity in continental Europe and Great Britain.
Alciati died at Pavia in 1550.

</doc>
<doc id="1962" url="http://en.wikipedia.org/wiki?curid=1962" title="Apparent magnitude">
Apparent magnitude

The apparent magnitude (m) of a celestial body is a measure of its brightness as seen by an observer on Earth, adjusted to the value it would have in the absence of the atmosphere. The brighter the object appears, the lower the value of its magnitude. Generally the visible spectrum (vmag) is used as a basis for the apparent magnitude, but other regions of the spectrum, such as the near-infrared J-band, are also used. In the visible spectrum Sirius is the brightest star in the visible sky, whereas in the near-infrared J-band, Betelgeuse is the brightest.
History.
<div style="clear:both;"/>
The scale used to indicate magnitude originates in the Hellenistic practice of dividing stars visible to the naked eye into six "magnitudes". The brightest stars in the night sky were said to be of first magnitude ("m" = 1), whereas the faintest were of sixth magnitude ("m" = 6), the limit of human visual perception (without the aid of a telescope). Each grade of magnitude was considered twice the brightness of the following grade (a logarithmic scale). This somewhat crude method of indicating the brightness of stars was popularized by Ptolemy in his "Almagest", and is generally believed to originate with Hipparchus. This original system did not measure the magnitude of the Sun.
In 1856, Norman Robert Pogson formalized the system by defining a typical first magnitude star as a star that is 100 times as bright as a typical sixth magnitude star; thus, a first magnitude star is about 2.512 times as bright as a second magnitude star. The fifth root of 100 is known as "Pogson's Ratio". Pogson's scale was originally fixed by assigning Polaris a magnitude of 2. Astronomers later discovered that Polaris is slightly variable, so they first switched to Vega as the standard reference star, and then switched to using tabulated zero points for the measured fluxes. The magnitude depends on the wavelength band (see below).
The modern system is no longer limited to 6 magnitudes or only to visible light. Very bright objects have "negative" magnitudes. For example, Sirius, the brightest star of the celestial sphere, has an apparent magnitude of â1.4. The modern scale includes the Moon and the Sun. The full Moon has a mean apparent magnitude of â12.74 and the Sun has an apparent magnitude of â26.74. The Hubble Space Telescope has located stars with magnitudes of 30 at visible wavelengths and the Keck telescopes have located similarly faint stars in the infrared.
Calculations.
As the amount of light received actually depends on the thickness of the Earth's atmosphere in the line of sight to the object, the apparent magnitudes are adjusted to the value they would have in the absence of the atmosphere. The dimmer an object appears, the higher the numerical value given to its apparent magnitude. Note that brightness varies with distance; an extremely bright object may appear quite dim, if it is far away. Brightness varies inversely with the square of the distance. The absolute magnitude, "M", of a celestial body (outside the Solar System) is the apparent magnitude it would have if it were at 10 parsecs (~32.6 light years) and that of a planet (or other Solar System body) is the apparent magnitude it would have if it were 1 astronomical unit from both the Sun and Earth. The absolute magnitude of the Sun is 4.83 in the V band (yellow) and 5.48 in the B band (blue).
The apparent magnitude, "m", in the band, "x", can be defined as,
where formula_2 is the observed flux in the band x, and formula_3 and formula_4 are a reference magnitude, and reference flux in the same band x, such as that of Vega. An increase of 1 in the magnitude scale corresponds to a decrease in brightness by a factor of formula_5. Based on the properties of logarithms, a difference in magnitudes, formula_6, can be converted to a variation in brightness as formula_7.
Example: Sun and Moon.
"What is the ratio in brightness between the Sun and the full moon?"
The apparent magnitude of the Sun is -26.74 (brighter), and the mean apparent magnitude of the full moon is -12.74 (dimmer).
Difference in magnitude : formula_8
Variation in Brightness : formula_9
The Sun appears about 400,000 times brighter than the full moon.
Magnitude addition.
Sometimes, it might be useful to add magnitudes. For example, to determine the combined magnitude of a double star when the magnitudes of the individual components are known. This can be done by setting an equation using the brightness (in linear units) of each magnitude.
formula_10
Solving for formula_11 yields
formula_12
where formula_11 is the resulting magnitude after adding formula_14 and formula_15. Note that the negative of each magnitude is used because greater intensities equate to lower magnitudes.
Standard reference values.
It is important to note that the scale is logarithmic: the relative brightness of two objects is determined by the difference of their magnitudes. For example, a difference of 3.2 means that one object is about 19 times as bright as the other, because Pogson's Ratio raised to the power 3.2 is approximately 19.05.
A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber-Fechner law), but it is now believed that the response is a power law (see Stevens' power law).
Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350Â nm, in the near ultraviolet), B (about 435Â nm, in the blue region) and V (about 555Â nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the light-adapted human eye, and when an apparent magnitude is given without any further qualification, it is usually the V magnitude that is meant, more or less the same as visual magnitude.
Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.
Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.
For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. This relationship does not apply for objects at very great distances (far beyond the Milky Way), because a correction for general relativity must then be taken into account due to the non-Euclidean nature of space. 
For planets and other Solar System bodies the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.
Table of notable celestial objects.
Some of the above magnitudes are only approximate. Telescope sensitivity also depends on observing time, optical bandpass, and interfering light from scattering and airglow.

</doc>
<doc id="1963" url="http://en.wikipedia.org/wiki?curid=1963" title="Absolute magnitude">
Absolute magnitude

Absolute magnitude is the measure of a celestial object's intrinsic brightness. It is the hypothetical apparent magnitude of an object at a standard luminosity distance of exactly 10.0Â parsecs or about 32.6 light years from the observer, assuming no astronomical extinction of starlight. This allows the true energy output of astronomical objects to be compared without regard to their variable distances. As with all astronomical magnitudes, the absolute magnitude can be specified for different wavelength intervals; for stars the most commonly quoted absolute magnitude is the absolute visual magnitude, which is the absolute magnitude in the visual (V) band of the UBV system. Also commonly used is the absolute bolometric magnitude, which is the total luminosity expressed in magnitude units; it takes into account energy radiated at all wavelengths, whether observed or not.
The absolute magnitude uses the same conventions as the visual magnitude: brighter objects have smaller magnitudes, and 5 magnitudes corresponds exactly to a factor of 100, so a factor of 100.4 (â2.512) ratio of brightness corresponds to a difference of 1.0 in magnitude. The Milky Way, for example, has an absolute magnitude of about â20.5, so a quasar with an absolute magnitude of â25.5 is 100 times brighter than our galaxy. If this particular quasar and our galaxy could be seen side by side at the same distance, the quasar would be 5 magnitudes (or 100 times) brighter than our galaxy. Similarly, Canopus has an absolute visual magnitude of about -5.5, while Ross 248 has an absolute visual magnitude of +14.8, for a difference of slightly more than 20 magnitudes, so if the two stars were at the same distance, Canopus would be seen as about 20 magnitudes brighter; stated another way, Canopus gives off slightly more than 100 million (108) times more visual power than Ross 248.
Stars and galaxies ("M").
In stellar and galactic astronomy, the standard distance is 10 parsecs (about 32.616 light years, 308.57 petameters or 308.57 trillion kilometres).
A star at 10 parsecs has a parallax of 0.1" (100 milli arc seconds).
Galaxies (and other extended objects) are much larger than 10 parsecs, their light is radiated over an extended patch of sky, and their overall brightness cannot be directly observed from relatively short distances, but the same convention is used. A galaxy's magnitude is defined by measuring all the light radiated over the entire object, treating that integrated brightness as the brightness of a single point-like or star-like source, and computing the magnitude of that point-like source as it would appear if observed at the standard 10 parsecs distance. Consequently, the absolute magnitude of any object "equals" the apparent magnitude it "would have" if it was 10 parsecs away.
In using an absolute magnitude one must specify the type of electromagnetic radiation being measured. When referring to total energy output, the proper term is bolometric magnitude. The bolometric magnitude usually is computed from the visual magnitude plus a bolometric correction, formula_1. This correction is needed because very hot stars radiate mostly ultraviolet radiation, while very cool stars radiate mostly infrared radiation (see Planck's law). 
Many stars visible to the naked eye have such a low absolute magnitude that they would appear bright enough to cast shadows if they were only 10 parsecs from the Earth: Rigel (â7.0), Deneb (â7.2), Naos (â6.0), and Betelgeuse (â5.6). For comparison, Sirius has an absolute magnitude of 1.4 which is brighter than the Sun, whose absolute visual magnitude is 4.83 (it actually serves as a reference point). The Sun's absolute bolometric magnitude is set arbitrarily, usually at 4.75.
Absolute magnitudes of stars generally range from â10 to +17. The absolute magnitudes of galaxies can be much lower (brighter). For example, the giant elliptical galaxy M87 has an absolute magnitude of â22 (i.e. as bright as about 60,000 stars of magnitude â10).
Computation.
For a negligible extinction, one can compute the absolute magnitude formula_2 of an object given its apparent magnitude formula_3 and luminosity distance formula_4:
where formula_4 is the star's luminosity distance in parsecs, where 1 parsec is 206,265 astronomical units, approximately 3.2616 light-years. For very large distances, the cosmological redshift complicates the relation between absolute and apparent magnitude, because the radiation observed at one wavelength was radiated at a significantly different one. For comparing the magnitudes of very distant objects with those of local objects, a k correction might have to be applied to the magnitudes of the distant objects.
For nearby astronomical objects (such as stars in our galaxy) luminosity distance "DL" is almost identical to the real distance to the object, because spacetime within our galaxy is almost Euclidean. For much more distant objects the Euclidean approximation is not valid, and General Relativity must be taken into account when calculating the luminosity distance of an object.
In the Euclidean approximation for nearby objects, the absolute magnitude formula_2 of a star can be calculated from its apparent magnitude and parallax:
where p is the star's parallax in arcseconds.
You can also compute the absolute magnitude formula_2 of an object given its apparent magnitude formula_3 and distance modulus formula_11:
Examples.
Rigel has a visual magnitude of formula_13 and distance about 860 light-years
Vega has a parallax of 0.129", and an apparent magnitude of +0.03
Alpha Centauri A has a parallax of 0.742" and an apparent magnitude of â0.01
The Black Eye Galaxy has a visual magnitude of mV=+9.36 and a distance modulus of 31.06.
Apparent magnitude.
Given the absolute magnitude formula_2, for objects within our galaxy you can also calculate the apparent magnitude formula_3 from any distance formula_20 (in parsecs):
For objects at very great distances (outside our galaxy) the luminosity distance "DL" must be used instead of "d" (in parsecs).
Given the absolute magnitude formula_2, you can also compute apparent magnitude formula_3 from its parallax formula_24:
Also calculating absolute magnitude formula_2 from distance modulus formula_11:
Bolometric magnitude.
Bolometric magnitude corresponds to luminosity, expressed in magnitude units; that is, after taking into account all electromagnetic wavelengths, including those unobserved due to instrumental pass-band, the Earth's atmospheric absorption, or extinction by interstellar dust. For stars, in the absence of extensive observations at many wavelengths, it usually must be computed assuming an effective temperature.
The difference in bolometric magnitude is related to the luminosity ratio according to:
which makes by inversion:
where
Solar System bodies ("H").
For planets and asteroids a different definition of absolute magnitude is used which is more meaningful for nonstellar objects.
In this case, the absolute magnitude (H) is defined as the apparent magnitude that the object would have if it were one astronomical unit (AU) from both the Sun and the observer. Because the object is illuminated by the Sun, absolute magnitude is a function of phase angle and this relationship is referred to as the phase curve.
To convert a stellar or galactic absolute magnitude into a planetary one, subtract 31.57. A comet's nuclear magnitude (M2) is a different scale and can not be used for a size comparison with an asteroid's (H) magnitude.
Apparent magnitude.
The absolute magnitude can be used to help calculate the apparent magnitude of a body under different conditions.
where formula_36 is 1 au, formula_37 is the phase angle, the angle between the SunâBody and BodyâObserver lines. By the law of cosines, we have:
formula_39 is the phase integral (integration of reflected light; a number in the 0 to 1 range).
Example: Ideal diffuse reflecting sphere. A reasonable first approximation for planetary bodies
A full-phase diffuse sphere reflects â as much light as a diffuse disc of the same diameter. 
Distances:
Note: because Solar System bodies are never perfect diffuse reflectors, astronomers use empirically derived relationships to predict apparent magnitudes when accuracy is required.
Example.
Moon:
How bright is the Moon from Earth?
Meteors.
For a meteor, the standard distance for measurement of magnitudes is at an altitude of at the observer's zenith.

</doc>
<doc id="1965" url="http://en.wikipedia.org/wiki?curid=1965" title="Apollo 1">
Apollo 1

Apollo 1 (initially designated Apollo Saturn-204 and AS-204) was the first manned mission of the U.S. Apollo manned lunar landing program. The planned low Earth orbital test of the Apollo Command/Service Module never made its target launch date of February 21, 1967 because a cabin fire during a launch rehearsal test on January 27 at Cape Canaveral Air Force Station Launch Complex 34 killed all three crew membersâCommand Pilot Virgil I. "Gus" Grissom, Senior Pilot Edward H. White II and Pilot Roger B. Chaffeeâand destroyed the Command Module (CM). The name "Apollo 1", chosen by the crew, was officially retired by NASA in commemoration of them on April 24, 1967.
Immediately after the fire, NASA convened the "Apollo 204 Accident Review Board" to determine the cause of the fire, and both houses of the United States Congress launched their own committee inquiries to oversee NASA's investigation. During the investigation, a NASA internal document citing problems with prime Apollo contractor North American Aviation was publicly revealed by a Senator and became known as the "Phillips Report", embarrassing NASA Administrator James E. Webb, who was unaware of the document's existence, and attracting controversy to the Apollo program. Despite congressional displeasure at NASA's openness, both congressional committees ruled that the issues raised in the report had no bearing on the accident, and allowed NASA to continue with the program.
Although the ignition source could not be conclusively identified, the astronauts' deaths were attributed to a wide range of lethal design and construction flaws in the early Apollo Command Module. Manned Apollo flights were suspended for 20 months while these problems were corrected. 
The Saturn IB launch vehicle, SA-204, scheduled for use on this mission, was later used for the first unmanned Lunar Module (LM) test flight, Apollo 5. The first successful manned Apollo mission was flown by Apollo 1's backup crew on Apollo 7 in October 1968.
Mission background.
AS-204 was to be the first manned test flight of the Apollo Command/Service Module (CSM) to Earth orbit, launched on a Saturn IB rocket. AS-204 was to test launch operations, ground tracking and control facilities and the performance of the Apollo-Saturn launch assembly and would have lasted up to two weeks, depending on how the spacecraft performed.
The CSM for this flight, number 012 built by North American Aviation (NAA), was a Block I version designed before the lunar orbit rendezvous landing strategy was chosen; therefore it lacked capability of docking with the Lunar Module. This was incorporated into the Block II CSM design, along with lessons learned in Block I. Block II would be test-flown with the LM when the latter was ready, and would be used on the Moon landing flights.
NASA announced on March 21, 1966, that Grissom, White and Chaffee had been selected to fly the first manned mission. James McDivitt, David Scott and Russell Schweickart were named as the backup crew, and Walter Schirra, Donn Eisele and Walter Cunningham were named as the prime crew for a second Block I CSM flight, AS-205. NASA planned to follow this with an unmanned test flight of the LM (AS-206), then the third manned mission would be a dual flight designated AS-278, in which AS-207 would launch the first manned Block II CSM, which would then rendezvous and dock with the LM launched unmanned on AS-208.
At the time, NASA was studying the possibility of flying the first Apollo mission as a joint space rendezvous with the final Project Gemini mission, Gemini 12 in November 1966. But by May, delays in making Apollo ready for flight just by itself, and the extra time needed to incorporate compatibility with the Gemini, made that impractical. This became moot when slippage in readiness of the AS-204 spacecraft caused the last-quarter 1966 target date to be missed, and the mission was rescheduled for February 21, 1967. Grissom was resolved to keep his craft in orbit for a full 14 days if there was any way to do so.
A newspaper article published on August 4, 1966, referred to the flight as "Apollo 1". CM-012 arrived at the Kennedy Space Center on August 26, labeled "Apollo One" by NAA on its packaging.
In October 1966, NASA announced the flight would carry a small television camera to broadcast live from the Command Module. The camera would also be used to allow flight controllers to monitor the spacecraft's instrument panel in flight. Television cameras were carried aboard all manned Apollo missions.
By December 1966, the second Block I flight AS-205 was canceled as unnecessary; and Schirra, Eisele and Cunningham were reassigned as the backup crew for Apollo 1. McDivitt's crew was now promoted to prime crew of the Block II / LM mission, re-designated AS-258 because the AS-205 launch vehicle would be used in place of AS-207. A third manned mission was planned to launch the CSM and LM together on a Saturn V (AS-503) to an elliptical medium Earth orbit (MEO), to be crewed by Frank Borman, Michael Collins and William Anders. McDivitt, Scott and Schweickart had started their training for AS-278 in CM-101 when the Apollo 1 accident occurred.
Mission insignia.
Grissom's crew received approval in June 1966 to design a mission patch with the name "Apollo 1". The design's center depicts a Command/Service Module flying over the southeastern United States with Florida (the launch point) prominent. The Moon is seen in the distance, symbolic of the eventual program goal. A yellow border carries the mission and astronaut names with another border set with stars and stripes, trimmed in gold. The insignia was designed by the crew, with the artwork done by North American Aviation employee Allen Stevens.
Spacecraft problems.
The Apollo Command/Service Module spacecraft was much bigger and far more complex than any previously implemented spacecraft design. In October 1963, Joseph F. Shea was named Apollo Spacecraft Program Office (ASPO) manager, responsible for managing the design and construction of both the CSM and the LM.
When North American shipped spacecraft CM-012 to Kennedy Space Center on August 26, 1966, there were 113 significant incomplete planned engineering changes, and an additional 623 engineering change orders were made after delivery. Grissom was so frustrated with the inability of the training simulator engineers to keep up with the actual spacecraft changes, that he took a lemon from a tree by his house and hung it on the simulator.
In a spacecraft review meeting held with Shea on August 19, 1966 (a week before delivery), the crew expressed concern about the amount of flammable material (mainly nylon netting and Velcro) in the cabin, which the technicians found convenient for holding tools and equipment in place. Though Shea gave the spacecraft a passing grade, after the meeting they gave him a crew portrait they had posed with heads bowed and hands clasped in prayer, with the inscription:
It isn't that we don't trust you, Joe, but this time we've decided to go over your head.
Shea gave his staff orders to tell North American to remove the flammables from the cabin, but did not supervise the issue personally.
Accident.
Plugs-out test.
The launch simulation on January 27, 1967, was a "plugs-out" test to determine whether the spacecraft would operate nominally on (simulated) internal power while detached from all cables and umbilicals. Passing this test was essential to making the February 21 launch date. The test was considered non-hazardous because neither the launch vehicle nor the spacecraft was loaded with fuel or cryogenics, and all pyrotechnic systems were disabled.
At 1:00Â pm EST (1800 GMT) on January 27, first Grissom, then Chaffee, and White entered the Command Module fully pressure-suited, and were strapped into their seats and hooked up to the spacecraft's oxygen and communication systems. There was an immediate problem: Grissom noticed a strange odor in the air circulating through his suit which he compared to "sour buttermilk", and the simulated countdown was held at 1:20Â pm, while air samples were taken. No cause of the odor could be found, and the countdown was resumed at 2:42Â pm. (The accident investigation found this odor not to be related in any way to the fire.)
Three minutes after the count was resumed, the hatch installation was started. The hatch consisted of three parts: a removable inner hatch, which stayed inside the cabin; a hinged outer hatch, which was part of the spacecraft's heat shield; and an outer hatch cover, which was part of the boost protective cover enveloping the entire Command Module to protect it from aerodynamic heating during launch and from launch escape rocket exhaust in the event of a launch abort. The boost hatch cover was partially but not fully latched in place, because the flexible boost protective cover was slightly distorted by some cabling run under it to provide the simulated internal power. (The spacecraft's fuel cell reactants were not loaded for this test.) After the hatches were sealed, the air in the cabin was replaced with pure oxygen at , 2 psi higher than atmospheric pressure.
Further problems included episodes of high oxygen spacesuit flow, which tripped an alarm. The likely cause was determined to be the astronauts' movements, which were detected by the spacecraft's inertial guidance gyroscope and Grissom's stuck-open microphone. The open microphone was part of the third major problem, with the communications loop connecting the crew, the Operations and Checkout Building and the Complex 34 blockhouse control room. The problems led Grissom to remark: "How are we going to get to the Moon if we can't talk between three buildings?" The simulated countdown was held again at 5:40Â pm while attempts were made to fix the problem. All countdown functions up to the simulated internal power transfer had been successfully completed by 6:20Â pm, but at 6:30 the count remained on hold at T minus 10 minutes.
Fire.
The crew members were using the time to run through their checklist again, when a voltage transient was recorded at 6:30:54 (23:30:54 GMT). Ten seconds later (at 6:31:04), Chaffee exclaimed "Hey!", and scuffling sounds followed for two seconds. White then reported, "I've got a fire in the cockpit!". Some witnesses said that they saw White on the television monitors, reaching for the inner hatch release handle as flames in the cabin spread from left to right and licked the window. The final voice transmission is believed to have come from Chaffee. Six seconds after White's report of a "fire in the cockpit", a voice cried out, "There's a bad fire!". The sound of the spacecraft's hull rupturing was heard immediately afterwards, followed by "I'm burning up!" and a scream. The transmission then ended abruptly at 6:31:21, only 17 seconds after the first report of fire. The cabin had ruptured due to rapidly expanding gases from the fire, which over-pressurized the Command Module to .
Flames and gases then rushed outside the Command Module through open access panels to two levels of the pad service structure. Intense heat, dense smoke, and ineffective gas masks designed for toxic fumes rather than heavy smoke hampered the ground crew's attempts to rescue the men. There were fears the Command Module had exploded, or soon would, and that the fire might ignite the solid fuel rockets in the launch escape tower above the Command Module, which would have likely killed nearby ground personnel. It took five minutes to open all three hatch layers, and they could not drop the inner hatch to the cabin floor as intended, so they pushed it out of the way to one side.
The initial phase of the fire lasted only about 15 seconds before the Command Module's hull ruptured. As the cabin depressurized, the convective rush of air caused the flames to spread rapidly, beginning the second phase. The third phase began when most of the atmosphere was consumed. At this point, the fire largely stopped, but massive amounts of smoke, dust, carbon monoxide, and fumes now filled the cabin. Although the cabin lights remained lit, the ground crew was at first unable to find the astronauts through the dense smoke. As the smoke cleared they found the bodies but were not able to remove them. The fire had partly melted Grissom's and White's nylon space suits and the hoses connecting them to the life support system. Grissom had removed his restraints and was lying on the floor of the spacecraft. White's restraints were burned through, and he was found lying sideways just below the hatch. It was determined that he had tried to open the hatch per the emergency procedure, but was not able to do so against the internal pressure. Chaffee was found strapped into his right-hand seat, as procedure called for him to maintain communication until White opened the hatch. Because of the large strands of melted nylon fusing the astronauts to the cabin interior, removing the bodies took nearly 90 minutes.
Investigation.
As a result of the in-flight failure of the Gemini 8 mission on March 17, 1966, NASA Deputy Administrator Robert Seamans wrote and implemented "Management Instruction 8621.1" on April 14, 1966, defining "Mission Failure Investigation Policy And Procedures". This modified NASA's existing accident procedures, based on military aircraft accident investigation, by giving the Deputy Administrator the option of performing independent investigations of major failures, beyond those for which the various Program Office officials were normally responsible. It declared, "It is NASA policy to investigate and document the causes of all major mission failures which occur in the conduct of its space and aeronautical activities and to take appropriate corrective actions as a result of the findings and recommendations."
Immediately after the Apollo 1 fire, Seamans directed establishment of the "Apollo 204 Review Board" chaired by Langley Research Center director Floyd L. Thompson, which included astronaut Frank Borman, spacecraft designer Maxime Faget, and six others. To avoid the possible appearance of a conflict of interest, NASA Administrator James E. Webb got the approval of President Lyndon B. Johnson for an internal NASA investigation, and notified appropriate leaders of Congress. According to Webb's official NASA bio:
Seamans immediately ordered all Apollo 1 hardware and software impounded, to be released only under control of the Board. On February 3, two members, a Cornell University professor and North American's Chief engineer for Apollo, left the Board, and a U.S. Bureau of Mines professor joined. After thorough stereo photographic documentation of the CM-012 interior, the board ordered its disassembly using procedures tested by disassembling the identical CM-014, and conducted a thorough investigation of every part. The board also reviewed the astronauts' autopsy results and interviewed witnesses. Seamans sent Webb weekly status reports of the investigation's progress, and the Board issued its final report on April 5, 1967.
According to the Board, Grissom suffered severe third degree burns on over one-third of his body and his spacesuit was mostly destroyed. White suffered third degree burns on almost half of his body and a quarter of his spacesuit had melted away. Chaffee suffered third degree burns over almost a quarter of his body and a small portion of his spacesuit was damaged. The autopsy report confirmed that the primary cause of death for all three astronauts was cardiac arrest caused by high concentrations of carbon monoxide. Burns suffered by the crew were not believed to be major factors, and it was concluded that most of them had occurred postmortem. Asphyxiation happened after the fire melted the astronauts' suits and oxygen tubes, exposing them to the lethal atmosphere of the cabin.
The review board identified five major factors which combined to cause the fire and the astronauts' deaths:
Ignition source.
The review board determined that the electrical power momentarily failed at 23:30:55 GMT, and found evidence of several electric arcs in the interior equipment. However, they were unable to conclusively identify a single ignition source. They determined that the fire most likely started near the floor in the lower left section of the cabin, close to the Environmental Control Unit. It spread from the left wall of the cabin to the right, with the floor being affected only briefly. The engulfed area on the left contained the manual depressurization valve which would have been used to vent the cabin atmosphere to the outside. Consequently, the astronauts were unable to reach it, however this was in any case insufficient to prevent heat and pressure buildup.
They noted a silver-plated copper wire running through an environmental control unit near the center couch had become stripped of its Teflon insulation and abraded by repeated opening and closing of a small access door. This weak point in the wiring also ran near a junction in an ethylene glycol/water cooling line which had been prone to leaks. The electrolysis of ethylene glycol solution with the silver anode was a notable hazard which could cause a violent exothermic reaction, igniting the ethylene glycol mixture in the CM's corrosive test atmosphere of pure, high-pressure oxygen.
In 1968, a team of MIT physicists went to Cape Kennedy and performed a static discharge test in the CM-103 Command Module while it was being prepared for the launch of Apollo 8. With an electroscope, they measured the approximate energy of static discharges caused by a test crew dressed in nylon flight pressure suits and reclining on the nylon flight seats. The MIT investigators repeatedly found sufficient energy for ignition was discharged when crew-members shifted in their seats and then touched the spacecraft's aluminum panels.
Flammable materials in the cabin.
The review board cited "many types and classes of combustible material" close to ignition sources. The NASA crew systems department had installed of Velcro throughout the spacecraft, almost like carpeting. This Velcro was found to be flammable in a high-pressure 100% oxygen environment. Up to 70 pounds of other non-metallic flammable materials had also crept into the design.
Buzz Aldrin states in his book "Men From Earth" that the flammable material had been removed (per the crew's August 19 complaints and Joseph Shea's order), but was replaced prior to the August 26 delivery to Cape Kennedy.
Pure oxygen atmosphere.
The plugs-out test had been run to simulate the launch procedure, with the cabin pressurized with pure oxygen at the nominal pre-launch level of , 2 psi above standard sea level atmospheric pressure. This is more than five times the 3 psi partial pressure of oxygen in the atmosphere, and provides an environment in which materials not normally considered highly flammable will burst into flame.
The high-pressure oxygen atmosphere was consistent with that used in the Mercury and Gemini programs. The pressure before launch was deliberately greater than ambient in order to drive out the nitrogen-containing air and replace it with pure oxygen. After liftoff, the pressure would have been reduced to the in-flight level of , providing sufficient oxygen for the astronauts to breathe while reducing the fire risk. The Apollo 1 crew had tested this procedure with their spacecraft in the Operations and Checkout Building altitude (vacuum) chamber on October 18 and 19, 1966, and the backup crew of Schirra, Eisele and Cunningham had repeated it on December 30.
When designing the Mercury spacecraft, NASA had considered using a nitrogen/oxygen mixture to reduce the fire risk near launch, but rejected it based on two considerations. First, nitrogen used with the in-flight pressure reduction carried the clear risk of decompression sickness (known as "the bends"). But the decision to eliminate the use of any gas but oxygen was crystalized when a serious accident occurred on April 21, 1960, in which McDonnell Aircraft test pilot G.B. North passed out and was seriously injured when testing a Mercury cabin / spacesuit atmosphere system in a vacuum chamber. The problem was found to be nitrogen-rich (oxygen-poor) air leaking from the cabin into his spacesuit feed. North American Aviation had suggested using an oxygen/nitrogen mixture for Apollo, but NASA overruled this. The pure oxygen design also carried the benefit of saving weight, by eliminating the need for nitrogen tanks.
In his monograph "Project Apollo: The Tough Decisions", Deputy Administrator Seamans wrote that NASA's single worst mistake in engineering judgment was not to run a fire test on the Command Module prior to the plugs-out test. In the first episode of the 2009 BBC documentary series "NASA: Triumph and Tragedy", Jim McDivitt said that NASA had no idea how a 100% oxygen atmosphere would influence burning. Similar remarks by other astronauts were expressed in the 2007 documentary film "In the Shadow of the Moon".
Other oxygen fires.
Several fires in high-oxygen environments had been known to occur prior to the Apollo fire. For example, in 1962, USAF Colonel B. Dean Smith was conducting a test of the Gemini space suit with a colleague in a pure oxygen chamber at Brooks Air Force Base in San Antonio, Texas when a fire broke out, destroying the chamber. Smith and his partner narrowly escaped.
Other oxygen fire occurrences are documented in certain U.S. reports archived in the National Air and Space Museum, such as: 
On January 28, 1986, the Soviet Union disclosed that cosmonaut Valentin Bondarenko died after a fire in a high-oxygen isolation chamber on March 23, 1961, less than three weeks before the first Vostok manned space flight. This revelation caused some speculation whether the Apollo 1 disaster might have been averted had NASA been aware of the incident.
Hatch design.
The higher than atmospheric cabin pressure made it impossible for the senior pilot to remove the inner hatch, until the excess cabin pressure (16.7 psi absolute, 2 psi above ambient) had been vented. Emergency procedure called for the command pilot to open the cabin vent first, but this was located near the origin of the fire, and while the system could easily vent the normal pressure, it was utterly incapable of handling the extra increase in pressure (to at least 29 psi absolute) caused by the fire.
North American had originally suggested the hatch open outward and use explosive bolts to blow the hatch in case of emergency, as had been done in Project Mercury. NASA did not agree, arguing the hatch could accidentally open, as it had on Grissom's "Liberty Bell 7" flight, so the inward-opening hatch was selected early in the Block I design.
Before the fire, the Apollo astronauts had recommended changing the design to an outward-opening hatch, and this was already slated for inclusion in the Block II Command Module design. According to Donald K. Slayton's testimony before the House investigation of the accident, this was based on ease of exit for spacewalks and at the end of flight, rather than for emergency exit.
Emergency preparedness.
The board noted that: the test planners had failed to identify the test as hazardous; the emergency equipment (such as gas masks) were inadequate to handle this type of fire; that fire, rescue, and medical teams were not in attendance; and that the spacecraft work and access areas contained many hindrances to emergency response such as steps, sliding doors, and sharp turns.
Political fallout.
Committees in both houses of the United States Congress with oversight of the space program soon launched investigations, including the Senate Committee on Aeronautical and Space Sciences, chaired by Senator Clinton P. Anderson. Seamans, Webb, Manned Space Flight Administrator Dr. George E. Mueller, and Apollo Program Director Maj Gen Samuel C. Phillips were called to testify before Sen. Anderson's committee.
In the February 27 hearing, Senator Walter F. Mondale asked Webb if he knew of a "report" of extraordinary problems with the performance of North American Aviation on the Apollo contract. Webb replied he did not, and deferred to his subordinates on the witness panel. Mueller and Phillips responded they too were unaware of any such "report".
However, in late 1965, just over a year before the accident, Phillips had headed a "tiger team" investigating the causes of inadequate quality, schedule delays, and cost overruns in both the Apollo CSM and the Saturn V second stage (for which North American was also prime contractor.) He gave an oral presentation (with transparencies) of his team's findings to Mueller and Seamans, and also presented them in a memo to North American president John L. Atwood, to which Mueller appended his own strongly worded memo to Atwood. 
During Mondale's 1967 questioning about what was to become known as the "Phillips Report", Seamans was afraid Mondale might actually have seen a hard copy of Phillips' presentation, and responded that contractors have occasionally been subjected to on-site progress reviews; perhaps this was what Mondale's information referred to. Mondale continued to refer to "the Report" despite Phillips' refusal to characterize it as such, and angered by what he perceived as Webb's deception and concealment of important program problems from Congress, he questioned NASA's selection of North American as prime contractor. Seamans later wrote that Webb roundly chastised him in the cab ride leaving the hearing, for volunteering information which led to the disclosure of Phillips' memo. 
On May 11, Webb issued a statement defending NASA's November 1961 selection of North American as the prime contractor for Apollo. This was followed on June 9 by Seamans filing a seven-page memorandum documenting the selection process. Webb eventually provided a controlled copy of Phillips' memo to Congress. The Senate committee noted in its final report NASA's testimony that "the findings of the [Phillips] task force had no effect on the accident, did not lead to the accident, and were not related to the accident", but stated in its recommendations: 
"Notwithstanding that in NASA's judgment the contractor later made significant progress in overcoming the problems, the committee believes it should have been informed of the situation. The committee does not object to the position of the Administrator of NASA, that all details of Government/contractor relationships should not be put in the public domain. However, that position in no way can be used as an argument for not bringing this or other serious situations to the attention of the committee."
Freshman Senators Edward W. Brooke III and Charles H. Percy jointly wrote an "Additional Views" section appended to the committee report, chastising NASA more strongly than Anderson for not having disclosed the Phillips review to Congress. Mondale wrote his own, even more strongly worded Additional View, accusing NASA of "evasiveness, ... lack of candor, ... patronizing attitude toward Congress, ... refusal to respond fully and forthrightly to legitimate Congressional inquiries, and ... solicitous concern for corporate sensitivities at a time of national tragedy."
The potential political threat to Apollo blew over, due in large part to the support of President Lyndon B. Johnson, who at the time still wielded a measure of influence with the Congress from his own Senatorial experience. He was a staunch supporter of NASA since its inception, had even recommended the Moon program to President John F. Kennedy in 1961, and was skilled at portraying it as part of Kennedy's legacy.
Internal acrimony developed between NASA and North American over assignment of blame. North American argued unsuccessfully it was not responsible for the fatal error in spacecraft atmosphere design. Finally, Webb contacted Atwood, and demanded either he or Chief Engineer Harrison A. Storms resign. Atwood elected to fire Storms.
On the NASA side, Joseph Shea became unfit for duty in the aftermath and was removed from his position, although not fired.
Program recovery.
Gene Kranz called a meeting of his staff in Mission Control three days after the accident, delivering a speech which has subsequently become one of NASA's principles. Speaking of the errors and overall attitude surrounding the Apollo program before the accident, he stated: "We were too 'gung-ho' about the schedule and we blocked out all of the problems we saw each day in our work. Every element of the program was in trouble and so were we." He reminded the team of the perils and mercilessness of their endeavor, and stated the new requirement that every member of every team in mission control be "tough and competent", requiring nothing less than perfection throughout NASA's programs. 36 years later, following the Space Shuttle "Columbia" disaster, then-NASA administrator Sean O'Keefe quoted Kranz's speech, adopting it in principle to honor the lives of Apollo 1's and Columbia's astronauts.
Command Module redesign.
After the fire, the Apollo program was grounded for review and redesign. The Command Module was found to be extremely hazardous and in some instances, carelessly assembled (for example, a misplaced socket wrench was found in the cabin).
It was decided that remaining Block I spacecraft would only be used for unmanned Saturn V test flights. All manned missions would use the Block II spacecraft, to which many Command Module design changes were made:
Thorough protocols were implemented for documenting spacecraft construction and maintenance.
New mission naming scheme.
The astronauts' widows asked that "Apollo 1" be reserved for the flight their husbands never made, and on April 24, 1967, Associate Administrator for Manned Space Flight, Dr. George E. Mueller, announced this change officially: AS-204 would be recorded as Apollo 1, "first manned Apollo Saturn flight â failed on ground test". Since three unmanned Apollo missions (AS-201, AS-202, and AS-203) had previously occurred, the next mission, the first unmanned Saturn V test flight (AS-501) would be designated Apollo 4, with all subsequent flights numbered sequentially in the order flown. The first three flights would not be renumbered, and the names "Apollo 2" and "Apollo 3" would go unused.
The manned flight hiatus allowed work to catch up on the Saturn V and Lunar Module, which were encountering their own delays. Apollo 4 flew in November 1967. Apollo 1's (AS-204) Saturn IB rocket was taken down from Launch Complex 34, later reassembled at Launch complex 37B and used to launch Apollo 5, an unmanned Earth orbital test flight of the first Lunar Module LM-1, in January 1968. A second unmanned Saturn V AS-502 flew as Apollo 6 in April 1968, and Grissom's backup crew of Wally Schirra, Don Eisele, and Walter Cunningham, finally flew the orbital test mission as Apollo 7 (AS-205), in a Block II CSM in October 1968.
Memorials.
Gus Grissom and Roger Chaffee were buried at Arlington National Cemetery. Ed White was buried at West Point Cemetery on the grounds of the United States Military Academy in West Point, New York.
Their names are among those of several astronauts and cosmonauts who have died in the line of duty, listed on the Space Mirror Memorial at the Kennedy Space Center Visitor Complex in Merritt Island, Florida.
An Apollo 1 mission patch was left on the Moon's surface after the first manned lunar landing by Apollo 11 crew members Neil Armstrong and Buzz Aldrin.
The Apollo 15 mission left on the surface of the Moon a tiny memorial statue, "Fallen Astronaut", along with a plaque containing the names of the Apollo 1 astronauts, among others including Soviet cosmonauts, who perished in the pursuit of human space flight.
Launch Complex 34.
After the Apollo 1 fire, Launch Complex 34 was subsequently used only for the launch of Apollo 7 and later dismantled down to the concrete launch pedestal, which remains at the site () along with a few other concrete and steel-reinforced structures. The pedestal bears two plaques commemorating the crew. Each year the families of the Apollo 1 crew are invited to the site for a memorial, and the Kennedy Space Center Visitor Center includes the site in its tour of the historic Cape Canaveral launch sites.
In January 2005, three granite benches, built by a college classmate of one of the astronauts, were installed at the site on the southern edge of the launch pad. Each bears the name of one of the astronauts and his military service insignia.
Remains of CM-012.
The Apollo 1 Command Module has never been on public display. After the accident, the spacecraft was removed and taken to Kennedy Space Center to facilitate the review board's disassembly in order to investigate the cause of the fire. When the investigation was complete, it was moved to the NASA Langley Research Center in Hampton, Virginia, and placed in a secured storage warehouse.
On February 17, 2007, the parts of CM-012 were moved approximately to a newer, environmentally controlled warehouse. Only a few weeks earlier, Gus Grissom's brother Lowell publicly suggested CM-012 be permanently entombed in the concrete remains of Launch Complex 34.

</doc>
<doc id="1966" url="http://en.wikipedia.org/wiki?curid=1966" title="Apollo 10">
Apollo 10

Apollo 10 was the fourth manned mission in the United States Apollo space program. It was an F type mission â its purpose was to be a dress rehearsal for the Apollo 11 mission, testing all of the procedures and components of a Moon landing without actually landing on the Moon itself. The mission included the second crew to orbit the Moon and an all-up test of the Lunar Module (LM) in lunar orbit. The LM came to within of the lunar surface during practice maneuvers.
According to the 2002 "Guinness World Records", Apollo 10 set the record for the highest speed attained by a manned vehicle at 39,897Â km/h (11.08Â km/s or 24,791Â mph) during the return from the Moon on May 26, 1969.
Due to the use of their names as call signs, the "Peanuts" characters Charlie Brown and Snoopy became semi-official mascots for the mission. "Peanuts" creator Charles Schulz also drew some special mission-related artwork for NASA.
Crew.
Crew notes.
Apollo 10 was the first of only two Apollo missions with an entirely flight-experienced crew. Thomas P. Stafford had flown on Gemini 6 and Gemini 9; John W. Young had flown on Gemini 3 and Gemini 10, and Eugene A. Cernan had flown with Stafford on Gemini 9.
In addition, Apollo 10 marked the only Saturn V flight from Launch Complex 39B, as preparations for Apollo 11 at LC-39A had begun in March almost immediately after Apollo 9's launch.
They were also the only Apollo crew all of whose members went on to fly subsequent missions aboard Apollo spacecraft: Young later commanded Apollo 16, Cernan commanded Apollo 17 and Stafford commanded the US vehicle on the ApolloâSoyuz Test Project.
The Apollo 10 crew holds the distinction of being the humans who have traveled to the farthest point away from home, some from their homes and families in Houston. While most Apollo missions orbited the Moon at the same from the lunar surface, timing makes this distinction possible as the distance between the Earth and Moon varies by approximately (between perigee and apogee) throughout the year, and the Earth's rotation make the distance to Houston vary by another each day. The Apollo 10 crew reached the farthest point in their orbit around the far side of the Moon at approximately the same time Earth had rotated around putting Houston nearly a full Earth diameter away. The Apollo 13 crew holds the distinction of being the farthest any human has traveled from the Earth's surface.
By the normal rotation in place during Apollo, the backup crew would have been scheduled to fly on Apollo 13. However, Alan Shepard was given the Apollo 13 command slot instead. L. Gordon Cooper, Jr., Commander of the Apollo 10 backup crew, was enraged and resigned from NASA. Later, Shepard's crew was forced to switch places with Jim Lovell's tentative Apollo 14 crew.
Deke Slayton wrote in his memoirs that Cooper and Donn F. Eisele were never intended to rotate to another mission as both were out of favor with NASA management for various reasons (Cooper for his lax attitude towards training and Eisele for incidents aboard Apollo 7 and an extramarital affair) and were assigned to the backup crew simply because of a lack of qualified manpower in the Astronaut Office at the time the assignment needed to be made. Cooper, Slayton noted, had a very small chance of receiving the Apollo 13 command if he did an outstanding job with the assignment, which he did not. Eisele, despite his issues with management, was always intended for future assignment to the Apollo Applications Program (which was eventually cut down to only the Skylab component) and not a lunar mission.
Mission parameters.
LM closest approach to lunar surface.
On May 22, 1969 at 20:35:02 UTC, a 27.4 second LM descent propulsion system burn inserted the LM into a descent orbit of so that the resulting lowest point in the orbit occurred about 15Â° from lunar landing site 2 (the Apollo 11 landing site). The lowest measured point in the trajectory was above the lunar surface at 21:29:43 UTC.
Mission highlights.
This dress rehearsal for a Moon landing brought the Apollo Lunar Module to from the lunar surface, at the point where powered descent would begin on the actual landing. Practicing this approach orbit would refine knowledge of the lunar gravitational field needed to calibrate the powered descent guidance system to within (LR altitude update lock) needed for a landing. Earth-based observations, unmanned spacecraft, and Apollo 8 had respectively allowed calibration to within , , and . Except for this final stretch, the mission went exactly as a landing would have gone, both in space and on the ground, putting NASA's flight controllers and extensive tracking and control network through a rehearsal.
Shortly after trans-lunar injection, the Command/Service Module (CSM) separated from the S-IVB stage, turned around, and docked its nose to the top of the Lunar Module (LM) still nestled in the S-IVB. The CSM/LM stack then separated from the S-IVB for the trip to the Moon.
Apollo 10 was the first mission to carry a color television camera inside the spacecraft, and made the first live color TV transmissions from space.
Upon reaching lunar orbit, Young remained alone in the Command Module (CM) "Charlie Brown" while Stafford and Cernan flew separately in the LM "Snoopy". The LM crew demonstrated their craft's radar and engines, rode out a momentary gyration in the lunar lander's motion (due to a faulty switch setting), and surveyed the Apollo 11 landing site in the Sea of Tranquility. The ascent stage was loaded with the amount of fuel it would have had remaining if it had lifted off from the surface and reached the altitude at which the Apollo 10 ascent stage fired. The fueled LM weighed , compared to for the Apollo 11 LM which made the first landing. Historian Craig Nelson wrote that NASA took special precaution to ensure Stafford and Cernan would not attempt to make the first landing. Nelson quoted Cernan as saying "A lot of people thought about the kind of people we were: 'Don't give those guys an opportunity to land, 'cause they might!' So the ascent module, the part we lifted off the lunar surface with, was short-fueled. The fuel tanks weren't full. So had we literally tried to land on the Moon, we couldn't have gotten off." In his own memoir, Cernan wrote "Our lander, LM-4...was still too heavy to guarantee safe margins for a moon landing."
Upon separation of the descent stage and ascent engine ignition, the Lunar Module began to roll violently due to the crew accidentally duplicating commands into the flight computer which took the LM out of abort mode, the correct configuration for this maneuver. The live network broadcasts caught Cernan and Stafford uttering several expletives before regaining control of the LM. Cernan has said he observed the horizon spinning eight times over, indicating eight rolls of the spacecraft under ascent engine power. While the incident was downplayed by NASA, the roll was just several revolutions from being unrecoverable, which would have resulted in the LM crashing into the lunar surface.
Splashdown occurred in the Pacific Ocean on May 26, 1969 at 16:52:23 UTC, approximately east of American Samoa. The astronauts were recovered by the USS "Princeton", and subsequently flown to Pago Pago International Airport in Tafuna for a greeting reception, before being flown on a C-141 cargo plane to Honolulu.
Hardware disposition.
The LM "Snoopy"'s descent stage was left in orbit, but eventually crashed onto the lunar surface because of the Moon's non-uniform gravitational field; its location was not tracked.
After being jettisoned, "Snoopy's" ascent stage flew on a trajectory past the Moon into a heliocentric orbit, 
making it the sole intact Lunar Module ascent stage remaining of the 10 LMs sent into space. All other ascent stages were either left in lunar orbit to eventually crash, intentionally steered into the Moon to obtain readings from seismometers placed on the lunar surface, or else burned up in Earth's atmosphere. 
"Snoopy"'s ascent stage location is currently unknown, and amateur astronomers are searching for it.
The Command Module "Charlie Brown" is currently on loan to the Science Museum in London, where it is on display. "Charlie Brown"'s Service Module (SM) was jettisoned just before re-entry and burned up in the Earth's atmosphere.
After Apollo 10, NASA required astronauts to choose more "dignified" names for their command and lunar module. The requirement was unenforceable: Apollo 16 astronauts Young, Mattingly and Duke chose "Casper", as in Casper the Friendly Ghost, for their Command Module name. The idea was to give children a way to identify with the mission by using humor.
After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object where it would continue to orbit the Sun for many years. , it remains in orbit.
Mission insignia.
The shield-shaped emblem for the flight shows a large, three-dimensional Roman numeral X sitting on the Moon's surface, in Stafford's words, "to show that we had left our mark." Although it did not land on the Moon, the prominence of the number represents the significant contributions the mission made to the Apollo program. A CSM circles the Moon as an LM ascent stage flies up from its low pass over the lunar surface with its engine firing. The earth is visible in the background. A wide, light blue border carries the word APOLLO at the top and the crew names around the bottom. The patch is trimmed in gold. The insignia was designed by Allen Stevens of Rockwell International.
External links.
NASA reports
Multimedia

</doc>
<doc id="1967" url="http://en.wikipedia.org/wiki?curid=1967" title="Apollo 12">
Apollo 12

Apollo 12 was the sixth manned flight in the United States Apollo program and the second to land on the Moon (an H type mission). It was launched on November 14, 1969 from the Kennedy Space Center, Florida, four months after Apollo 11. Mission commander Charles "Pete" Conrad and Lunar Module Pilot Alan L. Bean performed just over one day and seven hours of lunar surface activity while Command Module Pilot Richard F. Gordon remained in lunar orbit. The landing site for the mission was located in the southeastern portion of the Ocean of Storms.
Unlike the first landing on Apollo 11, Conrad and Bean achieved a precise landing at their expected location, the site of the Surveyor 3 unmanned probe, which had landed on April 20, 1967. They carried the first color television camera to the lunar surface on an Apollo flight, but transmission was lost after Bean accidentally destroyed the camera by pointing it at the Sun. On one of two moonwalks, they visited the Surveyor and removed some parts for return to Earth. The mission ended on November 24 with a successful splashdown.
Mission highlights.
Launch and transfer.
Apollo 12 launched on schedule from Kennedy Space Center, during a rainstorm. It was the first rocket launch attended by an incumbent US president, Richard Nixon. Thirty-six-and-a-half seconds after lift-off, the vehicle triggered a lightning discharge through itself and down to the earth through the Saturn's ionized plume. Protective circuits on the fuel cells in the Service Module (SM) falsely detected overloads and took all three fuel cells offline, along with much of the Command/Service Module (CSM) instrumentation. A second strike at 52 seconds after launch knocked out the "8-ball" attitude indicator. The telemetry stream at Mission Control was garbled. However, the vehicle continued to fly correctly; the strikes had not affected the Saturn V Instrument Unit.
The loss of all three fuel cells put the CSM entirely on batteries, which were unable to maintain normal 75-ampere launch loads on the 28-volt DC bus. One of the AC inverters dropped offline. These power supply problems lit nearly every warning light on the control panel and caused much of the instrumentation to malfunction.
Electrical, Environmental and Consumables Manager (EECOM) John Aaron remembered the telemetry failure pattern from an earlier test when a power supply malfunctioned in the CSM Signal Conditioning Equipment (SCE), which converted raw signals from instrumentation to standard voltages for the spacecraft instrument displays and telemetry encoders.
Aaron made a call, "Try SCE to aux," which switched the SCE to a backup power supply. The switch was fairly obscure, and neither Flight Director Gerald Griffin, CAPCOM Gerald Carr, nor Commander Conrad immediately recognized it. Lunar Module Pilot Alan Bean, flying in the right seat as the spacecraft systems engineer, remembered the SCE switch from a training incident a year earlier when the same failure had been simulated. Aaron's quick thinking and Bean's memory saved what could have been an aborted mission, and earned Aaron the reputation of a "steely-eyed missile man". Bean put the fuel cells back on line, and with telemetry restored, the launch continued successfully. Once in earth parking orbit, the crew carefully checked out their spacecraft before re-igniting the S-IVB third stage for trans-lunar injection. The lightning strikes had caused no serious permanent damage.
Initially, it was feared that the lightning strike could have caused the Command Module's (CM) parachute mechanism to prematurely fire, disabling the explosive bolts that open the parachute compartment to deploy them. If they were indeed disabled, the Command Module would have crashed uncontrollably into the Pacific Ocean and killed the crew instantly. Since there was no way to figure out whether or not this was the case, ground controllers decided not to tell the astronauts about the possibility. The parachutes deployed and functioned normally at the end of the mission.
After Lunar Module (LM) separation, the S-IVB was intended to fly into solar orbit. The S-IVB auxiliary propulsion system was fired, and the remaining propellants vented to slow it down to fly past the Moon's trailing edge (the Apollo spacecraft always approached the Moon's leading edge). The Moon's gravity would then slingshot the stage into solar orbit. However, a small error in the state vector in the Saturn's guidance system caused the S-IVB to fly past the Moon at too high an altitude to achieve Earth escape velocity. It remained in a semi-stable Earth orbit after passing the Moon on November 18, 1969. It finally escaped Earth orbit in 1971 but was briefly recaptured in Earth orbit 31 years later. It was discovered by amateur astronomer Bill Yeung who gave it the temporary designation J002E3 before it was determined to be an artificial object.
Landing.
The Apollo 12 mission landed on November 19, 1969, on an area of the Ocean of Storms that had been visited earlier by several unmanned missions (Luna 5, Surveyor 3, and Ranger 7). The International Astronomical Union, recognizing this, christened this region "Mare Cognitum (Known Sea)". The Lunar coordinates of the landing site were 3.01239Â° S latitude, 23.42157Â° W longitude. The landing site would thereafter be listed as "Statio Cognitum" on lunar maps. Conrad and Bean did not formally name their landing site, though Conrad nicknamed the intended touchdown area "Pete's Parking Lot".
The second lunar landing was an exercise in precision targeting, which would be needed for future Apollo missions. Most of the descent was automatic, with manual control assumed by Conrad during the final few hundred feet of descent. Unlike Apollo 11, where Neil Armstrong had to use the manual control to direct his lander downrange of the computer's target which was strewn with boulders, Apollo 12 succeeded in landing at its intended target - within walking distance of the Surveyor 3 probe, which had landed on the Moon in April 1967. This was the first â and, to date, only â occasion in which humans have "caught up" to a probe sent to land on another world.
Conrad actually landed "Intrepid" short of "Pete's Parking Lot", because it looked rougher during final approach than anticipated, and was a little under from Surveyor 3, a distance that was chosen to eliminate the possibility of lunar dust (being kicked up by "Intrepid's" descent engine during landing) from covering Surveyor 3. But the actual touchdown point â approximately from Surveyor 3 â did cause high velocity sandblasting of the probe. It was later determined that the sandblasting removed more dust than it delivered onto the Surveyor, because the probe was covered by a thin layer that gave it a tan hue as observed by the astronauts, and every portion of the surface exposed to the direct sandblasting was lightened back toward the original white color through the removal of lunar dust.
EVAs.
When Conrad, who was somewhat shorter than Neil Armstrong, stepped onto the lunar surface, his first words were "Whoopie! Man, that may have been a small one for Neil, but that's a long one for me." This was not an off-the-cuff remark: Conrad had made a bet with reporter Oriana Fallaci he would say these words, after she had queried whether NASA had instructed Neil Armstrong what to say as he stepped onto the Moon. Conrad later said he was never able to collect the money.
To improve the quality of television pictures from the Moon, a color camera was carried on Apollo 12 (unlike the monochrome camera that was used on Apollo 11). Unfortunately, when Bean carried the camera to the place near the Lunar Module where it was to be set up, he inadvertently pointed it directly into the Sun, destroying the SEC tube. Television coverage of this mission was thus terminated almost immediately. See also: Apollo TV camera.
Apollo 12 successfully landed within walking distance of the Surveyor 3 probe. Conrad and Bean removed pieces of the probe to be taken back to Earth for analysis. It is claimed that the common bacterium "Streptococcus mitis" was found to have accidentally contaminated the spacecraft's camera prior to launch and survived dormant in this harsh environment for two and a half years. However, this finding has since been disputed: see Reports of Streptococcus mitis on the Moon.
Astronauts Conrad and Bean also collected rocks and set up equipment that took measurements of the Moon's seismicity, solar wind flux and magnetic field, and relayed the measurements to Earth. The instruments were part of the first complete nuclear-powered ALSEP station set up by astronauts on the Moon to relay long-term data from the lunar surface. The instruments on Apollo 11 were not as extensive or designed to operate long term. The astronauts also took photographs, although by accident Bean left several rolls of exposed film on the lunar surface. Meanwhile Gordon, on board the "Yankee Clipper" in lunar orbit, took multi-spectral photographs of the surface.
The lunar plaque attached to the descent stage of "Intrepid" is unique in that unlike the other plaques, it (a) did not have a depiction of the Earth, and (b) it was textured differently (the other plaques had black lettering on polished stainless steel while the Apollo 12 plaque had the lettering in polished stainless steel while the background was brushed flat).
Return.
"Intrepid's" ascent stage was dropped (per normal procedures) after Conrad and Bean rejoined Gordon in orbit. It impacted the Moon on November 20, 1969, at . The seismometers the astronauts had left on the lunar surface registered the vibrations for more than an hour.
The crew stayed an extra day in lunar orbit taking photographs, for a total lunar surface stay of 31 and a half hours and a total time in lunar orbit of eighty-nine hours.
On the return flight to Earth after leaving lunar orbit, the crew of Apollo 12 witnessed (and photographed) a solar eclipse, though this one was of the Earth eclipsing the Sun.
Splashdown.
"Yankee Clipper" returned to Earth on November 24, 1969 at 20:58 UTC (3:58pm EST, 10:58am HST), in the Pacific Ocean, approximately 500 nautical miles (800Â km) east of American Samoa. During splashdown, a 16 mm film camera dislodged from storage and struck Bean in the forehead, rendering him briefly unconscious. He suffered a mild concussion and needed six stitches. After recovery by the USS "Hornet", they were flown to Pago Pago International Airport in Tafuna for a reception, before being flown on a C-141 cargo plane to Honolulu.
Mission insignia.
The Apollo 12 mission patch shows the crew's navy background; all three astronauts at the time of the mission were U.S. Navy commanders. It features a clipper ship arriving at the Moon, representing the Command Module "Yankee Clipper". The ship trails fire, and flies the flag of the United States. The mission name APOLLO XII and the crew names are on a wide gold border, with a small blue trim. Blue and gold are traditional U.S. Navy colors. The patch has four stars on it â one each for the three astronauts who flew the mission and one for Clifton Williams, a U.S. naval aviator and astronaut who was killed on October 5, 1967, after a mechanical failure caused the controls of his T-38 trainer to stop responding and crash. He trained with Conrad and Gordon as part of the backup crew for what would be the Apollo 9 mission, and would have been assigned as Lunar Module Pilot for Apollo 12.
Spacecraft location.
The Apollo 12 Command Module "Yankee Clipper" is on display at the Virginia Air and Space Center in Hampton, Virginia.
In 2002, astronomers thought they might have discovered another moon orbiting Earth, which they designated J002E3, that turned out to be the third stage of the Apollo 12 Saturn V rocket.
The Lunar Module "Intrepid" impacted the Moon November 20, 1969 at 22:17:17.7 UT (5:17 PM EST) . In 2009, the Lunar Reconnaissance Orbiter (LRO) photographed the Apollo 12 landing site. The "Intrepid" Lunar Module descent stage, experiment package (ALSEP), Surveyor 3 spacecraft, and astronaut footpaths are all visible. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
Depiction in media.
Portions of the Apollo 12 mission are dramatized in the miniseries "From the Earth to the Moon" episode entitled "That's All There Is". Conrad, Gordon, and Bean were portrayed by Paul McCrane, Tom Verica, and Dave Foley, respectively. Conrad had been portrayed by a different actor, Peter Scolari, in the first episode.
External links.
NASA reports
Multimedia

</doc>
