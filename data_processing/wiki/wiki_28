<doc id="4472" url="http://en.wikipedia.org/wiki?curid=4472" title="The Big O">
The Big O

 is a Japanese animated television series created by designer Keiichi Sato and director Kazuyoshi Katayama, and written by Chiaki J. Konaka for Sunrise. The writing staff was assembled by the series' head writer, Chiaki J. Konaka who is known for his work in the "Digimon" season, "Digimon Tamers".
The story takes place forty years after a mysterious occurrence causes the residents of Paradigm City to lose their memories. The series follows Roger Smith, Paradigm City's top Negotiator. He provides this much needed service with the help of a robot named R. Dorothy Wayneright and his butler Norman Burg. When the need arises, Roger calls upon Big O, a giant relic from the city's past.
The television series is designed as a tribute to Japanese and Western shows from the 1960s and 1970s. The series is done in the style of "film noir" and combines the feel of a detective show with the mecha genre of anime. The setpieces are reminiscent of tokusatsu productions of the 1950s and 1960s, particularly Toho's kaiju movies, and the score is an eclectic mix of styles and musical homages.
"The Big O" premiered October 13, 1999 on WOWOW satellite television. It finished its run on January 19, 2000. The English-language version premiered on Cartoon Network on April 2, 2001 and ended on April 18 2001. Originally a 26 episode series, low viewership in Japan cut it down to just the first 13. However, positive fan response internationally resulted in a second season consisting of the remaining 13 episodes and co-produced by Cartoon Network, Sunrise, and Bandai Visual. Season two premiered on Japan's SUN-TV on January 2, 2003, with the American premiere taking place seven months later. Following the 2012 closure of Bandai Entertainment, Sunrise announced at Otakon 2013, that Sentai Filmworks has rescued both seasons of "The Big O".
Plot.
Setting.
The story of "The Big O" sets in a fictional city-state known as . The city is located on a sea coast and surrounded by a vast desert wasteland, the partially domed city is wholly controlled by the monopolistic Paradigm Corporation. Paradigm is known as , as forty years prior to the story, an unknown occurrence known only as takes place which destroys the world outside the city and leaves the survivors without any memories. In the final episodes of the series, the city is implied to have been an elaborate fabrication produced by an unknown power.
The city is characterized by severe class inequity; the higher-income population resides inside the more pleasant geodesic domes, with the remainder left in tenements outside. Androids coexist with the human inhabitants of Paradigm City; while their numbers are fairly low, and they're something of a rarity, there are enough of them that denizens of the city are not shocked by them and don't consider it particularly unusual to encounter one.
Several episodes show inhabitants of Paradigm City practicing some shape or form of Christianity, as people congregate in meeting places with crucifixes prominently displayed. However, the practice appears to be based on custom rather than doctrine, which no one remembers. A cathedral is shown in ruins and forgotten, although some elderly people occasionally feel compelled to stand in front of it and sing scraps of hymns. It is revealed in episode 11 that almost no one remembers Christmas and its meaning save for Alex Rosewater.
A holiday is observed on December 25, but as a celebration to commemorate the founding of Paradigm City, known as "Heaven's Day." The inhabitants of the city still put up generic Christmas decorations like decorated Christmas trees and streamers, but they don't really know the underlying reason behind all of this. Alex Rosewater seems to be the series' only character with knowledge of pre-Event Christianity. Dastun at one point mentions that Rosewater had in his possession fragments of a "Book of Revelation", although neither Dastun nor Roger had previously heard of it. It is possible that Rosewater also has other fragments of The Bible, as Rosewater describes the real meaning of Heaven's Day as being "the day God's son was born."
Story.
Forty years prior to the events of the series, disaster struck. The world was turned into a vast desert wasteland and the survivors were left without memories. The story takes place in Paradigm City, a corporate police state run by the Paradigm Corporation. The town is recognized for its geodesic domes, giant structures that house the richer citizens and segregate the poor.
"The Big O" deals with the nature of memories. A memory is a record stored in the brain of an organism, but in Paradigm City memories can mean much more. embody the lost knowledge of its residents, and can take the form of records from before the Event, forgotten artifacts from the previous era, or manifest themselves as recollection, hallucinations, and recurring dreams.
The first half of the series is episodic. Each Act revolves around different citizens of Paradigm dealing with the resurgence of lost Memories and how they manage to go on living without knowledge of what did or did not happen. The final episodes introduce elements that come into play during season two like the existence of people outside of Paradigm City, the nature of the Cataclysm that destroyed the world and the "Power of God wielded by the hand of man."
The second season takes an arc-based approach. Instead of self-contained stories like in season one, season two features a continuous storyline. The second season makes Alex Rosewater, CEO of the Paradigm Corporation, a direct antagonist to The Negotiator and introduces The Union, agents of a foreign power working within Paradigm.
Ending.
The series ends with the awakening of another "Big" megadeus and the revelation that the world appears to be a simulated reality constructed in the form of a giant mechanical stage which was created by Angel. A climactic battle ensues between Big O and Big Fau, after which the city and everything surrounding it begins to be systematically erased by the new Big megadeus (an embodiment of Angel). The Big, referred to as "Big Venus" by Dorothy, walks towards Big O, erasing everything it passes, including Big Fau with Alex Rosewater. Roger delivers a final speech to Angel telling her to "let go of the past" whether it is real or not and focus only on the present and the future. In what appears to be a TV Director's control room, the real Angel, who had been observing the fight through camera as well as reminding of herself of her other encounters with Roger on all of the screens surrounding her, watches as Roger tells her to 'live as a human being'. On the control panel is "Metropolis" (a book which had been featured prominently since the thirteenth episode and said to have been published by Gordon Rosewater, containing the information of what happened forty years ago (which Gordon himself dismisses as 'a lie')) and on its cover is an illustration of angel wings and the author's name, 'Angel Rosewater'. Angel then sheds a tear and turns to see Roger, as he puts his hand on her shoulder reassuringly, and Dorothy who is standing next to him states Roger's profession, "Roger the Negotiator". At that moment, Big Venus finally reaches Big O and the two walk into each other, subsequently "resetting" the world. The screen is then blinded with a white flash and then cuts to the first few scenes of the first episode, even including the opening text. New versions of Dorothy and Angel are then seen as they watch Roger drive down the street and he begins to hesitantly speak the first lines of the series "My name is... Roger Smith. I perform a much needed job here... in the City of Amnesia." The ending title card "We have come to terms" appears, the credits roll and (like the final episode of the first season) there is no preview. This ending has been thoroughly debated, however, as many fans feel it was simply symbolism and that the true nature of the ending remains open to interpretation.
Characters.
Roger Smith is the series's protagonist. As a Negotiator, his job entails finding a resolution for the troubles of the City of Amnesia. He'd negotiate almost anything for anyone, but he is a professional and expects the parties involved to behave professionally. When memories betray the people and force them to reawaken monstrosities of the city's past, Roger's only option is to fight back with a monstrosity of his own, the black megadeus Big O. Roger is voiced by Mitsuru Miyamoto in Japanese and Steven Blum in English.
R. Dorothy Wayneright is Roger's assistant. Introduced in Act:01 as Dorothy Soldano, daughter of rich industrialist Miguel Soldano, she is later revealed to be an android constructed by him. Her actual "father" would be Timothy Wayneright, the man who commissioned her construction and father of the real and late Dorothy Wayneright. To show her gratitude, and as a form of payment for Roger's help, she decides to move in with him and help out Norman with the chores. Dorothy is voiced by Akiko Yajima in Japanese and Lia Sargent in English. The character's name is consistent with naming practices in the science fiction works of Isaac Asimov, the first initial "R" standing for "robot".
Norman Burg is Roger's butler. Forty years before the commencement of the story, Norman, like the rest of Paradigm, lost all memories from before that day, but he would not think twice before going once more unto the breach for his master. Resourceful and talented, he is also caretaker of the Big O. Norman's skills give him a purpose and a mission to accomplish for Roger. Norman is voiced by Motomu Kiyokawa in Japanese and Milton James and Alan Oppenheimer in English for seasons one and two respectively.
Dan Dastun is the middle-aged Chief of the Military Police, introduced in Act:01. He is Roger's former commander and the Negotiator's contact in the police force. Roger describes him as "a hard-nosed cop [who is] completely devoted to the force" and with "more pride in the Military Police than anything else." Dan is voiced by Tesshou Genda in Japanese and Peter Lurie in English.
Angel is the beautiful woman Roger encounters throughout the series. Introduced in Act:03 as Casey Jenkins, investigator for Paradigm Power Management, then again in Act:04 as Patricia Lovejoy, secretary for the publisher of Paradigm Press. Angel's true identity is a mystery, her motives questionable and her allegiance to no one but herself. Angel is voiced by Emi Shinohara in Japanese and Wendee Lee in English.
Production and release.
Development of the retro-styled series began in 1996. Keiichi Sato came up with the concept of "The Big O": a giant city-smashing robot, piloted by a man in black, in a Gotham-like environment. He later met up with Kazuyoshi Katayama, who had just finished directing "Those Who Hunt Elves", and started work on the layouts and character designs. But when things "were about to really start moving," production on Katayama's "Sentimental Journey" began, putting plans on-hold. Meanwhile, Sato was heavily involved with his work on "City Hunter".
Sato admits it all started as "a gimmick for a toy" but the representatives at Bandai Hobby Division did not see the same potential. From there on, the dealings would be with Bandai Visual, but Sunrise still needed some safeguards and requested more robots be designed to increase prospective toy sales. In 1999, with the designs complete, Chiaki J. Konaka was brought on as head writer. Among other things, Konaka came up with the idea of "a town without memory" and his writing staff put together the outline for a 26-episodes series.
"The Big O" premiered on 13 October 1999 on WOWOW. When the production staff was informed the series would be shortened to 13 episodes, the writers decided to end it with a cliffhanger, hoping the next 13 episodes would be picked up. In April 2001, "The Big O" premiered on Cartoon Network's Toonami lineup. In Canada, the English dub of both seasons aired as part of Anime Current, an Anime Television block, on pay-TV digital channel G4techTV Canada from March 30, 2007 to May 4, 2007.
The series garnered positive fan response internationally that resulted in a second season co-produced by Cartoon Network, Sunrise, and Bandai Visual. Season two premiered on Japan's SUN-TV on January 2003, with the American premiere taking place seven months later as an Adult Swim exclusive. The second season would not be seen on Toonami until July 27, 2013, 10 years after it began airing on Adult Swim.
The second season was scripted by Chiaki Konaka with input from the American producers. Along with the 13 episodes of season two, Cartoon Network had an option for 26 additional episodes to be written by Konaka, but according to Jason DeMarco, executive producer for season two, the middling ratings and DVD sales in the United States and Japan made any further episodes impossible to be produced.
Music.
"The Big O" was scored by "Geidai" alumnus Toshihiko Sahashi. His composition is richly symphonic and classical, with a number of pieces delving into electronica and jazz. Chosen because of his "frightening amount of musical knowledge about TV dramas overseas," Sahashi integrates musical homages into the soundtrack. The background music draws from "film noir", spy films and sci-fi television series like "The Twilight Zone". The battle themes are reminiscent of Akira Ifukube's compositions for the "Godzilla" series.
The first opening theme is the Queen-influenced "Big-O!". Composed, arranged and performed by Rui Nagai, the song resembles the theme to the "Flash Gordon" film. The second opening theme is "Respect," composed by Sahashi. The track is an homage to the music of Gerry Anderson's "UFO", composed by Barry Gray. In 2007, Rui Nagai composed "Big-O! Show Must Go On," a 1960s hard rock piece, for Animax's reruns of the show. The closing theme is the slow love ballad "And Forever," written by Chie and composed by Ken Shima. The duet is performed by Robbie Danzie and Naoki Takao.
Along with Sahashi's original compositions, the soundtrack features Chopin's Prelude No. 15 and a jazz saxophone rendition of "Jingle Bells." The complete score was released in two volumes by Victor Entertainment.
Publications.
"The Big O" was conceived as a media franchise. To this effect, Sunrise requested a manga be produced along with the animated series. "The Big O" manga started serialization in Kodansha's "Magazine Z" on July 1999, three months before the anime premiere. Authored by Hitoshi Ariga, the manga uses Keiichi Sato's concept designs in an all-new story. The series ended on October 2001. The issues were later collected in six volumes. The English version of the manga is published by Viz Media.
In anticipation to the broadcast of the second season, a new manga series was published. , authored by Hitoshi Ariga. "Lost Memory" takes place between volumes five and six of the original manga. The issues were serialized in "Magazine Z" from November 2002 to September 2003 and were collected in two volumes.
, a novel by Yuki Taniguchi, was released 16 July 2003 by Tokuma Shoten.
Design.
"The Big O" is the brainchild of Keiichi Sato and Kazuyoshi Katayama, an homage to the shows they grew up with. The show references the works of the superhero shows produced by the Toei Company and "old school" super robots. The series is done in the style of "film noir" and pulp fiction and combines the feel of a detective show with the giant robot genre.
Style.
"Film noir" is a stylistic approach to genre films forged in Depression-era detective and gangster films and hard-boiled detective stories which were a staple of pulp fiction. "The Big O" shares much of its themes, diction, archetypes and visual iconography with "film noirs" of the 1940s like "The Big Sleep" (1946).
Low-key lighting schemes mark most "noirs". The series incorporates the use of long dark shadows in the tradition of "chiaroscuro" and tenebrism. "Film noir" is also known for its use of odd angles, such as Roger's low shot introduction in the first episode. "Noir" cinematographers favoured this angle because it made characters almost rise from the ground, giving them dramatic girth and symbolic overtones. Other disorientating devices like dutch angles, mirror reflection and distorting shots are employed throughout the series.
The characters of "The Big O" fit the "noir" and pulp fiction archetypes. Roger Smith is a protagonist in the mold of Chandler's Philip Marlowe or Hammett's Sam Spade. He is canny and cynical, a disillusioned cop-turned-negotiator whose job has more in common with detective-style work than negotiating. Big Ear is Roger's street informant and Dan Dastun is the friend on the police force. The recurring Beck is the imaginative thug compelled by delusions of grandeur while Angel fills the role of the "femme fatale". Minor characters include crooked cops, corrupt business men and deranged scientists.
"Noir" characters often wisecrack and speak in double entendres. The dialogue in the series is recognized for its witty, wry sense of humor. The characters come off as charming and exchange banter not often heard in anime series, as the dialogue has the tendency to be straightforward. The plot is moved along by Roger's voice-over narration, a device used in "film noir" to place the viewer in the mind of the protagonist so it can intimately experience the character's angst and partly identify with the narrator.
The urban landscape, Paradigm City, is the perfect "noir" milieu. The tall buildings and giant domes create a sense of claustrophobia and paranoia characteristic of the style. The rural landscape, Ailesberry Farm, contrasts Paradigm City. "Noir" protagonists often look for sanctuary in such settings but, as seen in Act:23, they just as likely end up becoming a killing ground. The series score is representative of its setting. While no classic "noir" possesses a jazz score, the music could be heard in nightclubs within the films. Roger's recurring theme, a lone saxophone accompaniment to the protagonist's narration, best exemplifies the "noir" stylings of the series.
Amnesia is a common plot device in "film noir". Because most of these stories focused on a character proving his innocence, authors up the ante by making him an amnesiac, unable to prove his innocence even to himself. "The Big O" goes further, by removing the memories of the whole population. The convoluted past is told through the use of flashbacks. In most "noirs", the past is tangible and menacing. The characters are often trying to escape some trauma or crime tied to the Event, and confronting it becomes their only chance at redemption.
Influences.
Before "The Big O", Sunrise Studios was a subcontractor for Warner Bros. Animation's "", one of the series' influences.
Roger Smith is a pastiche of the Bruce Wayne persona and the Batman. The character design resembles Wayne, complete with slicked-back hair and double-breasted business suit. Like Bruce, Roger prides himself in being a rich playboy to the extent that one of his household's rules is only women may be let into his mansion without his permission. Like Batman, Roger Smith carries a no-gun policy, albeit more flexible. Unlike the personal motives of the Batman, Roger enforces this rule for "it's all part of being a gentleman." Among Roger's gadgetry is the Griffon, a large, black hi-tech sedan comparable to the Batmobile, a grappling cable that shoots out his wristwatch and the giant robot that Angel calls "Roger's alter ego."
"The Big O"'s cast of supporting characters includes Norman, Roger's faithful mechanically-inclined butler who fills the role of Alfred Pennyworth; R. Dorothy Wayneright, who plays the role of the sidekick; and Dan Dastun, a good honest cop who, like Jim Gordon, is both a friend to the hero and greatly respected by his comrades.
The other major influence is Mitsuteru Yokoyama's "Giant Robo". Before working on "The Big O", Kazuyoshi Katayama and other animators worked with Yasuhiro Imagawa on "". The feature, a "retro chic" homage to Yokoyama's career, took seven years to produce and suffered low sales and high running costs. Frustrated by the experience, Katayama and his staff put all their efforts into making "good" with "The Big O".
Like Giant Robo, the megadeuses of "Big O" are metal behemoths. The designs are strange and "more macho than practical," sporting big stovepipe arms and exposed rivets. Unlike the giants of other mecha series, the megadeuses do not exhibit ninja-like speed nor grace. Instead, the robots are armed with "old school" weaponry such as missiles, piston powered punches, machine guns and laser cannons.
Katayama also cited "Super Robot Red Baron" and "Super Robot Mach Baron" among influences on the inspiration of "The Big O". Believing that because "Red Baron" had such a low budget and the big fights always happened outside of a city setting, he wanted "Big O" to be the show he felt "Red Baron" could be with a bigger budget. He also spoke of how he first came up with designs for the robots first as if they were making designs to appeal to toy companies, rather than how "Gundam" was created with a toy company wanting an anime to represent their new product. Big O's large pumping chamber arms, for example, he felt would be cool gimmicks in a toy.
Media.
"The Big O Visual: The official companion to the TV series" (ISBN 4-575-29579-5) was published by Futabasha in 2003. The book contains full-color artwork, character bios and concept art, mecha sketches, video/LD/DVD jacket illustrations, history on the making of The Big O, staff interviews, "Roger's Monologues" comic strip and the original script for the final episode of the series.
"Walking Together On The Yellow Brick Road" was released by Victor Entertainment on 21 September 2000. The drama CD was written by series head writer Chiaki J. Konaka and featured the series' voice cast.
The first season of Big O is featured in "Super Robot Wars D" for the Game Boy Advance. The series, including its second season is also featured in Super Robot Wars Z, released in 2008.
Bandai released a non-scale model kit of Big O in 2000. Though it was an easy snap-together kit, it required painting, as all of the parts (except the clear orange crown and canopy) were molded in dark gray. The kit included springs that enabled the slide-action Side Piles on the forearms to simulate Big O's Sudden Impact maneuver. Also included was an unpainted Roger Smith figure.
PVC figures of Big O and Big Duo (Schwarzwald's Megadeus) were sold by Bandai America. Each came with non-poseable figures of Roger, Dorothy and Angel. Mini-figure sets were sold in Japan and America during the run of the second season. The characters included Big O (standard and attack modes), Roger, Dorothy & Norman, Griffon (Roger's car), Dorothy-1 (Big O's first opponent), Schwarzwald and Big Duo.
In 2009, Bandai released a plastic/diecast figure of the Big O under their Soul of Chogokin line. The figure has the same features as the model kit, but with added detail and accessories. Its design was closely supervised by original designer Keiichi Sato.
In 2011, Max Factory released action figures of Roger and Dorothy through their Figma toyline. Like most Figmas, they are very detailed, articulated and come with accessories and interchangeable faces. In the same year, Max Factory also released a 12-inch, diecast figure of Big O under their Max Gokin line. The figure contained most of the accessories as the Soul of Chogokin figure but also included some others that could be bought separately from the SOC figure, such as the Mobydick (hip) Anchors and Roger Smith's car: the Griffon. Like the Soul of Chogokin figure, its design was also supervised by Keiichi Sato. As well, in that same year, Max Factory released soft vinyl figures of Big Duo and Big Fau. These figures are high in detail but limited in articulation, such as the arms and legs being the only things to move. To date, this is the only action figure of Big Fau.
Reception.
"The Big O" premiered on 13 October 1999. The show was not a hit in its native Japan, rather it was reduced from an outlined 26 episodes to 13 episodes. Western audiences were more receptive and the series achieved the success its creators were looking for. In an interview with AnimePlay, Keiichi Sato said "This is exactly as we had planned", referring to the success overseas.
Several words appear constantly in the English-language reviews; adjectives like "hip", "sleek," "stylish",
 "classy", and, above all, "cool" serve to describe the artwork, the concept, and the series itself. Reviewers have pointed out references and homages to various works of fiction, namely Batman, Giant Robo, the works of Isaac Asimov, Fritz Lang's "Metropolis", James Bond, and "Cowboy Bebop". But "while saying that may cause one to think the show is completely derivative", reads an article at Anime on DVD, ""The Big O" still manages to stand out as something original amongst the other numerous cookie-cutter anime shows." One reviewer cites the extensive homages as one of the series problems and calls to unoriginality on the creators part.
The first season's reception was positive. Anime on DVD recommends it as an essential series. Chris Beveridge of the aforementioned site gave an A- to Vols. 1 and 2, and a B+ to Vols. 3 and 4. Mike Toole of Anime Jump gave it 4.5 (out of a possible 5) stars, while the review at the Anime Academy gave it a grade of 83, listing the series' high points as being "unique", the characters "interesting," and the action "nice." Reviewers, and fans alike, agree the season's downfall was the ending, or its lack thereof. The dangling plot threads frustrated the viewers and prompted Cartoon Network's involvement in the production of further episodes.
The look and feel of the show received a big enhancement in the second season. This time around, the animation is "near OVA quality" and the artwork "far more lush and detailed." Also enhanced are the troubles of the first season. The giant robot battles still seem out of place to some, while others praise the "over-the-top-ness" of their execution.
For some reviewers, the second season "doesn't quite match the first" addressing to "something" missing in these episodes. Andy Patrizio of IGN points out changes in Roger Smith's character, who "lost some of his cool and his very funny side in the second season." Like a repeat of season one, this season's ending is considered its downfall. Chris Beveridge of Anime on DVD wonders if this was head writer "Konaka's attempt to throw his hat into the ring for creating one of the most confusing and oblique endings of any series." Patrizio states "the creators watched "The Truman Show" and "The Matrix" a few times too many."
References.
http://en.wikipedia.org/wiki/Chiaki_J._Konaka#References. Wikipedia, www.wikipedia.org.

</doc>
<doc id="4473" url="http://en.wikipedia.org/wiki?curid=4473" title="BIOS">
BIOS

In IBM PC compatible computers, the Basic Input/Output System (BIOS, also known as System BIOS, ROM BIOS or PC BIOS ()) is a "de facto" standard defining a firmware interface. The name originated from the Basic Input/Output System used in the CP/M operating system in 1975. The BIOS software is built into the PC, and it is the first software run by a PC when powered on.
The fundamental purposes of the BIOS are to initialize and test the system hardware components, and to load a bootloader or an operating system from a mass memory device. The BIOS additionally provides an abstraction layer for the hardware, i.e. a consistent way for application programs and operating systems to interact with the keyboard, display, and other input/output devices. Variations in the system hardware are hidden by the BIOS from programs that use BIOS services instead of directly accessing the hardware. MS-DOS (PC DOS), which was the dominant PC operating system from the early 1980s until the mid 1990s, relied on BIOS services for disk, keyboard, and text display functions. MS Windows NT, Linux, and other protected mode operating systems in general ignore the abstraction layer provided by the BIOS and do not use it after loading, instead accessing the hardware components directly.
BIOS software is stored on a ROM chip on the motherboard. It is specifically designed to work with each particular model of computer, interfacing with various devices that make up the complementary chipset of the system. In modern computer systems, the BIOS contents are stored on flash memory so that the contents can be rewritten without removing the chip from the motherboard. This allows BIOS software to be easily upgraded to add new features or fix bugs, but can make the computer vulnerable to BIOS rootkits.
BIOS technology is in transitional process toward the Unified Extensible Firmware Interface (UEFI) since 2010. 
History.
The term BIOS (Basic Input/Output System) was invented by Gary Kildall and first appeared in the CP/M operating system in 1975, describing the machine-specific part of CP/M loaded during boot time that interfaces directly with the hardware. (A CP/M machine usually has only a simple boot loader in its ROM.)
Versions of MS-DOS, PC DOS or DR-DOS contain a file called variously "IO.SYS", "IBMBIO.COM", "IBMBIO.SYS", or "DRBIOS.SYS"; this file is known as the "DOS BIOS" (aka "DOS I/O System") and contains the lower-level hardware-specific part of the operating system. Together with the underlying hardware-specific, but operating system-independent "System BIOS", which resides in ROM, it represents the analogous to the "CP/M BIOS".
With the introduction of PS/2 machines, IBM divided the System BIOS into real-mode and protected mode portions. The real-mode portion was meant to provide backward-compatibility with existing operating systems such as DOS, and therefore was named "CBIOS" (for Compatibility BIOS), whereas the "ABIOS" (for Advanced BIOS) provided new interfaces specifically suited for multitasking operating systems such as OS/2.
For the history of BIOS user interfaces, see the next section.
BIOS user interface.
The BIOS of the original IBM PC XT had no interactive user interface. Error codes or messages were displayed on the screen, or coded series of sounds were generated to signal errors (when the POST had not proceeded to the point of successfully initializing a video display adapter). Options on the PC and XT were set by switches and jumpers on the main board and on peripheral cards. Starting around the mid-1990s, it became typical for the BIOS ROM to include a "BIOS configuration utility" or "BIOS setup utility", accessed at system power-up by a particular key sequence. This program allowed the user to set system configuration options, of the type formerly set using DIP switches, through an interactive menu system controlled through the keyboard. In the interim period, IBM-compatible PCs—including the IBM AT—held configuration settings in battery-backed RAM and used a bootable configuration program on disk, not in the ROM, to set the configuration options contained in this memory. The disk was supplied with the computer, and if it was lost the system settings could not be changed.
A modern Wintel-compatible computer provides a setup routine essentially unchanged in nature from the ROM-resident BIOS setup utilities of the late 1990s; the user can configure hardware options using the keyboard and video display. Also, when errors occur at boot time, a modern BIOS usually displays user-friendly error messages, often presented as pop-up boxes in a TUI style, and offers to enter the BIOS setup utility or to ignore the error and proceed if possible. Instead of battery-backed RAM, the modern Wintel machine may store the BIOS configuration settings in flash ROM, perhaps the same flash ROM that holds the BIOS itself.
Because it is the only visible feature of the BIOS to the average user, who is not familiar with hardware programming techniques and device abstraction layers, he often misidentifies the BIOS configuration utility as the BIOS, as in typical statements such as, "To add a second internal hard disk, you have to go into your BIOS and enable it," or, "To change the boot password you have to use the BIOS." This misuse of terms has become so common that it may now be considered that "BIOS configuration utility" or "BIOS setup menu" is a new second definition of the word "BIOS".
Operation.
CPU reset.
When an x86 microprocessor is reset, it loads its program counter with a fixed address near the top of the one-megabyte address space assignable in real mode. (Depending on the microprocessor model, it may also assert all of the address lines above the first 20, causing the real-mode address space to be mapped into the last megabyte of the physical address space, until the first FAR jump or call.) The address of the BIOS's memory is located such that it will be executed upon processor reset. There are only a few bytes in memory after the reset startup address, so a jump instruction near the end of the BIOS ROM directs the processor to start executing the BIOS startup code at an earlier address. If the system has just been powered up or the reset button was pressed ("cold boot"), the full power-on self-test (POST) is run. If Ctrl+Alt+Delete was pressed ("warm boot"), a special flag value is stored in nonvolatile BIOS memory before the processor is reset, and after the reset the BIOS startup code detects this flag and does not run the POST. This saves the time otherwise used to detect and test all memory. The non-volatile RAM (NVRAM) is in the real-time clock (RTC).
POST.
The POST checks, identifies, and initializes system devices such as the CPU, RAM, interrupt and DMA controllers and other parts of the chipset, video display card, keyboard, hard disk drive, optical disc drive and other basic hardware.
Early IBM PCs had a little-known routine in the POST that would attempt to download a program from into RAM through the keyboard port. (Note that no serial or parallel ports were standard on early IBM PCs, but a keyboard port of either the XT or AT / PS/2 type has been standard on practically every PC and clone.) If the download was apparently successful, the BIOS would verify a checksum on it and then run it. This feature was intended for factory test or diagnostic purposes. While it was of limited utility outside of factory or repair facilities, it could be used in a proprietary way to boot the PC as a satellite system to a host machine (which is essentially the same technical way it was used, if it was used, in the manufacturing environment).
Search for option ROM modules.
After the motherboard BIOS completes its POST, most BIOS versions search for option ROM modules, also called BIOS extension ROMs, and execute them. The motherboard BIOS scans for extension ROMs in a portion of the "upper memory area" (the part of the x86 real-mode address space at and above address 0xA0000) and runs each ROM found, in order. To discover memory-mapped ISA option ROMs, a BIOS implementation scans the real-mode address space from codice_1 to codice_2 on 2 KiB boundaries, looking for a 2-byte ROM "signature": 0x55 followed by 0xAA. In a valid expansion ROM, this signature is followed by a single byte indicating the number of 512-byte blocks the expansion ROM occupies in real memory, and the next byte is the option ROM's entry point (a.k.a. its "entry offset"). A checksum of the specified number of 512-byte blocks is calculated, and if the ROM has a valid checksum, the BIOS transfers control to the entry address, which in a normal BIOS extension ROM should be the beginning of the extension's initialization routine. At this point, the extension ROM code takes over, typically testing and initializing the hardware it controls and registering interrupt vectors for use by post-boot applications. It may use BIOS services (including those provided by previously initialized option ROMs) to provide a user configuration interface, to display diagnostic information, or to do anything else that it requires. While the actions mentioned are typical behaviors of BIOS entension ROMs, each option ROM receives total control of the computer and may do anything at all, as noted with more detail in the Extensions section below; it is possible that an option ROM will not returning to BIOS, pre-empting the BIOS's boot sequence altogether.
An option ROM should normally return to the BIOS after completing its initialization process. Once (and if) an option ROM returns, the BIOS continues searching for more option ROMs, calling each as it is found, until the entire option ROM area in the memory space has been scanned.
Option ROMs normally reside on adapter cards. However, the original PC, and perhaps also the PC XT, have a spare ROM socket on the motherboard (the "system board" in IBM's terms) into which an option ROM can be inserted, and the four ROMs that contain the BASIC interpreter can also be removed and replaced with custom ROMs which can be option ROMs. The IBM PCjr is unique among PCs in having two ROM cartridge slots on the front. Cartridges in these slots map into the same region of the upper memory area used for option ROMs, and the cartridges can contain option ROM modules that the BIOS would recognize. The cartridges can also contain other types of ROM modules, such as BASIC programs, that are handled differently. One PCjr cartridge can contain several ROM modules of different types, possibly stored together in one ROM chip. For more about option ROMs, see below.
Boot process.
After the option ROM scan is completed and all detected ROM modules with valid checksums have been called, or immediately after POST in a BIOS version that does not scan for option ROMs, the BIOS calls INT 19h to start boot processing. Post-boot, Programs loaded can also call INT 19h to reboot the system, but they must be careful to disable interrupts and other asynchronous hardware processes that may interfere with the BIOS rebooting process, or else the system may hang or crash while it is rebooting.
When INT 19h is called, the BIOS attempts to locate boot loader software held on a storage device designated as a 'boot device', such as a hard disk, a floppy disk, CD, or DVD. It loads and executes the first boot software it finds, giving it control of the PC. This is the process that is known as "booting" (sometimes informally called "booting up"), which is short for "bootstrapping".
The BIOS selects candidate boot devices using information collected by POST and configuration information from EEPROM, CMOS RAM or, in the earliest PCs, DIP switches. Following the boot priority sequence in effect, BIOS checks each device in order to see if it is bootable. For a disk drive or a device that logically emulates a disk drive, such as a USB Flash drive or perhaps a tape drive, to perform this check the BIOS attempts to load the first sector (boot sector) from the disk into RAM at memory address codice_3. If the sector cannot be read (due to a missing or unformatted disk, or due to a hardware failure), the BIOS considers the device unbootable and proceeds to check the next device. If the sector is read successfully, some BIOSes will also check for the boot sector signature 0x55 0xAA in the last two bytes of the (512 byte long) sector, before accepting a boot sector and considering the device bootable. The BIOS proceeds to test each device sequentially until a bootable device is found, at which time the BIOS transfers control to the loaded sector with a jump instruction to its first byte at address codice_3 (exactly 1 KiB below the 32 KiB mark); see MBR invocation and VBR invocation. (This location is one reason that an IBM PC requires at least 32 KiB of RAM in order to be equipped with a disk system; with 31 KiB or less, it would be impossible to boot from any disk, removable or fixed, using the BIOS boot protocol.) Most, but not all, BIOSes load the drive number (as used by INT 13h) of the boot drive into CPU register DL before jumping to the first byte of the loaded boot sector.
Note well that the BIOS does not interpret or process the contents of the boot sector other than to possibly check for the boot sector signature in the last two bytes; all interpretation of data structures like MBR partition tables and so-called BIOS Parameter Blocks is done by the boot program in the boot sector itself or by other programs loaded through the boot process and is beyond the scope of BIOS. Nothing about BIOS predicates these data structures or impedes their replacement or improvement.
A non-disk device such as a network adapter attempts booting by a procedure that is defined by its option ROM or the equivalent integrated into the motherboard BIOS ROM. As such, option ROMs may also influence or supplant the boot process defined by the motherboard BIOS ROM; see above (and below) for more information.
Boot priority.
The user can control the boot process, to cause one medium to be booted instead of another when two or more bootable media are present, by taking advantage of the boot priority implemented by the BIOS. For example, most computers have a hard disk that is bootable, but usually there is a removable-media drive that has higher boot priority, so the user can cause a removable disk to be booted, simply by inserting it, without removing the hard disk drive or altering its contents to make it unbootable. In most modern BIOSes, the boot priority order of all potentially bootable devices can be freely programmed by a user through the BIOS configuration utility. In older BIOSes, limited boot priority options are selectable; in the earliest BIOSes, a fixed priority scheme was implemented, with floppy disk drives first, fixed disks (i.e. hard disks) second, and typically no other boot devices supported, subject to modification of these rules by installed option ROMs. The BIOS in an early PC also usually would only boot from the first floppy disk drive or the first hard disk drive, even if there were two drives of either type installed. All more advanced boot priority sequences evolved as incremental improvements on this basic system.
Boot failure.
The behavior if the BIOS doesn't find a bootable device has varied as personal computers developed. The original IBM PC and XT had Microsoft Cassette BASIC in ROM, and if no bootable device was found, ROM BASIC was started by calling INT 18h. Therefore, barring a hardware failure, an original IBM PC or XT would never fail to boot, either into BASIC or from disk (or through an option ROM). One model of the original IBM PC was available with no disk drive; a cassette recorder could be attached via the cassette port on the rear, for loading and saving BASIC programs to tape. Since few programs used BASIC in ROM, clone PC makers left it out; then a computer that failed to boot from a disk would display "No ROM BASIC" and halt (in response to INT 18h). Later computers would display a message like "No bootable disk found"; some would prompt for a disk to be inserted and a key to be pressed, and when a key was pressed they would restart the boot process. Modern BIOSes may display nothing or may automatically enter the BIOS configuration utility when the boot process fails. Unlike earlier BIOSes, modern BIOSes are often written with the assumption that if the computer cannot be booted from a hard disk, the user will not have software that she wants to boot from removable media instead. (Lately, typically it will only be a specialist computer technician who does that, only to get the computer back into a condition where it can be booted from the hard disk.)
Booting optical media.
As previously mentioned, historically, the BIOS would try to boot from a floppy drive first and a hard disk second. The default for CD or DVD booting is an extension of this. With the El Torito optical media boot standard, the optical drive actually emulates a 3.5" high-density floppy disk to the BIOS for boot purposes. Optical disks are a special case, because their lowest level of data organization is typically a fairly high-level file system (e.g. ISO 9660 for CD-ROM). To read the "first sector" of a CD-ROM or DVD-ROM is not a simply defined operation like it is on a floppy disk or a hard disk. Furthermore, the complexity of the medium makes it difficult to write a useful boot program in one sector, even though optical media sectors are typically 2048 bytes each, four times the standard 512-byte size of floppy and legacy hard disk sectors. Therefore, optical media booting uses the El Torito standard, which specifies a way for an optical disk to contain an image of a high-density ("1.44MB") floppy disk and for the drive to provide access to this disk image in a simple manner that emulates floppy disk drive operations. Therefore, CD-ROM drives boot as emulated floppy disk drives; the bootable virtual floppy disk can contain software that provides access to the optical medium in its native format.
A little-known feature of the original IBM BIOS versions is that before beginning the normal boot process, and even before scanning for option ROMs, they would attempt to load a program through the keyboard port. For more details, see above.
Boot environment.
The environment for the boot program is very simple: the CPU is in real mode and the general-purpose and segment registers are undefined, except CS, SS, SP, and DL. CS is always zero and IP is initially codice_5. Because boot programs are always loaded at this fixed address, there is no need or motivation for a boot program to be relocatable. DL contains the drive number, as used with INT 13h, of the boot device, unless the BIOS is one that does not set the drive number in DL—then DL is undefined. SS:SP points to a valid stack that is presumably large enough to support hardware interrupts, but otherwise SS and SP are undefined. (A stack must be already set up in order for interrupts to be serviced, and interrupts must be enabled in order for the system timer-tick interrupt, which BIOS always uses at least to maintain the time-of-day count and which it initializes during POST, to be active and for the keyboard to work. The keyboard works even if the BIOS keyboard service is not called; keystrokes are received and placed in the 15-character type-ahead buffer maintained by BIOS.) The boot program must set up its own stack (or at least MS-DOS 6 acts like it must), because the size of the stack set up by BIOS is unknown and its location is likewise variable; although the boot program can investigate the default stack by examining SS:SP, it is easier and shorter to just unconditionally set up a new stack.
At boot time, all BIOS services are available, and the memory below address codice_6 contains the interrupt vector table. BIOS POST has initialized the system timers (8253 or 8254 IC), interrupt controller(s), DMA controller(s), and other motherboard/chipset hardware as necessary to bring all BIOS services to ready status. DRAM refresh for all system DRAM in conventional memory and extended memory, but not necessarily expanded memory, has been set up and is running. The interrupt vectors corresponding to the BIOS interrupts have been set to point at the appropriate entry points in the BIOS, hardware interrupt vectors for devices initialized by the BIOS have been set to point to the BIOS-provided ISRs, and some other interrupts, including ones that BIOS generates for programs to hook, have been set to a default dummy ISR that immediately returns. The BIOS maintains a reserved block of system RAM at addresses codice_7 with various parameters initialized during the POST. All memory at and above address codice_8 can be used by the boot program; it may even overwrite itself.
Extensions.
In the IBM PC and AT, peripheral cards such as some hard-drive controllers and some video display adapters have their own BIOS extension option ROMs, which provide additional functionality to BIOS. Code in these extensions runs before the BIOS boots the system from mass storage. These ROMs typically test and initialize hardware, add new BIOS services, and/or augment or replace existing BIOS services with their own versions of those services. For example, a SCSI controller usually has a BIOS extension ROM that adds support for hard drives connected through that controller. Some video cards have extension ROMs that replace the video services of the motherboard BIOS with their own video services. BIOS extension ROMs gain total control of the machine, so in they can in fact do anything, and they may never return control to the BIOS that invoked them. An extension ROM could in principle contain an entire operating system or an application program, or it could implement an entirely different boot process such as booting from a network. Operation of an IBM-compatible computer system can be completely changed by removing or inserting an adapter card (or a ROM chip) that contains a BIOS extension ROM.
The motherboard BIOS typically contains code to access hardware components necessary for bootstrapping the system, such as the keyboard, display, and storage. In addition, plug-in adapter cards such as SCSI, RAID, network interface cards, and video boards often include their own BIOS (e.g. Video BIOS), complementing or replacing the system BIOS code for the given component. Even devices built into the motherboard can behave in this way; their option ROMs can be stored as separate code on the main BIOS flash chip, and upgraded either in tandem with, or separately from, the main BIOS.
An add-in card requires an option ROM if the card is not supported by the main BIOS and the card needs to be initialized or made accessible through BIOS services before the operating system can be loaded (usually this means it is required in the bootstrapping process). Even when it is not required, an option ROM can allow an adapter card to be used without loading driver software from a storage device after booting begins—with an option ROM, no time is taken to load the driver, the driver does not take up space in RAM nor on disk, and the driver software on the ROM always stays with the device so the two cannot be accidentally separated. Also, if the ROM is on the card, both the peripheral hardware and the driver software provided by the ROM are installed together with no extra effort to install the software. An additional advantage of ROM on some early PC systems (notably including the IBM PCjr) was that ROM was faster than main system RAM. (On modern systems, the case is very much the reverse of this, and BIOS ROM code is usually copied—"shadowed"—into RAM so it will run faster.)
There are many methods and utilities for examining the contents of various motherboard BIOS and expansion ROMs, such as Microsoft DEBUG or the Unix dd.
If an expansion ROM wishes to change the way the system boots (such as from a network device or a SCSI adapter for which the BIOS has no driver code) in a cooperative way, it can use the "BIOS Boot Specification" (BBS) API to register its ability to do so. Once the expansion ROMs have registered using the BBS APIs, the user can select among the available boot options from within the BIOS's user interface. This is why most BBS compliant PC BIOS implementations will not allow the user to enter the BIOS's user interface until the expansion ROMs have finished executing and registering themselves with the BBS API. The specification can be downloaded from the ACPICA website. The official title is BIOS Boot Specification (Version 1.01, 11 January 1996). If an expansion ROM wishes to change the way the system boots unilaterally, it can simply hook INT 19h and/or other interrupts normally called from interrupt 19h, such as INT 13h, the BIOS disk service, to intercept the BIOS boot process. Then it can replace the BIOS boot process with one of its own, or it can merely modify the boot sequence by inserting its own boot actions into it, by preventing the BIOS from detecting certain devices as bootable, or both. Before the BIOS Boot Specification was promulgated, this was the only way for expansion ROMs to implement boot capability for devices not supported for booting by the native BIOS of the motherboard.
Operating system services.
The BIOS ROM is customized to the particular manufacturer's hardware, allowing low-level services (such as reading a keystroke or writing a sector of data to diskette) to be provided in a standardized way to programs, including operating systems. For example, an IBM PC might have either a monochrome or a color display adapter (using different display memory addresses and hardware), but a single, standard, BIOS system call may be invoked to display a character at a specified position on the screen in text mode or graphics mode.
The BIOS provides a small library of basic input/output functions to operate peripherals (such as the keyboard, rudimentary text and graphics display functions and so forth). When using MS-DOS, BIOS services could be accessed by an application program (or by MS-DOS) by executing an INT 13h interrupt instruction to access disk functions, or by executing one of a number of other documented BIOS interrupt calls to access video display, keyboard, cassette, and other device functions.
Operating systems and executive software that are designed to supersede this basic firmware functionality provide replacement software interfaces to application software. Applications can also provide these services to themselves. This began even in the 1980s under MS-DOS, when programmers observed that using the BIOS video services for graphics display was very slow. To increase the speed of screen output, many programs bypassed the BIOS and programmed the video display hardware directly. Other graphics programmers, particularly but not exclusively in the demoscene, observed that there were technical capabilities of the PC display adapters that were not supported by the IBM BIOS and could not be taken advantage of without circumventing it. Since the AT-compatible BIOS ran in Intel real mode, operating systems that ran in protected mode on 286 and later processors required hardware device drivers compatible with protected mode operation to replace BIOS services. In modern personal computers running modern operating systems the BIOS is used only during booting and initial loading of system software. Before the operating system's first graphical screen is displayed, input and output are typically handled through BIOS. A boot menu such as the textual menu of Windows, that allows one to choose an operating system to boot, to boot into Safe Mode, or to use the last known good configuration, is displayed through BIOS and receives keyboard input through BIOS. However, it is also important to note that modern PCs can still boot and run legacy operating systems such as MS-DOS or DR-DOS that rely heavily on BIOS for their console and disk I/O, and while the people who use them this way are surely a small minority of computer users, it would be a mistake to assume that literally no one does so. While not as central as they once were, the BIOS services are still important.
Processor microcode updates.
Intel processors have reprogrammable microcode since the P6 microarchitecture. The BIOS may contain patches to the processor microcode that fix errors in the initial processor microcode; reprogramming is not persistent, thus loading of microcode updates is performed each time the system is powered up. Without reprogrammable microcode, an expensive processor swap would be required; for example, the Pentium FDIV bug became an expensive fiasco for Intel as it required a product recall because the original Pentium processor's defective microcode could not be reprogrammed.
Identification.
Some BIOSes contain a "SLIC" (software licensing description table), a digital signature placed inside the BIOS by the manufacturer, for example Dell. (It is often casually called a BIOS tattoo or a tattooed BIOS.) This SLIC is inserted in the ACPI table and contains no active code.
Computer manufacturers that distribute OEM versions of Microsoft Windows and Microsoft application software can use the SLIC to authenticate licensing to the OEM Windows Installation disk and system recovery disc containing Windows software. Systems having a SLIC can be preactivated with an OEM product key, and they verify an XML formatted OEM certificate against the SLIC in the BIOS as a means of self-activating (see System Locked Preinstallation). If a user performs a fresh install of Windows, they will need to have possession of both the OEM key and the digital certificate for their SLIC in order to bypass activation; in practice this is extremely unlikely and hence the only real way this can be achieved is if the user performs a restore using a pre-customised image provided by the OEM. Cracks for non-genuine Windows distributions usually edit the SLIC or emulate it in order to bypass Windows activation.
Modern use.
Some operating systems, for example MS-DOS, rely on the BIOS to carry out most input/output tasks within the PC.
Because the BIOS still runs in 16-bit real mode, calling BIOS services directly is inefficient for protected-mode operating systems. BIOS services are not used by modern multitasking GUI operating systems after they initially load, so the importance of the primary part of BIOS is greatly reduced from what it was initially.
Later BIOS implementations took on more complex functions, by including interfaces such as Advanced Configuration and Power Interface (ACPI); these functions include power management, hot swapping, and thermal management. At the same time, since 2010 BIOS technology is claimed to be in a transitional process toward the UEFI.
Configuration.
Setup utility.
Historically, the BIOS in the IBM PC and XT had no built-in user interface. The BIOS versions in earlier PCs (XT-class) were not software configurable; instead, users set the options via DIP switches on the motherboard. Later computers, including all IBM-compatibles with 80286 CPUs, had a battery-backed nonvolatile BIOS memory (CMOS RAM chip) that held BIOS settings. These settings, such as video-adapter type, memory size, and hard-disk parameters, could only be configured by running a configuration program from a disk, not built into the ROM. A special "reference diskette" was inserted in an IBM AT to configure settings such as memory size.
Early BIOS versions did not have passwords or boot-device selection options. The BIOS was hard-coded to boot from the first floppy drive, or, if that failed, the first hard disk. Access control in early AT-class machines was by a physical keylock switch (which was not hard to defeat if the computer case could be opened). Anyone who could switch on the computer could boot it.
Later, 386-class computers started integrating the BIOS setup utility in the ROM itself, alongside the BIOS code; these computers usually boot into the BIOS setup utility if a certain key or key combination is pressed, otherwise the BIOS POST and boot process executes.
A modern BIOS setup utility has a menu-based user interface (UI) accessed by pressing a certain key on the keyboard when the PC starts. Usually the key is advertised for short time during the early startup, for example "Press F1 to enter CMOS setup". The actual key depends on specific hardware. In the BIOS setup utility, a user can usually:
Reprogramming.
In modern PCs the BIOS is stored in rewritable memory, allowing the contents to be replaced or "rewritten". This rewriting of the contents is sometimes termed "flashing", based on the common use of a kind of EEPROM known technically as "flash EEPROM" and colloquially as "flash memory". It can be done by a special program, usually provided by the system's manufacturer, or at POST, with a BIOS image in a hard drive or USB flash drive. A file containing such contents is sometimes termed "a BIOS image". A BIOS might be reflashed in order to upgrade to a newer version to fix bugs or provide improved performance or to support newer hardware, or a reflashing operation might be needed to fix a damaged BIOS. A person might also be tricked into flashing a BIOS in order to install destructive software or security holes into the BIOS ROM, or a corporation might wish to have BIOSes on computers it made reflashed in order to remove features or add new limiting controls so as to prevent the older computer products from competing with newer products or to restrict or control the use of the computers in order to further the interests of corporate or government partners or to gain a business advantage. Flash "updates" are a double-edged sword, of which computer owners and operators should be aware; they may not always be improvements.
Overclocking.
Some BIOS implementations allow overclocking, an action in which the CPU is adjusted to a higher clock rate than its manufacturer rating for guaranteed capability. Overclocking may, however, seriously compromise system reliability in insufficiently cooled computers and generally shorten component lifespan. Overclocking, when incorrectly performed, may also cause components to overheat so quickly that they mechanically destroy themselves.
Hardware.
The original IBM PC BIOS (and cassette BASIC) was stored on mask-programmed read-only memory (ROM) chips in sockets on the motherboard. ROMs could be replaced, but not altered, by users. To allow for updates, many compatible computers used re-programmable memory devices such as EPROM and later flash memory devices. According to Robert Braver, the president of the BIOS manufacturer Micro Firmware, Flash BIOS chips became common around 1995 because the electrically erasable PROM (EEPROM) chips are cheaper and easier to program than standard ultraviolet erasable PROM (EPROM) chips. Flash chips are programmed (and re-programmed) in-circuit, while EPROM chips need to be removed from the motherboard for re-programming. BIOS versions are upgraded to take advantage of newer versions of hardware and to correct bugs in previous revisions of BIOSes.
Beginning with the IBM AT, PCs supported a hardware clock settable through BIOS. It had a century bit which allowed for manually changing the century when the year 2000 happened. Most BIOS revisions created in 1995 and nearly all BIOS revisions in 1997 supported the year 2000 by setting the century bit automatically when the clock rolled past midnight, December 31, 1999.
The first flash chips were attached to the ISA bus. Starting in 1997, the BIOS flash moved to the LPC bus, a functional replacement for ISA, following a new standard implementation known as "firmware hub" (FWH). In 2006, the first systems supporting a Serial Peripheral Interface (SPI) appeared, and the BIOS flash memory moved again.
The size of the BIOS, and the capacity of the ROM, EEPROM, or other media it may be stored on, has increased over time as new features have been added to the code; BIOS versions now exist with sizes up to 16 megabytes. For contrast, the original IBM PC BIOS was contained in an 8 KiB mask ROM. Some modern motherboards are including even bigger NAND flash memory ICs on board which are capable of storing whole compact operating systems, such as some Linux distributions. For example, some ASUS motherboards included SplashTop Linux embedded into their NAND flash memory ICs. However, the idea of including an operating system along with BIOS in the ROM of a PC is not new; in the 1980s, Microsoft offered a ROM option for MS-DOS, and it was included in the ROMs of some PC clones such as the Tandy 1000 HX.
Another type of firmware chip was found on the IBM PC AT and early compatibles. In the AT, the keyboard interface was controlled by a microcontroller with its own programmable memory. On the IBM AT, this was a 40 pin socketed device. Some manufacturers used an EPROM version of this chip which resembled an EPROM. This controller was also assigned the A20 gate function to manage memory above the 1 megabyte range; occasionally an upgrade of this "keyboard BIOS" was necessary to take advantage of software that could use upper memory. (An earlier version of this article stated that the IBM PC also used a microcontroller to control the keyboard interface. This is incorrect; the PC and XT used a simple shift register and an 8-bit parallel port of the 8255A PPI chip for the computer end of the keyboard interface.)
The BIOS may contain components such as the Memory Reference Code (MRC), which is responsible for handling memory timings and related hardware settings.
Vendors and products.
IBM published the entire listings of the BIOS for its original PC, PC XT, PC AT, and other contemporary PC models, in an appendix of the Technical Reference manual for each machine type. The effect of the publication of the BIOS listings is that anyone can see exactly what a definitive BIOS does and how it does it. Phoenix Technologies was the first company to write a fully compatible and completely legal BIOS through clean-room reverse engineering.
New standards grafted onto the BIOS are usually without complete public documentation or any BIOS listings. As a result, it is not as easy to learn the intimate details about the many non-IBM additions to BIOS as about the core BIOS services.
Most PC motherboard suppliers license a BIOS "core" and toolkit from a commercial third-party, known as an "independent BIOS vendor" or IBV. The motherboard manufacturer then customizes this BIOS to suit its own hardware. For this reason, updated BIOSes are normally obtained directly from the motherboard manufacturer. Major BIOS vendors include American Megatrends (AMI), Insyde Software, Phoenix Technologies and Byosoft. Former vendors include Award Software and Microid Research which were acquired by Phoenix Technologies in 1998; Phoenix later phased out the Award Brand name. General Software, which was also acquired by Phoenix in 2007, sold BIOS for Intel processor based embedded systems.
The open source community increased their effort to develop a replacement for proprietary BIOSes and their future incarnations with an open sourced counterpart through the coreboot and OpenBIOS/Open Firmware projects. AMD provided product specifications for some chipsets, and Google is sponsoring the project. Motherboard manufacturer Tyan offers coreboot next to the standard BIOS with their Opteron line of motherboards. MSI and Gigabyte Technology have followed suit with the MSI K9ND MS-9282 and MSI K9SD MS-9185 resp. the M57SLI-S4 models.
Security.
EEPROM chips are advantageous because they can be easily updated by the user; hardware manufacturers frequently issue BIOS updates to upgrade their products, improve compatibility and remove bugs. However, this advantage had the risk that an improperly executed or aborted BIOS update could render the computer or device unusable. To avoid these situations, more recent BIOSes use a "boot block"; a portion of the BIOS which runs first and must be updated separately. This code verifies if the rest of the BIOS is intact (using hash checksums or other methods) before transferring control to it. If the boot block detects any corruption in the main BIOS, it will typically warn the user that a recovery process must be initiated by booting from removable media (floppy, CD or USB memory) so the user can try flashing the BIOS again. Some motherboards have a "backup" BIOS (sometimes referred to as DualBIOS boards) to recover from BIOS corruptions.
There are at least four known BIOS attack viruses, two of which were for demonstration purposes. The first one found in the wild was Mebromi, targeting Chinese users.
The first BIOS virus was CIH, whose name matches the initials of its creator, Chen Ing Hau. CIH was also called the "Chernobyl Virus", because its payload date was 1999-04-26, the 13th anniversary of the Chernobyl accident. CIH appeared in mid-1998 and became active in April 1999. It was able to erase flash ROM BIOS content. Often, infected computers could no longer boot, and people had to remove the flash ROM IC from the motherboard and reprogram it. CIH targeted the then-widespread Intel i430TX motherboard chipset and took advantage of the fact that the Windows 9x operating systems, also widespread at the time, allowed direct hardware access to all programs.
Modern systems are not vulnerable to CIH because of a variety of chipsets being used which are incompatible with the Intel i430TX chipset, and also other flash ROM IC types. There is also extra protection from accidental BIOS rewrites in the form of boot blocks which are protected from accidental overwrite or dual and quad BIOS equipped systems which may, in the event of a crash, use a backup BIOS. Also, all modern operating systems such as FreeBSD, Linux, OS X, Windows NT-based Windows OS like Windows 2000, Windows XP and newer, do not allow user-mode programs to have direct hardware access.
As a result, as of 2008, CIH has become essentially harmless, at worst causing annoyance by infecting executable files and triggering antivirus software. Other BIOS viruses remain possible, however; since most Windows home users without Windows Vista/7's UAC run all applications with administrative privileges, a modern CIH-like virus could in principle still gain access to hardware without first using an exploit. The operating system OpenBSD prevents all users from having this access and the grsecurity patch for the linux kernel also prevents this direct hardware access by default, the difference being an attacker requiring a much more difficult kernel level exploit or reboot of the machine.
The second BIOS virus was a technique presented by John Heasman, principal security consultant for UK-based Next-Generation Security Software. In 2006, at the Black Hat Security Conference, he showed how to elevate privileges and read physical memory, using malicious procedures that replaced normal ACPI functions stored in flash memory.
The third BIOS virus was a technique called "Persistent BIOS infection." It appeared in 2009 at the CanSecWest Security Conference in Vancouver, and at the SyScan Security Conference in Singapore. Researchers Anibal Sacco and Alfredo Ortega, from Core Security Technologies, demonstrated how to insert malicious code into the decompression routines in the BIOS, allowing for nearly full control of the PC at start-up, even before the operating system is booted. The proof-of-concept does not exploit a flaw in the BIOS implementation, but only involves the normal BIOS flashing procedures. Thus, it requires physical access to the machine, or for the user to be root. Despite these requirements, Ortega underlined the profound implications of his and Sacco's discovery: "We can patch a driver to drop a fully working rootkit. We even have a little code that can remove or disable antivirus."
Mebromi is a trojan which targets computers with AwardBIOS, Microsoft Windows, and antivirus software from two Chinese companies: Rising Antivirus and Jiangmin KV Antivirus. Mebromi installs a rootkit which infects the master boot record.
In a December 2013 interview with CBS 60 Minutes, Deborah Plunkett, Information Assurance Director for the US National Security Agency claimed that NSA analysts had uncovered and thwarted a possible BIOS attack by a foreign nation state. The attack on the world's computers could have allegedly "literally taken down the US economy." The segment further cites anonymous cyber security experts briefed on the operation as alleging the plot was conceived in China. A later article in The Guardian cast doubt on the likelihood of such a threat, quoting Berkeley computer-science researcher Nicholas Weaver, Matt Blaze, a computer and information sciences professor at the University of Pennsylvania, and cybersecurity expert Robert David Graham in an analysis of the NSA's claims.
Alternatives and successors.
In other types of computers, the terms "boot monitor", "boot loader", and "boot ROM" may be used instead.
As of 2011, the BIOS is being replaced by the more complex Extensible Firmware Interface (EFI) in many new machines. EFI is a specification which replaces the runtime interface of the legacy BIOS. Initially written for the Itanium architecture, EFI is now available for x86 and x86-64 platforms; the specification development is driven by The Unified EFI Forum, an industry Special Interest Group. EFI booting has been supported in only Microsoft Windows versions supporting GPT, the Linux kernel 2.6.1 and later, and Mac OS X on Intel-based Macs.
Other alternatives to the functionality of the "Legacy BIOS" in the x86 world include coreboot.
A number of larger, more powerful servers and workstations use a platform-independent Open Firmware (IEEE-1275) based on the Forth programming language; it is included with Sun's SPARC computers, IBM's RS/6000 line, and other PowerPC systems such as the CHRP motherboards, along with the x86-based OLPC XO-1. Later x86-based personal computer operating systems, like Windows NT, use their own, native drivers; this makes it much easier to extend support to new hardware.

</doc>
<doc id="4474" url="http://en.wikipedia.org/wiki?curid=4474" title="Bose–Einstein condensate">
Bose–Einstein condensate

A Bose–Einstein condensate (BEC) is a state of matter of a dilute gas of bosons cooled to temperatures very close to absolute zero (that is, very near or ). Under such conditions, a large fraction of the bosons occupy the lowest quantum state, at which point quantum effects become apparent on a macroscopic scale. These effects are called macroscopic quantum phenomena.
Although later experiments have revealed complex interactions, this state of matter was first predicted, generally, in 1924–25 by Satyendra Nath Bose and Albert Einstein.
History.
Bose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons). Einstein was impressed, translated the paper himself from English to German and submitted it for Bose to the "Zeitschrift für Physik", which published it. (The Einstein manuscript, once believed to be lost, was found in a library at Leiden University in 2005.). Einstein then extended Bose's ideas to material particles (or matter) in two other papers. The result of the efforts of Bose and Einstein is the concept of a Bose gas, governed by Bose–Einstein statistics, which describes the statistical distribution of identical particles with integer spin, now known as bosons. Bosonic particles, which include the photon as well as atoms such as helium-4 (4He), are allowed to share quantum states with each other. Einstein proposed that cooling bosonic atoms to a very low temperature would cause them to fall (or "condense") into the lowest accessible quantum state, resulting in a new form of matter.
In 1938 Fritz London proposed BEC as a mechanism for superfluidity in 4He and superconductivity.
In 1995 the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST–JILA lab, using a gas of rubidium atoms cooled to 170 nanokelvin (nK) (). For their achievements Cornell, Wieman, and Wolfgang Ketterle at MIT received the 2001 Nobel Prize in Physics. In November 2010 the first photon BEC was observed. 
Concept.
This transition to BEC occurs below a critical temperature, which for a uniform three-dimensional gas consisting of non-interacting particles with no apparent internal degrees of freedom is given by:
where:
Einstein's non-interacting model.
Consider a collection of "N" noninteracting particles, which can each be in one of two quantum states, formula_2 and formula_3. If the two states are equal in energy, each different configuration is equally likely.
If we can tell which particle is which, there are formula_4 different configurations, since each particle can be in formula_2 or formula_3 independently. In almost all of the configurations, about half the particles are in formula_2 and the other half in formula_3. The balance is a statistical effect: the number of configurations is largest when the particles are divided equally.
If the particles are indistinguishable, however, there are only "N"+1 different configurations. If there are "K" particles in state formula_3, there are particles in state formula_2. Whether any particular particle is in state formula_2 or in state formula_3 cannot be determined, so each value of "K" determines a unique quantum state for the whole system. If all these states are equally likely, there is no statistical spreading out; it is just as likely for all the particles to sit in formula_2 as for the particles to be split half and half.
Suppose now that the energy of state formula_3 is slightly greater than the energy of state formula_2 by an amount "E". At temperature "T", a particle will have a lesser probability to be in state formula_3 by exp(−"E"/"kT"). In the distinguishable case, the particle distribution will be biased slightly towards state formula_2, and the distribution will be slightly different from half-and-half. But in the indistinguishable case, since there is no statistical pressure toward equal numbers, the most-likely outcome is that most of the particles will collapse into state formula_2.
In the distinguishable case, for large "N", the fraction in state formula_2 can be computed. It is the same as flipping a coin with probability proportional to "p" = exp(−"E"/"T") to land tails. The probability to land heads is , which is a smooth function of "p", and thus of the energy.
In the indistinguishable case, each value of "K" is a single state, which has its own separate Boltzmann probability. So the probability distribution is exponential:
For large "N", the normalization constant "C" is . The expected total number of particles not in the lowest energy state, in the limit that formula_21, is equal to formula_22. It does not grow when "N" is large; it just approaches a constant. This will be a negligible fraction of the total number of particles. So a collection of enough Bose particles in thermal equilibrium will mostly be in the ground state, with only a few in any excited state, no matter how small the energy difference.
Consider now a gas of particles, which can be in different momentum states labeled formula_23. If the number of particles is less than the number of thermally accessible states, for high temperatures and low densities, the particles will all be in different states. In this limit, the gas is classical. As the density increases or the temperature decreases, the number of accessible states per particle becomes smaller, and at some point, more particles will be forced into a single state than the maximum allowed for that state by statistical weighting. From this point on, any extra particle added will go into the ground state.
To calculate the transition temperature at any density, integrate, over all momentum states, the expression for maximum number of excited particles, :
When the integral is evaluated with the factors of "k""B" and restored by dimensional analysis, it gives the critical temperature formula of the preceding section. Therefore, this integral defines the critical temperature and particle number corresponding to the conditions of negligible chemical potential. In Bose–Einstein statistics distribution, μ is actually still nonzero for BEC"'s"; however, μ is less than the ground state energy. Except when specifically talking about the ground state, μ can consequently be approximated for most energy or momentum states as μ ≈ 0.
Interacting models.
Gross–Pitaevskii equation.
In some simplest cases, the state of condensed particles can be described with a nonlinear Schrödinger equation, also known as Gross-Pitaevskii or Ginzburg-Landau equation. The validity of this approach is actually limited to the case of ultracold temperatures, which fits well for the most alkali atoms experiments.
This approach originates from the assumption that the state of the BEC can be described by the unique wavefunction of the condensate formula_26. For a system of this nature, formula_27 is interpreted as the particle density, so the total number of atoms is formula_28
Provided essentially all atoms are in the condensate (that is, have condensed to the ground state), and treating the bosons using mean field theory, the energy (E) associated with the state formula_26 is:
Minimizing this energy with respect to infinitesimal variations in formula_26, and holding the number of atoms constant, yields the Gross–Pitaevski equation (GPE) (also a non-linear Schrödinger equation):
where:
In the case of zero external potential, the dispersion law of interacting Bose-Einstein-condensed particles is given by so-called Bogoliubov spectrum (for formula_33):
The Gross-Pitaevskii equation (GPE) provides a relatively good description of the behavior of atomic BEC's. However, GPE does not take into account the temperature dependence of dynamical variables, and is therefore valid only for formula_33.
It is not applicable for example for the condensates of excitons, magnons and photons, where the critical temperature is up to room one.
Weaknesses of Gross–Pitaevskii model.
The Gross–Pitaevskii model of BEC is a physical approximation valid for certain classes of BECs only. By construction, GPE uses the following simplifications: it assumes that interactions between condensate particles are of the contact two-body type and also it neglects anomalous contributions to self-energy. These assumptions are suitable mostly for the dilute three-dimensional condensates. If one relaxes any of these assumptions, the equation for the condensate wavefunction acquires the terms containing higher-order powers of the wavefunction. Moreover, for some physical systems the amount of such terms turns out to be infinite, therefore, the equation becomes essentially non-polynomial. The examples where this could happen are the Bose–Fermi composite condensates, effectively lower-dimensional condensates, and dense condensates and superfluid clusters and droplets.
Other models.
However, it is clear that in a general case the behaviour of Bose-Einstein condensate can be described by coupled evolution equations for condensate density, superfluid velocity and distribution function of elementary excitations. This problem was in 1977 by Peletminskii et al. in microscopical approach. The Peletminskii equations are valid for any finite temperatures below the critical point. Years after, in 1985, Kirkpatrick and Dorfman obtained similar equations using another microscopical approach. The Peletminskii equations also reproduce Khalatnikov hydrodynamical equations for superfluid as a limiting case.
Superfluidity of BEC and Landau criterion.
The phenomena of superfluidity of a Bose gas and superconductivity of a strongly-correlated Fermi gas (a gas of Cooper pairs) are tightly connected to Bose-Einstein condensation. Under corresponding conditions, below the temperature of phase transition, these phenomena were observed in helium-4 and different classes of superconductors. (In this sense, the superconductivity is often called simply as the superfluidity of Fermi gas).
In the simplest form, the origin of superfluidity can be seen from the weakly interacting bosons model.
Experimental observation.
Superfluid He-4.
In 1938, Pyotr Kapitsa, John Allen and Don Misener discovered that helium-4 became a new kind of fluid, now known as a superfluid, at temperatures less than 2.17 K (the lambda point). Superfluid helium has many unusual properties, including zero viscosity (the ability to flow without dissipating energy) and the existence of quantized vortices. It was quickly believed that the superfluidity was due to partial Bose–Einstein condensation of the liquid. In fact, many of the properties of superfluid helium also appear in the gaseous Bose–Einstein condensates created by Cornell, Wieman and Ketterle (see below). Superfluid helium-4 is a liquid rather than a gas, which means that the interactions between the atoms are relatively strong; the original theory of Bose–Einstein condensation must be heavily modified in order to describe it. Bose–Einstein condensation remains, however, fundamental to the superfluid properties of helium-4. Note that helium-3, consisting of fermions instead of bosons, also enters a superfluid phase at low temperature, which can be explained by the formation of bosonic Cooper pairs of two atoms each (see also fermionic condensate).
Atomic condensates.
The first "pure" Bose–Einstein condensate was created by Eric Cornell, Carl Wieman, and co-workers at JILA on 5 June 1995. They did this by cooling a dilute vapor consisting of approximately two thousand rubidium-87 atoms to below 170 nK using a combination of laser cooling (a technique that won its inventors Steven Chu, Claude Cohen-Tannoudji, and William D. Phillips the 1997 Nobel Prize in Physics) and magnetic evaporative cooling. About four months later, an independent effort led by Wolfgang Ketterle at MIT created a condensate made of sodium-23. Ketterle's condensate had about a hundred times more atoms, allowing him to obtain several important results such as the observation of quantum mechanical interference between two different condensates. Cornell, Wieman and Ketterle won the 2001 Nobel Prize in Physics for their achievements. A group led by Randall Hulet at Rice University announced the creation of a condensate of lithium atoms only one month following the JILA work. Lithium has attractive interactions which causes the condensate to be unstable and to collapse for all but a few atoms. Hulet and co-workers showed in a subsequent experiment that the condensate could be stabilized by the quantum pressure from trap confinement for up to about 1000 atoms.
The Bose–Einstein condensation also applies to quasiparticles in solids. A magnon in an antiferromagnet carries spin 1 and thus obeys Bose–Einstein statistics. The density of magnons is controlled by an external magnetic field, which plays the role of the magnon chemical potential. This technique provides access to a wide range of boson densities from the limit of a dilute Bose gas to that of a strongly interacting Bose liquid. A magnetic ordering observed at the point of condensation is the analog of superfluidity. In 1999 Bose condensation of magnons was demonstrated in the antiferromagnet Tl Cu Cl3. The condensation was observed at temperatures as large as 14 K. Such a high transition temperature (relative to that of atomic gases) is due to the greater density achievable with magnons and the smaller mass (roughly equal to the mass of an electron). In 2006, condensation of magnons in ferromagnets was even shown at room temperature, where the authors used pumping techniques.
Velocity-distribution data graph.
In the image accompanying this article, the velocity-distribution data indicates the formation of a Bose–Einstein condensate out of a gas of rubidium atoms. The false colors indicate the number of atoms at each velocity, with red being the fewest and white being the most. The areas appearing white and light blue are at the lowest velocities. The peak is not infinitely narrow because of the Heisenberg uncertainty principle: since the atoms are trapped in a particular region of space, their velocity distribution necessarily possesses a certain minimum width. This width is given by the curvature of the magnetic trapping potential in the given direction. More tightly confined directions have bigger widths in the ballistic velocity distribution. This anisotropy of the peak on the right is a purely quantum-mechanical effect and does not exist in the thermal distribution on the left. This graph served as the cover design for the 1999 textbook "Thermal Physics" by Ralph Baierlein.
Peculiarities of a condensate.
Vortices.
As in many other systems, vortices can exist in BECs. These can be created, for example, by 'stirring' the condensate with lasers, or rotating the confining trap. The vortex created will be a quantum vortex. These phenomena are allowed for by the non-linear formula_27 term in the GPE. As the vortices must have quantized angular momentum the wavefunction may have the form formula_37 where formula_38 and formula_39 are as in the cylindrical coordinate system, and formula_40 is the angular number. This is particularly likely for an axially symmetric (for instance, harmonic) confining potential, which is commonly used. The notion is easily generalized. To determine formula_41, the energy of formula_26 must be minimized, according to the constraint formula_37. This is usually done computationally, however in a uniform medium the analytic form
where:
demonstrates the correct behavior, and is a good approximation.
A singly charged vortex (formula_45) is in the ground state, with its energy formula_46 given by
where:
For multiply charged vortices (formula_49) the energy is approximated by
which is greater than that of formula_40 singly charged vortices, indicating that these multiply charged vortices are unstable to decay. Research has, however, indicated they are metastable states, so may have relatively long lifetimes.
Closely related to the creation of vortices in BECs is the generation of so-called dark solitons in one-dimensional BECs. These topological objects feature a phase gradient across their nodal plane, which stabilizes their shape even in propagation and interaction. Although solitons carry no charge and are thus prone to decay, relatively long-lived dark solitons have been produced and studied extensively.
Attractive interactions.
The experiments led by Randall Hulet at Rice University from 1995 through 2000 showed that lithium condensates with attractive interactions could stably exist, but only up to a certain critical atom number. Beyond this critical number, the attraction overwhelmed the zero-point energy of the harmonic confining potential, causing the condensate to collapse in a burst reminiscent of a supernova explosion where an explosion is preceded by an implosion. By quench cooling the gas of lithium atoms, they observed the condensate to first grow, and subsequently collapse when the critical number was exceeded.
Further experimentation on attractive condensates was performed in 2000 by the JILA team, consisting of Cornell, Wieman and coworkers. They originally used rubidium-87, an isotope whose atoms naturally repel each other, making a more stable condensate. Their instrumentation now had better control over the condensate so experimentation was made on naturally "attracting" atoms of another rubidium isotope, rubidium-85 (having negative atom–atom scattering length). Through a process called Feshbach resonance involving a sweep of the magnetic field causing spin flip collisions, they lowered the characteristic, discrete energies at which the rubidium atoms bond into molecules, making their Rb-85 atoms repulsive and creating a stable condensate. The reversible flip from attraction to repulsion stems from quantum interference among condensate atoms which behave as waves.
When the JILA team raised the magnetic field strength still further, the condensate suddenly reverted to attraction, imploded and shrank beyond detection, and then exploded, expelling off about two-thirds of its 10,000 or so atoms. About half of the atoms in the condensate seemed to have disappeared from the experiment altogether, not being seen either in the cold remnant or the expanding gas cloud. Carl Wieman explained that under current atomic theory this characteristic of Bose–Einstein condensate could not be explained because the energy state of an atom near absolute zero should not be enough to cause an implosion; however, subsequent mean field theories have been proposed to explain it. The atoms that seem to have disappeared almost certainly still exist in some form, just not in a form that could be accounted for in that experiment. Most likely they formed molecules consisting of two bonded rubidium atoms. The energy gained by making this transition imparts a velocity sufficient for them to leave the trap without being detected.
Current research.
Compared to more commonly encountered states of matter, Bose–Einstein condensates are extremely fragile. The slightest interaction with the outside world can be enough to warm them past the condensation threshold, eliminating their interesting properties and forming a normal gas. 
Nevertheless, they have proven useful in exploring a wide range of questions in fundamental physics, and the years since the initial discoveries by the JILA and MIT groups have seen an explosion in experimental and theoretical activity. Examples include experiments that have demonstrated interference between condensates due to wave–particle duality, the study of superfluidity and quantized vortices, the creation of bright matter wave solitons from Bose condensates confined to one dimension, and the slowing of light pulses to very low speeds using electromagnetically induced transparency. Vortices in Bose–Einstein condensates are also currently the subject of analogue gravity research, studying the possibility of modeling black holes and their related phenomena in such environments in the lab. Experimenters have also realized "optical lattices", where the interference pattern from overlapping lasers provides a periodic potential for the condensate. These have been used to explore the transition between a superfluid and a Mott insulator, and may be useful in studying Bose–Einstein condensation in fewer than three dimensions, for example the Tonks–Girardeau gas.
Bose–Einstein condensates composed of a wide range of isotopes have been produced.
Related experiments in cooling fermions rather than bosons to extremely low temperatures have created degenerate gases, where the atoms do not congregate in a single state due to the Pauli exclusion principle. To exhibit Bose–Einstein condensation, the fermions must "pair up" to form compound particles (e.g. molecules or Cooper pairs) that are bosons. The first molecular Bose–Einstein condensates were created in November 2003 by the groups of Rudolf Grimm at the University of Innsbruck, Deborah S. Jin at the University of Colorado at Boulder and Wolfgang Ketterle at MIT. Jin quickly went on to create the first fermionic condensate composed of Cooper pairs.
In 1999, Danish physicist Lene Hau led a team from Harvard University which succeeded in slowing a beam of light to about 17 meters per second. She was able to achieve this by using a superfluid. Hau and her associates at Harvard University have since successfully made a group of condensate atoms recoil from a "light pulse" such that they recorded the light's phase and amplitude, which was recovered by a second nearby condensate, by what they term "slow-light-mediated atomic matter-wave amplification" using Bose–Einstein condensates: details of the experiment are discussed in an article in the journal "Nature", 8 February 2007.
Researchers in the new field of atomtronics use the properties of Bose–Einstein condensates when manipulating groups of identical cold atoms using lasers. Further, Bose–Einstein condensates have been proposed by Emmanuel David Tannenbaum to be used in anti-stealth technology.
Isotopes.
The effect has mainly been observed on alkaline atoms which have nuclear properties particularly suitable for working with traps. As of 2012, using ultra-low temperatures of or below, Bose–Einstein condensates had been obtained for a multitude of isotopes, mainly of alkaline, alkaline earth,
and lanthanoid atoms (7Li, 23Na, 39K, 41K, 85Rb, 87Rb, 133Cs, 52Cr, 40Ca, 84Sr, 86Sr, 88Sr, 174Yb, 164Dy, and 168Er ). Condensation research was finally successful even with hydrogen with the aid of special methods. In contrast, the superfluid state of the bosonic 4He at temperatures below is not a good example of Bose–Einstein condensation, because the interaction between the 4He bosons is too strong. Only 8% of the atoms are in the single-particle ground state near zero temperature, rather than the 100% expected of a true Bose–Einstein condensate.
The spin-statistics theorem of Wolfgang Pauli states that half-integer spins (in units of formula_52) lead to fermionic behavior, e.g., the Pauli exclusion principle forbidding that more than two electrons possess the same energy, whereas integer spins lead to bosonic behavior, e.g., condensation of identical bosonic particles in a common ground state.
The bosonic, rather than fermionic, behavior of some of these alkaline gases appears odd at first sight, because their nuclei have half-integer total spin. The bosonic behavior arises from a subtle interplay of electronic and nuclear spins: at ultra-low temperatures and corresponding excitation energies, the half-integer total spin of the electronic shell and the half-integer total spin of the nucleus of the atom are coupled by a very weak hyperfine interaction. The total spin of the atom, arising from this coupling, is an integer value leading to the bosonic ultra-low temperature behavior of the atom. The chemistry of the systems at room temperature is determined by the electronic properties, which is essentially fermionic, since at room temperature, thermal excitations have typical energies much higher than the hyperfine values.

</doc>
<doc id="4475" url="http://en.wikipedia.org/wiki?curid=4475" title="B (programming language)">
B (programming language)

B is a programming language developed at Bell Labs circa 1969. It is the work of Ken Thompson with Dennis Ritchie. B first appeared circa 1969.
B was derived from BCPL, and its name may be a contraction of BCPL. It is possible that its name may be based on Bon, an earlier but unrelated, and rather different, programming language that Thompson designed for use on Multics.
B was designed for recursive, non-numeric, machine independent applications, such as system and language software.
History.
Initially Ken Thompson and later Dennis Ritchie developed B basing it mainly on the BCPL language Thompson used in the Multics project. B was essentially the BCPL system stripped of any component Thompson felt he could do without in order to make it fit within the memory capacity of the minicomputers of the time. The BCPL to B transition also included changes made to suit Thompson's preferences (mostly along the lines of reducing the number of non-whitespace characters in a typical program). Much of the typical algol-like syntax of BCPL was rather heavily changed in this process, such as the := and = operators which were replaced with = for assignment and == for equality test. (The & and | of BCPL was later changed to && and || in the transition to what is now known as C.) 
Thomson invented arithmetic assignment operators for B, using x=+y to add y to x (nowadays the operator is spelled +=). B also introduced the increment and decrement operators (++ and --); Their prefix or postfix position determines whether the value is taken prior or post alteration of the operand. These innovations were not in the earliest versions of B. Some guess that they were created for the auto-increment and auto-decrement address modes of the DEC PDP-11. This is historically impossible as there was no PDP-11 at the time that B was developed.
B is typeless, or more precisely has one data type: the computer word. Most operators (e.g., +, -, *, /) treated this as an integer, but others treated it as a memory address to be dereferenced. In many other ways it looked a lot like an early version of C. There are a few library functions, including some that vaguely resemble functions from the standard I/O library in C.
Early implementations were for the DEC PDP-7 and PDP-11 minicomputers using early Unix, and Honeywell 36-bit mainframes running the operating system GCOS. The earliest PDP-7 implementations compiled to threaded code, and then Ritchie wrote a compiler using TMG which produced machine code. In 1970 a PDP-11 was acquired and threaded code was used for the port. An early version of yacc was produced with this PDP-11 configuration. Ritchie took over maintenance during this period.
The typeless nature of B made sense on the Honeywell, PDP-7 and many older computers, but was a problem on the PDP-11 because it was difficult to elegantly access the character data type that the PDP-11 and most modern computers fully support. Starting in 1971 Ritchie made changes to the language while converting its compiler to produce machine code, most notably adding data typing for variables. During 1971 and 1972 B evolved into "New B" (NB) and then C.
B continues to see use (as of 2014) on GCOS mainframes, and on certain embedded systems for a variety of reasons, including limited hardware in the small systems; extensive libraries, tools, licensing cost issues; and simply being good enough for the job on others. The highly influential AberMUD was originally written in B.
B is almost extinct, having been superseded by the C language.
Examples.
The following example is from the "Users' Reference to B" by Ken Thompson:

</doc>
<doc id="4476" url="http://en.wikipedia.org/wiki?curid=4476" title="Beer–Lambert law">
Beer–Lambert law

The Beer–Lambert law, also known as Beer's law, the Lambert–Beer law, or the Beer–Lambert–Bouguer law relates the attenuation of light to the properties of the material through which the light is traveling. The law is commonly applied to chemical analysis measurements and used in understanding attenuation in physical optics.
Equations.
The law states that there is a logarithmic dependence between the transmission (or transmissivity or transmittance), "T", of light through a substance and the product of the attenuation coefficient of the substance, "Σ", and the distance the light travels through the material (i.e., the path length), "ℓ". The attenuation coefficient can, in turn, be written as a product of either an absorptivity of the attenuator, "ε", and the concentration "c" of attenuating species in the material, or a total (absorption and scattering) cross section, "σ", and the (number) density "N' of attenuators.
In some chemistry applications for liquids these relations are usually written with the notation:
whereas in biology and physics, they are normally written as:
where formula_3 and formula_4 are the intensity (power per unit area) of the incident radiation and the transmitted radiation, respectively; σ is attenuation cross section and N is the concentration (number per unit volume) of attenuating medium.
The base 10 and base "e" conventions must not be confused because they have different numerical values for the attenuation coefficient: formula_5. However, it is easy to convert one to the other, using
The transmissivity (ability to transmit) is expressed in terms of an absorbance which is defined as
whereas it can be expressed in decibels as:
This implies that the absorbance becomes linear with the concentration (or number density of attenuators) according to
and
for the two cases, respectively.
Thus, if the path length and the attenuation coefficient (or the total cross section) are known and the absorbance is measured, the concentration of the substance (or the number density of attenuators) can be deduced.
Although several of the expressions above often are used as Beer–Lambert law, the name should strictly speaking only be associated with the latter two. The reason is that historically, the Lambert law states that attenuation is proportional to the light path length, whereas the Beer law states that attenuation is proportional to the concentration of attenuating species in the material.
If the concentration is expressed as a mole fraction i.e., a dimensionless fraction, the absorptivity of the attenuator ("ε") takes the same dimension as the attenuation coefficient, i.e., reciprocal length (e.g., m−1). However, if the concentration is expressed in moles per unit volume, the attenuation coefficient ("ε") is used in L·mol−1·cm−1, or sometimes in converted SI units of m2·mol−1.
The attenuation coefficient "Σ"' is one of many ways to describe the attenuation of electromagnetic waves. For the others, and their interrelationships, see the article: Mathematical descriptions of opacity. For example, "Σ"' can be expressed in terms of the imaginary part of the refractive index, "κ", and the wavelength of the radiation(in free space), "λ"0, according to
In molecular attenuation spectrometry, the attenuation cross section "σ" is expressed in terms of a linestrength, "S", and an (area-normalized) lineshape function, "Φ". The frequency scale in molecular spectroscopy is often in cm−1, where the lineshape function is expressed in units of 1/cm−1. Since "N" is given as a number density in units of 1/cm3, the linestrength is often given in units of cm2cm−1/molecule. A typical linestrength in one of the vibrational overtone bands of smaller molecules, e.g., around 1.5 μm in CO or CO2, is around 10−23 cm2cm−1, although it can be larger for species with strong transitions, e.g., C2H2. The linestrengths of various transitions can be found in large databases, e.g., HITRAN. The lineshape function often takes a value around a few 1/cm−1, up to around 10/cm−1 under low pressure conditions, when the transition is Doppler broadened, and below this under atmospheric pressure conditions, when the transition is collision broadened. It has also become commonplace to express the linestrength in units of cm−2/atm since then the concentration is given in terms of a pressure in units of atm. A typical linestrength is then often in the order of 10−3 cm−2/atm. Under these conditions, the detectability of a given technique is often quoted in terms of ppm•m.
The fact that there are two commensurate definitions of attenuation (in base 10 or e) implies that the absorbance and the attenuation coefficient for the cases with gases, "A"' and "Σ"', are ln 10 (approximately 2.3) times as large as the corresponding values for liquids, i.e., "A" and "Σ", respectively. Therefore, care must be taken when interpreting data that the correct form of the law is used.
The law tends to break down at very high concentrations, especially if the material is highly scattering. If the radiation is especially intense, nonlinear optical processes can also cause variances. The main reason, however, is the following. At high concentrations, the molecules are closer to each other and begin to interact with each other. This interaction will change several properties of the molecule, and thus will change the attenuation. If the attenuation is different at higher concentrations than at lower ones, then the plot of the attenuation coefficient will not be linear, as is suggested by the equation, so you can only use it when all the concentrations you are working with are low enough that the absorbtivity is the same for all of them.
Derivation.
Classically, the Beer–Lambert law was first devised independently where Lambert's law stated that absorbance is directly proportional to the thickness of the sample, and Beer's law stated that absorbance is proportional to the concentration of the sample. The modern derivation of the Beer–Lambert law combines the two laws and correlate the absorbance to both, the concentration as well as the thickness (path length) of the sample.
In concept, the derivation of the Beer–Lambert law is straightforward. Divide the attenuating sample into thin slices that are perpendicular to the beam of light. The light that emerges from a slice is slightly less intense than the light that entered because some of the photons have run into molecules in the sample and did not make it to the other side. For most cases where measurements of attenuation are needed, a vast majority of the light entering the slice leaves without being attenuated. Because the physical description of the problem is in terms of differences—intensity before and after light passes through the slice—we can easily write an ordinary differential equation model for attenuation. The difference in intensity due to the slice of attenuating material formula_12 is reduced; leaving the slice, it is a fraction formula_13 of the light entering the slice formula_4. The thickness of the slice is formula_15, which scales the amount of attenuation (thin slice does not attenuates much light but a thick slice attenuates a lot). In symbols, formula_16, or formula_17. This conceptual overview uses formula_13 to describe how much light is attenuated. All we can say about the value of this constant is that it will be different for each material. Also, its values should be constrained between −1 and 0. The following paragraphs cover the meaning of this constant and the whole derivation in much greater detail.
Assume that particles may be described as having an attenuation cross section (i.e., area), "σ", perpendicular to the path of light through a solution, such that a photon of light is attenuated if it strikes the particle, and is transmitted if it does not.
Define "z" as an axis parallel to the direction that photons of light are moving, and "A" and "dz" as the area and thickness (along the "z" axis) of a 3-dimensional slab of space through which light is passing.
We assume that "dz" is sufficiently small that one particle in the slab cannot obscure another particle in the slab when viewed along the "z" direction. The concentration of particles in the slab is represented by "N".
It follows that the fraction of photons attenuated (absorbed and scattered away) when passing through this slab is equal to the total opaque area of the particles in the slab, "σAN dz", divided by the area of the slab "A", which yields "σN dz". Expressing the number of photons attenuated by the slab as "dI""z", and the total number of photons incident on the slab as "I""z", the number of photons attenuated by the slab is given by
Note that because there are fewer photons which pass through the slab than are incident on it, "dI""z" is actually negative (It is proportional in magnitude to the number of photons attenuated).
The solution to this simple differential equation is obtained by integrating both sides to obtain "I""z" as a function of "z"
The difference of intensity for a slab of real thickness ℓ is "I"0 at "z" = 0, and "I"l at "z" = "ℓ". Using the previous equation, the difference in intensity can be written as,
rearranging and exponentiating yields,
This implies that
and
The quantity Σ is called the total macroscopic cross section or attenuation coefficient, depending on the topic (for example in respectively the first term is used transport theory and the second one in shielding and radiation protection).
The derivation assumes that every attenuating particle behaves independently with respect to the light and is not affected by other particles. While it is commonly thought that error is introduced when particles are lying along the same optical path such that some particles are in the "shadow" of others, this is actually a key part of the derivation and why integration is used.
When the path taken is long enough to make the medium attenuation coefficient not uniform, the original equation must be modified as follows:
where z is the distance along the path through the medium, all other symbols are as defined above. This is taken into account in each formula_26 in the atmospheric equation above.
Deviations from Beer–Lambert law.
Under certain conditions Beer–Lambert law fails to maintain a linear relationship between attenuation and concentration of analyte. These deviations are classified into three categories:
Prerequisites.
There are at least six conditions that need to be fulfilled in order for Beer’s law to be valid. These are:
If any of these conditions are not fulfilled, there will be deviations from Beer’s law.
Chemical analysis.
Beer's law can be applied to the analysis of a mixture by spectrophotometry, without the need for extensive pre-processing of the sample. An example is the determination of bilirubin in blood plasma samples. The spectrum of pure bilirubin is known, so the molar attenuation coefficient is known. Measurements are made at one wavelength that is nearly unique for bilirubin and at a second wavelength in order to correct for possible interferences.The concentration is given by "c" = "A"corrected / "ε".
For a more complicated example, consider a mixture in solution containing two components at concentrations "c"1 and "c"2. The absorbance at any wavelength, λ is, for unit path length, given by
Therefore, measurements at two wavelengths yields two equations in two unknowns and will suffice to determine the concentrations "c"1 and "c"2 as long as the molar absorbances of the two components, "ε"1 and "ε"2 are known at both wavelengths. This two system equation can be solved using Cramer's rule. In practice it is better to use linear least squares to determine the two concentrations from measurements made at more than two wavelengths. Mixtures containing more than two components can be analyzed in the same way, using a minimum of "n" wavelengths for a mixture containing "n" components.
The law is used widely in infra-red spectroscopy and near-infrared spectroscopy for analysis of polymer degradation and oxidation (also in biological tissue). The carbonyl group attenuation at about 6 micrometres can be detected quite easily, and degree of oxidation of the polymer calculated.
Beer–Lambert law in the atmosphere.
This law is also applied to describe the attenuation of solar or stellar radiation as it travels through the atmosphere. In this case, there is scattering of radiation as well as absorption. The Beer–Lambert law for the atmosphere is usually written
where each formula_26 is the optical depth whose subscript identifies the source of the absorption or scattering it describes:
formula_41 is the "optical mass" or "airmass factor", a term approximately equal (for small and moderate values of formula_42) to formula_43, where formula_42 is the observed object's zenith angle (the angle measured from
the direction perpendicular to the Earth's surface at the observation site).
This equation can be used to retrieve formula_45, the aerosol optical thickness,
which is necessary for the correction of satellite images and also important in accounting for the role of
aerosols in climate.
History.
The law was discovered by Pierre Bouguer before 1729. It is often attributed to Johann Heinrich Lambert, who cited Bouguer's "Essai d'Optique sur la Gradation de la Lumiere" (Claude Jombert, Paris, 1729) — and even quoted from it — in his "Photometria" in 1760. Much later, August Beer extended the exponential attenuation law in 1852 to include the concentration of solutions in the attenuation coefficient.

</doc>
<doc id="4477" url="http://en.wikipedia.org/wiki?curid=4477" title="The Beach Boys">
The Beach Boys

The Beach Boys are an American rock band, formed in Hawthorne, California in 1961. The group's original lineup consisted of brothers Brian, Dennis and Carl Wilson, their cousin Mike Love, and friend Al Jardine. Emerging at the vanguard of the "California Sound", the band's early music gained international popularity for their distinct vocal harmonies and lyrics reflecting a southern California youth culture of surfing, cars, and romance. Influenced by doo wop, 1950s rock and roll, and vocal jazz quartets, Brian led the band to experiment with several genres ranging from pop ballads to psychedelic and baroque while devising novel approaches to music production and jazz harmony arrangements. While initially managed by the Wilsons' father Murry, Brian's creative ambitions and sophisticated songwriting abilities dominated the group's musical direction.
Released in 1966, the "Pet Sounds" album and "Good Vibrations" single featured an intricate and multi-layered sound that represented a departure from the simple surf rock of the Beach Boys' early years. Soon after, Brian gradually ceded control to the rest of the band, reducing his input due to mental health and substance abuse issues. Though the more democratic incarnation of the Beach Boys recorded a string of albums in various musical styles that garnered international critical success, the group struggled to reclaim their commercial momentum in America. Since the 1980s, much-publicized legal wrangling over royalties, songwriting credits and use of the band's name transpired. Dennis drowned in 1983 and Carl died of lung cancer in 1998. After Carl's death, many live configurations of the band fronted by Mike Love and Bruce Johnston continued to tour into the 2000s while other members pursued solo projects. For the band's 50th anniversary, the surviving co-founders briefly reunited for a new studio album and world tour.
The Beach Boys are often touted "America's Band", and AllMusic stated that their "unerring ability…made them America's first, best rock band." The group had over eighty songs chart worldwide, thirty-six of them United States Top 40 hits (the most by an American rock band), four reaching number-one on the ""Billboard" Hot 100" chart. The Beach Boys have sold in excess of 100 million records worldwide, making them one of the world's best-selling bands of all time and are listed at number 12 on "Rolling Stone" magazine's 2004 list of the "100 Greatest Artists of All Time". The core quintet of the three Wilsons, Love and Jardine were inducted into the Rock and Roll Hall of Fame in 1988.
1958–66: Brian Wilson era.
Formation.
At the time of his sixteenth birthday on June 20, 1958, Brian Wilson shared a bedroom with his brothers, Dennis and Carl – aged thirteen and eleven, respectively – in their family home in Hawthorne. He had watched his father, Murry Wilson, play piano, and had listened intently to the harmonies of vocal groups such as the Four Freshmen. After dissecting songs such as "Ivory Tower" and "Good News", Brian would teach family members how to sing the background harmonies. For his birthday that year, Brian was given a reel-to-reel tape recorder. He learned how to overdub, using his vocals and those of Carl and their mother. Brian would play piano with Carl and David Marks, an eleven-year-old longtime neighbor, playing the guitars they had each received as Christmas presents.
Soon Brian was avidly listening to Johnny Otis on his KFOX radio show with Carl. Inspired by the simple structure and vocals of the rhythm and blues songs he heard, he changed his piano-playing style and started writing songs. His enthusiasm interfered with his music studies at school. Family gatherings brought the Wilsons in contact with cousin Mike Love. Brian taught Love's sister Maureen and a friend harmonies. Later, Brian, Mike Love and two friends performed at Hawthorne High School. Brian also knew Al Jardine, a high school classmate who had already played guitar in a folk group called the Islanders. Brian suggested to Jardine that they team up with his cousin and brother Carl. It was at these sessions, held in Brian's bedroom, that "the Beach Boys sound" began to form. Love encouraged Brian to write songs and gave the fledgling band its name: "The Pendletones", a portmanteau of "Pendleton", a style of woolen shirt popular at the time and "tone", the musical term. Although surfing motifs were very prominent in their early songs, Dennis was the only avid surfer in the group. He suggested that the group compose songs celebrating the sport and the lifestyle which had developed around it within Southern California.
Jardine and a singer friend, Gary Winfrey, went to Brian to see if he could help out with a version of a folk song they wanted to record—"Sloop John B". In Brian's absence, the two spoke with their father, a music industry veteran of modest success. Murry arranged for the Pendletones to meet his publisher, Hite Morgan. The group performed a slower ballad, "Their Hearts Were Full of Spring", but failed to impress Morgan. After an awkward pause, Dennis mentioned they had an original song, "Surfin. Brian finished the song, and together with Mike Love, wrote "Surfin' Safari". The group rented guitars, drums, amplifiers and microphones, and practiced for three days while the Wilsons' parents were on a short vacation. In October 1961, the Pendletones recorded the two surfing song demos in twelve takes at Keen Recording Studio. Murry brought the demos to Herb Newman, owner of Candix Records and Era Records, and he signed the group on December 8, 1961. When the boys eagerly unpacked the first box of singles – released both under the Candix label, and also as a promo issue under X Records (Morgan's label) – they were shocked to see their band had been renamed as the Beach Boys. Murry Wilson called Morgan and learned that Candix wanted to name the group the Surfers to directly associate them with the increasingly-popular teen sport. But Russ Regan, a young promoter with Era Records – who later became president of 20th Century Fox Records – noted that there already existed a group by that name, and he suggested calling them the Beach Boys.
Beach-themed period.
Released in December 1961, "Surfin" was soon aired on KFWB and KRLA, two of Los Angeles' most influential teen radio stations. It was a hit on the West Coast, going to number three in Southern California, and peaked at number 75 on the national pop charts. By the final weeks of 1961 "Surfin had sold more than 40,000 copies. Murry Wilson told the boys he did not like "Surfin. By now the de facto manager of the Beach Boys, he landed the group's first paying gig (for which they earned $300) on New Year's Eve, 1961, at the Ritchie Valens Memorial Dance in Long Beach, headlined by Ike & Tina Turner. Brian recalls how he wondered what they were doing there: "five clean-cut, unworldly white boys from a conservative white suburb, in an auditorium full of black kids". Brian describes the night as an "education"—he knew afterwards that success was all about "R&B, rock and roll, and money".
Although Murry effectively seized managerial control of the band, Brian acknowledged that he "deserves credit for getting us off the ground... he hounded us mercilessly... [but] also worked hard himself". In the first half of February 1962, Jardine left the band and was replaced by Marks. The band recorded two more originals on April 19 at Western Studios, Los Angeles; "Lonely Sea" and "409", also re-recording "Surfin' Safari". On June 4, the band released their second single "Surfin' Safari" backed with "409". The release prompted national coverage in the June 9 issue of "Billboard" where the magazine praised Love's lead vocal and deemed the song to have strong hit potential. After being turned down by Dot and Liberty, the Beach Boys eventually signed a seven-year contract with Capitol Records on July 16 based on the strength of the June demo session. This was at the urging of Capitol exec Nik Venet who signed the group, finding them to be the "teenage gold" he had been scouting. By November, their first album was ready—"Surfin' Safari" which reached 32 on the US Billboard charts. Their song output continued along the same commercial line, focusing on California youth lifestyle.
In January 1963, three months after the release of their debut album, the band began recording their sophomore effort, "Surfin' U.S.A.", placing a greater emphasis on surf rock instrumentals and tighter production. It has been hypothesized that the shift to a sound more typical of the surf rock genre was in response to the Californian surfer locals who were dismissive of the band's debut as it strayed from the sound of other surf acts. The LP was the start of Brian's penchant for doubletracking vocals. It was a pioneering innovation which provided the Beach Boys with an exceptionally bright sound.
After the moderate success of "Surfin' Safari", "Surfin' U.S.A.", released on March 25, 1963 met a more enthusiastic reception, reaching number two on the Billboard charts and propelling the band into a nationwide spotlight. Five days prior to the release of "Surfin' U.S.A." Brian produced "Surf City", a song he had written for Jan and Dean. "Surf City" hit number one on the "Billboard" charts in July 1963, a development that pleased Brian but angered Murry, who felt his son had "given away" what should have been the Beach Boys' first chart-topper.
At the beginning of a tour of the Mid-West in April 1963, Jardine rejoined the Beach Boys at Brian's request. As he began playing live gigs again, Brian left the road to focus on writing and recording. The result of this arrangement produced the albums "Surfer Girl", released on September 16, 1963 and "Little Deuce Coupe", released less than a month later on October 7, 1963. This sextet incarnation of the Beach Boys didn't extend beyond these two albums, as Marks officially left the band in early October due to conflict with manager Murry, pulling Brian back into touring.
Around this time, Brian began using members of the Wrecking Crew to augment his increasingly demanding studio arrangements. Session musicians that participated on Wilson's productions were said to have been awestruck by his musical abilities, as drummer Hal Blaine explained, "We all studied in conservatories; we were trained musicians. We thought it was a fluke at first, but then we realized Brian was writing these incredible songs. This was not just a young kid writing about high school and surfing." For composer Frank Zappa, the most exciting thing to him in "white-person-music" was when the Beach Boys used the progression V–II on "Little Deuce Coupe", calling it "an important step forward by going backward."
Following a successful Australasian tour in January and February 1964, the band returned home to face the British Invasion through the Beatles appearances on the Ed Sullivan Show. Reportedly, Brian wanted more time to complete their next album, yet their record label insisted they finish recording swiftly to avoid being forgotten in the throes of the impending invasion. Satisfying these demands, the band hastily finished the sessions on February 20, 1964 and titled the album "Shut Down Volume 2". Critics have found evaluating the album's worth difficult through the years. Though songs like "The Warmth of the Sun" and "Don't Worry Baby" are widely acclaimed and seen as impressive milestones in the artistic growth of the band.
In April 1964, during recording of the single "I Get Around", Murry was relieved of his duties as manager. Brian reflected, "We love the family thing – y'know: three brothers, a cousin and a friend is a really beautiful way to have a group – but the extra generation can become a hang-up". When the single was released in May of that month, it would climb to number one, their first single to do so. Two months later, the album that the song later appeared on, "All Summer Long", reached number four on the Billboard 200 charts. "All Summer Long" introduced exotic textures to the Beach Boys' sound exemplified by the piccolos and xylophones of its title track. The album was a swan-song to the surf and car music the Beach Boys built their commercial standing upon. Later albums took a different stylistic and lyrical path.
"Today!" and "Summer Days".
By the end of 1964, the stress of road travel, composing, producing and maintaining a high level of creativity became too much for Brian Wilson. On December 23, while on a flight, he suffered a panic attack and left the tour. In January 1965, he announced his withdrawal from touring to concentrate entirely on songwriting and record production. For the rest of 1964 and into 1965, Glen Campbell served as Wilson's temporary replacement in concert, until his own career success pulled him from the group in April 1965. Bruce Johnston was asked to locate a replacement for Campbell; having failed to find one, Johnston himself became a full-time member of the band on May 19, 1965, first replacing Brian on the road and later contributing in the studio, beginning with the vocal sessions for "California Girls" on June 4, 1965.
After Brian relinquished touring, he became a full-time studio artist, showcasing a great leap forward with "The Beach Boys Today!", an album containing a suite-like structure divided by songs and ballads which portended the Album Era with its cohesive artistic statement. During the recording sessions for "Today!", Love told Melody Maker that he and the band wanted to look beyond surf rock and to avoid living in the past or resting on their laurels. The resulting LP had largely guitar-oriented pop songs such as "Dance, Dance, Dance" and "Good to My Baby" on side A with B-side ballads such as "Please Let Me Wonder" and "She Knows Me Too Well".
"Today!" marked a maturation in the Beach Boys' lyric content by abandoning themes related to surfing, cars, or teenage love. Some love songs remained, but with a marked increase in depth, along with introspective tracks accompanied by adventurous and distinct arrangements.
In June 1965, the band released "Summer Days (And Summer Nights!!)". The album included a reworked arrangement of "Help Me, Rhonda" which had become the band's second number one single in the spring of 1965, displacing the Beatles' "Ticket to Ride". "Let Him Run Wild" tapped into the youthful angst that would later pervade their music. In November 1965, the group followed up their US number-three-charting "California Girls" from "Summer Days (And Summer Nights!!)" with another top-twenty single, "The Little Girl I Once Knew". It was considered the band's most experimental statement thus far, using silence as a pre-chorus, clashing keyboards, moody brass and vocal tics. The single continued Brian's ambitions for daring arrangements, featuring unexpected tempo changes and numerous false endings. Perhaps too extreme an arrangement to go much higher than its number 20 peak, it was the band's second single not to reach the top ten since their 1962 breakthrough. In December they scored an unexpected number two hit (number three in the UK) with "Barbara Ann", which Capitol released as a single with no band input. A cover of a 1961 song by the Regents, it became one of the Beach Boys' most recognized hits.
"Pet Sounds", "Good Vibrations", and "Smile".
In 1966, the Beach Boys formally established their use of unconventional instruments and elaborate layers of vocal harmonies on their groundbreaking record "Pet Sounds". It is considered Brian's most concise demonstration of his production and songwriting expertise. With songs such as "Wouldn't It Be Nice" and "Sloop John B", the album's innovative soundscape incorporates elements of pop, jazz, classical, exotica, and the avant-garde. The instrumentation combines found sounds such as bicycle bells and dog whistles with classically-inspired orchestrations and the usual rock set-up of drums and guitars; among others, silverware, accordions, plucked piano strings, barking dogs, and plastic water jugs. For the basic rhythmic feel for "God Only Knows", harpsichord, piano with slapback echo, sleigh bells, and strings spilled into each other to create a rich blanket of sound. It is considered by some as a Brian Wilson solo album in all but name, as other members contributed relatively little to the compositions or recordings.
Despite the critical praise it received, "Pet Sounds" was indifferently promoted by Capitol and failed to become the major hit Wilson had hoped it would be. Its failure to gain wider recognition in the US hurt him deeply. It was assumed that the label considered the album a risk, appealing more to an older demographic than the younger, female audience the Beach Boys built their commercial standing on. "Pet Sounds" reached number ten in the US and number two in the UK, an accomplishment which helped the Beach Boys become the strongest selling album act in the UK for the final quarter of 1966; dethroning the three-year reign of native bands such as the Beatles. In a 1972 review of the album, music journalist Stephen Davis wrote,
"Pet Sounds" went on to be acknowledged as an important historical and cultural work. In "The Album: A Guide to Pop Music's Most Provocative, Influential, and Important Creations", author James Perone championed the album for its complex orchestrations, sophisticated compositions, and varied tone colors, calling it a remove from "just about anything else that was going on in 1966 pop music." Beyond pop and rock, "Pet Sounds" expanded the field of music production. It was massively influential upon its release, vaunting the band to the top level of rock innovators. According to Brian, the album was designed as a collection of art pieces which belong together yet could stand alone. It is one of the earliest rock concept albums, and one of the earliest concept albums of the counterculture era. Many felt that in the realms of pop and rock music, the album set a higher standard. As an early album in the emerging psychedelic rock style, it signaled a turning point wherein rock, which previously had been considered dance music, became music that was made for listening to. Influenced by psychedelic drugs, Brian turned inward and probed his deep-seated self-doubts and emotional longings; the piece did not address the problems in the world around them, unlike other psychedelic rock groups. Instead, as Jim Miller wrote of the album's tone, "[It] vented Wilson's obsession with isolation cataloging a forlorn quest for security. The whole enterprise, which smacked of song cycle pretensions, was streaked with regret and romantic lagour."
The album remains an evocative release, with distinctive lushness and melancholy. In 1976, journalist Robin Denselow wrote: "With the 1966 "Pet Sounds" album … Wilson had become America's equivalent of the Beatles with his ability to expand the limits of popular taste." Paul McCartney named it one of his favorite albums of all time on multiple occasions, calling it the primary impetus for the Beatles' album "Sgt. Pepper's Lonely Hearts Club Band" (1967). In 2003, "Pet Sounds" was ranked second in "The 500 Greatest Albums of All Time" list selected by "Rolling Stone", behind only "Sgt. Pepper".
Seeking to expand on "Pet Sounds"' advances, Wilson began an even more ambitious project: "Good Vibrations". Like "Pet Sounds", Brian opted for an eclectic array of instruments rarely heard in pop music. Described by Brian as a "pocket symphony," it contains a mixture of classical, rock, and exotic instruments structured around a cut-up mosaic of musical sections represented by several discordant key and modal shifts. it became the Beach Boys' biggest hit to date and a US and UK number one single in 1966. Coming at a time when pop singles were usually made in under two hours, it was one of the most complex pop productions ever undertaken, and the most expensive single ever recorded to that point. The production costs were estimated between $50,000 and $75,000 ($ and $ today) with sessions for the song stretching over several months in at least four major studios. According to Domenic Priore, the making of "Good Vibrations" was unlike anything previous in the realms of classical, jazz, international, soundtrack or any other kind of recording. It was an unequivocal milestone in studio productions, and continued in establishing Brian as an extender of popular tastes. To the counterculture of the 1960s, "Good Vibrations" served as an anthem. Rock critic Gene Sculatti prophesied in 1968, "[It] may yet prove to be the most significantly revolutionary piece of the current rock renaissance."
Its instrumentation included the Tannerin, an easier-to-manipulate version of a theremin which helped the Beach Boys claim a new hippie audience. Upon release, the single prompted an unexpected revival in theremins while increasing awareness of analog synthesizers, leading Moog Music to produce their own brand of ribbon controlled instruments.
Brian met musician and songwriter Van Dyke Parks while working on "Pet Sounds". A year later while in the midst of recording "Good Vibrations", the duo began an intense collaboration that resulted in a suite of challenging new songs for the Beach Boys forthcoming album "Smile", intended to surpass "Pet Sounds". Recording for the album spanned about a year, from 1966 to 1967. It is known that Wilson and Parks intended "Smile" to be a continuous suite of songs that were linked both thematically and musically, with the main songs being linked together by small vocal pieces and instrumental segments that elaborated upon the musical themes of the major songs. Surviving recordings have shown that the music incorporated chanting, cowboy songs, explorations in Indian and Hawaiian music, jazz, tone poems with classical elements, cartoon sound effects, "musique concrète", and yodeling.
"Smile" would go on to become the most legendary unreleased album in the history of popular music. In the decades following its non-release, it became the subject of intense speculation and mystique. Many believe that, had the album been released, it would have substantially altered the group's direction, establishing them at the vanguard of rock innovators. Writing about the album for the "33 1/2" book series, Luis Sanchez believed: "If Brian Wilson and The Beach Boys were going to survive as the defining force of American pop music they were, "Smile" was a conscious attempt to rediscover the impulses and ideas that power American consciousness from the inside out. It was a collaboration that led to some incredible music, which, if it had been completed as an album and delivered to the public in 1966, might have had an incredible impact."
Many factors combined to put intense pressure on Brian Wilson as "Smile" neared completion: his own mental instability, the pressure to create against fierce internal opposition to his new music, the relatively unenthusiastic response to "Pet Sounds" in the United States, Carl Wilson's draft resistance, and a major dispute with Capitol Records. Furthermore, Wilson's reliance on both prescription drugs and amphetamines exacerbated his underlying mental health problems. Comparable to Brian Jones and Syd Barrett, Brian Wilson's use of psychedelic drugs—especially LSD—led to a nervous breakdown in the late-1960s. As his legend grew, the "Smile" period came to be seen as the pivotal episode in his decline and he became tagged as one of the most notorious celebrity drug casualties of the rock era.
1967–75: the Beach Boys as a democratic unit.
"Smiley Smile" and "Wild Honey".
Some "Smile" tracks were salvaged and re-recorded in scaled-down versions at Brian's new home studio. Along with the single version of "Good Vibrations", these tracks were released on "Smiley Smile", an album which elicited positive critical and commercial response abroad, but was the first real commercial failure for the group in the United States. By this time the Beach Boys' management (Nick Grillo and David Anderle) had created the band's own record label, Brother. One of the first labels to be owned by a rock group, Brother Records was intended for releases of Beach Boys side projects, and as an invitation to new talent. The initial output of the label, however, was limited to "Smiley Smile" and two resulting singles from the album; the failure of "Gettin' Hungry" caused the band to shelve Brother until 1970. Despite the cancellation of "Smile", several tracks—including "Our Prayer", "Cabin Essence" and "Surf's Up"—continued to trickle out in later albums often as filler songs to offset Brian's unwillingness to contribute. The band was still expecting to complete and release "Smile" as late as 1973 before it became clear that only Brian could comprehend the endless fragments that had been recorded. "Smiley Smile" was followed up three months later with "Wild Honey", featuring songs written by Wilson and Love, including the hit "Darlin'" and a rendition of Stevie Wonder's "I Was Made to Love Her". The album fared better than its predecessor, reaching number 24 in the US.
Compounding the group's recent setbacks, their public image took a cataclysmic hit following their withdrawal from the 1967 Monterey Pop Festival for the reason that they had no new material to play while their forthcoming single and album lay in limbo. Their cancellation was seen as "a damning admission that they were washed up [and] unable to compete with the 'new music'". This notion was exacerbated by "Rolling Stone" writer Jann Wenner, whom within contemporary publications criticized Brian Wilson for his oft-repeated "genius" label which he called a "promotional shuck" and an attempt to compare with the Beatles. However, Wenner later responded to their "Wild Honey" album with more optimism, remarking two months later that "[i]n any case it's good to see that the Beach Boys are getting their heads straight once again".
Former band publicist Derek Taylor later recalled a conversation with Brian and Dennis where they denied that the group had ever written surf music or songs about cars, and that the Beach Boys had never been involved with the surf and hot rod fads, as Taylor claimed, "…they would not concede." As a result of their initial target demographic and subsequent failures to blend with the hippie movement, the group was viewed as unhip relics, even though they had once been, as biographer Peter Ames Carlin wrote, "the absolute center of the American rock ’n’ roll scene," a time when they had ushered the psychedelic era. In early 1969, Brian proposed that the group change their name from "the Beach Boys" to "the Beach", reasoning for the simple fact that the band members were now grown men. Going to the effort of acquiring a contract which would declare a five-way agreement to officially rename the group, Stephen Desper reported, "They all just kind of shrugged and said, 'Aw, come on, Brian, we don't wanna do that. That's how the public knows us, man. And that was it. He put the paper on the piano and it stayed there until I picked it up and took it away."
"Friends", "20/20", and "Sunflower".
After meeting Maharishi Mahesh Yogi at a UNICEF Variety Gala in Paris, France on December 15, 1967, Love, along with other high-profile celebrities such as Donovan and the Beatles traveled to Rishikesh in India during February and March 1968. The following Beach Boys album "Friends" (1968) had songs influenced by the Transcendental Meditation taught by the Maharishi. The album reached number 13 in the UK and 126 in the US, the title track placing at number 25 in the UK and number 47 in the US, the band's lowest singles peak since 1962. In support of the "Friends" album, Love had arranged for the Beach Boys to tour with the Maharishi in the US, which has been called "one of the more bizarre entertainments of the era". Starting on May 3, 1968, the tour lasted five shows and was cancelled when the Maharishi had to withdraw to fulfill film contracts. Due to disappointing audience numbers and the Maharishi's withdrawal, twenty-four tour dates were subsequently cancelled at a cost estimated at US$250,000 (approximately US$ today) for the band. This tour was followed by the release of "Do It Again", a single which critics described as an update of the Beach Boys' surf rock past in a late-1960's style. The single went to the top of the Australian and UK single charts in 1968 and was moderately successful in the US, peaking at number 20.
For a short time in mid-1968, Brian Wilson sought psychological treatment in hospital. During his absence, other members began writing and producing material themselves. To complete their contract with Capitol, they produced one more album. "20/20" (1969) was one of the group's most stylistically diverse albums, including hard rock songs such as "All I Want to Do", the waltz-based "Time to Get Alone" and a remake of the Ronettes' "I Can Hear Music". The diversity of genres have been described as an indicator that the group was trying to establish an updated identity. The album performed strongly in the UK, reaching number three on the charts. In the US, the album reached a modest 68.
In spring 1968, Dennis began a tumultuous relationship with musician Charles Manson which persisted for several months afterward. Dennis bought him time at Brian's home studio where recording sessions were attempted while Brian stayed in his room. It was then proposed by Dennis that Manson be signed to Brother Records, though Brian reportedly disliked Charlie, and so a deal was never made. Without Manson's involvement, the Beach Boys did record one song penned by Manson: "Cease to Exist" rewritten as "Never Learn Not To Love". The idea of the Beach Boys recording one of his songs reportedly thrilled Manson, and it was released as a Beach Boys single. After accruing a large monetary debt to the group, Dennis deliberately omitted Manson's credit on its release while also altering the song's arrangement and lyrics. This greatly angered Manson. Growing fearful, Dennis gradually distanced himself from Manson, whose family had taken over his home. He was eventually convicted for murder conspiracy; from there on, Dennis was too afraid of the Manson family to ever speak publicly on his relationship, let alone testify against him.
On April 12, 1969, the band revisited their 1967 lawsuit against Capitol Records after they alleged an audit undertaken revealed the band were owed over US$2,000,000 (US$ today) for unpaid royalties and production duties. The band's contract with Capitol Records expired on June 30, 1969, after which Capitol Records deleted the Beach Boys' catalog from print, effectively cutting off their royalty flow. In November 1969, Murry Wilson sold Sea of Tunes, the Beach Boys' catalogue, to Irving Almo Music, a decision which according to Marilyn Wilson "devastated Brian". In late 1969, the Beach Boys reactivated their Brother label and signed with Reprise. Around this time, the band commenced recording for a new album. At the time the Beach Boys tenure ended with Capitol in 1969, they had sold 65 million records worldwide, closing the decade as the most commercially successful American group in popular music.
In 1970, armed with the new Reprise contract, the band appeared rejuvenated, releasing the album "Sunflower" to critical acclaim. The album features a strong group presence with significant writing contributions from all band members. Brian was active during this period, writing or co-writing seven of the twelve songs on "Sunflower" and performing at half of the band's domestic concerts in 1970. "Sunflower" reached number 29 in the UK and number 151 in the US, the band's lowest domestic chart showing to that point. A version of "Cottonfields" arranged by Al Jardine appeared on European releases of "Sunflower" and as a single, reached number one in Australia, Norway, South Africa and Sweden and the top-five in six other countries, including the UK.
"Surf's Up", "Carl and the Passions", and "Holland".
After "Sunflower", the band hired Jack Rieley as their manager. Under Rieley's management, the group's music began emphasizing political and social awareness. During this time, Carl Wilson gradually assumed leadership of the band and Rieley contributed lyrics. On August 30, 1971 the band released "Surf's Up", named after the Brian Wilson/Van Dyke Parks composition "Surf's Up". The album was moderately successful, reaching the US top 30, a marked improvement over their recent releases. While the record charted, the Beach Boys added to their renewed fame by performing a near-sellout set at Carnegie Hall, followed by an appearance with the Grateful Dead at Fillmore East on April 27, 1971. The live shows during this era included reworked arrangements of many of the band's previous songs. A large portion of their set lists culled from "Pet Sounds" and "Smile", as author Domenic Priore observes, "They basically played what they could have played at the Monterey Pop Festival in the summer of 1967."
Johnston ended his first stint with the band shortly after "Surf's Up"'s release, reportedly because of friction with Rieley. At Carl's suggestion, the addition of Ricky Fataar and Blondie Chaplin in February 1972 led to a dramatic restructuring in the band's sound. The album "Carl and the Passions – "So Tough"" was an uncharacteristic mix that included two songs written by Fataar and Chaplin. For their next project the band, their families, assorted associates and technicians moved to the Netherlands for the summer of 1972. They rented a farmhouse to convert into a makeshift studio where recording sessions for the new project would take place. By the end of their sessions, the band felt they had produced one of their strongest efforts yet. Reprise, however, felt that the album required a strong single. This resulted in the song "Sail On, Sailor", a collaboration between Brian Wilson, Tandyn Almer, Ray Kennedy, Jack Rieley and Van Dyke Parks featuring a soulful lead vocal by Chaplin. Reprise subsequently approved and the resulting album, "Holland", was released early in 1973, peaking at number 37. Brian's musical children story, "Mount Vernon and Fairway (A Fairy Tale)", narrated by Rieley and strongly directly influenced by Randy Newman's "Sail Away" album, was included as a bonus EP. Despite indifference from Reprise, the band's concert audience started to grow.
"The Beach Boys in Concert", a double album documenting the 1972 and 1973 US tours, was another top-30 album and became the band's first gold record under Reprise. During this period the band established itself as one of America's most popular live acts. Chaplin and Fataar helped organize the concerts to obtain a high quality live performance, playing material off "Surf's Up", "Carl and the Passions" and "Holland" and adding songs from their older catalog. This concert arrangement lifted them back into American public prominence. In late 1973, the soundtrack to American Graffiti, "41 Original Hits from the Soundtrack of American Graffiti", was released to mass commercial and critical success. The soundtrack included early Beach Boy songs "Surfin' Safari" and "All Summer Long" and was a catalyst in creating a wave of nostalgia that reintroduced the Beach Boys into contemporary American consciousness. In 1974, Capitol Records issued "Endless Summer", the band's first major pre-"Pet Sounds" greatest hits package. The record sleeve's sunny, colorful graphics caught the mood of the nation and surged to the top of the "Billboard" album charts. It was the group's first multi-million selling record since "Good Vibrations", and remained on the album chart for two years. The following year, Capitol released a second compilation, "Spirit of America", which also sold well. With these compilations, the Beach Boys became one of the most popular acts in rock, propelling themselves from opening for Crosby, Stills, Nash and Young to headliners selling out basketball arenas in a matter of weeks. "Rolling Stone" named the Beach Boys the "Band of the Year" for 1974, solely on the basis of their juggernaut touring schedule and material written over a decade earlier.
Rieley, who remained in the Netherlands after "Holland"s release, was relieved of his managerial duties in late 1973. Chaplin also left in late 1973 after an argument with Steve Love, the band's business manager (and Mike's brother). Fataar remained until 1974, when he was offered a chance to join a new group led by future Eagles member Joe Walsh. Chaplin's replacement, James William Guercio, started offering the group career advice that resulted in his becoming their new manager. Under Guercio, the Beach Boys staged a highly successful 1975 joint concert tour with Chicago, with each group performing some of the other's songs, including their previous year's collaboration on Chicago's hit "Wishing You Were Here". Beach Boys vocals were also heard on Elton John's 1974 hit "Don't Let the Sun Go Down on Me". Nostalgia had settled into the Beach Boys' hype; the group had not officially released any new material since 1973's "Holland". While their concerts continuously sold out, the stage act slowly changed from a contemporary presentation followed by oldies encores to an entire show made up of mostly pre-1967 music.
1976–77: second Brian Wilson era.
"15 Big Ones" (1976) marked Brian's return as a major force in the group. The album included new songs by Brian, as well as cover versions of oldies such as "Rock and Roll Music" (#5), "Blueberry Hill", and "In the Still of the Night". Brian and Love's "It's O.K." was in the vein of their early sixties style and was a moderate hit. The album was publicized by an August 1976 NBC-TV special, simply titled "The Beach Boys". The special, produced by "Saturday Night Live" (SNL) creator Lorne Michaels, featured appearances by "SNL" cast members John Belushi and Dan Aykroyd.
For the remainder of 1976 to early 1977, Brian Wilson spent his time making sporadic public appearances and producing the band's next album "Love You" (1977), a quirky collection of 14 songs mostly written, arranged and produced by Brian. Brian revealed to Peter Ames Carlin that "Love You" is one of his favorite Beach Boys releases, telling him "That's when it all happened for me. That's where my heart lies." "Love You" peaked at number 28 in the UK and number 53 in the US and developed a cult following; regarded as one of the band's best albums by fans and critics alike.
After "Love You" was released, Brian began to record and assemble "Adult/Child" an effort largely consisting of songs written by Wilson from 1976 and 1977 with select big band arrangements by Dick Reynolds. Though publicized as the Beach Boys' next release, "Adult/Child" reportedly caused tension within the group and was ultimately shelved. Following this period, his concert appearances with the band gradually diminished and their performances were occasionally erratic.
The internal wrangling came to a head after a show at Central Park on September 1, 1977, when the band effectively split into two camps; Dennis and Carl Wilson on one side, Mike Love and Al Jardine on the other with Brian remaining neutral. Following a confrontation on an airport tarmac, the band broke up for two and a half weeks, until a band meeting on September 17, at Brian's house. In light of a potential new Caribou Records the parties negotiated a settlement resulting in Love gaining control of Brian's vote in the group, allowing Love and Jardine to outvote Carl and Dennis Wilson on any matter.
1978–present: fluctuating control.
Group infighting.
The Beach Boys' last album for Reprise, "M.I.U. Album" (1978), was recorded at Maharishi International University in Iowa at the suggestion of Love. Dennis and Carl made limited contributions; the album was mostly produced by Jardine and Ron Altbach, with Brian appearing as "Executive Producer". "M.I.U." was largely a contractual obligation to finish out their association with Reprise, who likewise did not promote the result. The record cemented the divisions in the group. Love and Jardine focused on rock and roll-oriented material while Carl and Dennis chose the progressive focus they had established with the albums "Carl and the Passions" and "Holland". Dennis withdrew from the group to focus on his second solo album and follow-up to "Pacific Ocean Blue" entitled "Bambu". However, alcoholism and marital problems overcame all three Wilson brothers and "Bambu" was shelved. Carl appeared intoxicated during concerts (notably at appearances on their disastrous 1978 Australia tour) and Brian gradually slid back into addiction and an unhealthy lifestyle.
After departing Reprise, the Beach Boys signed with CBS Records. They received a substantial advance and were paid $1 million per album even as CBS deemed their preliminary review of the band's first product, "L.A. (Light Album)" as unsatisfactory. Faced with the realization that Brian was unable to contribute, the band recruited Johnston as producer. The result paid off, as "Good Timin'" became a top 40 single. The group enjoyed moderate success with a disco reworking of the "Wild Honey" song "Here Comes the Night" which was followed by their highest charting UK single in nine years: Jardine's "Lady Lynda" peaked at #6 in the UK Singles Chart. The album was followed in 1980 by "Keepin' the Summer Alive", with Johnston once again producing. Carl was the only Wilson to influence the finished product. Brian managed to contribute several ideas, as seen in the "Going Platinum" television special documenting the album's release, but was otherwise "persona non grata".
In 1981, Carl quit the group due to unhappiness with the band's nostalgia format and lackluster live performances, subsequently pursuing a solo career. He returned in May 1982 — after approximately 14 months of being away — on the condition that the group reconsider their rehearsal and touring policies, along with refraining from "Las Vegas-type engagements".
From 1980 through 1982, the Beach Boys and The Grass Roots performed Independence Day concerts at the National Mall in Washington, D.C., attracting large crowds. However, in April 1983, James G. Watt, President Ronald Reagan's Secretary of the Interior, banned Independence Day concerts on the Mall by such groups. Watt said that "rock bands" that had performed on the Mall on Independence Day in 1981 and 1982 had encouraged drug use and alcoholism and had attracted "the wrong element", who would mug attendees. During the ensuing uproar, which included over 40,000 complaints to the Department of the Interior, the Beach Boys stated that the Soviet Union, which had invited them to perform in Leningrad in 1978, "obviously … did not feel that the group attracted the wrong element". Vice President George H. W. Bush said of the Beach Boys, "They're my friends and I like their music". Watt later apologized to the band after learning that President Reagan and First Lady Nancy Reagan were fans. White House staff presented Watt with a plaster foot with a hole in it, showing that he had "shot himself in the foot". The band returned to D.C. for Independence Day in 1984 and performed to a crowd of 750,000 people.
Dennis' ongoing personal problems kept him out of the group's recent activities. His alcoholism continued to escalate, and on December 28, 1983, he drowned in Marina del Rey while diving from a friend's boat trying to recover items he had previously thrown overboard in fits of rage. Despite his death, the Beach Boys continued as a successful touring act.
On July 4, 1985, the Beach Boys played to an afternoon crowd of one million in Philadelphia and the same evening they performed for over 750,000 people on the Mall in Washington (the day's historic achievement was recorded in the Guinness Book of World Records). They also appeared nine days later at the Live Aid concert. That year, they released the eponymous album "The Beach Boys" and enjoyed a resurgence of interest later in the 1980s, assisted by tributes such as David Lee Roth's hit version of "California Girls". In 1987, they played with the rap group The Fat Boys, performing the song "Wipe Out" and filming a music video.
By 1988, Brian Wilson had officially left the Beach Boys and released his first solo album, which received critical acclaim. During this period the band unexpectedly claimed their first US number one hit single in 22 years with "Kokomo", which had appeared in the movie "Cocktail". Written by John Phillips, Scott McKenzie, Mike Love and Terry Melcher, the song became the band's largest selling single of all time. The video for the song received heavy airplay on the music video channel VH1, and prominently featured actor John Stamos on conga drums. Inducted into the Rock and Roll Hall of Fame earlier in the year, the group became the second artist after Aretha Franklin to hit number one in the US after their induction. They released the album "Still Cruisin"', which went gold in the US and gave them their best chart showing since 1976. In 1990, the band gathered several studio musicians and recorded the Melcher-produced title track of the comedy "Problem Child". Stamos again appeared on the video, and later appeared singing lead vocals on "Forever" (written by Dennis Wilson for the "Sunflower" album) and on their 1992 album "Summer in Paradise". Having no new contributions from Brian Wilson due to interference from caretaker Eugene Landy, "Summer in Paradise" was poorly regarded by both critics and fans, was a commercial disaster and would become their last album of original material for two decades. Members of the band appeared on several television shows such as "Full House", "Home Improvement", and "Baywatch" in the late 1980s and 1990s.
In 1989, Wilson filed a lawsuit to reclaim the rights to his songs and the group's publishing company, Sea of Tunes, which he had supposedly signed away to his father Murry in 1969. He successfully argued that he had not been mentally fit to make an informed decision and that his father had potentially forged his signature. While Wilson failed to regain his copyrights, he was awarded $25 million for unpaid royalties. Soon after Wilson won his case, Love discovered that Murry Wilson had not properly credited him as co-writer on dozens of Beach Boys songs. With Love and Brian Wilson unable to determine exactly what Love was properly owed, Love sued Wilson in 1992, winning $13 million in 1994 for lost royalties. In interviews, Love revealed that on some songs he wrote most of the lyrics, on others only a line or two. Even though Love sued Wilson, both parties said in interviews that there was no malice between them; they simply couldn't come up with an agreeable settlement by themselves.
In 1993, the band appeared in Michael Feeney Callan's film "The Beach Boys Today", which included in-depth interviews with all members except Brian. Carl confided to Callan that Brian would record again with the band at some point in the near future. A few Beach Boys sessions devoted to new Brian Wilson compositions occurred during the mid-1990s, but they remain largely unreleased, and the album was quickly aborted due to tenuous relations. In February 1996, the Beach Boys guested with Status Quo on a re-recording of "Fun, Fun, Fun", which became a British Top-30 hit. In June, the group worked with comedian Jeff Foxworthy on the recording "Howdy From Maui", and eventually released "Stars and Stripes Vol. 1" in August 1996. The album consisted of country renditions of several Beach Boys hits, performed by popular country artists such as Toby Keith and Willie Nelson. Brian Wilson, who was in a better mental state at the time, acted as co-producer.
In early 1997, Carl Wilson was diagnosed with lung cancer and brain cancer after years of heavy smoking. Despite his terminal condition, Carl continued to perform with the band on its 1997 summer tour while undergoing chemotherapy. During performances, he sat on a stool and reportedly needed oxygen after every song. Carl was able to stand, however, when he played on "God Only Knows". Carl died on February 6, 1998, two months after the death of the Wilsons' mother, Audree.
Members' split and band name conflicts.
Following Carl's death, the remaining members splintered. Love, Johnston and former guitarist Marks continued to tour without Jardine, initially as "America's Band", but following several cancelled bookings under that name, they sought authorization through Brother Records Inc. (BRI) to tour as "The Beach Boys" and secured the necessary license. In turn Jardine began to tour regularly with his band dubbed "Beach Boys: Family & Friends" until he ran into legal issues for using the name without license; meanwhile, Jardine sued Love and Wilson claiming that he had been excluded from their concerts. BRI, through its longtime attorney, Ed McPherson, sued Jardine in Federal Court. Jardine, in turn, counter-claimed against BRI for wrongful termination. BRI ultimately prevailed after several years. Love was allowed to continue to tour as The Beach Boys, while Jardine was prohibited from touring using any form of the name.
Released from Landy's control, Brian Wilson sought different treatments for his illnesses that aided him in his solo career. He toured regularly with his backing band consisting of members of Wondermints and other LA/Chicago musicians. Marks also maintained a solo career. Their tours remained reliable draws, with Wilson and Jardine both remaining legal members of the Beach Boys organization.
In September 2004, Brian Wilson issued a free CD through the "Mail On Sunday" that included Beach Boys songs he'd rerecorded, five of which he'd co-authored with Love. The 10 track compilation had 2.6 million copies distributed and prompted Love to file a lawsuit claiming the promotion hurt the sales of the original recordings. Love's suit was dismissed in 2007 when a judge determined that there were no triable issues.
On June 13, 2006, the five surviving Beach Boys (Wilson, Love, Jardine, Johnston and Marks) appeared together for the celebration of the 40th anniversary of "Pet Sounds" and the double-platinum certification of their greatest hits compilation, "", in a ceremony atop the Capitol Records building in Hollywood. Plaques were awarded for their efforts, with Brian accepting on behalf of Dennis and Carl.
50th year reunion celebration.
On October 31, 2011, the Beach Boys released surviving 1960s recordings from "Smile" in the form of "The Smile Sessions". The album—even in its incomplete form—garnered universal critical acclaim and experienced popular success, charting in both the Billboard US and UK Top 30. The band was rewarded with glowing reviews, including inclusion in Rolling Stone's Top 500 album list at number 381. "The Smile Sessions" went on to win Best Historical Album at the 2013 Grammy Awards. Brian Wilson personally accepted the award stating "I guess Van Dyke and I were on to something after all."
In February 2011, the Beach Boys released "Don't Fight the Sea", a charity single to aid the victims of the 2011 Japan earthquake. The single, released on Jardine's 2011 album "A Postcard From California" featured Jardine, Wilson, Love and Johnston, with prerecorded vocals by Carl Wilson. Rumors then circulated regarding a potential 50th anniversary band reunion.
On December 16, 2011, it was announced that Wilson, Love, Jardine, Johnston and Marks would reunite for a new album and 50th anniversary tour in 2012 to include a performance at the New Orleans Jazz Festival in April 2012. On February 12, 2012, the Beach Boys performed at the 2012 Grammy Awards, in what was billed as a "special performance" by organizers. It marked the group's first live performance to include Brian since 1996. The Beach Boys then appeared at the April 10, 2012, season opener for the Los Angeles Dodgers and performed "Surfer Girl" along with "The Star-Spangled Banner". In April, the new album's title was revealed as "That's Why God Made the Radio". The first single from the album, the title track, made its national radio debut April 25, 2012, on ESPN's "Mike and Mike in the Morning" and was released on iTunes and other digital platforms on April 26. "That's Why God Made the Radio" debuted at number three on US charts, the group's highest charting album since 1974's compilation "Endless Summer" and its highest charting studio album since 1965's "Summer Days (And Summer Nights!!)". It became the band's first top ten studio album since 1976's "15 Big Ones". The album made its debut in the UK at number 15, its highest studio album debut since 1971's "Surf's Up". The album also made US chart history by expanding the group's span of "Billboard" 200 top ten albums across 49 years and one week, passing the Beatles with 47 years of top ten albums.
Later in 2012, the group released the "Fifty Big Ones" and "Greatest Hits" compilations along with reissues of 12 of their albums. The next year, the group released "Live – The 50th Anniversary Tour" a 41 song, 2-CD set documenting their "50th Anniversary Tour". While there were no definite plans, Brian stated that he would like to make another Beach Boys album following the world tour. In August 2013, the group released "Made in California", a six disc collection featuring more than seven and a half hours of music, including more than 60 previously unreleased tracks, and concluding the Beach Boys' 50th anniversary campaign. That same year, former members of the Beach Boys touring band, Bobby Figueroa, Billy Hinsche, Ed Carter, Matt Jardine (son of Al Jardine), and Philip Bardowell (sometimes with Randell Kirsch and others) united to form California Surf, Incorporated, performing Beach Boy songs.
Resumed band split.
In June 2012, Love announced additional touring dates which would not feature Wilson. Wilson then denied knowledge of these new dates.
On October 5, Love announced in a self-written press release to the "LA Times" that the band would return to its pre-50th Reunion Tour lineup with him and Johnston touring as the Beach Boys without Wilson, Jardine, and Marks:
Four days later, Wilson and Jardine submitted a written response to the rumors stating: "After Mike booked a couple of shows with Bruce, Al and I were, of course, disappointed. Then there was confusion in some markets when photos of me, Al and David and the 50th reunion band appeared on websites advertising his shows … I was completely blindsided by his press release … We hadn't even discussed as a band what we were going to do with all the offers that were coming in for more 50th shows." Love accused Wilson's statements in this press release to be falsified by his agents, again affirming that the presupposed agreements were "well-documented", and that Wilson had halted further touring dates. On December 13, Wilson and Jardine played a Christmas show at which they performed the Beach Boys Christmas songs. Following this appearance, Wilson announced concert dates featuring himself, Jardine and Marks. Love and Johnston continued to perform under the Beach Boys name, while Wilson, Jardine, and Marks continued to tour as a trio, and a subsequent tour with guitarist Jeff Beck also included former Beach Boy Blondie Chaplin at select dates. Reflecting upon the band's reunion in 2013, Love stated: "I had a wonderful experience being in the studio together. Brian has lost none of his ability to structure those melodies and chord progressions, and when we heard us singing together coming back over the speakers it sounded like 1965 again. Touring was more for the fans. … It was a great experience, it had a term to it, and now everyone's going on with their ways of doing things."
Jardine, Marks, Johnston and Love appeared together at the 2014 Ella Awards Ceremony, where Love was honored for his work as a singer. Marks sang "409" in honor of Love while Jardine performed "Help Me Rhonda". They closed the show by performing "Fun, Fun, Fun". Wilson's long time band associate Jeff Foskett also appeared, but not Wilson. On May 15, 2014 the touring Beach Boys (Love and Johnston) announced a tour celebrating "50 Years of 'Fun Fun Fun, named for their 1964 single. The tour featured the addition of Foskett, who replaced Mike's son Christian. Foskett left Wilson's band due to encumbering responsibilities, and hopes that Wilson and Love's band would someday converge, believing that the two Beach Boys don't "personally have a problem with each other." As of September 2014, Jardine has maintained that a continued reunion with the Beach Boys is "really up to him [Love] … He claims he didn't, that he fired us after the reunion … He’s a brilliant songwriter, and unfortunately he has brilliant lawyers. We wish him all the best, but doggonit, you know, we’d like to be Beach Boys, too. There you go." As Jardine restates "[Love] doesn’t really want to work with us," biographer Jon Stebbins speculated that Love declined to continue working with the group due to the lesser control he had over the touring process, coupled with the lower financial gain, noting: "Night after night after night after night, Mike is making less money getting reminded that Brian is more popular than him. And he has to answer to people instead of calling all the shots himself."
Musical style and development.
In "Understanding Rock: Essays in Musical Analysis", music theorist Daniel Harrison summarizes:
Influenced by doo-wop and rhythm and blues, they began as a garage band playing 1950s style rock and roll. During their early years, the Beach Boys released music that displayed an increasing level of sophistication, a period where Brian Wilson consistently acted as the group's primary bandleader, songwriter, producer, and arranger for the group's most commercially and critically successful work. Together, the band reassembled styles of music such as surf to include vocal jazz harmony, creating their unique sound. In addition, they introduced their signature approach to common genres such as the pop ballad by applying harmonic or formal twists not native to rock and roll. Miller observed, "On straight rockers they sang tight harmonies behind Love's lead … on ballads, Brian played his falsetto off against lush, jazz-tinged voicings, often using (for rock) unorthodox harmonic structures." Harrison adds, "But even the least distinguished of the Beach Boys' early uptempo rock 'n' roll songs show traces of structural complexity at some level; Brian was simply too curious and experimental to leave convention alone." This new sound was quickly associated with the Modernism movement blooming in the Los Angeles music scene. The band later went on to incorporate many genres, from baroque pop to psychedelia and synthpop.
In early 1964, Brian began his breakaway from beach-themed music. Later in November of the same year, the group expressed desires to advance from the surf rock style for which they initially became known for. Experimentation with psychotropic substances proved pivotal to the group's development as artists. The following month, Brian was introduced to cannabis before quickly progressing to LSD in early 1965. Of his first acid trip, Brian recalled that the drug had subjected him to "a very religious experience" which enlightened him to indescribable philosophies. The music for "California Girls", the first Beach Boys song which Bruce Johnston participated in, came from this first LSD experience, as did much of the group's subsequent work where they would often partake in drug use during recording sessions.
Brian is quoted saying: "Everyone contributed something. Carl kept us hip to the latest tunes, Al taught us his repertoire of folk songs, and Dennis, though he didn't [initially] play anything, added a combustible spark just by his presence." Early on, Love sang lead vocals in the rock-oriented songs, while Carl contributed crisp guitar lines on the group's ballads. In a 1966 article which asks "Do the Beach Boys rely too much on sound genius Brian?", Carl responded that every member of the group contributes ideas, but admitted that Brian was majorly responsible for their music.
Influences.
The band's earliest influences came primarily from the work of Chuck Berry and the Four Freshmen. Performed by the Four Freshmen, "Their Hearts Were Full of Spring" (1961) was a particular favorite of the group. By deconstructing their arrangements of pop standards, Brian educated himself on jazz harmony. Taking this into mind, Philip Lambert noted, "If Bob Flanigan helped teach Brian how to sing, then Gershwin, Kern, Porter, and the other members of this pantheon helped him learn how to craft a song." Other general influences on the group included the Hi-Los, the Penguins, the Robins, Bill Haley & His Comets, Otis Williams, the Cadets, the Everly Brothers, the Belmonts, the Shirelles, the Regents, and the Crystals. While the Beach Boys are not often associated with blues, Brian has called this a misapprehension, citing Smokey Robinson and Stevie Wonder as influences. Regarding surf rock pioneer Dick Dale, Brian clarified that his influence on the group was limited to Carl and his style of guitar playing.
The influence of the Beach Boys' peers combined with Brian's competitive nature drove him to reach higher creative peaks. Sometime around late 1963, he heard the song "Be My Baby" (1963) by the Ronettes for the first time, revamping his creative interests and songwriting. "Be My Baby" is considered the epitome of Phil Spector's Wall of Sound production technique, a recording method that would fascinate Wilson for the next several decades. Brian later reflected: "I was unable to really think as a producer up until the time where I really got familiar with Phil Spector's work. That was when I started to design the experience to be a record rather than just a song." He kept "Be My Baby" on his living room jukebox, and would listen to it whenever the mood struck him.
Other prominent inspirations for Brian included Gershwin's "Rhapsody in Blue" (1924), the Beatles' "Rubber Soul" (1965), and composer Burt Bacharach. Author Domenic Priore wrote that, in a subtle way, Brian grew to appreciate the potential of what a pop song could do after being partially spurred on by the dynamics of Bacharach's "Walk On By" (1964), a song which would become just as influential to him as "Be My Baby", supporting his strive to achieve a sense of dynamics in his recordings while he began pulling away from a purely Spector-inspired approach to production. Brian supported this by saying, "Burt Bacharach and Hal David are more like me. They’re also the best pop team — per se — today. As a producer, Bacharach has a very fresh, new approach."
Vocal ability.
Brian identified each member individually for their vocal range, once detailing the ranges for Carl, Dennis, Jardine ("[they] progress upwards through G, A, and B") Love ("can go from bass to the E above middle C"), and himself ("I can take the second D in the treble clef"). He declared in 1966 that his greatest interest was to expand modern vocal harmony, owing his fascination with voice to the Four Freshmen which he considered a "groovy sectional sound". He added, "The harmonies that we are able to produce give us a uniqueness which is really the only important thing you can put into records — some quality that no one else has got. I love peaks in a song — and enhancing them on the control panel. Most of all, I love the human voice for its own sake." Rock critic Erik Davis wrote, "The 'purity' of tone and genetic proximity that smoothed their voices was almost creepy, pseudo-castrato, [and] a 'barbershop' sound." According to Brian: "Jack Good once told us, 'You sing like eunuchs in a Sistine Chapel,' which was a pretty good quote."
From lowest intervals to highest, the group's vocal harmony stack usually began with Love or Dennis, followed by Jardine or Carl, and finally Brian on top. Jardine explains, "We always sang the same vocal intervals. … As soon as we heard the chords on the piano we’d figure it out pretty easily. If there was a vocal move [Brian] envisioned, he’d show that particular singer that move. We had somewhat photographic memory as far as the vocal parts were concerned so that never a problem for us." Striving for absolute perfection, Brian's intricate vocal arrangements exercised the group's calculated blend of intonation, attack, phrasing, and expression. Sometimes, he would sing each vocal harmony part alone through multi-track tape. Jimmy Webb has said, "They used very little vibrato and sing in very straight tones. The voices all lie down beside each other very easily — there's no bumping between them because the pitch is very precise."
As instrumentalists.
The group's instrumental combo initially involved Brian on bass guitar and keyboards, Carl on guitar, and Dennis on drums. From an early age, Brian demonstrated an extraordinary skill for learning music by ear on keyboard. Using major Hollywood recording studios, he arranged many of his compositions for a conglomerate of session musicians informally known as the Wrecking Crew due to the increasingly complicated nature of the material. As a result, a number of songs do not credit the Beach Boys as instrumentalists, but nearly invariably as lead, harmony, or backing vocalists. It's the belief of Richie Unterberger that "before session musicians took over most of the parts, the Beach Boys could play respectably gutsy surf rock as a self-contained unit." In spite of this, Carl Wilson continued to play beside these musicians whenever he was available to attend sessions. In archivist Craig Slowinski's view, "One should not sell short Carl's own contributions; the youngest Wilson had developed as a musician sufficiently to play alongside the horde of high-dollar session pros that big brother was now bringing into the studio. Carl's guitar playing [was] a key ingredient."
Songwriting and production.
Brian's experiments with his Wollensack tape recorder provide early examples of his flair for exotica and unusual percussive patterns and arranging ideas that he would recycle in later prominent work. Through attending Phil Spector's sessions sporadically, Brian learned how to act as a producer for records while being educated on the Wall of Sound process. From then on, Brian received some production advice from Jan Berry. As they collaborated on several hit singles written and produced for other artists, they recorded what would later be regarded the California Sound. The positive commercial response to Brian's structurally irregular and harmonically varied pop compositions gave him the prestige, resources, and courage to further his creative aspirations. He proceeded to explore many unusual combinations of instruments while emphasizing inventive percussion and progressively ambitious lyricism.
Although he was often dubbed a perfectionist, Brian was an inexperienced musician, and his understanding was mostly self-taught. He handled most stages of the group's recording process from the beginning, despite Nik Venet being credited for producing their early recordings. With regards to Brian's mid 1960s productions, ethnomusicologist David Toop characterized his style as "cartoon music and Disney influence mutating into avant-garde pop". Before 1966, Brian's mastery of songwriting proved that he was capable of applying odd harmonic progressions, unexpected disruptions of hypermeter, jazz theory, tempo changes, metrical ambiguity, and unusual tone colors successfully within a pop context. He made on-the-spot decisions about notes, articulation, and timbre; composing at the mixing board and using the studio as a musical instrument. Despite this, in most cases he was forced to rely on outside collaborators when it came to adding lyricism to his compositions. It was at this stage that Brian usually worked with bandmate Mike Love whose assertive persona provided youthful swagger that contrasted Brian's explorations in romanticism and sensitivity.
He preferred mixing live as performances were recorded, as opposed to mixing after the fact. He was open to changes suggested by others while recording, often taking advice and suggestions and even incorporating apparent mistakes if they provided a useful or interesting alternative. He experimented with processed effects including varispeed, reverberation, slapback echo, and filtering signals through a Leslie speaker. Once an instrumental track was completed, vocals would then be overdubbed by the group. On "Surfin' U.S.A." (1963), Brian began doubletracking. As was practiced by other record producers from the 1960s, most of his mixes ended up in single-channel monaural, believing that varied stereo speaker placement took his control over the sound image away to the listener.
Eschewing Capitol Studios which Brian considered inadequate, engineer Chuck Britz often collaborated with him at Western 3 of United Western Recorders, also serving as a buffer between Brian and the oft-berating Murry whenever he was present. Once Britz assembled a preliminary recording setup, Brian would take over the console, directing the instrumentalists from the booth using an intercom or verbal gestures after supplying them with chord charts which were sometimes written incorrectly. It's reported that even though Britz was responsible for setting up recording, Brian would then adjust his configuration to a large extent. As Brian's productions advanced, he became recognized for his pop artistry, vocal harmonization, incessant studio perfectionism, forward-thinking song structures, engineering and mixing know-how, and creative multitasking abilities. Session bassist Carol Kaye noted, "We had to create [instrumental] parts for all the other groups we cut for, but not Brian. We were in awe of Brian." Friend Danny Hutton expressed similar feelings while highlighting Brian's studio proficiency, citing what he believed to be an extraordinary talent at harnessing several different studio spaces while piecing together discrete instrumental patterns and timbres cohesively. He noted, "Somebody could go in right after Brian’s session and try to record, and they could never get the sound he got. There was a lot of subtle stuff he did. … People don’t talk that much about it. They always talk about his music. He was fabulous in the studio, in terms of getting sounds. You’d sit there, and that was him. He was just hands-on. He would change the reverb and the echo, and all of a sudden, something just — "whoa!" — got twice as big and fat."
Foreshadowed by "Beach Boys' Party!" (1965), much of the group's recordings from 1967–1970 displayed sparse instrumentation, a more relaxed ensemble, and a seeming inattention to production quality. Brian briefly experimented with "musique concrete" and minimalist rock approaches to music before retreating to his home recording studio to record "manic" material in the 1970s, enacting syncopated exercises and counterpoints layered on jittery eighth note tone clusters and loping shuffle grooves. During the infancy of Brian's home studio, the group was forced to improvise many technical aspects of recording. In one instance, they used an empty swimming pool as an echo chamber.
When Brian abdicated from the group, the other members were forced to take a more active production role. This is believed to have faltered the quality of their music. Richie Unterberger believes that after the December 1967 release of "Wild Honey", "the Beach Boys were revealed as a group that, although capable of producing some fine and interesting music, were no longer innovators on the level of the Beatles and other figureheads." The album marked the beginning of Carl's increased role as producer, who described it as "music for Brian to cool out by," signaling a mellower approach which would pervade into the 1970s. In 1968, Dennis contributed original songs to "Friends", revealing himself as a broodingly soulful songwriter and singer, while Bruce Johnston devised a moody instrumental, "The Nearest Faraway Place", for "20/20" the following year. "Sunflower" (1970) marked an end to the experimental songwriting and production phase initiated by "Smiley Smile" (1967). Of the albums between "Surf's Up" (1971) and "Holland" (1973), Daniel Harrison wrote that they "contain a mixture of middle-of-the-road music entirely consonant with pop style during the early 1970s with a few oddities that proved that the desire to push beyond conventional boundaries was not dead." While Harrison adamantly states "1974 is the year in which the Beach Boys ceased to be a rock 'n' roll act and became an oldies act," "Love You" (1977) is perceived by some as an oddity that sounds like no other record in their catalog with synthesizer-laden arrangements played almost entirely by Brian. Referring to "naysayers" of the album, the underground fanzine "Scram" wrote "fuck [them] … [the album showcases] a truly original mix of humor and sadness. The original numbers always dance just a step away from the cliché, dealing with simple lyrical themes that make you wonder why they had never been explored before."
Legacy.
Cultural and musical impact.
Regarded by some critics as one of the greatest American rock groups and an important catalyst in the evolution of popular music, the Beach Boys are one of the most critically acclaimed and commercially successful bands of all time. The Beach Boys' sales estimates range from 100 to 350 million records worldwide, and have influenced artists spanning many genres and decades. The group's early songs made them major pop stars in the United States, the United Kingdom, Australia and other countries, having sixteen hit singles between 1962 and 1965. They were one of the few American bands formed prior to the 1964 British Invasion to continue their success. Among artists of the 1960s, they are one of few central figures in the histories of rock. Their early hits helped raise the profile of the state of California, creating its first major regional style with national significance, and establishing a musical identity for Southern California, as opposed to Hollywood. This also associated the band with surfing, hot-rod racing, and a contemporaneous teenage lifestyle and fantasy. There had been surf bands formed prior to the Beach Boys, but none which projected a world view as the Beach Boys did.
Brian's work is credited as a major innovation in the field of music production. According to Erik Davis, "Not only did the Beach Boys write a soundtrack to the early '60s, but Brian let loose a delicate and joyful art pop unique in music history and presaged the mellowness so fundamental to '70s California pop." Only 21-years-old when he received the freedom to produce his own records with total creative autonomy, he ignited an explosion of like-minded California producers, supplanting New York as the center of popular records, and becoming the first rock producer to use the studio as a discrete instrument. The Beach Boys were thus one of the first rock groups to exert studio control.
The group was among early (or earliest) instigators of psychedelic rock, acid rock, art rock, art pop, and progressive rock, Besides the Beatles, the Beach Boys attracted a following from a great number of their pop or rock contemporaries during the 1960s, including the Rolling Stones, Harry Nilsson, Cream, George Martin, the Who, Roger Waters of Pink Floyd, Lou Reed of The Velvet Underground, and Frank Zappa. In the 1970s, the Beach Boys' were paid homage by punk rock artists such as Ramones, Patti Smith, and Lester Bangs. Additionally, they influenced pioneering musicians for glam rock: David Bowie and Marc Bolan; krautrock: Faust, Kraftwerk; power pop: Big Star; and post-punk: Talking Heads. Later, the group bore a strong influence on indie rock.
Over the years, the group's songs have been the subject of many tribute albums, several of which are compiled from cover versions contributed by various artists from a wide range of backgrounds including Japanese noise, pop punk, rockabilly, and trip hop. In the 1990s, the Beach Boys received a resurgence of popularity with alternative rock groups. Those who advocated for the band included founding members of the Elephant 6 Collective: Neutral Milk Hotel, the Olivia Tremor Control, the Apples in Stereo, and of Montreal. United by a shared love of the Beach Boys' music, they named Pet Sounds Studio in honor of the group. Other influenced artists who gained prominence in underground circles during the 1980s and 1990s include shoegaze band My Bloody Valentine, electronica outfits Daft Punk, Saint Etienne, The High Llamas, the Avalanches, Stereolab, and alternative rock musicians Radiohead, Sonic Youth, Frank Black of Pixies, and Paddy McAloon of Prefab Sprout. In Japan, their music affected the work of noise rock bands Seagull Screaming Kiss Her Kiss Her and Melt-Banana, along with J-pop performers Tatsuro Yamashita, Flipper's Guitar, and Yellow Magic Orchestra.
The Beach Boys' influence has continued to pervade in such millennial artists as Air, Animal Collective, Fleet Foxes, MGMT, and Frank Ocean.
Reappraisal.
Professor of cultural studies James M. Curtis wrote in 1987,
Throughout their career, the Beach Boys struggled with their public image and audiences. Musicologist Charlie Gillett explains, "By 1965, the Beach Boys had become an American pop institution, but although they continued to cultivate a visual image in line with their name and early repertoire, there was a limit to how many different ways Wilson could celebrate the wonders of living in Southern California … Originally, many serious pop fans dismissed the group as trashy pop for kids, the Beach Boys began to attract wide admiration just as Brian Wilson found the strain of public adulation more than he had bargained for; retreating from live performances, Wilson spent most of his time in the recording studio, constructing the songs." Their growing complexity caused their live performances to suffer in the mid 1960s, when the group began to be derided by audiences for their uniformed striped shirts compounded by low key reproductions of songs which demanded complicated orchestrations. In their earliest performances, the band wore heavy wool jacket-like shirts which were favored by surfers in the South Bay before switching to their trademark striped shirts and white pants. In the latter half of the decade, the Beach Boys promoted anti-war activism, Transcendental Meditation and environmentalism. In 1970, the group ceased wearing matching uniforms on stage and began emphasizing political and social awareness. Drawing from their associations with Charles Manson and Ronald Reagan, Erik Davis observed, "the Beach Boys may be the only bridge between those deranged poles. There is a wider range of political and aesthetic sentiments in their records than in any other band in those heady times—like the state [of California], they expand and bloat and contradict themselves."
Peter Ames Carlin noted that while the Beach Boys' contemporaries grew more intellectually aware, "Capitol continued to bill them as 'America’s Top Surfin' Group!' Their TV appearances still took place on sets dressed with surfboards, beach balls, and chicks doing the twist in candy-colored bikinis. And when the summer sales season neared, everyone still expected Brian to crank out another batch of ready-made tunes set on the beaches, highways, and backseats he’d long since lost interest in describing." He adds, "Clearly, the group’s disconnect from the cultural avant-garde was not all Capitol’s doing or even entirely a product of the group’s own commercial motivations. Bear in mind that the Beach Boys themselves were still college-aged, only none of them had been to college, nor (with the possible exception of Brian) had they shown much discernible interest in what you might call the world of ideas. … This makes the humanities students among us slap our foreheads and moan with sorrow." Collaborator Van Dyke Parks has spoken that he and Brian were conscious of the counterculture, and that the two had felt estranged from it, but also that it was necessary to adhere to due to a willingness to "get out of the Eisenhower mindset." Parks stresses, "At the same time, he didn't want to lose that kind of gauche sensibility that he had. He was doing stuff that nobody would dream of doing. You would never, for example, use one string on a banjo when you had five; it just wasn't done. But when I asked him to bring a banjo in, that’s what he did. This old-style plectrum thing. One string. That’s gauche."
Despite the group's immense popularity and success, some consider that the extent of their contribution to Western music canon is undervalued. In 1967, Lou Reed famously wrote, "Will none of the powers that be realize what Brian Wilson did with "the chords"?" Pitchfork Media posited, "At some point, you learn that the Beach Boys weren't just a fun 1960s surf band with a run of singles that later came to be used in commercials; at their best, they were making capital-A Art. … Once you've absorbed ["Pet Sounds"], you find yourself going back through songs like "Don't Worry Baby", "The Warmth of the Sun", and "I Get Around", finding a deeper brilliance where you once heard only pop craftsmanship." Discussing the 2011 release of "The Smile Sessions", "The Los Angeles Times" wrote, "…certainly every library of American recording history needs this; university composition departments, music professors, budding recording engineers and composers should study it." Online publication "NewMusicBox" — which normally devotes itself to new American music that is outside the commercial mainstream — argued,
Daniel Harrison contests that the group produced work which could "almost" be considered art music in the Western classical tradition, and that group's innovations in the musical language of rock can be compared to those that introduced atonal and other nontraditional techniques into that classical tradition. He explains, "The spirit of experimentation is just as palpable in "Smiley Smile" as it is in, say, Schoenberg's op. 11 piano pieces." While the group "went into the great void beyond," such notions were not widely acknowledged by rock audiences nor by the classically-minded at the time. Harrison concludes: "What influences could these innovations then have? The short answer is, not much. "Smiley Smile", "Wild Honey", "Friends", and "20/20" sound like few other rock albums; they are "sui generis". … It must be remembered that the commercial failure of the Beach Boys' experiments was hardly motivation for imitation. In the end, we must conclude that the Beach Boys' late-1960s experiments were not reproducible." Referring to the groups' reaction to the commercial success of their 1976 greatest hits compilation "Endless Summer", Harrison writes, "they returned to the beach, knowing they would never leave it again." Erik Davis wrote that by 1990, "the Beach Boys are either dead, deranged, or dinosaurs; their records are Eurocentric, square, unsampled; they've made too much money to merit hip revisionism." From the same period, Jim Miller wrote, "They have become a figment of their own past, prisoners of their unflagging popularity—incongruous emblems of a sunny myth of eternal youth belied by much of their own best music. … The group is still largely identified with its hits from the early Sixties."
Awards and honors.
The group routinely appears in the upper reaches of ranked lists such as "The Top 1000 Albums of All Time". Many of the group's songs and albums including "The Beach Boys Today!" (1965), "Smiley Smile" (1967), "Sunflower" (1970), and "Surf's Up" (1971) are featured in several lists devoted to the greatest of all time. The 1966 releases "Pet Sounds" and "Good Vibrations" frequently rank among the top of critics' lists of the greatest albums and singles of all time. In 2004, "Pet Sounds" was preserved in the National Recording Registry by the Library of Congress for being "culturally, historically, and aesthetically significant." On Acclaimed Music, "Good Vibrations" is ranked the third best song of all time, while "God Only Knows" is ranked twenty-first; the group itself is ranked eleven in its 1000 most recommended artists of all time.
In 1966 and 1967, reader polls conducted by the UK magazine "NME" crowned the Beach Boys as the world's number one vocal group, ahead of the Beatles and the Rolling Stones. In 1974, the Beach Boys were awarded "Band of the Year" by Rolling Stone. On December 30, 1980, the Beach Boys were awarded a star on the Hollywood Walk of Fame, located at 1500 Vine Street. The group was inducted into the Rock and Roll Hall of Fame in 1988. Ten years later they were selected for the Vocal Group Hall of Fame. In 2001, the group received a Grammy Lifetime Achievement Award. In 2004, "Rolling Stone" ranked the Beach Boys number 12 on its list of the 100 Greatest Artists of All Time. Brian Wilson was inducted into the UK Rock and Roll Hall of Fame in November 2006.
The Wilsons' California house, where the Wilson brothers grew up and the group began, was demolished in 1986 to make way for Interstate 105, the Century Freeway. A Beach Boys Historic Landmark (California Landmark No. 1041 at 3701 West 119th Street), dedicated on May 20, 2005, marks the location.
Selected filmography.
The Beach Boys also appear in the beach party films "The Girls on the Beach" in which they perform three songs "The Girls on the Beach", "Lonely Sea", and "Little Honda" and "The Monkey's Uncle" in which they perform "The Monkey's Uncle" with Annette Funicello.
The life of the Beach Boys is the subject of two TV movies: ' and '.
The Beach Boys appeared in an episode of "Full House" entitled "Beach Boy Bingo", which aired on November 18, 1988.
The Beach Boys also appeared in Season 6, Episode 4 of "Baywatch" (1995).

</doc>
<doc id="4479" url="http://en.wikipedia.org/wiki?curid=4479" title="BCE (disambiguation)">
BCE (disambiguation)

BCE may stand for: 

</doc>
<doc id="4480" url="http://en.wikipedia.org/wiki?curid=4480" title="BC">
BC

BC may refer to:

</doc>
<doc id="4481" url="http://en.wikipedia.org/wiki?curid=4481" title="Beatrix Potter">
Beatrix Potter

Helen Beatrix Potter (28 July 186622 December 1943) was an English author, illustrator, natural scientist and conservationist best known for her imaginative children's books, featuring animals such as those in "The Tale of Peter Rabbit", which celebrated the British landscape and country life.
Born into a wealthy Unitarian family, Potter, along with her younger brother Walter Bertram (1872–1918), grew up with few friends outside her large extended family. Her parents were artistic, interested in nature and enjoyed the countryside. As children, Beatrix and Bertram had numerous small animals as pets which they observed closely and drew endlessly. Summer holidays were spent away from London, in Scotland and in the English Lake District where Beatrix developed a love of the natural world which was the subject of her painting from an early age.
She was educated by private governesses until she was 18. Her study of languages, literature, science and history was broad and she was an eager student. Her artistic talents were recognized early. She enjoyed private art lessons, and developed her own style, favouring watercolour. Along with her drawings of her animals, real and imagined, she illustrated insects, fossils, archaeological artifacts, and fungi. 
In the 1890s her mycological illustrations and research into the reproduction of fungus spores generated interest from the scientific establishment. Following some success illustrating cards and booklets, Potter wrote and illustrated "The Tale of Peter Rabbit", publishing it first privately in 1901, and a year later as a small, three-colour illustrated book with Frederick Warne & Co. She became unofficially engaged to her editor Norman Warne in 1905 despite the disapproval of her parents, but he died suddenly a month later of leukemia.
With the proceeds from the books and a legacy from an aunt, Potter bought Hill Top Farm in Near Sawrey, a tiny village, then in Lancashire, the English Lake District near Windermere, in 1905. Over the following decades, she purchased additional farms to preserve the unique hill country landscape. In 1913, at the age of 47, she married William Heelis, a respected local solicitor from Hawkshead. 
Potter was also a prize-winning breeder of Herdwick sheep and a prosperous farmer keenly interested in land preservation. She continued to write, illustrate and design spin-off merchandise based on her children's books for Warne until the duties of land management and her diminishing eyesight made it difficult to continue. 
Potter published over 23 books: the best known are those written between 1902 and 1922. She died of pneumonia and heart disease on 22 December 1943 at her home in Near Sawrey (Lancashire) at age 77, leaving almost all her property to the National Trust. She is credited with preserving much of the land that now comprises the Lake District National Park.
Potter's books continue to sell throughout the world, in many languages. Her stories have been retold in song, film, ballet and animation. "The World of Peter Rabbit and Friends", a TV series based on her stories, has been released on VHS by Pickwick Video and later Carlton Video.
Biography.
Early life.
Potter's paternal grandfather, Edmund Potter, from Glossop in Derbyshire, owned what was then the largest calico printing works in England, and later served as a Member of Parliament. Beatrix's father, Rupert William Potter (1832–1914), was educated at Manchester College by the Unitarian philosopher Dr. James Martineau. He then trained as a barrister in London. Rupert practised law, specialising in equity law and conveyancing. He married Helen Leech (1839–1932) on 8 August 1863 at Hyde Unitarian Chapel, Gee Cross. Helen was the daughter of Jane Ashton (1806-1884) and John Leech, a wealthy cotton merchant and shipbuilder from Stalybridge. Helen's first cousin was Harriet Lupton née Ashton - the sister of Lord Ashton. Harriet was the great great great aunt of Catherine, Duchess of Cambridge. It was reported in July, 2014 that Beatrix had personally given a number of her own original hand-painted illustrations to the two daughters of Dr Arthur and Harriet Lupton, who were blood cousins to both Beatrix and the Duchess. 
Beatrix's parents lived comfortably at 2 Bolton Gardens, South Kensington, where Helen Beatrix was born on 28 July 1866 and her brother Walter Bertram on 14 March 1872. Both parents were artistically talented, and Rupert was an adept amateur photographer. Rupert had invested in the stock market and by the early 1890s was extremely wealthy.
Potter's family on both sides were from the Manchester area. They were English Unitarians, a dissenting Protestant sect who rejected the doctrine of the Trinity.
Beatrix was educated by three able governesses, the last of whom was Annie Moore ("née" Carter), just three years older than Beatrix, who tutored Beatrix in German as well as acting as lady's companion. She and Beatrix remained friends throughout their lives and Annie's eight children were the recipients of many of Potter's delightful picture letters. It was Annie who later suggested that these letters might make good children’s books.
In their school room Beatrix and Bertram kept a variety of small pets, mice, rabbits, a hedgehog and some bats, along with collections of butterflies and other insects which they drew and studied. Beatrix was devoted to the care of her small animals, often taking them with her on long holidays. In most of the first fifteen years of her life, Beatrix spent summer holidays at Dalguise, an estate on the River Tay in Perthshire, Scotland. There she sketched and explored an area that nourished her imagination and her observation. Beatrix and her brother were allowed great freedom in the country and both children became adept students of natural history. In 1887, when Dalguise was no longer available, the Potters took their first summer holiday in the Lake District, at Wray Castle near Windermere. Here Beatrix met Hardwicke Rawnsley, vicar of Wray and later the founding secretary of the National Trust, whose interest in the countryside and country life inspired the same in Beatrix and who was to have a lasting impact on her life.
About the age of 14 Beatrix began to keep a diary. It was written in a code of her own devising which was a simple letter for letter substitution. Her "Journal" was important to the development of her creativity, serving as both sketchbook and literary experiment: in tiny handwriting she reported on society, recorded her impressions of art and artists, recounted stories and observed life around her. The "Journal", decoded and transcribed by Leslie Linder in 1958, does not provide an intimate record of her personal life, but it is an invaluable source for understanding a vibrant part of British society in the late 19th century. It describes Potter's maturing artistic and intellectual interests, her often amusing insights on the places she visited, and her unusual ability to observe nature and to describe it. Started in 1881, her journal ends in 1897 when her artistic and intellectual energies were absorbed in scientific study and in efforts to publish her drawings. Precocious but reserved and often bored, she was searching for more independent activities and wished to earn some money of her own whilst dutifully taking care of her parents, dealing with her especially demanding mother, and managing their various households.
Scientific illustrations and work in mycology.
Beatrix Potter's parents did not discourage higher education. As was common in the Victorian era, women of her class were privately educated and rarely went to university.
Beatrix Potter was interested in every branch of natural science save astronomy. Botany was a passion for most Victorians and nature study was a popular enthusiasm. Potter was eclectic in her tastes: collecting fossils, studying archeological artefacts from London excavations, and interested in entomology. In all these areas she drew and painted her specimens with increasing skill. By the 1890s her scientific interests centred on mycology. First drawn to fungi because of their colours and evanescence in nature and her delight in painting them, her interest deepened after meeting Charles McIntosh, a revered naturalist and mycologist, during a summer holiday in Dunkeld in Perthshire in 1892. He helped improve the accuracy of her illustrations, taught her taxonomy, and supplied her with live specimens to paint during the winter. Curious as to how fungi reproduced, Potter began microscopic drawings of fungus spores (the agarics) and in 1895 developed a theory of their germination. Through the connections of her uncle Sir Henry Enfield Roscoe, a chemist and vice-chancellor of the University of London, she consulted with botanists at Kew Gardens, convincing George Massee of her ability to germinate spores and her theory of hybridisation. She did not believe in the theory of symbiosis proposed by Simon Schwendener, the German mycologist, as previously thought; rather she proposed a more independent process of reproduction.
Rebuffed by William Thiselton-Dyer, the Director at Kew, because of her gender and her amateur status, Beatrix wrote up her conclusions and submitted a paper, "On the Germination of the Spores of the Agaricineae", to the Linnean Society in 1897. It was introduced by Massee because, as a female, Potter could not attend proceedings or read her paper. She subsequently withdrew it, realising that some of her samples were contaminated, but continued her microscopic studies for several more years. Her paper has only recently been rediscovered, along with the rich, artistic illustrations and drawings that accompanied it. Her work is only now being properly evaluated. Potter later gave her other mycological and scientific drawings to the Armitt Museum and Library in Ambleside, where mycologists still refer to them to identify fungi. There is also a collection of her fungus paintings at the Perth Museum and Art Gallery in Perth, Scotland donated by Charles McIntosh. In 1967 the mycologist W.P.K. Findlay included many of Potter's beautifully accurate fungus drawings in his "Wayside & Woodland Fungi", thereby fulfilling her desire to one day have her fungus drawings published in a book. In 1997 the Linnean Society issued a posthumous apology to Potter for the sexism displayed in its handling of her research.
Artistic and literary career.
Potter’s artistic and literary interests were deeply influenced by fairies, fairy tales and fantasy. She was a student of the classic fairy tales of Western Europe. As well as stories from the Old Testament, John Bunyan's "The Pilgrim's Progress" and Harriet Beecher Stowe's "Uncle Tom's Cabin", she grew up with "Aesop's Fables", the fairy tales of the Brothers Grimm and Hans Christian Andersen, Charles Kingsley's "The Water Babies", the folk tales and mythology of Scotland, the German Romantics, Shakespeare, and the romances of Sir Walter Scott. As a young child, before the age of eight, Edward Lear's "Book of Nonsense", including the much loved "The Owl and the Pussycat", and Lewis Carroll's "Alice in Wonderland" had made their impression, although she later said of "Alice" that she was more interested in Tenniel's illustrations than what they were about. The "Brer Rabbit" stories of Joel Chandler Harris had been family favourites, and she later studied his "Uncle Remus" stories and illustrated them. She studied book illustration from a young age and developed her own tastes, but the work of the picture book triumvirate Walter Crane, Kate Greenaway and Randolph Caldecott, the last an illustrator whose work was later collected by her father, was a great influence. When she started to illustrate, she chose first the traditional rhymes and stories, "Cinderella", "Sleeping Beauty", "Ali Baba and the Forty Thieves", "Puss-in-boots", and "Red Riding Hood". But most often her illustrations were fantasies featuring her own pets: mice, rabbits, kittens, and guinea pigs.
In her teenage years Potter was a regular visitor to the art galleries of London, particularly enjoying the summer and winter exhibitions at the Royal Academy in London. Her "Journal" reveals her growing sophistication as a critic as well as the influence of her father's friend, the artist Sir John Everett Millais, who recognised Beatrix's talent of observation. Although Potter was aware of art and artistic trends, her drawing and her prose style were uniquely her own.
As a way to earn money in the 1890s, Beatrix and her brother began to print Christmas cards of their own design, as well as cards for special occasions. Mice and rabbits were the most frequent subject of her fantasy paintings. In 1890 the firm of Hildesheimer and Faulkner bought several of her drawings of her rabbit Benjamin Bunny to illustrate verses by Frederic Weatherly titled "A Happy Pair". In 1893 the same printer bought several more drawings for Weatherly's "Our Dear Relations", another book of rhymes, and the following year Potter sold a series of frog illustrations and verses for "Changing Pictures", a popular annual offered by the art publisher Ernest Nister. Potter was pleased by this success and determined to publish her own illustrated stories.
Whenever Potter went on holiday to the Lake District or Scotland, she sent letters to young friends, illustrating them with quick sketches. Many of these letters were written to the children of her former governess Annie Carter Moore, particularly to her eldest son Noel who was often ill. In September 1893 Potter was on holiday at Eastwood in Dunkeld, Perthshire. She had run out of things to say to Noel and so she told him a story about "four little rabbits whose names were Flopsy, Mopsy, Cottontail and Peter". It became one of the most famous children's letters ever written and the basis of Potter's future career as a writer-artist-storyteller.
In 1900, Potter revised her tale about the four little rabbits, and fashioned a dummy book of it - it has been suggested, in imitation of Helen Bannerman's 1899 bestseller "The Story of Little Black Sambo". Unable to find a buyer for the work, she published it for family and friends at her own expense in December 1901. It was drawn in black and white with a coloured frontispiece. Family friend Canon Hardwicke Rawnsley had great faith in Potter's tale, recast it in didactic verse, and made the rounds of the London publishing houses. Frederick Warne & Co had previously rejected the tale but, eager to compete in the booming small format children's book market, reconsidered and accepted the "bunny book" (as the firm called it) following the recommendation of their prominent children's book artist L. Leslie Brooke. The firm declined Rawnsley's verse in favour of Potter's original prose, and Potter agreed to colour her pen and ink illustrations, choosing the then new Hentschel three-colour process to reproduce her watercolours.
On 2 October 1902 "The Tale of Peter Rabbit" was published, and was an immediate success. It was followed the next year by "The Tale of Squirrel Nutkin" and "The Tailor of Gloucester", which had also first been written as picture letters to the Moore children. Working with Norman Warne as her editor, Potter published two or three little books each year: 23 books in all. The last book in this format was "Cecily Parsley's Nursery Rhymes" in 1922, a collection of favourite rhymes. Although "The Tale of Little Pig Robinson" was not published until 1930, it had been written much earlier. Potter continued creating her little books until after the First World War, when her energies were increasingly directed toward her farming, sheep-breeding and land conservation.
The immense popularity of Potter's books was based on the lively quality of her illustrations, the non-didactic nature of her stories, the depiction of the rural countryside, and the imaginative qualities she lent to her animal characters.
Potter was also a canny businesswoman. As early as 1903 she made and patented a Peter Rabbit doll. It was followed by other "spin-off" merchandise over the years, including painting books, board games, wall-paper, figurines, baby blankets and china tea-sets. All were licensed by Frederick Warne & Co and earned Potter an independent income, as well as immense profits for her publisher.
In 1905, Potter and Norman Warne became unofficially engaged. Potter's parents objected to the match because Warne was "in trade" and thus not socially suitable. The engagement lasted only one month until Warne died of leukemia at age 37. That same year Potter used some of her income and a small inheritance from an aunt to buy Hill Top Farm in Near Sawrey in the English Lake District. Potter and Warne may have hoped that Hill Top Farm would be their holiday home, but after Warne's death Potter went ahead with its purchase as she had always wanted to own that farm, and live in "that charming village".
Country life.
The tenant farmer John Cannon and his family agreed to stay on to manage the farm for her while she made physical improvements and learned the techniques of fell farming and of raising livestock, including pigs, cows and chickens; the following year she added sheep. Realising she needed to protect her boundaries, she sought advice from W.H. Heelis & Son, a local firm of solicitors with offices in nearby Hawkshead. With William Heelis acting for her she bought contiguous pasture, and in 1909 the Castle Farm across the road from Hill Top Farm. She visited Hill Top at every opportunity, and her books written during this period (such as "The Tale of Ginger and Pickles", about the local shop in Near Sawrey and "The Tale of Mrs. Tittlemouse", a wood mouse) reflect her increasing participation in village life and her delight in country living.
Owning and managing these working farms required routine collaboration with the widely respected William Heelis. By the summer of 1912 Heelis had proposed marriage and Beatrix had accepted; although she did not immediately tell her parents, who once again disapproved because Heelis was only a country solicitor. Potter and Heelis were married on 15 October 1913 in London at St Mary Abbots in Kensington. The couple moved immediately to Near Sawrey, residing at Castle Cottage, the renovated farm house on Castle Farm. Hill Top remained a working farm but was now remodelled to allow for the tenant family and Potter's private studio and work shop. At last her own woman, Potter settled into the partnerships that shaped the rest of her life: her country solicitor husband and his large family, her farms, the Sawrey community and the predictable rounds of country life. "The Tale of Jemima Puddle-Duck" and "The Tale of Tom Kitten" are representative of Hill Top Farm and of her farming life, and reflect her happiness with her country life.
After Rupert Potter died in 1914, Potter, now a wealthy woman, found Lindeth Howe, a large house in nearby Windermere where her mother lived until her death in 1931 at the age of 93. Potter continued to write stories for Frederick Warne & Co and fully participated in country life. She established a Nursing Trust for local villages, and served on various committees and councils responsible for footpaths and other rural issues.
Sheep farming.
Beatrix Potter Heelis became keenly interested in the breeding and raising of Herdwick sheep, the indigenous fell sheep, soon after acquiring Hill Top Farm. In 1923 she bought a former deer park and vast sheep farm in the Troutbeck Valley called Troutbeck Park Farm, restoring its land with thousands of Herdwick sheep. This established her as one of the major Herdwick sheep farmers in the area. She was admired by her shepherds and farm managers for her willingness to experiment with the latest biological remedies for the common diseases of sheep, and for her employment of the best shepherds, sheep breeders, and farm managers.
By the late 1920s Potter and her Hill Top farm manager Tom Storey had made a name for their prize-winning Herdwick flock, for which she won many prizes at the local agricultural shows, where she was also often asked to serve as a judge. In 1942 she was named President-elect of The Herdwick Sheepbreeders’ Association, the first time a woman had ever been elected to that office, but died before taking office.
Lake District conservation.
Potter had been a disciple of the land conservation and preservation ideals of her long-time friend and mentor, Canon Hardwicke Rawnsley, the first secretary and founding member of the National Trust for Places of Historic Interest or Natural Beauty. She supported the efforts of the National Trust to preserve not just the places of extraordinary beauty but also those heads of valleys and low grazing lands that would be irreparably ruined by development. She was also an authority on the traditional Lakeland crafts, period furniture and stonework. She restored and preserved the farms that she bought or managed, making sure that each farm house had in it a piece of antique Lakeland furniture. Potter was interested in preserving not only the Herdwick sheep, but also the way of life of fell farming. In 1930 the Heelises became partners with the National Trust in buying and managing the fell farms included in the large Monk Coniston Estate. The estate was composed of many farms spread over a wide area of north-western Lancashire, including the famously beautiful Tarn Hows. Potter was the "de facto" estate manager for the Trust for seven years until the National Trust could afford to buy most of the property back from her. Her stewardship of these farms earned her wide regard, but she was not without her critics. She was notable in observing the problems of afforestation, preserving the intake grazing lands, and husbanding the quarries and timber on these farms. All her farms were stocked with Herdwick sheep and frequently with Galloway cattle.
Later life.
Potter continued to write stories and to draw, although mostly for her own pleasure. Her books in the late 1920s included the semi-autobiographical "The Fairy Caravan", a fanciful tale set in her beloved Troutbeck fells. It was published only in the US during Potter's lifetime, and not until 1952 in the UK. "Sister Anne", Potter's version of the story of Bluebeard, was written especially for her American readers, but illustrated by Katharine Sturges. A final folktale, "Wag by Wall", was published posthumously by "The Horn Book" in 1944. Potter was a generous patron of the Girl Guides, whose troops she allowed to make their summer encampments on her lands and whose company she enjoyed as an older woman.
Potter and William Heelis enjoyed a happy marriage of thirty years, continuing their farming and preservation efforts throughout the hard days of the Second World War. Although they were childless, Potter played an important role in William’s large family, particularly enjoying her relationship with several nieces whom she helped educate and giving comfort and aid to her husband’s brothers and sisters.
Potter died of complications from pneumonia and heart disease on 22 December 1943 at Castle Cottage and her remains were cremated at Carleton Crematorium. She left nearly all her property to the National Trust, including over of land, sixteen farms, cottages and herds of cattle and Herdwick sheep. Hers was the largest gift at that time to the National Trust and it enabled the preservation of the lands now included in the Lake District National Park and the continuation of fell farming. The central office of the National Trust in Swindon was named "Heelis" in 2005 in her memory. William Heelis continued his stewardship of their properties and of her literary and artistic work for the eighteen months he survived her. When he died in August 1945 he left the remainder to the National Trust.
Legacies.
Potter left almost all the original illustrations for her books to the National Trust. The copyright to her stories and merchandise was then given to her publisher Frederick Warne & Co, now a division of the Penguin Group. On January 1, 2014, the copyright expired in the UK and other countries with a 70-years-after-death limit. Hill Top Farm was opened to the public by the National Trust in 1946; her artwork was displayed there until 1985 when it was moved to William Heelis’s former law offices in Hawkshead, also owned by the National Trust as the Beatrix Potter Gallery.
Potter gave her folios of mycological drawings to the Armitt Library and Museum in Ambleside before her death. "The Tale of Peter Rabbit" is owned by Frederick Warne and Company, "The Tailor of Gloucester" by the Tate Gallery and "The Tale of the Flopsy Bunnies" by the British Museum.
The largest public collection of her letters and drawings is the Leslie Linder Bequest and Leslie Linder Collection at the Victoria and Albert Museum in London. In the United States, the largest public collections are those in the Special Collections of the Free Library of Philadelphia, and the Lloyd Cotsen Children’s Library at Princeton University.
Themes.
There are many interpretations of Potter’s literary work, the sources of her art, and her life and times. These include critical evaluations of her corpus of children's literature, and Modernist interpretations of Humphrey Carpenter and Katherine Chandler. Judy Taylor, "That Naughty Rabbit: Beatrix Potter and Peter Rabbit" (rev. 2002) tells the story of the first publication and many editions.
Potter’s country life and her farming has also been widely discussed in the work of Susan Denyer and by other authors in the publications of The National Trust.
Potter's work as a scientific illustrator and her work in mycology is highlighted in several chapters in Linda Lear, "Beatrix Potter: A Life in Nature", 2007; "Beatrix Potter: The Extraordinary Life of a Victorian Genius". 2008, UK.
Adaptations and fictionalisations.
In 1971 a ballet film was released, "The Tales of Beatrix Potter", directed by Reginald Mills. Set to music by John Lanchbery with choreography by Frederick Ashton and performed in character costume by members of the Royal Ballet and the Royal Opera House orchestra. The ballet of the same name has been performed by other dance companies around the world.
In 1982, the BBC produced "The Tale of Beatrix Potter". This dramatisation of her life was written by John Hawkesworth and directed by Bill Hayes. It starred Holly Aird and Penelope Wilton as the young and adult Beatrix respectively.
In 2006 Chris Noonan directed "Miss Potter", a biopic of Potter’s life focusing on her early career and romance with her editor Norman Warne. Renée Zellweger and Ewan McGregor play the lead roles.
Potter is also featured in a series of light mysteries called "The Cottage Tales of Beatrix Potter" by Susan Wittig Albert. The eight books in the series starting with the "Tale of Hill Top Farm" (2004) deal with her life in the Lake District and the village of Near Sawrey between 1905 and 1913.
Publications.
The 23 Tales
Other books

</doc>
<doc id="4482" url="http://en.wikipedia.org/wiki?curid=4482" title="Liberal Party (UK)">
Liberal Party (UK)

The Liberal Party was a liberal political movement that formed one of the two major political parties in the United Kingdom during the 19th and early 20th centuries. Its influence then waned, but not before it had moved toward social liberalism and introduced important elements of Britain's welfare state.
The party arose from an alliance of Whigs and free-trade Peelites and Radicals during the 1850s. By the end of the nineteenth century, it had formed four governments under William Gladstonealthough they were punctuated by heavy election defeats. Despite becoming divided over the issue of Irish Home Rule, the party returned to power in 1906 with a landslide victory and, between then and the onset of World War I, Liberal governments oversaw the welfare reforms that created a basic British welfare state. During this time, the party's other two most significant leaders came to the fore: H. H. Asquith, Prime Minister between 1908 and 1916; and David Lloyd George, who followed Asquith as Prime Minister for the rest of World War I and thereafter until 1922.
1922 marked the end of the coalition the party had formed with the Conservative ("Tory") Party during the war and the last time the party was, in government, anything more than a junior coalition partner. By the end of the 1920s, the Labour Party had replaced it as the Tories' primary rival and the party went into a decline that, by the 1950s, saw it winning no more than six seats at general elections. Apart from a few notable by-election victories, the party's fortunes did not improve significantly until it formed the SDP–Liberal Alliance with the newly formed Social Democratic Party (SDP) in 1981. At the next general election, in 1983, the Alliance received over a quarter of the overall vote, but only secured 23 of the 650 seats contested. After the 1987 general election saw this share fall below 23%, the Liberal and SDP parties formally merged in 1988 to form the Liberal Democrats. A small splinter Liberal Party was formed in 1989 by former party members opposed to the merger.
Two of the most prominent intellectuals associated with the Liberal Party were the economist John Maynard Keynes and social planner William Beveridge.
Ideology.
During the 19th century, the Liberal Party was broadly in favour of what would today be called classical liberalism: supporting "laissez-faire" economic policies such as free trade and minimal government interference in the economy (this doctrine was usually termed 'Gladstonian Liberalism' after the Victorian era Liberal Prime Minister William Ewart Gladstone). The Liberal Party favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists) and an extension of the electoral franchise. Sir William Harcourt, a prominent Liberal politician in the Victorian era, said this about liberalism in 1872:
If there be any party which is more pledged than another to resist a policy of restrictive legislation, having for its object social coercion, that party is the Liberal party. (Cheers.) But liberty does not consist in making others do what you think right, (Hear, hear.) The difference between a free Government and a Government which is not free is principally this—that a Government which is not free interferes with everything it can, and a free Government interferes with nothing except what it must. A despotic Government tries to make everybody do what it wishes; a Liberal Government tries, as far as the safety of society will permit, to allow everybody to do as he wishes. It has been the tradition of the Liberal party consistently to maintain the doctrine of individual liberty. It is because they have done so that England is the place where people can do more what they please than in any other country in the world...It is this practice of allowing one set of people to dictate to another set of people what they shall do, what they shall think, what they shall drink, when they shall go to bed, what they shall buy, and where they shall buy it, what wages they shall get and how they shall spend them, against which the Liberal party have always protested.
The political terms of "modern", "progressive" or "new" Liberalism began to appear in the mid to late 1880s and became increasingly common to denote the tendency in the Liberal Party to favour an increased role for the state as more important than the classical liberal stress on self-help and freedom of choice.
By the early 20th century the Liberals stance began to shift towards "New Liberalism", what would today be called social liberalism: a belief in personal liberty with a support for government intervention to provide minimum levels of welfare. This shift was best exemplified by the Liberal government of H. H. Asquith and his Chancellor David Lloyd George, whose Liberal reforms in the early 1900s created a basic welfare state.
David Lloyd George adopted a programme at the 1929 general election entitled "We Can Conquer Unemployment!", although by this stage the Liberals had declined to third-party status. The Liberals now (as expressed in the Liberal Yellow Book) regarded opposition to state intervention as being a characteristic of right-wing extremists.
After nearly becoming extinct in the 1940s and 50s, the Liberal Party revived its fortunes somewhat under the leadership of Jo Grimond in the 1960s, by positioning itself as a radical centrist non-socialist alternative to the Conservative and Labour Party governments of the time.
Origins.
The Liberal Party grew out of the Whigs, who had their origins in an aristocratic faction in the reign of Charles II, and the early 19th century Radicals. The Whigs were in favour of reducing the power of the Crown and increasing the power of Parliament. Although their motives in this were originally to gain more power for themselves, the more idealistic Whigs gradually came to support an expansion of democracy for its own sake. The great figures of reformist Whiggery were Charles James Fox (died 1806) and his disciple and successor Earl Grey. After decades in opposition, the Whigs returned to power under Grey in 1830 and carried the First Reform Act in 1832.
The Reform Act was the climax of Whiggism, but it also brought about the Whigs' demise. The admission of the middle classes to the franchise and to the House of Commons led eventually to the development of a systematic middle class liberalism and the end of Whiggery, although for many years reforming aristocrats held senior positions in the party. In the years after Grey's retirement, the party was led first by Lord Melbourne, a fairly traditional Whig, and then by Lord John Russell, the son of a Duke but a crusading radical, and by Lord Palmerston, a renegade Irish Tory and essentially a conservative, although capable of radical gestures.
As early as 1839 Russell had adopted the name of "Liberals", but in reality his party was a loose coalition of Whigs in the House of Lords and Radicals in the Commons. The leading Radicals were John Bright and Richard Cobden, who represented the manufacturing towns which had gained representation under the Reform Act. They favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists), avoidance of war and foreign alliances (which were bad for business), and above all free trade. For a century, free trade remained the one cause which could unite all Liberals.
In 1841 the Liberals lost office to the Conservatives under Sir Robert Peel, but their period in opposition was short, because the Conservatives split over the repeal of the Corn Laws, a free trade issue, and a faction known as the Peelites (but not Peel himself, who died soon after) defected to the Liberal side. This allowed ministries led by Russell, Palmerston, and the Peelite Lord Aberdeen to hold office for most of the 1850s and 1860s. A leading Peelite was William Ewart Gladstone, who was a reforming Chancellor of the Exchequer in most of these governments. The formal foundation of the Liberal Party is traditionally traced to 1859 and the formation of Palmerston's second government.
The Whig-Radical amalgam could not become a true modern political party, however, while it was dominated by aristocrats, and it was not until the departure of the "Two Terrible Old Men", Russell and Palmerston, that Gladstone could become the first leader of the modern Liberal Party. This was brought about by Palmerston's death in 1865 and Russell's retirement in 1868. After a brief Conservative government (during which the Second Reform Act was passed by agreement between the parties) Gladstone won a huge victory at the 1868 election and formed the first Liberal government. The establishment of the party as a national membership organisation came with the foundation of the National Liberal Federation in 1877.
The Gladstonian era.
For the next thirty years Gladstone and Liberalism were synonymous. William Ewart Gladstone served as prime minister four times (1868–74, 1880–85, 1886, and 1892–94). His financial policies, based on the notion of balanced budgets, low taxes, and laissez-faire, were suited to a developing capitalist society, but they could not respond effectively as economic and social conditions changed. Called the "Grand Old Man" later in life, Gladstone was always a dynamic popular orator who appealed strongly to the working class and to the lower middle class. Deeply religious, Gladstone brought a new moral tone to politics, with his evangelical sensibility and his opposition to aristocracy. His moralism often angered his upper-class opponents (including Queen Victoria), and his heavy-handed control split the Liberal Party.
In foreign policy, Gladstone was in general against foreign entanglements, but he did not resist the realities of imperialism. For example, he approved of the occupation of Egypt by British forces in 1882. His goal was to create a European order based on co-operation rather than conflict and on mutual trust instead of rivalry and suspicion; the rule of law was to supplant the reign of force and self-interest. This Gladstonian concept of a harmonious Concert of Europe was opposed to and ultimately defeated by a Bismarckian system of manipulated alliances and antagonisms.
As prime minister 1868 to 1874, Gladstone headed a Liberal Party which was a coalition of Peelites like himself, Whigs, and Radicals; he was now a spokesman for "peace, economy and reform." One major achievement was the Elementary Education Act of 1870, which provided England with an adequate system of elementary schools for the first time and made attendance compulsory. He also secured the abolition of the purchase of commissions in the army and of religious tests for admission to Oxford and Cambridge; the introduction of the secret ballot in elections; the legalization of trade unions; and the reorganization of the judiciary in the Judicature Act.
Regarding Ireland, the major Liberal achievements were land reform, where he ended centuries of landlord oppression, and the disestablishment of the (Anglican) Church of Ireland through the Irish Church Act 1869.
In the 1874 general election Gladstone was defeated by the Conservatives under Disraeli during a sharp economic recession. He formally resigned as Liberal leader and was succeeded by the Marquess of Hartington, but he soon changed his mind and returned to active politics. He strongly disagreed with Disraeli's pro-Ottoman foreign policy and in 1880 he conducted the first outdoor mass-election campaign in Britain, known as the Midlothian campaign. The Liberals won a large majority in the 1880 election. Hartington ceded his place and Gladstone resumed office.
Among the consequences of the Third Reform Act (1884–85) was the giving of the vote to the Catholic peasants in Ireland, and the consequent creation of an Irish Parliamentary Party led by Charles Stewart Parnell. In the 1885 general election this party won the balance of power in the House of Commons, and demanded Irish Home Rule as the price of support for a continued Gladstone ministry. Gladstone personally supported Home Rule, but a strong Liberal Unionist faction led by Joseph Chamberlain, along with the last of the Whigs, Hartington, opposed it. The Irish Home Rule bill gave all owners of Irish land a chance to sell to the state at a price equal to 20 years' purchase of the rents and allowing tenants to purchase the land. Irish nationalist reaction was mixed, Unionist opinion was hostile, and the election addresses during the 1886 election revealed English radicals to be against the bill also. Among the Liberal rank and file, several Gladstonian candidates disowned the bill, reflecting fears at the constituency level that the interests of the working people were being sacrificed to finance a rescue operation for the landed elite.
The result was a catastrophic split in the Liberal Party, and heavy defeat in the 1886 election at the hands of Lord Salisbury. There was a final weak Gladstone ministry in 1892, but it also was dependent on Irish support and failed to get Irish Home Rule through the House of Lords.
Gladstone finally retired in 1894, and his ineffectual successor, Lord Rosebery, led the party to another heavy defeat in the 1895 general election.
A major long-term consequence of the Third Reform Act was the rise of Lib-Lab candidates, in the absence of any committed Labour Party. The Act split all county constituencies (which were represented by multiple MPs) into single-member constituencies, roughly corresponding to population patterns. In areas with working class majorities, in particular coal-mining areas, Lib-Lab candidates were popular, and they received sponsorship and endorsement from trade unions. In the first election after the Act was passed (1885), thirteen were elected, up from two in 1874. The Third Reform Act also facilitated the demise of the Whig old guard: in two-member constituencies, it was common to pair a Whig and a radical under the Liberal banner. After the Third Reform Act, fewer Whigs were selected as candidates.
The Liberal zenith.
The Liberals languished in opposition for a decade, while the coalition of Salisbury and Chamberlain held power. The 1890s were marred by infighting between the three principal successors to Gladstone, party leader William Harcourt, former Prime Minister Lord Rosebery, and Gladstone's personal secretary, John Morley. This intrigue finally led Harcourt and Morley to resign their positions in 1898 as they continued to be at loggerheads with Rosebery over Irish home rule and issues relating to imperialism. Replacing Harcourt as party leader was Sir Henry Campbell-Bannerman. Harcourt's resignation briefly muted the turmoil in the party, but the beginning of the Second Boer War soon nearly broke the party apart, with Rosebery and a circle of supporters including important future Liberal leaders H.H. Asquith, Edward Grey, and Richard Burdon Haldane forming a clique dubbed the "Liberal Imperialists" that supported the government in the prosecution of the war. On the other side, more radical members of the party formed a Pro-Boer faction that denounced the conflict and called for an immediate end to hostilities. Quickly rising to prominence among the Pro-Boers was David Lloyd George, a relatively new MP and a master of rhetoric, who took advantage of having a national stage to speak out on a controversial issue to make his name in the party. Harcourt and Morley also sided with this group, though with slightly different aims. Campbell-Bannerman tried to keep these forces together at the head of a moderate Liberal rump, but in 1901 he delivered a speech on the government's "methods of barbarism" in South Africa that pulled him further to the left and nearly tore the party in two. The party was saved after Salisbury's retirement in 1902 when his successor, Arthur Balfour, pushed a series of unpopular initiatives such as a new education bill and Joseph Chamberlain called for a new system of protectionist tariffs. Campbell-Bannerman was able to rally the party around the traditional liberal platform of free trade and land reform and led them to the greatest election victory in their history. This would prove the last time the Liberals won a majority in their own right.
Although he presided over a large majority, Sir Henry Campbell-Bannerman was overshadowed by his ministers, most notably H. H. Asquith at the Exchequer, Edward Grey at the Foreign Office, Richard Burdon Haldane at the War Office and David Lloyd George at the Board of Trade. An ill Campbell-Bannerman retired in 1908 and died later that year. He was succeeded by Asquith, who stepped up the government's radicalism. Lloyd George succeeded Asquith at the Exchequer, and was in turn succeeded at the Board of Trade by Winston Churchill, a recent defector from the Conservatives.
The Liberals pushed through much legislation, including the regulation of working hours, National Insurance and welfare. It was at this time that a political battle over the so-called People's Budget resulted in the passage of an act ending the power of the House of Lords to block legislation. The cost was high, however, as the government was required by the king to call two general elections in 1910 to validate its position and ended up frittering away most of its large majority, being left once again dependent on the Irish Nationalists.
As a result Asquith was forced to introduce a new third Home Rule bill in 1912. Since the House of Lords no longer had the power to block the bill, the Unionist's Ulster Volunteers led by Sir Edward Carson, launched a campaign of opposition that included the threat of armed resistance in Ulster and the threat of mutiny by army officers in Ireland in 1914 ("see Curragh Incident"). In their resistance to Home Rule the Ulster Protestants had the full support of the Conservatives, whose leader, Andrew Bonar Law, was of Ulster-Scots descent. The country seemed to be on the brink of civil war when the First World War broke out in August 1914. Historian George Dangerfield has argued that the multiplicity of crises in 1910 to 1914, before the war broke out, so weakened the Liberal coalition that it marked the "Strange Death of Liberal England." However, most historians date the collapse to the crisis of the First World War.
Liberal decline.
The war struck at the heart of everything British Liberals believed in. The party divided over the distinctly illiberal policies that were introduced under its auspices, including conscription and the Defence of the Realm Act. Several Cabinet ministers resigned, and Asquith, the master of domestic politics, proved a poor war leader. Lloyd George and Churchill, however, were zealous supporters of the war, and gradually forced the old peace-oriented Liberals out. The poor British performance in the early months of the war forced Asquith to invite the Conservatives into a coalition (on 17 May 1915). This marked the end of the last all-Liberal government. This coalition fell apart at the end of 1916, when the Conservatives withdrew their support from Asquith and gave it instead to Lloyd George, who became Prime Minister at the head of a coalition government largely made up of Conservatives. Asquith and his followers moved to the opposition benches in Parliament and the Liberal Party was split once again.
Wilson argues that Lloyd George abandoned many liberal principles in his single-minded crusade to win the war at all costs. That brought him and like-minded Liberals into a coalition with the Conservatives, largely on the ground long occupied by Conservatives: they were not oriented toward world peace or liberal treatment of Germany, nor discomfited by aggressive and authoritarian measures of state power. More deadly to the future of the Party, says Wilson, was its repudiation by ideological Liberals, who decided sadly that it no longer represented their principles. Finally the presence of the vigorous new Labour Party on the left gave a new home to voters disenchanted with the Liberal Party.
In the 1918 general election Lloyd George, "the Man Who Won the War", led his coalition into another "khaki election", and won a sweeping victory over the Asquithian Liberals and the newly emerging Labour Party. Lloyd George and the Conservative leader Andrew Bonar Law wrote a joint letter of support to candidates to indicate they were considered the official Coalition candidates – this "coupon", as it became known, was issued against many sitting Liberal MPs, often to devastating effect, though not against Asquith himself. Asquith and most of his colleagues lost their seats. Lloyd George still claimed to be leading a Liberal government, but he was increasingly under the influence of the rejuvenated Conservative party. In 1922 the Conservative backbenchers rebelled against the continuation of the coalition, citing in particular the Chanak Crisis over Turkey and Lloyd George's corrupt sale of honours, amongst other grievances, and Lloyd George was forced to resign. The Conservatives came back to power under Bonar Law and then Stanley Baldwin.
At the 1922 and 1923 elections the Liberals won barely a third of the vote and only a quarter of the seats in the House of Commons, as many radical voters abandoned the divided Liberals and went over to Labour. In 1922 Labour became the official opposition. A reunion of the two warring factions took place in 1923 when the new Conservative Prime Minister Stanley Baldwin committed his party to protective tariffs, causing the Liberals to reunite in support of free trade. The party gained ground in the 1923 general election but ominously made most of its gains from Conservatives whilst losing ground to Labour – a sign of the party's direction for many years to come. The party remained the third largest in the House of Commons, but the Conservatives had lost their majority. There was much speculation and fear about the prospect of a Labour government, and comparatively little about a Liberal government, even though it could have plausibly presented an experienced team of ministers compared to Labour's almost complete lack of experience, as well as offering a middle ground that could obtain support from both Conservatives and Labour in crucial Commons divisions. But instead of trying to force the opportunity to form a Liberal government, Asquith decided instead to allow Labour the chance of office, in the belief that they would prove incompetent and this would set the stage for a revival of Liberal fortunes at Labour's expense. It was a fatal error.
Labour was determined to destroy the Liberals and become the sole party of the left. Ramsay MacDonald was forced into a snap election in 1924, and although his government was defeated, he achieved his objective of virtually wiping the Liberals out as many more radical voters now moved to Labour, whilst moderate middle-class Liberal voters concerned about socialism moved to the Conservatives. The Liberals were reduced to a mere forty seats in Parliament, only seven of which had been won against candidates from both parties; and none of these formed a coherent area of Liberal survival. The party seemed finished, and during this period some Liberals, such as Churchill, went over to the Conservatives, while others went over to Labour. (Several Labour ministers of later generations, such as Michael Foot and Tony Benn, were the sons of Liberal MPs.)
Asquith died in 1928 and the enigmatic figure of Lloyd George returned to the leadership and began a drive to produce coherent policies on many key issues of the day. In the 1929 general election he made a final bid to return the Liberals to the political mainstream, with an ambitious programme of state stimulation of the economy called "We Can Conquer Unemployment!", largely written for him by the Liberal economist John Maynard Keynes. The Liberals gained ground, but once again it was at the Conservatives' expense whilst also losing seats to Labour. Indeed the urban areas of the country suffering heavily from unemployment, which might have been expected to respond the most to the radical economic policies of the Liberals, instead gave the party its worst results. By contrast most of the party's seats were won either due to the absence of a candidate from one of the other parties or in rural areas on the "Celtic fringe", where local evidence suggests that economic ideas were at best peripheral to the electorate's concerns. The Liberals now found themselves with 59 members, holding the balance of power in a Parliament where Labour was the largest party but lacked an overall majority. Lloyd George offered a degree of support to the Labour government in the hope of winning concessions, including a degree of electoral reform to introduce the alternative vote, but this support was to prove bitterly divisive as the Liberals increasingly divided between those seeking to gain what Liberal goals they could achieve, those who preferred a Conservative government to a Labour one and vice-versa.
The last majority Liberal Government in Britain was elected in 1906.
The years preceding the First World War were marked by worker strikes and civil unrest and saw many violent confrontations between civilians and the police and armed forces. Other issues of the period included women's suffrage and the Irish Home Rule movement.
After the carnage of 1914–1918, the democratic reforms of the Representation of the People Act 1918 instantly tripled the number of people entitled to vote in Britain from seven to twenty-one million. The Labour Party benefited most from this huge change in the electorate, forming its first minority government in 1924.
The splits over the National Government.
In 1931 MacDonald's government fell apart under the Great Depression, and the Liberals agreed to join his National Government, dominated by the Conservatives. Lloyd George himself was ill and did not actually join. Soon, however, the Liberals faced another divisive crisis when a National Government was proposed to fight the 1931 general election with a mandate for tariffs. From the outside, Lloyd George called for the party to abandon the government completely in defence of free trade, but only a few MPs and candidates followed. Another group under Sir John Simon then emerged, who were prepared to continue their support for the government and take the Liberal places in the Cabinet if there were resignations. The third group under Sir Herbert Samuel pressed for the parties in government to fight the election on separate platforms. In doing so the bulk of Liberals remained supporting the government, but two distinct Liberal groups had emerged within this bulk – the Liberal Nationals (officially the "National Liberals" after 1947) led by Simon, also known as "Simonites", and the "Samuelites" or "official Liberals", led by Samuel who remained as the official party. Both groups secured about 34 MPs but proceeded to diverge even further after the election, with the Liberal Nationals remaining supporters of the government throughout its life. There were to be a succession of discussions about them rejoining the Liberals, but these usually foundered on the issues of free trade and continued support for the National Government. The one significant reunification came in 1946 when the Liberal and Liberal National party organisations in London merged.
The official Liberals found themselves a tiny minority within a government committed to protectionism. Slowly they found this issue to be one they could not support. In early 1932 it was agreed to suspend the principle of collective responsibility to allow the Liberals to oppose the introduction of tariffs. Later in 1932 the Liberals resigned their ministerial posts over the introduction of the Ottawa Agreement on Imperial Preference. However, they remained sitting on the government benches supporting it in Parliament, though in the country local Liberal activists bitterly opposed the government. Finally in late 1933 the Liberals crossed the floor of the House of Commons and went into complete opposition. By this point their number of MPs was severely depleted. In the 1935 general election, just 17 Liberal MPs were elected, along with Lloyd George and three followers as "independent Liberals". Immediately after the election the two groups reunited, though Lloyd George declined to play much of a formal role in his old party. Over the next ten years there would be further defections as MPs deserted to either the Liberal Nationals or Labour. Yet there were a few recruits, such as Clement Davies, who had deserted to the National Liberals in 1931 but now returned to the party during the Second World War and who would lead it after the war.
Near extinction.
Samuel had lost his seat in the 1935 election and the leadership of the party fell to Sir Archibald Sinclair. With many traditional domestic Liberal policies now regarded as irrelevant, he focused the party on opposition to both the rise of Fascism in Europe and the appeasement foreign policy of the British government, arguing that intervention was needed, in contrast to the Labour calls for pacifism. Despite the party's weaknesses, Sinclair gained a high profile as he sought to recall the Midlothian Campaign and once more revitalise the Liberals as the party of a strong foreign policy.
In 1940 they joined Churchill's wartime coalition government, with Sinclair serving as Secretary of State for Air, the last British Liberal to hold Cabinet rank office for seventy years. However, it was a sign of the party's lack of importance that they were not included in the War Cabinet. At the 1945 general election, Sinclair and many of his colleagues lost their seats to both Conservatives and Labour, and the party returned just 12 MPs to Westminster. But this was just the beginning of the decline. In 1950, the general election saw the Liberals return just nine MPs. Another general election was called in 1951, and the Liberals were left with just six MPs in parliament; all but one of them were aided by the fact that the Conservatives refrained from fielding candidates in those constituencies.
In 1957 this total fell to five when one of the Liberal MPs died and the subsequent by-election was lost to the Labour Party, which selected the former Liberal Deputy Leader Lady Megan Lloyd George as its own candidate. The Liberal Party seemed close to extinction. During this low period, it was often joked that Liberal MPs could hold meetings in the back of one taxi.
Liberal revival.
Through the 1950s and into the 1960s the Liberals survived only because a handful of constituencies in rural Scotland and Wales clung to their Liberal traditions, whilst in two English towns, Bolton and Huddersfield, local Liberals and Conservatives agreed to each contest only one of the town's two seats. Jo Grimond, for example, who became Liberal leader in 1956, was MP for the remote Orkney and Shetland islands. Under his leadership a Liberal revival began, marked by the Orpington by-election of March 1962 which was won by Eric Lubbock. There, the Liberals won a seat in the London suburbs for the first time since 1935.
The Liberals became the first of the major British political parties to advocate British membership of the European Economic Community. Grimond also sought an intellectual revival of the party, seeking to position it as a non-socialist radical alternative to the Conservative government of the day. In particular he canvassed the support of the young post-war university students and recent graduates, appealing to younger voters in a way that many of his recent predecessors had not, and asserting a new strand of Liberalism for the post-war world.
The new middle-class suburban generation began to find the Liberals' policies attractive again. Under Grimond (who retired in 1967) and his successor, Jeremy Thorpe, the Liberals regained the status of a serious third force in British politics, polling up to 20% of the vote but unable to break the duopoly of Labour and Conservative and win more than fourteen seats in the Commons. An additional problem was competition in the Liberal heartlands in Scotland and Wales from the Scottish National Party and Plaid Cymru who both grew as electoral forces from the 1960s onwards. Although Emlyn Hooson held on to the seat of Montgomeryshire, upon Clement Davies death in 1962, the party lost five Welsh seats between 1950 and 1966. In September 1966 the Welsh Liberal Party formed their own state party, moving the Liberal Party into a fully federal structure.
In local elections Liverpool remained a Liberal stronghold, with the party taking the plurality of seats on the elections to the new Liverpool Metropolitan Borough Council in 1973. In the February 1974 general election the Conservative government of Edward Heath won a plurality of votes cast, but the Labour Party gained a plurality of seats due to the Ulster Unionist MPs refusing to support the Conservatives after the Northern Ireland Sunningdale Agreement. The Liberals now held the balance of power in the Commons. Conservatives offered Thorpe the Home Office if he would join a coalition government with Heath. Thorpe was personally in favour of it, but the party insisted on a clear government commitment to introducing proportional representation and a change of Prime Minister. The former was unacceptable to Heath's Cabinet and the latter to Heath personally, so the talks collapsed. Instead a minority Labour government was formed under Harold Wilson but with no formal support from Thorpe. In the October 1974 general election the Liberals slipped back slightly and the Labour government won a wafer-thin majority.
Thorpe was subsequently forced to resign after allegations about his private life. The party's new leader, David Steel, negotiated the Lib-Lab pact with Wilson's successor as Prime Minister, James Callaghan. According to this pact, the Liberals would support the government in crucial votes in exchange for some influence over policy. The agreement lasted from 1977 to 1978, but proved mostly fruitless, for two reasons: the Liberals' key demand of proportional representation was rejected by most Labour MPs, whilst the contacts between Liberal spokespersons and Labour ministers often proved detrimental, such as between finance spokesperson John Pardoe and Chancellor of the Exchequer Denis Healey, who were mutually antagonistic.
Alliance with Social Democrats.
When the Labour government fell in 1979, the Conservatives under Margaret Thatcher won a victory which served to push the Liberals back into the margins.
In 1981, defectors from the moderate wing of the Labour Party, led by former Cabinet ministers Roy Jenkins, David Owen and Shirley Williams, founded the Social Democratic Party. The new party and the Liberals quickly formed an alliance, which for a while polled as high as 50% in the opinion polls and appeared capable of winning the next general election. However, they were later overtaken in the polls by the Conservatives and at the 1983 general election the Conservatives triumphed by a landslide, with Labour once again forming the opposition, while the SDP-Liberal Alliance came close to Labour in terms of votes (a share of more than 25%) although it only had 23 MPs compared to Labour's 209.
The 1987 election saw the Alliance's share of the votes drop slightly and it now had 22 MPs, and in the election's aftermath Liberal leader David Steel proposed a merger of the two parties. Most SDP members voted in favour of the merger, but SDP leader David Owen objected and continued to lead a "rump" SDP, with the merger of the two parties being completed in March 1988 to form the Social and Liberal Democrats, becoming the Liberal Democrats in October 1989.
Merger with Social Democrats.
In 1988 the Liberals and Social Democrats merged to create what came to be called the Liberal Democrats. Over two-thirds of the members, and all the serving MPs, of the Liberal Party joined this party, led first jointly by Steel and the SDP leader Robert Maclennan, and later by Paddy Ashdown (1988–99), Charles Kennedy (1999–2006), Sir Menzies Campbell (2006–07) and Nick Clegg (incumbent).
Though the merger process was traumatic and the new party suffered a few years of extremely poor poll results, it gradually found much greater electoral success than the Liberal Party had achieved in the post-war era. In the 2005 general election, 62 Liberal Democrat MPs were elected to the House of Commons, a far cry from the days when the Liberals had just 5 MPs and Liberalism as a political force had seemed moribund.
As with the Liberal Party for most of the 20th century, the Liberal Democrats face constant questioning about which of the other two parties they are closer to, in particular about which they would support in the event of a hung parliament. The party is keen to maintain its independent identity, however, and argues that the need for a modern Liberal force in British politics has never been greater.
In the 2010 General Election, the Conservative Party won more seats than any other, but not enough to form a majority government. After several days of negotiation, the Liberal Democrats agreed to join the Conservatives as part of a coalition government.
The post-1988 Liberal Party.
A group of Liberal opponents of the merger with the Social Democrats, including Michael Meadowcroft (formerly Liberal MP for Leeds West) and Paul Wiggin (who served on Peterborough City Council as a Liberal), continued under the old name of "the Liberal Party". This was legally a new organisation (the headquarters, records, assets and debts of the old party were inherited by the Liberal Democrats), but its constitution asserts it to be the same Liberal party. The party retains influence in some local councils. Meadowcroft himself eventually joined the Liberal Democrats in 2007.
National Liberal Party.
In 1931 a number of Liberal MPs split from the original Liberal Party in protest at the leadership's adherence to free trade. The party name was appropriated in 2006 by the far-right political think tank the Third Way.

</doc>
<doc id="4484" url="http://en.wikipedia.org/wiki?curid=4484" title="Bank of England">
Bank of England

The Bank of England, formally the Governor and Company of the Bank of England, is the central bank of the United Kingdom and the model on which most modern central banks have been based. Established in 1694, it is the second oldest central bank in the world, after the Sveriges Riksbank, and the world's 8th oldest bank. It was established to act as the English Government's banker, and is still the banker for the Government of the United Kingdom. The Bank was privately owned by stockholders from its foundation in 1694 until nationalised in 1946.
In 1998, it became an independent public organisation, wholly owned by the Treasury Solicitor on behalf of the government, with independence in setting monetary policy.
The Bank is one of eight banks authorised to issue banknotes in the United Kingdom, but has a monopoly on the issue of banknotes in England and Wales and regulates the issue of banknotes by commercial banks in Scotland and Northern Ireland.
The Bank's Monetary Policy Committee has devolved responsibility for managing monetary policy. The Treasury has reserve powers to give orders to the committee "if they are required in the public interest and by extreme economic circumstances" but such orders must be endorsed by Parliament within 28 days. The Bank's Financial Policy Committee held its first meeting in June 2011 as a macro prudential regulator to oversee regulation of the UK's financial sector.
The Bank's headquarters have been in London's main financial district, the City of London, on Threadneedle Street, since 1734. It is sometimes known by the metonym "The Old Lady of Threadneedle Street" or "The Old Lady", a name taken from the legend of Sarah Whitehead, whose ghost is said to haunt the Bank's garden. The busy road junction outside is known as Bank junction.
Mark Carney assumed the post of The Governor of the Bank of England on 1 July 2013. He succeeded Mervyn King, who took over on 30 June 2003. Carney, a Canadian, will serve an initial five-year term rather than the typical eight, and will seek UK citizenship. He is the first non-British citizen to hold the post. As of January 2014, the Bank also has three Deputy Governors.
History.
England's crushing defeat by France, the dominant naval power, in naval engagements culminating in the 1690 Battle of Beachy Head, became the catalyst for England's rebuilding itself as a global power. England had no choice but to build a powerful navy. No public funds were available, and the credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8 per cent) that the government wanted.
To induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The Bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue bank notes. The lenders would give the government cash (bullion) and issue notes against the government bonds, which can be lent again. The £1.2m was raised in 12 days; half of this was used to rebuild the navy.
As a side effect, the huge industrial effort needed, from establishing iron-works to make more nails to agriculture feeding the quadrupled strength of the navy, started to transform the economy. This helped the new Kingdom of Great Britain – England and Scotland were formally united in 1707 – to become powerful. The power of the navy made Britain the dominant world power in the late eighteenth and early nineteenth centuries.
The establishment of the bank was devised by Charles Montagu, 1st Earl of Halifax, in 1694, to the plan which had been proposed by William Paterson three years before, but not acted upon. He proposed a loan of £1.2m to the government; in return the subscribers would be incorporated as The Governor and Company of the Bank of England with long-term banking privileges including the issue of notes. The Royal Charter was granted on 27 July through the passage of the Tonnage Act 1694. Public finances were in so dire a condition at the time that the terms of the loan were that it was to be serviced at a rate of 8% per annum, and there was also a service charge of £4,000 per annum for the management of the loan. The first governor was Sir John Houblon, who is depicted in the £50 note issued in 1994. The charter was renewed in 1742, 1764, and 1781.
The Bank's original home was in Walbrook in the City of London, where during reconstruction in 1954 archaeologists found the remains of a Roman temple of Mithras (Mithras was – rather fittingly – worshipped as being the God of Contracts); the Mithraeum ruins are perhaps the most famous of all twentieth-century Roman discoveries in the City of London and can be viewed by the public.
The Bank moved to its current location on Threadneedle Street, and thereafter slowly acquired neighbouring land to create the edifice seen today. Sir Herbert Baker's rebuilding of the Bank, demolishing most of Sir John Soane's earlier building, was described by architectural historian Nikolaus Pevsner as "the greatest architectural crime, in the City of London, of the twentieth century".
When the idea and reality of the National Debt came about during the 18th century this was also managed by the Bank. By the charter renewal in 1781 it was also the bankers' bank – keeping enough gold to pay its notes on demand until 26 February 1797 when war had so diminished gold reserves that the government prohibited the Bank from paying out in gold. This prohibition lasted until 1821.
The Bank also had a narrow escape of a different nature. In 1780, mobs involved in the Gordon Riots tried to storm the building. Thereafter, every night until 1973, a detachment of soldiers known as the Bank Picquet usually provided by the Brigade of Guards patrolled the perimeter to ensure the safety of the nation's gold.
19th century.
The 1844 Bank Charter Act tied the issue of notes to the gold reserves and gave the Bank sole rights with regard to the issue of banknotes. Private banks that had previously had that right retained it, provided that their headquarters were outside London and that they deposited security against the notes that they issued. A few English banks continued to issue their own notes until the last of them was taken over in the 1930s. Scottish and Northern Irish private banks still have that right.
The bank acted as lender of last resort for the first time in the panic of 1866.
20th century.
Britain remained on the gold standard until 1931 when the gold and foreign exchange reserves were transferred to the treasury, but their management was still handled by the Bank.
During the governorship of Montagu Norman, from 1920-44, the Bank made deliberate efforts to move away from commercial banking and become a central bank. In 1946, shortly after the end of Norman's tenure, the bank was nationalised by the Labour government.
After 1945 the Bank pursued the multiple goals of Keynesian economics, especially "easy money" and low interest rates to support aggregate demand. It tried to keep a fixed exchange rate, and attempted to deal with inflation and sterling weakness by credit and exchange controls.
In 1977, the Bank set up a wholly owned subsidiary called Bank of England Nominees Limited (BOEN), a private limited company, with two of its hundred £1 shares issued. According to its Memorandum & Articles of Association, its objectives are:- “To act as Nominee or agent or attorney either solely or jointly with others, for any person or persons, partnership, company, corporation, government, state, organisation, sovereign, province, authority, or public body, or any group or association of them...” Bank of England Nominees Limited was granted an exemption by Edmund Dell, Secretary of State for Trade, from the disclosure requirements under Section 27(9) of the Companies Act 1976, because, “it was considered undesirable that the disclosure requirements should apply to certain categories of shareholders.” The Bank of England is also protected by its Royal Charter status, and the Official Secrets Act. BOEN is a vehicle for governments and heads of state to invest in UK companies (subject to approval from the Secretary of State), providing they undertake "not to influence the affairs of the company". BOEN is no longer exempt from company law disclosure requirements. Although a dormant company, dormancy does not preclude a company actively operating as a nominee shareholder. BOEN has two shareholders: the Bank of England, and the Secretary of the Bank of England.
In 1981 the reserve requirement for banks to hold a minimum fixed proportion of their deposits as reserves at the Bank of England was abolished – see reserve requirement#United Kingdom for more details.
On 6 May 1997, following the 1997 general election which brought a Labour government to power for the first time since 1979, it was announced by the Chancellor of the Exchequer, Gordon Brown, that the Bank would be granted operational independence over monetary policy. Under the terms of the Bank of England Act 1998 (which came into force on 1 June 1998), the Bank's Monetary Policy Committee was given sole responsibility for setting interest rates to meet the Government's Retail Prices Index (RPI) inflation target of 2.5%. The target has changed to 2% since the Consumer Price Index (CPI) replaced the Retail Prices Index as the treasury's inflation index. If inflation overshoots or undershoots the target by more than 1%, the Governor has to write a letter to the Chancellor of the Exchequer explaining why, and how he will remedy the situation.
The handing over of monetary policy to the Bank had been a key plank of the Liberal Democrats' economic policy since the 1992 general election. Conservative MP Nicholas Budgen had also proposed this as a private member's bill in 1996, but the bill failed as it had the support of neither the government nor the opposition.
Functions of the Bank.
The Bank performs all the functions of a central bank. The most important of these is supposed to be maintaining price stability and supporting the economic policies of the Government, thus promoting economic growth. There are two main areas which are tackled by the Bank to ensure it carries out these functions efficiently:
Financial stability.
The Bank works together with other institutions to secure both monetary and financial stability, including:
The 1997 Memorandum of Understanding describes the terms under which the Bank, the Treasury and the FSA work toward the common aim of increased financial stability. In 2010 the incoming Chancellor announced his intention to merge the FSA back into the Bank. As of 2012, the current director for financial stability is Andy Haldane.
The Bank acts as the government's banker, and it maintains the government's Consolidated Fund account. It also manages the country's foreign exchange and gold reserves. The Bank also acts as the bankers' bank, especially in its capacity as a lender of last resort.
The Bank has a monopoly on the issue of banknotes in England and Wales. Scottish and Northern Irish banks retain the right to issue their own banknotes, but they must be backed one to one with deposits in the Bank, excepting a few million pounds representing the value of notes they had in circulation in 1845. The Bank decided to sell its bank note printing operations to De La Rue in December 2002, under the advice of Close Brothers Corporate Finance Ltd.
Since 1998, the Monetary Policy Committee (MPC) has had the responsibility for setting the official interest rate. However, with the decision to grant the Bank operational independence, responsibility for government debt management was transferred to the new UK Debt Management Office in 1998, which also took over government cash management in 2000. Computershare took over as the registrar for UK Government bonds (gilt-edged securities or "gilts") from the Bank at the end of 2004.
The Bank used to be responsible for the regulation and supervision of the banking and insurance industries, although this responsibility was transferred to the Financial Services Authority in June 1998. After the financial crises in 2008 new banking legislation transferred the responsibility for regulation and supervision of the banking and insurance industries back to the Bank.
In 2011 the interim Financial Policy Committee (FPC) was created as a mirror committee to the MPC to spearhead the Bank's new mandate on financial stability. The FPC is responsible for macro prudential regulation of all UK banks and insurance companies.
To help maintain economic stability, the Bank attempts to broaden understanding of its role, both through regular speeches and publications by senior Bank figures, a semiannual Financial Stability Report, and through a wider education strategy aimed at the general public. It maintains a free museum and runs the Target Two Point Zero competition for A-level students.
Asset purchase facility.
The Bank has operated, since January 2009, an Asset Purchase Facility (APF) to buy "high-quality assets financed by the issue of Treasury bills and the DMO's cash management operations" and thereby improve liquidity in the credit markets. It has, since March 2009, also provided the mechanism by which the Bank's policy of quantitative easing (QE) is achieved, under the auspices of the MPC. Along with the managing the £200 billion of QE funds, the APF continues to operate its corporate facilities. Both are undertaken by a subsidiary company of the Bank of England, the Bank of England Asset Purchase Facility Fund Limited (BEAPFF).
Banknote issues.
The Bank has issued banknotes since 1694. Notes were originally hand-written; although they were partially printed from 1725 onwards, cashiers still had to sign each note and make them payable to someone. Notes were fully printed from 1855. Until 1928 all notes were "White Notes", printed in black and with a blank reverse. In the 18th and 19th centuries White Notes were issued in £1 and £2 denominations. During the 20th century White Notes were issued in denominations between £5 and £1000.
Until the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. The Bank Charter Act 1844 began the process of restricting note issue to the Bank; new banks were prohibited from issuing their own banknotes and existing note-issuing banks were not permitted to expand their issue. As provincial banking companies merged to form larger banks, they lost their right to issue notes, and the English private banknote eventually disappeared, leaving the Bank with a monopoly of note issue in England and Wales. The last private bank to issue its own banknotes in England and Wales was Fox, Fowler and Company in 1921. However, the limitations of the 1844 Act only affected banks in England and Wales, and today three commercial banks in Scotland and four in Northern Ireland continue to issue their own banknotes, regulated by the Bank.
At the start of the First World War, the Currency and Bank Notes Act 1914 was passed, which granted temporary powers to HM Treasury for issuing banknotes to the value of £1 and 10/- (ten shillings). Treasury notes had full legal tender status and were not convertible for gold through the Bank, replacing the gold coin in circulation to prevent a run on sterling and to enable raw material purchases for armament production. These notes featured an image of King George V (Bank of England notes did not begin to display an image of the monarch until 1960). The wording on each note was:
Treasury notes were issued until 1928, when the Currency and Bank Notes Act 1928 returned note-issuing powers to the banks. The Bank of England issued notes for ten shillings and one pound for the first time on 22 November 1928.
During the Second World War the German Operation Bernhard attempted to counterfeit denominations between £5 and £50 producing 500,000 notes each month in 1943. The original plan was to parachute the money on the UK in an attempt to destabilise the British economy, but it was found more useful to use the notes to pay German agents operating throughout Europe – although most fell into Allied hands at the end of the war, forgeries frequently appeared for years afterwards, which led banknote denominations above £5 to be removed from circulation.
In 2006, over £53 million in banknotes belonging to the bank was stolen from a depot in Tonbridge, Kent.
Modern banknotes are printed by contract with De La Rue Currency in Loughton, Essex.
The vault.
The Bank is custodian to the official gold reserves of the United Kingdom and many other countries. The vault, beneath the City of London, covers a floor space greater than that of the second-tallest building in the City, Tower 42, and needs keys that are three feet long to open. The Bank is the 15th-largest custodian of gold reserves, holding around 4600 tonnes. These gold deposits were estimated in February 2012 to have a current market value of £156,000,000,000.
Governors of the Bank of England.
Governors of the Bank of England (20~21st century) is as follows:

</doc>
<doc id="4485" url="http://en.wikipedia.org/wiki?curid=4485" title="Bakelite">
Bakelite

Bakelite ( ), or polyoxybenzylmethylenglycolanhydride, is an early plastic. It is a thermosetting phenol formaldehyde resin, formed from an elimination reaction of phenol with formaldehyde. It was developed by Shaun Evans and Alex Roberts.
One of the first plastics made from synthetic components, Bakelite was used for its electrical nonconductivity and heat-resistant properties in electrical insulators, radio and telephone casings, and such diverse products as kitchenware, jewelry, pipe stems, firearms, and children's toys. Bakelite was designated a National Historic Chemical Landmark in 1993 by the American Chemical Society in recognition of its significance as the world's first synthetic plastic. The "retro" appeal of old Bakelite products has made them collectible.
History.
Dr. Baekeland had originally set out to find a replacement for shellac, made from the excretion of lac bugs. Chemists had begun to recognize that many natural resins and fibres were polymers, and Baekeland investigated the reactions of phenol and formaldehyde. He first produced a soluble phenol-formaldehyde shellac called "Novolak" that never became a market success, then turned to developing a binder for asbestos which, at that time, was molded with rubber. By controlling the pressure and temperature applied to phenol and formaldehyde, he produced a hard moldable material and patented in 1907 known as Bakelite. It was the first synthetic thermosetting plastic ever made. It was often referred to as "the material of 1000 uses," a phrase originated by Baekeland himself.
He announced the invention at a meeting of the American Chemical Society on February 5, 1909.
The Bakelite Corporation was formed in 1922 (after patent litigation favorable to Baekeland) from a merger of three companies: the General Bakelite Company, which Baekeland founded in 1910, the Condensite Company, founded by J.W. Aylesworth, and the Redmanol Chemical Products Company, founded by L.V. Redman. A factory was built near Bound Brook, New Jersey, in 1929.
Bakelite Limited, a merger of three British phenol formaldehyde resin suppliers (Damard Lacquer Company Limited of Birmingham, Mouldensite Limited of Darley Dale and Redmanol Chemical Products Company of London) was formed in 1926. A new factory opened in Tyseley, Birmingham, England around 1928. It was demolished in 1998.
In 1939 the companies were acquired by Union Carbide and Carbon Corporation. Union Carbide's phenolic resin business including the Bakelite and Bakelit registered trademarks are owned by Momentive Specialty Chemicals.
Properties.
Phenolics are more rarely used in general consumer products today, due to the cost and complexity of production and their brittle nature. Nevertheless they are still used in some applications where their specific properties are required, such as small precision-shaped components, molded disc brake cylinders, saucepan handles, electrical plugs and switches and parts for electrical irons. Today, Bakelite is manufactured and produced in sheet, rod and tube form for hundreds of industrial applications in the electronics, power generation and aerospace industries, and under a variety of commercial brand names. 
Phenolic sheet is a hard, dense material made by applying heat and pressure to layers of paper or glass cloth impregnated with synthetic resin. These layers of laminations are usually of cellulose paper, cotton fabrics, synthetic yarn fabrics, glass fabrics or unwoven fabrics. When heat and pressure are applied to the layers, a chemical reaction (polymerization) transforms the layers into a high-pressure thermosetting industrial laminated plastic. When rubbed or burnt, Bakelite has a distinctive, acrid, sickly-sweet fishy odor.
Bakelite phenolic sheet is produced in dozens of commercial grades and with various additives to meet diverse mechanical, electrical and thermal requirements. Some common types include:
Note that phenolic resin products are apt to swell slightly if they are used in areas that are perpetually damp. Varnishing the product helps to prevent this.
Synthesis.
Bakelite was a combination of phenol, formaldehyde, and wood flour. The mixture is put under pressure, and after curing, a hard plastic material forms.
Applications and usage.
In its industrial applications, Bakelite was particularly suitable for the emerging electrical and automobile industries because of its extraordinarily high resistance - not only to electricity, but to heat and chemical action. It was soon used for all nonconducting parts of radios and other electrical devices, such as bases and sockets for light bulbs and vacuum tubes, supports for electrical components, automobile distributor caps and other insulators.
Bakelite is used today for wire insulation, brake pads and related automotive components, and industrial electrical-related applications.
In the early 20th century, it was found in myriad applications including saxophone mouthpieces, whistles, cameras, solid-body electric guitars, telephone housings and handsets, early machine guns, pistol grips, and appliance casings. In the pure form it was made into such articles as pipe stems, buttons, etc.
The thermosetting phenolic resin was at one point considered for the manufacture of coins, due to a shortage of traditional material; in 1943, Bakelite and other non-metal materials were tested for usage for the one cent coin in the US before the Mint settled on zinc-coated steel.
After the Second World War, factories were retrofitted to produce Bakelite using a more efficient extrusion process, which increased production and enabled the uses of Bakelite to extend into other genres: jewelry boxes, desk sets, clocks, radios, game pieces like chessmen, poker chips, billiard balls and Mah Jong sets. Kitchenware such as canisters and tableware were also made of Bakelite through the 1950s. Beads, bangles and earrings were produced by the Catalin Company, which introduced 15 new colors in 1927. The creation of marbled Bakelite was also attributed to the Catalin Company. Translucent Bakelite jewelry, poker chips and other gaming items such as chess sets were also introduced in the 1940s under the Prystal Corporation name; however, its basic chemical composition remained the same.
The primary commercial uses for Bakelite today remain in the area of inexpensive board and tabletop games produced in China, India and Hong Kong. Items such as billiard balls, dominoes and pieces for games like chess, checkers, and backgammon are constructed of Bakelite for its look, durability, fine polish, weight, and sound. Common dice are sometimes made of Bakelite for weight and sound, but the majority are made of a thermoplastic polymer such as acrylonitrile butadiene styrene (ABS).
Bakelite is used to make the presentation boxes of Breitling watches and sometimes as a substitute for metal firearm magazines. Bakelite is also used in the mounting of metal samples in metallography.
Phenolic resins have been commonly used in ablative heat shields. Soviet heatshields for ICBM warheads and spacecraft reentry consisted of asbestos textolite, impregnated with Bakelite.
Patents.
The United States Patent and Trademark Office granted Baekeland a patent for a "Method of making insoluble products of phenol and formaldehyde" on December 7, 1909. Producing hard, compact, insoluble and infusable condensation products of phenols and formaldehyde marked the beginning of the modern plastics industry.

</doc>
<doc id="4487" url="http://en.wikipedia.org/wiki?curid=4487" title="Bean">
Bean

Bean () is a common name for large plant seeds used for human food or animal feed of several genera of the family Fabaceae (alternately Leguminosae).
Terminology.
The term "bean" originally referred to the seed of the broad or fava bean, but was later expanded to include members of the New World genus "Phaseolus", such as the common bean and the runner bean, and the related genus "Vigna". The term is now applied generally to many other related plants such as Old World soybeans, peas, chickpeas (garbanzos), vetches, and lupins.
"Bean" is sometimes used as a synonym of "pulse", an edible legume, though the term "pulses" is normally reserved for leguminous crops harvested for their dry grain. The term "bean" usually excludes crops used mainly for oil extraction (such as soybeans and peanuts), as well as those used exclusively for sowing purposes (such as clover and alfalfa). Leguminous crops harvested green for food, such as snap peas, snow peas, and so on, are not considered beans, and are classified as vegetable crops. According to United Nation's Food and Agriculture Organization the term "bean" should include only species of "Phaseolus"; however, a strict consensus definition has proven difficult because in the past, several species such as "Vigna" "angularis" (azuki bean), "mungo" (black gram), "radiata" (green gram), "aconitifolia" (moth bean)) were classified as "Phaseolus" and later reclassified. The use of the term "bean" to refer to species other than "Phaseolus" thus remains. In some countries, the term "bean" can mean a host of different species.
In English usage, the word "bean" is also sometimes used to refer to the seeds or pods of plants that are not in the family leguminosae, but which bear a superficial resemblance to true beans—for example coffee beans, castor beans and cocoa beans (which resemble bean seeds), and vanilla beans, which superficially resemble bean pods.
Cultivation.
Unlike the closely related pea, beans are a summer crop that needs warm temperatures to grow. Maturity is typically 55–60 days from planting to harvest. As the bean pods mature, they turn yellow and dry up, and the beans inside change from green to their mature colour. As a vine, bean plants need external support, which may be provided in the form of special "bean cages" or poles. Native Americans customarily grew them along with corn and squash (the so-called Three Sisters), with the tall cornstalks acting as support for the beans.
In more recent times, the so-called "bush bean" has been developed which does not require support and has all its pods develop simultaneously (as opposed to pole beans which develop gradually). This makes the bush bean more practical for commercial production.
History.
Beans are one of the longest-cultivated plants. Broad beans, also called fava beans, in their wild state the size of a small fingernail, were gathered in Afghanistan and the Himalayan foothills. In a form improved from naturally occurring types, they were grown in Thailand since the early seventh millennium BCE, predating ceramics. They were deposited with the dead in ancient Egypt. Not until the second millennium BCE did cultivated, large-seeded broad beans appear in the Aegean, Iberia and transalpine Europe. In the "Iliad" (late-8th century) is a passing mention of beans and chickpeas cast on the threshing floor.
Beans were an important source of protein throughout Old and New World history, and still are today.
The oldest-known domesticated beans in the Americas were found in Guitarrero Cave, an archaeological site in Peru, and dated to around the second millennium BCE.
Most of the kinds commonly eaten fresh or dried, those of the genus "Phaseolus", come originally from the Americas, being first seen by a European when Christopher Columbus, during his exploration of what may have been the Bahamas, found them growing in fields. Five kinds of "Phaseolus" beans were domesticated by pre-Columbian peoples: common beans ("Phaseolus vulgaris") grown from Chile to the northern part of what is now the United States, and lima and sieva beans ("Phaseolus lunatus"), as well as the less widely distributed teparies ("Phaseolus acutifolius"), scarlet runner beans ("Phaseolus coccineus") and polyanthus beans ("Phaseolus polyanthus") One especially famous use of beans by pre-Columbian people as far north as the Atlantic seaboard is the "Three Sisters" method of companion plant cultivation:
Dry beans come from both Old World varieties of broad beans (fava beans) and New World varieties (kidney, black, cranberry, pinto, navy/haricot).
Beans are a heliotropic plant, meaning that the leaves tilt throughout the day to face the sun. At night, they go into a folded "sleep" position.
Types.
Currently, the world genebanks hold about 40,000 bean varieties, although only a fraction are mass-produced for regular consumption.
Some bean types include:
Toxins.
Some kinds of raw beans, especially red and kidney beans, contain a harmful toxin (lectin phytohaemagglutinin) that must be removed by cooking. A recommended method is to boil the beans for at least ten minutes; undercooked beans may be more toxic than raw beans.
Cooking beans in a slow cooker, because of the lower temperatures often used, may not destroy toxins even though the beans do not smell or taste 'bad' (though this should not be a problem if the food reaches boiling temperature and stays there for some time).
Fermentation is used in some parts of Africa to improve the nutritional value of beans by removing toxins. Inexpensive fermentation improves the nutritional impact of flour from dry beans and improves digestibility, according to research co-authored by Emire Shimelis, from the Food Engineering Program at Addis Ababa University. Beans are a major source of dietary protein in Kenya, Malawi, Tanzania, Uganda and Zambia.
Nutrition.
Beans have significant amounts of fiber and soluble fiber, with one cup of cooked beans providing between nine and 13 grams of fiber. Soluble fiber can help lower blood cholesterol. Beans are also high in protein, complex carbohydrates, folate, and iron.
Flatulence.
Many edible beans, including broad beans and soybeans, contain oligosaccharides (particularly raffinose and stachyose), a type of sugar molecule also found in cabbage. An anti-oligosaccharide enzyme is necessary to properly digest these sugar molecules. As a normal human digestive tract does not contain any anti-oligosaccharide enzymes, consumed oligosaccharides are typically digested by bacteria in the large intestine. This digestion process produces flatulence-causing gases as a byproduct. This aspect of bean digestion is the basis for the children's rhyme "Beans, Beans, the Musical Fruit". Since sugar dissolves in water, another method of reducing flatulence associated with eating beans is to drain the water in which the beans have been cooked. 
Some species of mold produce alpha-galactosidase, an anti-oligosaccharide enzyme, which humans can take to facilitate digestion of oligosaccharides in the small intestine. This enzyme, currently sold in the United States under the brand-names Beano and Gas-X Prevention, can be added to food or consumed separately. In many cuisines beans are cooked along with natural carminatives such as anise seeds, coriander seeds and cumin .
One effective strategy is to soak beans in alkaline (baking soda) water overnight before rinsing thoroughly . Sometimes vinegar is added, but only after the beans are cooked as vinegar interferes with the beans' softening.
Fermented beans will usually not produce most of the intestinal problems that unfermented beans will, since yeast can consume the offending sugars.
Production.
The world leader in production of dry bean is India, followed by Brazil and Myanmar. In Africa, the most important producer is Tanzania.

</doc>
<doc id="4489" url="http://en.wikipedia.org/wiki?curid=4489" title="Breast">
Breast

The breast is the upper ventral region of the torso of a primate, in left and right sides, containing the mammary gland which in a female can secrete milk used to feed infants.
Both men and women develop breasts from the same embryological tissues. However, at puberty, female sex hormones, mainly estrogen, promote breast development which does not occur in men due to the higher amount of testosterone. As a result, women's breasts become far more prominent than those of men.
During pregnancy, the breast is responsive to a complex interplay of hormones that cause tissue development and enlargement in order to produce milk. Three such hormones are estrogen, progesterone and prolactin, which cause glandular tissue in the breast and the uterus to change during the menstrual cycle.
Each breast contains 15–20 lobes. The subcutaneous adipose tissue covering the lobes gives the breast its size and shape. Each lobe is composed of many lobules, at the end of which are sacs where milk is produced in response to hormonal signals.
Etymology.
The English word "breast" derives from the Old English word "brēost" (breast, bosom) from Proto-Germanic "breustam" (breast), from the Proto-Indo-European base bhreus– (to swell, to sprout). The "breast" spelling conforms to the Scottish and North English dialectal pronunciations.
Structure.
In women, the breasts overlay the pectoralis major muscles and usually extend from the level of the second rib to the level of the sixth rib in the front of the human rib cage; thus, the breasts cover much of the chest area and the chest walls. At the front of the chest, the breast tissue can extend from the clavicle (collarbone) to the middle of the sternum (breastbone). At the sides of the chest, the breast tissue can extend into the axilla (armpit), and can reach as far to the back as the latissimus dorsi muscle, extending from the lower back to the humerus bone (the longest bone of the upper arm). As a mammary gland, the breast is composed of layers of different types of tissue, among which predominate two types, adipose tissue and glandular tissue, which effects the lactation functions of the breasts.
Morphologically, the breast is a cone with the base at the chest wall, and the apex at the nipple, the center of the NAC (nipple-areola complex). The superﬁcial tissue layer (superficial fascia) is separated from the skin by 0.5–2.5 cm of subcutaneous fat (adipose tissue). The suspensory Cooper's ligaments are fibrous-tissue prolongations that radiate from the superficial fascia to the skin envelope. The adult breast contains 14–18 irregular lactiferous lobes that converge to the nipple, to ducts 2.0–4.5 mm in diameter; the milk ducts (lactiferous ducts) are immediately surrounded with dense connective tissue that functions as a support framework. The glandular tissue of the breast is biochemically supported with estrogen; thus, when a woman reaches menopause (cessation of menstruation) and her body estrogen levels decrease, the milk gland tissue then atrophies, withers, and disappears, resulting in a breast composed of adipose tissue, superﬁcial fascia, suspensory ligaments, and the skin envelope.
The dimensions and weight of the breast vary among women, ranging from approximately 500 to 1,000 grams (1.1 to 2.2 pounds) each; thus, a small-to-medium-sized breast weighs 500 grams (1.1 pounds) or less; and a large breast weighs approximately 750 to 1,000 grams (1.7 to 2.2 pounds.) The tissue composition ratios of the breast likewise vary among women; some breasts have greater proportions of glandular tissue than of adipose or connective tissues, and vice versa; therefore the fat-to-connective-tissue ratio determines the density (firmness) of the breast. In the course of a woman's life, her breasts will change size, shape, and weight, because of the hormonal bodily changes occurred in thelarche (pubertal breast development), menstruation (fertility), pregnancy (reproduction), the breast-feeding of an infant child, and the climacterium (the end of fertility).
Glandular structure.
The breast is an apocrine gland that produces milk to feed an infant child; for which the nipple of the breast is centred in (surrounded by) an areola (nipple-areola complex, NAC), the skin color of which varies from pink to dark brown, and has many sebaceous glands. The basic units of the breast are the terminal duct lobular units (TDLUs), which produce the fatty breast milk. They give the breast its offspring-feeding functions as a mammary gland. They are distributed throughout the body of the breast; approximately two-thirds of the lactiferous tissue is within 30 mm of the base of the nipple. The terminal lactiferous ducts drain the milk from TDLUs into 4–18 lactiferous ducts, which drain to the nipple; the milk-glands-to-fat ratio is 2:1 in a lactating woman, and 1:1 in a non-lactating woman. In addition to the milk glands, the breast also is composed of connective tissues (collagen, elastin), white fat, and the suspensory Cooper's ligaments. Sensation in the breast is provided by the peripheral nervous system innervation, by means of the front (anterior) and side (lateral) cutaneous branches of the fourth-, the fifth-, and the sixth intercostal nerves, while the T-4 nerve (Thoracic spinal nerve 4), which innervates the dermatomic area, supplies sensation to the nipple-areola complex.
Lymphatic drainage.
Approximately 75% of the lymph from the breast travels to the axillary lymph nodes on the same side of the body, whilst 25% of the lymph travels to the parasternal nodes (beside the sternum bone). A small amount of remaining lymph travels to the other breast, and to the abdominal lymph nodes. The axillary lymph nodes include the pectoral (chest), subscapular (under the scapula), and humeral (humerus-bone area) lymph-node groups, which drain to the central axillary lymph nodes and to the apical axillary lymph nodes. The lymphatic drainage of the breasts is especially relevant to oncology, because breast cancer is a cancer common to the mammary gland, and cancer cells can metastasize (break away) from a tumour and be dispersed to other parts of the woman's body by means of the lymphatic system.
Shape and support.
The morphologic variations in the size, shape, volume, tissue density, pectoral locale, and spacing of the breasts determine their natural shape, appearance, and configuration upon the chest of a woman; yet such features do not indicate its mammary-gland composition (fat-to-milk-gland ratio), nor the potential for nursing an infant child. The size and the shape of the breasts are influenced by normal-life hormonal changes (thelarche, menstruation, pregnancy, menopause) and medical conditions (e.g. virginal breast hypertrophy). The shape of the breasts is naturally determined by the support of the suspensory Cooper's ligaments, the underlying muscle and bone structures of the chest, and the skin envelope. The suspensory ligaments sustain the breast from the clavicle (collarbone) and the clavico-pectoral fascia (collarbone and chest), by traversing and encompassing the fat and milk-gland tissues, the breast is positioned, affixed to, and supported upon the chest wall, while its shape is established and maintained by the skin envelope.
The base of each breast is attached to the chest by the deep fascia over the pectoralis major muscles. The space between the breast and the pectoralis major muscle is called retromammary space and gives mobility to the breast. Some breasts are mounted high upon the chest wall, are of rounded shape, and project almost horizontally from the chest, which features are common to girls and women in the early stages of thelarchic development, the sprouting of the breasts. In the high-breast configuration, the dome-shaped and the cone-shaped breast is affixed to the chest at the base, and the weight is evenly distributed over the base area. In the low-breast configuration, a proportion of the breast weight is supported by the chest, against which rests the lower surface of the breast, thus is formed the inframammary fold (IMF). Because the base is deeply affixed to the chest, the weight of the breast is distributed over a greater area, and so reduces the weight-bearing strain upon the chest, shoulder, and back muscles that bear the weight of the bust.
The chest (thoracic cavity) progressively slopes outwards from the thoracic inlet (atop the breastbone) and above to the lowest ribs that support the breasts. The inframammary fold, where the lower portion of the breast meets the chest, is an anatomic feature created by the adherence of the breast skin and the underlying connective tissues of the chest; the IMF is the lower-most extent of the anatomic breast. In the course of thelarche, some girls develop breasts the lower skin-envelope of which touches the chest below the IMF, and some girls do not; both breast anatomies are statistically normal morphologic variations of the size and shape of women's breasts.
Development.
The basic morphological structure of the human breast is the same in both male and female children. For a girl in puberty, during thelarche (the breast-development stage), the female sex hormones (principally estrogens) promote the sprouting, growth, and development of the breasts, in the course of which, as mammary glands, they grow in size and volume, and usually rest on her chest; these development stages of secondary sex characteristics (breasts, pubic hair, etc.) are illustrated in the five-stage Tanner Scale. During thelarche, the developing breasts sometimes are of unequal size, and usually the left breast is slightly larger; said condition of asymmetry is transitory and statistically normal to female physical and sexual development. Moreover, breast development sometimes is abnormal, manifested either as overdevelopment (e.g. virginal breast hypertrophy) or as underdevelopment (e.g. tuberous breast deformity) in girls and women; and manifested in boys and men as gynecomastia (woman's breasts), the consequence of a biochemical imbalance between the normal levels of the estrogen and testosterone hormones of the male body.
Asymmetry.
Approximately two years after the onset of puberty (a girl's first menstrual cycle), the hormone estrogen stimulates the development and growth of the glandular, fat, and suspensory tissues that compose the breast. This continues for approximately four years until establishing the final shape of the breast (size, volume, density) when she is a woman of approximately 21 years of age.
About 90% of women's breasts are asymmetrical to some degree, either in size, volume, or relative position upon the chest. Asymmetry can be manifested in the size of the breast, the position of the nipple-areola complex (NAC), the angle of the breast, and the position of the inframammary fold, where the breast meets the chest.
For about 5% to 10% women, their breasts are severely different, with the left breast being larger in 62% of cases. This is due to the breast proximity to the heart and a greater number of arteries and veins, along with a protective layer of fat surrounding the heart located beneath it. Up to 25% of women experience notable breast asymmetry of at least one cup size difference.
If a woman is uncomfortable with her breasts' asymmetry, she can minimize the difference with a corrective bra or use gel bra inserts. Alternatively, she can seek a surgical solution. Options include a minimally invasive procedure known as platelet injection fat transfer, which transfers fat cells from a woman's thighs to her smaller breast. More invasive procedures include corrective mammoplasty, such as mastopexy, breast reduction plasty, or breast augmentation, depending on the nature of the asymmetry. Most surgeons will only perform an augmentation procedure to treat asymmetry if the woman's breasts differ by at least one cup size.
Hormonal change.
The breasts are principally composed of adipose, glandular and connective tissues. Because these tissues have hormone receptors, their sizes and volumes fluctuate according to the hormonal changes particular to thelarche (sprouting of breasts), menstruation (egg production), pregnancy (reproduction), lactation (feeding of offspring), and menopause (end of menstruation).
Estrogens cause elongated growth of mammary duct cells through activation of estrogen receptor alpha (ER-α). Progesterone receptor (PR) activation, believed to be specific to progesterone receptor B (PRB), by progestogens causes growth of milk producing cells or sidebranching. Progestogens also cause stimulation of connective breast tissue or cooper's ligaments. Density, Areolar gland development, and gland lactation development are caused by prolactin activating the prolactin receptor (PR).
For example, during the menstrual cycle, the breasts are enlarged by premenstrual water retention and temporary growth; during pregnancy, the breasts become enlarged and denser (firmer) because of the prolactin-caused organ hypertrophy, which begins the production of breast milk, increases the size of the nipples, and darkens the skin color of the nipple-areola complex; these changes continue during the lactation and the breastfeeding periods. Afterwards, the breasts generally revert to their pre-pregnancy size, shape, and volume, yet might present stretch marks and breast ptosis. 
At menopause, the breasts can decrease in size when the levels of circulating estrogen decline, followed by the withering of the adipose tissue and the milk glands. Additional to such natural biochemical stimuli, the breasts can become enlarged consequent to an adverse side effect of combined oral contraceptive pills; and the size of the breasts can also increase and decrease in response to the body weight fluctuations of the woman. Moreover, the physical changes occurred to the breasts often are recorded in the stretch marks of the skin envelope; they can serve as historical indicators of the increments and the decrements of the size and the volume of a woman's breasts throughout the course of her life.
Breast ptosis.
Ptosis is a normal consequence of aging where the breast tissue sags lower on the chest and the nipple points downward. The rate at which a woman develops ptosis depends on many factors including genetics, smoking, body mass index, number of pregnancies, the size of breasts before pregnancy, and age.
Plastic surgeons categorize ptosis by evaluating the position of the nipple relative to the inframammary crease (where the underside of the breast meets the chest wall). This is determined by measuring from the center of the nipple to the sternal notch (at the top of the breast bone) to gauge how far the nipple has fallen. The standard anthropometric measurement for young women is . This measurement is used to assess both breast ptosis and breast symmetry. The surgeon will assess the breast's angle of projection. The apex of the breast, which includes the nipple, can have a flat angle of projection (180 degrees) or acute angle of projection (greater than 180 degrees). The apex rarely has an angle greater than 60 degrees. The angle of the breast apex is partly determined by the tautness of the suspensory Cooper's ligaments. For example, when a woman lies on her back, the angle of the breast apex becomes a flat, obtuse angle (less than 180 degrees) while the base-to-length ratio of the breast ranges from 0.5 to 1.0.
Functions and health.
Lactation.
The primary function of the breasts – as mammary glands – is the feeding and the nourishing of an infant child with breast milk during the maternal lactation period. The round shape of the breast helps to limit the loss of maternal body heat, because milk production depends upon a higher-temperature environment for the proper, milk-production function of the mammary gland tissues, the lactiferous ducts. Regarding the shape of the breast, the study "The Evolution of the Human Beast" (2001) proposed that the rounded shape of a woman's breast evolved to prevent the sucking infant offspring from suffocating while feeding at the teat; that is, because of the human infant's small jaw, which did not project from the face to reach the nipple, he or she might block the nostrils against the mother's breast if it were of a flatter form (cf. chimpanzee); theoretically, as the human jaw receded into the face, the woman's body compensated with round breasts.
In a woman, the condition of lactation unrelated to pregnancy can occur as galactorrhea (spontaneous milk flow), and because of the adverse effects of drugs (e.g. antipsychotic medications), of extreme physical stress, and of endocrine disorders. In a newborn infant, the capability of lactation is consequence of the mother's circulating hormones (prolactin, oxytocin, etc.) in his or her blood stream, which were introduced by the shared circulatory system of the placenta. In men, the mammary glands are also present in the body, but normally remain undeveloped because of the hormone testosterone, however, when male lactation occurs, it is considered a pathological symptom of a disorder of the pituitary gland.
Reproduction.
In considering the human animal, zoologists proposed that the human female is the only primate that possesses permanent, full-form breasts when not pregnant. Other mammalian females develop full breasts only when pregnant.
Clinical significance.
The breast is susceptible to numerous benign and malignant conditions. The most frequent benign conditions are puerperal mastitis, fibrocystic breast changes and mastalgia. Breast cancer is one of the leading causes of death among women.
Society and culture.
Anthropomorphic geography.
There are many mountains named after the breast because they resemble it in appearance and so are objects of religious and ancestral veneration as a fertility symbol and of well-being. In Asia, there was "Breast Mountain," which had a cave where the Buddhist monk Bodhidharma (Da Mo) spent much time in meditation. Other such breast mountains are Mount Elgon on the Uganda-Kenya border, Beinn Chìochan and the Maiden Paps in Scotland, the "Bundok ng Susong Dalaga" (Maiden's breast mountains) in Talim Island, Philippines, the twin hills known as the Paps of Anu ("Dá Chích Anann" or the breasts of Anu), near Killarney in Ireland, the 2,086 m high "Tetica de Bacares" or "La Tetica" in the Sierra de Los Filabres, Spain, and Khao Nom Sao in Thailand, Cerro Las Tetas in Puerto Rico and the Breasts of Aphrodite in Mykonos, among many others. In the United States, the Teton Range is named after the French word for "breast."
Art history.
In European pre-historic societies, sculptures of female figures with pronounced or highly exaggerated breasts were common. A typical example is the so-called Venus of Willendorf, one of many Paleolithic Venus figurines with ample hips and bosom. Artifacts such as bowls, rock carvings and sacred statues with breasts have been recorded from 15,000 BC up to late antiquity all across Europe, North Africa and the Middle East. Many female deities representing love and fertility were associated with breasts and breast milk. Figures of the Phoenician goddess Astarte were represented as pillars studded with breasts. Isis, an Egyptian goddess who represented, among many other things, ideal motherhood, was often portrayed as suckling pharaohs, thereby confirming their divine status as rulers. Even certain male deities representing regeneration and fertility were occasionally depicted with breast-like appendices, such as the river god Hapy who was considered to be responsible for the annual overflowing of the Nile. Female breasts were also prominent in the Minoan civilization in the form of the famous Snake Goddess statuettes. In Ancient Greece there were several cults worshipping the "Kourotrophos", the suckling mother, represented by goddesses such as Gaia, Hera and Artemis. The worship of deities symbolized by the female breast in Greece became less common during the first millennium. The popular adoration of female goddesses decreased significantly during the rise of the Greek city states, a legacy which was passed on to the later Roman Empire.
During the middle of the first millennium BC, Greek culture experienced a gradual change in the perception of female breasts. Women in art were covered in clothing from the neck down, including female goddesses like Athena, the patron of Athens who represented heroic endeavor. There were exceptions: Aphrodite, the goddess of love, was more frequently portrayed fully nude, though in postures that were intended to portray shyness or modesty, a portrayal that has been compared to modern pin ups by historian Marilyn Yalom. Although nude men were depicted standing upright, most depictions of female nudity in Greek art occurred "usually with drapery near at hand and with a forward-bending, self-protecting posture". A popular legend at the time was of the Amazons, a tribe of fierce female warriors who socialized with men only for procreation and even removed one breast to become better warriors (the idea being that the right breast would interfere with the operation of a bow and arrow). The legend was a popular motif in art during Greek and Roman antiquity and served as an antithetical cautionary tale.
Body image.
Many women regard their breasts, which are female secondary sex characteristics, as important to their sexual attractiveness, as a sign of femininity that is important to their sense of self. Due to this, when a woman considers her breasts deficient in some respect, she might choose to undergo a plastic surgery procedure to enhance them, either to have them augmented or to have them reduced, or to have them reconstructed if she suffered a deformative disease, such as breast cancer. After mastectomy, the reconstruction of the breast or breasts is done with breast implants or autologous tissue transfer, using fat and tissues from the abdomen, which is performed with a TRAM flap or with a back (latissiumus muscle flap). Breast reduction surgery is a procedure that involves removing excess breast tissue, fat, and skin, and the repositioning of the nipple-areola complex.
Cosmetic improvement procedures include breast lift (mastopexy), breast augmentation with implants, and combination procedures; the two types of available breast implants are models filled with silicone gel, and models filled with saline solution. These types of breast surgery can also repair inverted nipples by releasing milk duct tissues that have become tethered. Furthermore, in the case of the obese woman, a breast lift (mastopexy) procedure, with or without a breast volume reduction, can be part of an upper-body lift and contouring for the woman who has undergone massive body weight loss.
Surgery of the breast presents the health risk of interfering with the ability to breast-feed an infant child, and might include consequences such as altered sensation in the nipple-areola complex, interference with mammography (breast x-rays images) when there are breast implants present in the breasts. Regarding breast-feeding capability after breast reduction surgery, studies reported that women who underwent breast reduction can retain the ability to nurse an infant child, when compared to women in a control group who underwent breast surgery using a modern pedicle surgical technique. Plastic surgery organizations generally discourage elective cosmetic breast augmentation surgery for teen-aged girls, because, at that age, the volume of the breast tissues (milk glands and fat) can continue to grow as the girl matures to womanhood. Breast reduction surgery for teen-aged girls, however, is a matter handled according to the particulars of the case of hypoplasia. (see: breast hypertrophy)
Clothing.
Because breasts are mostly fatty tissue, their shape can within limits be molded by clothing, such as foundation garments. Bras are commonly worn by about 90% of Western women, and are often worn for support. The social norm in most Western cultures is to cover breasts in public, though the extent of coverage varies depending on the social context. Some religions ascribe a special status to the female breast, either in formal teachings or through symbolism. Islam forbids women from exposing their breasts in public.
Many cultures associate breasts with sexuality and tend to regard bare breasts as immodest or indecent. In some cultures, like the Himba in northern Namibia, bare-breasted women are normal, while a thigh is highly sexualised and not exposed in public. In a few Western countries female toplessness at a beach is acceptable, although it may not be acceptable in the town center. In some areas, exposing a woman's breasts applies only to the exposure of nipples.
In the United States, women who breast-feed in public can receive negative attention. There have been instances where women have been asked to leave public venues. In New York, the topfreedom equality movement helped to bring a case, "People v. Santorelli" (1992), to the New York State Court of Appeals. They ruled that New York's indecent exposure laws did not apply to a bare-breasted woman. Other (gender equality) efforts succeeded in most of Canada in the 1990s. Bare-breasted women are legal and culturally acceptable at public beaches in Australia and much of Europe.
Sexual characteristic.
In some cultures, breasts play a role in human sexual activity. Breasts and especially the nipples are among the various human erogenous zones. They are sensitive to the touch as they have many nerve endings; and it is common to press or massage them with hands or orally before or during sexual activity. Some women can achieve an orgasm from such activities. Research has suggested that the sensations are genital orgasms caused by nipple stimulation, and may also be directly linked to "the genital area of the brain". Sensation from the nipples travels to the same part of the brain as sensations from the vagina, clitoris and cervix. Nipple stimulation may trigger uterine contractions, which then produce a sensation in the genital area of the brain. In the ancient Indian work the "Kama Sutra", light scratching of the breasts with nails and biting with teeth are considered erotic. During sexual arousal, breast size increases, venous patterns across the breasts become more visible, and nipples harden. Compared to other primates, human breasts are proportionately large throughout adult females' lives. Some writers have suggested that they may have evolved as a visual signal of sexual maturity and fertility.
Many people regard the female human body, of which breasts are an important aspect, to be aesthetically pleasing, as well as erotic. Research conducted at the Victoria University of Wellington showed that breasts are often the first thing men look at, and for a longer time than other body parts. The writers of the study had initially speculated that the reason for this is due to endocrinology with larger breasts indicating higher levels of estrogen and a sign of greater fertility, but the researchers said that "Men may be looking more often at the breasts because they are simply aesthetically pleasing, regardless of the size."
Many people regard bare female breasts to be erotic, and they can elicit heightened sexual desires in men in many cultures. Some people show a sexual interest in female breasts distinct from that of the person, which may be regarded as a breast fetish. While U.S. culture prefers breasts that are youthful and upright, some cultures venerate women with drooping breasts, indicating mothering and the wisdom of experience.
Symbolism.
In Christian iconography, some works of art depict women with their breasts in their hands or on a platter, signifying that they died as a martyr by having their breasts severed; one example of this is Saint Agatha of Sicily.

</doc>
<doc id="4492" url="http://en.wikipedia.org/wiki?curid=4492" title="Baghdad">
Baghdad

Baghdad ( "", ) is the capital of the Republic of Iraq, as well as the coterminous Baghdad Province. The population of Baghdad, as of 2011, is approximately 7,216,040, making it the largest city in Iraq, the second largest city in the Arab world (after Cairo, Egypt), and the second largest city in Western Asia (after Tehran, Iran). According to the government, which is preparing for a census, the population of the country has reached 35 million, with 9 million in the capital.
Located along the Tigris River, the city was founded in the 8th century and became the capital of the Abbasid Caliphate. Within a short time of its inception, Baghdad evolved into a significant cultural, commercial, and intellectual center for the Islamic world. This, in addition to housing several key academic institutions (e.g. House of Wisdom), garnered the city a worldwide reputation as the "Center of Learning". Throughout the High Middle Ages, Baghdad was considered to be the largest city in the world with an estimated population of 1,200,000 people. According to some archeologists it was the first city to reach a population over one million inhabitants. The city was largely destroyed at the hands of the Mongol Empire in 1258, resulting in a decline that would linger through many centuries due to frequent plagues and multiple successive empires. With the recognition of Iraq as an independent state (formerly the British Mandate of Mesopotamia) in 1938, Baghdad gradually regained some of its former prominence as a significant center of Arab culture.
In contemporary times, the city has often faced severe infrastructural damage, most recently due to the 2003 invasion of Iraq, and the subsequent Iraq War that lasted until December 2011. In recent years, the city has been frequently subjected to insurgency attacks. Though the nation continues to work toward rebuilding and reconciliation, as of 2012, Baghdad continues to be listed as one of the least hospitable places in the world to live, and was ranked by Mercer as the worst of 221 major cities as measured by quality-of-life.
City name.
The name Baghdad is pre-Islamic and its origins are under some dispute. The site where the city of Baghdad came to stand has been populated for millennia and by the 8th century AD several Aramaic Christian villages had developed there, one of which was called "Baghdad", the name which would come to be used for the Abbasid metropolis.
The name has been used as "Baghdadu" on Assyrian cuneiform and Babylonian records going back to at least 2000 BC. An inscription by Nebuchadnezzar (600 BC) describes how he rebuilt the old Babylonian town of "Bagh-dadu". There used to be another Babylonian settlement called Baghdad, in upper Mesopotamia, near the ancient city of Edessa. The name has not been attested outside of Mesopotamia.
Even though the name has been attested in pre-Persian times, a Persian origin has been accepted by most scholars. It has been proposed that the name is a Middle Persian compound of "Bag" "god" and "dād" "given", translating to "God-given" or "God's gift", from which comes Modern Persian '. This in turn can be traced to Old Persian. Another proposal is the Persian compound "bāğ" "garden" and "dād" "fair", translating to "The fair garden". However, a Persian explanation remains somewhat problematic, given that the name was used long before the Persians arrived in Mesopotamia.
When the Abbasid caliph, al-Mansur, founded a completely new city for his capital, he chose the name Madinat al-Salaam or "City of Peace". This was the official name on coins, weights, and other official usage, although the common people continued to use the old name. By the 11th century, "Baghdad" became almost the exclusive name for the world-renowned metropolis.
History.
Foundation.
After the fall of the Umayyads, the first Muslim dynasty, the victorious Abbasid rulers wanted their own capital. Choosing a site north of the Sassanid capital of Ctesiphon (and just north of where ancient Babylon and Seleucia once stood), on 30 July 762, the caliph Al-Mansur commissioned the construction of the city and it was built under the supervision of the Barmakids. Mansur believed that Baghdad was the perfect city to be the capital of the Islamic empire under the Abbasids. Mansur loved the site so much he is quoted saying, "This is indeed the city that I am to found, where I am to live, and where my descendants will reign afterward".
The city's growth was helped by its location, which gave it control over strategic and trading routes, along the Tigris. A reason why Baghdad provided an excellent location was the abundance of water and the dry climate. Water exists on both north and south ends of the city gates, allowing all households to have a plentiful supply, which was very uncommon during this time.
Baghdad eclipsed Ctesiphon, the capital of the Persian Empire, which was located some to the southeast. Today, all that remains of Ctesiphon is the shrine town of Salman Pak, just to the south of Greater Baghdad. Ctesiphon itself had replaced and absorbed Seleucia, the first capital of the Seleucid Empire. Seleucia had earlier replaced the city of Babylon.
In its early years, the city was known as a deliberate reminder of an expression in the Qur'an, when it refers to Paradise. It took four years to build (764-768). Mansur assembled engineers, surveyors, and art constructionists from around the world to come together and draw up plans for the city. Over 100,000 construction workers came to survey the plans; many were distributed salaries to start the building of the city. July was chosen as the starting time because two Astrologers, Naubakht Ahvazi and Mashallah, believed that the city should be built under the sign of the lion, Leo. Leo is associated with fire and symbolises productivity, pride, and expansion.
The bricks used to make the city were on all four sides. Abū Ḥanīfa was the counter of the bricks and he developed a canal, which brought water to the work site for the use of both human consumption and the manufacturing of the bricks. Marble was also used to make buildings throughout the city, and marble steps led down to the river's edge.
The basic framework of the city consists of two large semicircles about in diameter. The city was designed as a circle about in diameter, leading it to be known as the "Round City". The original design shows as single ring of residential and commercial structures along the inside of the city walls, but the final construction added another ring inside the first. Within the city there were many parks, gardens, villas, and promenades. In the center of the city lay the mosque, as well as headquarters for guards. The purpose or use of the remaining space in the center is unknown. The circular design of the city was a direct reflection of the traditional Persian Sasanian urban design. The Sasanian city of Gur in Fars, built 500 years before Baghdad, is nearly identical in its general circular design, radiating avenues, and the government buildings and temples at the centre of the city. This style of urban planning contrasted with Ancient Greek and Roman urban planning, in which cities are designed as squares or rectangles with streets intersecting each other at right angles.
Surrounding walls.
The four surrounding walls of Baghdad were named Kufa, Basra, Khurasan, and Damascus; named because their gates pointed in the directions of these destinations. The distance between these gates was a little less than . Each gate had double doors that were made of iron; the doors were so heavy it took several men to open and close them. The wall itself was about 44 m thick at the base and about 12 m thick at the top. Also, the wall was 30 m high, which included merlons, a solid part of an embattled parapet usually pierced by embrasures. This wall was surrounded by another wall with a thickness of 50 m. The second wall had towers and rounded merlons, which surrounded the towers. This outer wall was protected by solid glacis, which is made out of bricks and quicklime. Beyond the outer wall was a water-filled moat.
Golden Gate Palace.
In the middle of Baghdad, in the central square was the Golden Gate Palace. The Palace was the residence of the caliph and his family. In the central part of the building was a green dome that was 39 m high. Surrounding the palace was an esplanade, a waterside building, in which only the caliph could come riding on horseback. In addition, the palace was near other mansions and officer's residences. Near the Gate of Syria a building served as the home for the guards. It was made of brick and marble. The palace governor lived in the latter part of the building and the commander of the guards in the front. In 813, after the death of caliph Al-Amin the palace was no longer used as the home for the caliph and his family.
The roundness points to the fact that it was based on Arabic script. The two designers who were hired by Al-Mansur to plan the city's design were Naubakht, a Zoroastrian who also determined that the date of the foundation of the city would be astrologically auspicious, and Mashallah, a Jew from Khorasan, Iran.
Abbasids and the round city.
The Abbasid Caliphate was based on their being the descendants of the uncle of Muhammad and being part of the Quraysh tribe. They used Shi'a resentment, Khorasanian movement, and appeals to the ambitions and traditions of the newly conquered Persian aristocracy to overthrow the Umayyads.
The Abbasids sought to combine the hegemony of the Arab tribes with the imperial, court, ceremonial, and administrative structures of the Persians. The Abbasids considered themselves the inherittures and the need of Mansur to place the capital in a place that was representative of Arab-Islamic identity by building the House of Wisdom, where ancient texts were translated from their original language, such as Greek, to Arabic. Mansur is credited with the "Translation Movement" for this. Further, Baghdad is also near the ancient Sassanid imperial seat of Ctesiphon on the Tigris River.
Centre of learning (8th to 13th centuries).
Within a generation of its founding, Baghdad became a hub of learning and commerce. The House of Wisdom was an establishment dedicated to the translation of Greek, Middle Persian and Syriac works. Scholars headed to Baghdad from all over the Abbasid Caliphate, facilitating the introduction of Persian, Greek and Indian science into the Arabic and Islamic world at that time. Baghdad was likely the largest city in the world from shortly after its foundation until the 930s, when it was tied by Córdoba.
Several estimates suggest that the city contained over a million inhabitants at its peak. Many of the "One Thousand and One Nights" tales are set in Baghdad during this period.
End of the Abbasids in Baghdad.
By the 10th century, the city's population was between 1.2 million and 2 million. Baghdad's early meteoric growth eventually slowed due to troubles within the Caliphate, including relocations of the capital to Samarra (during 808–819 and 836–892), the loss of the western and easternmost provinces, and periods of political domination by the Iranian Buwayhids (945–1055) and Seljuk Turks (1055–1135).
The Seljuks were a clan of the Oghuz Turks from the Central Asia that converted to the Sunni branch of Islam. In 1040, they destroyed the Ghaznavids, taking over their land and in 1055, Tughril Beg, the leader of the Seljuks, took over Baghdad. The Seljuks expelled the Buyid dynasty of Shiites that ruled for some time and took over power and control of Baghdad. They ruled as Sultans in the name of the Abbasid caliphs (they saw themselves as being part of the Abbasid regime). Tughril Beg saw himself as the protector of the Abbasid Caliphs.
In 1058, Baghdad was captured by the Fatimids under the Turkish general Abu'l-Ḥārith Arslān al-Basasiri, an adherent of the Ismailis along with the 'Uqaylid Quraysh. Not long before the arrival of the Saljuqs in Baghdad, al-Basasiri petitioned to the Fatimid Imam-Caliph al-Mustansir to support him in conquering Baghdad on the Ismaili Imam's behalf. It has recently come to light that the famed Fatimid "da'i", al-Mu'ayyad al-Shirazi, had a direct role in supporting al-Basasiri and helped the general to succeed in taking Mawṣil, Wāsit and Kufa. Soon after, by December 1058, a Shi'i "adhān" (call to prayer) was implemented in Baghdad and a "khutbah" (sermon) was delivered in the name of the Fatimid Imam-Caliph. Despite his Shi'i inclinations, Al-Basasiri received support from Sunnis and Shi'is alike, for whom opposition to the Saljuq power was a common factor.
On 10 February 1258, Baghdad was captured by the Mongols led by Hulegu, a grandson of Chingiz Khan (Genghis Khan), during the siege of Baghdad. Many quarters were ruined by fire, siege, or looting. The Mongols massacred most of the city's inhabitants, including the caliph Al-Musta'sim, and destroyed large sections of the city. The canals and dykes forming the city's irrigation system were also destroyed. The sack of Baghdad put an end to the Abbasid Caliphate, a blow from which the Islamic civilization never fully recovered.
At this point, Baghdad was ruled by the Il-Khanids, the Mongol rulers of Iran. In 1401, Baghdad was again sacked, by the Central Asian Turkic conqueror Timur ("Tamerlane"). When his forces took Baghdad, he spared almost no one, and ordered that each of his soldiers bring back two severed human heads. It became a provincial capital controlled by the Mongol Jalayirid (1400–1411), Turkic Kara Koyunlu (1411–1469), Turkic Ak Koyunlu (1469–1508), and the Iranian Safavid (1508–1534) dynasties.
Ottoman era (16th to 19th centuries).
In 1534, Baghdad was captured by the Ottoman Turks. Under the Ottomans, Baghdad fell into a period of decline, partially as a result of the enmity between its rulers and Iranian Safavids, which did not accept the Sunni control of the city. Between 1623 and 1638, it returned to Iranian rule before falling back into Ottoman hands.
Baghdad has suffered severely from visitations of the plague and cholera, and sometimes two-thirds of its population has been wiped out.
For a time, Baghdad had been the largest city in the Middle East. The city saw relative revival in the latter part of the 18th century under a Mamluk government. Direct Ottoman rule was reimposed by Ali Rıza Pasha in 1831. From 1851 to 1852 and from 1861 to 1867, Baghdad was governed, under the Ottoman Empire by Mehmed Namık Pasha. The Nuttall Encyclopedia reports the 1907 population of Baghdad as 185,000. Baghdad was also home to a substantial Jewish community, which comprised over a quarter of the city's population.
20th and 21st centuries.
Baghdad and southern Iraq remained under Ottoman rule until 1917, when captured by the British during World War I. From 1920, Baghdad became the capital of the British Mandate of Mesopotamia and, after 1932, Baghdad was the capital of the Kingdom of Iraq. Iraq was given formal independence in 1932 and increased autonomy in 1946. The city's population grew from an estimated 145,000 in 1900 to 580,000 in 1950.
On 1 April 1941, members of the "Golden Square" and Rashid Ali staged a coup in Baghdad. Rashid Ali installed a pro-German and pro-Italian government to replace the pro-British government of Regent Abdul Ilah. On 31 May, after the resulting Anglo-Iraqi War and after Rashid Ali and his government had fled, the Mayor of Baghdad surrendered to British and Commonwealth forces.
On 14 July 1958, members of the Iraqi Army, under Abd al-Karim Qasim, staged a coup to topple the Kingdom of Iraq. King Faisal II, former Prime Minister Nuri as-Said, former Regent Prince 'Abd al-Ilah, members of the royal family, and others were brutally killed during the coup. Many of the victim's bodies were then dragged through the streets of Baghdad.
During the 1970s, Baghdad experienced a period of prosperity and growth because of a sharp increase in the price of petroleum, Iraq's main export. New infrastructure including modern sewerage, water, and highway facilities were built during this period. The masterplans of the city (1967, 1973) were delivered by the Polish planning office Miastoprojekt-Kraków, mediated by Polservice. However, the Iran–Iraq War of the 1980s was a difficult time for the city, as money was diverted by Saddam Hussein to the army and thousands of residents were killed. Iran launched a number of missile attacks against Baghdad in retaliation for Saddam Hussein's continuous bombardments of Tehran's residential districts.
In 1991 and 2003, the Gulf War and the 2003 invasion of Iraq caused significant damage to Baghdad's transportation, power, and sanitary infrastructure as the US-led coalition forces launched massive aerial assaults in the city in the two wars.
Main sights.
Points of interest include the National Museum of Iraq whose priceless collection of artifacts was looted during the 2003 invasion, and the iconic Hands of Victory arches. Multiple Iraqi parties are in discussions as to whether the arches should remain as historical monuments or be dismantled. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command.
Baghdad Zoo.
The Baghdad Zoo was the largest zoo in the Middle East. Within eight days following the 2003 invasion, however, only 35 of the 650 animals in the facility survived. This was a result of theft of some animals for human food, and starvation of caged animals that had no food. South African Lawrence Anthony and some of the zoo keepers cared for the animals and fed the carnivores with donkeys they had bought locally. Eventually, L. Paul Bremer, Director of the Coalition Provisional Authority in Iraq from May 11, 2003 to June 28, 2004 ordered protection of the zoo and U.S. engineers helped to reopen the facility.
Kadhimiya mosque.
The Al-Kādhimiya Mosque is a shrine that is located in the Kādhimayn suburb of Baghdad, Iraq. It contains the tombs of the seventh Twelver Shīa Imām Musa al-Kadhim and the ninth Twelver Shīa Imām Muhammad at-Taqī al-Jawād. Many Shias travel to the mosque from far away places to commemorate.
Geography.
The city is located on a vast plain bisected by the River Tigris. The Tigris splits Baghdad in half, with the eastern half being called 'Risafa' and the Western half known as 'Karkh'. The land on which the city is built is almost entirely flat and low-lying, being of alluvial origin due to the periodic large floods which have occurred on the river.
Climate.
Baghdad has a subtropical arid climate (Köppen climate classification "BWh") and is, in terms of maximum temperatures, one of the hottest cities in the world. In the summer from June to August, the average maximum temperature is as high as accompanied by blazing sunshine: rainfall has in fact been recorded on fewer than half a dozen occasions at this time of year and has never exceeded . Temperatures exceeding in the shade are by no means unheard of, and even at night temperatures in summer are seldom below . Because the humidity is very low (usually under 10%) due to Baghdad's distance from the marshy Persian Gulf, dust storms from the deserts to the west are a normal occurrence during the summer.
Winters boast mild days and variable nights. From December to February, Baghdad has maximum temperatures averaging , though highs above are not unheard of. Morning temperatures can be chilly: the average January low is but lows below freezing only occur a couple of times per year.
Annual rainfall, almost entirely confined to the period from November to March, averages around , but has been as high as and as low as . On January 11, 2008, light snow fell across Baghdad for the first time in memory.
Administrative divisions.
The city of Baghdad has 89 official neighbourhoods within 9 districts. These official subdivisions of the city served as administrative centres for the delivery of municipal services but until 2003 had no political function. Beginning in April 2003, the U.S. controlled Coalition Provisional Authority (CPA) began the process of creating new functions for these. The process initially focused on the election of neighbourhood councils in the official neighbourhoods, elected by neighbourhood caucuses.
The CPA convened a series of meetings in each neighbourhood to explain local government, to describe the caucus election process and to encourage participants to spread the word and bring friends, relatives and neighbours to subsequent meetings. Each neighbourhood process ultimately ended with a final meeting where candidates for the new neighbourhood councils identified themselves and asked their neighbours to vote for them.
Once all 88 (later increased to 89) neighbourhood councils were in place, each neighbourhood council elected representatives from among their members to serve on one of the city's nine district councils. The number of neighbourhood representatives on a district council is based upon the neighbourhood's population. The next step was to have each of the nine district councils elect representatives from their membership to serve on the 37 member Baghdad City Council. This three tier system of local government connected the people of Baghdad to the central government through their representatives from the neighbourhood, through the district, and up to the city council.
The same process was used to provide representative councils for the other communities in Baghdad Province outside of the city itself. There, local councils were elected from 20 neighbourhoods (Nahia) and these councils elected representatives from their members to serve on six district councils (Qada). As within the city, the district councils then elected representatives from among their members to serve on the 35 member Baghdad Regional Council.
The first step in the establishment of the system of local government for Baghdad Province was the election of the Baghdad Provincial Council. As before, the representatives to the Provincial Council were elected by their peers from the lower councils in numbers proportional to the population of the districts they represent. The 41 member Provincial Council took office in February, 2004 and served until national elections held in January 2005, when a new Provincial Council was elected.
This system of 127 separate councils may seem overly cumbersome; however, Baghdad Province is home to approximately seven million people. At the lowest level, the neighbourhood councils, each council represents an average of 75,000 people.
The nine District Advisory Councils (DAC) are as follows:
The nine districts are subdivided into 89 smaller neighborhoods which may make up sectors of any of the districts above. The following is a "selection" (rather than a complete list) of these neighborhoods:
Economy.
Iraqi Airways, the national airline of Iraq, has its headquarters on the grounds of Baghdad International Airport in Baghdad. Al-Naser Airlines has its head office in Karrada, Baghdad.
Reconstruction efforts.
Most Iraqi reconstruction efforts have been devoted to the restoration and repair of badly damaged urban infrastructure. More visible efforts at reconstruction through private development, like architect and urban designer Hisham N. Ashkouri's Baghdad Renaissance Plan and the Sindbad Hotel Complex and Conference Center have also been made.
 A plan was proposed by a Government agency to rebuild a tourist island in 2008. In late 2009, a construction plan was proposed to rebuild the heart of Baghdad, but the plan was never realized because corruption was involved in it.
The Baghdad Eye, a tall Ferris wheel, was proposed for Baghdad in August 2008. At that time, three possible locations had been identified, but no estimates of cost or completion date were given.<ref name=msnbc.msn.com/id/26425911></ref> In October 2008, it was reported that Al-Zawraa Park was expected to be the site, and a wheel was installed there in March 2011.
Iraq's Tourism Board is also seeking investors to develop a "romantic" island on the River Tigris in Baghdad that was once a popular honeymoon spot for newlywed Iraqis. The project would include a six-star hotel, spa, an 18-hole golf course and a country club. In addition, the go-ahead has been given to build numerous architecturally unique skyscrapers along the Tigris that would develop the city's financial centre in Kadhehemiah.
In October 2008, the Baghdad Metro resumed service. It connects the center to the southern neighborhood of Dora.
In May 2010, a new residential and commercial project nicknamed Baghdad Gate was announced. This project not only addresses the urgent need for new residential units in Baghdad but also acts as a real symbol of progress in the war torn city, as Baghdad has not seen projects of this scale for decades.
Housing.
In 2012, the Central Bank of Iraq signed a deal with Zaha Hadid Architects to build a tower which will be used as the bank's new headquarters.
Education.
The Mustansiriya Madrasah was established in 1227 by the Abbasid Caliph al-Mustansir. The name was changed to Al-Mustansiriya University in 1963. The University of Baghdad is the largest university in Iraq and the second largest in the Arab world.
Culture.
Baghdad has always played a significant role in the broader Arab cultural sphere, contributing several significant writers, musicians and visual artists. Famous Arab poets and singers such as Nizar Qabbani, Umm Kulthum, Fairuz, Salah Al-Hamdani, Ilham al-Madfai and others have performed for the city.
The dialect of Arabic spoken in Baghdad today differs from that of other large urban centres in Iraq, having features more characteristic of nomadic Arabic dialects (Verseegh, "The Arabic Language"). It is possible that this was caused by the repopulating of the city with rural residents after the multiple sacks of the late Middle Ages.
Institutions.
Some of the important cultural institutions in the city include:
The live theatre scene received a boost during the 1990s, when UN sanctions limited the import of foreign films. As many as 30 movie theatres were reported to have been converted to live stages, producing a wide range of comedies and dramatic productions.
Institutions offering cultural education in Baghdad include The Music and Ballet School of Baghdad and the Institute of Fine Arts Baghdad. Baghdad is also home to a number of museums which housed artifacts and relics of ancient civilization; many of these were stolen, and the museums looted, during the widespread chaos immediately after United States forces entered the city.
During the 2003 occupation of Iraq, AFN Iraq ("Freedom Radio") broadcast news and entertainment within Baghdad, among other locations. There is also a private radio station called "Dijlah" (named after the Arabic word for the Tigris River) that was created in 2004 as Iraq's first independent talk radio station. Radio Dijlah offices, in the Jamia neighborhood of Baghdad, have been attacked on several occasions.
Sport.
Baghdad is home to some of the most successful football (soccer) teams in Iraq, the biggest being current Iraqi League champions Al-Shorta (Police), Al-Quwa Al-Jawiya (Airforce club), Al-Zawra'a, and Talaba (Students). The largest stadium in Baghdad is Al-Shaab Stadium, which was opened in 1966. Another, but much larger stadium, is still in the opening stages of construction.
The city has also had a strong tradition of horse racing ever since World War I, known to Baghdadis simply as 'Races'. There are reports of pressures by the Islamists to stop this tradition due to the associated gambling.
Further reading.
Books:

</doc>
<doc id="4493" url="http://en.wikipedia.org/wiki?curid=4493" title="Outline of biology">
Outline of biology

The following outline is provided as an overview of and topical guide to biology:
Biology – study of living organisms. It is concerned with the characteristics, classification, and behaviors of organisms, how species come into existence, and the interactions they have with each other and with the environment. Biology encompasses a broad spectrum of academic fields that are often viewed as independent disciplines. However, together they address phenomena related to living organisms (biological phenomena) over a wide range of scales, from biophysics to ecology. All concepts in biology are subject to the same laws that other branches of science obey, such as the laws of thermodynamics and conservation of energy.

</doc>
<doc id="4495" url="http://en.wikipedia.org/wiki?curid=4495" title="British thermal unit">
British thermal unit

The British thermal unit (BTU or Btu) is a traditional unit of energy equal to about 1055 joules. It is the amount of energy needed to cool or heat one pound of water by one degree Fahrenheit. In science, the joule, the SI unit of energy, has largely replaced the BTU.
The BTU is most often used as a measure of power (as BTU/h) in the power, steam generation, heating, and air conditioning industries, and also as a measure of agricultural energy production (BTU/kg). It is still used in metric English-speaking countries (such as Canada), and remains the standard unit of classification for air conditioning units manufactured and sold in many non-English-speaking metric countries. In North America, the heat value (energy content) of fuels is expressed in BTUs.
Definitions.
A BTU is the amount of heat required to raise the temperature of of liquid water by at a constant pressure of one atmosphere. As with the calorie, several definitions of the BTU exist, because the temperature response of water to heat energy is non-linear. This means that the change in temperature of a water mass caused by adding a certain amount of heat to it will be a function of the water's initial temperature. Definitions of the BTU based on different water temperatures can therefore vary by up to 0.5%.
A BTU can be approximated as the heat produced by burning a single wooden match or as the amount of energy it takes to lift a one-pound weight .
The unit MBtu or mBtu was defined as one thousand BTU, presumably from the Roman numeral system where "M" or "m" stands for one thousand (1,000). This notation is easily confused with the SI mega- (M) prefix, which denotes multiplication by a factor of one million (×106), or with the SI milli- (m) prefix, which denotes division by a factor of one thousand (×10−3). To avoid confusion, many companies and engineers use the notation MMBtu or mmBtu to represent one million BTU (although, confusingly, MM in Roman numerals would traditionally represent 2,000) and in many contexts this form of notation is deprecated and discouraged in favour of the more modern SI prefixes. Alternatively, the term therm may be used to represent 100,000 (or 105) BTU, and quad for 1015 BTU. Some companies also use BtuE6 in order to reduce confusion between 103 BTU and 106 BTU.
Conversions.
One BTU is approximately:
As a unit of power.
When used as a unit of power for heating and cooling systems, BTU "per hour" (BTU/h) is the correct unit, though this is often abbreviated to just "BTU"..
Associated units.
The BTU should not be confused with the Board of Trade Unit (B.O.T.U.), which is a much larger quantity of energy ().
The BTU is often used to express the conversion-efficiency of heat into electrical energy in power plants. Figures are quoted in terms of the quantity of heat in BTU required to generate 1 kW·h of electrical energy. A typical coal-fired power plant works at 10,500 BTU/kW·h, an efficiency of 32–33%.

</doc>
<doc id="4497" url="http://en.wikipedia.org/wiki?curid=4497" title="Bugatti">
Bugatti

Automobiles Ettore Bugatti was a French car manufacturer of high-performance automobiles, founded in 1909 in the then German city of Molsheim, Alsace by Italian-born Ettore Bugatti. Bugatti cars were known for their design beauty (Ettore Bugatti was from a family of artists and considered himself to be both an artist and constructor) and for their many race victories. Famous Bugattis include the Type 35 Grand Prix cars, the Type 41 "Royale", the Type 57 "Atlantic" and the Type 55 sports car.
The death of Ettore Bugatti in 1947 proved to be the end for the marque, and the death of his son Jean Bugatti in 1939 ensured there was not a successor to lead the factory. No more than about 8000 cars were made. The company struggled financially, and released one last model in the 1950s, before eventually being purchased for its airplane parts business in the 1960s. In the 1990s, an Italian entrepreneur revived it as a builder of limited production exclusive sports cars. Today, the name is owned by German automobile manufacturing group Volkswagen.
Under Ettore Bugatti.
Founder Ettore Bugatti was born in Milan, Italy, and the automobile company that bears his name was founded in 1909 in Molsheim located in the Alsace region which was part of the German Empire from 1871 to 1919. The company was known both for the level of detail of its engineering in its automobiles, and for the artistic way in which the designs were executed, given the artistic nature of Ettore's family (his father, Carlo Bugatti (1856–1940), was an important Art Nouveau furniture and jewelry designer).
World War I and its aftermath.
During the war Ettore Bugatti was sent away, initially to Milan and later to Paris, but as soon as hostilities had been concluded he returned to his factory at Molsheim. Less than four months after the Versailles Treaty formalised the transfer of Alsace from Germany to France, Bugatti was able to obtain, at the last minute, a stand at the 15th Paris motor show in October 1919. He exhibited three light cars, all of them closely based on their pre-war equivalents, and each fitted with the same overhead camshaft 4-cylinder 1,368cc engine with four valves per cylinder. Smallest of the three was a "Type 13" with a racing body (constructed by Bugatti themselves) and using a chassis with a wheelbase. The others were a "Type 22" and a "Type 23" with wheelbases of respectively.
Racing successes.
The company also enjoyed great success in early Grand Prix motor racing: in 1929 a privately entered Bugatti won the first ever Monaco Grand Prix. Racing success culminated with driver Jean-Pierre Wimille winning the 24 hours of Le Mans twice (in 1937 with Robert Benoist and 1939 with Pierre Veyron).
Bugatti cars were extremely successful in racing. The little Bugatti Type 10 swept the top four positions at its first race. The 1924 Bugatti Type 35 is probably the most successful racing car of all time, with over 2,000 wins. Bugattis swept to victory in the Targa Florio for five years straight from 1925 through 1929. Louis Chiron held the most podiums in Bugatti cars, and the modern marque revival Bugatti Automobiles S.A.S. named the 1999 Bugatti 18/3 Chiron concept car in his honour. But it was the final racing success at Le Mans that is most remembered—Jean-Pierre Wimille and Pierre Veyron won the 1939 race with just one car and meagre resources.
Aeroplane racing.
In the 1930s, Ettore Bugatti got involved in the creation of a racer airplane, hoping to beat the Germans in the Deutsch de la Meurthe prize. This would be the Bugatti 100P, which never flew. It was designed by Belgian engineer Louis de Monge who had already applied Bugatti Brescia engines in his "Type 7.5" lifting body.
Railcar.
Ettore Bugatti also designed a successful motorised railcar, the "Autorail" ().
A family tragedy.
The death of Ettore Bugatti's son, Jean Bugatti, on 11 August 1939 marked a turning point in the company's fortunes. Jean died while testing a Type 57 tank-bodied race car near the Molsheim factory.
After World War II.
World War II left the Molsheim factory in ruins and the company lost control of the property. During the war, Bugatti planned a new factory at Levallois, a northwestern suburb of Paris. After the war, Bugatti designed and planned to build a series of new cars, including the Type 73 road car and Type 73C single seat racing car, but in all Bugatti built only five Type 73 cars.
Development of a 375 cc supercharged car was stopped when Ettore Bugatti died on 21 August 1947. Following Ettore Bugatti's death, the business declined further and made its last appearance as a business in its own right at a Paris Motor Show in October 1952.
After a long decline, the original incarnation of Bugatti ceased operations in 1952.
Design.
Bugattis are noticeably focused on design. Engine blocks were hand scraped to ensure that the surfaces were so flat that gaskets were not required for sealing, many of the exposed surfaces of the engine compartment featured Guilloché (engine turned) finishes on them, and safety wires had been threaded through almost every fastener in intricately laced patterns. Rather than bolt the springs to the axles as most manufacturers did, Bugatti's axles were forged such that the spring passed though a carefully sized opening in the axle, a much more elegant solution requiring fewer parts. He famously described his arch competitor Bentley's cars as "the world's fastest lorries" for focusing on durability. According to Bugatti, "weight was the enemy".
Gallery.
Notable finds in the modern era.
Relatives of Dr. Harold Carr found a rare 1937 Bugatti Type 57S Atalante when cataloguing the doctor's belongings after his death in 2009. Dr. Carr's Type 57S is notable because it was originally owned by British race car driver Earl Howe. Because much of the car's original equipment is intact, it can be restored without relying on replacement parts.
On 10 July 2009, a 1925 Bugatti Brescia Type 22 which had lain at the bottom of Lake Maggiore on the border of Switzerland and Italy for 75 years was recovered from the lake. The Mullin Museum in Oxnard, California bought it at auction for $351,343 at Bonham's Retromobile sale in Paris in 2010.
Attempts at revival.
The company attempted a comeback under Roland Bugatti in the mid-1950s with the mid-engined Type 251 race car. Designed with help from Gioacchino Colombo, the car failed to perform to expectations and the company's attempts at automobile production were halted.
In the 1960s, Virgil Exner designed a Bugatti as part of his "Revival Cars" project. A show version of this car was actually built by Ghia using the last Bugatti Type 101 chassis, and was shown at the 1965 Turin Motor Show. Finance was not forthcoming, and Exner then turned his attention to a revival of Stutz.
Bugatti continued manufacturing airplane parts and was sold to Hispano-Suiza, also a former auto maker turned aircraft supplier, in 1963. Snecma took over Hispano-Suiza in 1968. After acquiring Messier, Snecma merged Messier and Bugatti into Messier-Bugatti in 1977.
Modern revivals.
Bugatti Automobili SpA 1987–1995.
Italian entrepreneur Romano Artioli acquired the Bugatti brand in 1987, and established Bugatti Automobili SpA. Bugatti commissioned architect Giampaolo Benedini to design the factory which was built in Campogalliano, Italy.
By 1989 the plans for the new Bugatti revival were presented by Paolo Stanzani and Marcello Gandini, designers of the Lamborghini Miura and Lamborghini Countach. Bugatti called their first production vehicle the Bugatti EB110 GT. Bugatti advertised the EB110 as the most technically advanced sports car ever produced.
Famed racing car designer Mauro Forghieri served as Bugatti's technical director from 1992 through 1994.
On 27 August 1993, through his holding company, ACBN Holdings S.A. of Luxembourg, Romano Artioli purchased Lotus Cars from General Motors. Bugatti made plans to list the company's shares on international stock exchanges.
Bugatti presented a prototype large saloon called the EB112 in 1993.
Perhaps the most famous Bugatti EB110 owner was seven-time Formula One World Champion racing driver Michael Schumacher who purchased an EB110 in 1994. Schumacher sold his EB110, which had been repaired after a severe 1994 crash, to Modena Motorsport, a Ferrari service and race preparation garage in Germany.
By the time the EB110 came to market, the North American and European economies were in recession. Poor economic conditions forced the company to fail and operations ceased in September 1995. A model specific to the US market called the "Bugatti America" was in the preparatory stages when the company ceased operations.
Bugatti's liquidators sold Lotus Cars to Proton of Malaysia. German firm Dauer Racing purchased the EB110 licence and remaining parts stock in 1997 in order to produce five more EB110 SS vehicles. These five SS versions of the EB110 were greatly refined by Dauer. The Campogalliano factory was sold to a furniture-making company, which subsequently collapsed before moving in, leaving the building unoccupied. After Dauer stopped producing cars in 2011, Toscana-Motors GmbH of Germany purchased the remaining parts stock from Dauer.
Bugatti Automobiles S.A.S. 1998–present.
Volkswagen AG acquired the Bugatti brand in 1998.
Bugatti Automobiles S.A.S. commissioned Giorgetto Giugiaro of ItalDesign to produce Bugatti's first concept vehicle, the EB118, a coupé that debuted at the 1998 Paris Auto Show. The EB118 concept featured a , W-18 engine. After its Paris debut, the EB118 concept was shown again in 1999 at the Geneva Auto Show and the Tokyo Motor Show.
Bugatti introduced its next concepts, the EB 218 at the 1999 Geneva Motor Show and the 18/3 Chiron at the 1999 Frankfurt Motor Show (IAA).
Bugatti Automobiles S.A.S. began assembling its first regular-production vehicle, the Bugatti Veyron 16.4 (the 1001 BHP super car with an 8-litre W-16 engine with four turbochargers) in September 2005 at the Bugatti Molsheim, France assembly "studio".

</doc>
<doc id="4498" url="http://en.wikipedia.org/wiki?curid=4498" title="Benchmark">
Benchmark

 
Benchmark may refer to:

</doc>
<doc id="4499" url="http://en.wikipedia.org/wiki?curid=4499" title="Band">
Band

Band may refer to:

</doc>
<doc id="4501" url="http://en.wikipedia.org/wiki?curid=4501" title="Black Death">
Black Death

The Black Death was one of the most devastating pandemics in human history, resulting in the deaths of an estimated people and peaking in Europe in the years 1346–53.<ref name="ABC/Reuters"></ref> Although there were several competing theories as to the etiology of the Black Death, analysis of DNA from victims in northern and southern Europe published in 2010 and 2011 indicates that the pathogen responsible was the "Yersinia pestis" bacterium, probably causing several forms of plague.
The Black Death is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road, reaching the Crimea by 1343. From there, it was most likely carried by Oriental rat fleas living on the black rats that were regular passengers on merchant ships. Spreading throughout the Mediterranean and Europe, the Black Death is estimated to have killed 30–60% of Europe's total population. All in all, the plague reduced the world population from an estimated 450 million down to 350–375 million in the 14th century.
The aftermath of the plague created a series of religious, social, and economic upheavals, which had profound effects on the course of European history. It took for Europe's population to recover. The plague recurred occasionally in Europe until the 19th century.
Chronology.
Origins of the disease.
The plague disease, caused by "Yersinia pestis", is enzootic (commonly present) in populations of fleas carried by ground rodents, including marmots, in various areas including Central Asia, Kurdistan, Western Asia, Northern India and Uganda. Nestorian graves dating to 1338–9 near Lake Issyk Kul in Kyrgizstan have inscriptions referring to plague and are thought by many epidemiologists to mark the outbreak of the epidemic, from which it could easily have spread to China and India. In October 2010, medical geneticists suggested that all three of the great waves of the plague originated in China. In China, the 13th century Mongol conquest caused a decline in farming and trading. However, economic recovery had been observed at the beginning of the 14th century. In the 1330s a large number of natural disasters and plagues led to widespread famine, starting in 1331, with a deadly plague arriving soon after. Epidemics which may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years before it reached Constantinople in 1347. However, according to George Sussman, the first obvious medical description of plague in China dates to 1644.
The disease may have travelled along the Silk Road with Mongol armies and traders or it could have come via ship. By the end of 1346, reports of plague had reached the seaports of Europe: "India was depopulated, Tartary, Mesopotamia, Syria, Armenia were covered with dead bodies".
Plague was reportedly first introduced to Europe at the trading city of Caffa in the Crimea in 1347. After a protracted siege, during which the Mongol army under Jani Beg was suffering from the disease, the army catapulted the infected corpses over the city walls to infect the inhabitants. The Genoese traders fled, taking the plague by ship into Sicily and the south of Europe, whence it spread north. Whether or not this hypothesis is accurate, it is clear that several existing conditions such as war, famine, and weather contributed to the severity of the Black Death.
European outbreak.
There appear to have been several introductions into Europe. The plague reached Sicily in October 1347, carried by twelve Genoese galleys, and rapidly spread all over the island. Galleys from Caffa reached Genoa and Venice in January 1348, but it was the outbreak in Pisa a few weeks later that was the entry point to northern Italy. Towards the end of January, one of the galleys expelled from Italy arrived in Marseille.
From Italy, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348–50. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague spared some parts of Europe, including the Kingdom of Poland, the majority of the Basque Country and isolated parts of Belgium and the Netherlands.
Middle Eastern outbreak.
The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 1348–49, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.
Mecca became infected in 1349. During the same year, records show the city of Mawsil (Mosul) suffered a massive epidemic, and the city of Baghdad experienced a second round of the disease. In 1351 Yemen experienced an outbreak of the plague, coinciding with the return of King Mujahid of Yemen from imprisonment in Cairo. His party may have brought the disease with them from Egypt.
Symptoms.
Contemporary accounts of the plague are often varied or imprecise. The most commonly noted symptom was the appearance of buboes (or gavocciolos) in the groin, the neck and armpits, which oozed pus and bled when opened. Boccaccio's description is graphic:
Ziegler comments that the only medical detail that is questionable is the infallibility of approaching death, as if the bubo discharges, recovery is possible.
This was followed by acute fever and vomiting of blood. Most victims died two to seven days after initial infection. David Herlihy identifies another potential sign of the plague: freckle-like spots and rashes which could be caused by flea-bites.
Some accounts, like that of Louis Heyligen, a musician in Avignon who died of the plague in 1348, noted a distinct form of the disease which infected the lungs and led to respiratory problems and which is identified with pneumonic plague.
Naming.
Medieval people called the catastrophe of the 14th century either the "Great Pestilence"' or the "Great Plague". Writers contemporary to the plague referred to the event as the "Great Mortality". Swedish and Danish chronicles of the 17th century described the events as "black" for the first time, not to describe the late-stage sign of the disease, in which the sufferer's skin would blacken due to subepidermal hemorrhages and the extremities would darken with a form of gangrene, acral necrosis, but more likely to refer to black in the sense of glum or dreadful and to denote the terror and gloom of the events. Gasquet (1908) claimed that the Latin name ""atra mors" (Black Death) for an epidemic first appeared in modern times in 1631 in a book on Danish history by J.I. Pontanus, where Pontanus wrote about a disease that occurred in 1348: "Vulgo & ab effectu "atram mortem" vocatibant."" Commonly and from its effects, they called it the black death. This may have been a mistranslation, as "atra" can mean "black", "brooding" or "terrible". Nevertheless, the name spread through Scandinavia and then Germany. In England, it was not until 1823 that the medieval epidemic was first called the Black Death.
Causes.
Medical knowledge had stagnated during the Middle Ages. The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens, in the form of a conjunction of three planets in 1345 that caused a "great pestilence in the air". This report became the first and most widely circulated of a series of "plague tracts" that sought to give advice to sufferers. That the plague was caused by "bad air" became the most widely accepted theory. The word "plague" had no special significance at this time, and only the recurrence of outbreaks during the Middle Ages gave it the name that has become the medical term.
The importance of hygiene was recognised only in the nineteenth century; until then it was common that the streets were filthy, with live animals of all sorts around and human parasites abounding. A transmissible disease will spread easily in such conditions. One development as a result of the Black Death was the establishment of the idea of quarantine in Dubrovnik in 1377 after continuing outbreaks.
The dominant explanation for the Black Death is the plague theory, which attributes the outbreak to "Yersinia pestis", also responsible for an epidemic that began in southern China in 1865, eventually spreading to India. The investigation of the pathogen that caused the 19th-century plague was begun by teams of scientists who visited Hong Kong in 1894, among whom was the French-Swiss bacteriologist Alexandre Yersin, after whom the pathogen was named "Yersinia pestis". The mechanism by which "Y. pestis" was usually transmitted was established in 1898 by Paul-Louis Simond and was found to involve the bites of fleas whose midguts had become obstructed by replicating "Y. pestis" several days after feeding on an infected host. This blockage results in starvation and aggressive feeding behaviour by the fleas, which repeatedly attempt to clear their blockage by regurgitation, resulting in thousands of plague bacteria being flushed into the feeding site, infecting the host. The bubonic plague mechanism was also dependent on two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic; and a second that lack resistance. When the second population dies, the fleas move on to other hosts, including people, thus creating a human epidemic.
The historian Francis Aidan Gasquet, who had written about the 'Great Pestilence' in 1893 and suggested that "it would appear to be some form of the ordinary Eastern or bubonic plague" was able to adopt the epidemiology of the bubonic plague for the Black Death for the second edition in 1908, implicating rats and fleas in the process, and his interpretation was widely accepted for other ancient and medieval epidemics, such as the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE.
Other forms of plague have been implicated by modern scientists. The modern bubonic plague has a mortality rate of 30–75% and symptoms including fever of , headaches, painful aching joints, nausea and vomiting, and a general feeling of malaise. Left untreated, of those that contract the bubonic plague, 80 percent die within eight days. Pneumonic plague has a mortality rate of 90 to 95 percent. Symptoms include fever, cough, and blood-tinged sputum. As the disease progresses, sputum becomes free flowing and bright red. Septicemic plague is the least common of the three forms, with a mortality rate near 100%. Symptoms are high fevers and purple skin patches (purpura due to disseminated intravascular coagulation). In cases of pneumonic and particularly septicemic plague the progress of the disease is so rapid that there would often be no time for the development of the enlarged lymph nodes that were noted as buboes.
"Many modern scholars accept that the lethality of the Black Death stemmed from the combination of bubonic and pneumonic plague with other diseases and warn that every historical mention of 'pest' was not necessarily bubonic plague... In her study of 15th-century outbreaks, Ann Carmichael states that worms, the pox, fevers and dysentery clearly accompanied bubonic plague."
DNA evidence.
In October 2010, the open-access scientific journal "PLoS Pathogens" published a paper by a multinational team who undertook a new investigation into the role of "Yersinia pestis" in the Black Death following the disputed identification by Drancourt and Raoult in 1998. Their surveys tested for DNA and protein signatures specific for "Y. pestis" in human skeletons from widely distributed mass graves in northern, central and southern Europe that were associated archaeologically with the Black Death and subsequent resurgences. The authors concluded that this new research, together with prior analyses from the south of France and Germany
The study also found that there were two previously unknown but related clades (genetic branches) of the "Y. pestis" genome associated with medieval mass graves. These clades (which are thought to be extinct) were found to be ancestral to modern isolates of the modern "Y. pestis" strains "Y. p. orientalis" and "Y. p. medievalis", suggesting the plague may have entered Europe in two waves. Surveys of plague pit remains in France and England indicate the first variant entered Europe through the port of Marseille around November 1347 and spread through France over the next two years, eventually reaching England in the spring of 1349, where it spread through the country in three epidemics. Surveys of plague pit remains from the Dutch town of Bergen op Zoom showed the "Y. pestis" genotype responsible for the pandemic that spread through the Low Countries from 1350 differed from that found in Britain and France, implying Bergen op Zoom (and possibly other parts of the southern Netherlands) was not directly infected from England or France in 1349 and suggesting a second wave of plague, different from those in Britain and France, may have been carried to the Low Countries from Norway, the Hanseatic cities or another site.
The results of the Haensch study have since been confirmed and amended. Based on genetic evidence derived from Black Death victims in the East Smithfield burial site in England, Schuenemann et al. concluded in 2011 "that the Black Death in medieval Europe was caused by a variant of "Y. pestis" that may no longer exist." A study published in "Nature" in October 2011 sequenced the genome of "Y. pestis" from plague victims and indicated that the strain that caused the Black Death is ancestral to most modern strains of the disease.
DNA taken from 25 skeletons in London that died in the 14th century, have shown the plague is a strain of "Y. pestis" that is almost identical to that which hit Madagascar in 2013.
Alternative explanations.
This interpretation was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001).
It is recognised that an epidemiological account of the plague is as important as an identification of symptoms, but researchers are hampered by the lack of reliable statistics from this period. Most work has been done on the spread of the plague in England, and even estimates of overall population at the start vary by over 100% as no census was undertaken between the time of publication of the Domesday Book and the year 1377. Estimates of plague victims are usually extrapolated from figures from the clergy.
In addition to arguing that the rat population was insufficient to account for a bubonic plague pandemic, sceptics of the bubonic plague theory point out that the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance and that the DNA results may be flawed and might not have been repeated elsewhere, despite extensive samples from other mass graves. Other arguments include the lack of accounts of the death of rats before outbreaks of plague between the 14th and 17th centuries; temperatures that are too cold in northern Europe for the survival of fleas; that, despite primitive transport systems, the spread of the Black Death was much faster than that of modern bubonic plague; that mortality rates of the Black Death appear to be very high; that, while modern bubonic plague is largely endemic as a rural disease, the Black Death indiscriminately struck urban and rural areas; and that the pattern of the Black Death, with major outbreaks in the same areas separated by 5 to 15 years, differs from modern bubonic plague—which often becomes endemic for decades with annual flare-ups.
Walløe complains that all of these authors "take it for granted that Simond's infection model, black rat → rat flea → human, which was developed to explain the spread of plague in India, is the only way an epidemic of "Yersinia pestis" infection could spread", whilst pointing to several other possibilities.
A variety of alternatives to the "Y. pestis" have been put forward. Twigg suggested that the cause was a form of anthrax and N. F. Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as "hemorrhagic" plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the "Y. pestis" was spread from fleas on rats and argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the "Y. pestis" as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of "blood poisoning") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.
Consequences.
Death toll.
There are no exact figures for the death toll; the rate varied widely by locality. It killed some people in Eurasia. According to medieval historian Philip Daileader in 2007:
The trend of recent research is pointing to a figure more like 45–50% of the European population dying during a four-year period. There is a fair amount of geographic variation. In Mediterranean Europe, areas such as Italy, the south of France and Spain, where plague ran for about four years consecutively, it was probably closer to 75–80% of the population. In Germany and England ... it was probably closer to 20%.
The most widely accepted estimate for the Middle East, including Iraq, Iran and Syria, during this time, is for a death rate of about a third. The Black Death killed about 40% of Egypt's population. Half of Paris's population of 100,000 people died. In Italy, Florence's population was reduced from 110–120 thousand inhabitants in 1338 down to 50 thousand in 1351. At least 60% of Hamburg's and Bremen's population perished, and a similar percentage of Londoners may have died from the disease as well. Before 1350, there were about 170,000 settlements in Germany, and this was reduced by nearly 40,000 by 1450. In 1348, the plague spread so rapidly that before any physicians or government authorities had time to reflect upon its origins, about a third of the European population had already perished. In crowded cities, it was not uncommon for as much as 50% of the population to die. The disease bypassed some areas, and the most isolated areas were less vulnerable to contagion. Monks and priests were especially hard hit since they cared for the Black Death's victims.
Persecutions.
Renewed religious fervor and fanaticism bloomed in the wake of the Black Death. Some Europeans targeted "various groups such as Jews, friars, foreigners, beggars, pilgrims", lepers and Romani, thinking that they were to blame for the crisis. Lepers, and other individuals with skin diseases such as acne or psoriasis, were singled out and exterminated throughout Europe.
Because 14th-century healers were at a loss to explain the cause, Europeans turned to astrological forces, earthquakes, and the poisoning of wells by Jews as possible reasons for the plague's emergence. The governments of Europe had no apparent response to the crisis because no one knew its cause or how it spread. The mechanism of infection and transmission of diseases was little understood in the 14th century; many people believed only God's anger could produce such horrific displays.
There were many attacks against Jewish communities. In August 1349, the Jewish communities of Mainz and Cologne were exterminated. In February of that same year, the citizens of Strasbourg murdered 2,000 Jews. By 1351, 60 major and 150 smaller Jewish communities were destroyed.
Recurrence.
The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, the plague was present somewhere in Europe in every year between 1346 and 1671. The was particularly widespread in the following years: 1360–63; 1374; 1400; 1438–39; 1456–57; 1464–66; 1481–85; 1500–03; 1518–31; 1544–48; 1563–66; 1573–88; 1596–99; 1602–11; 1623–40; 1644–54; and 1664–67. Subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). According to Geoffrey Parker, "France alone lost almost a million people to the plague in the epidemic of 1628–31."
In England, in the absence of census figures, historians propose a range of preincident population figures from as high as 7 million to as low as 4 million in 1300, and a postincident population figure as low as 2 million. By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–62, 1369, 1379–83, 1389–93, and throughout the first half of the 15th century. An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–80 could have been as high as 20%. The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665.
In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623–25, and again in 1635–36, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576–77 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629–1631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348–50. The last plague outbreak ravaged Oslo in 1654.
In the first half of the 17th century, a plague claimed some 1.7 million victims in Italy, or about 14% of the population. In 1656, the plague killed about half of Naples' 300,000 inhabitants. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain. The plague of 1649 probably reduced the population of Seville by half. In 1709–13, a plague epidemic that followed the Great Northern War (1700–21, Sweden v. Russia and allies) killed about 100,000 in Sweden, and 300,000 in Prussia. The plague killed two-thirds of the inhabitants of Helsinki, and claimed a third of Stockholm's population. Europe's last major epidemic occurred in 1720 in Marseille.
The Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30 to 50 thousand inhabitants to it in 1620–21, and again in 1654–57, 1665, 1691, and 1740–42. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, thirty-seven larger and smaller epidemics were recorded in Constantinople, and an additional thirty-one between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out.
Third plague pandemic.
The Third plague pandemic (1855–1859) started in China in the middle of the 19th century, spreading to all inhabited continents and killing 10 million people in India alone. Twelve plague outbreaks in Australia in 1900–25 resulted in well over , chiefly in Sydney. This led to the establishment of a Public Health Department there which undertook some leading-edge research on plague transmission from rat fleas to humans via the bacillus "Yersinia pestis".
The first North American plague epidemic was the San Francisco plague of 1900–1904, followed by another outbreak in 1907–08. From 1944 through 1993, 362 cases of human plague were reported in the United States; approximately 90% occurred in four western states: Arizona, California, Colorado, and New Mexico. Plague was confirmed in the United States from 9 western states during 1995. Currently, 5 to 15 people in the United States are estimated to catch the disease each year—typically in western states.
The plague bacterium could develop drug-resistance and again become a major health threat. One case of a drug-resistant form of the bacterium was found in Madagascar in 1995.

</doc>
<doc id="4502" url="http://en.wikipedia.org/wiki?curid=4502" title="Biotechnology">
Biotechnology

Biotechnology is the use of living systems and organisms to develop or make useful products, or "any technological application that uses biological systems, living organisms or derivatives thereof, to make or modify products or processes for specific use" (UN Convention on Biological Diversity, Art. 2). Depending on the tools and applications, it often overlaps with the (related) fields of bioengineering and biomedical engineering.
For thousands of years, humankind has used biotechnology in agriculture, food production, and medicine. The term itself is largely believed to have been coined in 1919 by Hungarian engineer Károly Ereky. In the late 20th and early 21st century, biotechnology has expanded to include new and diverse sciences such as genomics, recombinant gene technologies, applied immunology, and development of pharmaceutical therapies and diagnostic tests.
Definitions.
The wide concept of "biotech" or "biotechnology" encompasses a wide range of procedures for modifying living organisms according to human purposes, going back to domestication of animals, cultivation of plants, and "improvements" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms such as pharmaceuticals, crops, and livestock. Biotechnology also writes on the pure biological sciences (animal cell culture, biochemistry, cell biology, embryology, genetics, microbiology, and molecular biology). In many instances, it is also dependent on knowledge and methods from outside the sphere of biology including:
Conversely, modern biological sciences (including even concepts such as molecular ecology) are intimately entwined and heavily dependent on the methods developed through biotechnology and what is commonly thought of as the life sciences industry. Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).
By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials "directly") for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals. Relatedly, biomedical engineering is an overlapping field that often draws upon and applies "biotechnology" (by various definitions), especially in certain sub-fields of biomedical and/or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.
History.
Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of "'using a biotechnological system to make products". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.
Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best suited crops, having the highest yields, to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.
These processes also were included in early fermentation of beer. These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods. In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains were broken down into alcohols such as ethanol. Later other cultures produced the process of lactic acid fermentation which allowed the fermentation and preservation of other forms of food, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.
For thousands of years, humans have used selective breeding to improve production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.
In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using "Clostridium acetobutylicum," to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.
Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold "Penicillium". His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley - to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.
The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of "Diamond v. Chakrabarty". Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the "Pseudomonas" genus) capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the "Pseudomonas" bacterium.
Revenue in the industry is expected to grow by 12.9% in 2008. Another factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.
Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds which are resistant to pests and drought. By boosting farm productivity, biotechnology plays a crucial role in ensuring that biofuel production targets are met.
Applications.
Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non food (industrial) uses of crops and other products (e.g. biodegradable plastics, vegetable oil, biofuels), and environmental uses.
For example, one application of biotechnology is the directed use of organisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, cleanup sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.
A series of derived terms have been coined to identify several branches of biotechnology; for example:
The investment and economic output of all of these types of applied biotechnologies is termed as "bioeconomy".
Medicine.
In medicine, modern biotechnology finds applications in areas such as pharmaceutical drug discovery and production, pharmacogenomics, and genetic testing (or genetic screening).
Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs. It deals with the influence of genetic variation on drug response in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity. By doing so, pharmacogenomics aims to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects. Such approaches promise the advent of "personalized medicine"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.
Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology - biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium "Escherichia coli". Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle and/or pigs). The resulting genetically engineered bacterium enabled the production of vast quantities of synthetic human insulin at relatively low cost. Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well. 
Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins. Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use. Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.
Agriculture.
Genetically modified crops ("GM crops", or "biotech crops") are plants used in agriculture, the DNA of which has been modified using genetic engineering techniques. In most cases the aim is to introduce a new trait to the plant which does not occur naturally in the species. 
Examples in food crops include resistance to certain pests, diseases, stressful environmental conditions, resistance to chemical treatments (e.g. resistance to a herbicide), reduction of spoilage, or improving the nutrient profile of the crop. Examples in non-food crops include production of pharmaceutical agents, biofuels, and other industrially useful goods, as well as for bioremediation.
Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from to 1,600,000 km2 (395 million acres). 10% of the world's crop lands were planted with GM crops in 2010. As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the USA, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.
Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA using the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding. Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato. To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed, although as of November 2013 none are currently on the market.
There is broad scientific consensus that food on the market derived from GM crops poses no greater risk to human health than conventional food. GM crops also provide a number of ecological benefits, if not used in excess. However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.
Industrial biotechnology.
Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as micro-organisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels. In doing so, biotechnology uses renewable raw materials and may contribute to lowering greenhouse gas emissions and moving away from a petrochemical-based economy.
Regulation.
The regulation of genetic engineering concerns the approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the USA and Europe. Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety. The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing. The cultivation of GMOs has triggered a debate about coexistence of GM and nonGM crops. Depending on the coexistence regulations incentives for cultivation of GM crops differ.
Learning.
In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support is provided for two or three years during the course of their Ph.D. thesis work. Nineteen institutions offer NIGMS supported BTPs. Biotechnology training is also offered at the undergraduate level and in community colleges.

</doc>
<doc id="4503" url="http://en.wikipedia.org/wiki?curid=4503" title="Battle of Poitiers">
Battle of Poitiers

The Battle of Poitiers was a major battle of the Hundred Years' War between England and France. The battle occurred on 19 September 1356 near Poitiers, France. Preceded by the Battle of Crécy in 1346, and followed by the Battle of Agincourt in 1415, it was the second of the three great English victories of the war.
Background.
Edward, Prince of Wales (later known as the Black Prince), the eldest son of King Edward III, began a great "chevauchée" on 8 August 1356. He conducted many scorched earth raids northwards from the English base in Aquitaine, in an effort to bolster his troops in central France, as well as to raid and ravage the countryside. His forces met little resistance, burning numerous towns to the ground and living off the land, until they reached the River Loire at Tours. They were unable to take the castle or burn the town due to a heavy downpour. This delay allowed John II, King of France, to attempt to catch Edward's army. The King, who had been besieging Breteuil in Normandy, arranged the bulk of his army at Chartres to the north of the besieged Tours, dismissing approximately 15,000–20,000 of his lower-quality infantry to increase the speed of his forces.
Negotiations prior to the Battle of Poitiers.
There were negotiations before the battle of Poitiers that are recorded in the writings of the life of Sir John Chandos. He records the final moments of a meeting of both sides in an effort to avoid the bloody conflict at Poitiers. The extraordinary narrative occurred just before that battle and reads as follows:
Nobles and men-at-arms who fought with the Black Prince.
Jean Froissart states these men fought at Poitiers: Earl of Warwick, the Earl of Suffolk; Earl of Salisbury, Earl of Oxford, Raynold Cobham, Lord Spencer, Lord James Audley, Lord Peter his brother, Lord Berkeley, Lord Basset, Lord Warin, Lord Delaware, Lord Manne, Lord Willoughby, Lord Bartholomew de Burghersh, Lord of Felton, Lord Richard of Pembroke, Lord Stephen of Cosington; Lord Bradetane and other Englishmen; Lord of Pommiers from Gascon, Lord of Languiran, the captal of Buch, Lord John the 69th of Caumont, Lord de Lesparre, Lord of Rauzan, Lord of Condon, Lord of Montferrand, the Lord of Landiras, Lord Soudic of Latrau and other (men-at-arms); from Hainowes, Lord Eustace d'Aubrecicourt; Lord John of Ghistelles, and two other strangers, Lord Daniel Pasele and Lord Denis of Amposta, a fortress in Catalonia.Edward le Despencer, 1st Baron le Despencer also fought at Poitiers under The Black Prince. Sir Thomas Felton fought not only at Poitiers but also at the Battle of Crécy.
One of the chief commanders at both Crécy and Poitiers was John de Vere, Earl of Oxford, mentioned above.
Another account states that John of Ghistelles perished at the Battle of Crécy so there is some ambiguity as to this man.
Nobles and men-at-arms who fought with King Jean II at, or just prior to, the battle.
Froissart describes, with less specificity in this passage, some of the nobles that were assembled at, or just prior to the Battle: The Englishmen were shadowed by some expert French knights, who reported back to the king what the Englishmen did. The king came to the Haye in Touraine and his men had passed the river Loire, some at the bridge of Orléans and some at Meung, at Saumur, at Blois, and at Tours and other places: there were twenty thousand men of arms and other soldiers. He estimates there were twenty-six dukes and earls (Counts), more than 120 banners, and the four sons of the king, Duke Charles of Normandy, the Duke Louis, the Duke of Anjou, and John, Duke of Berry, and Lord Philip.
The French army also included a contingent of Scots commanded by Sir William Douglas. Douglas fought in the King's own Battle, but when the fight seemed over Douglas was dragged by his men from the melee. Froissart states that "... the Earl Douglas of Scotland, who fought a season valiantly, but when he saw the discomfiture he departed and saved himself; for in no wise would he be taken by the Englishmen, he would rather there be slain".
Others who were either killed or captured in the battle were: King John II; Prince Philip (youngest son and progenitor of the House of Valois-Burgundy), Geoffroi de Charny, carrier of the Oriflamme, Peter I, Duke of Bourbon, Walter VI, Count of Brienne and Constable of France, Jean de Clermont, Marshal of France, Arnoul d'Audrehem, the Count of Eu, the Count of Marche and Ponthieu Jacques de Bourbon taken prisoner at the battle and died 1361, the Count of Étampes, the Count of Tancarville, the Count of Dammartin, the Count of Joinville, Guillaume de Melun, Archbishop of Sens.
The battle.
At the beginning of the battle, the English removed their baggage train leading the French to think they were about to retreat which provoked a hasty charge by the French knights against the archers. According to Froissart, the English attacked the enemy, especially the horses, with a shower of arrows. Geoffrey the Baker writes that the French armour was invulnerable to the English arrows, that the arrowheads either skidded off the armour or shattered on impact. Given the following actions of the archers, it seems likely Baker was correct. The armour on the horses was weaker on the sides and back, so the archers moved to the sides of the cavalry and shot the horses in the flanks. This was a popular method of stopping a cavalry charge, as a falling horse often destroyed the cohesion of the enemy's line. The results were devastating. The Dauphin attacked Salisbury and pressed his advance in spite of heavy shot by the English archers and complications of running into the retreating vanguard of Clermont's force. Green suggests that the Dauphin had thousands of troops with him in this phase of the attack. He advanced to the English lines but ultimately fell back. The French were unable to penetrate the protective hedge the English were using. This phase of the attack lasted about two hours.
This cavalry attack was followed by infantry attack. The Dauphin's infantry engaged in heavy fighting, but withdrew to regroup. The next wave of infantry under Orléans, seeing that the Dauphin's men were not attacking, turned back and panicked. This stranded the forces led by the King himself. This was a formidable fighting force, and the English archers were running very low on arrows; the archers joined the infantry in the fight and some of both groups mounted horses to form an improvised cavalry.
At about this time, King John sent two sons from the battlefield. His youngest son, Philip, stayed with him and fought at his side in the final phase of the battle. When the Dauphin and other sons withdrew, the duke of Orléans also withdrew. Combat was hard, but the Black Prince still had a mobile reserve hidden in the woods, commanded by Jean de Grailly, the Captal de Buch; which was able to circle around and attack the French in the flank and rear. The French were fearful of encirclement and attempted to flee. King John was captured with his immediate entourage only after a memorable resistance.
Amongst the notable captured or killed according to Froissart were:
The capture of the French king.
Froissart again gives us a vivid description of the capture of King Jean II and his youngest son in this passage: 
Aftermath of the battle.
As Edward, the Black Prince, wrote shortly afterward in a letter to the people of London:
See also Ransom of King John II of France.
Aftermath in France.
Jean de Venette, a Carmelite friar and medieval chronicler vividly describes the chaos in France which he states he himself witnessed, after the time of this Battle. He states: Jean is referring here not only to the French Nobles, but to the Companies who also plundered the peasants and Churches.

</doc>
<doc id="4505" url="http://en.wikipedia.org/wiki?curid=4505" title="Backbone cabal">
Backbone cabal

The backbone cabal was an informal organization of large-site administrators of the worldwide distributed newsgroup-based discussion system Usenet. It existed from about 1983 at least into the 2000s.
The cabal was created in an effort to facilitate reliable propagation of new Usenet posts: While in the 1970s and 1980s many news servers only operated during night time to save on the cost of long distance communication, servers of the backbone cabal were available 24 hours a day. The administrators of these servers gained sufficient influence in the otherwise anarchic Usenet community to be able to push through controversial changes, for instance the Great Renaming of Usenet newsgroups during the 1980s.
History.
As Usenet has few technologically or legally enforced hierarchies, just about the only ones that formed were social hierarchies. People acquired power through persuasion, exerted both publicly and privately, public debate, force of will (often via aggressive flames), garnering authority and respect by spending much time and effort contributing to the community (by being a maintainer of a FAQ, for example; see also Kibo, etc.).
Credit for organizing the backbone about 1983 is commonly attributed to Gene "Spaf" Spafford, although it is also claimed by Mary Ann Horton. Other prominent members of the cabal were Brian Reid, Richard Sexton, Chuq von Rospach, Neil Crellin and Rick Adams.
In Internet culture.
During most of its existence, the cabal (sometimes capitalized) steadfastly denied its own existence; those involved would often respond "There is no Cabal" (sometimes abbreviated as "TINC"), whenever the existence or activities of the group were speculated on in public. It is sometimes used humorously to dispel cabal-like organizational conspiracy theories, or as an ironic statement, indicating one who knows the existence of "the cabal" will invariably deny there is one.
This belief became a model for various conspiracy theories about various Cabals with dark nefarious objectives beginning with taking over Usenet or the Internet. Spoofs include the of moustachioed hackers named "Eric"; ex-members of the P.H.I.R.M.; and the Lumber Cartel putatively funding anti-spam efforts to support the paper industry.
The result of this policy was an aura of mystery, even a decade after the cabal mailing list disbanded in late 1988 following an internal fight.

</doc>
<doc id="4506" url="http://en.wikipedia.org/wiki?curid=4506" title="Bongo (antelope)">
Bongo (antelope)

The bongo ("Tragelaphus eurycerus") is a herbivorous, mostly nocturnal forest ungulate; it is among the largest of the African forest antelope species.
Bongos are characterised by a striking reddish-brown coat, black and white markings, white-yellow stripes and long slightly spiralled horns. Indeed, bongos are the only tragelaphid in which both sexes have horns. They have a complex social interaction and are found in African dense forest mosaics.
The western or lowland bongo, "T. e. eurycerus", faces an ongoing population decline, and the IUCN Antelope Specialist Group considers it to be Near Threatened on the conservation status scale.
The eastern or mountain bongo, "T. e. isaaci", of Kenya, has a coat even more vibrant than that of "T. e. eurycerus". The mountain bongo is only found in the wild in one remote region of central Kenya. This bongo is classified by the IUCN Antelope Specialist Group as Critically Endangered, with more specimens in captivity than in the wild.
In 2000, the Association of Zoos and Aquariums in the USA (AZA) upgraded the bongo to a Species Survival Plan participant and in 2006 added the Bongo Restoration to Mount Kenya Project to its list of the Top Ten Wildlife Conservation Success Stories of the year. However, in 2013, it seems, these successes have been negated with reports of possibly only 100 mountain bongos left in the wild due to logging and poaching.
Taxonomy.
The bongo belongs to the genus "Tragelaphus", which includes the sitatunga ("T. spekeii"), the nyala ("T. angasii"), the bushbuck ("T. scriptus"), the mountain nyala ("T. buxtoni"), the lesser kudu ("T. imberbis"), and the greater kudu ("T. strepsiceros").
Bongos are further classified into two subspecies: "T. e. eurycerus", the lowland or western bongo, and the far rarer "T. e. isaaci", the mountain or eastern bongo, restricted to northeastern Central Africa. The eastern bongo is larger and heavier than the western bongo. Two other subspecies are described from West and Central Africa, but taxonomic clarification is required. They have been observed to live up to 19 years.
The generic name "Tragelaphus" is derived from the Greek words "trago" (a male goat) and "elaphos" (a deer), in combination referring to "an antelope". The specific name" eurycerus" originated from the fusion of "eurus" (broad, widespread) and "keras" (an animal's horn). "Bongo" is derived from a West African native name.
Distribution and habitat.
Bongos are found in tropical jungles with dense undergrowth up to an altitude of 4,000 meters (12,800 ft) in Central Africa, with isolated populations in Kenya, and these West African countries: Angola, Benin, Burkina Faso, Cameroon, the Central African Republic, the Republic of the Congo, the Democratic Republic of Congo, the Ivory Coast, Equatorial Guinea, Ethiopia, Gabon, Ghana, Guinea, Guinea-Bissau, Kenya, Liberia, Mali, Niger, Sierra Leone, South Sudan, Togo, and Uganda (regionally extinct, IUCN, 2002).
Historically, bongos occurred in three disjunct parts of Africa: East, Central and West. Today, all three populations’ ranges have shrunk in size due to habitat loss for agriculture and uncontrolled timber cutting, as well as hunting for meat.
Bongos favour disturbed forest mosaics that provide fresh, low-level green vegetation. Such habitats may be promoted by heavy browsing by elephants, fires, flooding, tree-felling (natural or by logging), and fallowing. Mass bamboo die-off provides ideal habitat in East Africa. They can live in bamboo forests.
Appearance.
Bongos are one of the largest of the forest antelopes. In addition to the deep chestnut colour of their coats, they have bright white stripes on their sides to help with camouflage.
Adults of both sexes are similar in size. Adult height is about at the shoulder and length is , including a tail of . Females weigh around , while males weigh about . Its large size puts it as the third-largest in the Bovidae tribe of Strepsicerotini, behind both the common and greater elands by about , and above the greater kudu by about .
Both sexes have heavy spiral horns; those of the male are longer and more massive. All bongos in captivity are from the isolated Aberdare Mountains of central Kenya.
Coat and body.
The bongo sports a bright auburn or chestnut coat, with the neck, chest, and legs generally darker than the rest of the body. Coats of male bongos become darker and buffy as they age until they reach a dark mahogany-brown colour. Coats of female bongos are usually more brightly coloured than those of males.
The pigmentation in the coat rubs off quite easily; anecdotal reports suggest rain running off a bongo may be tinted red with pigment. The smooth coat is marked with 10–15 vertical white-yellow stripes, spread along the back from the base of the neck to the rump. The number of stripes on each side is rarely the same. It also has a short, bristly, and vertical brown ridge of hair along the spine from the shoulder to the rump; the white stripes run into this ridge.
A white chevron appears between the eyes, and two large white spots grace each cheek. Another white chevron occurs where the neck meets the chest. The large ears are to sharpen hearing, and the distinctive coloration may help bongos identify one another in their dark forest habitats. Bongos have no special secretion glands, so rely less on scent to find one another than do other similar antelopes. The lips of a bongo are white, topped with a black muzzle.
Horns.
Bongos have two heavy and slightly spiralled horns that slope over their backs, and like in many other antelope species, both male and female bongos have horns. Bongos are the only tragelaphids in which both sexes have horns. The horns of bongos are in the form of a lyre and bear a resemblance to those of the related antelope species of nyalas, sitatungas, bushbucks, kudus and elands.
Unlike deer, which have branched antlers shed annually, bongos and other antelopes have pointed horns they keep throughout their lives. Males have massive backswept horns, while females have smaller, thinner, and more parallel horns. The size of the horns range between 75 and 99 cm (30–39 in). The horns twist once.
Like all other horns of antelopes, the core of a bongo's horn is hollow and the outer layer of the horn is made of keratin, the same material that makes up human fingernails, toenails. and hair. The bongo runs gracefully and at full speed through even the thickest tangles of lianas, laying its heavy spiralled horns on its back so the brush cannot impede its flight. Bongos are hunted for their horns by humans.
Social organization and behavior.
Like other forest ungulates, bongos are seldom seen in large groups. Males, called bulls, tend to be solitary, while females with young live in groups of six to eight. Bongos have seldom been seen in herds of more than 20. Gestation is about 285 days (9.5 months), with one young per birth, and weaning occurs at six months. Sexual maturity is reached at 24–27 months. The preferred habitat of this species is so dense and difficult to operate in, that few Europeans or Americans observed this species until the 1960s.
As young males mature and leave their maternal groups, they most often remain solitary, although rarely they join an older male. Adult males of similar size/age tend to avoid one another. Occasionally, they meet and spar with their horns in a ritualised manner and rarely serious fights take place. However, such fights are usually discouraged by visual displays, in which the males bulge their necks, roll their eyes, and hold their horns in a vertical position while slowly pacing back and forth in front of the other male. They seek out females only during mating time. When they are with a herd of females, males do not coerce them or try to restrict their movements as do some other antelopes.
Although mostly nocturnal, they are occasionally active during the day. However, like deer, bongos may exhibit crepuscular behaviour. Bongos are both timid and easily frightened; after a scare, a bongo moves away at considerable speed, even through dense undergrowth. They seek cover, where they stand still and alert, facing away from the disturbance and turning their heads from time to time to check on the situation. The bongo's hindquarters are less conspicuous than the forequarters, and from this position the animal can quickly flee.
When in distress, the bongo emits a bleat. It uses a limited number of vocalisations, mostly grunts and snorts; females have a weak mooing contact-call for their young. Females prefer to use traditional calving grounds restricted to certain areas, while newborn calves lie in hiding for a week or more, receiving short visits by the mother to suckle.
The calves grow rapidly and can soon accompany their mothers in the nursery herds. Their horns grow rapidly and begin to show in 3.5 months. They are weaned after six months and reach sexual maturity at about 20 months.
Diet.
Like many forest ungulates, bongos are herbivorous browsers and feed on tree/bush leaves, bushes, vines, bark and pith of rotting trees, grasses/herbs, roots, cereals, shrubs, and fruits.
Bongos require salt in their diets, and are known to regularly visit natural salt licks. Examination of bongo feces revealed the charcoal from trees burnt by lightning is consumed. This behavior is believed to be a means of getting salts and minerals into their diets. This behavior has also been reported in the okapi. Another similarity to the okapi, though the bongo is unrelated, is that the bongo has a long prehensile tongue which it uses to grasp grasses and leaves.
Suitable habitats for bongos must have permanent water available. A large animal, the bongo requires an ample amount of food, and is restricted to areas with abundant year-round growth of herbs and low shrubs. 
Population and conservation.
Few estimates of population density are available. Assuming average population densities of 0.25 animals per km2 in regions where it is known to be common or abundant, and 0.02 per km2 elsewhere, and with a total area of occupancy of 327,000 km2, a total population estimate of around 28,000 is suggested. Only about 60% are in protected areas, suggesting the actual numbers of the lowland subspecies may only be in the low tens of thousands. In Kenya, their numbers have declined significantly and on Mt. Kenya, they were within the last decade due to illegal hunting with dogs. Although information on their status in the wild is lacking, lowland bongos are not presently considered endangered.
Bongos are susceptible to diseases such as rinderpest, which almost exterminated the species during the 1890s. "Tragelaphus eurycerus" may suffer from goitre. Over the course of the disease, the thyroid glands greatly enlarge (up to 10 x 20 cm) and may become polycystic. Pathogenesis of goiter in the bongo may reflect a mixture of genetic predisposition coupled with environmental factors, including a period of exposure to a goitrogen. Leopards and spotted hyenas are the primary natural predators (lions are seldom encountered due to differing habitat preferences); pythons sometimes eat bongo calves. Humans prey on them for their pelts, horns, and meat, with the species being a common local source for "bush meat". Bongo populations have been greatly reduced by hunting, poaching, and animal trapping, although some bongo refuges exist.
Although bongos are quite easy for humans to catch using snares, many people native to the bongos' habitat believed that if they ate or touched bongo, they would have spasms similar to epileptic seizures. Because of this superstition, bongos were less harmed in their native ranges than expected. However, these taboos are said no longer to exist, which may account for increased hunting by humans in recent times.
Zoo programmes.
An international studbook is maintained to help manage animals held in captivity. Because of its bright colour, it is very popular in zoos and private collections. In North America, over 400 individuals are thought to be held, a population that probably exceeds that of the mountain bongo in the wild.
In 2000, the Association of Zoos and Aquariums (AZA) upgraded the bongo to a Species Survival Plan participant, which works to improve the genetic diversity of managed animal populations. The target population for participating zoos and private collections in North America is 250 animals. Through the efforts of zoos in North America, a reintroduction to the population in Kenya is being developed. 
At least one collaborative effort for reintroduction between North American wildlife facilities has already been carried out. In 2004, 18 eastern bongos born in North American zoos gathered at White Oak Conservation in Yulee, Florida for release in Kenya. White Oak staff members traveled with the bongos to a Mt. Kenya holding facility, where they stayed until being reintroduced.
Conservation.
In the last few decades, a rapid decline in the numbers of wild mountain bongo has occurred due to poaching and human pressure on their habitat, with local extinctions reported in Cherangani and Chepalungu hills, Kenya.
The Bongo Surveillance Programme, working alongside the Kenya Wildlife Service, have recorded photos of bongos at remote salt licks in the Aberdare Forests using camera traps, and, by analyzing DNA extracted from dung, have confirmed the presence of bongo in Mount Kenya, Eburru and Mau forests. The programme estimate as few as 140 animals left in the wild – spread across four isolated populations. Whilst captive breeding programmes can be viewed as having been successful in ensuring survival of this species in Europe and North America, the situation in the wild has been less promising. Evidence exists of bongo surviving in Kenya. However, these populations are believed to be small, fragmented, and vulnerable to extinction.
Animal populations with impoverished genetic diversity are inherently less able to adapt to changes in their environments (such as climate change, disease outbreaks, habitat change, etc.). The isolation of the four remaining small bongo populations, which themselves would appear to be in decline, means a substantial amount of genetic material is lost each generation. Whilst the population remains small, the impact of transfers will be greater, so the establishment of a "metapopulation management plan" occurs concurrently with conservation initiatives to enhance "in situ" population growth, and this initiative is both urgent and fundamental to the future survival of mountain bongo in the wild.
The western/lowland bongo faces an ongoing population decline as habitat destruction and hunting pressures increase with the relentless expansion of human settlement. Its long-term survival will only be assured in areas which receive active protection and management. At present, such areas comprise about 30,000 km2, and several are in countries where political stability is fragile. So, a realistic possibility exists whereby its status could decline to Threatened in the near future.
As the largest and most spectacular forest antelope, the western/lowland bongo is both an important flagship species for protected areas such as national parks, and a major trophy species which has been taken in increasing numbers in Central Africa by sport hunters during the 1990s. Both of these factors are strong incentives to provide effective protection and management of populations. 
Trophy hunting has the potential to provide economic justification for the preservation of larger areas of bongo habitat than national parks, especially in remote regions of Central Africa, where possibilities for commercially successful tourism are very limited.
The eastern/mountain bongo’s survival in the wild is dependent on more effective protection of the surviving remnant populations in Kenya. If this does not occur, it will eventually become extinct in the wild. The existence of a healthy captive population of this subspecies offers the potential for its reintroduction.
Groups supporting bongo conservation in Kenya.
In 2004, Dr. Jake Veasey; the head of the Department of Animal Management and Conservation at Woburn Safari Park and a member of the European Association of Zoos and Aquariums Population Management Advisory Group, with the assistance of Lindsay Banks, took over responsibility for the management and coordination of the European Endangered Species Programme for the eastern bongo. This includes some 250 animals across Europe and the Middle East.
Along with the Rothschild giraffe, the eastern bongo is arguably one of the most threatened large mammals in Africa, with recent estimates numbering less than 140 animals, below a minimum sustainable viable population. The situation is exacerbated because these animals are spread across four isolated populations. Whilst the bongo endangered species program can be viewed as having been successful in ensuring survival of this species in Europe, it has not yet become actively involved in the conservation of this species in the wild in a coordinated fashion. The plan is to engage in conservation activities in Kenya to assist in reversing the decline of the eastern bongo populations and genetic diversity in Africa, and in particular, applying population management expertise to help ensure the persistence of genetic diversity in the free ranging wild populations.
To illustrate significance of genetic diversity loss, assume the average metapopulation size is 35 animals based on 140 animals spread across four populations (140/4=35). Assuming stable populations, these populations will lose 8% of their genetic diversity every decade. By managing all four populations as one, through strategic transfers, gene loss is reduced from 8% to 2% per decade, without any increase in bongo numbers in Kenya. By managing the European and African populations as one – by strategic exports from Europe combined with "in situ" transfers, gene loss is reduced to 0.72% every 100 years, with both populations remaining stable. If populations in Kenya are allowed to grow through the implementation of effective conservation, including strategic transfers, gene loss can be effectively halted in this species and its future secured in the wild.
The initial aims of the project are: 
If effective protection were implemented immediately and bongo populations allowed to expand without transfers, then this would create a bigger population of genetically impoverished bongos. These animals would be less able to adapt to a dynamic environment. Whilst the population remains small, the impact of transfers will be greater. For this reason, the 'metapopulation management plan' must occur concurrently with conservation strategies to enhance "in situ" population growth. This initiative is both urgent and fundamental to the future survival of the mountain bongo in the wild.
In 2013, SafariCom telecommunications donated money to the Bongo Surveillance Programme to try and keep tabs on what is thought to be the last 100 eastern bongos left in the wild in the Mau Eburu Forest in central Kenya, whose numbers are still declining due to logging of their habitat and illegal poaching.
Status.
In 2002, the IUCN, listed the western/lowland species as Near Threatened. These bongos may be endangered due to human environmental interaction, as well as hunting and illegal actions towards wildlife. CITES lists bongos as an Appendix III species, only regulating their exportation from a single country, Ghana. It is not protected by the US Endangered Species Act and is not listed by the USFWS.
The IUCN Antelope Specialist Group considers the western or lowland bongo, "T. e. eurycerus", to be Lower Risk (Near Threatened), and the eastern or mountain bongo, "T. e. isaaci", of Kenya, to be Critically Endangered. Other subspecific names have been used, but their validity has not been tested.

</doc>
<doc id="4507" url="http://en.wikipedia.org/wiki?curid=4507" title="Bunyip">
Bunyip

The bunyip, or kianpraty, is a large mythical creature from Aboriginal mythology, said to lurk in swamps, billabongs, creeks, riverbeds, and waterholes.
The origin of the word "bunyip" has been traced to the Wemba-Wemba or Wergaia language of Aboriginal people of South-Eastern Australia. However, the bunyip appears to have formed part of traditional Aboriginal beliefs and stories throughout Australia, although its name varied according to tribal nomenclature. In his 2001 book, writer Robert Holden identified at least nine regional variations for the creature known as the bunyip across Aboriginal Australia. Various written accounts of bunyips were made by Europeans in the early and mid-19th century, as settlement spread across the country.
Meaning.
The word "bunyip" is usually translated by Aboriginal Australians today as "devil" or "evil spirit". However, this translation may not accurately represent the role of the bunyip in pre-contact Aboriginal mythology or its possible origins before written accounts were made. Some modern sources allude to a linguistic connection between the bunyip and Bunjil, "a mythic 'Great Man' who made the mountains and rivers and man and all the animals." The word "bunyip" may not have appeared in print in English until the mid-1840s.
By the 1850s, "bunyip" had also become a "synonym for impostor, pretender, humbug and the like" in the broader Australian community. The term "bunyip aristocracy" was first coined in 1853 to describe Australians aspiring to be aristocrats. In the early 1990s, it was famously used by Prime Minister Paul Keating to describe members of the conservative Liberal Party of Australia opposition.
The word "bunyip" can still be found in a number of Australian contexts, including place names such as the Bunyip River (which flows into Westernport Bay in southern Victoria) and the town of Bunyip, Victoria.
Characteristics.
Descriptions of bunyips vary widely. George French Angus may have collected a description of a bunyip in his account of a "water spirit" from the Moorundi people of the Murray River before 1847, stating it is "much dreaded by them ... It inhabits the Murray; but ... they have some difficulty describing it. Its most usual form ... is said to be that of an enormous starfish." Robert Brough Smyth's "Aborigines of Victoria" of 1878 devoted ten pages to the bunyip, but concluded "in truth little is known among the blacks respecting its form, covering or habits; they appear to have been in such dread of it as to have been unable to take note of its characteristics." However, common features in many 19th-century newspaper accounts include a dog-like face, a crocodile like head, dark fur, a horse-like tail, flippers, and walrus-like tusks or horns or a duck-like bill.
The Challicum bunyip, an outline image of a bunyip carved by Aborigines into the bank of Fiery Creek, near Ararat, Victoria, was first recorded by "The Australasian" newspaper in 1851. According to the report, the bunyip had been speared after killing an Aboriginal man. Antiquarian Reynell Johns claimed that until the mid-1850s, Aboriginal people made a "habit of visiting the place annually and retracing the outlines of the figure [of the bunyip] which is about 11 paces long and 4 paces in extreme breadth." The outline image no longer exists.
Debate over origins of the bunyip.
Non-Aboriginal Australians have made various attempts to understand and explain the origins of the bunyip as a physical entity over the past 150 years.
Writing in 1933, Charles Fenner suggested that it was likely that the "actual origin of the bunyip myth lies in the fact that from time to time seals have made their way up the ... Murray and Darling (Rivers)". He provided examples of seals found as far inland as Overland Corner, Loxton, and Conargo and reminded readers that "the smooth fur, prominent 'apricot' eyes and the bellowing cry are characteristic of the seal."
Another suggestion is that the bunyip may be a cultural memory of extinct Australian marsupials such as the "Diprotodon", "Zygomaturus", "Nototherium" or "Palorchestes". This connection was first formally made by Dr George Bennett of the Australian Museum in 1871, but in the early 1990s, palaeontologist Pat Vickers-Rich and geologist Neil Archbold also cautiously suggested that Aboriginal legends "perhaps had stemmed from an acquaintance with prehistoric bones or even living prehistoric animals themselves ... When confronted with the remains of some of the now extinct Australian marsupials, Aborigines would often identify them as the bunyip." They also note that "legends about the" mihirung paringmal" of western Victorian Aborigines ... may allude to the ... extinct giant birds the Dromornithidae."
Another connection to the bunyip is the shy Australasian bittern ("Botaurus poiciloptilus"). During the breeding season, the male call of this marsh-dwelling bird is a "low pitched boom"; hence, it is occasionally called the "bunyip bird".
Early accounts of settlers.
During the early settlement of Australia by Europeans, the notion that the bunyip was an actual unknown animal that awaited discovery became common. Early European settlers, unfamiliar with the sights and sounds of the island continent's peculiar fauna, regarded the bunyip as one more strange Australian animal and sometimes attributed unfamiliar animal calls or cries to it. It has also been suggested that 19th-century bunyip lore was reinforced by imported European memories, such as that of the Irish Púca.
A large number of bunyip sightings occurred during the 1840s and 1850s, particularly in the southeastern colonies of Victoria, New South Wales and South Australia, as European settlers extended their reach. The following is not an exhaustive list of accounts:
Hume find of 1818.
One of the earliest accounts relating to a large unknown freshwater animal was in 1818, when Hamilton Hume and James Meehan found some large bones at Lake Bathurst in New South Wales. They did not call the animal a bunyip, but described the remains indicating the creature as very much like a hippopotamus or manatee. The Philosophical Society of Australasia later offered to reimburse Hume for any costs incurred in recovering a specimen of the unknown animal, but for various reasons, Hume did not return to the lake. It might be noted that Diprotodon skeletons have sometimes been compared to the hippopotamus; they are a land animal, but have sometimes been found in a lake or water course.
Wellington Caves fossils, 1830.
More significant was the discovery of fossilised bones of "some quadruped much larger than the ox or buffalo" in the Wellington Caves in mid-1830 by bushman George Rankin and later by Thomas Mitchell. Sydney's Reverend John Dunmore Lang announced the find as "convincing proof of the deluge". However, it was British anatomist Sir Richard Owen who identified the fossils as the gigantic marsupials "Nototherium" and "Diprotodon". At the same time, some settlers observed "all natives throughout these ... districts have a tradition (of) a very large animal having at one time existed in the large creeks and rivers and by many it is said that such animals now exist."
First written use of the word "bunyip", 1845.
In July 1845, "The Geelong Advertiser" announced the discovery of fossils found near Geelong, under the headline "Wonderful Discovery of a new Animal". This was a continuation of a story on 'fossil remains' from the previous issue. The newspaper continued, "On the bone being shown to an intelligent black (sic), he at once recognised it as belonging to the bunyip, which he declared he had seen. On being requested to make a drawing of it, he did so without hesitation." The account noted a story of an Aboriginal woman being killed by a bunyip and the "most direct evidence of all" – that of a man named Mumbowran "who showed several deep wounds on his breast made by the claws of the animal". The account provided this description of the creature:
Shortly after this account appeared, it was repeated in other Australian newspapers. However, it appears to be the first use of the word "bunyip" in a written publication.
The Australian Museum's bunyip of 1847.
In January 1846, a peculiar skull was taken from the banks of Murrumbidgee River near Balranald, New South Wales. Initial reports suggested that it was the skull of something unknown to science. The squatter who found it remarked, "all the natives to whom it was shown called [it] a bunyip". By July 1847, several experts, including W. S. Macleay and Professor Owen, had identified the skull as the deformed foetal skull of a foal or calf. At the same time, however, the purported bunyip skull was put on display in the Australian Museum (Sydney) for two days. Visitors flocked to see it, and "The Sydney Morning Herald" said that it prompted many people to speak out about their "bunyip sightings". Reports of this discovery used the phrase 'Kine Pratie' as well as Bunyip and explorer William Hovell, who examined the skull, also called it a 'katen-pai'.
In March of that year "a bunyip or an immense Platibus" (Platypus) was sighted "sunning himself on the placid bosom of the Yarra, just opposite the Custom House" in Melbourne. "Immeadiately a crowd gathered" and three men set off by boat "to secure the stranger" who "disappeared" when they were "about a yard from him".
William Buckley's account of bunyips, 1852.
Another early written account is attributed to escaped convict William Buckley in his 1852 biography of thirty years living with the Wathaurong people. His 1852 account records "in ... Lake Moodewarri [now Lake Modewarre] as well as in most of the others inland ... is a ... very extraordinary amphibious animal, which the natives call Bunyip." Buckley's account suggests he saw such a creature on several occasions. He adds, "I could never see any part, except the back, which appeared to be covered with feathers of a dusky grey colour. It seemed to be about the size of a full grown calf ... I could never learn from any of the natives that they had seen either the head or tail." Buckley also claimed the creature was common in the Barwon River and cites an example he heard of an Aboriginal woman being killed by one. He emphasized the bunyip was believed to have supernatural powers.
In popular culture and fiction.
The word "bunyip" has been used in other Australian contexts, including "The Bunyip" newspaper as the banner of a local weekly newspaper published in the town of Gawler, South Australia. First published as a pamphlet by the Gawler Humbug Society in 1863, the name was chosen because "the Bunyip is the true type of Australian Humbug!" The word is also used in numerous other Australian contexts, including the House of the Gentle Bunyip in Clifton Hill, Victoria. 
Numerous tales of the bunyip in written literature appeared in the 19th and early 20th centuries. One of the earliest known is a story in Andrew Lang's "The Brown Fairy Book" (1904).
Alexander Bunyip, created by children's author and illustrator Michael Salmon, first appeared in print in "The Monster That Ate Canberra" in 1972, Alexander Bunyip went on to appear in many other books and a live-action television series, "Alexander Bunyip's Billabong". A statue of Alexander was opened in front of the Gungahlin Library in 2011. The artwork by Anne Ross, called 'A is for Alexander, B is for Bunyip, C is for Canberra' was commissioned by the ACT Government for Gungahlin’s $3.8 million town park.
The Australian tourism boom of the 1970s brought a renewed interest in bunyip mythology.
Bunyip stories have also appeared outside of Australia.
In the 21st century the bunyip can be considered part of the international consciousness.

</doc>
<doc id="4508" url="http://en.wikipedia.org/wiki?curid=4508" title="Brabant">
Brabant

Brabant may refer to:
Historically:
In contemporary usage:
In Belgium:
In the Netherlands:
In France:
Outside Europe:
Other uses:

</doc>
<doc id="4512" url="http://en.wikipedia.org/wiki?curid=4512" title="Boone, North Carolina">
Boone, North Carolina

Boone is a town located in the Blue Ridge Mountains of western North Carolina, United States. Boone's population was 17,122 in 2010. Boone is the county seat of Watauga County and the home of Appalachian State University.
The town is named for famous American pioneer and explorer Daniel Boone, and every summer since 1952 has hosted an outdoor amphitheatre portrayal of the life and times of its namesake.
In 2012, Boone was listed among the 10 best places to retire in the U.S. by "U.S. News".
Boone.
 Boone took its name from the famous pioneer and explorer Daniel Boone, who on several occasions camped at a site generally agreed to be within the present city limits. Daniel's nephews, Jesse and Jonathan (sons of brother Israel Boone), were founders of the town's first church, Three Forks Baptist, still in existence today.
Boone was served by the narrow gauge East Tennessee and Western North Carolina Railroad (nicknamed "Tweetsie") until the flood of 1940. The flood washed away much of the tracks and it was decided not to replace them.
Boone is the home of Appalachian State University, a constituent member of the University of North Carolina.
Appalachian State is the sixth largest university in the seventeen-campus system.
Caldwell Community College & Technical Institute also operates a satellite campus in Boone.
"Horn in the West", a dramatization of the life and times of the early settlers of the mountain area, which features Daniel Boone as one of its characters, has been performed in an outdoor amphitheatre above the town every summer since 1952.
The original actor in the role of "Daniel Boone" was Ned Austin. His "Hollywood Star" stands on a pedestal on King Street in downtown Boone. He was followed in the role by Glenn Causey, who portrayed the rugged frontiersman for 41 years, and whose image is still seen in many of the depictions of Boone featured in the area today.
The late guitarist Michael Houser was born in Boone.
He is best known as a founding member of and lead guitarist for the band Widespread Panic.
The late, Grammy Award-winning guitar player Doc Watson also came from the Boone area, as do many bluegrass musicians and Appalachian storytellers.
Geography and climate.
Boone is located at (36.211364, −81.668657) and has an elevation of 3,333 feet (1015.9 m) above mean sea level. An earlier survey gave the elevation as 3,332 ft and since then it has been published as having an elevation of 3,333 ft (1,016 m). Boone has the highest elevation of any town of its size (over 10,000 population) east of the Mississippi River. As such, Boone features, depending on the isotherm used, a subtropical highland climate (Köppen "Cfb"), or a humid continental climate (Köppen "Dfb"), a rarity for the Southeastern United States, and straddles the boundary between USDA Plant Hardiness Zones 6B and 7A; the elevation also results in enhanced precipitation, with of average annual precipitation. Compared to the lower elevations of the Carolinas, winters are long and cold, with frequent sleet and snowfall. The daily average temperature in January is , which gives Boone a winter climate more similar to coastal southern New England rather than the Southeast, where a humid subtropical climate dominates. Blizzard-like conditions are not unusual during many winters. Summers are warm, but far cooler and less humid than lower regions to the south and east, with a July daily average temperature of . Boone typically receives on average nearly of snowfall annually, far higher than the lowland areas in the rest of North Carolina.
Demographics.
As of the census of 2000, there were 13,472 people, 4,374 households, and 1,237 families residing in the town. The population density was 2,307.0 people per square mile (890.7/km²). There were 4,748 housing units at an average density of 813.0 per square mile (313.9/km²). The racial makeup of the town was 93.98% White, 3.42% Black or African American, 0.30% Native American, 1.19% Asian, 0.05% Pacific Islander, 0.46% from other races, and 0.60% from two or more races. 1.64% of the population were Hispanic or Latino of any race.
There were 4,374 households out of which 9.6% had children under the age of 18 living with them, 21.0% were married couples living together, 5.6% had a female householder with no husband present, and 71.7% were non-families. 38.4% of all households were made up of individuals and 7.3% had someone living alone who was 65 years of age or older. The average household size was 1.97 and the average family size was 2.63.
The age distribution is 5.8% under 18, 65.9% from 18 to 24, 12.1% from 25 to 44, 9.1% from 45 to 64, and 7.1% who were 65 or older. The median age was 21 years. Both the overall age distribution and the median age are driven by the presence of the local university, Appalachian State. For every 100 females there are 95.6 males. For every 100 females age 18 and over, there were 94.7 males.
The median household income is $20,541, and the median family income is $49,762. The per capita income is $12,256. 37.0% of the population and 9.2% of families were below the poverty line.
Men had a median income of $28,060 versus $20,000 for women. However, poverty statistics that are based on surveys of the entire population can be extremely misleading in communities dominated by students, such as Boone. Out of the total population, 6.3% of those under the age of 18 and 9.1% of those 65 and older were living below the poverty line.
Media.
Newspaper.
Boone is mainly served by three local newspapers:
A smaller newspaper, The Appalachian, is Appalachian State University's campus newspaper published twice a week on Tuesdays and Thursdays. In addition to the locally printed papers, a monthly entertainment pamphlet named Kraut Creek Revival has limited circulation and is funded by a Denver, NC-based newspaper.
Law and government.
Boone operates under a mayor-council government. The city council consists of five members. The mayor presides over the council and casts a vote on issues only in the event of a tie. As of December 2013, the Town Council members are: Andy Ball, Mayor; Rennie Brantz, Mayor Pro Tem, and Councilors: Lynne Mason, Fred Hay, Jennifer Peña, and Quint David.
Development.
Industrial, commercial, and residential development in the town of Boone is a controversial issue due to its location in the mountains of Appalachia. On October 16, 2009, the town council accepted the "Boone 2030 Land Use Plan." While the document itself is not in any way actual law, it is used by the town council, board of adjustment, and other committees to guide decision making as to what types of development are appropriate.
In 2009, the North Carolina Department of Transportation began widening 1.1 miles of U.S. 421 (King Street) to a four to six-lane divided highway with a raised concrete median from U.S. 321 (Hardin Street) to east of N.C. 194 (Jefferson Road), including a new entrance and exit to the new Watauga High School, at a cost of $16.2 million. The widening has displaced 25 businesses and 63 residences east of historic downtown King Street. The project was slated to be completed by December 31, 2011; as of 16 March 2012, construction is ongoing.
Economy.
Samaritan's Purse is based in Boone.
Largest Employers.
According to the Town's 2011 Comprehensive Annual Financial Report, the top employers in the city are:

</doc>
<doc id="4513" url="http://en.wikipedia.org/wiki?curid=4513" title="Banshee">
Banshee

The banshee (or banchee) ( ), from ("woman of the barrows") is a female spirit in Irish mythology, usually seen as an omen of death and a messenger from the underworld.
In legend, a banshee is a fairy woman who begins to wail if someone is about to die. In Scottish Gaelic mythology, she is known as the bean sìth or bean nighe and is seen washing the bloodstained clothes or armour of those who are about to die. Alleged sightings of banshees have been reported as recently as 1948. Similar beings are also found in Welsh, Norse and American folklore.
History and mythology.
In legend, a banshee wails nearby when someone is about to die. There are Irish families who are believed to have banshees attached to them, and whose cries herald the death of a member of that family. Most, though not all, surnames associated with banshees have the "Ó" or "Mac" prefix, indicating their name is native to Ireland, not descended from invaders. They were also associated with the Airlie clan. Accounts of banshees go back as far as 1380 with the publication of the "Cathreim Thoirdhealbhaigh" ("Triumphs of Torlough") by Sean mac Craith. Mentions of banshees can also be found in Norman literature of that time.
Traditionally, when a person died a woman would sing a lament (in , or , "caoin" meaning "to weep, to wail") at the funeral. These women are referred to as "keeners" and the best keeners would be in high demand. Legend has it that for great Gaelic families – the O'Gradys, the O'Neills, the Ó Longs, the McCnaimhíns, the Ó Briains, the Ó Conchobhairs, and the Caomhánachs – the lament would be sung by a fairy woman; having foresight, she would sing it when a family member died, even if the person had died far away and news of their death had not yet come, so that the wailing of the banshee was the first warning the household had of the death.
The Ó Briains' banshee was thought to have the name of Eevul, and was ruler of 25 other banshees who would always be at her attendance. It is thought that from this myth comes the idea that the wailing of numerous banshees signifies the death of a great person.
In later versions, the banshee might appear before the death and warn the family by wailing. When several banshees appeared at once, it indicated the death of someone great or holy. The tales sometimes recounted that the woman, though called a fairy, was a ghost, often of a specific murdered woman, or a mother who died in childbirth.
Banshees are frequently described as dressed in white or grey, often having long, pale hair which they brush with a silver comb, a detail scholar Patricia Lysaght attributes to confusion with local mermaid myths. This comb detail is also related to the centuries-old traditional romantic Irish story that, if you ever see a comb lying on the ground in Ireland, you must never pick it up, or the banshees (or mermaids – stories vary), having placed it there to lure unsuspecting humans, will spirit such gullible humans away. Other stories portray banshees as dressed in green, red, or black with a grey cloak.
One explanation for the origin of the banshee is in the screech of the Barn owl (Tyto alba). The nocturnal hunter is known for its chilling screech and has long been associated with agricultural activities in Ireland, attracted to the rodent activity around grain stores and barns.
The banshee can appear in a variety of guises. Most often she appears as an ugly, frightening hag, but she can also appear as a stunningly beautiful woman of any age that suits her. In some tales, the figure who first appears to be a banshee or other hag is later revealed to be the Irish battle goddess, the Morrígan.
The banshee may also appear in a variety of other forms, such as that of a hooded crow, stoat, hare and weasel – animals associated in Ireland with witchcraft.
In 1437, King James I of Scotland was approached by an Irish seer who was later identified as a banshee who foretold his murder at the instigation of the Earl of Atholl. There are records of several prophets believed to be incarnate banshees attending the great houses of Ireland and the courts of local Irish kings.
In some parts of Leinster, she is referred to as the "bean chaointe" (keening woman) whose wail can be so piercing that it shatters glass. In Kerry in the southwest of Ireland, her keen is experienced as a "low, pleasant singing"; in Tyrone in the north, as "the sound of two boards being struck together"; and, on Rathlin Island, as "a thin, screeching sound somewhere between the wail of a woman and the moan of an owl".
In other mythologies.
American folklore.
Stories of banshees can also be found in America in the late 18th century. The most prevalent of the American stories comes from Tar River in Edgecombe County, North Carolina. However, in this variation of the story, the banshee is simply a ghoul, as opposed to a sign of misfortune.
In the badlands of South Dakota, a banshee is said to wail upon a hill near Watch Dog Butte. Like other American tales of banshees, this legend does not connect her to any particular death (aside, perhaps, from her own).
Other Celtic cultures.
In Scottish mythology, a similar creature is known as the bean nighe or "ban nigheachain" (little washerwoman) or "nigheag na h-àth" (little washer at the ford). In Welsh folklore, a similar creature is known as the Hag of the mist.
In popular culture.
See Banshee in popular culture

</doc>
<doc id="4514" url="http://en.wikipedia.org/wiki?curid=4514" title="Genetically modified maize">
Genetically modified maize

Genetically modified maize (corn) is a genetically modified crop. Specific maize strains have been genetically engineered to express agriculturally-desirable traits, including resistance to pests and to herbicides. Maize strains with both traits are now in use in multiple countries. GM maize has also caused controversy with respect to possible health effects, impact on other insects and impact on other plants via gene flow. One strain, called Starlink, was approved only for animal feed in the US, but was found in food, leading to a series of recalls starting in 2000.
Marketed Products.
Herbicide resistant maize.
Corn varieties resistant to glyphosate herbicides were first commercialized in 1996 by Monsanto, and are known as "Roundup Ready Corn". They tolerate the use of Roundup. Bayer CropScience developed "Liberty Link Corn" that is resistant to glufosinate. Pioneer Hi-Bred has developed and markets corn hybrids with tolerance to imidazoline herbicides under the trademark "Clearfield" - though in these hybrids, the herbicide-tolerance trait was bred using tissue culture selection and the chemical mutagen ethyl methanesulfonate not genetic engineering. Consequently, the regulatory framework governing the approval of transgenic crops does not apply for Clearfield.
As of 2011, herbicide-resistant GM corn was grown in 14 countries. By 2012, 26 varieties herbicide-resistant GM maize were authorised for import into the European Union. In 2012 the EU was reported to import 30 million tons a year of GM crops, but such imports remain controversial. Cultivation of herbicide-resistant corn in the EU provides substantial farm-level benefits.
Insecticide-producing corn.
Bt corn is a variant of maize that has been genetically altered to express one or more proteins from the Bacillus thuringiensis bacteria. The protein is poisonous to certain insect pests and is widely used in organic gardening. The European corn borer causes about a billion dollars in damage to corn crops each year.
In recent years, traits have been added to ward off Corn ear worms and root worms, the latter of which annually causes about a billion dollars in damages.
The Bt protein is expressed throughout the plant. When a vulnerable insect eats the Bt-containing plant, the protein is activated in its gut, which is alkaline (the human gut is acidic). In the alkaline environment the protein partially unfolds and is cut by other proteins, forming a toxin that paralyzes the insect's digestive system and forms holes in the gut wall. The insect stops eating within a few hours and eventually starves.
In 1996, the first GM maize producing a Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.
Bt genes that are approved include the following, singly and stacked (event name between brackets): Cry1A.105 (MON89034), CryIAb (MON810), CryIF (1507), Cry2Ab (MON89034), Cry3Bb1 (MON863 and MON88017), Cry34Ab1 (59122), Cry35Ab1 (59122), mCry3A (MIR604), and Vip3A (MIR162), in both corn and cotton. Corn genetically modified to produce VIP was first approved in the US in 2010.
Drought resistance.
In 2013 Monsanto launched the first transgenic drought tolerance trait in a line of corn hybrids called DroughtGard. The MON 87460 trait is provided by the insertion of the cspB gene from the soil microbe "Bacillus subtilis"; it was approved by the USDA in 2011 and by China in 2013.
Sweet corn.
GM sweet corn varieties include "Attribute", the brand name for insect-resistant sweet corn developed by Syngenta.
Products in development.
In 2007, South African researchers announced the production of transgenic maize resistant to maize streak virus (MSV), although it has not been released as a product.
Refuges.
US Environmental Protection Agency (EPA) regulations require farmers who plant Bt corn to plant non-Bt corn nearby (called a "refuge") to provide a location to harbor vulnerable pests.
The theory behind these refuges is to slow the evolution of resistance to the pesticide. EPA regulations also require seed companies to train farmers how to maintain refuges, to collect data on the refuges and to report that data to the EPA. A study of these reports found that from 2003 to 2005 farmer compliance with keeping refuges was above 90%, but that by 2008 approximately 25% of Bt corn farmers did not keep refuges properly, raising concerns that resistance would develop.
Unmodified crops received most of the economic benefits of Bt corn in the US in 1996-2007, because of the overall reduction of pest populations. This reduction came because females laid eggs on modified and unmodified strains alike.
Resistance.
Resistant strains of the European corn borer have developed in areas with defective or absent refuge management.
In November 2009, Monsanto scientists found the pink bollworm had become resistant to first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, "Cry1Ac". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Bollworm resistance to first generation Bt cotton has been identified in the Australia, China, Spain and the United States. In 2012, a Florida field trial demonstrated that army worms were resistant to pesticide-containing GM corn produced by Dupont-Dow; armyworm resistance was first discovered in Puerto Rico in 2006, prompting Dow and DuPont to voluntarily stop selling the product on the island.
Regulation.
Regulation of GM crops varies between countries, with some of the most-marked differences occurring between the USA and Europe. Regulation varies in a given country depending on intended uses.
Controversy.
Broad scientific consensus holds that food derived from GM crops poses no greater risk to human health than conventional food. The scientific rigor of the studies regarding human health has been disputed due to alleged lack of independence and due to conflicts of interest involving governing bodies and some of those who perform and evaluate the studies.
GM crops provide a number of ecological benefits, but there are also concerns for their overuse, stalled research outside of the Bt seed industry, proper management and issues with Bt resistance arising from their misuse.
Critics have objected to GM crops on ecological, economic and health grounds. The economic issues derive from those organisms that are subject to intellectual property law, mostly patents. The first generation of GM crops lose patent protection beginning in 2015. Monsanto has claimed it will not to pursue farmers who retain seeds of off-patent varieties. These controversies have led to litigation, international trade disputes, protests and to restrictive legislation in most countries.
Effects on nontarget insects.
Critics claim that Bt proteins could target predatory and other beneficial or harmless insects as well as the targeted pest. These proteins have been used as organic sprays for insect control in France since 1938 and the USA since 1958 with no ill effects on the environment reported. While "cyt" proteins are toxic towards the insect orders Coleoptera (beetles) and Diptera (flies), "cry" proteins selectively target Lepidopterans (moths and butterflies). As a toxic mechanism, "cry" proteins bind to specific receptors on the membranes of mid-gut (epithelial) cells, resulting in rupture of those cells. Any organism that lacks the appropriate gut receptors cannot be affected by the "cry" protein, and therefore Bt. Regulatory agencies assess the potential for the transgenic plant to impact nontarget organisms before approving commercial release.
A 1999 study found that in a lab environment, pollen from Bt maize dusted onto milkweed could harm the monarch butterfly. Several groups later studied the phenomenon in both the field and the laboratory, resulting in a risk assessment that concluded that any risk posed by the corn to butterfly populations under real-world conditions was negligible. A 2002 review of the scientific literature concluded that "the commercial large-scale cultivation of current Bt–maize hybrids did not pose a significant risk to the monarch population". A 2007 review found that "nontarget invertebrates are generally more abundant in Bt cotton and Bt maize fields than in nontransgenic fields managed with insecticides. However, in comparison with insecticide-free control fields, certain nontarget taxa are less abundant in Bt fields."
Gene flow.
Gene flow is the transfer of genes and/or alleles from one species to another. Concerns focus on the interaction between GM and other maize varieties in Mexico, and of gene flow into refuges.
In 2009 the government of Mexico created a regulatory pathway for genetically modified maize, but because Mexico is the center of diversity for maize, gene flow could affect a large fraction of the world's maize strains. A 2001 report in "Nature" presented evidence that Bt maize was cross-breeding with unmodified maize in Mexico. The data in this paper was later described as originating from an artifact. "Nature" later stated, "the evidence available is not sufficient to justify the publication of the original paper". A 2005 large-scale study failed to find any evidence of contamination in Oaxaca. However, other authors also found evidence of cross-breeding between natural maize and transgenic maize.
A 2004 study found Bt protein in kernels of refuge corn.
Food.
The French High Council of Biotechnologies Scientific Committee reviewed the 2009 Vendômois "et al." study and concluded that it "..presents no admissible scientific element likely to ascribe any haematological, hepatic or renal toxicity to the three re-analysed GMOs." However, the French government applies the precautionary principle with respect to GMOs.
A review by Food Standards Australia New Zealand and others of the same study concluded that the results were due to chance alone.
A 2011 Canadian study looked at the presence of CryAb1 protein (BT toxin) in non-pregnant women, pregnant women and fetal blood. All groups had detectable levels of the protein, including 93% of pregnant women and 80% of fetuses at concentrations of 0.19 ± 0.30 and 0.04 ± 0.04 mean ± SD ng/ml, respectively. The paper did not discuss safety implications or find any health problems. The paper was found to be unconvincing by multiple authors and organizations. In a swine model, Cry1Ab-specific antibodies were not detected in pregnant sows or their offspring and no negative effects from feeding Bt maize to pregnant sows were observed.
In January 2013, the European Food Safety Authority released all data submitted by Monsanto in relation to the 2003 authorisation of maize genetically modified for glyphosate tolerance.
Starlink corn recalls.
StarLink contains Cry9C, which had not previously been used in a GM crop. Starlink's creator, Plant Genetic Systems had applied to the US Environmental Protection Agency (EPA) to market Starlink for use in animal feed and in human food. However, because the Cry9C protein lasts longer in the digestive system than other Bt proteins, the EPA had concerns about its allergenicity, and PGS did not provide sufficient data to prove that Cry9C was not allergenic. As a result PGS split its application into separate permits for use in food and use in animal feed. Starlink was approved by the EPA for use in animal feed only in May 1998.
StarLink corn was subsequently found in food destined for consumption by humans in the US, Japan, and South Korea. This corn became the subject of the widely publicized Starlink corn recall, which started when Taco Bell-branded taco shells sold in supermarkets were found to contain the corn. Sales of StarLink seed were discontinued. The registration for Starlink varieties was voluntarily withdrawn by Aventis in October 2000. (Pioneer had been bought by AgrEvo which then became Aventis CropScience at the time of the incident, which was later bought by Bayer
Fifty-one people reported adverse effects to the FDA; US Centers for Disease Control (CDC), which determined that 28 of them were possibly related to Starlink. However, the CDC studied the blood of these 28 individuals and concluded there was no evidence of hypersensitivity to the Starlink Bt protein.
A subsequent review of these tests by the Federal Insecticide, Fungicide, and Rodenticide Act Scientific Advisory Panel points out that while "the negative results decrease the probability that the Cry9C protein is the cause of allergic symptoms in the individuals examined ... in the absence of a positive control and questions regarding the sensitivity and specificity of the assay, it is not possible to assign a negative predictive value to this."
The US corn supply has been monitored for the presence of the Starlink Bt proteins since 2001.
In 2005, aid sent by the UN and the US to Central American nations also contained some StarLink corn. The nations involved, Nicaragua, Honduras, El Salvador and Guatemala refused to accept the aid.
Corporate Espionage.
On December 19, 2013 six Chinese citizens were indicted in Iowa on charges of plotting to steal genetically modified seeds worth tens of millions of dollars to Monsanto and DuPont. Mo Hailong, director of international business at the Beijing Dabeinong Technology Group Co., part of the Beijing-based DBN Group, was accused of stealing trade secrets after he was found digging in an Iowa cornfield.

</doc>
<doc id="4516" url="http://en.wikipedia.org/wiki?curid=4516" title="Body substance isolation">
Body substance isolation

Body substance isolation is a practice of isolating all body substances (blood, urine, feces, tears, etc.) of individuals undergoing medical treatment, particularly emergency medical treatment of those who might be infected with illnesses such as HIV, or hepatitis so as to reduce as much as possible the chances of transmitting these illnesses. BSI is similar in nature to universal precautions, but goes further in isolating workers from pathogens, including substances now known to carry HIV.
Place of body substance isolation practice in history.
Practice of Universal precautions was introduced in 1985–88. In 1987, the practice of Universal precautions was adjusted by a set of rules known as body substance isolation. In 1996, both practices were replaced by the latest approach known as standard precautions (health care). Nowadays and in isolation, practice of body substance isolation has just historical significance.
Body substance isolation went further than universal precautions in isolating workers from pathogens, including substances now currently known to carry HIV. These pathogens fall into two broad categories, bloodborne (carried in the body fluids) and airborne. The practice of BSI was common in Pre-Hospital care and Emergency Medical Services due to the often unknown nature of the patient and his/her disease or medical conditions. It was a part of the National Standards Curriculum for Prehospital Providers and Firefighters.
Types of body substance isolation included:
It was postulated that BSI precautions should be practiced in environment where treaters were exposed to bodily fluids, such as:
Such infection control techniques that were recommended following the AIDS outbreak in the 1980s. Every patient was treated as if infected and therefore precautions were taken to minimize risk. Other conditions which called for minimizing risks with BSI:
or any combination of the above.
Further reading.
Epidemiology

</doc>
<doc id="4517" url="http://en.wikipedia.org/wiki?curid=4517" title="Boudica">
Boudica

Boudica (; alternative spelling: Boudicca), also known as Boadicea , and known in Welsh as Buddug (d. AD 60 or 61) was queen of the British Iceni tribe, a Celtic tribe who led an uprising against the occupying forces of the Roman Empire.
Boudica's husband Prasutagus was ruler of the Iceni tribe. He ruled as a nominally independent ally of Rome and left his kingdom jointly to his daughters and the Roman emperor in his will. However, when he died, his will was ignored and the kingdom was annexed as if conquered. Boudica was flogged, her daughters were raped, and Roman financiers called in their loans.
In AD 60 or 61, while the Roman governor Gaius Suetonius Paulinus was leading a campaign on the island of Anglesey off the northwest coast of Wales, Boudica led the Iceni as well as the Trinovantes and others in revolt. They destroyed Camulodunum (modern Colchester). Camulodunum was earlier the capital of the Trinovantes, but at that time was a "colonia"—a settlement for discharged Roman soldiers, as well as the site of a temple to the former Emperor Claudius. Upon hearing the news of the revolt, Suetonius hurried to Londinium (modern London), the twenty-year-old commercial settlement that was the rebels' next target.
The Romans, having concluded that they did not have the numbers to defend the settlement, evacuated and abandoned Londinium. Boudica led 100,000 Iceni, Trinovantes and others to fight the Legio IX Hispana and burned and destroyed Londinium, and Verulamium (modern-day St Albans). An estimated 70,000–80,000 Romans and British were killed in the three cities by those led by Boudica. Suetonius, meanwhile, regrouped his forces in the West Midlands, and despite being heavily outnumbered, defeated the Britons in the Battle of Watling Street.
The crisis caused the Emperor Nero to consider withdrawing all Roman forces from Britain, but Suetonius's eventual victory over Boudica confirmed Roman control of the province. Boudica then either killed herself so she would not be captured, or fell ill and died. The extant sources, Tacitus and Cassius Dio, differ.
Interest in the history of these events was revived during the English Renaissance and led to a resurgence of Boudica's fame during the Victorian era, and Queen Victoria was portrayed as her namesake.Boudica has since remained an important cultural symbol in the United Kingdom. However, the absence of native British literature during the early part of the first millennium means that knowledge of Boudica's rebellion comes solely from the writings of the Romans.
History.
Boudica's name.
Boudica has been known by several versions of her name. Raphael Holinshed calls her Voadicia, while Edmund Spenser calls her Bunduca, a version of the name that was used in the popular Jacobean play "Bonduca", in 1612. William Cowper's poem, "Boadicea, an ode" (1782) popularised an alternate version of the name. From the 19th century and much of the late 20th century, "Boadicea" was the most common version of the name, which is probably derived from a mistranscription when a manuscript of Tacitus was copied in the Middle Ages.
Her name was clearly spelled Boudicca in the best manuscripts of Tacitus, but also "Βουδουικα", "Βουνδουικα", and "Βοδουικα" in the (later and probably secondary) epitome of Cassius Dio. The name is attested in inscriptions as "Boudica" in Lusitania, Boudiga in Bordeaux, and Bodicca in Algeria.
Kenneth Jackson concludes, based on later development of Welsh and Irish, that the name derives from the Proto-Celtic feminine adjective "*boudīka", "victorious", that in turn is derived from the Celtic word "*bouda", "victory" (cf. Irish "bua" (Classical Irish "buadh"), "Buaidheach", Welsh "buddugoliaeth"), and that the correct spelling of the name in the British language is "Boudica", pronounced .
The closest English equivalent to the vowel in the first syllable is the "ow" in "bow-and-arrow"). The modern English pronunciation is , and it has been suggested that the most comparable English name, in meaning only, would be "Victoria".
Background.
Tacitus and Dio agree that Boudica was of royal descent. Dio says that she was "possessed of greater intelligence than often belongs to women", that she was tall and had hair described as red, reddish-brown, or tawny hanging below her waist. Dio also says she had a harsh voice and piercing glare, and habitually wore a large golden necklace (perhaps a torc), a many-coloured tunic, and a thick cloak fastened by a brooch.
Her husband Prasutagus was the king of the Iceni, the people who inhabited roughly what is now Norfolk. They initially were not part of the territory under direct Roman control, having voluntarily allied themselves to Rome following Claudius' conquest of AD 43. They were proud of their independence, and had revolted in AD 47 when the then-governor Publius Ostorius Scapula threatened to disarm them. Prasutagus had lived a long life of conspicuous wealth and, hoping to preserve his line, made the Roman emperor co-heir to his kingdom, along with his wife and two daughters.
It was normal Roman practice to allow allied kingdoms their independence only for the lifetime of their client king, who would then agree to leave his kingdom to Rome in his will. For example, the provinces of Bithynia and Galatia, were incorporated into the Empire in just this way. Roman law also allowed inheritance only through the male line, so when Prasutagus died, his attempts to preserve his line were ignored and his kingdom was annexed as if it had been conquered. His lands and property were confiscated and nobles treated like slaves. According to Tacitus, Boudica was flogged and her daughters were raped. Cassius Dio says that Roman financiers, including Seneca the Younger, chose this time to call in their loans. Tacitus does not mention this, but does single out the Roman procurator Catus Decianus for criticism for his "avarice". Prasutagus, it seems, had lived well on borrowed Roman money, and on his death his subjects had become liable for the debt.
Boudica's uprising.
In AD 60 or 61, while the current governor, Gaius Suetonius Paulinus, was leading a campaign against the island of Mona (modern Anglesey) in the north of Wales, which was a refuge for British rebels and a stronghold of the druids, the Iceni conspired with their neighbours the Trinovantes, amongst others, to revolt. Boudica was chosen as their leader. According to Tacitus, they drew inspiration from the example of Arminius, the prince of the Cherusci who had driven the Romans out of Germany in AD 9, and their own ancestors who had driven Julius Caesar from Britain. Dio says that at the outset Boudica employed a form of divination, releasing a hare from the folds of her dress and interpreting the direction in which it ran, and invoked Andraste, a British goddess of victory.
The rebels' first target was Camulodunum (Colchester), the former Trinovantian capital and, at that time, a Roman "colonia". The Roman veterans who had been settled there mistreated the locals and a temple to the former emperor Claudius had been erected there at local expense, making the city a focus for resentment. The Roman inhabitants sought reinforcements from the procurator, Catus Decianus, but he sent only two hundred auxiliary troops. Boudica's army fell on the poorly defended city and destroyed it, besieging the last defenders in the temple for two days before it fell. Archaeologists have shown that the city was methodically demolished. The future governor Quintus Petillius Cerialis, then commanding the Legio IX "Hispana", attempted to relieve the city, but suffered an overwhelming defeat. His infantry was wiped out—only the commander and some of his cavalry escaped. The location of this famous destruction of the Legio IX is now claimed by some to be the village of Great Wratting, in Suffolk, which lies in the Stour Valley on the Icknield Way West of Colchester, and by a village in Essex. After this defeat, Catus Decianus fled to Gaul.
When news of the rebellion reached him, Suetonius hurried along Watling Street through hostile territory to Londinium. Londinium was a relatively new settlement, founded after the conquest of AD 43, but it had grown to be a thriving commercial centre with a population of travellers, traders, and, probably, Roman officials. Suetonius considered giving battle there, but considering his lack of numbers and chastened by Petillius's defeat, decided to sacrifice the city to save the province.
...Alarmed by this disaster and by the fury of the province which he had goaded into war by his rapacity, the procurator Catus crossed over into Gaul. Suetonius, however, with wonderful resolution, marched amidst a hostile population to Londinium, which, though undistinguished by the name of a colony, was much frequented by a number of merchants and trading vessels. Uncertain whether he should choose it as a seat of war, as he looked round on his scanty force of soldiers, and remembered with what a serious warning the rashness of Petilius had been punished, he resolved to save the province at the cost of a single town. Nor did the tears and weeping of the people, as they implored his aid, deter him from giving the signal of departure and receiving into his army all who would go with him. Those who were chained to the spot by the weakness of their sex, or the infirmity of age, or the attractions of the place, were cut off by the enemy.
Londinium was abandoned to the rebels who burnt it down, slaughtering anyone who had not evacuated with Suetonius. Archaeology shows a thick red layer of burnt debris covering coins and pottery dating before AD 60 within the bounds of Roman Londinium., whilst Roman-era skulls found in the Walbrook in 2013 were potentially linked to victims of the rebels. Verulamium (St Albans) was next to be destroyed.
In the three settlements destroyed, between seventy and eighty thousand people are said to have been killed. Tacitus says that the Britons had no interest in taking or selling prisoners, only in slaughter by gibbet, fire, or cross. Dio's account gives more detail; that the noblest women were impaled on spikes and had their breasts cut off and sewn to their mouths, "to the accompaniment of sacrifices, banquets, and wanton behaviour" in sacred places, particularly the groves of Andraste.
Romans rally.
While Boudica's army continued their assault in Verulamium (St. Albans), Suetonius regrouped his forces. According to Tacitus, he amassed a force including his own Legio XIV "Gemina", some "vexillationes" (detachments) of the XX "Valeria Victrix", and any available auxiliaries. The prefect of Legio II "Augusta", Poenius Postumus, stationed near Exeter, ignored the call, and a fourth legion, IX "Hispana", had been routed trying to relieve Camulodunum, but nonetheless the governor was able to call on almost ten thousand men.
Suetonius took a stand at an unidentified location, probably in the West Midlands somewhere along the Roman road now known as Watling Street, in a defile with a wood behind him — but his men were heavily outnumbered. Dio says that, even if they were lined up one deep, they would not have extended the length of Boudica's line. By now the rebel forces were said to have numbered 230,000, however, this number should be treated with scepticism — Dio's account is known only from a late epitome, and ancient sources commonly exaggerate enemy numbers.
Boudica exhorted her troops from her chariot, her daughters beside her. Tacitus gives her a short speech in which she presents herself not as an aristocrat avenging her lost wealth, but as an ordinary person, avenging her lost freedom, her battered body, and the abused chastity of her daughters. She said their cause was just, and the deities were on their side; the one legion that had dared to face them had been destroyed. She, a woman, was resolved to win or die; if the men wanted to live in slavery, that was their choice.
However, the lack of manoeuvrability of the British forces, combined with lack of open-field tactics to command these numbers, put them at a disadvantage to the Romans, who were skilled at open combat due to their superior equipment and discipline. Also, the narrowness of the field meant that Boudica could put forth only as many troops as the Romans could at a given time.
First, the Romans stood their ground and used volleys of "pila" (heavy javelins) to kill thousands of Britons who were rushing toward the Roman lines. The Roman soldiers, who had now used up their "pila", were then able to engage Boudica's second wave in the open. As the Romans advanced in a wedge formation, the Britons attempted to flee, but were impeded by the presence of their own families, whom they had stationed in a ring of wagons at the edge of the battlefield, and were slaughtered. This is not the first instance of this tactic—the women of the Cimbri, in the Battle of Vercellae against Gaius Marius, were stationed in a line of wagons and acted as a last line of defence. Ariovistus of the Suebi is reported to have done the same thing in his battle against Julius Caesar. Tacitus reports that "according to one report almost eighty thousand Britons fell" compared with only four hundred Romans.
According to Tacitus in his "Annals", Boudica poisoned herself, though in the "Agricola" which was written almost twenty years prior he mentions nothing of suicide and attributes the end of the revolt to "socordia" ("indolence"); Dio says she fell sick and died and then was given a lavish burial; though this may be a convenient way to remove her from the story. Considering Dio must have read Tacitus, it is worth noting he mentions nothing about suicide (which was also how Postumus and Nero ended their lives).
Postumus, on hearing of the Roman victory, fell on his sword. Catus Decianus, who had fled to Gaul, was replaced by Gaius Julius Alpinus Classicianus. Suetonius conducted punitive operations, but criticism by Classicianus led to an investigation headed by Nero's freedman Polyclitus. Fearing Suetonius' actions would provoke further rebellion, Nero replaced the governor with the more conciliatory Publius Petronius Turpilianus. The historian Gaius Suetonius Tranquillus tells us the crisis had almost persuaded Nero to abandon Britain.
Location of her defeat.
The location of Boudica's defeat is unknown. Most historians favour a site in the West Midlands, somewhere along the Roman road now known as Watling Street. Kevin K. Carroll suggests a site close to High Cross in Leicestershire, on the junction of Watling Street and the Fosse Way, which would have allowed the Legio II "Augusta", based at Exeter, to rendezvous with the rest of Suetonius's forces, had they not failed to do so. Manduessedum (Mancetter), near the modern town of Atherstone in Warwickshire, has also been suggested, as has "The Rampart" near Messing in Essex, according to legend. More recently, a discovery of Roman artefacts in Kings Norton close to Metchley Camp has suggested another possibility, and a thorough examination of a stretch of Watling Street between St. Albans, Boudica's last known location, and the Fosse Way junction has suggested the Cuttle Mill area of Paulerspury in Northamptonshire, which has topography very closely matching that described by Tacitus of the scene of the battle.
In 2009 it was suggested that the Iceni were returning to East Anglia along the Icknield Way when they encountered the Roman army in the vicinity of Arbury Bank, Hertfordshire. In March 2010, evidence was published suggesting the site may be located at Church Stowe, Northamptonshire.
Historical sources.
Tacitus, the most important Roman historian of this period, took a particular interest in Britain as his father-in-law Gnaeus Julius Agricola served there three times (and was the subject of his first book). Agricola was a military tribune under Suetonius Paulinus, which almost certainly gave Tacitus an eyewitness source for Boudica's revolt. Cassius Dio's account is only known from an epitome, and his sources are uncertain. He is generally agreed to have based his account on that of Tacitus, but he simplifies the sequence of events and adds details, such as the calling in of loans, that Tacitus does not mention.
Gildas, in his 6th century "De Excidio et Conquestu Britanniae", may have been alluding to Boudica when he wrote "A treacherous lioness butchered the governors who had been left to give fuller voice and strength to the endeavours of Roman rule".
Cultural depictions.
History and literature.
By the Middle Ages Boudica was forgotten. She makes no appearance in Bede's work, the "Historia Brittonum", the "Mabinogion" or Geoffrey of Monmouth's "History of the Kings of Britain". But the rediscovery of the works of Tacitus during the Renaissance allowed Polydore Vergil to reintroduce her into British history as "Voadicea" in 1534. Raphael Holinshed also included her story in his "Chronicles" (1577), based on Tacitus and Dio, and inspired Shakespeare's younger contemporaries Francis Beaumont and John Fletcher to write a play, "Bonduca", in 1610. William Cowper wrote a popular poem, "Boadicea, an ode", in 1782.
It was in the Victorian era that Boudica's fame took on legendary proportions as Queen Victoria came to be seen as Boudica's "namesake", their names being identical in meaning. Victoria's Poet Laureate, Alfred, Lord Tennyson, wrote a poem, "Boadicea", and several ships were named after her. 
Statue.
A great bronze statue of Boudica with her daughters in her war chariot (furnished with scythes after the Persian fashion) was commissioned by Prince Albert and executed by Thomas Thornycroft. It was completed in 1905 and stands next to Westminster Bridge and the Houses of Parliament, with the following lines from Cowper's poem, referring to the British Empire:
<poem>
Regions Caesar never knew
Thy posterity shall sway.
</poem>
Ironically, the great anti-imperialist rebel was now identified with the head of the British Empire, and her statue stood guard over the city she razed to the ground.
In more recent times, Boudica has been the subject of numerous documentaries, including some by Discovery Channel, History International Channel (now known as H2), and the BBC.
Boudica and King's Cross.
The area of King's Cross, London was previously a village known as Battle Bridge which was an ancient crossing of the River Fleet. The original name of the bridge was Broad Ford Bridge.
The name "Battle Bridge" led to a tradition that this was the site of a major battle between the Romans and the Iceni tribe led by Boudica. The tradition is not supported by any historical evidence and is rejected by modern historians. However, Lewis Spence's 1937 book "Boadicea - warrior queen of the Britons" went so far as to include a map showing the positions of the opposing armies. There is a belief that she was buried between platforms 9 and 10 in King's Cross station in London, England. There is no evidence for this and it is probably a post-World War II invention.
Other cultural references.
In 2003 an LTR retrotransposon from the genome of the human blood fluke "Schistosoma mansoni" was named "Boudicca". The Boudicca retrotransposon, a high-copy retroviral-like element, was the first mobile genetic element of this type to be discovered in "S. mansoni".
In July 2008, the British television series "Bonekickers", dedicated an hour to Boudica in the episode named "The Eternal Fire". Various female politicians, including former Prime Minister of New Zealand Helen Clark have been called Boadicea.

</doc>
<doc id="4518" url="http://en.wikipedia.org/wiki?curid=4518" title="Borneo">
Borneo

Borneo is the third-largest island in the world and the largest island of Asia. At the geographic centre of Maritime Southeast Asia, in relation to major Indonesian islands, it is located north of Java, west of Sulawesi, and east of Sumatra.
The island is divided among three countries: Brunei and Malaysia on the north, and Indonesia to the south. Approximately 73% of the island is Indonesian territory. In the north, the East Malaysian states of Sabah and Sarawak make up about 26% of the island. Additionally, the Malaysian federal territory of Labuan is situated on a small island just off the coast of Borneo. The sovereign state of Brunei, located on the north coast, comprises about 1% of Borneo's land area. Borneo is home to one of the oldest rainforests in the world.
Geography.
 by the South China Sea to the north and northwest, the Sulu Sea to the northeast, the Celebes Sea and the Makassar Strait to the east, and the Java Sea and Karimata Strait to the south. To the west of Borneo are the Malay Peninsula and Sumatra. To the south and east are islands of Indonesia: Java and Sulawesi, respectively. To the northeast are the Philippines.
With an area of , it is the third-largest island in the world, and is the largest island of Asia (the largest continent). Its highest point is Mount Kinabalu in Sabah, Malaysia, with an elevation of .
The largest river system is the Kapuas in West Kalimantan, with a length of . Other major rivers include the Mahakam in East Kalimantan (), the Barito in South Kalimantan (), and Rajang in Sarawak ().
Borneo has significant cave systems. Clearwater Cave, for example, has one of the world's longest underground rivers. Deer Cave is home to over three million bats, with guano accumulated to over deep.
Before sea levels rose at the end of the last Ice Age, Borneo was part of the mainland of Asia, forming, with Java and Sumatra, the upland regions of a peninsula that extended east from present day Indochina. The South China Sea and Gulf of Thailand now submerge the former low-lying areas of the peninsula. Deeper waters separating Borneo from neighbouring Sulawesi prevented a land connection to that island, creating the divide between Asian and Australia-New Guinea biological regions, known as Wallace's Line.
Ecology.
The Borneo rainforest is 140 million years old, making it one of the oldest rainforests in the world. There are about 15,000 species of flowering plants with 3,000 species of trees (267 species are dipterocarps), 221 species of terrestrial mammals and 420 species of resident birds in Borneo. There are about 440 freshwater fish species in Borneo (about the same as Sumatra and Java combined). It is the centre of the evolution and distribution of many endemic species of plants and animals. The Borneo rainforest is one of the few remaining natural habitats for the endangered Bornean orangutan. It is an important refuge for many endemic forest species, including the Asian elephant, the Sumatran rhinoceros, the Bornean clouded leopard, the Hose's civet and the dayak fruit bat.
In 2010 the World Wide Fund for Nature stated that 123 species have been discovered in Borneo since the "Heart of Borneo" agreement was signed in 2007.
The WWFN has classified the island into seven distinct ecoregions. Most are lowland regions: 
The island historically had extensive rainforest cover, but the area was reduced due to heavy logging for the Malaysian and Indonesian plywood industry. Half of the annual global tropical timber acquisition comes from Borneo. Palm oil plantations have been widely developed and are rapidly encroaching on the last remnants of primary rainforest. Forest fires of 1997 to 1998, started by the locals to clear the forests for plantations were exacerbated by an exceptionally dry El Niño season, worsening the annual shrinkage of the rainforest. During these fires, hotspots were visible on satellite images and the resulting haze affected four countries: Brunei, Malaysia, Indonesia, and Singapore.
In 2010 Sarawak announced a plan for energy production, the Sarawak Corridor of Renewable Energy, to try to establish sustainability.
History.
Early history.
According to ancient Chinese, Indian and Javanese manuscripts, western coastal cities of Borneo had become trading ports by the first millennium. In Chinese manuscripts, gold, camphor, tortoise shells, hornbill ivory, rhinoceros horn, crane crest, beeswax, lakawood (a scented heartwood and root wood of a thick liana, "Dalbergia parviflora"), dragon's blood, rattan, edible bird's nests and various spices were described as among the most valuable items from Borneo. The Indians named Borneo "Suvarnabhumi" (the land of gold) and also "Karpuradvipa" (Camphor Island). The Javanese named Borneo "Puradvipa", or Diamond Island. Archaeological findings in the Sarawak river delta reveal that the area was a thriving trading centre between India and China from the 500s until about 1300 AD.
One of the earliest evidence of Hindu influence in Southeast Asia were stone pillars which bear inscriptions in the Pallava script, found in Kutai along the Mahakam River in East Kalimantan, dating to around the second half of the 300s AD.
By the 14th century, Borneo was under the control of the Majapahit kingdom based in present-day Indonesia. Muslims entered the island and converted many of the indigenous peoples to Islam.
During the 1450s, Shari'ful Hashem Syed Abu Bakr, an Arab born in Johor, arrived in Sulu from Malacca. In 1457, he founded the Sultanate of Sulu; he titled himself as "Paduka Maulana Mahasari Sharif Sultan Hashem Abu Bakr". The Sultanate of Brunei, during its golden age from the 15th century to the 17th century, ruled a large part of northern Borneo. In 1703 (other sources say 1658), the Sultanate of Sulu received the eastern part of North Borneo from the Sultan of Brunei, after Sulu sent aid against a rebellion in Brunei.
Dutch and British control.
The Sultanate of Brunei granted large parts of land in Sarawak in 1842 to the English adventurer James Brooke, as reward for his having helped quell a local rebellion. Brooke established the Kingdom of Sarawak and was recognised as its rajah after paying a fee to the Sultanate. He established a monarchy, and the Brooke dynasty (through his nephew and great-nephew) ruled Sarawak for 100 years; the leaders were known as the White Rajahs.
In the early 19th century, British and Dutch governments signed the Anglo-Dutch Treaty of 1824 to exchange trading ports under their controls and assert spheres of influence. This resulted in indirectly establishing British- and Dutch-controlled areas in Borneo, in the north and south, respectively. The Malay and Sea Dayak pirates preyed on maritime shipping in the waters between Singapore and Hong Kong from their haven in Borneo.
The British North Borneo Company controlled the territory of North Borneo (present-day Sabah) from 1882 to 1941.
World War II.
During World War II, Japanese forces gained control and occupied Borneo (1941–45). They decimated many local populations and killed Malay intellectuals. Sultan Muhammad Ibrahim Shafi ud-din II of Sambas in Kalimantan was executed in 1944. The Sultanate was thereafter suspended and replaced by a Japanese council. During the Japanese occupation, the Dayak played a role in guerrilla warfare against the occupying forces, particularly in the Kapit Division. They temporarily revived headhunting of Japanese toward the end of the war. Allied Z Special Unit provided assistance to them. After the Fall of Singapore, the Japanese sent several thousand British and Australian prisoners of war to camps in Borneo. At one of the worst sites, around Sandakan in Borneo, only six of some 2,500 prisoners survived. In 1945 the island was liberated by the Allies from the Japanese.
Recent history.
Borneo was the main site of the confrontation between Indonesia and Malaysia between 1962 and about 1969. The British Army was deployed against the Indonesians and communist revolts to gain control of the whole area. Before the formation of Malaysian Federation, the Philippines claimed that the eastern part of the Malaysian state of Sabah was within their territory. They based this on the history of the Sultanate of Sulu's leasing agreement with the British North Borneo Company.
Demographics.
The demonym for Borneo is Bornean or Bornese.
Borneo has 19.8 million inhabitants (in mid-2010), a population density of 26 inhabitants per square km. Most of the population lives in coastal cities, although the hinterland has small towns and villages along the rivers. The population consists mainly of Malay, Banjar, Chinese and Dayak ethnic groups. The Chinese, who make up 29% of the population of Sarawak and 17% of total population in West Kalimantan, Indonesia are descendants of immigrants primarily from southeastern China.
The religion of the majority of the population in Kalimantan is Muslim, and some indigenous groups continue to practice animism. But, approximately 91% of the Dayak are Christian, a religion introduced by missionaries in the 19th century. In Central Kalimantan is a small Hindu minority. In the interior of Borneo are the Penan, some of who still live as nomadic hunter-gatherers. Some coastal areas have marginal settlements of the Bajau, who historically lived in a sea-oriented, boat-dwelling, nomadic culture. In the northwest of Borneo, the Dayak ethnic group is represented by the Iban, with about 710,000 members.
In Kalimantan since the 1990s, the Indonesian government has undertaken an intense transmigration program; it financed the relocation to that area of poor, landless families from Java, Madura, and Bali. By 2001, transmigrants made up 21% of the population in Central Kalimantan. Since the 1990s, the indigenous Dayak and Malays have resisted encroachment by these migrants: violent conflict has occurred between some transmigrant and indigenous populations. In the 1999 Sambas riots, Malays and Dayaks joined together to massacre thousands of the Madurese migrants. In Kalimantan, thousands were killed in 2001 fighting between Madurese transmigrants and the Dayak people in the Sampit conflict.
Largest cities.
The following is a list of 20 largest cities in Borneo by population, based on 2010 census for Indonesia and 2010 census for Malaysia. Population data signifies number within official districts and does not include adjoining or nearby conurbation outside defined districts—such as, but not limited to, Kota Kinabalu and Banjarbaru. In other instances, the district area is much larger than the actual city it represents thereby "inflating" the population by including the rural population living further outside the actual city—such as, but not limited to, Tawau and Palangkaraya.
Administration.
The island of Borneo is divided administratively by three countries. It is the only island in the world so divided:
1) Brunei: Census of Population 2001
2) islands administered as Borneo, geologically part of Borneo, on nearshore islands (2.5 km off the main island of Borneo)
3) Citypopulation.de reports on Official Decennial Censuses in 2010 for both Indonesia and Malaysia, independent estimate for Brunei.

</doc>
<doc id="4519" url="http://en.wikipedia.org/wiki?curid=4519" title="Ballpoint pen">
Ballpoint pen

A ballpoint pen, also known as a and "ball pen", is a pen that dispenses ink over a metal ball at its point, i.e. over a "ball point". The metal commonly used is steel, brass or tungsten carbide. It was conceived and developed as a cleaner and more reliable alternative to quill and fountain pens and is now the world's most-used writing instrument: millions are manufactured and sold daily. As a result, it has influenced art and graphic design and spawned an artwork genre.
Pen manufacturers produce designer ballpoint pens for the high-end and collectors' markets.
The Bic Cristal is a popular disposable type of ballpoint pen whose design is recognised by its place in the permanent collection of the Museum of Modern Art, New York.
History.
Origins.
The concept of using a "ball point" within a writing instrument as a method of applying ink to paper has existed since the late 19th century.
In these inventions, the ink was placed in a thin tube whose end was blocked by a tiny ball, held so that it could not slip into the tube or fall out of the pen. The ink clung to the ball, which spun as the pen was drawn across the paper.
The first patent for a ballpoint pen was issued on 30 October 1888, to John J. Loud, a leather tanner, who was attempting to make a writing instrument that would be able to write on his leather products, which then-common fountain pens could not. Loud's pen had a small rotating steel ball, held in place by a socket. Although it could be used to mark rough surfaces such as leather, as Loud intended, it proved to be too coarse for letter-writing. With no commercial viability, its potential went unexploited and the patent eventually lapsed.
The manufacture of economical, reliable ballpoint pens as we know them arose from experimentation, modern chemistry, and precision manufacturing capabilities of the early 20th century.
Patents filed worldwide during early development are testaments to failed attempts at making the pens commercially viable and widely available.
Early ballpoints did not deliver the ink evenly; overflow and clogging were among the obstacles inventors faced toward developing reliable ballpoint pens. If the ball socket were too tight, or the ink too thick, it would not reach the paper. If the socket were too loose, or the ink too thin, the pen would leak or the ink would smear.
Ink reservoirs pressurized by piston, spring, capillary action, and gravity would all serve as solutions to ink-delivery and flow problems.
László Bíró, a Hungarian newspaper editor frustrated by the amount of time that he wasted filling up fountain pens and cleaning up smudged pages, noticed that inks used in newspaper printing dried quickly, leaving the paper dry and smudge free. He decided to create a pen using the same type of ink.
Bíró enlisted the help of his brother György, a chemist, to develop viscous ink formulas for new ballpoint designs.
Bíró's innovation successfully coupled ink-viscosity with a ball-socket mechanism which act compatibly to prevent ink from drying inside the reservoir while allowing controlled flow.
Bíró filed a British patent on 15 June 1938.
In 1941 the Bíró brothers and a friend, Juan Jorge Meyne, fled Germany and moved to Argentina, where they formed "Bíró Pens of Argentina" and filed a new patent in 1943. Their pen was sold in Argentina as the "Birome" (portmanteau of the names Bíró and Meyne), which is how ballpoint pens are still known in that country.
This new design was licensed by the British, who produced ball point pens for RAF aircrew as the "Biro". Ballpoint pens were found to be more versatile than fountain pens, especially at high altitudes where fountain pens were prone to ink-leakage.
Postwar proliferation.
Following World War II, many companies vied to commercially produce their own ballpoint pen design.
In post-war Argentina, success of the Birome ballpoint was limited, but in mid-1945 the "Eversharp" Co., a maker of mechanical pencils, teamed up with Eberhard Faber Co. to license the rights from Birome for sales in the United States.
During the same period, American entrepreneur Milton Reynolds came across a Birome ballpoint pen during a business trip to Buenos Aires, Argentina.
Recognizing commercial potential, he purchased several ballpoint samples, returned to the United States, and founded "Reynolds International Pen Company".
Reynolds bypassed the Birome patent with sufficient design alterations to obtain an American patent, beating Eversharp and other competitors to introduce the pen to the US market.
Debuting at Gimbels department store in New York City on 29 October 1945, for US$9.75 each, "Reynolds Rocket" became the first commercially successful ballpoint pen.
Reynolds went to great extremes to market the pen, with great success; Gimbel's sold many thousands of pens within one week.
In Britain, the Miles Martin pen company was producing the first commercially successful ballpoint pens there by the end of 1945.
Neither Reynolds' nor Eversharp's ballpoint lived up to consumer expectations in America and, although ballpoint pen sales peaked in 1946, consumer interest subsequently plunged due to market-saturation.
By the early 1950s the ballpoint boom had subsided and Reynolds' company folded.
Paper Mate pens, among the emerging ballpoint brands of the 1950s, bought the rights to distribute their own ballpoint pens in Canada.
Facing concerns about ink-reliability, Paper Mate would pioneer new ink formulas and advertise them as "banker-approved".
In 1954 Parker Pens released "The Jotter"—that company's first ballpoint—boasting additional features and technological advances which would also include the use of tungsten-carbide textured ball-bearings in their pens.
In less than a year, Parker sold several million pens at prices between three and nine dollars.
In the 1960s, the failing Eversharp Co. sold its pen division to Parker and ultimately folded.
Marcel Bich also introduced a ballpoint pen to the American marketplace in the 1950s, licensed from Bíró and based on the Argentine designs.
Bich shortened his name to Bic in 1953, becoming the ballpoint brand now recognised globally.
Bic pens struggled until the company launched its "Writes The First Time, Every Time!" advertising campaign in the 1960s.
Competition during this era forced unit prices to drop considerably.
Types of ballpoint pens.
Ballpoint pens are produced in both disposable and refillable models.
Refills allow for the entire internal ink reservoir, including a ballpoint and socket, to be replaced.
Such characteristics are usually associated with designer-type pens or those constructed of finer materials.
The simplest types of ballpoint pens are disposable and have a cap to cover the tip when the pen is not in use, or a mechanism for retracting the tip, which varies between manufacturers but is usually a spring- or screw-mechanism.
Rollerball pens employ the same ballpoint mechanics, but with the use of water-based inks instead of oil-based inks.
Compared to oil-based ballpoints, rollerball pens are said to provide more fluid ink-flow, but the water-based inks will blot if held stationary against the writing surface.
Water-based inks also remain wet longer when freshly applied and are thus prone to "smearing"—posing problems to left-handed people (or right handed people writing right-to-left script) —and "running", should the writing surface become wet.
Because of a ballpoint pen's reliance on gravity to coat the ball with ink, most cannot be used to write upside-down. However, technology developed by Fisher pens in the United States resulted in the production of what came to be known as the "Fisher Space Pen."
Space Pens combine a more viscous ink with a pressurised ink reservoir which forces the ink toward the point. Unlike standard ballpoints, the rear end of a Space Pen's pressurized reservoir is sealed, eliminating evaporation and leakage, thus allowing the pen to write upside-down, in zero-gravity environments, and reportedly underwater. Astronauts have made use of these pens in outer space.
Ballpoint pens with "erasable ink" were pioneered by the Paper Mate pen company.
The ink formulas of erasable ballpoints have properties similar to rubber cement, allowing the ink it to be literally rubbed clean from the writing surface before drying and eventually becoming permanent.
Erasable ink is much thicker than standard ballpoint inks, requiring pressurised cartridges to facilitate inkflow—meaning they may also write upside-down.
Though these pens are equipped with erasers, any eraser will suffice.
The inexpensive, disposable Bic Cristal (also simply "Bic pen" or "Biro") is reportedly the most widely sold pen in the world.
It was the Bic company's first product and is still synonymous with the company name.
The Bic Cristal is part of the permanent collection at the Museum of Modern Art in New York City, acknowledged for its industrial design.
Its hexagonal barrel mimics that of a wooden pencil and is transparent, showing the ink level in the reservoir.
The pen's streamlined cap has a small hole to prevent suffocation if children suck it into the throat.
Ballpoint pens are sometimes provided free by businesses, such as hotels, as a form of advertising—printed with a company's name; a ballpoint pen is a relatively low cost advertisement that is highly effective (customers will use, and therefore see, a pen daily). Businesses and charities include ballpoint pens in direct mail campaigns to increase a customer's interest in the mailing. Ballpoints have also been produced to commemorate events, such as a pen commemorating the 1963 assassination of President John F. Kennedy.
As art medium.
Ballpoint pens have proven to be a versatile art medium for professional artists as well as amateur doodlers. Low cost, availability, and portability are cited by practitioners as qualities which make this common writing tool a convenient, alternative art supply.
Some artists use them within mixed-media works, while others use them solely as their medium-of-choice.
Effects not generally associated with ballpoint pens can be achieved.
Traditional pen-and-ink techniques such as stippling and cross-hatching can be used to create half-tones or the illusion of form and volume.
For artists whose interests necessitate precision line-work, ballpoints are an obvious attraction; ballpoint pens allow for sharp lines not as effectively executed using a brush.
Finely applied, the resulting imagery has been mistaken for airbrushed artwork and photography, causing reactions of disbelief which ballpoint artist Lennie Mace refers to as the "Wow Factor".
Famous 20th Century artists such as Andy Warhol, among others, have utilised ballpoint pens to some extent during their careers. Ballpoint pen artwork continues to attract interest in the 21st Century, with contemporary artists gaining recognition for their specific use of ballpoint pens; for their technical proficiency, imagination and innovation. Korean-American artist Il Lee has been creating large-scale, ballpoint-only abstract artwork since the late 1970s.
Since the 1980s, Lennie Mace creates imaginative, ballpoint-only artwork of varying content and complexity, applied to unconventional surfaces including wood and denim. The artist coined terms such as "PENtings" and "Media Graffiti" to describe his varied output.
More recently, British artist James Mylne has been creating photo-realistic artwork using mostly black ballpoints, sometimes with minimal mixed-media color.
In the mid-2000s (decade) Juan Francisco Casas generated Internet attention for a series of large-scale, photo-realistic ballpoint duplications of his own snapshots of friends, utilising only blue pens.
Using ballpoint pens to create artwork is not without limitations.
Color availability and sensitivity of ink to light are among concerns of ballpoint pen artists.
Mistakes pose greater risks to ballpoint artists; once a line is drawn, it generally cannot be erased.
Additionally, "blobbing" of ink on the drawing surface and "skipping" of ink-flow require consideration when using ballpoint pens for artistic purposes.
Although the mechanics of ballpoint pens remain relatively unchanged, ink composition has evolved to solve certain problems over the years, resulting in unpredictable sensitivity to light and some extent of fading.
Manufacturing.
Although designs and construction vary between brands, basic components of all ballpoint pens are universal.
Standard components include the freely-rotating "ball point" itself (distributing the ink), a "socket" holding the ball in place, and a self-contained "ink reservoir" supplying ink to the ball.
In modern pens, narrow plastic tubes contain the ink, which is compelled downward to the ball by gravity. Brass, steel or tungsten carbide are used to manufacture the ball bearing-like points, then housed in a brass socket.
The function of these components can be compared with the ball-applicator of roll-on antiperspirant; the same technology at a larger scale. The ball point delivers the ink to the writing surface while acting as a "buffer" between the ink in the reservoir and the air outside, preventing the quick-drying ink from drying inside the reservoir. Modern ballpoints are said to have a two-year shelf life, on average.
The common ballpoint pen is a product of mass-production, with components produced separately on assembly-lines.
Basic steps in the manufacturing process include production of ink formulas, moulding of metal and plastic components, and assembly.
Marcel Bich was involved in developing the production of inexpensive ballpoint pens.
Standards.
The International Organization for Standardization has published standards for ball point and roller ball pens:

</doc>
<doc id="4524" url="http://en.wikipedia.org/wiki?curid=4524" title="Burroughs Corporation">
Burroughs Corporation

The Burroughs Corporation was a major American manufacturer of business equipment. The company was founded in 1886 as the American Arithmometer Company, and after the 1986 merger with Sperry Univac was renamed Unisys. The company's history paralleled many of the major developments in computing. At its start it produced mechanical adding machines, and later moved into programmable ledgers and then computers. And while it was one of the largest producers of mainframe computers in the world, Burroughs also produced related equipment as well, including typewriters and printers.
Early history.
In 1886, the American Arithmometer Company was established in St. Louis, Missouri to produce and sell an adding machine invented by William Seward Burroughs (grandfather of Beat Generation author William S. Burroughs). In 1904, six years after Burroughs' death, the company moved to Detroit and changed its name to the Burroughs Adding Machine Company. It soon was the biggest adding machine company in America.
Evolving product lines.
The adding machine range began with the basic, hand-cranked P100 which was only capable of adding. The design included some revolutionary features, foremost of which was the dashpot. The P200 offered a subtraction capability and the P300 provided a means of keeping 2 separate totals. The P400 and the P600, the top of the range of which was the P612 provided a moveable carriage and, in the case of the P600/P612 range some limited programmability based upon the position of the carriage. The range was further extended by the inclusion of the "J" series which provided a single finger calculation facility, and the "c" series of both manual and electrical assisted comptometers. In the late 60's the Burroughs sponsored "nixi-tube" provided an electronic display calculator.
Burroughs developed a range of adding machines with different capabilities, gradually increasing in their capabilities. A revolutionary adding machine was the "Sensimatic", which was able to perform many business functions semi-automatically. It had a moving programmable carriage to maintain ledgers. It could store 9, 18 or 27 balances during the ledger posting operations and worked with a mechanical adder named a Crossfooter. The Sensimatic developed into the "Sensitronic" which could store balances on a magnetic stripe which was part of the ledger card. This balance was read into the accumulator when the card was inserted into the carriage. The Sensitronic was followed by the E1000, the E2000, E4000, E6000 and the E8000, which was computer system supporting magnetic tape, card reader/punches and a line printer.
Later, Burroughs was selling more than adding machines, including typewriters. But the biggest shift in company history came in 1953; the Burroughs Adding Machine Company was renamed the Burroughs Corporation and began moving into computer products, initially for banking institutions. This move began with Burrough's purchase, in June 1956, of the ElectroData Corporation in Pasadena, California, a spinoff of the Consolidated Engineering Corporation which had designed test instruments and had a cooperative relationship with Caltech in Pasadena. ElectroData had built the Datatron 205 and was working on the Datatron 220. The first major computer product that came from this marriage was the B205 tube computer. In the late 1960s the D2000, D4000 range was produced (also known as the TC500—Terminal Computer 500) which had a golf ball printer and a 1K (80 bit) disk memory. These were popular as branch terminals to the B5500/6500/6700 systems, which sold well in the banking sector, where they were often connected to non-Burroughs mainframes. In conjunction with these products, Burroughs also manufactured an extensive range of cheque processing equipment, normally attached as terminals to a larger system such as a B2700 or B1700.
A force in the computing industry.
Burroughs was one of the nine major United States computer companies (with IBM, the largest, Honeywell, NCR Corporation, Control Data Corporation, General Electric, Digital Equipment Corporation, RCA and UNIVAC) through most of the 1960s. In terms of sales, Burroughs was always a distant second to IBM. In fact, IBM's share of the market at the time was so much larger than all of the others, that this group was often referred to as "IBM and the Seven Dwarfs." By 1972 when GE and RCA were no longer in the mainframe business, the remaining five companies behind IBM became known as the BUNCH, an acronym based on their initials.
At the same time, Burroughs was very much a competitor. Like IBM, Burroughs tried to supply a complete line of products for its customers, including Burroughs-designed printers, disk drives, tape drives, computer printing paper, and even typewriter ribbons.
In the 1950s, Burroughs worked with the Federal Reserve Bank on the development and computer processing of magnetic ink character recognition (MICR) especially for the processing of bank cheques. Burroughs made special MICR/OCR sorter/readers which attached to their medium systems line of computers (2700/3700/4700) and this entrenched the company in the computer side of the banking industry.
Developments and innovations.
The Burroughs Corporation developed three highly innovative architectures, based on the design philosophy of "language directed design". Their machine instruction sets favored one or many high level programming languages, such as ALGOL, COBOL or FORTRAN. All three architectures were considered mainframe class machines:
Merger.
In September 1986, Burroughs Corporation merged with Sperry Corporation to form Unisys. For a time, the combined company retained the Burroughs processors as the A- and V-systems lines. However, as the market for large systems shifted from proprietary architectures to common servers, the company eventually dropped the V-Series line, although customers continued to use V-series systems . Unisys continues to develop and market the A-Series, now known as ClearPath.
Re-emergence of the Burroughs name.
In 2010, UNISYS sold off its Payment Systems Division to Marlin Equity Partners, a California-based private investment firm, which incorporated it as Burroughs Payment Systems based in Plymouth, Michigan.
References in popular culture.
Burroughs B205 hardware has appeared as props in many Hollywood television and film productions from the late 1950s. For example a B205 console was often shown in the television series "Batman" as the "Bat Computer"; also as the computer in "Lost in Space". B205 tape drives were often seen in series such as "The Time Tunnel" and "Voyage to the Bottom of the Sea".

</doc>
<doc id="4526" url="http://en.wikipedia.org/wiki?curid=4526" title="Brick">
Brick

A brick is a block or a single unit of a kneaded clay-bearing soil, sand and lime, or concrete material, fire hardened or air dried, used in masonry construction. Lightweight bricks (also called "lightweight blocks") are made from expanded clay aggregate. Fired brick are the most numerous type and are laid in "courses" and numerous patterns known as "bonds", collectively known as "brickwork", and may be laid in various kinds of "mortar" to hold the bricks together to make a durable structure. Brick are produced in numerous types, materials, and sizes which vary with region and time period, and are produced in bulk quantities. Two most basic categories of brick are "fired" and "non-fired" brick. Fired brick are one of the longest lasting and strongest building materials sometimes referred to as artificial stone and have been used since circa 5000 BC. Air dried bricks have a history older than fired bricks, are known by the synonyms "mud brick" and "adobe", and have an additional ingredient of a mechanical "binder" such as straw.
History.
Middle East.
The earliest bricks were "dried brick", meaning they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir. Other more recent findings, dated between 7,000 and 6,395 BC, come from Jericho, Catal Hüyük, the ancient Egyptian fortress of Buhen, and the ancient Indus Valley cities of Mohenjo-daro, Harappa, and Mehrgarh.
Ceramic, or "fired brick" was used as early as 2900 BC in early Indus Valley cities.
China.
In pre-modern China, bricks were being used from the 2nd millennium BCE at a site near Xi'an. Bricks were produced on a larger scale under the Western Zhou dynasty about 3,000 years ago, and evidence for some of the first fired bricks ever produced has been discovered in ruins dating back to the Zhou. The carpenter's manual "Yingzao Fashi", published in 1103 at the time of the Song Dynasty described the brick making process and glazing techniques then in use. Using the 17th century encyclopaedic text "Tiangong Kaiwu", historian Timothy Brook outlined the brick production process of Ming Dynasty China:
Europe.
Early civilisations around the Mediterranean adopted the use of fired bricks, including the Ancient Greeks and Romans. The Roman legions operated mobile kilns, and built large brick structures throughout the Roman Empire, stamping the bricks with the seal of the legion.
During the Early Middle Ages the use of bricks in construction became popular in Northern Europe, after being introduced there from Northern-Western Italy. An independent style of brick architecture, known as brick Gothic (similar to Gothic architecture) flourished in places that lacked indigenous sources of rocks. Examples of this architectural style can be found in modern-day Denmark, Germany, Poland, and Russia.
This style evolved into Brick Renaissance as the stylistic changes associated with the Italian Renaissance spread to northern Europe, leading to the adoption of Renaissance elements into brick building. A clear distinction between the two styles only developed at the transition to Baroque architecture. In Lübeck, for example, Brick Renaissance is clearly recognisable in buildings equipped with terracotta reliefs by the artist Statius von Düren, who was also active at Schwerin (Schwerin Castle) and Wismar (Fürstenhof).
Long distance bulk transport of bricks and other construction equipment remained prohibitively expensive until the development of modern transportation infrastructure, with the construction of canal, roads and railways.
Industrial era.
Production of bricks increased massively with the onset of the Industrial Revolution and the rise in factory building in England. For reasons of speed and economy, bricks were increasingly preferred as building material to stone, even in areas where the stone was available. It was at this time in London, that bright red brick was chosen for construction to make the buildings more visible in the heavy fog and to prevent traffic accidents.
The transition from the traditional method of production known as 'hand-moulding' to a mechanised form of mass production slowly took place during the first half of the nineteenth century. Possibly the first successful brick-making machine was patented by a Mr Henry Clayton, employed at the Atlas Works in Middlesex, England, in 1855, and was capable of producing up to 25,000 bricks daily with minimal supervision. His mechanical apparatus soon achieved widespread attention after it was adopted for use by the South Eastern Railway Company for brick-making at their factory near Folkestone. The Bradley & Craven Ltd ‘Stiff-Plastic Brickmaking Machine’ was patented in 1853, apparently predating Clayton. Bradley & Craven went on to be a dominant manufacturer of brickmaking machinery. Predating both Clayton and Bradley & Craven Ltd. however was the brick making machine patented by Richard A. Ver Valen of Haverstraw, New York in 1852.
The demand for high office building construction at the turn of the 20th century, led to a much greater use of cast and wrought iron and later steel and concrete. The use of brick for skyscraper construction severely limited the size of the building - the Monadnock Building, built in 1896 in Chicago required exceptionally thick walls to maintain the structural integrity of its 17 storeys.
Following pioneering work in the 1950s at the Swiss Federal Institute of Technology and the Building Research Establishment in Watford, UK, the use of improved masonry for the construction of tall structures up to 18 storeys high was made viable. However, the use of brick has largely remained restricted to small to medium sized buildings, as steel and concrete remain superior materials for high-rise construction.
Methods of manufacture.
Three basic types of brick are un-fired, fired, and chemically set bricks. Each type is manufactured differently. 
Fired brick.
Fired bricks are burned in a kiln which makes them durable. Modern, fired, clay bricks are formed in one of three processes – soft mud, dry press, or extruded.
Normally, brick contains the following ingredients:
The soft mud method is the most common, as it is the most economical. It starts with the raw clay, preferably in a mix with 25–30% sand to reduce shrinkage. The clay is first ground and mixed with water to the desired consistency. The clay is then pressed into steel moulds with a hydraulic press. The shaped clay is then fired ("burned") at 900–1000 °C to achieve strength.
Rail kilns.
In modern brickworks, this is usually done in a continuously fired tunnel kiln, in which the bricks are fired as they move slowly through the kiln on conveyors, rails, or kiln cars, which achieves a more consistent brick product. The bricks often have lime, ash, and organic matter added, which accelerates the burning process.
Bull's Trench Kilns.
In India, brick making is typically a manual process. The most common type of brick kiln in use there is the Bull's Trench Kiln (BTK), based on a design developed by British engineer W. Bull in the late 19th century.
An oval or circular trench is dug, 6–9 metres wide, 2-2.5 metres deep, and 100–150 metres in circumference. A tall exhaust chimney is constructed in the centre. Half or more of the trench is filled with "green" (unfired) bricks which are stacked in an open lattice pattern to allow airflow. The lattice is capped with a roofing layer of finished brick.
In operation, new green bricks, along with roofing bricks, are stacked at one end of the brick pile; cooled finished bricks are removed from the other end for transport to their destinations. In the middle, the brick workers create a firing zone by dropping fuel (coal, wood, oil, debris, and so on) through access holes in the roof above the trench.
The advantage of the BTK design is a much greater energy efficiency compared with clamp or scove kilns. Sheet metal or boards are used to route the airflow through the brick lattice so that fresh air flows first through the recently burned bricks, heating the air, then through the active burning zone. The air continues through the green brick zone (pre-heating and drying the bricks), and finally out the chimney, where the rising gases create suction which pulls air through the system. The reuse of heated air yields savings in fuel cost.
As with the rail process above, the BTK process is continuous. A half dozen labourers working around the clock can fire approximately 15,000–25,000 bricks a day. Unlike the rail process, in the BTK process the bricks do not move. Instead, the locations at which the bricks are loaded, fired, and unloaded gradually rotate through the trench.
Dry pressed bricks.
The dry press method is similar to the soft mud brick method, but starts with a much thicker clay mix, so it forms more accurate, sharper-edged bricks. The greater force in pressing and the longer burn make this method more expensive.
Extruded bricks.
For extruded bricks the clay is mixed with 10–15% water (stiff extrusion) or 20–25% water (soft extrusion) in a pugmill. This mixture is forced through a die to create a long cable of material of the desired width and depth. This mass is then cut into bricks of the desired length by a wall of wires. Most structural bricks are made by this method as it produces hard, dense bricks, and suitable dies can produce perforations as well. The introduction of such holes reduces the volume of clay needed, and hence the cost. Hollow bricks are lighter and easier to handle, and have different thermal properties from solid bricks. The cut bricks are hardened by drying for 20 to 40 hours at 50 to 150 °C before being fired. The heat for drying is often waste heat from the kiln.
European-style extruded bricks or blocks are used in single-wall construction with finishes applied on the inside and outside. Their many voids comprise a greater proportion of the volume than the solid, thin walls of fired clay. Such bricks are made in 15-, 25-, 30-, 42- and 50-cm widths. Some models have very high thermal insulation properties, making them suitable for zero-energy buildings.
Types of fired clay bricks.
There are thousands of types of bricks that are named for their use, size, forming method, origin, quality, texture, and/or materials.
Chemically set bricks.
Chemically set bricks are not fired but may have the curing process accelerated by the application of heat and pressure in an autoclave.
Calcium-silicate bricks.
Calcium silicate bricks are also called sandlime or flintlime bricks depending on their ingredients. Rather than being made with clay they are made with lime binding the silicate material. The raw materials for calcium-silicate bricks include lime mixed in a proportion of about 1 to 10 with sand, quartz, crushed flint or crushed siliceous rock together with mineral colourants. The materials are mixed and left until the lime is completely hydrated; the mixture is then pressed into moulds and cured in an autoclave for three to fourteen hours to speed the chemical hardening. The finished bricks are very accurate and uniform, although the sharp arrises need careful handling to avoid damage to brick and bricklayer. The bricks can be made in a variety of colours; white, black, buff and grey-blues are common and pastel shades can be achieved. This type of brick is common in Sweden, especially in houses built or renovated in the 1970s, where it is known as "Mexitegel" (en: Mexi[can] Bricks). In India these are known as fly ash bricks, manufactured using the FaL-G (fly ash, lime and gypsum) process. Calcium-silicate bricks are also manufactured in Canada and the United States, and meet the criteria set forth in ASTM C73 – 10 Standard Specification for Calcium Silicate Brick (Sand-Lime Brick). It has lower embodied energy than cement based man-made stone and clay brick.
Concrete bricks.
Bricks of concrete with sand aggregate can be made using a simple machine and a basic assembly line. A conveyor belt adds the mixture to a machine, which pours a measured amount of concrete into a form. The form is vibrated to remove bubbles, after which it is raised to reveal the wet bricks, spaced out on a plywood sheet. A small elevator then stacks these palettes, after which a forklift operator moves them to the brickyard for drying.
Concrete bricks are available in many colours and as an engineering brick made with sulfate-resisting Portland cement or equivalent. When made with adequate amount of cement they are suitable for harsh environments such as wet conditions and retaining walls. They are made to standards BS 6073, EN 771-3. Concrete bricks expand and contract more than clay or sandlime bricks so they need movement joints every 5 to 6 metres, but are similar to other bricks of similar density in thermal and sound resistance and fire resistance.
Influence on fired colour.
The fired colour of clay bricks is influenced by the chemical and mineral content of the raw materials, the firing temperature, and the atmosphere in the kiln. For example, pink coloured bricks are the result of a high iron content, white or yellow bricks have a higher lime content. Most bricks burn to various red hues; as the temperature is increased the colour moves through dark red, purple and then to brown or grey at around . Calcium silicate bricks have a wider range of shades and colours, depending on the colourants used. The names of bricks may reflect their origin and colour, such as London stock brick and Cambridgeshire White.
"Bricks" formed from concrete are usually termed blocks, and are typically pale grey in colour. They are made from a dry, small aggregate concrete which is formed in steel moulds by vibration and compaction in either an "egglayer" or static machine. The finished blocks are cured rather than fired using low-pressure steam. Concrete blocks are manufactured in a much wider range of shapes and sizes than clay bricks and are also available with a wider range of face treatments – a number of which simulate the appearance of clay bricks.
An impervious and ornamental surface may be laid on brick either by salt glazing, in which salt is added during the burning process, or by the use of a "slip," which is a glaze material into which the bricks are dipped. Subsequent reheating in the kiln fuses the slip into a glazed surface integral with the brick base.
Natural stone bricks are of limited modern utility due to their enormous comparative mass, the consequent foundation needs, and the time-consuming and skilled labour needed in their construction and laying. They are very durable and considered more handsome than clay bricks by some. Only a few stones are suitable for bricks. Common materials are granite, limestone and sandstone. Other stones may be used (for example, marble, slate, quartzite, and so on) but these tend to be limited to a particular locality.
Optimal dimensions, characteristics, and strength.
For efficient handling and laying, bricks must be small enough and light enough to be picked up by the bricklayer using one hand (leaving the other hand free for the trowel). Bricks are usually laid flat and as a result the effective limit on the width of a brick is set by the distance which can conveniently be spanned between the thumb and fingers of one hand, normally about four inches (about 100 mm). In most cases, the length of a brick is about twice its width, about eight inches (about 200 mm) or slightly more. This allows bricks to be laid "bonded" in a structure which increases stability and strength (for an example, see the illustration of bricks laid in "English bond", at the head of this article). The wall is built using alternating courses of "stretchers", bricks laid longways, and "headers", bricks laid crossways. The headers tie the wall together over its width. In fact, this wall is built in a variation of "English bond" called "English cross bond" where the successive layers of stretchers are displaced horizontally from each other by half a brick length. In true "English bond", the perpendicular lines of the stretcher courses are in line with each other.
A bigger brick makes for a thicker (and thus more insulating) wall. Historically, this meant that bigger bricks were necessary in colder climates (see for instance the slightly larger size of the Russian brick in table below), while a smaller brick was adequate, and more economical, in warmer regions. A notable illustration of this correlation is the Green Gate in Gdansk; built in 1571 of imported Dutch brick, too small for the colder climate of Gdansk, it was notorious for being a chilly and drafty residence. Nowadays this is no longer an issue, as modern walls typically incorporate specialised insulation materials.
The correct brick for a job can be selected from a choice of colour, surface texture, density, weight, absorption and pore structure, thermal characteristics, thermal and moisture movement, and fire resistance.
In England, the length and width of the common brick has remained fairly constant over the centuries (but see brick tax), but the depth has varied from about two inches (about 51 mm) or smaller in earlier times to about two and a half inches (about 64 mm) more recently. In the United Kingdom, the usual size of a modern brick is 215 × 102.5 × 65 mm (about × ×  inches), which, with a nominal 10 mm ( inch) mortar joint, forms a "unit size" of 225 × 112.5 × 75 mm (9 × × 3 inches), for a ratio of 6:3:2.
In the United States, modern standard bricks are (controlled by American Society for Testing and Materials ASTM ) about 8 ×   ×  inches (203 × 92 × 57 mm). The more commonly used is the modular brick   ×   ×  inches (194 × 92 × 57 mm). This modular brick of plus a mortar joint eased the calculations of the number of bricks in a given run.
Some brickmakers create innovative sizes and shapes for bricks used for plastering (and therefore not visible) where their inherent mechanical properties are more important than their visual ones. These bricks are usually slightly larger, but not as large as blocks and offer the following advantages:
Blocks have a much greater range of sizes. Standard co-ordinating sizes in length and height (in mm) include 400×200, 450×150, 450×200, 450×225, 450×300, 600×150, 600×200, and 600×225; depths (work size, mm) include 60, 75, 90, 100, 115, 140, 150, 190, 200, 225, and 250. They are usable across this range as they are lighter than clay bricks. The density of solid clay bricks is around 2,000 kg/m³: this is reduced by frogging, hollow bricks, and so on, but aerated autoclaved concrete, even as a solid brick, can have densities in the range of 450–850 kg/m³.
Bricks may also be classified as "solid" (less than 25% perforations by volume, although the brick may be "frogged," having indentations on one of the longer faces), "perforated" (containing a pattern of small holes through the brick, removing no more than 25% of the volume), "cellular" (containing a pattern of holes removing more than 20% of the volume, but closed on one face), or "hollow" (containing a pattern of large holes removing more than 25% of the brick's volume). Blocks may be solid, cellular or hollow
The term "frog" can refer to the indentation or the implement used to make it. Modern brickmakers usually use plastic frogs but in the past they were made of wood. It is best practice to lay bricks with the frog facing up to achieve a more full, compact bed.
The compressive strength of bricks produced in the United States ranges from about 1000 lbf/in² to 15,000 lbf/in² (7 to 105 MPa or N/mm² ), varying according to the use to which the brick are to be put. In England clay bricks can have strengths of up to 100 MPa, although a common house brick is likely to show a range of 20–40 MPa.
Use.
Bricks are used for building, block paving and pavement. In the USA, brick pavement was found incapable of withstanding heavy traffic, but it is coming back into use as a method of traffic calming or as a decorative surface in pedestrian precincts. For example, in the early 1900s, most of the streets in the city of Grand Rapids, Michigan were paved with brick. Today, there are only about 20 blocks of brick paved streets remaining (totalling less than 0.5 percent of all the streets in the city limits).
Bricks in the metallurgy and glass industries are often used for lining furnaces, in particular refractory bricks such as silica, magnesia, chamotte and neutral (chromomagnesite) refractory bricks. This type of brick must have good thermal shock resistance, refractoriness under load, high melting point, and satisfactory porosity. There is a large refractory brick industry, especially in the United Kingdom, Japan, the United States, Belgium and the Netherlands.
In Northwest Europe, bricks have been used in construction for centuries. Until recently, almost all houses were built almost entirely from bricks. Although many houses are now built using a mixture of concrete blocks and other materials, many houses are skinned with a layer of bricks on the outside for aesthetic appeal.
Engineering bricks are used where strength, low water porosity or acid (flue gas) resistance are needed.
In the UK a redbrick university is one founded and built in the Victorian era. The term is used to refer to such institutions collectively to distinguish them from the older Oxbridge institutions, the post-war 'plate glass' universities, and the 'new' universities of the 1990s.
Colombian architect Rogelio Salmona was noted for his extensive use of red brick in his buildings and for using natural shapes like spirals, radial geometry and curves in his designs. Most buildings in Colombia are made of brick, given the abundance of clay in equatorial countries like this one.
Limitations.
Starting in the 20th century, the use of brickwork declined in some areas due to concerns with earthquakes. Earthquakes such as the San Francisco earthquake of 1906 and the 1933 Long Beach earthquake revealed the weaknesses of brick masonry in earthquake-prone areas. During seismic events, the mortar cracks and crumbles, and the bricks are no longer held together. Brick masonry with steel reinforcement, which helps hold the masonry together during earthquakes, was used to replace many of the unreinforced masonry buildings. Retrofitting older unreinforced masonry structures has been mandated in many jurisdictions.

</doc>
<doc id="4527" url="http://en.wikipedia.org/wiki?curid=4527" title="Béla Bartók">
Béla Bartók

Béla Viktor János Bartók (; ; March 25, 1881 – September 26, 1945) was a Hungarian composer and pianist. He is considered one of the most important composers of the 20th century; he and Liszt are regarded as Hungary's greatest composers (Gillies 2001). Through his collection and analytical study of folk music, he was one of the founders of comparative musicology, which later became ethnomusicology.
Biography.
Childhood and early years (1881–98).
Béla Bartók was born in the small Banatian town of Nagyszentmiklós in the Kingdom of Hungary, Austria-Hungary (since 1920 Sânnicolau Mare, Romania) on March 25, 1881. Bartók's family reflected some of the ethno-cultural diversities of the country. His father, Béla Sr., considered himself thoroughly Hungarian, because on his father's side the Bartók family was a Hungarian lower noble family, originating from Borsod county (Móser 2006a, 44; Bartók 1981, 13), though his mother, Paula (born Paula Voit), had German as a mother tongue, but was ethnically of "mixed Hungarian" ancestry (Bayley 2001, 16) of Danube Swabian origin. Among her closest forefathers there were families with such names as Polereczky (Magyarized Polish or Slovak) and Fegyveres (Magyar).
Béla displayed notable musical talent very early in life: according to his mother, he could distinguish between different dance rhythms that she played on the piano before he learned to speak in complete sentences (Gillies 1990, 6). By the age of four he was able to play 40 pieces on the piano and his mother began formally teaching him the next year.
Béla was a small and sickly child and suffered from severe eczema until the age of 5 (Gillies 1990, 5). In 1888, when he was seven, his father (the director of an agricultural school) died suddenly. Béla's mother then took him and his sister, Erzsébet, to live in Nagyszőlős (today Vinogradiv, Ukraine) and then to Pozsony (German: Pressburg, today Bratislava, Slovakia). In Pozsony, Béla gave his first public recital at age 11 to a warm critical reception. Among the pieces he played was his own first composition, written two years previously: a short piece called "The Course of the Danube" (de Toth 1999). Shortly thereafter László Erkel accepted him as a pupil.
Early musical career (1899–1908).
From 1899 to 1903, Bartók studied piano under István Thomán, a former student of Franz Liszt, and composition under János Koessler at the Royal Academy of Music in Budapest. There he met Zoltán Kodály, who influenced him greatly and became his lifelong friend and colleague. In 1903, Bartók wrote his first major orchestral work, "Kossuth", a symphonic poem which honored Lajos Kossuth, hero of the Hungarian Revolution of 1848.
The music of Richard Strauss, whom he met in 1902 at the Budapest premiere of "Also sprach Zarathustra," strongly influenced his early work. When visiting a holiday resort in the summer of 1904, Bartók overheard a young nanny, Lidi Dósa from Kibéd in Transylvania, sing folk songs to the children in her care. This sparked his lifelong dedication to folk music.
From 1907, he also began to be influenced by the French composer Claude Debussy, whose compositions Kodály had brought back from Paris. Bartók's large-scale orchestral works were still in the style of Johannes Brahms and Richard Strauss, but he wrote a number of small piano pieces which showed his growing interest in folk music. The first piece to show clear signs of this new interest is the String Quartet No. 1 in A minor (1908), which contains folk-like elements.
In 1907, Bartók began teaching as a piano professor at the Royal Academy. This position freed him from touring Europe as a pianist and enabled him to work in Hungary. Among his notable students were Fritz Reiner, Sir Georg Solti, György Sándor, Ernő Balogh, and Lili Kraus. After Bartók moved to the United States, he taught Jack Beeson and Violet Archer.
In 1908, he and Kodály traveled into the countryside to collect and research old Magyar folk melodies. Their growing interest in folk music coincided with a contemporary social interest in traditional national culture. They made some surprising discoveries. Magyar folk music had previously been categorised as Gypsy music. The classic example is Franz Liszt's famous "Hungarian Rhapsodies" for piano, which he based on popular art songs performed by Romani bands of the time. In contrast, Bartók and Kodály discovered that the old Magyar folk melodies were based on pentatonic scales, similar to those in Asian folk traditions, such as those of Central Asia, Anatolia and Siberia.
Bartók and Kodály quickly set about incorporating elements of such Magyar peasant music into their compositions. They both frequently quoted folk song melodies "verbatim" and wrote pieces derived entirely from authentic songs. An example is his two volumes entitled "For Children" for solo piano, containing 80 folk tunes to which he wrote accompaniment. Bartók's style in his art music compositions was a synthesis of folk music, classicism, and modernism. His melodic and harmonic sense was profoundly influenced by the folk music of Hungary, Romania, and other nations. He was especially fond of the asymmetrical dance rhythms and pungent harmonies found in Bulgarian music. Most of his early compositions offer a blend of nationalist and late Romanticism elements.
Middle years and career (1909–39).
Personal life.
In 1909 at the age of 28, Bartók married Márta Ziegler (1893–1967), aged 16. Their son, Béla III, was born on August 22, 1910. After nearly 15 years together, Bartók divorced Márta in June 1923.
Two months later he married Ditta Pásztory (1903–1982), a piano student, ten days after proposing to her. She was aged 19, he 42. Their son, Péter, was born in 1924.
Opera.
In 1911, Bartók wrote what was to be his only opera, "Bluebeard's Castle", dedicated to Márta. He entered it for a prize by the Hungarian Fine Arts Commission, but they rejected his work as not fit for the stage (Chalmers 1995, 93). In 1917 Bartók revised the score for the 1918 première, and rewrote the ending. Following the 1919 revolution, he was pressured by the new Soviet government to remove the name of the librettist Béla Balázs from the opera (Chalmers 1995, 123), as he was blacklisted and had left the country for Vienna. "Bluebeard's Castle" received only one revival, in 1936, before Bartók emigrated. For the remainder of his life, although he was passionately devoted to Hungary, its people and its culture, he never felt much loyalty to the government or its official establishments.
Folk music and composition.
After his disappointment over the Fine Arts Commission competition, Bartók wrote little for two or three years, preferring to concentrate on collecting and arranging folk music. He collected first in the Carpathian Basin (then the Kingdom of Hungary), where he notated Hungarian, Slovakian, Romanian, and Bulgarian folk music. He also collected in Moldavia, Wallachia, and (in 1913) Algeria. The outbreak of World War I forced him to stop the expeditions; and he returned to composing, writing the ballet "The Wooden Prince" (1914–16) and the String Quartet No. 2 in (1915–17), both influenced by Debussy.
Raised as a Roman Catholic, by his early adulthood Bartók had become an atheist. He believed that the existence of God could not be determined and was unnecessary. He later became attracted to Unitarianism and publicly converted to the Unitarian faith in 1916. As an adult, his son later became president of the Hungarian Unitarian Church (Hughes 1999–2007).
Bartók wrote another ballet, "The Miraculous Mandarin" influenced by Igor Stravinsky, Arnold Schoenberg, as well as Richard Strauss. He next wrote his two violin sonatas (written in 1921 and 1922 respectively), which are harmonically and structurally some of his most complex pieces. "The Miraculous Mandarin", a modern story of prostitution, robbery, and murder, was started in 1918, but not performed until 1926 because of its sexual content.
In 1927–28, Bartók wrote his Third and Fourth String Quartets, after which his compositions demonstrated his mature style. Notable examples of this period are "Music for Strings, Percussion and Celesta" (1936) and "Divertimento for String Orchestra BB 118" (1939). The Fifth String Quartet was composed in 1934, and the Sixth String Quartet (his last) in 1939.
In 1936 he travelled to Turkey to collect and study folk music. He worked in collaboration with Turkish composer Ahmet Adnan Saygun mostly around Adana (Özgentürk 2008; Sipos 2000).
World War II and last years in America (1940–45).
In 1940, as the European political situation worsened after the outbreak of World War II, Bartók was increasingly tempted to flee Hungary. He was strongly opposed to the Nazis and Hungary's siding with Germany. After the Nazis came to power in the early 1930s, Bartók refused to give concerts in Germany and broke away from his publisher there. His anti-fascist political views caused him a great deal of trouble with the establishment in Hungary. Having first sent his manuscripts out of the country, Bartók reluctantly emigrated to the U.S. with his wife Ditta in October that year. They settled in New York City. After joining them in 1942, their son, Péter Bartók, enlisted in the United States Navy where he served in the Pacific during the remainder of the war and later settled in Florida where he became a recording and sound engineer. His oldest son, Béla Bartók, III, remained in Hungary where he survived the war and later worked as a railroad official until his retirement in the early 1980s.
Although he became an American citizen in 1945, shortly before his death (Gagné 2012, 28), Bartók never became fully at home in the USA. He initially found it difficult to compose. Although well known in America as a pianist, ethnomusicologist and teacher, he was not well known as a composer. There was little American interest in his music during his final years. He and his wife Ditta gave some concerts, although demand for them was low. Bartók, who had made some recordings in Hungary, also recorded for Columbia Records after he came to the US; many of these recordings (some with Bartók's own spoken introductions) were later issued on LP and CD (Bartók 1994, 1995a, 1995b, 2003, 2007, 2008).
Supported by a research fellowship from Columbia University, for several years, Bartók and Ditta worked on a large collection of Serbian and Croatian folk songs in Columbia's libraries. Bartók's economic difficulties during his first years in America were mitigated by publication royalties, teaching and performance tours. While his finances were always precarious, he did not live and die in poverty as was the common myth. He had enough friends and supporters to ensure that there was sufficient money and work available for him to live on. Bartók was a proud man and did not easily accept charity. Despite being short on cash at times, he often refused money that his friends offered him out of their own pockets. Although he was not a member of the ASCAP, the society paid for any medical care he needed during his last two years. Bartók reluctantly accepted this (Chalmers 1995, 196–203).
The first symptoms of his health problems began late in 1940, when his right shoulder began to show signs of stiffening. In 1942, symptoms increased and he started having bouts of fever, but no underlying disease was diagnosed, in spite of medical examinations. Finally, in April 1944, leukemia was diagnosed, but by this time, little could be done (Chalmers 1995, 202–207).
As his body slowly failed, Bartók found more creative energy, and he produced a final set of masterpieces, partly thanks to the violinist Joseph Szigeti and the conductor Fritz Reiner (Reiner had been Bartók's friend and champion since his days as Bartók's student at the Royal Academy). Bartók's last work might well have been the String Quartet No. 6 but for Serge Koussevitzky's commission for the "Concerto for Orchestra". Koussevitsky's Boston Symphony Orchestra premièred the work in December 1944 to highly positive reviews. "Concerto for Orchestra" quickly became Bartók's most popular work, although he did not live to see its full impact. In 1944, he was also commissioned by Yehudi Menuhin to write a Sonata for Solo Violin. In 1945, Bartók composed his Piano Concerto No. 3, a graceful and almost neo-classical work, as a surprise 42nd birthday present for Ditta, but he died just over a month before her birthday, with the scoring not quite finished. He had sketched his Viola Concerto, but had barely started the scoring at his death.
Béla Bartók died at age 64 in a hospital in New York City from complications of leukemia (specifically, of secondary polycythemia) on September 26, 1945. His funeral was attended by only ten people. Among them were his wife Ditta, their son Péter, and his pianist friend György Sándor (anon. 2006).
Bartók's body was initially interred in Ferncliff Cemetery in Hartsdale, New York. During the final year of communist Hungary in the late 1980s, the Hungarian government, along with his two sons, Béla III and Péter, requested that his remains be exhumed and transferred back to Budapest for burial, where Hungary arranged a state funeral for him on July 7, 1988. He was reinterred at Budapest's Farkasréti Cemetery, next to the remains of Ditta, who died in 1982, the year after his centenary (Chalmers 1995, 214).
The Third Piano Concerto was nearly finished at his death. For his Viola Concerto, Bartók had completed only the viola part and sketches of the orchestral part. Both works were later completed by his pupil, Tibor Serly. György Sándor was the soloist in the first performance of the Third Piano Concerto on February 8, 1946. Ditta Pásztory-Bartók later played and recorded it. The Viola Concerto was revised and polished in the 1990s by Bartók's son, Peter; this version may be closer to what Bartók intended (Chalmers 1995, 210).
Concurrently, Peter Bartók, in association with Nelson Dellamaggiore, worked to re-print and revise past editions of the Third Piano Concerto (Somfai 1996).
Compositions.
Bartók's music reflects two trends that dramatically changed the sound of music in the 20th century: the breakdown of the diatonic system of harmony that had served composers for the previous two hundred years (Griffiths 1978, 7); and the revival of nationalism as a source for musical inspiration, a trend that began with Mikhail Glinka and Antonín Dvořák in the last half of the 19th century (Einstein 1947, 332). In his search for new forms of tonality, Bartók turned to Hungarian folk music, as well as to other folk music of the Carpathian Basin and even of Algeria and Turkey; in so doing he became influential in that stream of modernism which exploited indigenous music and techniques (Botstein [n.d.], §6).
One characteristic style of music is his Night music, which he used mostly in slow movements of multi-movement ensemble or orchestral compositions in his mature period. It is characterised by "eerie dissonances providing a backdrop to sounds of nature and lonely melodies" (Schneider 2006, 84). An example is the third movement "Adagio" of his "Music for Strings, Percussion and Celesta".
His music can be grouped roughly in accordance with the different periods in his life.
Youth: Late-Romanticism (1890–1902).
The works of his youth are of a late-Romantic style. Between 1890 and 1894 (nine to 13 years of age) he wrote 31 pieces with corresponding opus numbers. He started numbering his works anew with "opus 1" in 1894 with his first large scale work, a piano sonata. Up to 1902, Bartók wrote in total 74 works which can be considered in Romantic style. Most of these early compositions are either scored for piano solo or include a piano. Additionally, there is some chamber music for strings.
New influences (1903–11).
Under the influence of Richard Strauss (among other works "Also sprach Zarathustra") (Stevens 1993, 15–17), Bartók composed in 1903 Kossuth, a symphonic poem in ten tableaux. In 1904 followed his "Rhapsody for piano and orchestra" which he numbered opus 1 again, marking it himself as the start of a new era in his music. An even more important occurrence of this year was his overhearing the eighteen-year-old nanny Lidi Dósa from Transylvania sing folk songs, sparking Bartók's lifelong dedication to folk music (Stevens 1993, 22). When criticised for not composing his own melodies Bartók pointed out that Molière and Shakespeare mostly based their plays on well-known stories too. Regarding the incorporation of folk music into art music he said:
The question is, what are the ways in which peasant music is taken over and becomes transmuted into modern music? We may, for instance, take over a peasant melody unchanged or only slightly varied, write an accompaniment to it and possibly some opening and concluding phrases. This kind of work would show a certain analogy with Bach's treatment of chorales. ... Another method ... is the following: the composer does not make use of a real peasant melody but invents his own imitation of such melodies. There is no true difference between this method and the one described above. ... There is yet a third way ... Neither peasant melodies nor imitations of peasant melodies can be found in his music, but it is pervaded by the atmosphere of peasant music. In this case we may say, he has completely absorbed the idiom of peasant music which has become his musical mother tongue. (Bartók 1931/1976, 341–44)
Bartók became first acquainted with Debussy's music in 1907 and regarded his music highly. In an interview in 1939 Bartók said
Debussy's great service to music was to reawaken among all musicians an awareness of harmony and its possibilities. In that, he was just as important as Beethoven, who revealed to us the possibilities of progressive form, or as Bach, who showed us the transcendent significance of counterpoint. Now, what I am always asking myself is this: is it possible to make a synthesis of these three great masters, a living synthesis that will be valid for our time? (Moreux 1953, 92) Debussy's influence is present in the Fourteen Bagatelles (1908). These made Ferruccio Busoni exclaim 'At last something truly new!' (Bartók, 1948, 2:83). Until 1911, Bartók composed widely differing works which ranged from adherence to romantic-style, to folk song arrangements and to his modernist opera "Bluebeard's Castle". The negative reception of his work led him to focus on folk music research after 1911 and abandon composition with the exception of folk music arrangements (Gillies 1993, 404; Stevens 1964, 47–49).
New inspiration and experimentation (1916–21).
His pessimistic attitude towards composing was lifted by the stormy and inspiring contact with Klára Gombossy in the summer of 1915 (Gillies 1993, 405). This interesting episode in Bartók's life remained hidden until it was researched by Denijs Dille between 1979 and 1989 (Dille 1990, 257–77). Bartók started composing again, including the Suite for piano opus 14 (1916), and "The Miraculous Mandarin" (1918) and he completed "The Wooden Prince" (1917).
Bartók felt the result of World War I as a personal tragedy (Stevens 1993, 3). Many regions he loved were severed from Hungary: Transylvania, the Banat where he was born, and Pozsony where his mother lived. Additionally, the political relations between Hungary and the other successor states to the Austro-Hungarian empire prohibited his folk music research outside of Hungary (Somfai, 1996, 18). Bartók also wrote the noteworthy "Eight Improvisations on Hungarian Peasant Songs" in 1920, and the sunny "Dance Suite" in 1923, the year of his second marriage.
"Synthesis of East and West" (1926–45).
In 1926, Bartók needed a significant piece for piano and orchestra with which he could tour in Europe and America. In the preparation for writing his First Piano Concerto, he wrote his Sonata, "Out of Doors", and "Nine Little Pieces", all for solo piano (Gillies 1993, 173). He increasingly found his own voice in his maturity. The style of his last period—named "Synthesis of East and West" (Gillies 1993, 189)—is hard to define let alone to put under one term. In his mature period, Bartók wrote relatively few works but most of them are large-scale compositions for large settings. Only his voice works have programmatic titles and his late works often adhere to classical forms.
Among his masterworks are all the six string quartets (1908, 1917, 1927, 1928, 1934, and 1939), the "Cantata Profana" (1930, Bartók declared that this was the work he felt and professed to be his most personal "credo", Szabolcsi 1974, 186), the "Music for Strings, Percussion and Celesta" (1936), the "Concerto for Orchestra" (1943) and the Third Piano Concerto (1945).
Bartók also made a lasting contribution to the literature for younger students: for his son Péter's music lessons, he composed "Mikrokosmos", a six-volume collection of graded piano pieces.
Musical analysis.
Paul Wilson lists as the most prominent characteristics of Bartók's music from late 1920s onwards the influence of the Carpathian basin and European art music, and his changing attitude toward (and use of) tonality, but without the use of the traditional harmonic functions associated with major and minor scales (Wilson 1992, 2–4).
Although Bartók claimed in his writings that his music was always tonal, he rarely uses the chords or scales of tonality, and so the descriptive resources of tonal theory are of limited use. George Perle (1955) and Elliott Antokoletz (1984) focus on alternative methods of signaling tonal centers, via axes of inversional symmetry. Others view Bartók's axes of symmetry in terms of atonal analytic protocols. Richard Cohn (1988) argues that inversional symmetry is often a byproduct of another atonal procedure, the formation of chords from transpositionally related dyads. Atonal pitch-class theory also furnishes the resources for exploring polymodal chromaticism, projected sets, privileged patterns, and large set types used as source sets such as the equal tempered twelve tone aggregate, octatonic scale (and alpha chord), the diatonic and "heptatonia secunda" seven-note scales, and less often the whole tone scale and the primary pentatonic collection (Wilson 1992, 24–29).
He rarely used the simple aggregate actively to shape musical structure, though there are notable examples such as the second theme from the first movement of his "Second Violin Concerto", commenting that he "wanted to show Schoenberg that one can use all twelve tones and still remain tonal" (Gillies 1990, 185). More thoroughly, in the first eight measures of the last movement of his "Second Quartet", all notes gradually gather with the twelfth (G♭) sounding for the first time on the last beat of measure 8, marking the end of the first section. The aggregate is partitioned in the opening of the "Third String Quartet" with C♯–D–D♯–E in the accompaniment (strings) while the remaining pitch classes are used in the melody (violin 1) and more often as 7–35 (diatonic or "white-key" collection) and 5–35 (pentatonic or "black-key" collection) such as in no. 6 of the "Eight Improvisations". There, the primary theme is on the black keys in the left hand, while the right accompanies with triads from the white keys. In measures 50–51 in the third movement of the "Fourth Quartet", the first violin and 'cello play black-key chords, while the second violin and viola play stepwise diatonic lines (Wilson 1992, 25). On the other hand, from as early as the Suite for piano, op. 14 (1914), he occasionally employed a form of serialism based on compound interval cycles, some of which are maximally distributed, multi-aggregate cycles (Martins 2004; Gollin 2007).
Ernő Lendvaï (1971) analyses Bartók's works as being based on two opposing tonal systems, that of the acoustic scale and the axis system, as well as using the golden section as a structural principle.
Milton Babbitt, in his 1949 critique of Bartók's string quartets, criticized Bartók for using tonality and non tonal methods unique to each piece. Babbitt noted that "Bartók's solution was a specific one, it cannot be duplicated" (Babbitt 1949, 385). Bartók's use of "two organizational principles"—tonality for large scale relationships and the piece-specific method for moment to moment thematic elements—was a problem for Babbitt, who worried that the "highly attenuated tonality" requires extreme non-harmonic methods to create a feeling of closure (Babbitt 1949, 377–78).
Catalogues and opus numbers.
The cataloguing of Bartók's works is somewhat complex. Bartók assigned opus numbers to his works three times, the last of these series ending with the Sonata for Violin and Piano No. 1, Op. 21 in 1921. He ended this practice because of the difficulty of distinguishing between original works and ethnographic arrangements, and between major and minor works. Since his death, three attempts—two full and one partial—have been made at cataloguing. The first, and still most widely used, is András Szőllősy's chronological Sz. numbers, from 1 to 121. Denijs Dille subsequently reorganised the juvenilia (Sz. 1–25) thematically, as DD numbers 1 to 77. The most recent catalogue is that of László Somfai; this is a chronological index with works identified by BB numbers 1 to 129, incorporating corrections based on the Béla Bartók Thematic Catalogue.

</doc>
<doc id="4528" url="http://en.wikipedia.org/wiki?curid=4528" title="Bill Haley">
Bill Haley

William John Clifton Haley (; July 6, 1925 – February 9, 1981), known as Bill Haley, was an American rock and roll musician. He is credited by many with first popularizing this form of music in the early 1950s with his group Bill Haley & His Comets (inspired by Halley's Comet) and million selling hits such as "Rock Around the Clock", "See You Later, Alligator", "Shake, Rattle and Roll", "Rocket 88", "Skinny Minnie", and "Razzle Dazzle". He has sold over 25 million records worldwide.
Biography.
Early life and career.
Bill Haley was born July 6, 1925 in Highland Park, Michigan as William John Clifton Haley. Because of the effects of the Great Depression on the Detroit area, his father moved the family to Boothwyn, near Chester, Pennsylvania, when Bill was seven years old. Haley's father William Albert Haley was from Kentucky and played the banjo and mandolin, and his mother, Maude Green, who was originally from Ulverston in Cumbria, England, was a technically accomplished keyboardist with classical training. Haley told the story that when he made a simulated guitar out of cardboard, his parents bought him a real one.
The anonymous sleeve notes accompanying the 1956 Decca album "Rock Around The Clock" describe Haley's early life and career thus: "Bill got his first professional job at the age of 13, playing and entertaining at an auction for the fee of $1 a night. Very soon after this he formed a group of equally enthusiastic youngsters and managed to get quite a few local bookings for his band."
The sleeve notes continue: "When Bill Haley was fifteen [c. 1940] he left home with his guitar and very little else and set out on the hard road to fame and fortune. The next few years, continuing this story in a fairy-tale manner, were hard and poverty-stricken, but crammed full of useful experience. Apart from learning how to exist on one meal a day and other artistic exercises, he worked at an open-air park show, sang and yodelled with any band that would have him, and worked with a traveling medicine show. Eventually he got a job with a popular group known as the "Down Homers" while they were in Hartford, Connecticut. Soon after this he decided, as all successful people must decide at some time or another, to be his own boss again – and he has been that ever since.' [Note: these notes fail to account for his early band, known as the Four Aces of Western Swing. During the 1940s Haley was considered one of the top cowboy yodelers in America as "Silver Yodeling Bill Haley".]
The sleeve notes conclude: "For six years Bill Haley was a musical director of Radio Station WPWA in Chester, Pennsylvania, and led his own band all through this period. It was then known as Bill Haley's Saddlemen, indicating their definite leaning toward the tough Western style. They continued playing in clubs as well as over the radio around Philadelphia, and in 1951 made their first recordings on Ed Wilson's Keystone Records in Philadelphia." On June 14, 1951 the Saddlemen recorded a cover of "Rocket 88". Many Rock historians regard this song, with its fusion of African-American R&B and Haley's country swing, as the very first "Rock and Roll" recording. This pre-dated Haley's "Rock Around The Clock" recording by over a year, and its chart success by three years!
Bill Haley & His Comets.
During the Labor Day weekend in 1952, the Saddlemen were renamed Bill Haley with Haley's Comets (inspired by the supposedly official pronunciation of Halley's Comet, a name suggested by the DJ Alan Freed), and in 1953, Haley's recording of "Crazy Man, Crazy" (co-written by him and his bass player, Marshall Lytle, although Lytle would not receive credit until 2001) became the first rock and roll song to hit the American charts, peaking at no.15 on Billboard and no.11 on Cash Box. Soon after, the band's name was revised to Bill Haley & His Comets.
In 1953, a song called "Rock Around the Clock" was written for Haley. He was unable to record it until April 12, 1954. Initially, it was relatively unsuccessful, peaking at no. 23 on the "Billboard" pop singles chart and staying on the charts for only one week.
Haley soon scored a major worldwide hit with a cover version of Big Joe Turner's "Shake, Rattle and Roll", which went on to sell a million copies and was the first ever rock 'n' roll song to enter the British singles charts in December 1954, becoming a Gold Record. He retained elements of the original, but threw some country music aspects into the song (specifically, Western Swing) and cleaned up the lyrics. Haley and his band were important in launching the music known as "Rock and Roll" to a wider, mostly white audience after a period of it being considered an underground genre.
When "Rock Around the Clock" appeared as the theme song of the 1955 film "Blackboard Jungle" starring Glenn Ford, it soared to the top of the American "Billboard" chart for eight weeks. The single is commonly used as a convenient line of demarcation between the "rock era" and the music industry that preceded it. "Billboard" separated its statistical tabulations into 1890–1954 and 1955–present. After the record rose to number one, Haley was quickly given the title "Father of Rock and Roll" by the media, and by teenagers who had come to embrace the new style of music. With the song's success, the age of rock music began overnight and instantly ended the dominance of the jazz and pop standards performed by Frank Sinatra, Jo Stafford, Perry Como, Bing Crosby and others.
Success came at somewhat of a price as the new music confused and horrified most people over the age of 30, leading to Cold War-fueled suspicion that rock and roll was part of a communist plot to corrupt the minds of American teenagers. FBI chief J. Edgar Hoover attempted to dig up incriminating material on Bill Haley, who took to carrying a gun with him on tours for his own safety.
"Rock Around the Clock" was the first record ever to sell over one million copies in both Britain and Germany and, in 1957, Haley became the first major American rock singer to tour Europe. Haley continued to score hits throughout the 1950s such as "See You Later, Alligator" and he starred in the first rock and roll musical films "Rock Around the Clock" and "Don't Knock the Rock", both in 1956. Haley was already 30 years old and so he was soon eclipsed in the United States by the younger, sexier Elvis Presley, but continued to enjoy great popularity in Latin America, Europe and Australia during the 1960s.
Bill Haley and the Comets performed "Rock Around the Clock" on the Texaco Star Theater hosted by Milton Berle on May 31, 1955 on NBC in an a cappella and lip-synched version. Berle predicted that the song would go no. 1: "A group of entertainers who are going right to the top." Berle also sang and danced to the song which was performed by the entire cast of the show. This was one of the earliest nationally televised performances by a rock and roll band and provided the new musical genre called "rock and roll" a much wider audience.
Bill Haley and the Comets were the first rock and roll act to appear on the iconic American musical variety series the Ed Sullivan Show on Sunday, August 7, 1955 on CBS in a broadcast that originated from the Shakespeare Festival Theater in Hartford, Connecticut. They performed a live version of "Rock Around the Clock" with Franny Beecher on lead guitar and Dick Richards on drums. The band made their second appearance on the show on Sunday, April 28, 1957 performing the songs "Rudy's Rock" and "Forty Cups of Coffee".
Bill Haley and the Comets appeared on "American Bandstand" hosted by Dick Clark on ABC twice in 1957, on the prime time show October 28, 1957 and on the regular daytime show on November 27, 1957. The band also appeared on Dick Clark's "Saturday Night Beechnut Show", also known as "The Dick Clark Show", a primetime TV series from New York on March 22, 1958 during the first season and on February 20, 1960, performing "Rock Around the Clock", "Shake, Rattle, and Roll", and "Tamiami".
Death and legacy.
A self-admitted alcoholic (as indicated in a 1974 radio interview for the BBC), Haley fought a battle with alcohol into the 1970s. Nonetheless, he and his band continued to be a popular touring act, benefiting from a 1950s nostalgia movement that began in the late 1960s and the signing of a lucrative record deal with the European Sonet label. After performing for Queen Elizabeth II at the Royal Variety Performance on November 10, 1979, Haley made his final performances in South Africa in May and June 1980. Before the South African tour, he was diagnosed with a brain tumor, and a planned tour of Germany in the autumn of 1980 was cancelled.
The October 25, 1980 edition of the German paper "Bild" reported that Haley had a brain tumor. It quoted British manager Patrick Maylan as saying that Haley "had taken a fit and went over the seat. He didn't recognize anyone anymore" after being taken to his home in Beverly Hills. It also reported that a doctor at the clinic where Haley had been taken said, "The tumor can't be operated on anymore."
The "Berliner Zeitung" reported a few days later that Haley had collapsed after a performance in Texas and been taken to the hospital in his home town of Harlingen, Texas. However, this account is questionable as Bill Haley did not perform in the United States at all in 1980.
Despite his ill health, Haley began compiling notes for possible use as a basis for either a biographical film based on his life, or a published autobiography (accounts differ), and there were plans for him to record an album in Memphis, Tennessee, when the brain tumor began affecting his behavior and he went back to his home in Harlingen, where he died early in the morning of February 9, 1981.
Martha, Bill's widow, who was with him in these troubling times, denies he had a brain tumor as does his old, very close friend, Hugh McCallum. Martha and friends related that Bill did not want to go on the road any more and that ticket sales for that planned tour of Germany in the fall of 1980 were slow. According to McCallum, "It's my unproven gut feeling that that [the brain tumor] was said to curtail talks about the tour and play the sympathy card."
It was obvious that his drinking problem was getting worse. By this time, Bill and Martha fought all the time and she told him to stop drinking or move out. He then did move out into a room in their pool house. Martha still took care of him and sometimes he would come in the house to eat, but he ate very little. "There were days we never saw him," said his daughter Martha Maria.
In addition to Haley's drinking problems, it had also become obvious that he was having serious mental problems as well; Martha Maria said that, "It was like sometimes he was drunk even when he wasn't drinking." After he'd been jailed by the Harlingen Police, Martha had the judge put Haley in the hospital where he was seen by a psychiatrist who said Bill's brain was overproducing a chemical, like adrenaline. The doctor prescribed a medication to stop the overproduction but said Bill would have to stop drinking. Martha said, "This is pointless." She took him home, however, fed him and gave him his first dose. As soon as he felt better, he went back out to his room in the pool house and the downward spiral continued until his death on February 9, 1981.
Haley's death certificate listed "Natural causes: Most likely heart attack" as the 'Immediate Cause' of death. The next lines, 'Due to, or as a consequence Of" were blank.
Haley made a succession of bizarre, mostly monologue late-night phone calls to friends and relatives in which he seemed incoherently drunk or ill. Haley's first wife has been quoted as saying, "He would call and ramble and dwell on the past, his mind was really warped." A belligerent phone call to a business associate was taped and gives evidence of Haley's troubled state of mind.
Media reports immediately following his death indicated Haley displayed deranged and erratic behavior in his final weeks, although beyond a biography by John Swenson, released a year later, which described Haley painting the windows of his home black, there is little information about Haley's final days.
After a small funeral service, Haley was cremated, although his widow Martha won't say what was done with the ashes.
Haley was posthumously inducted into the Rock and Roll Hall of Fame in 1987. His son Pedro represented him at the ceremony. He has a star on the Hollywood Walk of Fame. The Comets were separately inducted into the Hall of Fame as a group in 2012, after a rule change allowed the induction of backing groups (the Comets were mass-inducted alongside several other groups such as Hank Ballard's Midnighters and Gene Vincent's Blue Caps).
Songwriters Tom Russell and Dave Alvin addressed Haley's demise in musical terms with "Haley's Comet" on Alvin's 1991 album "Blue Blvd." Dwight Yoakam sang backup on the tribute.
Surviving members of the 1954-55 contingent of Haley's Comets reunited in the late 1980s and continued to perform for many years around the world. They released a concert DVD in 2004 on Hydra Records, played the Viper Room in West Hollywood in 2005, and performed at Dick Clark's American Bandstand Theater in Branson, Missouri beginning in 2006–07. As of 2014, only two members of this particular contingent are still alive (Joey Ambrose and Dick Richards), but they continue to perform in Branson and in Europe. At least two other groups also continue to perform in North America under the Comets name as of 2014.
In March 2007, the Original Comets pre-opened the Bill Haley Museum in Munich, Germany. On October 27, 2007, ex-Comets guitar player Bill Turner opened the Bill Haley Museum for the public.
Two of Haley's children, Bill Haley Jr. and Gina Haley, are themselves musicians and have in recent years recorded albums of their father's music and headlined tribute musical shows.
Asteroid.
In February 2006, the International Astronomical Union announced the naming of asteroid 79896 Billhaley to mark the 25th anniversary of Bill Haley's death.
Children.
Married three times, Bill Haley had at least four children. John W. Haley, his eldest son, wrote "Sound and Glory", a biography of Haley, while his youngest daughter, Gina Haley, is a professional musician based in Texas. Scott Haley is an athlete. His youngest son Pedro is also a musician.
He also had a daughter, Martha Maria, from his last marriage with Martha Velasco.
Bill Haley Jr. (Haley's second son and first with Joan Barbara "Cuppy" Haley-Hahn) publishes a regional business magazine in Southeastern Pennsylvania ("Route 422 Business Advisor"). He sings and plays guitar with a band called "Bill Haley and the Satellites," and released a CD in 2011.
 He also has occasionally appeared with the "Original Comets" at the Bubba Mac Shack in Somers Point, New Jersey from 2004–2011, and at the Twin Bar re-dedication ceremony in Gloucester City, New Jersey, in 2007. In February 2011, he formed a tribute band "Bill Haley Jr. and the Comets," performing his father's music and telling the stories behind the songs.
Film portrayals.
Unlike his contemporaries, Bill Haley has rarely been portrayed on screen. Following the success of "The Buddy Holly Story" in 1978, Haley expressed interest in having his life story committed to film, but this never came to fruition. In the 1980s and early 1990s, numerous media reports emerged that plans were underway to do a biopic based upon Haley's life, with Beau Bridges, Jeff Bridges and John Ritter all at one point being mentioned as actors in line to play Haley (according to "Goldmine Magazine", Ritter attempted to buy the film rights to "Sound and Glory").
Bill Haley has also been portrayed – not always in a positive light – in several "period" films:
In March 2005, the British network Sky TV reported that Tom Hanks was planning to produce a biopic on the life of Bill Haley, with production tentatively scheduled to begin in 2006. However this rumor was quickly debunked by Hanks.
Discography.
Before the formation of Bill Haley and the Saddlemen, which later became the Comets, Haley released several singles with other groups. Dates are approximate due to lack of documentation.
As Bill Haley and the Four Aces of Western Swing:
1948
1949
As Johnny Clifton and His String Band:
1950
Many Haley discographies list two 1946 recordings by the Down Homers released on the Vogue Records label as featuring Haley. Haley historian Chris Gardner, as well as surviving members of the group, have confirmed that the two singles: "Out Where the West Winds Blow"/"Who's Gonna Kiss You When I'm Gone" (Vogue R736) and "Boogie Woogie Yodel"/"Baby I Found Out All About You" (Vogue R786) do not feature Haley. However, the tracks were nonetheless included in the compilation box set "Rock 'n' Roll Arrives" released by Bear Family Records in 2006.
Unreleased recordings.
Bill Haley recorded prolifically during the 1940s, often at the radio stations where he worked, or in formal studio settings. Virtually none of these recordings were ever released. Liner notes for a 2003 CD release by Hydra Records entitled "Bill Haley and Friends Vol. 2: The Legendary Cowboy Recordings" reveal that several additional Cowboy label single releases were planned for the Four Aces, but this never occurred.
A number of previously unreleased Haley country-western recordings from the 1946–1950 period began to emerge near the end of Haley's life, some of which were released by the Arzee label, with titles such as "Yodel Your Blues Away" and "Rose of My Heart." Still more demos, alternate takes, and wholly unheard-before recordings have been released since Haley's death. Notable examples of such releases include the albums "Golden Country Origins" by Grassroots Records of Australia and "Hillbilly Haley" by the British label, Rollercoaster, as well as the aforementioned German release by Hydra Records. In 2006, Bear Family Records of Germany released what is considered to be the most comprehensive (yet still incomplete) collection of Haley's 1946–1950 recordings as part of its Haley box set "Rock n' Roll Arrives".
Compositions.
Bill Haley's compositions included "Four Leaf Clover Blues", "Crazy Man, Crazy", "What'Cha Gonna Do", "Fractured", "Live It Up", "Farewell, So Long, Goodbye", "Real Rock Drive", "Rocking Chair on the Moon", "Sundown Boogie", "Green Tree Boogie", "Tearstains on My Heart", "Down Deep in My Heart", "Straight Jacket", "Birth of the Boogie", "Two Hound Dogs", "Rock-A-Beatin' Boogie", "Hot Dog Buddy Buddy", "R-O-C-K", "Rudy's Rock", "Calling All Comets", "Tonight's the Night", "Hook, Line and Sinker", "Sway with Me", "Paper Boy (On Main Street U.S.A.)", "Skinny Minnie", "B.B. Betty", "Eloise", "Whoa Mabel!", "Vive la Rock and Roll", "I've Got News For You", "So Right Tonight", "Ana Maria", "Yucatán Twist", "Football Rock and Roll", and "Chick Safari".
He also wrote or co-wrote songs for other artists such as "Calypso Rock" for Dave Day and The Red Coats on Kapp in 1956, "A.B.C. Rock" and "Rocky the Rockin' Rabbit" for Sally Starr on Arcade, and "(Ya Gotta) Sing For the Ladies" and "Butterfly Love" for Ginger Shannon and Johnny Montana in 1960 on Arcade as well as the unissued "I'm Shook" in 1958.
Quotations.
"NME" – October 1955
"NME" – January 1957
Grammy Hall of Fame.
The following recording by Bill Haley was inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least 25 years old and that have "qualitative or historical significance."

</doc>
<doc id="4529" url="http://en.wikipedia.org/wiki?curid=4529" title="Northern bobwhite">
Northern bobwhite

The northern bobwhite, Virginia quail or (in its home range) bobwhite quail ("Colinus virginianus") is a ground-dwelling bird native to the United States, Mexico, and the Caribbean. It is a member of the group of species known as New World quails (Odontophoridae). They were initially placed with the Old World quails in the pheasant family (Phasianidae), but are not particularly closely related. The name "bobwhite" derives from its characteristic whistling call. Despite its secretive nature, the northern bobwhite is one of the most familiar quails in eastern North America because it is frequently the only quail in its range. There are 21 subspecies of northern bobwhite, and many of the birds are hunted extensively as game birds. One subspecies, the masked bobwhite ("Colinus virginianus ridgewayi"), is listed as endangered with wild populations located in Sonora, Mexico, and a reintroduced population in Buenos Aires National Wildlife Refuge in southern Arizona.
Appearance.
This is a moderately-sized quail and is the only small galliform native to eastern North America. The bobwhite can range from in length with a wingspan. As indicated by body mass, weights increase in birds found further north, as corresponds to Bergmann's rule. In Mexico, northern bobwhites weigh from whereas in the north they average and large males can attain as much as . Among standard measurements, the wing chord is , the tail is the culmen is and the tarsus is . It has the typical chunky, rounded shape of a quail. The bill is short, curved and brown-black in color. This species is sexually dimorphic. Males have a white throat and brow stripe bordered by black. The overall rufous plumage has gray mottling on the wings, white scalloped stripes on the flanks, and black scallops on the whitish underparts. The tail is gray. Females are similar but are duller overall and have a buff throat and brow without the black border. Both sexes have pale legs and feet.
Taxonomy.
Subspecies.
There are twenty-one recognized subspecies in 3 groups. 1 subspecies is extinct.
Diet.
The northern bobwhite's diet consists of plants and small bugs, like snails, grasshoppers, and potato beetles. Plant sources include grass seeds, wild berries, partridge peas, and cultivated grains. It forages on the ground in open areas with some spots of taller vegetation.
Habitat.
The northern bobwhite can be found year-round in agricultural fields, grassland, open woodland areas, roadsides and wood edges. Its range covers the southeastern quadrant of the United States from the Great Lakes and southern Minnesota east to Pennsylvania and southern Massachusetts, and extending west to southern Nebraska, Kansas, Oklahoma and all but westernmost Texas. It is absent from the southern tip of Florida and the highest elevations of the Appalachian Mountains, but occurs in eastern Mexico and in Cuba. Isolated populations have been introduced in Oregon and Washington. The northern bobwhite has also been introduced to New Zealand.
Vocalizations.
The clear whistle "bob-WHITE" or "bob-bob-WHITE" call is very recognizable. The syllables are slow and widely spaced, rising in pitch a full octave from beginning to end. Other calls include lisps, peeps, and more rapidly whistled warning calls.
Behavior.
Like most game birds, the northern bobwhite is shy and elusive. When threatened, it will crouch and freeze, relying on camouflage to stay undetected, but will flush into low flight if closely disturbed. It is generally solitary or paired early in the year, but family groups are common in the late summer and winter roosts may have two dozen or more birds in a single covey.
Reproduction.
The species is generally monogamous, but there is some evidence of polygamy. Both parents incubate a brood for 23 to 24 days, and the precocial young leave the nest shortly after hatching. Both parents lead the young birds to food and care for them for 14 to 16 days until their first flight. A pair may raise one or two broods annually, with 12 to 16 eggs per clutch.
Introduced populations.
European Union.
Northern bobwhite were introduced into Italy in 1927, and are reported in the plains and hills in the northwest of the country. Other reports from the EU are in France, Spain, and Yugoslavia. As bobwhites are highly productive and popular aviary subjects, it is reasonable to expect other introductions have been made in other parts of the EU, especially in the UK and Ireland, where game-bird breeding, liberation, and naturalisation are almost national-pastimes.
New Zealand.
From 1898 to 1902, some 1300 birds were imported from America and released in many parts of the North and South Islands, from Northland to Southland. The bird was briefly on the Nelson game shooting licence, but: "It would seem that the committee was a little too eager in placing these Quail on the licence, or the shooters of the day were over-zealous and greedy in their bag limits, for the Virginian Quail, like the Mountain Quail were soon thing of the past." The Taranaki (Acclimatisation) Society released a few in 1900 and was confidant that in a year or two they might offer good sport; two years later, broods were reported and the species was said to be "steadily increasing"; but after another two years they seemed "to have disappeared" and that was the end of them. The Otago (Acclimatisation) Society imported more in 1948, but these releases "did no good". After 1923, no more genuinely wild birds were sighted until 1952, when a small population was found North-West of Wairoa in the Ruapapa Road area. Since then bobwhite have been found at several localities around Waikaremoana, in farmland, open bush and along roadsides.
More birds have been imported into New Zealand by private individuals since the 1990s and a healthy captive population is now held by backyard aviculturalists and have been found to be easily cared for, bred, and are popular for their "song" and "good-looks". A larger proportion of the national captive population belong to a few game preserves and game bird breeders. Though the birds would be self-sustaining in the wild if they were protected; it is tricky to guess what the effect of an annual population subsidy and hunting has on any of the original populations from the Acclimatisation Society releases. It would be fair to suggest most birds in the wild are no more than one generation from captive stock.
An albino hen was present in a covey in Bayview, Hawkes Bay for a couple of seasons sometime around 2000.
Captive populations.
Bobwhites are popular throughout the world, with healthy captive populations everywhere where bird-keeping is enjoyed. Certain countries/states require permits and record keeping, as the possibility that an introduced population may compete with or spread diseases with native quail is a real threat.
Captivity.
Housing.
Bobwhites are generally compatible with most parrots, softbills and doves. This species should, however, be the only ground-dwelling species in the aviary. Most individuals will do little damage to finches, but one should watch that nests are not being crushed when the species perches at night. Single pairs are preferred, unless the birds have been raised together as a group since they were chicks. Some fighting will occur between cocks at breeding time. One cock may be capable of servicing several hens at once, but the fertility seems to be highest in the eggs from the "preferred" hen. Aviary style is a compromise between what is tolerated by the bird and what is best for the bird. Open parrot-style type aviaries may be used, but some birds will remain flighty and shy in this situation. In a planted aviary, this species will generally settle down to become quite tame and confiding. Parents with chicks will roost on the ground, forming a circular arrangement, with heads facing outwards. In the early morning and late afternoon, the cock will utter its call, which, although not loud, carries well and may offend noise-sensitive neighbors. Most breeding facilities keep birds in breeding groups, on wire up off the ground. This keeps the birds clean and generally avoids diseases and parasites which can devastate a covey. Cages with mesh floors for pairs and trios are also employed, but usually where there is a photo-period manipulation to keep birds breeding through winter.
Feeding.
In the wild the bobwhite feeds on a variety of seeds of weeds, grasses, as well as insects. These are generally collected on the ground or from low foliage. Birds in the aviary are easily catered for with a commercial small seed mix (finch, budgerigar, or small parrot mix) when supplemented with greenfeed. Live food is not usually necessary for breeding, but will be ravenously accepted. High protein foods such as chicken grower crumble are more convenient to supply and will be useful for the stimulation of breeding birds. Extra calcium is required, especially by laying hens; it can be supplied in the form of shell grit, or cuttlefish bone.
Breeding.
In an open aviary hens will lay all over the show if a nesting site and privacy are not provided. Hens that do this may lay upwards of 80 eggs in a season which can be taken for artificial incubation - and chicks hand raised. Otherwise hens with nesting cover, that do make a nest (on the ground) will build up 8–25 eggs in a clutch, with eggs being laid daily.
Mutations and hybrids.
Some captive bobwhite hybrids recorded are between blue quail (scaled quail), Gambel's quail, California quail, mountain quail and it has long been suggested that there are Japanese quail hybrids being bred commercially; there is a distinct lack of photographic proof to substantiate this. Inter-subspecific hybrids have been common.
Several mutations have long been established, including Californian Jumbo, Wisconsin Jumbo, Northern Giant, Albino, Snowflake, Blonde, Fawn, Barred, Silver, and Red.
Similar species.
The Central American spot-bellied bobwhite looks very similar, but lacks black facial coloration. The Asian rain quail is larger in size and has a black breast.

</doc>
<doc id="4531" url="http://en.wikipedia.org/wiki?curid=4531" title="Bipolar disorder">
Bipolar disorder

Bipolar disorder, also known as bipolar affective disorder (and originally called manic-depressive illness), is a mental disorder characterized by periods of elevated mood and periods of depression. The elevated mood is significant and is known as mania or hypomania depending on the severity or whether there is psychosis. During mania an individual feels or acts abnormally happy, energetic, or irritable. They often make poorly thought out decisions with little regard to the consequences. The need for sleep is usually reduced. During periods of depression there may be crying, poor eye contact with others, and a negative outlook on life. The risk of suicide among those with the disorder is high at greater than 6% over 20 years, while self harm occurs in 30–40%. Other mental health issues such as anxiety disorder and drug misuse are commonly associated.
The cause is not clearly understood, but both genetic and environmental factors play a role. Many genes of small effect contribute to risk. Environmental factors include long term stress and a history of childhood abuse. It is divided into bipolar I disorder if there is at least one manic episode and bipolar II disorder if there are at least one hypomanic episode and one major depressive episode. In those with less severe symptoms of a prolonged duration the condition cyclothymic disorder may be present. If due to drugs or medical problems it is classified separately. Other conditions that may present in a similar manner include: drug misuse, personality disorders, attention deficit hyperactivity disorder, and schizophrenia as well as a number of medical conditions.
Treatment commonly includes psychotherapy and medications such as mood stabilizers or antipsychotics. Examples of mood stabilizers that are commonly used include: lithium and anticonvulsants. Treatment in hospital against a person's wishes may be required at times as people may be at risk to themselves or others yet refuse treatment. Severe behavioural problems may be managed with short term benzodiazepines or antipsychotics. In periods of mania it is recommended that antidepressants be stopped. If antidepressants are used for periods of depression they should be used with a mood stabilizer. Electroconvulsive therapy may be helpful in those who do not respond to other treatments. If treatments is stopped it is recommended that this be done slowly. Most people have social, financial or work related problem due to the disorder. These difficulties occur a quarter to a third of the time on average. The risk of death from natural causes such as heart disease is twice that of the general population. This is due to poor lifestyle choices and the side effects from medications.
About 3% of people in the United States have bipolar disorder at some point in their life. Lower rates of around 1% are found in other countries. The most common age at which symptoms begin is 25. Rates appear to be similar in males as females. The economic costs of the disorder has been estimated at $45 billion for the United States in 1991. A large proportion of this was related to a higher number of missed work days estimated at 50 per year. People with bipolar disorder often face problems with social stigma.
Signs and symptoms.
Mania is the defining feature of bipolar disorder, and can occur with different levels of severity. With milder levels of mania, known as hypomania, individuals appear energetic, excitable, and may be highly productive. As mania worsens, individuals begin to exhibit erratic and impulsive behavior, often making poor decisions due to unrealistic ideas about the future, and sleep very little. At the most severe level, manic individuals can experience very distorted beliefs about the world known as psychosis. A depressive episode commonly follows an episode of mania. The biological mechanisms responsible for switching from a manic or hypomanic episode to a depressive episode or vice versa remain poorly understood.
Manic episodes.
Mania is a distinct period of at least one week of elevated or irritable mood, which can take the form of euphoria, and exhibit three or more of the following behaviors (four if irritable): speak in a rapid, uninterruptible manner, are easily distracted, have racing thoughts, display an increase in goal-oriented activities or feel agitated, or exhibit behaviors characterized as impulsive or high-risk such as hypersexuality or excessive money spending. To meet the definition for a manic episode, these behaviors must impair the individual's ability to socialize or work. If untreated, a manic episode usually lasts three to six months.
People with mania may also experience a decreased need for sleep, speak excessively in addition to speaking rapidly, and may have impaired judgment. Manic individuals often have issues with substance abuse due to a combination of thrill-seeking and poor judgment. At more extreme levels, a person in a manic state can experience psychosis, or a break with reality, a state in which thinking is affected along with mood. They may feel out of control or unstoppable, or as if they have been "chosen" and are on a special mission, or have other grandiose or delusional ideas. Approximately 50% of those with bipolar disorder experience delusions or hallucinations. This may lead to violent behaviors and hospitalization in an inpatient psychiatric hospital. The severity of manic symptoms can be measured by rating scales such as the Young Mania Rating Scale.
The onset of a manic (or depressive) episode is often foreshadowed by sleep disturbances. Mood changes, psychomotor and appetite changes, and an increase in anxiety can also occur up to three weeks before a manic episode develops.
Hypomanic episodes.
Hypomania is a milder form of mania defined as at least four days of the same criteria as mania, but does not cause a significant decrease in the individual's ability to socialize or work, lacks psychotic features (i.e., delusions or hallucinations), and does not require psychiatric hospitalization. Overall functioning may actually increase during episodes of hypomania and is thought to serve as a defense mechanism against depression. Hypomanic episodes rarely progress to true manic episodes. Some hypomanic people show increased creativity while others are irritable or demonstrate poor judgment. Hypomanic people generally have increased energy and increased activity levels.
Hypomania may feel good to the person who experiences it. Thus, even when family and friends recognize mood swings, the individual often will deny that anything is wrong. What might be called a "hypomanic event", if not accompanied by depressive episodes, is often not deemed as problematic, unless the mood changes are uncontrollable, volatile or mercurial. Most commonly, symptoms continue for a few weeks to a few months.
Depressive episodes.
Signs and symptoms of the depressive phase of bipolar disorder include persistent feelings of sadness, anxiety, guilt, anger, isolation, or hopelessness; disturbances in sleep and appetite; fatigue and loss of interest in usually enjoyable activities; problems concentrating; loneliness, self-loathing, apathy or indifference; depersonalization; loss of interest in sexual activity; shyness or social anxiety; irritability, chronic pain (with or without a known cause); lack of motivation; and morbid suicidal thoughts. In severe cases, the individual may become psychotic, a condition also known as severe bipolar depression with psychotic features. These symptoms include delusions or, less commonly, hallucinations, which are usually unpleasant. A major depressive episode persists for at least two weeks, and may continue for over six months if left untreated.
The earlier the age of onset, the more likely the first few episodes are to be depressive. Because a bipolar diagnosis requires a manic or hypomanic episode, many patients are initially diagnosed and treated as having major depression.
Mixed affective episodes.
In the context of bipolar disorder, a mixed state is a condition during which symptoms of both mania and depression occur at the same time. Individuals experiencing a mixed state may have manic symptoms such as grandiose thoughts while at the same time experiencing depressive symptoms such as excessive guilt or feeling suicidal. Mixed states are considered to be high-risk for suicidal behavior since depressive emotions such as hopelessness are often paired with mood swings or difficulties with impulse control. Anxiety disorder occurs more frequently as a comorbidity in mixed bipolar episodes than in non mixed bipolar depression or mania. Substance abuse (including alcohol) also follows this trend.
Associated features.
Associated features are clinical phenomena that often accompany the disorder but are not part of the diagnostic criteria. In adults with the condition, bipolar disorder is often accompanied by changes in cognitive processes and abilities. These include reduced attentional and executive capabilities and impaired memory. How the individual processes the world also depends on the phase of the disorder, with differential characteristics between the manic, hypomanic and depressive states. Some studies have found a significant association between bipolar disorder and creativity. Those with bipolar disorder may have difficulty in maintaining relationships. There are several common childhood precursors seen in children who later receive a diagnosis of bipolar disorder; these disorders include mood abnormalities, full major depressive episodes, and attention deficit hyperactivity disorder (ADHD).
Comorbid conditions.
The diagnosis of bipolar disorder can be complicated by coexisting (comorbid) psychiatric conditions including the following: obsessive-compulsive disorder, substance abuse, eating disorders, attention deficit hyperactivity disorder, social phobia, premenstrual syndrome (including premenstrual dysphoric disorder), or panic disorder. A careful longitudinal analysis of symptoms and episodes, enriched if possible by discussions with friends and family members, is crucial to establishing a treatment plan where these comorbidities exist.
Causes.
The causes of bipolar disorder likely vary between individuals and the exact mechanism underlying the disorder remains unclear. Genetic influences are believed to account for 60–80% of the risk of developing the disorder indicating a strong hereditary component. The overall heritability of the bipolar spectrum has been estimated at 0.71. Twin studies have been limited by relatively small sample sizes but have indicated a substantial genetic contribution, as well as environmental influence. For bipolar disorder type I, the (probandwise) concordance rates in modern studies have been consistently estimated at around 40% in identical twins (same genes), compared to about 5% in fraternal twins. A combination of bipolar I, II and cyclothymia produced concordance rates of 42% vs 11%, with a relatively lower ratio for bipolar II that likely reflects heterogeneity. There is overlap with unipolar depression and if this is also counted in the co-twin the concordance with bipolar disorder rises to 67% in monozygotic twins and 19% in dizygotic. The relatively low concordance between dizygotic twins brought up together suggests that shared family environmental effects are limited, although the ability to detect them has been limited by small sample sizes.
Genetic.
Genetic studies have suggested that many chromosomal regions and candidate genes are related to bipolar disorder susceptibility with each gene exerting a mild to moderate effect. The risk of bipolar disorder is nearly ten-fold higher in first degree-relatives of those affected with bipolar disorder when compared to the general population; similarly, the risk of major depressive disorder is three times higher in relatives of those with bipolar disorder when compared to the general population.
Although the first genetic linkage finding for mania was in 1969, the linkage studies have been inconsistent. The largest and most recent genome-wide association study failed to find any particular locus that exerts a large effect reinforcing the idea that no single gene is responsible for bipolar disorder in most cases.
Findings point strongly to heterogeneity, with different genes being implicated in different families. Robust and replicable genome-wide significant associations showed several common single nucleotide polymorphisms, including variants within the genes CACNA1C, ODZ4, and NCAN.
Advanced paternal age has been linked to a somewhat increased chance of bipolar disorder in offspring, consistent with a hypothesis of increased new genetic mutations.
Physiological.
Abnormalities in the structure and/or function of certain brain circuits could underlie bipolar. Meta-analyses of structural MRI studies in bipolar disorder report an increase in the volume of the lateral ventricles, globus pallidus and increase in the rates of deep white matter hyperintensities. Functional MRI findings suggest that abnormal modulation between ventral prefrontal and limbic regions, especially the amygdala, are likely contribute to poor emotional regulation and mood symptoms.
According to the "kindling" hypothesis, when people who are genetically predisposed toward bipolar disorder experience stressful events, the stress threshold at which mood changes occur becomes progressively lower, until the episodes eventually start (and recur) spontaneously. There is evidence supporting an association between early-life stress and dysfunction of the hypothalamic-pituitary-adrenal axis (HPA axis) leading to its over activation, which may play a role in the pathogenesis of bipolar disorder.
Other brain components which have been proposed to play a role are the mitochondria and a sodium ATPase pump. Alterations to these components are believed to cause cyclical periods of poor neuron firing (depression) and hypersensitive neuron firing (mania). Circadian rhythms and melatonin activity also seem to be altered.
Environmental.
Evidence suggests that environmental factors play a significant role in the development and course of bipolar disorder, and that individual psychosocial variables may interact with genetic dispositions. There is fairly consistent evidence from prospective studies that recent life events and interpersonal relationships contribute to the likelihood of onsets and recurrences of bipolar mood episodes, as they do for onsets and recurrences of unipolar depression. There have been repeated findings that 30–50% of adults diagnosed with bipolar disorder report traumatic/abusive experiences in childhood, which is associated on average with earlier onset, a higher rate of suicide attempts, and more co-occurring disorders such as PTSD. The total number of reported stressful events in childhood is higher in those with an adult diagnosis of bipolar spectrum disorder compared to those without, particularly events stemming from a harsh environment rather than from the child's own behavior.
Neurological.
Less commonly bipolar disorder or a bipolar-like disorder may occur as a result of or in association with a neurological condition or injury. Such conditions and injuries include (but are not limited to) stroke, traumatic brain injury, HIV infection, multiple sclerosis, porphyria, and rarely temporal lobe epilepsy.
Neuroendocrinological.
Dopamine, a known neurotransmitter responsible for mood cycling, has been shown to have increased transmission during the manic phase. The dopamine hypothesis states that the increase in dopamine results in secondary homeostatic down regulation of key systems and receptors such as an increase in dopamine mediated G protein-coupled receptors. This results in decreased dopamine transmission characteristic of the depressive phase. The depressive phase ends with homeostatic up regulation potentially restarting the cycle over again.
Two additional neurotransmitters, gamma-Aminobutyric acid (GABA) and glutamate, have been found to cause elevated mood states. Glutamate is significantly increased within the left dorsolateral prefrontal cortex during the manic phase of bipolar disorder, and returns to normal levels once the phase is over. GABA is found in higher concentrations in people with bipolar disorder, overall leading to a decrease in GABA B receptors. The increase in GABA is possibly caused by a disturbance in early development causing a disturbance of cell migration and the formation of normal lamination, the layering of brain structures commonly associated with the cerebral cortex.
Evolutionary.
Because bipolar disorder affects an individual’s ability to function in society and has a high morbidity rate, evolutionary theory would suggest that the genes responsible would have been naturally selected against, effectively culling the disorder. Yet there continue to be high rates of bipolar disorder in many populations, suggesting the genes responsible may have an evolutionary benefit.
Proponents of evolutionary medicine hypothesize that the genes that cause severe bipolar disorder when inherited in large doses may increase fitness when inherited in small doses. High rates of bipolar disorder throughout history suggest that the ability to switch between depressive and manic moods conveyed some evolutionary advantage on ancestral humans. Theories put forward to explain the evolutionary advantages of major depressive disorder may also explain the adaptiveness of the depressive episodes of bipolar disorder. For example, in individuals under increased stress, depressive mood may serve as a defensive strategy that causes the individual to retreat from the external stressor, increase sleep, and preserve resources and energy for better times. Additionally, manic moods may convey advantage in some situations. Creativity, confidence, and high energy have all been linked to mania and hypomania. The ability to utilize mild manic symptoms to be more productive and think more creatively during stress-free times may have increased the fitness of ancestral humans. Being able to employ both hypomania and mild depression convey advantages that benefit individuals in a variable environment. However, if the genes enabling the manipulation of both of these moods are over activated, the manic and/or severe depressive moods of full bipolar disorder may be triggered instead.
Evolutionary biologists have hypothesized that bipolar disorder could have come from an adaptation to extreme climactic conditions in the northern temperate zone during the Pleistocene. The Evolutionary Origin of Bipolar Disorder (EOBD) hypothesis states that during the short summers of extreme climactic zones, hypomania would be adaptive, allowing the completion of many tasks necessary for survival within a short period of time. During long winters the lethargy, hypersomnia, lack of interest in social activities, and overeating of depression would be adaptive to group cohesion and survival. Evidence for the EOBD hypothesis include an association between bipolar disorder and a cold-adapted build, correlation between seasonality and mood changes in those with bipolar disorder, and low rates of bipolar disorder in African Americans. The EOBD hypothesis suggests that in the absence of the extreme climactic conditions that fostered the success of bipolar disorder genes, many bipolar disorder behaviors are maladaptive and can often severely impair normal functioning.
Prevention.
Prevention of bipolar has focused on stress (such as childhood adversity or highly conflictual families) which, although not a diagnostically specific causal agent for bipolar, does place genetically and biologically vulnerable individuals at risk for a more pernicious course of illness. There has been debate regarding the causal relationship between usage of cannabis and bipolar disorder.
Diagnosis.
Bipolar disorder often goes unrecognized and is commonly diagnosed during adolescence or early adulthood. The disorder can be difficult to distinguish from unipolar depression and the mean delay in diagnosis is 5–10 years after symptoms begin. Diagnosis of bipolar disorder takes several factors into account and considers the self-reported experiences of the symptomatic individual, behavior abnormalities reported by family members, friends or co-workers, and observable signs of illness as assessed by a psychiatrist, nurse, social worker, clinical psychologist or other health professional. Assessment is usually done on an outpatient basis; admission to an inpatient facility is considered if there is a risk to oneself or others. The most widely used criteria for diagnosing bipolar disorder are from the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders, the current version being DSM-IV-TR, and the World Health Organization's International Statistical Classification of Diseases and Related Health Problems, currently the ICD-10. The latter criteria are typically used in Europe and other regions while the DSM criteria are used in the USA and other regions, as well as prevailing in research studies. The DSM-V, published in 2013, included further and more accurate sub-typing.
An initial assessment may include a physical exam by a physician. Although there are no biological tests that are diagnostic of bipolar disorder, tests may be carried out to exclude medical illnesses with clinical presentations similar to that of bipolar disorder such as hypothyroidism or hyperthyroidism, metabolic disturbance, a chronic disease, or an infection such as HIV or syphilis. An EEG may be used to exclude a seizure disorder such as epilepsy, and a CT scan of the head may be used to exclude brain lesions. Investigations are not generally repeated for a relapse unless there is a specific medical indication.
Several rating scales for the screening and evaluation of bipolar disorder exist, such as the Bipolar spectrum diagnostic scale. The use of evaluation scales can not substitute a full clinical interview but they serve to systematize the recollection of symptoms. On the other hand, instruments for the screening of bipolar disorder have low sensitivity and limited diagnostic validity.
Bipolar spectrum.
Bipolar spectrum disorders (BSD) include the following four disorders: bipolar I disorder, bipolar II disorder, cyclothymic disorder, and bipolar disorder not otherwise specified. These disorders typically involve depressive symptoms or episodes that alternate with elevated mood states or with mixed episodes that feature symptoms of both depressive and elevated mood states. The concept of the bipolar spectrum is similar to that of Emil Kraepelin's original concept of manic depressive illness. 
Unipolar hypomania without accompanying depression has been noted in the medical literature. There is speculation as to whether this condition may occur with greater frequency in the general, untreated population; successful social function of these potentially high-achieving individuals may lead to being labeled as normal, rather than as individuals with substantial dysregulation.
Criteria and subtypes.
There is no clear consensus as to how many types of bipolar disorder exist. In DSM-IV-TR and ICD-10, bipolar disorder is conceptualized as a spectrum of disorders occurring on a continuum. The DSM-IV-TR lists three specific subtypes and one for non-specified:
The bipolar I and II categories have specifiers that indicate the presentation and course of the disorder. For example, the "with full interepisode recovery" specifier applies if there was full remission between the two most recent episodes.
Rapid cycling.
Most people who meet criteria for bipolar disorder experience a number of episodes, on average 0.4 to 0.7 per year, lasting three to six months. "Rapid cycling", however, is a course specifier that may be applied to any of the above subtypes. It is defined as having four or more mood disturbance episodes within a one year span and is found in a significant proportion of individuals with bipolar disorder. These episodes are separated from each other by a remission (partial or full) for at least two months or a switch in mood polarity (i.e., from a depressive episode to a manic episode or vice versa). The definition of rapid cycling most frequently cited in the literature (including the DSM) is that of Dunner and Fieve: at least four major depressive, manic, hypomanic or mixed episodes are required to have occurred during a 12-month period. Ultra-rapid (days) and ultra-ultra rapid or ultradian (within a day) cycling have also been described. The literature examining the pharmacological treatment of rapid cycling is sparse and there is no clear consensus with respect to its optimal pharmacological management.
Differential diagnosis.
There are several other mental disorders with symptoms similar to those seen in bipolar disorder. These disorders include schizophrenia, major depressive disorder, attention deficit hyperactivity disorder (ADHD), and certain personality disorders, such as borderline personality disorder.
It has been noted that the bipolar disorder diagnosis is officially characterized in historical terms such that, technically, anyone with a history of (hypo)mania and depression has bipolar disorder whatever their current or future functioning and vulnerability. This has been described as "an ethical and methodological issue", as it means no one can be considered as being recovered (only "in remission") from bipolar disorder according to the official criteria. This is considered especially problematic given that brief hypomanic episodes are widespread among people generally and not necessarily associated with dysfunction.
Management.
There are a number of pharmacological and psychotherapeutic techniques used to treat bipolar disorder. Individuals may use self-help and pursue recovery.
Hospitalization may be required especially with the manic episodes present in bipolar I. This can be voluntary or (if mental health legislation allows and varying state-to-state regulations in the USA) involuntary (called civil or involuntary commitment). Long-term inpatient stays are now less common due to deinstitutionalization, although these can still occur. Following (or in lieu of) a hospital admission, support services available can include drop-in centers, visits from members of a community mental health team or an Assertive Community Treatment team, supported employment and patient-led support groups, intensive outpatient programs. These are sometimes referred to as partial-inpatient programs.
Psychosocial.
Psychotherapy is aimed at alleviating core symptoms, recognizing episode triggers, reducing negative expressed emotion in relationships, recognizing prodromal symptoms before full-blown recurrence, and, practicing the factors that lead to maintenance of remission. Cognitive behavioral therapy, family-focused therapy, and psychoeducation have the most evidence for efficacy in regard to relapse prevention, while interpersonal and social rhythm therapy and cognitive-behavioral therapy appear the most effective in regard to residual depressive symptoms. Most studies have been based only on bipolar I, however, and treatment during the acute phase can be a particular challenge. Some clinicians emphasize the need to talk with individuals experiencing mania, to develop a therapeutic alliance in support of recovery.
Medication.
A number of medications are used to treat bipolar disorder. The medication with the best evidence is lithium, which is effective in treating acute manic episodes and preventing relapses; lithium is also an effective treatment for bipolar depression. Lithium reduces the risk of suicide, self-harm, and death in people with bipolar disorder.
Four anticonvulsants are used in the treatment of bipolar disorder. Carbamazepine effectively treats manic episodes, with some evidence it has greater benefit in rapid-cycling bipolar disorder, or those with more psychotic symptoms or a more schizoaffective clinical picture. It is less effective in preventing relapse than lithium or valproate. Carbamazepine became a popular treatment option for bipolar in the late 1980s and early 1990s, but was displaced by sodium valproate in the 1990s. Since then, valproate has become a commonly prescribed treatment, and is effective in treating manic episodes. Lamotrigine has some efficacy in treating bipolar depression, and this benefit is greatest in more severe depression. It has also been shown to have some benefit in preventing further episodes, though there are concerns about the studies done, and is of no benefit in rapid cycling disorder. The effectiveness of topiramate is unknown. Depending on the severity of the case, anticonvulsants may be used in combination with lithium or on their own.
Antipsychotic medications are effective for short-term treatment of bipolar manic episodes and appear to be superior to lithium and anticonvulsants for this purpose. However, other medications such as lithium are preferred for long-term use. Olanzapine is effective in preventing relapses, although the evidence is not as solid as for lithium. Antidepressants have not been found to be of any benefit over that found with mood stabilizers.
Short courses of benzodiazepines may be used in addition to other medications until mood stabilizing become effective.
Alternative medicine.
There is some evidence that the addition of omega 3 fatty acids may have beneficial effects on depressive symptoms, although studies have been scarce and of variable quality.
Prognosis.
For many individuals with bipolar disorder a good prognosis results from good treatment, which, in turn, results from an accurate diagnosis. Of the various forms of bipolar disorder, rapid cycling bipolar disorder is associated with the worst prognosis. Because bipolar disorder can have a high rate of both under-diagnosis and misdiagnosis, it is often difficult for individuals with the condition to receive timely and competent treatment.
Bipolar disorder can be a severely disabling medical condition. However, many individuals with bipolar disorder can live full and satisfying lives. Quite often, medication is needed to enable this. Persons with bipolar disorder may have periods of normal or near normal functioning between episodes.
Functioning.
Functioning in bipolar I and II varies over time along a spectrum from good to fair to poor. During periods of major depression or mania (in BPI), functioning was on average poor, with depression being more persistently associated with disability than mania. Functioning between episodes was on average good — more or less normal. Subthreshold symptoms were generally still substantially impairing, however, except for hypomania (below or above threshold) which was associated with improved functioning.
Recovery and recurrence.
A naturalistic study from first admission for mania or mixed episode (representing the hospitalized and therefore most severe cases) found that 50% achieved syndromal recovery (no longer meeting criteria for the diagnosis) within six weeks and 98% within two years. Within two years, 72% achieved symptomatic recovery (no symptoms at all) and 43% achieved functional recovery (regaining of prior occupational and residential status). However, 40% went on to experience a new episode of mania or depression within 2 years of syndromal recovery, and 19% switched phases without recovery.
Symptoms preceding a relapse (prodromal), specially those related to mania, can be reliably identified by people with bipolar disorder. There have been intents to teach patients coping strategies when noticing such symptoms with encouraging results.
Suicide.
Bipolar disorder can cause suicidal ideation that leads to suicidal attempts. Individuals whose bipolar disorder begins with a depressive or mixed affective episode seem to have a poorer prognosis and an increased risk of suicide. One out of two people with bipolar disorder attempt suicide at least once during their lifetime and many attempts are successfully completed. The annual average suicide rate is 0.4%, which is 10–20 times that of the general population. The standardized mortality ratio from suicide in bipolar disorder is between 18 and 25. The lifetime risk of suicide has been estimated to be as high as 20% in those with bipolar disorder.
Epidemiology.
Bipolar disorder is the sixth leading cause of disability worldwide and has a lifetime prevalence of about 3% in the general population. However, a reanalysis of data from the National Epidemiological Catchment Area survey in the United States suggested that 0.8% of the population experience a manic episode at least once (the diagnostic threshold for bipolar I) and a further 0.5% have a hypomanic episode (the diagnostic threshold for bipolar II or cyclothymia). Including sub-threshold diagnostic criteria, such as one or two symptoms over a short time-period, an additional 5.1% of the population, adding up to a total of 6.4%, were classified as having a bipolar spectrum disorder. A more recent analysis of data from a second US National Comorbidity Survey found that 1% met lifetime prevalence criteria for bipolar I, 1.1% for bipolar II, and 2.4% for subthreshold symptoms. There are conceptual and methodological limitations and variations in the findings. Prevalence studies of bipolar disorder are typically carried out by lay interviewers who follow fully structured/fixed interview schemes; responses to single items from such interviews may suffer limited validity. In addition, diagnoses (and therefore estimates of prevalence) vary depending on whether a categorical or spectrum approach is used. This consideration has led to concerns about the potential for both underdiagnosis and overdiagnosis.
The incidence of bipolar disorder is similar in men and women as well as across different cultures and ethnic groups. A 2000 study by the World Health Organization found that prevalence and incidence of bipolar disorder are very similar across the world. Age-standardized prevalence per 100,000 ranged from 421.0 in South Asia to 481.7 in Africa and Europe for men and from 450.3 in Africa and Europe to 491.6 in Oceania for women. However, severity may differ widely across the globe. Disability-adjusted life year rates, for example, appear to be higher in developing countries, where medical coverage may be poorer and medication less available.
Within the United States, African and European Americans have similar rates of bipolar disorder, while Asian Americans have lower rates.
Late adolescence and early adulthood are peak years for the onset of bipolar disorder. One study also found that in 10% of bipolar cases, the onset of mania had happened after the patient had turned 50.
History.
Variations in moods and energy levels have been observed as part of the human experience since throughout history. The words "melancholia" (an old word for depression) and "mania" originated in Ancient Greek. The word melancholia is derived from "melas"/μελας, meaning "black", and "chole"/χολη, meaning "bile" or "gall", indicative of the term's origins in pre-Hippocratic humoral theories. Within the humoral theories, mania was viewed as arising from an excess of yellow bile, or a mixture of black and yellow bile. The linguistic origins of mania, however, are not so clear-cut. Several etymologies are proposed by the Roman physician Caelius Aurelianus, including the Greek word "ania", meaning "to produce great mental anguish", and "manos", meaning "relaxed" or "loose", which would contextually approximate to an excessive relaxing of the mind or soul. There are at least five other candidates, and part of the confusion surrounding the exact etymology of the word mania is its varied usage in the pre-Hippocratic poetry and mythologies.
In the early 1800s, French psychiatrist Jean-Étienne Dominique Esquirol's lypemania, one of his affective monomanias, was the first elaboration on what was to become modern depression. The basis of the current conceptualisation of manic–depressive illness can be traced back to the 1850s; on January 31, 1854, Jules Baillarger described to the French Imperial Academy of Medicine a biphasic mental illness causing recurrent oscillations between mania and depression, which he termed "folie à double forme" ("dual-form insanity"). Two weeks later, on February 14, 1854, Jean-Pierre Falret presented a description to the Academy on what was essentially the same disorder, and designated "folie circulaire" ("circular insanity") by him.
These concepts were developed by the German psychiatrist Emil Kraepelin (1856–1926), who, using Kahlbaum's concept of cyclothymia, categorized and studied the natural course of untreated bipolar patients. He coined the term "manic depressive psychosis", after noting that periods of acute illness, manic or depressive, were generally punctuated by relatively symptom-free intervals where the patient was able to function normally.
The term "manic–depressive "reaction"" appeared in the first American Psychiatric Association Diagnostic Manual in 1952, influenced by the legacy of Adolf Meyer. Subclassifying this into 'unipolar' and 'bipolar' disorder was first proposed by German psychiatrists Karl Kleist and Karl Leonhard in the 1950s and, since the DSM-III, Major Depressive Disorder has been regarded as a separate condition to Bipolar Disorder. Subtypes of 'Bipolar II' and 'rapid cycling' have been included since the DSM-IV, based on work from the 1970s by David L. Dunner, Elliot S. Gershon, Frederick K. Goodwin, Ronald R. Fieve and Joseph L. Fleiss.
Society and culture.
There are widespread problems with social stigma, stereotypes, and prejudice against individuals with a diagnosis of bipolar disorder.
Kay Redfield Jamison, a clinical psychologist and Professor of Psychiatry at the Johns Hopkins University School of Medicine, profiled her own bipolar disorder in her memoir "An Unquiet Mind" (1995). In his autobiography "" (2008) Chris Joseph describes his struggle between the creative dynamism which allowed the creation of his multi-million pound advertising agency Hook Advertising, and the money-squandering dark despair of his bipolar illness.
Several dramatic works have portrayed characters with traits suggestive of the diagnosis that has been the subject of discussion by psychiatrists and film experts alike. A notable example is "Mr. Jones" (1993), in which Mr. Jones (Richard Gere) swings from a manic episode into a depressive phase and back again, spending time in a psychiatric hospital and displaying many of the features of the syndrome. In "The Mosquito Coast" (1986), Allie Fox (Harrison Ford) displays some features including recklessness, grandiosity, increased goal-directed activity and mood lability, as well as some paranoia. Psychiatrists have suggested that Willy Loman, the main character in Arthur Miller's classic play "Death of a Salesman", suffers from bipolar disorder, though that specific term for the condition did not exist when the play was written.
TV specials, for example the BBC's "", MTV's "True Life: I'm Bipolar", talk shows, and public radio shows, and the greater willingness of public figures to discuss their own bipolar disorder, have focused on psychiatric conditions, thereby, raising public awareness.
On April 7, 2009, the nighttime drama "90210" on the CW network, aired a special episode where the character Silver was diagnosed with bipolar disorder. Stacey Slater, a character from the BBC soap EastEnders, has been diagnosed with the disorder. The storyline was developed as part of the BBC's Headroom campaign. The Channel 4 soap "Brookside" had earlier featured a story about bipolar disorder when the character Jimmy Corkhill was diagnosed with the condition. In April 2014, ABC premiered a medical drama, "Black Box", in which the main character, a world-renowned neuroscientist, is bipolar.
Currently, British secret intelligence agency, SIS, doesn't accept job applications from those who "have ever suffered from bipolar disorder".
Specific populations.
Children.
In the 1920s, Emil Kraepelin noted that manic episodes are rare before puberty. In general, bipolar disorder in children was not recognized in the first half of the twentieth century. This issue diminished with an increased following of the DSM criteria in the last part of the twentieth century.
While in adults the course of bipolar disorder is characterized by discrete episodes of depression and mania with no clear symptomatology between them, in children and adolescents very fast mood changes or even chronic symptoms are the norm. Pediatric bipolar disorder is commonly characterized by outbursts of anger, irritability and psychosis, rather than euphoric mania, which is more likely to be seen in adults. Early onset bipolar disorder is more likely to manifest as depression rather than mania or hypomania.
The diagnosis of childhood bipolar disorder is controversial, although it is not under discussion that the typical symptoms of bipolar disorder have negative consequences for minors suffering them. The debate is mainly centered on whether what is called bipolar disorder in children refers to the same disorder as when diagnosing adults, and the related question of whether the criteria for diagnosis for adults are useful and accurate when applied to children. Regarding diagnosis of children, some experts recommend following the DSM criteria. Others believe that these criteria do not correctly separate children with bipolar disorder from other problems such as ADHD, and emphasize fast mood cycles. Still others argue that what accurately differentiates children with bipolar disorder is irritability. The practice parameters of the AACAP encourage the first strategy. American children and adolescents diagnosed with bipolar disorder in community hospitals increased 4-fold reaching rates of up to 40% in 10 years around the beginning of the 21st century, while in outpatient clinics it doubled reaching 6%. Studies using DSM criteria show that up to 1% of youth may have bipolar disorder.
Treatment involves medication and psychotherapy. Drug prescription usually consists in mood stabilizers and atypical antipsychotics. Among the former, lithium is the only compound approved by the FDA for children. Psychological treatment combines normally education on the disease, group therapy and cognitive behavioral therapy. Chronic medication is often needed.
Current research directions for bipolar disorder in children include optimizing treatments, increasing the knowledge of the genetic and neurobiological basis of the pediatric disorder and improving diagnostic criteria. The DSM-V has proposed a new diagnosis which is considered to cover some presentations currently thought of as childhood-onset bipolar.
Elderly.
There is a relative lack of knowledge about bipolar disorder in late life. There is evidence that it becomes less prevalent with age but nevertheless accounts for a similar percentage of psychiatric admissions; that older bipolar patients had first experienced symptoms at a later age; that later onset of mania is associated with more neurologic impairment; that substance abuse is considerably less common in older groups; and that there is probably a greater degree of variation in presentation and course, for instance individuals may develop new-onset mania associated with vascular changes, or become manic only after recurrent depressive episodes, or may have been diagnosed with bipolar disorder at an early age and still meet criteria. There is also some weak evidence that mania is less intense and there is a higher prevalence of mixed episodes, although there may be a reduced response to treatment. Overall, there are likely more similarities than differences from younger adults. In the elderly, recognition and treatment of bipolar disorder may be complicated by the presence of dementia or the side effects of medications being taken for other conditions.

</doc>
<doc id="4536" url="http://en.wikipedia.org/wiki?curid=4536" title="Blitz">
Blitz

Blitz, German for "lightning", may refer to:

</doc>
<doc id="4537" url="http://en.wikipedia.org/wiki?curid=4537" title="Burt Lancaster">
Burt Lancaster

Burton Stephen "Burt" Lancaster (November 2, 1913 – October 20, 1994) was an American film actor noted for his athletic physique, blue eyes, and distinctive smile (which he called "The Grin"). After initially building his career on "tough guy" roles Lancaster abandoned his "all-American" image in the late 1950s in favor of more complex and challenging roles, and came to be regarded as one of the best motion picture actors of his generation.
Lancaster was nominated four times for Academy Awards and won once for his work in "Elmer Gantry" in 1960. He also won a Golden Globe for that performance and BAFTA Awards for "The Birdman of Alcatraz" (1962) and "Atlantic City" (1980). His production company, Hecht-Hill-Lancaster, was the most successful and innovative star-driven independent production company in Hollywood in the 1950s, making movies such as "Marty" (1955), "Trapeze" (1956), "Sweet Smell of Success" (1957), and "Separate Tables" (1958).
In 1999, the American Film Institute named Lancaster 19th among the greatest male stars of all time.
Youth.
Lancaster was born in Manhattan, New York City, at his parents' home at 209 East 106th Street, between Second and Third Avenues, today the site of Benjamin Franklin Plaza. Lancaster was the son of Elizabeth (née Roberts) and James Henry Lancaster, who was a postman. Both of his parents were Protestants of working-class origin. All of Lancaster's grandparents were Northern Irish immigrants to the U.S.; his maternal grandparents were from Belfast, and were descendants of English immigrants to Ireland. The family believed themselves to be related to Frederick Roberts, 1st Earl Roberts. Lancaster grew up in East Harlem and spent much of his time on the streets, where he developed great interest and skill in gymnastics while attending DeWitt Clinton High School, where he was a basketball star. Before he graduated from DeWitt Clinton, his mother died of a cerebral hemorrhage. Lancaster was accepted into New York University with an athletic scholarship but subsequently dropped out.
Circus career and military.
At the age of 19, Lancaster met Nick Cravat, with whom he continued to work throughout his life. Together they learned to act in local theatre productions and circus arts at Union Settlement, one of the city's oldest settlement houses. They formed the acrobat duo "Lang and Cravat" in the 1930s and soon joined the Kay Brothers circus. However, in 1939, an injury forced Lancaster to give up the profession, with great regret. He then found temporary work until 1942—first as a salesman for Marshall Fields, and then as a singing waiter in various restaurants.
The United States having then entered World War II, Lancaster joined the US Army and performed with the Army's Twenty-First Special Services Division, one of the military groups organized to follow the troops on the ground and provide USO entertainment to keep up morale. He served with General Mark Clark's Fifth Army in Italy from 1943–1945.
Film career.
Acting.
Though initially unenthusiastic about acting, he returned from service, auditioned for a Broadway play, and was offered a role. Although Harry Brown's "A Sound of Hunting" had a run of only three weeks, Lancaster's performance drew the attention of a Hollywood agent, Harold Hecht, and through him to Hal Wallis, who cast Lancaster in "The Killers" (1946). (Hecht and Lancaster later formed several production companies in the 1950s to give Lancaster greater creative control.) The tall, muscular actor won significant acclaim and appeared in two more films the following year. Subsequently, he played in a variety of films, especially in dramas, thrillers, and military and adventure films. In two, "The Flame and the Arrow" and "The Crimson Pirate," a friend from his circus years, Nick Cravat, played a key supporting role, and both actors impressed audiences with their acrobatic prowess.
In 1953, Lancaster played one of his best remembered roles with Deborah Kerr in "From Here to Eternity." The American Film Institute acknowledged the iconic status of the scene from that film in which he and Deborah Kerr make love on a Hawaiian beach amid the crashing waves. The organization named it one of "AFI's top 100 Most Romantic Films" of all time.
Lancaster won the 1960 Academy Award for Best Actor, a Golden Globe Award, and the New York Film Critics Award for his performance in "Elmer Gantry."
In 1966, at the age of 52, Lancaster appeared semi-nude in director Frank Perry's film, "The Swimmer" in what Roger Ebert called, "his finest performance." Prior to working on "The Swimmer", Lancaster was terrified of the water because he didn't know how to swim. In preparation for the film, the athlete took swimming lessons from UCLA swim coach, Bob Horn. The film was not released until 1968 when it proved to be a commercial failure, though Lancaster remained proud of the movie and his performance.
During the latter part of his career, Lancaster left adventure and acrobatic films behind and portrayed more distinguished characters. This period brought him work on several European productions, with directors such as Luchino Visconti and Bernardo Bertolucci. Lancaster sought demanding roles, and if he liked a part or a director, he was prepared to work for much lower pay than he might have earned elsewhere. He even helped to finance movies whose artistic value he believed in. He also mentored directors such as Sydney Pollack and John Frankenheimer and appeared in several television films. Lancaster's last film was "Field of Dreams" (1989).
For his contribution to the motion picture industry, Lancaster has a star on the Hollywood Walk of Fame at 6801 Hollywood Boulevard.
Directing and producing.
Lancaster was an early and successful actor/producer. In 1952, Lancaster co-produced "The Crimson Pirate" with producer Harold Hecht (who had previously produced three Lancaster films under his own production company Norma Productions; "Kiss the Blood Off My Hands" (1948), "The Flame and the Arrow" (1950), and "Ten Tall Men" (1951)). In 1954, they collaborated again on "His Majesty O'Keefe", with Lancaster acting and Hecht producing. The writer for this film was James Hill. The trio started a production company, originally with Hill as a silent partner, under the name "Hecht-Lancaster." The name was later extended to include all three with "Hecht-Hill-Lancaster."
The "H-H-L" team impressed Hollywood with its success; as "Life" wrote in 1957, "[a]fter the independent production of a baker's dozen of pictures it has yet to have its first flop ... (They were also good pictures.)." Together they produced the films "Apache" (1954), "Vera Cruz" (1954), "Marty" (1955) (which won both the Academy Award for Best Picture and the Palme d'Or award at the Cannes Film Festival), "The Kentuckian" (1955), "Trapeze" (1956), "The Bachelor Party" (1956), "Sweet Smell of Success" (1957), "Run Silent, Run Deep" (1958), "Separate Tables" (1958), "The Devil's Disciple" (1959), "Take a Giant Step" (1959), "Summer of the Seventeenth Doll" (1960), and "The Unforgiven" (1960). The company dissolved in 1960, but Hecht would produce two more films in which Lancaster acted, under Norma Productions, "The Young Savages" (1961) and "Birdman of Alcatraz" (1962). Twelve years later, Hecht and Lancaster produced "Ulzana's Raid" (1972) together.
In the late 1960s, Lancaster teamed with Roland Kibbee to form "Norlan Productions" and along with "Bristol Films" produce "The Scalphunters" (1968), "Valdez Is Coming" (1971), and "The Midnight Man" (1974).
In addition, Lancaster directed two films, "The Kentuckian" (1955) and "The Midnight Man" (1974). "The Midnight Man" was in fact starred in, co-written, produced, and directed by Lancaster.
Frequent collaborations.
Apart from acting in a total of seventeen films produced by Harold Hecht, Lancaster also appeared in eight films produced by Hal B. Wallis.
Lancaster made seven films over the years with Kirk Douglas, including "I Walk Alone" (1948), "Gunfight at the O.K. Corral" (1957), "The Devil's Disciple" (1959), "The List of Adrian Messenger" (1963), "Seven Days in May" (1964), "Victory at Entebbe" (1976) and "Tough Guys" (1986). Additionally, the pair acted in comedy musical sketches for the 1958 and 1959 Oscar celebrations. Although perceived as a friendly collaboration, the two actors were never friends or fond of each other in real life, as was depicted in their respective biographical books. 
Lancaster also often asked his close friend Nick Cravat to appear in his films. They co-starred together in nine films: "The Flame and the Arrow" (1952), "Ten Tall Men" (1951), "The Crimson Pirate" (1952), "Run Silent, Run Deep" (1958), "The Scalphunters" (1968), "Valdez Is Coming" (1971), "Ulzana's Raid" (1972), "The Midnight Man" (1974), and "The Island of Dr. Moreau" (1977).
Lancaster starred in three films with Deborah Kerr; "From Here to Eternity", "Separate Tables", and "The Gypsy Moths".
In addition, John Frankenheimer directed five films with Lancaster: "The Young Savages" (1961), "Birdman of Alcatraz" (1962), "Seven Days in May" (1964), "The Train" (1964), and "The Gypsy Moths" (1969).
Among his final films, he had a small role in the 1983 British film "Local Hero".
Lancaster used make-up veteran Robert Schiffer in 20 credited films. Lancaster hired Schiffer on nearly all the films he produced.
Legacy.
The centennial of Lancaster's birth was honored at New York City's Film Society of Lincoln Center in May 2013 with the screening of twelve of the actor's finest films, from "The Killers" of 1946 to "Atlantic City" in 1980.
Personal life.
Marriages and relationships.
Lancaster vigorously guarded his private life. He was married three times. His first two marriages ended in divorce—to June Ernst from 1935 to 1946, and to Norma Anderson from 1946 to 1969. His third marriage, to Susan Martin, was from September 1990 until his death in 1994. All five of his children were with Norma Anderson: Bill, who became a screenwriter; James; Susan; Joanna; and Sighle (pronounced Sheila). He claimed he was romantically involved with Deborah Kerr during the filming of "From Here to Eternity" in 1953. However, Kerr stated that while there was a spark of attraction, nothing ever happened. He did have an affair with Joan Blondell. In her 1980 autobiography, Shelley Winters claimed to have had a long affair with him.
Political views.
Lancaster was a vocal supporter of liberal political causes, and frequently spoke out in support of racial minorities, including at the March on Washington in 1963. He was a vocal opponent of the Vietnam War and political movements such as McCarthyism, and he helped pay for the successful defense of a soldier accused of "fragging" (murdering) another soldier during the war. In 1968, Lancaster actively supported the presidential candidacy of antiwar Senator Eugene McCarthy of Minnesota, and frequently spoke on his behalf during the Democratic primaries. He heavily campaigned for George McGovern in the 1972 presidential election. In 1985, Lancaster joined the fight against AIDS after his close friend, Rock Hudson, contracted the disease. He campaigned for Michael Dukakis in the 1988 presidential election.
Health problems and death.
As Lancaster grew older, he became increasingly plagued by atherosclerosis, barely surviving a routine gall bladder operation in January 1980. Following two minor heart attacks he had to undergo an emergency quadruple coronary bypass in 1983, after which he was extremely weak, but he still managed to continue acting and attended a 1988 Congressional hearing with old colleagues such as Jimmy Stewart and Ginger Rogers to protest media magnate Ted Turner's plan to colorize various black-and-white films from the 1930s and 1940s. His acting career ended after he suffered a stroke on November 30, 1990 which left him partly paralyzed and largely unable to speak. He died in his Century City apartment in Los Angeles from a third heart attack at 4:50 A.M. on October 20, 1994 at the age of 80.
Lancaster was cremated and his ashes were buried under a large oak tree in Westwood Memorial Park located in Westwood Village, Los Angeles County, California. A small square ground plaque inscribed only with "BURT LANCASTER 1913–1994" marks his final resting place. Upon his death, as he requested, he had no memorial or funeral service.
Filmography and awards.
Box Office Ranking.
For a number of years exhibitors voted Lancaster as among the most popular stars in the country:
In other media.
Spanish music group Hombres G released an album named La cagaste, Burt Lancaster (You messed up, Burt Lancaster) in 1986.

</doc>
<doc id="4540" url="http://en.wikipedia.org/wiki?curid=4540" title="Balts">
Balts

The Balts or Baltic people (, ) are an Indo-European ethno-linguistic group who speak the Baltic languages, a branch of the Indo-European language family, which was originally spoken by tribes living in area east of Jutland peninsula in the west and Moscow, Oka and Volga rivers basins in the east. One of the features of Baltic languages is the number of conservative or archaic features retained. Among the Baltic peoples are modern Lithuanians, Latvians (including Latgalians) — all Eastern Balts — as well as the Old Prussians, Yotvingians and Galindians — the Western Balts — whose people also survived, but their languages and cultures are now extinct, and are now being assimilated into the Eastern Baltic community.
Etymology.
Adam of Bremen in the latter part of the 11th century AD was the first writer to use the term Baltic in its modern sense to mean the sea of that name. Although he must have been familiar with the ancient name, Balcia, meaning a supposed island in the Baltic Sea, and although he may have been aware of the Baltic words containing the stem balt-, "white", as "swamp", he reports that he followed the local use of balticus from baelt ("belt") because the sea stretches to the east "in modum baltei" ("in the manner of a belt"). This is the first reference to "the Baltic or Barbarian Sea, a day's journey from Hamburg."
The Germanics, however, preferred some form of "East Sea" (in different languages) until after about 1600, when they began to use forms of "Baltic Sea." Around 1840 the German nobles of the Governorate of Livonia devised the term "Balts" to mean themselves, the German upper classes of Livonia, excluding the Latvian and Estonian lower classes. They spoke an exclusive dialect, Baltic German. For all practical purposes that was the Baltic language until 1919.
Scandinavians begin settling in Western Baltic lands in Lithuania and Latvia.
In 1845 Georg Heinrich Ferdinand Nesselmann proposed a distinct language group for Latvian and Lithuanian to be called Baltic. It found some credence among linguists but was not generally adopted until the creation of the Baltic states as part of the settlement of World War I in 1919. Gradually the non-Baltic Estonian was excluded from the "linguistic" meaning of Baltic, as was Livonian, a now extinct Finnic language in present-day Latvia, while Old Prussian — long recognized as close to Lithuanian and Latvian — was added. Estonia and Finland (the states of Baltic Finns), however, also became counted among the Baltic states in the geopolitical sense. (Finland was dropped from this definition after World War II, though Estonia remains within the definition.)
History.
Origins.
The Balts or Baltic peoples, defined as speakers of one of the Baltic languages, a branch of the Indo-European language family, are descended from a group of Indo-European tribes who settled the area between the lower Vistula and upper Daugava and Dnieper rivers on the southeast shore of the Baltic Sea. Because the thousands of lakes and swamps in this area contributed to the Balts' geographical isolation, the Baltic languages retain a number of conservative or archaic features.
It is possible that around 3,500–2,500 B.C., there was massive migration of peoples representing the Corded Ware culture. They came from the southeast and spread all across Eastern and Central Europe, reaching even southern Finland. It is believed that Corded Ware culture peoples were Indo-European ancestors of many Europeans, including Balts. It is thought that those Indo-European newcomers were quite numerous and in the Eastern Baltic assimilated earlier indigenous cultures (Europidic cultures – Narva culture and Neman culture). Over time the new people formed the Baltic peoples and they spread in the area from the Baltic sea in the west to the Volga in the east.
Some of the major authorities on Balts, such as Būga, Vasmer, Toporov and Trubachov, in conducting etymological studies of eastern European river names, were able to identify in certain regions names of specifically Baltic provenance, which most likely indicate where the Balts lived in prehistoric times. This information is summarized and synthesized by Marija Gimbutas in "The Balts" (1963) to obtain a likely proto-Baltic homeland. Its borders are approximately: from a line on the Pomeranian coast eastward to include or nearly include the present-day sites of Berlin, Warsaw, Kiev, and Kursk, northward through Moscow to the River Berzha, westward in an irregular line to the coast of the Gulf of Riga, north of Riga.
Proto-history.
A possible early reference to a Baltic people occurs in 98 AD, when Tacitus names a tribe living near the Baltic Sea ("Mare Svebicum") as the Aesti ("Aestiorum gentes") and describes them as amber gatherers. However, it is not clear if the Aesti mentioned by Tacitus were: (1) a (now-extinct) Baltic people (possibly synonymous with the Brus/Prūsa), or; (2) a Finno-Ugric people (e.g. modern Estonians). The Aesti appear to have inhabited the Sambian peninsula (in or near the present Kaliningrad Oblast .
Over time, the area of Baltic habitation shrank, due to assimilation by other groups, and invasions. According to one of the theories which has gained considerable traction over the years, one of the western Baltic tribes, the Galindians, Galindae, or Goliad, migrated to the Eastern end of Baltic realm around the 4th century AD, and settled around modern day Moscow, Russia. Finally, according to Slavic chronicles of the time, they warred with Slavs, and perhaps, were defeated and assimilated some time in the 11th to 13th centuries.
Balts became differentiated into Western and Eastern Balts in the late centuries BCE. The eastern Baltic region was inhabited by ancestors of the Western Balts: Brus/Prūsa ("Old Prussians"), Sudovians/Jotvingians, Scalvians, Nadruvians, and Curonians. The Eastern Balts, including the hypothesised Dniepr Balts, were living in modern day Belarus, Ukraine and Russia.
Subsequent Germanic and Gothic domination in the first half of the first millennium AD in Northern and Eastern Europe, as well as later Slavic expansion, caused large migrations of the Balts — first, the Galindae or Galindians towards the east, and later, Eastern Balts towards the west — until, in the 13th and 14th centuries, they reached the general area that the present-day Balts inhabit. Many other Eastern and Southern Balts either assimilated with other Balts, or Slavs in the 4th–7th centuries and were gradually slavicized.
Middle Ages.
In the 12th and the 13th centuries, internal struggles, as well as invasions by Ruthenians and Poles and later the expansion of the Teutonic Order resulted in an almost complete annihilation of the Galindians, Curonians, and Yotvingians. Gradually Old Prussians became Germanized or some Lithuanized during period from the 15th to the 17th centuries, especially after the Reformation in Prussia. The cultures of the Lithuanians and Latgalians/Latvians survived and became the ancestors of the populations of the modern countries of Latvia and Lithuania.
Old Prussian was closely related to the other extinct Western Baltic languages, Curonian, Galindian and Sudovian. It is more distantly related to the surviving Eastern Baltic languages, Lithuanian and Latvian. Compare the Prussian word "seme" ("zemē"), the Latvian "zeme", the Lithuanian "žemė" ("land" in English).
Old Prussian contained a few borrowings specifically from Gothic (e.g., Old Prussian "ylo" "awl," as with Lithuanian "ýla", Latvian "īlens") and even North Germanic.
List of Baltic tribes.
†"Extinct"

</doc>
<doc id="4541" url="http://en.wikipedia.org/wiki?curid=4541" title="Burnt-in timecode">
Burnt-in timecode

Burnt-in timecode (often abbreviated to BITC by analogy to VITC) is a human-readable on-screen version of the timecode information for a piece of material superimposed on a video image. BITC is sometimes used in conjunction with "real" machine-readable timecode, but more often used in copies of original material on to a non-broadcast format such as VHS, so that the VHS copies can be traced back to their master tape and the original time codes easily located. 
Many professional VTRs can "burn" (overlay) the tape timecode onto one of their composite outputs. This output (which usually also displays the setup menu or on-screen display) is known as the "super out" or "monitor out". The "character" switch or menu item turns this behaviour on or off. The "character" function is also used to display the timecode on the preview monitors in linear editing suites.
Videotapes that are recorded with timecode numbers overlaid on the video are referred to as "window dubs", named after the "window" that displays the burnt-in timecode on-screen.
Timecode can also be superimposed on video using a dedicated overlay device, often called a "window dub inserter". This inputs a video signal and its separate timecode audio signal, reads the timecode, superimposes the timecode display over the video, and outputs the combined display (usually via composite), all in real time. Stand-alone timecode generator / readers often have the window dub function built-in. 
Some consumer cameras, in particular DV cameras, can "burn" (overlay) the tape timecode onto the composite output. This output typically is semi-transparent and may include other tape information. It is usually activated by turning on the 'display' info in one of the camera's sub-menus. While not as 'professional' as an overlay as created by a professional VCRs, it is a cheap alternative that is just as accurate.
Timecode is stored in the metadata areas of captured DV AVI files, and some software is able to "burn" (overlay) this into the video frames. For example, DVMP Pro is able to "burn" timecode or other items of DV metadata (such as date and time, iris, shutter speed, gain, white balance mode, etc.) into DV AVI files.
Some modern editing systems can use OCR techniques to read BITC in situations where other forms of timecode are not available.
BITC can also be referred to as Viz-Code.

</doc>
<doc id="4542" url="http://en.wikipedia.org/wiki?curid=4542" title="Bra–ket notation">
Bra–ket notation

In quantum mechanics, bra-ket notation is a standard notation for describing quantum states, composed of angle brackets and vertical bars. It can also be used to denote abstract vectors and linear functionals in mathematics. It is so called because the inner product (or dot product on a complex vector space) of two states is denoted by 
consisting of a left part, formula_2 called the bra , and a right part, formula_3, called the ket . The notation was introduced in 1939 by Paul Dirac and is also known as Dirac notation, though the notation has precursors in Grassmann's use of the notation formula_4 for his inner products nearly 100 years previously.
Bra-ket notation is widespread in quantum mechanics: almost every phenomenon that is explained using quantum mechanics—including a large portion of modern physics — is usually explained with the help of bra-ket notation. Part of the appeal of the notation is the abstract representation-independence it encodes, together with its versatility in producing a specific representation (e.g. , or , or eigenfunction base) without much ado, or excessive reliance on the nature of the linear spaces involved. The overlap expression formula_1 is typically interpreted as the probability amplitude for the state to collapse into the state .
Vector spaces.
Background: Vector spaces.
In physics, basis vectors allow any Euclidean vector to be represented geometrically using angles and lengths, in different directions, i.e. in terms of the spatial orientations. It is simpler to see the notational equivalences between ordinary notation and bra-ket notation; so, for now, consider a vector starting at the origin and ending at an element of 3-d Euclidean space; the vector then is specified by this end-point, a triplet of elements in the field of real numbers, symbolically dubbed as .
The vector can be written using any set of basis vectors and corresponding coordinate system. Informally basis vectors are like "building blocks of a vector": they are added together to compose a vector, and the coordinates are the numerical coefficients of basis vectors in each direction. Two useful representations of a vector are simply a linear combination of basis vectors, and column matrices. Using the familiar Cartesian basis, a vector may be written as
respectively, where , , denote the Cartesian basis vectors (all are orthogonal unit vectors) and , , are the corresponding coordinates, in the "x", "y", "z" directions. In a more general notation, for any basis in 3-d space one writes
Generalizing further, consider a vector in an -dimensional vector space over the field of complex numbers , symbolically stated as . The vector is still conventionally represented by a linear combination of basis vectors or a column matrix:
though the coordinates are now all complex-valued.
Even more generally, can be a vector in a complex Hilbert space. Some Hilbert spaces, like , have finite dimension, while others have infinite dimension. In an infinite-dimensional space, the column-vector representation of would be a list of infinitely many complex numbers.
Ket notation for vectors.
Rather than boldtype, over arrows, underscores etc. conventionally used elsewhere, formula_10, Dirac's notation for a vector uses vertical bars and angular brackets: formula_11. When this notation is used, these vectors are called "ket", read as "ket-A". This applies to all vectors, the resultant vector and the basis. The previous vectors are now written
or in a more easily generalized notation,
The last one may be written in short as
Note how any symbols, letters, numbers, or even words — whatever serves as a convenient label — can be used as the label inside a ket. In other words, the symbol has a specific and universal mathematical meaning, while just the "" by itself does not. Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers. Further note that a ket and its representation by a coordinate vector are not the same mathematical object: a ket does not require specification of a basis, whereas the coordinate vector needs a basis in order to be well defined (the same holds for an operator and its representation by a matrix). In this context, one should best use a symbol different than the equal sign, for example the symbol , read as "is represented by".
Inner products and bras.
An inner product is a generalization of the dot product. The inner product of two vectors is a complex number. bra-ket notation uses a specific notation for inner products:
For example, in three-dimensional complex Euclidean space,
where formula_17 denotes the complex conjugate of . A special case is the inner product of a vector with itself, which is the square of its norm (magnitude):
bra-ket notation splits this inner product (also called a "bracket") into two pieces, the "bra" and the "ket":
where is called a bra, read as "bra-A", and is a ket as above.
The purpose of "splitting" the inner product into a bra and a ket is that "both" the bra and the ket  are meaningful "on their own", and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets:
Bras and kets as row and column vectors.
For a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:
Based on this, the bras and kets can be defined as:
and then it is understood that a bra next to a ket implies matrix multiplication.
The conjugate transpose (also called "Hermitian conjugate") of a bra is the corresponding ket and vice versa:
because if one starts with the bra
then performs a complex conjugation, and then a matrix transpose, one ends up with the ket
Bras as linear operators on kets.
A more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on kets, i.e. operators that input a ket and output a complex number. The bra operators are defined to be consistent with the inner product.
In mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.
Non-normalizable states and non-Hilbert spaces.
bra-ket notation can be used even if the vector space is not a Hilbert space.
In quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalisable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of "Hilbert space" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra-ket notation continues to work in an analogous way in this broader context.
For a rigorous treatment of the Dirac inner product of non-normalizable states, see the definition given by D. Carfì. For a rigorous definition of basis with a continuous set of indices and consequently for a rigorous definition of position and momentum basis, see. For a rigorous statement of the expansion of an S-diagonalizable operator, or observable, in its eigenbasis or in another basis, see.
Banach spaces are a different generalization of Hilbert spaces. In a Banach space , the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.
Usage in quantum mechanics.
The mathematical structure of quantum mechanics is based in large part on linear algebra:
Since virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often "does" involve, bra-ket notation. A few examples follow:
Spinless position–space wave function.
The Hilbert space of a spin-0 point particle is spanned by a "position basis" where the label extends over the set of all points in position space. Since there are uncountably infinitely many vectors in the basis, this is an uncountably infinite-dimensional Hilbert space. The dimensions of the Hilbert space (usually infinite) and position space (usually 1, 2 or 3) are not to be conflated.
Starting from any ket in this Hilbert space, we can "define" a complex scalar function of , known as a wavefunction:
On the left side, is a function mapping any point in space to a complex number; on the right side, is a ket.
It is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, by
For instance, the momentum operator p has the following form,
One occasionally encounters a sloppy expression like
though this is something of a (common) abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,
even though, in the momentum basis, the operator amounts to a mere multiplication operator (by ).
Overlap of states.
In quantum mechanics the expression is typically interpreted as the probability amplitude for the state to collapse into the state . Mathematically, this means the coefficient for the projection of onto . It is also described as the projection of state onto state .
Changing basis for a spin-1/2 particle.
A stationary spin-½ particle has a two-dimensional Hilbert space. One orthonormal basis is:
where formula_32 is the state with a definite value of the spin operator "Sz" equal to +1/2 and formula_33 is the state with a definite value of the spin operator "Sz" equal to −1/2.
Since these are a basis, "any" quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:
where , are complex numbers.
A "different" basis for the same Hilbert space is:
defined in terms of "Sx" rather than "Sz".
Again, "any" state of the particle can be expressed as a linear combination of these two:
In vector form, you might write
depending on which basis you are using. In other words, the "coordinates" of a vector depend on the basis used.
There is a mathematical relationship between , , , ; see change of basis.
Misleading uses.
There are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.
It is common among physicists to use the same symbol for "labels" and "constants" in the same equation. It supposedly becomes easier to identify that the constant is related to the labeled object, and is claimed that the divergent nature of each will
eliminate any ambiguity and no further differentiation is required. For example, , where the symbol is used simultaneously as the "name of the operator" , its "eigenvector" and the associated "eigenvalue" .
Something similar occurs in component notation of vectors. While (uppercase) is traditionally associated with wavefunctions, (lowercase) may be used to denote a "label", a "wave function" or "complex constant" in the same context, usually differentiated only by a subscript.
The main abuses are including operations inside the vector labels. This is usually done for a fast notation of scaling vectors. E.g. if the vector is scaled by , it might be denoted by , which makes no sense since is a label, not a function or a number, so you can't perform operations on it.
This is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot. E.g. . Here
part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since is meant to refer to the norm of the first vector – which is a "label" is denoting a "value".
Linear operators.
Linear operators acting on kets.
A linear operator is a map that inputs a ket and outputs a ket. (In order to be called "linear", it is required to have certain properties.) In other words, if is a linear operator and is a ket, then is another ket.
In an -dimensional Hilbert space, can be written as an column vector, and then is an matrix with complex entries. The ket can be computed by normal matrix multiplication.
Linear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.
Linear operators acting on bras.
Operators can also be viewed as acting on bras "from the right hand side". Specifically, if is a linear operator and is a bra, then is another bra defined by the rule
In an -dimensional Hilbert space, can be written as a row vector, and (as in the previous section) is an matrix. Then the bra can be computed by normal matrix multiplication.
If the same state vector appears on both bra and ket side,
then this expression gives the expectation value, or mean or average value, of the observable represented by operator for the physical system in the state .
Outer products.
A convenient way to define linear operators on is given by the outer product: if is a bra and is a ket, the outer product
denotes the rank-one operator with the rule 
For a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:
The outer product is an N×N matrix, as expected for a linear operator.
One of the uses of the outer product is to construct projection operators. Given a ket of norm 1, the orthogonal projection onto the subspace spanned by is
Hermitian conjugate operator.
Just as kets and bras can be transformed into each other (making into ), the element from the dual space corresponding to is , where denotes the Hermitian conjugate (or adjoint) of the operator . In other words,
If is expressed as an matrix, then is its conjugate transpose.
Self-adjoint operators, where , play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If is a self-adjoint operator, then is always a real number (not complex). This implies that expectation values of observables are real.
Properties.
bra-ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, and denote arbitrary complex numbers, denotes the complex conjugate of , and denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.
Associativity.
Given any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra-ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:
and so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously "because" of the equalities on the left. Note that the associative property does "not" hold for expressions that include non-linear operators, such as the antilinear time reversal operator in physics.
Hermitian conjugation.
bra-ket notation makes it particularly easy to compute the Hermitian conjugate (also called "dagger", and denoted ) of expressions. The formal rules are:
These rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:
Composite bras and kets.
Two Hilbert spaces and may form a third space by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in and respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)
If is a ket in and is a ket in , the direct product of the two kets is a ket in . This is written in various notations:
See quantum entanglement and the EPR paradox for applications of this product.
The unit operator.
Consider a complete orthonormal system ("basis"), formula_57, for a Hilbert space , with respect to the norm from an inner product formula_58. From basic functional analysis we know that any ket can also be written as
with formula_60 the inner product on the Hilbert space.
From the commutativity of kets with (complex) scalars now follows that
must be the identity operator, which sends each vector to itself. This can be inserted in any expression without affecting its value, for example
where, in the last identity, the Einstein summation convention has been used.
In quantum mechanics, it often occurs that little or no information about the inner product formula_63 of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients formula_64 and formula_65 of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.
For more information, see Resolution of the identity, , where ; since , plane waves follow, .
Notation used by mathematicians.
The object physicists are considering when using the "bra-ket" notation is a Hilbert space (a complete inner product space).
Let formula_66 be a Hilbert space and formula_67 is a vector in formula_66. What physicists would denote as is the vector itself. That is
Let formula_70 be the dual space of formula_66. This is the space of linear functionals on formula_72. The isomorphism formula_73 is defined by formula_74 where for all formula_75 we have
where formula_77 and formula_78
are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in "any" inner product space). Notational confusion arises when identifying formula_79 and formula_80 with formula_81 and formula_82 respectively. This is because of literal symbolic substitutions. Let formula_83 and let formula_84. This gives
One ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.
Moreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they don't use the *-symbol, but an overline (which the physicists reserve for averages and Dirac conjugation) to denote conjugate-complex numbers, i.e. for scalar products mathematicians usually write
whereas physicists would write for the same quantity

</doc>
<doc id="4543" url="http://en.wikipedia.org/wiki?curid=4543" title="Blue">
Blue

Blue is the colour of the clear sky and the deep sea. It is located between violet and green on the optical spectrum.
Surveys in the U.S. and Europe show that blue is the colour most commonly associated with harmony, faithfulness, confidence, distance, infinity, the imagination, cold, and sometimes with sadness. In U.S. and European public opinion polls it is overwhelmingly the most popular colour, chosen by almost half of both men and women as their favourite colour. 
Shades and variations.
Blue is the colour of light between violet and green on the visible spectrum. Hues of blue include indigo and ultramarine, closer to violet; pure blue, without any mixture of other colours; Cyan, which is midway on the spectrum between blue and green, and the other blue-greens turquoise, teal, and aquamarine.
Blues also vary in shade or tint; darker shades of blue contain black or grey, while lighter tints contain white. Darker shades of blue include ultramarine, cobalt blue, navy blue, and Prussian blue; while lighter tints include sky blue, azure, and Egyptian blue. (For a more complete list see the List of colours).
Blue pigments were originally made from minerals such as lapis lazuli, cobalt and azurite, and blue dyes were made from plants; usually woad in Europe, and Indigofera tinctoria, or True indigo, in Asia and Africa. Today most blue pigments and dyes are made by a chemical process.
Etymology and linguistic differences.
The modern English word "blue" comes from Middle English "bleu" or "blewe", from the Old French "bleu", a word of Germanic origin, related to the Old High German word "blao". In heraldry, the word azure is used for blue.
In Russian and some other languages, there is no single word for blue, but rather different words for light blue (голубой, goluboy) and dark blue (синий, siniy).
Several languages, including Japanese, Thai, Korean, and Lakota Sioux, use the same word to describe blue and green. For example, in Vietnamese the colour of both tree leaves and the sky is "xanh". In Japanese, the word for blue (青 ao) is often used for colours that English speakers would refer to as green, such as the colour of a traffic signal meaning "go". (For more on this subject, see Distinguishing blue from green in language)
History.
In the ancient world.
Blue was a latecomer among colours used in art and decoration, as well as language and literature. Reds, blacks, browns, and ochres are found in cave paintings from the Upper Paleolithic period, but not blue. Blue was also not used for dyeing fabric until long after red, ochre, pink and purple. This is probably due to the perennial difficulty of making good blue dyes and pigments. The earliest known blue dyes were made from plants – woad in Europe, indigo in Asia and Africa, while blue pigments were made from minerals, usually either lapis lazuli or azurite.
Lapis lazuli, a semi-precious stone, has been mined in Afghanistan for more than three thousand years, and was exported to all parts of the ancient world. In Iran and Mesopotamia, it was used to make jewellery and vessels. In Egypt, it was used for the eyebrows on the funeral mask of King Tutankhamun (1341–1323 BC).
The cost of importing lapis lazuli by caravan across the desert from Afghanistan to Egypt was extremely high. Beginning in about 2500 BC, the ancient Egyptians began to produce their own blue pigment known as Egyptian blue, made by grinding silica, lime, copper and alkalai, and heating it to 800 or 900 degrees C. This is considered the first synthetic pigment. Egyptian blue was used to paint wood, papyrus and canvas, and was used to colour a glaze to make faience beads, inlays, and pots. It was particularly used in funeral statuary and figurines and in tomb paintings. Blue was considered a beneficial colour which would protect the dead against evil in the afterlife. Blue dye was also used to colour the cloth in which mummies were wrapped.
In Egypt, blue was associated with the sky and with divinity. The Egyptian god Amun could make his skin blue so that he could fly, invisible, across the sky. Blue could also protect against evil; many people around the Mediterranean still wear a blue amulet, representing the eye of God, to protect them from misfortune.
Blue glass was manufactured in Mesopotamia and Egypt as early as 2500 BC, using the same copper ingredients as Egyptian blue pigment. They also added cobalt, which produced a deeper blue, the same blue produced in the Middle Ages in the stained glass windows of the cathedrals of Saint-Denis and Chartres.
The Ishtar Gate of ancient Babylon (604–562 BC) was decorated with deep blue glazed bricks used as a background for pictures of lions, dragons and aurochs.
The ancient Greeks classified colours by whether they were light or dark, rather than by their hue. The Greek word for dark blue, "kyaneos", could also mean dark green, violet, black or brown. The ancient Greek word for a light blue, "glaukos", also could mean light green, grey, or yellow.
The Greeks imported indigo dye from India, calling it indikon. They used Egyptian blue in the wall paintings of Knossos, in Crete, (2100 BC). It was not one of the four primary colours for Greek painting described by Pliny the Elder (red, yellow, black and white), but nonetheless it was used as a background colour behind the friezes on Greek temples and to colour the beards of Greek statues.
The Romans also imported indigo dye, but blue was the colour of working class clothing; the nobles and rich wore white, black, red or violet. Blue was considered the colour of mourning. It was also considered the colour of barbarians; Julius Caesar reported that the Celts and Germans dyed their faces blue to frighten their enemies, and tinted their hair blue when they grew old.
Nonetheless, the Romans made extensive use of blue for decoration. According to Vitruvius, they made dark blue pigment from indigo, and imported Egyptian blue pigment. The walls of Roman villas in Pompeii had frescoes of brilliant blue skies, and blue pigments were found in the shops of colour merchants. The Romans had many different words for varieties of blue, including "caeruleus", "caesius", "glaucus", "cyaneus", "lividus", "venetus", "aerius", and "ferreus", but two words, both of foreign origin, became the most enduring; "blavus", from the Germanic word "blau", which eventually became "bleu" or blue; and "azureus", from the Arabic word "lazaward", which became azure.
In the Byzantine Empire and the Islamic World.
Dark blue was widely used in the decoration of churches in the Byzantine Empire. In Byzantine art Christ and the Virgin Mary usually wore dark blue or purple. Blue was used as a background colour representing the sky in the magnificent mosaics which decorated Byzantine churches.
In the Islamic world, blue was of secondary importance to green, believed to be the favourite colour of the Prophet Mohammed. At certain times in Moorish Spain and other parts of the Islamic world, blue was the colour worn by Christians and Jews, because only Muslims were allowed to wear white and green. Dark blue and turquoise decorative tiles were widely used to decorate the facades and interiors of mosques and palaces from Spain to Central Asia. Lapis lazuli pigment was also used to create the rich blues in Persian miniatures.
During the Middle Ages.
In the art and life of Europe during the early Middle Ages, blue played a minor role. The nobility wore red or purple, while only the poor wore blue clothing, coloured with poor-quality dyes made from the woad plant. Blue played no part in the rich costumes of the clergy or the architecture or decoration of churches. This changed dramatically between 1130 and 1140 in Paris, when the Abbe Suger rebuilt the Saint Denis Basilica. He installed stained glass windows coloured with cobalt, which, combined with the light from the red glass, filled the church with a bluish violet light. The church became the marvel of the Christian world, and the colour became known as the "bleu de Saint-Denis". In the years that followed even more elegant blue stained glass windows were installed in other churches, including at Chartres Cathedral and Sainte-Chapelle in Paris.
Another important factor in the increased prestige of the colour blue in the 12th century was the veneration of the Virgin Mary, and a change in the colours used to depict her clothing. In earlier centuries her robes had usually been painted in sombre black, grey, violet, dark green or dark blue. In the 12th century they began to be painted a rich lighter blue, usually made with a new pigment imported from Asia; ultramarine. Blue became associated with holiness, humility and virtue.
Ultramarine was made from lapis lazuli, from the mines of Badakshan, in the mountains of Afghanistan, near the source of the Oxus River. The mines were visited by Marco Polo in about 1271; he reported, "here is found a high mountain from which they extract the finest and most beautiful of blues." Ground lapis was used in Byzantine manuscripts as early as the 6th century, but it was impure and varied greatly in colour. Ultramarine refined out the impurities through a long and difficult process, creating a rich and deep blue. It was called "bleu outremer" in French and "blu otramere" in Italian, since it came from the other side of the sea. It cost far more than any other colour, and it became the luxury colour for the Kings and Princes of Europe.
King Louis IX of France, better known as Saint Louis (1214–1270), became the first King of France to regularly dress in blue. This was copied by other nobles. Paintings of the mythical King Arthur began to show him dressed in blue. The coat of arms of the Kings of France became an azure or light blue shield, sprinkled with golden fleur-de-lis or lilies. Blue had come from obscurity to become the royal colour.
Once blue became the colour of the King, it also became the colour of the wealthy and powerful in Europe. In the Middle Ages in France and to some extent in Italy, the dyeing of blue cloth was subject to license from the crown or state. In Italy, the dyeing of blue was assigned to a specific guild, the "tintori di guado," and could not be done by anyone else without severe penalty. The wearing of blue implied some dignity and some wealth.
Besides ultramarine, several other blues were widely used in the Middle Ages and later in the Renaissance. Azurite, a form of copper carbonate, was often used as a substitute for ultramarine. The Romans used it under the name lapis armenius, or Armenian stone. The British called it azure of Amayne, or German azure. The Germans themselves called it bergblau, or mountain stone. It was mined in France, Hungary, Spain and Germany, and it made a pale blue with a hint of green, which was ideal for painting skies. It was a favourite background colour of the German painter Albrecht Dürer.
Another blue often used in the Middle Ages was called tournesol or folium. It was made from the plant Crozophora tinctoria, which grew in the south of France. It made a fine transparent blue valued in medieval manuscripts.
Another common blue pigment was smalt, which was made by grinding blue cobalt glass into a fine powder. It made a deep violet blue similar to ultramarine, and was vivid in frescoes, but it lost some of its brilliance in oil paintings. It became especially popular in the 17th century, when ultramarine was difficult to obtain. It was employed at times by Titian, Tintoretto, Veronese, El Greco, Van Dyck, Rubens and Rembrandt.
In the European Renaissance.
In the Renaissance, a revolution occurred in painting; artists began to paint the world as it was actually seen, with perspective, depth, shadows, and light from a single source. Artists had to adapt their use of blue to the new rules. In medieval paintings, blue was used to attract the attention of the viewer to the Virgin Mary, and identify her. In Renaissance paintings, artists tried to create harmonies between blue and red, lightening the blue with lead white paint and adding shadows and highlights. Raphael was a master of this technique, carefully balancing the reds and the blues so no one colour dominated the picture.
Utramarine was the most prestigious blue of the Renaissance, and patrons sometimes specified that it be used in paintings they commissioned. The contract for the "Madone des Harpies" by Andrea del Sarto (1514) required that the robe of the Virgin Mary be coloured with ultramarine costing "at least five good florins an ounce." Good ultramarine was more expensive than gold; in 1508 the German painter Albrecht Dürer reported in a letter that he had paid twelve ducats- the equivalent of forty-one grams of gold - for just thirty grams of ultramarine.
Often painters or clients saved money by using less expensive blues, such as azurite smalt, or pigments made with indigo, but this sometimes caused problems. Pigments made from azurite were less expensive, but tended to turn dark and green with time. An example is the robe of the Virgin Mary in The "Madonna Enthroned with Saints" by Raphael in the Metropolitan Museum in New York. The Virgin Mary's azurite blue robe has degraded into a greenish-black.
The introduction of oil painting changed the way colours looked and how they were used. Ultramarine pigment, for instance, was much darker when used in oil painting than when used in tempera painting, in frescoes. To balance their colours, Renaissance artists like Raphael added white to lighten the ultramarine. The sombre dark blue robe of the Virgin Mary became a brilliant sky blue. Titian created his rich blues by using many thin glazes of paint of different blues and violets which allowed the light to pass through, which made a complex and luminous colour, like stained glass. He also used layers of finely ground or coarsely ground ultramarine, which gave subtle variations to the blue.
Blue and white porcelain.
In about the 9th century, Chinese artisans abandoned the Han blue colour they had used for centuries, and began to use cobalt blue, made with cobalt salts of alumina, to manufacture fine blue and white porcelain, The plates and vases were shaped, dried, the paint applied with a brush, covered with a clear glaze, then fired at a high temperature. Beginning in the 14th century, this type of porcelain was exported in large quantity to Europe where it inspired a whole style of art, called Chinoiserie. European courts tried for many years to imitate Chinese blue and white porcelain, but only succeeded in the 18th century after a missionary brought the secret back from China.
Other famous white and blue patterns appeared in Delft, Meissen, Staffordshire, and Saint Petersburg, Russia.
The war of the blues – indigo versus woad.
While blue was an expensive and prestigious colour in European painting, it became a common colour for clothing during the Renaissance. The rise of the colour blue in fashion in the 12th and 13th centuries led to the creation of a thriving blue dye industry in several European cities, notably Amiens, Toulouse and Erfurt. They made a dye called pastel from woad, a plant common in Europe, which had been used to make blue dye by the Celts and German tribes. Blue became a colour worn by domestics and artisans, not just nobles. In 1570, when Pope Pius V listed the colours that could be used for ecclesiastical dress and for altar decoration, he excluded blue, because he considered it too common.
The process of making blue with woad was particularly long and noxious- it involved soaking the leaves of the plant for from three days to a week in human urine, ideally urine from men who had been drinking a great deal of alcohol, which was said to improve the colour. The fabric was then soaked for a day in the urine, then put out in the sun, where as it dried it turned blue.
The pastel industry was threatened in the 15th century by the arrival from India of new blue dye, indigo, made from a shrub widely grown in Asia. Indigo blue had the same chemical composition as woad, but it was more concentrated and produced a richer and more stable blue. In 1498, Vasco de Gama opened a trade route to import indigo from India to Europe. In India, the indigo leaves were soaked in water, fermented, pressed into cakes, dried into bricks, then carried to the ports London, Marseille, Genoa and Bruges. Later, in the 17th century, the British, Spanish and Dutch established indigo plantations in Jamaica, South Carolina, the Virgin Islands and South America, and began to import American indigo to Europe.
The countries with large and prosperous pastel industries tried to block the use of indigo. The German government outlawed the use of indigo in 1577, describing it as a "pernicious, deceitful and corrosive substance, the Devil's dye." In France, Henry IV, in an edict of 1609, forbade under pain of death the use of "the false and pernicious Indian drug". It was forbidden in England until 1611, when British traders established their own indigo industry in India and began to import it into Europe.
The efforts to block indigo were in vain; the quality of indigo blue was too high and the price too low for pastel made from woad to compete. In 1737 both the French and German governments finally allowed the use of indigo. This ruined the dye industries in Toulouse and the other cities that produced pastel, but created a thriving new indigo commerce to seaports such as Bordeaux, Nantes and Marseille.
Another war of the blues took place at the end of the 19th century, between indigo and the new synthetic indigo, first discovered in 1868 by the German chemist Johann Friedrich Wilhelm Adolf von Baeyer. The German chemical firm BASF put the new dye on the market in 1897, in direct competition with the British-run indigo industry in India, which produced most of the world's indigo. In 1897 Britain sold ten thousand tons of natural indigo on the world market, while BASF sold six hundred tons of synthetic indigo. The British industry cut prices and reduced the salaries of its workers, but it was unable to compete; the synthetic indigo was more pure, made a more lasting blue, and was not dependent upon good or bad harvests. In 1911, India sold only 660 tons of natural indigo, while BASF sold 22,000 tons of synthetic indigo.
Not long after the battle between natural and synthetic indigo, chemists discovered a new synthetic blue dye, called indanthrene, which made a blue which did not fade. By the 1950s almost all fabrics, including blue jeans, were dyed with the new synthetic dye. In 1970, BASF stopped making synthetic indigo, and switched to newer synthetic blues. 
The blue uniform.
In the 17th century, Frederick William, Elector of Brandenburg, was one of the first rulers to give his army blue uniforms. The reasons were economic; the German states were trying to protect their pastel dye industry against competition from imported indigo dye. When Brandenburg became the Kingdom of Prussia in 1701, the uniform colour was adopted by the Prussian army. Most German soldiers wore dark blue uniforms until the First World War, with the exception of the Bavarians, who wore light blue.
Thanks in part to the availability of indigo dye, the 18th century saw the widespread use of blue military uniforms. Prior to 1748, British naval officers simply wore upper-class civilian clothing and wigs. In 1748, the British uniform for naval officers was officially established as an embroidered coat of the colour then called marine blue, now known as navy blue. When the Continental Navy of the United States was created in 1775, it largely copied the British uniform and colour.
In the late 18th century, the blue uniform became a symbol of liberty and revolution. In October 1774, even before the United States declared its independence, George Mason and one hundred Virginia neighbours of George Washington organised a voluntary militia unit (the Fairfax County Independent Company of Volunteers) and elected Washington the honorary commander. For their uniforms they chose blue and buff, the colours of the Whig Party, the opposition party in England, whose policies were supported by George Washington and many other patriots in the American colonies.
When the Continental Army was established in 1775 at the outbreak of the American Revolution, the first Continental Congress declared that the official uniform colour would be brown, but this was not popular with many militias, whose officers were already wearing blue. In 1778 the Congress asked George Washington to design a new uniform, and in 1779 Washington made the official colour of all uniforms blue and buff. Blue continued to be the colour of the field uniform of the U.S. Army until 1902, and is still the colour of the dress uniform.
In France, the Gardes Françaises, the elite regiment which protected Louis XVI, wore dark blue uniforms with red trim. In 1789, the soldiers gradually changed their allegiance from the King to the people, and they played a leading role in the storming of the Bastille. After the fall of Bastille, a new armed force, the Garde Nationale, was formed under the command of the Marquis de Lafayette, who had served with George Washington in America. Lafayette gave the Garde Nationale dark blue uniforms similar to those of the Continental Army. Blue became the colour of the Revolutionary armies, opposed to the white uniforms of the Royalists and the Austrians.
Napoleon Bonaparte abandoned many of the doctrines of the French Revolution but he kept blue as the uniform colour for his army, although he had great difficulty obtaining the blue dye, since the British controlled the seas and blocked the importation of indigo to France. Napoleon was forced to dye uniforms with woad, which had an inferior blue colour. The French army wore a dark blue uniform coat with red trousers until 1915, when it was found to be a too visible target on the battlefields of World War I. It was replaced with uniforms of a light blue-grey colour called horizon blue.
Blue was the colour of liberty and revolution in the 18th century, but in the 19th it increasingly became the colour of government authority, the uniform colour of policemen and other public servants. It was considered serious and authoritative, without being menacing. In 1829, when Robert Peel created the first London Metropolitan Police, he made the colour of the uniform jacket a dark, almost black blue, to make the policemen look different from soldiers, who until then had patrolled the streets. The traditional blue jacket with silver buttons of the London "bobbie" was not abandoned until the mid-1990s, when it was replaced by a light blue shirt and a jumper or sweater of the colour officially known as NATO blue.
The New York City Police Department, modelled after the London Metropolitan Police, was created in 1844, and in 1853, they were officially given a navy blue uniform, the colour they wear today.
The search for the perfect blue.
During the 17th and 18th centuries, chemists in Europe tried to discover a way to create synthetic blue pigments, avoiding the expense of importing and grinding lapis lazuli, azurite and other minerals. The Egyptians had created a synthetic colour, Egyptian blue, three thousand years BC, but the formula had been lost. The Chinese had also created synthetic pigments, but the formula was not known in the west.
In 1709, a German druggist and pigment maker named Diesbach accidentally discovered a new blue while experimenting with potassium and iron sulphides. The new colour was first called Berlin blue, but later became known as Prussian blue. By 1710 it was being used by the French painter Antoine Watteau, and later his successor Nicolas Lancret. It became immensely popular for the manufacture of wallpaper, and in the 19th century was widely used by French impressionist painters.
Beginning in 1820s, Prussian blue was imported into Japan through the port of Nagasaki. It was called "bero-ai", or Berlin Blue, and it became popular because it did not fade like traditional Japanese blue pigment, "ai-gami", made from the dayflower. Prussian blue was used by both Hokusai, in his famous wave paintings, and Hiroshige.
In 1824, the Societé pour l'Encouragement d'Industrie in France offered a prize for the invention of an artificial ultramarine which could rival the natural colour made from lapis lazuli. The prize was won in 1826 by a chemist named Jean Baptiste Guimet, but he refused to reveal the formula of his colour. In 1828, another scientist, Christian Gmelin then a professor of chemistry in Tübingen, found the process and published his formula. This was the beginning of new industry to manufacture artificial ultramarine, which eventually almost completely replaced the natural product.
In 1878, a German chemist named a. Von Baeyer discovered a synthetic substitute for indigotine, the active ingredient of indigo. This product gradually replaced natural indigo, and after the end of the First World War, it brought an end to the trade of indigo from the East and West Indies.
In 1901, a new synthetic blue dye, called Indanthrone blue, was invented, which had even greater resistance to fading during washing or in the sun. This dye gradually replaced artificial indigo, whose production ceased in about 1970. Today almost all blue clothing is dyed with an indanthrone blue.
The Impressionist painters.
The invention of new synthetic pigments in the 18th and 19th centuries considerably brightened and expanded the palette of painters. J.M.W. Turner experimented with the new cobalt blue, and of the twenty colours most used by the Impressionists, twelve were new and synthetic colours, including cobalt blue, ultramarine and cerulean blue.
Another important influence on painting in the 19th century was the theory of complementary colours, developed by the French chemist Michel Eugene Chevreul in 1828 and published in 1839. He demonstrated that placing complementary colours, such as blue and yellow-orange or ultramarine and yellow, next to each other heightened the intensity of each colour "to the apogee of their tonality." In 1879 an American physicist, Ogden Rood, published a book charting the complementary colours of each colour in the spectrum. This principle of painting was used by Claude Monet in his "Impression – Sunrise – Fog" (1872), where he put a vivid blue next to a bright orange sun, (1872) and in "Régate à Argenteuil" (1872), where he painted an orange sun against blue water. The colours brighten each other. Renoir used the same contrast of cobalt blue water and an orange sun in "Canotage sur la Seine" (1879–1880). Both Monet and Renoir liked to use pure colours, without any blending.
Monet and the impressionists were among the first to observe that shadows were full of colour. In his "La Gare Saint-Lazare", the grey smoke, vapour and dark shadows are actually composed of mixtures of bright pigment, including cobalt blue, cerulean blue, synthetic ultramarine, emerald green, Guillet green, chrome yellow, vermilion and ecarlate red. Blue was a favourite colour of the impressionist painters, who used it not just to depict nature but to create moods, feelings and atmospheres. Cobalt blue, a pigment of cobalt oxide-aluminium oxide, was a favourite of Auguste Renoir and Vincent van Gogh. It was similar to smalt, a pigment used for centuries to make blue glass, but it was much improved by the French chemist Louis Jacques Thénard, who introduced it in 1802. It was very stable but extremely expensive. Van Gogh wrote to his brother Theo, "'Cobalt [blue] is a divine colour and there is nothing so beautiful for putting atmosphere around things ..."
Van Gogh described to his brother Theo how he composed a sky: "The dark blue sky is spotted with clouds of an even darker blue than the fundamental blue of intense cobalt, and others of a lighter blue, like the bluish white of the Milky Way ... the sea was very dark ultramarine, the shore a sort of violet and of light red as I see it, and on the dunes, a few bushes of prussian blue."
The blue suit.
Blue had first become the high fashion colour of the wealthy and powerful in Europe in the 13th century, when it was worn by Louis IX of France, better known as Saint Louis (1214-1270). Wearing blue implied dignity and wealth, and blue clothing was restricted to the nobility. However, blue was replaced by black as the power colour in the 14th century, when European princes, and then merchants and bankers, wanted to show their seriousness, dignity and devoutness (see Black).
Blue gradually returned to court fashion in the 17th century, as part of a palette of peacock-bright colours shown off in extremely elaborate costumes. The modern blue business suit has its roots in England in the middle of the 17th century. Following the London plague of 1665 and the London fire of 1666, King Charles II of England ordered that his courtiers wear simple coats, waistcoats and breeches, and the palette of colours became blue, grey, white and buff. Widely imitated, this style of men's fashion became almost a uniform of the London merchant class and the English country gentleman.
During the American Revolution, the leader of the Whig Party in England, Charles James Fox, wore a blue coat and buff waistcoat and breeches, the colours of the Whig Party and of the uniform of George Washington, whose principles he supported. The men's suit followed the basic form of the military uniforms of the time, particularly the uniforms of the cavalry.
In the early 19th century, during the Regency of the future King George IV, the blue suit was revolutionized by a courtier named George Beau Brummel. Brummel created a suit that closely fitted the human form. The new style had a long tail coat cut to fit the body and long tight trousers to replace the knee-length breeches and stockings of the previous century. He used plain colours, such as blue and grey, to concentrate attention on the form of the body, not the clothes. Brummel observed, "If people turn to look at you in the street, you are not well dressed." This fashion was adopted by the Prince Regent, then by London society and the upper classes. Originally the coat and trousers were different colours, but in the 19th century the suit of a single colour became fashionable. By the late 19th century the black suit had become the uniform of businessmen in England and America. In the 20th century, the black suit was largely replaced by the dark blue or grey suit.
In the 20th and 21st century.
At the beginning of the 20th century, many artists recognised the emotional power of blue, and made it the central element of paintings. During his Blue Period (1901–1904) Pablo Picasso used blue and green, with hardly any warm colours, to create a melancholy mood. In Russia, the symbolist painter Pavel Kuznetsov and the Blue Rose art group (1906–1908) used blue to create a fantastic and exotic atmosphere. In Germany, Wassily Kandinsky and other Russian émigrés formed the art group called Der Blaue Reiter (The Blue Rider), and used blue to symbolise spirituality and eternity. Henri Matisse used intense blues to express the emotions he wanted viewers to feel. Matisse wrote, "A certain blue penetrates your soul."
In the art of the second half of the 20th century, painters of the abstract expressionist movement began to use blue and other colours in pure form, without any attempt to represent anything, to inspire ideas and emotions. Painter Mark Rothko observed that colour was "only an instrument;" his interest was "in expressing human emotions tragedy, ecstasy, doom, and so on."
In fashion, blue, particularly dark blue, was seen as a colour which was serious but not grim. In the mid-20th century, blue passed black as the most common colour of men's business suits, the costume usually worn by political and business leaders. Public opinion polls in the United States and Europe showed that blue was the favourite colour of over fifty per cent of respondents. Green was far behind with twenty per cent, while white and red received about eight per cent each.
In 1873 a German immigrant in San Francisco, Levi Strauss, invented a sturdy kind of work trousers, made of denim fabric and coloured with indigo dye, called blue jeans. In 1935, they were raised to the level of high fashion by Vogue magazine. Beginning in the 1950s, they became an essential part of uniform of young people in the United States, Europe, and around the world.
Blue was also seen as a colour which was authoritative without being threatening. Following the Second World War, blue was adopted as the colour of important international organisations, including the United Nations, the Council of Europe, UNESCO, the European Union, and NATO. United Nations peacekeepers wear blue helmets to stress their peacekeeping role. Blue is used by the NATO Military Symbols for Land Based Systems to denote friendly forces, hence the term "blue on blue" for friendly fire, and Blue Force Tracking for location of friendly units. The People's Liberation Army of China (formerly known as the "Red Army") uses the term "Blue Army" to refer to hostile forces during exercises.
The 20th century saw the invention of new ways of creating blue, such as chemiluminescence, making blue light through a chemical reaction.
In the 20th century, it also became possible to own your own colour of blue. The French artist Yves Klein, with the help of a French paint dealer, created a specific blue called International Klein blue, which he patented. It was made of ultramarine combined with a resin called Rhodopa, which gave it a particularly brilliant colour. The baseball team the Los Angeles Dodgers developed its own blue, called Dodger blue, and several American universities invented new blues for their colours.
With the dawn of the World Wide Web, blue has become the standard colour for hyperlinks in graphic browsers (though in most browsers links turn purple if you visit their target), to make their presence within text obvious to readers.
In science and industry.
Pigments and dyes.
Blue pigments were made from minerals, especially lapis lazuli and azurite (. These minerals were crushed, ground into powder, and then mixed with a quick-drying binding agent, such as egg yolk (tempera painting); or with a slow-drying oil, such as linseed oil, for oil painting. To make blue stained glass, cobalt blue (cobalt(II) aluminate: )pigment was mixed with the glass. Other common blue pigments made from minerals are ultramarine (), cerulean blue (primarily cobalt (II) stanate: ), and Prussian blue (milori blue: primarily ).
Natural dyes to colour cloth and tapestries were made from plants. Woad and true indigo were used to produce indigo dye used to colour fabrics blue or indigo. Since the 18th century, natural blue dyes have largely been replaced by synthetic dyes. 
Optics.
Human eyes perceive blue when observing light which has a wavelength between 450-495 nanometres. Blues with a higher frequency and thus a shorter wavelength gradually look more violet, while those with a lower frequency and a longer wavelength gradually appear more green. Pure blue, in the middle, has a wavelength of 470 nanometres.
Isaac Newton included blue as one of the seven colours in his first description the visible spectrum, He chose seven colours because that was the number of notes in the musical scale, which he believed was related to the optical spectrum. He included indigo, the hue between blue and violet, as one of the separate colours, though today it is usually considered a hue of blue.
In painting and traditional colour theory, blue is one of the three primary colours of pigments (red, yellow, blue), which can be mixed to form a wide gamut of colours. Red and blue mixed together form violet, blue and yellow together form green. Mixing all three primary colours together produces a dark grey. From the Renaissance onwards, painters used this system to create their colours. (See RYB colour system.)
The RYB model was used for colour printing by Jacob Christoph Le Blon as early as 1725. Later, printers discovered that more accurate colours could be created by using combinations of magenta, cyan, yellow and black ink, put onto separate inked plates and then overlaid one at a time onto paper. This method could produce almost all the colours in the spectrum with reasonable accuracy.
In the 19th century the Scottish physicist James Clerk Maxwell found a new way of explaining colours, by the wavelength of their light. He showed that white light could be created by combining red, blue and green light, and that virtually all colours could be made by different combinations of these three colours. His idea, called additive colour or the RGB colour model, is used today to create colours on televisions and computer screens. The screen is covered by tiny pixels, each with three fluorescent elements for creating red, green and blue light. If the red, blue and green elements all glow at once, the pixel looks white. As the screen is scanned from behind with electrons, each pixel creates its own designated colour, composing a complete picture on the screen.
On the HSV colour wheel, the complement of blue is yellow; that is, a colour corresponding to an equal mixture of red and green light. On a colour wheel based on traditional colour theory (RYB) where blue was considered a primary colour, its complementary colour is considered to be orange (based on the Munsell colour wheel).
Why the sky and sea appear blue.
Of the colours in the visible spectrum of light, blue has a very short wavelength, while red has the longest wavelength. When sunlight passes through the atmosphere, the blue wavelengths are scattered more widely by the oxygen and nitrogen molecules, and more blue comes to our eyes. This effect is called Rayleigh scattering, after Lord Rayleigh, the British physicist who discovered it. It was confirmed by Albert Einstein in 1911.
Near sunrise and sunset, most of the light we see comes in nearly tangent to the Earth's surface, so that the light's path through the atmosphere is so long that much of the blue and even green light is scattered out, leaving the sun rays and the clouds it illuminates red. Therefore, when looking at the sunset and sunrise, you will see the colour red more than any of the other colours.
The sea is seen as blue for largely the same reason: the water absorbs the longer wavelengths of red and reflects and scatters the blue, which comes to the eye of the viewer. The colour of the sea is also affected by the colour of the sky, reflected by particles in the water; and by algae and plant life in the water, which can make it look green; or by sediment, which can make it look brown.
Atmospheric perspective.
The farther away an object is, the more blue it often appears to the eye. For example, mountains in the distance often appear blue. This is the effect of atmospheric perspective; the farther an object is away from the viewer, the less contrast there is between the object and its background colour, which is usually blue. In a painting where different parts of the composition are blue, green and red, the blue will appear to be more distant, and the red closer to the viewer. The cooler a colour is, the more distant it seems.
Blue eyes.
Blue eyes do not actually contain any blue pigment. Eye colour is determined by two factors: the pigmentation of the eye's iris and the scattering of light by the turbid medium in the stroma of the iris. In humans, the pigmentation of the iris varies from light brown to black. The appearance of blue, green, and hazel eyes results from the Rayleigh scattering of light in the stroma, an optical effect similar to that which accounts for the blueness of the sky. The irises of the eyes of people with blue eyes contain less dark melanin than those of people with brown eyes, which means that they absorb less short-wavelength blue light, which is instead reflected out to the viewer. Eye colour also varies depending on the lighting conditions, especially for lighter-coloured eyes.
Blue eyes are most common in Ireland, the Baltic Sea area and Northern Europe, and are also found in Eastern, Central, and Southern Europe. Blue eyes are also found in parts of Western Asia, most notably in Afghanistan, Syria, Iraq, and Iran. In Estonia, 99% of people have blue eyes. In Denmark 30 years ago, only 8% of the population had brown eyes, though through immigration, today that number is about 11%. In Germany, about 75% have blue eyes.
In the United States, as of 2006, one out of every six people, or 16.6% of the total population, and 22.3% of the white population, have blue eyes, compared with about half of Americans born in 1900, and a third of Americans born in 1950. Blue eyes are becoming less common among American children. In the U.S., boys are 3-5 per cent more likely to have blue eyes than girls.
Lasers.
Lasers emitting in the blue region of the spectrum became widely available to the public in 2010 with the release of inexpensive high-powered 445-447 nm Laser diode technology. Previously the blue wavelengths were accessible only through DPSS which are comparatively expensive and inefficient, however these technologies are still widely used by the scientific community for applications including Optogenetics, Raman spectroscopy, and Particle image velocimetry, due to their superior beam quality. Blue Gas lasers are also still commonly used for Holography, DNA sequencing, Optical pumping, and other scientific and medical applications.
In world culture.
As a national and international colour.
Various shades of blue are used as the national colours for many nations.
Gender.
Blue was first used as a gender signifier just prior to World War I (for either girls or boys), and first established as a male gender signifier in the 1940s.
Sports.
Many sporting teams make blue their official colour, or use it as detail on kit of a different colour. In addition, the colour is present on the logos of many sports associations.
Association football.
In international association football, blue is a common colour on kits, as a majority of nations wear the colours of their national flag. A notable exception is four-time FIFA World Cup winners Italy, who wear a blue kit based on the "Azzuro Savoia" (Savoy blue) of the royal House of Savoy which unified the Italian states. The team themselves are known as "Gli Azzurri" (the Blues). Another World Cup winning nation with a blue shirt is France, who are known as "Les Bleus" (the Blues). Two neighbouring countries with two World Cup victories each, Argentina and Uruguay wear a light blue shirt, the former with white stripes. Uruguay are known as the "La Celeste", Spanish for 'the sky blue one', while Argentina are known as "Los Albicelestes", Spanish for 'the sky blue and whites'.
Football clubs which have won the European Cup or Champions League and wear blue include FC Barcelona of Spain (red and blue stripes), FC Internazionale Milano of Italy (blue and black stripes) and FC Porto of Portugal (blue and white stripes). Another European Cup-winning club, Aston Villa of England, wear light blue detailing on a mostly claret shirt, often as the colour of the sleeves. Clubs which have won the Copa Libertadores, a tournament for South American clubs, and wear blue include six-time winners Boca Juniors of Buenos Aires, Argentina. They wear a blue shirt with a yellow band across.
Blue features on the logo of football's governing body FIFA, as well as featuring highly in the design of their website. The European governing body of football, UEFA, uses two tones of blue to create a map of Europe in the centre of their logo. The Asian Football Confederation, Oceania Football Confederation and CONCACAF (the governing body of football in North and Central America and the Caribbean) use blue text on their logos.
North American sporting leagues.
In Major League Baseball, the premier baseball league in the United States of America and Canada, blue is one of the three colours, along with white and red, on the league's official logo. A team from Toronto, Ontario, are the Blue Jays. The Los Angeles Dodgers use blue prominently on their uniforms and the phrase "Dodger Blue" is may be said to describe Dodger fans' "blood". The Texas Rangers also use Blue prominently on their uniforms and logo.
The National Basketball Association, the premier basketball league in the United States and Canada, also has blue as one of the colours on their logo, along with red and white also, as does its female equivalent, the WNBA. The Sacramento Monarchs of the WNBA wear blue. Former NBA player Theodore Edwards was nicknamed "Blue". The only NBA teams to wear blue as first choice are the Charlotte Hornets and the Indiana Pacers; however, blue is a common away colour for many other franchises.
The National Football League, the premier American football league in the United States, also uses blue as one of three colours, along with white and red, on their official logo. The Seattle Seahawks, New York Giants, Buffalo Bills, Indianapolis Colts, New England Patriots, Tennessee Titans, Denver Broncos, Houston Texans, San Diego Chargers, Dallas Cowboys, Chicago Bears and Detroit Lions feature blue prominently on their uniforms.
The National Hockey League, the premier Ice hockey league in Canada and the United States, uses blue on its official logo. Blue is the main colour of many teams in the league: the Buffalo Sabres, Columbus Blue Jackets, Edmonton Oilers, New York Islanders, New York Rangers, St. Louis Blues, Toronto Maple Leafs, Tampa Bay Lightning, Vancouver Canucks and the Winnipeg Jets.

</doc>
<doc id="4544" url="http://en.wikipedia.org/wiki?curid=4544" title="Blind Willie McTell">
Blind Willie McTell

Blind Willie McTell (born William Samuel McTier; May 5, 1898 – August 19, 1959) was a Piedmont and ragtime blues singer and guitarist. He played with a fluid, syncopated fingerstyle guitar technique, common among many exponents of Piedmont blues, although, unlike his contemporaries, he came to use twelve-string guitars exclusively. McTell was also an adept slide guitarist, unusual among ragtime bluesmen. His vocal style, a smooth and often laid-back tenor, differed greatly from many of the harsher voice types employed by Delta bluesmen, such as Charley Patton. McTell embodied a variety of musical styles, including blues, ragtime, religious music and hokum.
Born in the town of Thomson, Georgia, McTell learned how to play guitar in his early teens. He soon became a street performer around several Georgia cities including Atlanta and Augusta, and first recorded in 1927 for Victor Records. Although he never produced a major hit record, McTell's recording career was prolific, recording for different labels under different names throughout the 1920s and 30s. In 1940, he was recorded by folklorist John A. Lomax and Ruby Terrill Lomax for the Library of Congress's folk song archive. He would remain active throughout the 1940s and 50s, playing on the streets of Atlanta, often with his longtime associate, Curley Weaver. Twice more he recorded professionally. McTell's last recordings originated during an impromptu session recorded by an Atlanta record store owner in 1956. McTell would die three years later after suffering for years from diabetes and alcoholism. Despite his mainly failed releases, McTell was one of the few archaic blues musicians that would actively play and record during the 1940s and 50s. However, McTell never lived to be "rediscovered" during the imminent American folk music revival, as many other bluesmen would.
McTell's influence extended over a wide variety of artists, including The Allman Brothers Band, who famously covered McTell's "Statesboro Blues", and Bob Dylan, who paid tribute to McTell in his 1983 song "Blind Willie McTell"; the refrain of which is, "And I know no one can sing the blues, like Blind Willie McTell". Other artists influenced by McTell include Taj Mahal, Alvin Youngblood Hart, Ralph McTell, Chris Smither and The White Stripes.
Biography.
Born William Samuel McTier in Thomson, Georgia, blind in one eye, McTell had lost his remaining vision by late childhood. He attended schools for the blind in the states of Georgia, New York and Michigan and showed proficiency in music from an early age, first playing harmonica and accordion, learning to read and write music in Braille, and turning to the six-string guitar in his early teens. His family background was rich in music, both of his parents and an uncle played guitar; he is also a relation of bluesman and gospel pioneer Thomas A. Dorsey. His father left the family when McTell was still young, and, when his mother died in the 1920s, he left his hometown and became a wandering musician, or "songster". He began his recording career in 1927 for Victor Records in Atlanta.
McTell married Ruth Kate Williams, now better known as Kate McTell, in 1934. She accompanied him on stage and on several recordings before becoming a nurse in 1939. Most of their marriage from 1942 until his death was spent apart, with her living in Fort Gordon near Augusta and him working around Atlanta.
In the years before World War II, McTell traveled and performed widely, recording for a number of labels under many different names, including Blind Willie McTell (Victor and Decca), Blind Sammie (Columbia), Georgia Bill (Okeh), Hot Shot Willie (Victor), Blind Willie (Vocalion and Bluebird), Barrelhouse Sammie (Atlantic), and Pig & Whistle Red (Regal). The "Pig 'n Whistle" appellation was a reference to a chain of Atlanta barbecue restaurants, one of which was located on the south side of East Ponce de Leon between Boulevard and Moreland Avenue, which later became a Krispy Kreme. McTell would frequently played for tips in the parking lot of this location. He was also known to play behind the nearby building that later became Ray Lee's Blue Lantern Lounge. Like his fellow songster Lead Belly, who began his career as a street artist, McTell favored the somewhat unwieldy and unusual twelve-string guitar, whose greater volume made it suitable for outdoor playing.
In 1940 John A. Lomax and his wife, Ruby Terrill Lomax, Classics professor at the University of Texas at Austin, interviewed and recorded McTell for the Library of Congress's Folk Song Archive in a two-hour session held in their hotel room in Atlanta, Georgia. These recordings document McTell's distinctive musical style, which bridges the gap between the raw country blues of the early part of the 20th century and the more conventionally melodious, Ragtime-influenced East-Coast Piedmont blues sound. Mr. and Mrs. Lomax also elicited from the singer a number of traditional songs (such as "The Boll Weevil" and "John Henry") as well as spirituals (such as "Amazing Grace"), which were not part of his usual commercial repertoire. In the interview, John A. Lomax is heard asking if McTell knows any "complaining" songs (an earlier term for protest songs), to which the singer replies somewhat uncomfortably and evasively that he does not. The Library of Congress paid McTell $10, the equivalent of $154.56 in 2011, for this two-hour session. The material from this 1940 session was issued in 1960 in LP and later in CD form, under the somewhat misleading title of "The Complete Library of Congress Recordings", notwithstanding the fact that it was in fact truncated, in that it omitted some of John A. Lomax's interactions with the singer and cut out entirely the contributions of Ruby Terrill Lomax.
Postwar, McTell recorded for Atlantic Records and Regal Records in 1949, but these recordings met with less commercial success than his previous works. He continued to perform around Atlanta, but his career was cut short by ill health, predominantly diabetes and alcoholism. In 1956, an Atlanta record store manager, Edward Rhodes, discovered McTell playing in the street for quarters and enticed him with a bottle of corn liquor into his store, where he captured a few final performances on a tape recorder. These were released posthumously on Prestige/Bluesville Records as "Last Session". Beginning in 1957, McTell occupied himself as a preacher at Atlanta's Mt. Zion Baptist Church.
McTell died in Milledgeville, Georgia, of a stroke in 1959. He was buried at Jones Grove Church, near Thomson, Georgia, his birthplace. A fan paid to have a gravestone erected on his resting place. The name given on his gravestone is Eddie McTier. He was inducted into the Blues Foundation's Hall of Fame in 1981, and into the Georgia Music Hall of Fame in 1990.
Influence.
One of McTell's most famous songs, "Statesboro Blues," was frequently covered by The Allman Brothers Band and is considered one of their earliest signature songs. A short list of some of the artists who also perform it includes Taj Mahal, David Bromberg, The Devil Makes Three and Ralph McTell, who changed his name on account of liking the song. Ry Cooder covered McTell's "Married Man's a Fool" on his 1973 album, "Paradise and Lunch". Jack White of The White Stripes considers McTell an influence, as their 2000 album "De Stijl" was dedicated to him and featured a cover of his song "Southern Can Is Mine". The White Stripes also covered McTell's "Lord, Send Me an Angel", releasing it as a single in 2000. In 2013 Jack White's Third Man Records teamed up with Document Records to reissue The Complete Recorded Works in Chronological Order of Charley Patton, Blind Willie McTell and The Mississippi Sheiks.
Bob Dylan has paid tribute to McTell on at least four occasions: Firstly, in his 1965 song "Highway 61 Revisited", the second verse begins with "Georgia Sam he had a bloody nose", referring to one of Blind Willie McTell's many recording names; later in his song "Blind Willie McTell", recorded in 1983 but released in 1991 on "The Bootleg Series Volumes 1-3"; then with covers of McTell's "Broke Down Engine" and "Delia" on his 1993 album, "World Gone Wrong"; also, in his song "Po' Boy", on 2001's "Love & Theft", which contains the lyric, "had to go to Florida dodging them Georgia laws", which comes from McTell's "Kill It Kid".
Also, Bath-based band "Kill It Kid" is named after that song.
A blues bar in Atlanta is named after McTell and regularly features blues musicians and bands. The Blind Willie McTell Blues Festival is held annually in Thomson, Georgia.

</doc>
<doc id="4545" url="http://en.wikipedia.org/wiki?curid=4545" title="BDSM">
BDSM

BDSM is a variety of erotic practices involving dominance and submission, role-playing, restraint, and other interpersonal dynamics. Given the wide range of practices, some of which may be engaged in by people who do not consider themselves as practicing BDSM, inclusion in the BDSM community or subculture is usually dependent on self-identification and shared experience. Interest in BDSM can range from one-time experimentation to a lifestyle.
The term "BDSM" dates back to 1969; however, the origin of the term BDSM is unclear and is believed to have been formed either from joining the term "B&D" (bondage and discipline) with "S&M" (sadomasochism or sadism and masochism), or as a compound initialism from B&D, D&S (dominance and submission), and S&M. Regardless of its origin, BDSM is used as a catch-all phrase to include a wide range of activities, forms of interpersonal relationships, and distinct subcultures. BDSM communities generally welcome anyone with a non-normative streak who identifies with the community; this may include cross-dressers, extreme body mod enthusiasts, animal players, latex or rubber aficionados, and others.
Unlike the usual "power neutral" relationships and play styles commonly followed by couples, activities and relationships within a BDSM context are often characterized by the participants' taking on complementary, but unequal roles; thus, the idea of informed consent of both the partners becomes essential. Participants who exert sexual control over their partners are known as dominants or tops while participants who take the passive, receiving, or obedient role are known as submissives or bottoms.
Individuals are also sometimes abbreviated when referred to in writing, so a dominant person may be referred to as a Dom for a man, and a Domme for a woman. Individuals who can change between top/dominant and bottom/submissive roles—whether from relationship to relationship or within a given relationship—are known as switches, though the term "switch" in this context is occasionally seen as derogatory and is rejected by many who might simplistically fit the definition. The precise definition of roles and self-identification is a common subject of debate, reflection, and discussion within the community.
Fundamentals.
"BDSM" has become an umbrella term for certain kinds of erotic behavior between consenting adults. There are various subcultures under this umbrella term and distinct difference between these subcultures. Terminology for roles varies widely within the various BDSM subcultures. "Top" and "dominant" are widely recognized for those partner(s) in the relationship or activity who are, respectively, the physically active or controlling participants. "Bottom" and "submissive" are widely recognized terms for those partner(s) in the relationship or activity who are, respectively, the physically receptive or controlled participants. The interaction between tops and bottoms—where physical or mental control of the bottom is surrendered to the top—is sometimes known as power exchange, whether in the context of an encounter or a relationship.
BDSM actions can often take place during a specific period of time agreed to by both parties, referred to as "play", "a scene" or "a session". Participants usually derive pleasure from this, even though many of the practices—such as inflicting pain or humiliation or being restrained—would be unpleasant under other circumstances. Explicit sexual activity, such as sexual penetration, may occur within a session, but is not essential. Such explicit sexual interaction is seen only rarely in public play spaces, and it is sometimes specifically banned by the rules of a party or playspace. Whether it is a public "playspace"—ranging from a party at an established community dungeon to a hosted play "zone" at a nightclub or social event—the parameters of allowance can vary. Some restrict a policy of panties/nipple tape for women (underwear for men) and some allow full nudity with explicit sexual interaction allowed.
The fundamental principles for the exercise of BDSM require that it should be performed with the informed consent of all involved parties. Since the 1980s, many practitioners and organizations have adopted the motto (originally from the statement of purpose of GMSMA—a gay SM activist organization) "safe, sane and consensual", commonly abbreviated as "SSC", which means that everything is based on safe activities, that all participants be of sufficiently sound/sane mind to consent, and that all participants do consent. It is mutual consent that makes a clear legal and ethical distinction between BDSM and such crimes as sexual assault or domestic violence.
Some BDSM practitioners prefer a code of behavior that differs from "SSC" and is described as "risk aware consensual kink" (RACK), indicating a preference for a style in which the "individual" responsibility of the involved parties is emphasized more strongly, with each participant being responsible for his or her own well-being. Advocates of RACK argue that SSC can hamper discussion of risk because no activity is truly "safe", and that discussion of even low-risk possibilities is necessary for truly informed consent. They further argue that setting a discrete line between "safe" and "not-safe" activities ideologically denies consenting adults the right to evaluate risks vs rewards for themselves; that some adults will be drawn to certain activities regardless of the risk; and that BDSM play—particularly higher-risk play or edgeplay—should be treated with the same regard as extreme sports, with both respect and the demand that practitioners educate themselves and practice the higher-risk activities to decrease risk. RACK may be seen as focusing primarily upon awareness and informed consent, rather than accepted safe practices. Consent is the most important criterion here. The consent and compliance for a sadomasochistic situation can be granted only by people who can judge the potential results. For their consent, they must have relevant information (extent to which the scene will go, potential risks, if a safeword will be used, what that is, and so on) at hand and the necessary mental capacity to judge. The resulting consent and understanding is occasionally summarized in a written "contract", which is an agreement of what can and cannot take place.
In general, BDSM play is usually structured such that it is possible for the consenting partner to withdraw his or her consent during a scene; for example, by using a safeword that was agreed on in advance. Use of the agreed safeword (or occasionally a "safe symbol" such as dropping a ball or ringing a bell, especially when speech is restricted) is seen by some as an explicit withdrawal of consent. Failure to honor a safeword is considered serious misconduct and could even change the sexual consent situation into a crime, depending on the relevant law, since the bottom has explicitly revoked his or her consent to any actions that follow the use of the safeword (see Legal status). For other scenes, particularly in established relationships, a safeword may be agreed to signify a warning ("this is getting too intense") rather than explicit withdrawal of consent; and a few choose not to use a safeword at all. This is sometimes the case for "punishment scenes" between master/slave couples or for some extreme or edgeplay scenes which may include abductions, rape play, or interrogation. This scene dynamic may be referred to as "consensual nonconsent". In some scenes or relationships it may be impossible for consent to be withdrawn in the middle of a scene, or the bottom may have the ability to revoke consent for a relationship as a whole, but not for a particular scene.
Safety.
Aside from the general advice related to safe sex, BDSM sessions often require a wider array of safety precautions than vanilla sex (sexual behavior without BDSM elements). In theory, to ensure consent related to BDSM activity, pre-play negotiations are commonplace, especially among partners who do not know each other very well. In practice, pick-up scenes at clubs or parties may sometimes be low in negotiation (much as pick-up sex from singles bars may not involve much negotiation or disclosure). Ideally, these negotiations concern the interests and fantasies of each partner and establish a framework of both acceptable and unacceptable activities. This kind of discussion is a typical "unique selling proposition" of BDSM sessions and quite commonplace. Additionally, safewords are often arranged to provide for an immediate stop of any activity if any participant should so desire.
Safewords are, by definition, not commonly used words during any kind of play. Words such as "no", "stop", and "don't", are often not appropriate as a safeword if the roleplaying aspect includes the illusion of non-consent. A safeword is a word or phrase, usually something both parties can remember and recognize and not a word that might be used playfully during a scene (such as "stop" or "don't"), that is called out when things are either not going as planned or have crossed a threshold one cannot handle. The most commonly used safewords are "red" and "yellow", with "red" meaning that play must stop immediately, and "yellow" meaning that the activity needs to slow down. At most clubs and group-organized BDSM parties and events, Dungeon monitors (DMs) provide an additional safety net for the people playing there, ensuring that house rules are followed and safewords respected.
BDSM participants ideally are expected to understand practical safety aspects. For instance, they are expected to recognize that parts of the body can be damaged, such as nerves and blood vessels by contusion, or that skin that can be scarred. Using crops, whips, or floggers, the top's fine motor skills and anatomical knowledge can make the difference between a satisfying session for the bottom and a highly unpleasant experience that may even entail severe physical harm. The very broad range of BDSM "toys" and physical and psychological control techniques often requires a far-reaching knowledge of details related to the requirements of the individual session, such as anatomy, physics, and psychology. Despite these risks, BDSM activities usually result in far less severe injuries than sports like boxing and football, and BDSM practitioners do not visit emergency rooms any more often than the general population.
It is necessary to be able to identify each person's psychological "squicks" or triggers in advance to avoid them. Such losses of emotional balance due to sensory or emotional overload are a fairly commonly discussed issue. It is important to follow participants' reactions empathetically and continue or stop accordingly. For some players, sparking "freakouts" or deliberately using triggers may be a desired outcome. Safe words are one way for BDSM practices to protect both parties. However, partners should be aware of each other's psychological states and behaviors to prevent instances where the "freakouts" prevent the use of safewords.
Aspects.
The initialism BDSM includes psychological and physiological facets:
This model for differentiating among these three aspects of BDSM is increasingly used in literature today. Nevertheless, it is only an attempt at phenomenological differentiation. Individual tastes and preferences in the area of sexuality may overlap among these areas, which are discussed separately here.
Bondage and Discipline are two aspects of BDSM that do not seem to relate to each other because of the type of activities involved, but they have conceptual similarities, and that is why they appear jointly. Contrary to the other two types, B/D does not define the Tops and Bottoms itself, and is used to describe the general activities with either partner being the receiver and the giver.
The term "bondage" describes the practice of Physical restraining. Bondage is usually, but not always, a sexual practice. While bondage is a very popular variation within the larger field of BDSM, it is nevertheless sometimes differentiated from the rest of this field. Studies among BDSM practitioners in the US have shown that about half of all men find the idea of bondage to be erotic; many women do as well. Strictly speaking, bondage means binding the partner by tying their appendages together; for example, by the use of handcuffs or by lashing their arms to an object. Bondage can also be achieved by spreading the appendages and fastening them with chains to a St. Andrews cross or spreader bars.
The term "discipline" describes psychological restraining, with the use of rules and punishment to control overt behavior. Punishment can be pain caused physically (such as caning), humiliation caused psychologically (such as a public flagellation) or loss of freedom caused physically (for example, chaining the submissive partner to the foot of a bed). Another aspect is the structured training of the Bottom.
"Dominance and submission" (also known as D&s, Ds or D/s) is a set of behaviors, customs and rituals relating to the giving and accepting of control of one individual over another in an erotic or lifestyle context. It explores the more mental aspect of BDSM. This is also the case in many relationships not considering themselves as sadomasochistic; it is considered to be a part of BDSM if it is practiced purposefully. The range of its individual characteristics is thereby wide.
Examples of mentally oriented practices are "education games", during which the dominant requires certain forms of behavior from the submissive. Special forms include erotic roleplay like ageplay, in which a difference in age, either real or enacted, formulates the background; or petplay. Concerted deployed sexual rejection exercised on the partner can be an aspect of "Dominance and Submission" as well. The most established and probably most cliché set form of dominance and submission is "Dominance and slavery". These can be administered for the short duration of a session among otherwise-emancipated partners, but also can be integrated into everyday life indefinitely. In a few relationships, it leads as far as total submission of one partner in the truest sense of the phrase "total power exchange" (frequently abbreviated as TPE). Compensating elements of the total dominance and submission are care and devotion complementing one another, thus facilitating stable relationships. The consensual submission of the submissive is sometimes demonstrated to others by symbols indicating his/her belonging to the dominant, such as wearing a collar, special tattoos, piercings, a very short haircut or a bald head.
Often, "slave contracts" are set out in writing to record the formal consent of the parties to the power exchange, stating their common vision of the relationship dynamic. The purpose of this kind of agreement is primarily to encourage discussion and negotiation in advance, and then to document that understanding for the benefit of all parties. Such documents have not been recognized as being legally binding, nor are they intended to be. These agreements are binding in the sense that the parties have the expectation that the negotiated rules will be followed. Often other friends and community members may witness the signing of such a document in a ceremony, and so parties violating their agreement can result in loss of face, respect or status with their friends in the community.
In general, as compared to conventional relationships, BDSM participants go to great lengths to negotiate the important aspects of their relationships in advance, and to take great care in learning about and following safe practices.
The term "sadomasochism" is derived from the words "sadism" and "masochism" (see Etymology). In the context of consensual erotic activities, sadism and masochism are not strictly accurate terms; there is a significant difference from the medical or psychological usage of both terms. "Sadomasochism" refers to the aspects of BDSM surrounding the exchange of physical or emotional pain. Sadism describes sexual pleasure derived by inflicting pain, degradation, humiliation on another person or causing another person to suffer. On the other hand, the masochist enjoys being hurt, humiliated, or suffering within the consensual scenario. Sadomasochistic scenes sometimes reach a level that appear more extreme or cruel than other forms of BDSM—for example, when a masochist is brought to tears or is severely bruised—and is occasionally unwelcome at BDSM events or parties. Sadomasochism does not imply enjoyment through causing or receiving pain in other situations (for example, accidental injury, medical procedures).
Discipline often incorporates sadomasochistic aspects, though some sadomasochists distance themselves from D/s practices such as punishment. Sadomasochism is practiced in isolation relatively rarely, though some masochists report biting, pinching, or even stun-gunning themselves as a prelude to, or as part of, masturbation.
In D/S, the Dominant is the Top and the submissive is the Bottom. In S/M, the Sadist is usually the Top and the Masochist the Bottom, but these roles are frequently more complicated or jumbled (as in the case of being dominant, masochists who may arrange for their submissive to carry out s/m activities on them). As in B/D, the declaration of the Top/Bottom may be required, though sadomasochists may also play without any Power Exchange at all, with both partners equally in control of the play.
Physical aspects.
On a physical level, BDSM is commonly misconceived to be "all about pain". Most often, though, BDSM practitioners are primarily concerned with power, humiliation, and pleasure. Of the three categories of BDSM, only sadomasochism specifically requires pain, but this is typically a vehicle for feelings of humiliation, dominance, etc. The aspects of D/s and B/D may not include physical suffering at all, but include the sensations inherited by different emotions of the mind. Dominance & submission of power is an entirely different experience, and is not always psychologically associated with physical pain. Many BDSM activities might not involve any kind of pain or humiliation, but just the exchange of Powers (Power Exchange). During the activities, the practitioners may feel endorphins comparable to the so-called "runner's high" or to the afterglow of orgasm. The corresponding trance-like mental state is also known as "subspace" for the submissive, or "topspace" for the dominant. Some use the term "body stress" to describe this physiological sensation. This experience of algolagnia is important, but is not the only motivation for many BDSM practitioners. The philosopher Edmund Burke defines this sensation of pleasure derived from pain by the word "sublime". There is a wide array of BDSM practitioners who take part in sessions for which they do not receive any personal gratification. They enter such situations solely with the intention to allow their partners to fulfill their own needs or fetishes. They do this in exchange of money for the session activities.
In some BDSM sessions, the Top exposes the Bottom to a wide range of sensual impressions, for example: pinching, biting, scratching with fingernails, spanking or the use of various objects such as crops, whips, liquid wax, icecubes, Wartenberg wheels, erotic electrostimulation or others. Fixation by handcuffs, ropes or chains may be used as well. The repertoire of possible "toys" is limited only by the imagination of both partners. To some extent, everyday items like clothes-pins, wooden spoons or plastic wrap are used as pervertables. It is commonly considered that a pleasurable BDSM experience during a session is very strongly dependent upon the top's competence and experience and the bottom's physical and mental state at the time of the session. Trust and sexual arousal help the partners enter a shared mindset. Some BDSM practitioners compare related sensations with musical compositions and representation, in which single sensual impressions are the musical notes of the situation. From this point of view, different sensuous impressions are combined to create a total experience leaving a lasting impression.
Types of play.
Some types of BDSM play include, but are not limited to:
Relationships.
Roles.
In a BDSM relationship, the partner who has the active role in a session or in the entire relationship is described as the "top", a role that often involves inflicting pain, degradation or subjugation. The partner referred to as the "bottom" submits voluntarily to the actions of the top.
Although the top is usually also the dominant partner and the bottom the submissive partner, it is not inevitably so. In some cases the top (known in this case as a service top) follows instructions from the bottom according to the bottom's desires and in a way the bottom expressly requires. If the bondage/discipline aspect of BDSM involves a top actively performing a skill while a bottom willingly submits, then the dominance/submission aspect here is reversed from what is typically expected.
By contrast, a dominant top controls their submissive bottom partner, sometimes by using physical or psychological techniques, although consent is always established first. This power relationship is so common, "bottom" and "submissive" are sometimes used interchangeably. However, most frequently, bottom refers to anyone receiving an act while submissive refers to someone who is obedient to a dominant, i.e.; a dominant receiving a massage from a submissive is considered a bottom while the submissive is the top during the act of giving the massage. The top or bottom actors may change but the roles of dominant and submissive remain the same.
A similar distinction also may apply to bottoms. At one end of the spectrum are those who are indifferent to, or even reject physical stimulation. At the other end of the spectrum are bottoms who enjoy discipline and erotic humiliation but are not willing to be subordinate to the person who applies it. The bottom is frequently the partner who specifies the basic conditions of the session and gives instructions, directly or indirectly, in the negotiation, while the top often respects this guidance. Other bottoms often called "brats" try to incur punishment from their tops by provoking them or "misbehaving". Nevertheless a small, very puristic "school" exists within the BDSM community, which regards such "topping from the bottom" as rude or even incompatible with the standards of BDSM relations.
BDSM practitioners may also "switch", meaning they play either or both roles. He or she may practice this with one or more specific partners. The many reasons for switching are often very subjective and sometimes situational. A switch may simply enjoy both top and bottom roles or may be experimenting. Sometimes a relationship with a partner of the same primary preference (for example, two tops) requires switching to fulfill various BDSM needs within that relationship. Some change roles but do not regard themselves as switches as they do so irregularly or under specific circumstances only.
Types of relationships.
BDSM practitioners sometimes regard the practice of BDSM in their sex life as role playing and so often use the terms "Play" and "Playing" to describe activities where in their roles. Play of this sort for a specified period of time is often called a "Session", and the contents and the circumstances of play are often referred to as the "Scene". It is also common in personal relationships to use the term "Kink Play" for BDSM activities, or more specific terms for the type of activity. The relationships can be of varied types.
Early writings on BDSM both by the academic and BDSM community spoke little of long-term relationships with some in the Gay Leather community suggesting short-term play relationships to be the only feasible relationship models, and recommending people to get married and "play" with BDSM outside of marriage. In recent times though writers of BDSM and sites for BDSM have been more focused on long-term relationships.
A 2003 study, the first to look at these relationships, fully demonstrated that "quality long-term functioning relationships" exist among practitioners of BDSM, with either sex being the top or bottom (homosexual couples were not looked at). Respondents in the study expressed their BDSM orientation to be built into who they are, but considered exploring their BDSM interests an ongoing task, and showed flexibility and adaptability in order to match their interests with their partners. The "perfect match" where both in the relationship shared the same tastes and desires was rare, and most relationships required both partners to take up or put away some of their desires. The BDSM activities that the couples partook in varied in sexual to nonsexual significance for the partners who reported doing certain BDSM activities for "couple bonding, stress release, and spiritual quests". The most reported issue amongst respondents was not finding enough time to be in role with most adopting a 24/7 lifestyle wherein both partners maintain their dominant or submissive role throughout the day.
Amongst the respondents it was typically the bottoms who wanted to play harder, and be more restricted into their roles when there was a difference in desire to play in the relationship. The author of the study, Bert Cutler, speculated that tops may be less often in the mood to play due to the increased demand for responsibility on their part: being aware of the safety of the situation and prepared to remove the bottom from a dangerous scenario, being conscious of the desires and limits of the bottom, and so on. The author of the study stressed that successful long-term BDSM relationships came after "early and thorough disclosure" from both parties of their BDSM interests.
Many of those engaged in long-term BDSM relationships learned their skills from larger BDSM organizations and communities There was a lot of discussion by the respondents on the amount of control the top possessed in the relationships with almost non-existent discussion of the top "being better, or smarter, or of more value" than the bottom.
Couples were generally of the same mind of whether or not they were in 24/7 relationship, but noted that in such cases the bottom is not locked up 24/7, but that their role in the context of the relationship was always present, even when the top is doing non-dominant activities such as household chores: cleaning, taking out the trash, and so on., or the bottom being in a more dominant position. In its conclusion the study states:
"The respondents valued themselves, their partners, and their relationships. All couples expressed considerable good will toward their partners. The power exchange between the cohorts appears to be serving purposes beyond any sexual satisfaction, including experiencing a sense of being taken care of and bonding with a partner."
The study further goes on to list three aspects that made the successful relationships work: early disclosure of interests and continued transparency, a commitment to personal growth, and the use of the dominant/submissive roles as a tool to maintain the relationship. In his closing remarks the author of the study theorizes that due to the serious potential for harm that couples in BDSM relationships develop increased communication that may be higher than in mainstream relationships
A professional dominatrix or professional dominant, often referred to within the culture as a "pro-dom(me)", offers services encompassing the range of bondage, discipline, and dominance in exchange for money. The term "Dominatrix" is little-used within the non-professional BDSM scene. A non-professional dominant woman is more commonly referred to simply as a "Domme", "Dominant", or "Femdom". There are also services provided by professional female submissives ("pro-subs"). A professional submissive consents to her client's dominant behavior within negotiated limits, and often works within a professional dungeon. Professional submissives, although far more rare, do exist. Most of the people who work as subs normally have tendencies towards such activities, especially when sadomasochism is involved. Males also work as professional "Tops" in BDSM, and are called "Masters" or "Doms". However it is much more rare to find a male in this profession. A male "pro-dom" typically only works with male clientele.
BDSM scene.
In BDSM, a scene is the stage or setting where BDSM activity takes place, as well as the activity itself. The physical place where a BDSM activity takes place is usually called a dungeon, though some prefer less dramatic terms, including "playspace", or "club". A BDSM activity can, but need not, involve sexual activity or sexual roleplay. A characteristic of many BDSM relationships is the power exchange from the bottom to the dominant partner, and bondage features prominently in BDSM scenes and sexual roleplay.
The term is also used in the BDSM community to refer to the BDSM community as a whole.
Types.
A scene can take place in private between two or more people, and can involve a domestic arrangement, such as servitude or a casual or committed lifestyle master/slave relationship. BDSM elements may involve settings of slave training or punishment for breaches of instructions.
A scene can also take place in a club, where the play can be viewed by others. When a scene takes place in a public setting, it may be because the participants enjoy being watched by others, or because of the equipment available, or because having third parties present adds safety for play partners who have only recently met.
BDSM etiquette.
Standard social etiquette rules still apply when at a BDSM event, such as not intimately touching someone you don't know, not touching someone else's belongings (including toys), and abiding by dress codes. Many events open to the public also have rules addressing alcohol consumption, recreational drugs, cell phones, and photography. 
A specific scene takes place within the general conventions and etiquette of BDSM, such as requirements for mutual consent and agreement as to the limits of any BDSM activity. This agreement can be incorporated into a formal contract. In addition, most clubs have additional rules which regulate how onlookers may interact with the actual participants in a scene.
Subculture.
Today the BDSM culture exists in most western countries. This offers BDSM practitioners the opportunity to discuss BDSM relevant topics and problems with like-minded people. This culture is often viewed as a subculture, mainly because BDSM is often still regarded as "unusual" by some of the public. Many people hide their leaning from society since they are afraid of the incomprehension and of social exclusion. It is commonly known in the BDSM culture that there are practitioners living on all continents, but there is no documented evidence for many countries (due to restrictive laws and censorship motivated by politics or religion) except their presence in online BDSM communities and dating sites.
This scene appears particularly on the Internet, in publications, and in meetings such as SM parties, gatherings called munches, and erotic fairs. The annual Folsom Street Fair is the world's largest BDSM event. It has its roots in the gay leather movement. There are also conventions like Living in Leather, TESfest, Shibaricon, Spankfest, and Black Rose. North American cities that have large BDSM communities include New York City, Washington D.C., Baltimore, Atlanta, Seattle, Denver, Los Angeles, Boston, Chicago, Houston, Philadelphia, San Francisco, San Diego, Dallas, Minneapolis, Toronto, Winnipeg, and Vancouver. European cities with large BDSM communities include London, Paris, Berlin, Amsterdam, Munich, Cologne, Hamburg, Moscow and Rome.
Symbols.
One of the most commonly-used symbols of the BDSM community is a derivation of a triskelion shape within a circle. Various forms of triskele (a shape with three-fold rotational symmetry) have had many uses and many meanings in many cultures; its BDSM usage derives from the "Ring of O" in the classic book "Story of O". The BDSM Emblem Project claims copyright over one particular specified form of the triskelion symbol; other variants of the triskelion are free from such copyright claims.
The "Leather Pride flag" is a symbol for the Leather subculture and also widely used within BDSM. In continental Europe the "Ring of O" is widespread among BDSM practitioners.
The BDSM Rights Flag, shown to the left, is intended to represent the belief that people whose sexuality or relationship preferences include Bondage and Discipline, Dominance and Submission, or Sadism and Masochism ("BDSM") deserve the same human rights as everyone else, and should not be discriminated against for pursuing BDSM with consenting adults.
The flag is inspired by the Leather Pride Flag and BDSM Emblem, but is specifically intended to represent the concept of BDSM Rights and to be without the other symbols' restrictions against commercial use. It's designed to be recognisable by people familiar with either the Leather Pride Flag or BDSM Triskelion (or Triskele) as "something to do with BDSM"; and to be distinctive whether reproduced in full colour, or in black and white (or another pair of colours).
BDSM and fetish items and styles have been spread widely in western societies' everyday life by different factors, such as avant-garde fashion, heavy metal, goth subculture, and science fiction TV series, and are often not consciously connected with their BDSM roots by many people. While it was mainly confined to the Punk and BDSM subcultures in the 1990s, it has since spread into wider parts of western societies.
Prejudices.
Most common misconceptions about BDSM form when limited understanding become intertwined with prejudices, clichés and stereotypes. Misunderstandings may arise from general lack of knowledge concerning sexuality and sexual practices as well as misconceptions on how one's personal life and public persona can vary greatly. For example, it is sometimes assumed that a submissive would prefer to experience pain and degradation in their everyday life, or conversely, that they would prefer to have exactly the opposite. There is no clear correlation between the position in everyday life and BDSM preferences. Another misconception is the idea of women generally being the dominant party in BDSM relationships. Quite often the picture of BDSM is reduced to the idea of crude corporal punishment, neglecting the broad spectrum of behaviors within the culture. Along with the whip-swinging dominatrix, the sadomasochist in full leather regalia is another common cliché. While overlaps between different kinds of fetishism can exist, there is no inevitable connection between BDSM and fetishism (for example: latex, PVC or leather). The frequent occurrence of such clothing can be partly explained by its function as a quasi-formalized dress code.
Common misconceptions about BDSM.
It has been claimed that BDSM stems from childhood abuse: there is no evidence for this claim.
Since the term BDSM covers a broad range of human behavior, the arising spectrum of individual interests and personalities is large and extremely diverse. Due to the lack of information in the total population and the reluctance with many to come out about matters of an extremely personal nature leads to situations in which actions and statements of individual BDSM practitioners are accredited to the community at large just as the larger LGBT community has been characterized by drag queens and other minority communities similarly mischaracterized.
At least in the western, industrialized countries and Japan, since the 1980s sadomasochists have begun to form information exchange and support groups to counter discriminatory images. This has happened independently in the United States and in several European countries. With the advent of the web, international cooperation has started to develop—for example "Datenschlag" is a joint effort of sadomasochists in the three major German-speaking countries, and the mailing list "Schlagworte" uses the model of a news agency to connect six countries. Some credit highly publicized events like Operation Spanner and the International leather contests with fostering international cooperation and collaboration.
Coming out.
Some people who feel attracted by the situations usually compiled under the term BDSM reach a point where they decide to come out of the closet, though many sadomasochists keep themselves closeted. Even so, depending upon a survey's participants, about 5 to 25 percent of the US-American population show affinity to the subject. Other than a few artists, practically no celebrities are publicly known as sadomasochists.
Public knowledge of one's BDSM lifestyle can have devastating vocational and social effects (Persona non grata) for sadomasochists. The reason for this is seen by some authors as primarily a lack of public educational advertising, exacerbated by overly lurid and sensationalized media coverage.
Within feminist circles the discussion has been split roughly into two camps: some who see BDSM as an aspect or reflection of oppression (for example, Alice Schwarzer) and, on the other side, pro-BDSM feminists, often grouped under the banner of sex-positive feminism (see Samois); both of them can be traced back to the 1970s.
Some feminists have criticized BDSM for eroticizing power and violence, and for reinforcing misogyny. They argue that women who engage in BDSM are making a choice that is ultimately bad for women. Feminist defenders of BDSM argue that consensual BDSM activities are enjoyed by many women and validate the sexual inclinations of these women. They argue that feminists should not attack other woman's sexual desires as being "anti-feminist", and point out that there is no connection between consensual kinky activities and sex crimes. The main point of feminism is giving an individual woman free choices in her life; that includes her sexual desire. While some radical feminists suggest connections between consensual BDSM scenes and non-consensual rape and sexual assault, sex-positive feminists may tend to find this insulting to women.
It is often mentioned that in BDSM, roles are not fixed to gender, but personal preferences. The dominant partner in a heterosexual relationship may be the woman rather than the man; or BDSM may be part of male/male or female/female sexual relationships. Finally, some people switch, taking either a dominant or submissive role on different occasions. Several studies on the correlation of BDSM pornography and the violence against women recapitulate that there is no correlation. Japan is a useful example: a country which has the lowest rate of sexual crimes of all industrialized nations while being well known for its comprehensive BDSM and bondage pornography (see Pornography in Japan). In 1991 a lateral survey came to the conclusion that between 1964 and 1984, despite the increase in amount and availability of sadomasochistic pornography in the US, Germany, Denmark and Sweden there is no correlation with the national number of rapes to be found.
Operation Spanner in the UK proves that BDSM practitioners still run the risk of being stigmatized as criminals. In 2003, the media coverage of Jack McGeorge showed that simply participating and working in BDSM support groups poses risks to one's job, even in countries where no law restricts it. Here a clear difference can be seen to the situation of homosexuals. The psychological strain appearing in some individual cases is normally neither articulated nor acknowledged in public. Nevertheless it leads to a difficult psychological situation in which the person concerned can be exposed to high levels of emotional stress.
In the stages of "self awareness", he or she realizes their desires related to BDSM scenarios or decides to be open for such. Some authors call this "internal coming-out". Two separate surveys on this topic independently came to the conclusion that 58 percent and 67 percent of the sample respectively, had realized their disposition before their 19th birthday. Other surveys on this topic show comparable results. Independent of age, coming-out can potentially result in a difficult life crisis, sometimes leading to thoughts or acts of suicide. While homosexuals have created support networks in the last decades, sadomasochistic support networks are just starting to develop in most countries. In German speaking countries they are only moderately more developed. The internet is the prime contact point for support groups today, allowing for local and international networking. In the US Kink Aware Professionals (KAP) a privately funded, non-profit service provides the community with referrals to psychotherapeutic, medical, and legal professionals who are knowledgeable about and sensitive to the BDSM, fetish, and leather community. In the US and the UK, the Woodhull Freedom Foundation & Federation, National Coalition for Sexual Freedom (NCSF) and Sexual Freedom Coalition (SFC) have emerged to represent the interests of sadomasochists. The German Bundesvereinigung Sadomasochismus e.V. is committed to the same aim of providing information and driving press relations. In 1996 the website and mailing list Datenschlag went online in German and English providing the largest bibliography, as well as one of the most extensive historical collections of sources related to BDSM.
Parties and clubs.
BDSM parties are events on which BDSM practitioners and other similarly interested people meet in order to communicate, share experiences and knowledge, and to "play" in an erotic atmosphere. The parties show similarities with ones in the dark culture, being based on a more or less strictly enforced dress code; most often clothing made of latex, leather or vinyl/PVC, lycra and so on., emphasizing the body's shape and the primary and secondary sexual characteristic. The requirement for such dress codes differ. While some events have none, others have a policy in order to create a more coherent atmosphere and to prevent voyeurs from taking part.
At these parties, BDSM can be publicly performed on a stage, or more privately in separate "dungeons". A reason for the relatively fast spread of this kind of event is the opportunity to use a wide range of "playing equipment", which in most apartments or houses is unavailable. Slings, St. Andrews crosses (or similar restraining constructs), spanking benches, and punishing supports or cages are often made available. The problem of noise disturbance is also lessened at these events, while in the home setting many BDSM activities can be limited by this factor. In addition, such parties offer both exhibitionists and voyeurs a forum to indulge their inclinations without social opprobrium. Sexual intercourse is taboo within most public BDSM play spaces or not often seen in others, because it is not the emphasis of this kind of play. In order to ensure the maximum safety and comfort for the participants certain standards of behavior have evolved, these include aspects of courtesy, privacy, respect and safewords among others. Today BDSM parties are taking place in most of the larger cities in the western world.
In some cities there are specialized BDSM clubs with a more or less structured program schedule, in which theme parties alternate with topic-free "play evenings", similar to the business concepts of more conventional nightclubs. Social control of these parties or in the clubs is far higher than in a normal discothèque. Consensuality in the public BDSM sessions is strictly monitored and enforced. Apart from commercial events there are also privately organized or only moderately profit-oriented parties, which are organized by BDSM groups and individuals. Minors are not allowed at parties or clubs, even though intercourse and drinking are usually not found in these parties.
Psychology.
Prevalence.
BDSM is practiced in all social strata and is common in both heterosexual and homosexual men and women in varied occurrences and intensities. The spectrum ranges from couples with no connections to the subculture outside of their bedrooms or homes, without any awareness of the concept of BDSM, playing "tie-me-up-games", to public scenes on St. Andrew's crosses at large events such as the Folsom Street Fair in San Francisco. Estimation on the overall percentage of BDSM related sexual behaviour vary but it is no longer assumed to be uncommon.
A non-representative survey on the sexual behaviour of American students published in 1997 and based on questionnaires had a response rate of about 8–9%. Its results showed 15% of openly homosexual males, 21% of openly lesbian and female bisexual students, 11% of heterosexual males and 9% of female heterosexual students committed to BDSM related fantasies. In all groups the level of practical BDSM experiences were around 6%. Within the group of openly lesbian and bisexual females the quote was significantly higher, at 21%. Independent of their sexual orientation, about 12% of all questioned students, 16% of lesbians and female bisexuals and 8% of heterosexual males articulated an interest in spanking. Experience with this sexual behaviour was indicated by 30% of male heterosexuals, 33% of female bisexuals and lesbians, and 24% of the male gay and bisexual men and female heterosexual women. Even though this study was not considered representative, other surveys indicate similar dimensions in a differing target groups.
A representative study done from 2001 to 2002 in Australia found that 1.8% of sexually active people (2.2% men, 1.3% women but no significant sex difference) had engaged in BDSM activity in the previous year. Of the entire sample, 1.8% men and 1.3% women had been involved in BDSM. BDSM activity was significantly more likely in bisexual and gay men. But among men in general, there was no relationship effect of age, education, language spoken at home, or relationship status. Among women, in this study, activity was most common for those between 16 and 19 years of age and least likely for females over 50 years. Activity was also significantly more likely for bisexual women, lesbians, and women who had a regular partner they did not live with, but was not significantly related with speaking a language other than English or education.
Richters et al. (2008) study also found that people who engaged in BDSM were more likely to have experienced a wider range of different sexual practices (e.g. oral or anal sex, more than one partner, group sex, phone sex, viewed pornography, used a sex toy, fisting, rimming, etc.). They were, however, not any more likely to have been coerced, unhappy, anxious, or experiencing sexual difficulties. On the contrary, men who had engaged in BDSM scored lower on a psychological distress scale than men who did not.
Another representative study, published in 1999 by the German Institut für rationale Psychologie, found that about 2/3 of the interviewed women stated a desire to be at the mercy of their sexual partners from time to time. 69% admitted to fantasies dealing with sexual submissiveness, 42% stated interest in explicit BDSM techniques, 25% in bondage. A 1976 study in the general US population suggests three percent have had positive experiences with Bondage or master-slave role playing. Overall 12% of the interviewed females and 18% of the males were willing to try it. A 1990 Kinsey Institute report stated that 5% to 10% of Americans occasionally engage in sexual activities related to BDSM. 11% of men and 17% of women reported trying bondage. Some elements of BDSM have been popularized through increased media coverage since the middle 1990s. Thus both black leather clothing, sexual jewellery such as chains and dominance role play appear increasingly outside of BDSM contexts.
According to yet another survey of 317,000 people in 41 countries, about 20% of the surveyed have at least used masks, blindfolds or other bondage utilities once, and 5% explicitly connected themselves with BDSM. In 2004, 19% mentioned spanking as one of their practices and 22% confirmed the use of blindfolds or handcuffs.
A 1985 study found 52 out of 182 female respondents (28%) were involved in sadomasochistic activities.
Psychological categorization.
In the past, many activities and fantasies related to BDSM were generally attributed to sadism or masochism and were regarded by psychiatrists as an illness. For example, the International Classification of Diseases (ICD-10) categorized 'sadomasochism' as a "Disorder of sexual preference" () and described it as follows: "A preference for sexual activity which involves the infliction of pain or humiliation, or bondage. If the subject prefers to be the recipient of such stimulation this is called masochism; if the provider, sadism. Often an individual obtains sexual excitement from both sadistic and masochistic activities."
With the 1994 publication of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV) new criteria of diagnosis were added. The DSM-IV asserts that "The fantasies, sexual urges, or behaviors" must "cause clinically significant distress or impairment in social, occupational, or other important areas of functioning" in order for sexual sadism or masochism to be considered a disorder. In an AASECT article providing guidelines for therapists working with BDSM clients, sexologists Charles Allen Moser and Peggy Kleinplatz highlight that distress can occur in BDSM patients due to stigma and discrimination surrounding BDSM, and that in these circumstances the role of the therapist is to ""validate the distress rather than to 'cure' the BDSM desires"." The DSM-IVs' latest edition (DSM-IV-TR) further requires that the activity must be the sole means of sexual gratification for a period of six (6) months, and either cause "clinically significant distress or impairment in social, occupational, or other important areas of functioning" or involve a violation of "consent" to be diagnosed as a paraphilia.
That said, overlap of sexual preference disorders and the practice of BDSM practices can occur.
In Europe, an organization called ReviseF65 has worked towards this purpose in the International Classification of Diseases (ICD-10). In 1995, Denmark became the first European Union country to have completely removed sadomasochism from its national classification of diseases. This was followed by Sweden in 2009, Norway in 2010 and Finland 2011. Recent surveys on the spread of BDSM fantasies and practices show strong variations in the range of their results. Nevertheless it can be stated that the vast majority of the researchers assume 5 to 25 percent of the population showing sexual behavior related to joyfully experienced pain or dominance and submission. The population with related fantasies is considered even higher.
There have been few studies on the psychological aspects of BDSM using modern scientific standards. One pivotal survey on the subject was published by US-American psychotherapist Charles Moser in 1988 in the "Journal of Social Work and Human Sexuality". His conclusion was that while there is a general lack of data on the psychological problems of BDSM practitioners, some fundamental results are obvious. He emphasizes that there is no evidence for the theory that BDSM has common symptoms or "any" common psychopathology; Clinical literature, though does not give a consistent picture of BDSM practitioners. Moser emphasizes that there is no evidence at all supporting the theory of BDSM practitioners having any special psychiatric problems or even problems based solely on their preferences.
Problems do sometimes occur in the area of self classification by the person concerned. During the phase of the "coming-out", self questioning related to one's own "normality" is quite common. According to Moser, the discovery of BDSM preferences "can" result in fear of the current non-BDSM relationship's destruction. This, combined with the fear of discrimination in everyday life, leads in some cases to a double life which can be highly burdensome. At the same time, the denial of BDSM preferences can induce stress and dissatisfaction with one's own "vanilla"-lifestyle, feeding the apprehension of finding no partner. Moser states that BDSM practitioners having problems finding BDSM partners would probably have problems in finding a non-BDSM partner as well. The wish to remove BDSM preferences is another possible reason for psychological problems since it is not possible in most cases. Finally, the scientist states that BDSM practitioners seldom commit violent crimes. From his point of view, crimes of BDSM practitioners usually have no connection with the BDSM components existing in their life. Moser's study comes to the conclusion that there is no scientific evidence, which could give reason to refuse members of this group work- or safety certificates, adoption possibilities, custody or other social rights or privileges. The Swiss psychoanalyst Fritz Morgenthaler shares a similar perspective in his book, "Homosexuality, Heterosexuality, Perversion" (1988). He states that possible problems result not necessarily from the non-normative behavior, but in most cases primarily from the real or feared reactions of the social environment towards the own preferences. In 1940 psychoanalyst Theodor Reik reached implicitly the same conclusion in his standard work "Aus Leiden Freuden. Masochismus und Gesellschaft".
Moser's results are further supported by Richters et al.'s (2008) study on the demographic and psychosocial features of participants in BDSM done in Australia. Richters et al. (2008) found that BDSM practitioners were no more likely to have experienced sexual assault than the control group, and were not more likely to feel unhappy or anxious. The BDSM males reported higher levels of psychological well-being than the controls. It was concluded that "BDSM is simply a sexual interest or subculture attractive to a minority, not a pathological symptom of past abuse or difficulty with 'normal' sex."
In contrast to frameworks seeking to explain sadomasochism through psychological, psychoanalytic, medical or forensic approaches, which seek to categorize behaviour and desires and find a root "cause," Romana Byrne suggests that such practices can be seen as examples of "aesthetic sexuality," in which a founding physiological or psychological impulse is irrelevant. Rather, sadism and masochism may be practiced through choice and deliberation, driven by certain aesthetic goals tied to style, pleasure, and identity. These practices, in certain circumstances and contexts, can be compared with the creation of art.
Gender observances in research.
Not much empirical research has been done on gender differences or prevalence rates of categorized roles within BDSM. Though, one Australian demographic study found higher rates of female participants than males, it can not be generalized.
Gender differences in masochistic scripts.
One common misconception of BDSM and kink is that women are more likely to take on masochistic roles than men. Roy Baumeister (2010) actually had more male masochists in his study than female, and fewer male dominants than female. The fact that neither of these gender differences were significant, suggests no assumptions should be made regarding gender and masochistic roles in BDSM. One explanation why we might think otherwise lies in our social and cultural ideals about femininity; masochism may emphasize certain stereotypically feminine elements through activities like feminization of men and ultra-feminine clothing for women. But such tendencies of the submissive masochistic role should not be interpreted as a connection between it and the stereotypical female role—many masochistic scripts do not include any of these tendencies.
Baumeister (2010) found that masochistic males experienced greater: severity of pain, frequency of humiliation (status-loss, degrading, oral), partner infidelity, active participation by other persons, and cross dressing. Trends also suggested that male masochism included more bondage and oral sex than female (though the data was not significant). Female masochists, on the other hand, experienced greater: frequency in pain, pain as punishment for 'misdeeds' in the relationship context, display humiliation, genital intercourse, and presence of non-participating audiences. The exclusiveness of dominant males in a heterosexual relationship should be noted because, historically, men in power preferred multiple partners. Finally, Baumeister (2010) observes a contrast between the 'intense sensation' focus of male masochism to a more 'meaning and emotion' centred female masochistic script.
Women in S/M culture.
Levitt, Moser, & Jamison's 1994 study provides a general, if outdated, description of characteristics of women in the sadomasochistic (S/M) subculture. They state that women in S/M tend to have higher education, become more aware of their desires as a young adult, are less likely to be married than the general population. The researchers found the majority of females identified as heterosexual and submissive, a substantial minority were versatile—able to switch between dominant and submissive roles—and a smaller minority identified with the dominant role exclusively. Oral sex, bondage and master-slave script were among the most popular activities, while feces/watersports were the least popular.
Orientation observances in research.
Though BDSM in itself can be considered a sexual orientation or identity, and is considered one by some of its practitioners, the BDSM and kink scene is more often seen as a diverse pansexual community. Ideally, this is a non-judgmental community where gender, sexuality, orientation, preferences are accepted as is or worked at to become something a person can be happy with. In research, studies have focused on bisexuality and its parallels with BDSM, as well as gay-straight differences between practitioners.
Differences and similarities between gay and straight men in S/M.
Demographically, Nordling et al.'s (2006) study found no differences in age, but 43% gay male respondents compared to 29% straight males had university level education. The gay men also had higher incomes than the general population, and tended to work in white collar jobs while straight men tended toward blue collar ones. Because there were not enough female respondents (22), no conclusions could be drawn from them.
Sexually speaking, the same 2006 study by Nordling et al. found that gay males were aware of their S/M preferences and took part in them at an earlier age, preferring leather, anal sex, rimming, dildos and special equipment or uniform scenes. In contrast, straight men preferred verbal humiliation, mask and blindfolds, gags, rubber/latex outfits, caning, vaginal sex, and cross-dressing among other activities. From the questionnaire, researchers were able to identify 4 separate sexual themes: hyper-masculinity, giving and receiving pain, physical restriction (i.e. bondage), and psychological humiliation. Gay men preferred activities that tended towards hyper-masculinity while straight men showed greater preference for humiliation. Though there were not enough female respondents to draw a similar conclusion with, the fact that there is a difference in gay and straight men suggests strongly that S/M (and BDSM in general) can not be considered a homogenous phenomenon. As Nordling et al. (2006) puts it, “People who identify as sadomasochists mean different things by these identifications.” (54)
Bisexuality and BDSM.
In Steve Lenius' original 2001 paper he explored the acceptance of bisexuality in a supposedly pansexual BDSM community. The reasoning behind this is that 'coming-out' had become primarily the territory of the gay and lesbian, with bisexuals feeling the push to be one or the other (and being right only half the time either way). What he found in 2001, was that people in BDSM were open to discussion about the topic of bisexuality and pansexuality and all controversies they bring to the table, but personal biases and issues stood in the way of actively using such labels. A decade later, Lenius (2011) looks back on his study and considers if anything has changed. He concluded that the standing of bisexuals in the BDSM and kink community was unchanged, and believed that positive shifts in attitude were moderated by society's changing views towards different sexualities and orientations. But Lenius (2011) does emphasize that the pansexual promoting BDSM community helped advance greater acceptance of alternative sexualities.
Brandy Lin Simula (2012), on the other hand, argues that BDSM actively resists gender conforming and identified three different types of BDSM bisexuality: gender-switching, gender-based styles (taking on a different gendered style depending on gender of partner when playing), and rejection of gender (resisting the idea that gender matters in their play partners). Simula (2012) explains that practitioners of BDSM routinely challenge our concepts of sexuality by pushing the limits on pre-existing ideas of sexual orientation and gender norms. For some, BDSM and kink provides a platform in creating identities that are fluid, ever-changing.
History of psychotherapy and current recommendations.
Psychiatry has an insensitive history in the area of BDSM. There have been many involvements by institutions of political power to marginalize subgroups and sexual minorities. Mental health professionals have a long history of holding negative assumptions and stereotypes about the BDSM community. Beginning with the DSM-II, Sexual Sadism and Sexual Masochism have been listed as sexually deviant behaviours. Sadism and masochism were also found in the personality disorder section This negative assumption has not changed significantly evident in the continued inclusion of Sexual Sadism and Sexual Masochism as paraphilias in the DSM-IV-TR. These said biases and misinformation can result in pathologizing and unintentional harm to clients who identifies as sadists or masochists.
According to Kolmes et al. (2006), major themes of biased and inadequate care to BDSM clients are:
These same researchers suggested that therapists should be open to learning more about BDSM, to show comfort in talking about BDSM issues, and to understand and promote "safe, sane, consensual" BDSM.
Clinical issues.
Nichols (2006) compiled some common clinical issues: countertransference, non-disclosure, coming-out, partner/families, and bleed-through.
Countertransference is a common problem in clinical settings. Despite having no evidence, therapists may find themselves believing that their client’s pathology is "self-evident". Therapists may feel intense disgust and aversive reactions. Feelings of countertransference can interfere with therapy. Another common problem is when clients conceal their sexual preferences from their therapists. This can compromise any therapy. To avoid non-disclosure, therapists are encouraged to communicate their openness in indirect ways with literatures and artworks in the waiting room. Therapists can also deliberately bring up BDSM topics during the course of therapy. With less informed therapists, sometimes they over-focus on clients’ sexuality which detracts from original issues such as family relationships, depression, etc. A special subgroup that needs counselling is the "newbie". Individuals just coming out might have internalized shame, fear, and self-hatred about their sexual preferences. Therapists need to provide acceptance, care, and model positive attitude; providing reassurance, Psychoeducation, and bibliotherapy for these clients is crucial. The average age when BDSM individuals realize their sexual preference is around 26 years. Many people hide their sexuality until they can no longer contain their desires. However, they may have married or had children by this point. Therefore, therapists need to facilitate couples counselling and disclosure. It is important for therapists to consider fairness to partner and family of clients. In situations when boundaries between roles in the bedroom and roles in the rest of the relationship blurs, a "bleed-through" problem has occurred. Therapists need to help clients resolve distress and deal with any underlying problems that led to the initial bleed-through.
Spirituality.
For many people, BDSM practices can lead to profound transcendental experiences, reminiscent of shamanic ordeals. Additionally, many have turned to various spiritual traditions to guide them ethically, or have sought to reconcile their BDSM desires with their chosen religious or spiritual tradition. There are BDSM support groups for members of several religions.
History.
Origins.
Practices of BDSM survive from some of the oldest textual records in the world, associated with rituals to the Goddess Inanna (Ishtar in Akkadian). Academic historian and archaeologist Anne O Nomis has undertaken research revealing cuneiform texts dedicated to Inanna which incorporate domination rituals. In particular she points to ancient writings such as Inanna and Ebih (in which the Goddess dominates Ebih), and Hymn to Inanna describing cross-dressing transformations and rituals "imbued with pain and ecstasy, bringing about initation and journeys of altered states of consciousness; punishment, moaning, ecstasy, lament and song, participants exhausting themselves in weeping and grief."
During the ninth century BC, ritual flagellations were performed in Artemis Orthia, one of the most important religious areas of ancient Sparta, where the Cult of Orthia, a preolympic religion, was practiced. Here ritual flagellation called "diamastigosis" took place,in which young adolescent men were whipped in a ceremony overseen by the priestess. These are referred to by a number of ancient authors, including Pausanius (III, 16: 10-11).
One of the oldest graphical proofs of sadomasochistic activities is found in the Etruscan Tomb of the Whipping near Tarquinia, which dates to the fifth century BC. Inside the tomb there is fresco which portrays two men who flagellate a woman with a cane and a hand during an erotic situation. Another reference related to flagellation is to be found in the sixth book of the "Satires" of the ancient Roman Poet Juvenal (1st–2nd century A.D.), further reference can be found in Petronius's "Satyricon" where a delinquent is whipped for sexual arousal. Anecdotal narratives related to humans who have had themselves voluntary bound, flagellated or whipped as a substitute for sex or as part of foreplay reach back to the third and fourth century.
In Pompeii, a Whipstress figure with wings is depicted on the wall of the Villa of Mysteries, as part of an initiation of a young woman into the Mysteries. Nomis notes that the Whipstress role drove the sacred initiation of ceremonial death and rebirth. The archaic Greek Aphrodite may too once have been armed with an implement, with archaeological evidence of armed Aphrodites known as hoplismene known from a number of locations in Cythera, Acrocorinth and Sparta, and which based on Nomis's study on Eastern Goddesses she notes may well have been a whip.
The "Kama Sutra" of India describes four different kinds of hitting during lovemaking, the allowed regions of the human body to target and different kinds of joyful "cries of pain" practiced by bottoms. The collection of historic texts related to sensuous experiences explicitly emphasizes that impact play, biting and pinching during sexual activities should only be performed consensually since only some women consider such behavior to be joyful. From this perspective the Kama Sutra can be considered as one of the first written resources dealing with sadomasochistic activities and safety rules. Further texts with sadomasochistic connotation appear worldwide during the following centuries on a regular basis.
There are anecdotal reports of people willingly being bound or whipped, as a prelude to or substitute for sex, during the 14th century. The medieval phenomenon of courtly love in all of its slavish devotion and ambivalence has been suggested by some writers to be a precursor of BDSM. Some sources claim that BDSM as a distinct form of sexual behavior originated at the beginning of the 18th century when Western civilization began medically and legally categorizing sexual behavior (see Etymology).
The professional occupation of the Dominatrix has been traced by academic historian Anne O. Nomis from British Library Rare Book sources, which reveals flagellation practiced within an erotic setting recorded from at least the 1590s evidenced by a John Davies epigram, and other references to flogging schools in Shadwell's 'Virtuoso' (1676) and Troth's 'Knavery of Astrology' (1680). Visual evidence such as mezzotints and print media is also identified revealing scenes of flagellation, such as "The Cully Flaug'd" from the British Museum collection. 
John Cleland's novel "Fanny Hill", published in 1749, incorporates a flagellation scene between the character's protagonist Fanny Hill and Mr Barville. A large number of flagellation publications followed, including "Fashionable Lectures Delivered With Birch Discipline" (c1761), promoting the names of ladies offering the service in a lecture room with rods and cat o' nine tails.
Other sources give a broader definition, citing BDSM-like behavior in earlier times and other cultures, such as the medieval flagellates and the physical ordeal rituals of some Native American societies.
BDSM ideas and imagery have existed on the fringes of Western culture throughout the twentieth century. Robert Bienvenu attributes the origins of modern BDSM to three sources, which he names as "European Fetish" (from 1928), "American Fetish" (from 1934), and "Gay Leather" (from 1950). Another source are the sexual games played in brothels, which go back into the 19th century if not earlier. Irving Klaw, during the 1950s and 1960s, produced some of the first commercial film and photography with a BDSM theme (most notably with Bettie Page) and published comics by the now-iconic bondage artists John Willie and Eric Stanton.
Stanton's model Bettie Page became at the same time one of the first successful models in the area of fetish photography and one of the most famous pin-up girls of American mainstream culture. Italian author and designer Guido Crepax was deeply influenced by him, coining the style and development of European adult comics in the second half of the twentieth century. The artists Helmut Newton and Robert Mapplethorpe are the most prominent examples of the increasing use of BDSM-related motives in modern photography and the public discussions still resulting from this.
Alfred Binet first coined the term erotic fetishism in his 1887 book, "Du fétichisme dans l’amour"
Richard von Krafft-Ebing saw BDSM interests as the end of a continuum.
Leather movement.
Leather has been a predominantly gay male term to refer to one fetish, but it can stand for many more. Members of the gay male leather community may wear leathers such as Motorcycle leathers, or may be attracted to men wearing leather. Leather and BDSM are seen as two parts of one whole. Much of the BDSM culture can be traced back to the gay male leather culture, which formalized itself out of the group of men who were soldiers returning home after World War II (1939–1945). WWII was the setting where countless homosexual men and women tasted the life among homosexual peers. Post-war, homosexual individuals congregated in larger cities such as New York, Chicago, San Francisco, and Los Angeles. They formed leather clubs and bike clubs, some were fraternal services. The establishment of Mr. Leather Contest and Mr. Drummer Contest were made around this time. This was the genesis of the gay male leather community. Many of the members were attracted to extreme forms of sexuality, for which peak expression was in the pre-AIDS 1970s. This subculture is epitomized by the "Leatherman's Handbook" by Larry Townsend, published in 1972, which describes in detail the practices and culture of gay male sadomasochists in the late 1960s and early 1970s. 
In the early 1980s, lesbians also joined the leathermen as a recognizable element of the gay leather community. They also formed leather clubs, but there were some gender differences such as the absence of leatherwomen’s bar. In 1981, the publication of "Coming to Power" by lesbian-feminist group Samois led to a greater knowledge and acceptance of BDSM in the lesbian community. By the 1990s, the gay men’s and women’s leather communities were no longer underground and played an important role in the kink community.
Today the Leather Movement is generally seen as a part of the BDSM-culture instead of as a development deriving from gay subculture, even if a huge part of the BDSM-subculture was gay in the past. In the 1990s the so-called New Guard leather subculture evolved. This new orientation started to integrate psychological aspects into their play.
Internet.
In the late-eighties, the Internet provided a way of finding people with specialized interests around the world as well as on a local level, and communicating with them anonymously. This brought about an explosion of interest and knowledge of BDSM, particularly on the usenet group alt.sex.bondage. When that group became too cluttered with spam, the focus moved to .
In addition to traditional sex shops, which sell sex paraphernalia, there has also been an explosive growth of online adult toy companies that specialize in leather/latex gear and BDSM toys. Once a very niche market, there are now very few sex toy companies that do not offer some sort of BDSM or fetish gear in their catalog. Kinky elements seem to have worked their way into "vanilla" markets. The former niche expanded to an important pillar of the business with adult accessories. Today practically all suppliers of sex toys do offer items which originally found usage in the BDSM subculture. Padded handcuffs, latex and leather garments, as well as more exotic items like soft whips for fondling and TENS for erotic electro stimulation can be found in catalogs aiming on classical vanilla target groups, indicating that former boundaries increasingly seem to shift.
During the last years the Internet also provides a central platform for networking among individuals who are interested in the subject. Besides countless private and commercial choices there is an increasing number of local networks and support groups emerging. These groups often offer comprehensive background and health related information for people who have been unwillingly outed as well as contact lists with information on psychologists, physicians and lawyers who are familiar with BDSM related topics.
Etymology.
The terms "sadism" and "masochism" are derived from the names of the Marquis de Sade and Leopold von Sacher-Masoch, based on the content of the authors' works. Although the names of the Sade and Sacher-Masoch are attached to the terms sadism and masochism respectively, the scenes described in Sade's works do not meet modern BDSM standards of informed consent. BDSM is solely based on consensual activities, and based on its system and laws, the concepts presented by Sade are not agreed upon the BDSM culture, even though they are sadistic in nature. In 1843 the Hungarian physician Heinrich Kaan published "Psychopathia sexualis" ("Psychopathy of Sex"), a writing in which he converts the sin conceptions of Christianity into medical diagnoses. With his work the originally theological terms "perversion", "aberration" and "deviation" became part of the scientific terminology for the first time. The German psychiatrist Richard von Krafft Ebing introduced the terms "Sadism" and "Masochism" to the medical community in his work "Neue Forschungen auf dem Gebiet der Psychopathia sexualis" ("New research in the area of Psychopathy of Sex") in 1890.
In 1905, Sigmund Freud described "Sadism" and "Masochism" in his "Drei Abhandlungen zur Sexualtheorie" ("Three papers on Sexual theory") as diseases developing from an incorrect development of the child psyche and laid the groundwork for the scientific perspective on the subject in the following decades. This led to the first time use of the compound term "Sado-Masochism" (German "Sado-Masochismus") by the Viennese Psychoanalytic Isidor Isaak Sadger in its work "Über den sado-masochistischen Komplex" ("Regarding the sadomasochistic complex") in 1913.
In the later 20th century, BDSM activists have protested against these conceptual models. Not only were these models were derived from the philosophies of two singular historical figures. Both Freud and Krafft-Ebing were psychiatrists. Their observations on Sadism and Masochism were dependent on psychiatric patients, and their models were built on the assumption of Psychopathology. BDSM activists argues that it is illogical to attribute human behavioural phenomena as complex as sadism and masochism to the 'inventions' of two historic individuals. Advocates of BDSM have sought to distinguish themselves from widely held notions of antiquated psychiatric theory by the adoption of the initialized term, "BDSM" as a distinction from the now common usage of those psychological terms, abbreviated as "S&M".
Legal status.
The legal status of BDSM is entirely dependent on the legal structure of individual countries. Each individual country makes its own determination to whether the practice of BDSM has any criminal relevance or legal consequences. Criminalization of consensually implemented BDSM practices is usually not with explicit reference to BDSM itself (i.e., spanking an adult who specifically requests another to do such action to the requesting person), but results from the fact that such behavior could be considered a breach of personal rights, which in principle constitutes a criminal offense though some jurisdictions especially in the United States do specifically list such acts as flagellation (spanking) between consenting adults as "assault" regardless of consent between such consenting parties.
In Germany, Netherlands, Japan and Scandinavia such behavior is legal in principle. In Austria the legal status is unclear, while in Switzerland certain BDSM practices can be considered criminal. Spectacular incidents like the US-American scandal of "People v. Jovanovic" and the British Operation Spanner demonstrate the degree to which difficult grey areas can pose a problem for the individuals and authorities involved. For these reasons it is important for practitioners of BDSM to learn the legal status concerning BDSM activities in the country they reside in.
Germany.
The practice of BDSM is not generally penalized in Germany if it is conducted with the mutual consent of the partners involved. To fulfill the charge of coercion the use of violence, or the threat of a "severe mistreatment" must involve an endangerment to life and limb. In cases where the continued application of the treatment could be ended through the use of a safeword, neither coercion nor sexual coercion may be charged. Similar principles apply for charges of sexual abuse of people incapable of resistance. In such cases taking advantage of a person's inability to resist in order to perform sexual acts on that person would be clearly punishable. The potential use of the safeword is considered to be sufficient possibility for resistance since this would lead to the cessation of the act, so a true inability to resist is not considered to be in effect.
According to §194 the charge of insult (slander) can only be prosecuted if the defamed person chooses to press charges. False imprisonment can be charged if the victim—when applying an objective view—can be considered to be impaired in his or her rights of free movement. According to §228 of the German criminal code a person inflicting a bodily injury on another person with that person's permission violates the law only in cases where the act can be considered to have violated good morals in spite of permission having been given. On 26 May 2004 the Criminal Panel No. 2 of the Bundesgerichtshof (German Federal Court) ruled that sado-masochistically motivated physical injuries are not per se indecent and thus subject to §228.
Still, this ruling makes the question of indecency dependent on the degree to which the bodily injury might be likely to impair the health of the receiving party. According to the BGH, the line of indecency is definitively crossed when "under an objectively prescient consideration of all relevant circumstances the party granting consent could be brought into concrete danger of death by the act of bodily injury". In its ruling, the court overturned a verdict by the Provincial Court of Kassel, according to which a man who had accidentally strangled his partner to death had been sentenced to probation for negligent manslaughter. The court had rejected a conviction on charges of bodily injury leading to death on the grounds that the victim had, in its opinion, consented to the act.
Following cases in which sado-masochistic practices had been repeatedly used as pressure tactics against former partners in custody cases, the Appeals Court of Hamm ruled in February 2006 that sexual inclinations toward sado-masochism are no indication of a lack of capabilities for successful child-raising.
United Kingdom.
In British law, consent is an absolute defence to common assault, but not necessarily to actual bodily harm, where courts may decide that consent is not valid, as occurred in the case of R v Brown. Accordingly consensual activities in the UK may not constitute "assault occasioning actual or grievous bodily harm" in law. The Spanner Trust states that this is defined as activities which have caused injury "of a lasting nature" but that only a slight duration or injury might be considered "lasting" in law. The decision contrasts with the later case of R v Wilson in which conviction for non-sexual consensual branding within a marriage was overturned, the appeal court ruling that R v Brown was not an authority in all cases of consensual injury and criticizing the decision to prosecute.
Following Operation Spanner the European Court of Human Rights ruled in January 1999 in Laskey, Jaggard and Brown v. United Kingdom that no violation of Article 8 occurred because the amount of physical or psychological harm that the law allows between any two people, even consenting adults, is to be determined by the jurisdiction the individuals live in, as it is the State's responsibility to balance the concerns of public health and well-being with the amount of control a State should be allowed to exercise over its citizens. In the Criminal Justice and Immigration Bill 2007, the British Government cited the Spanner case as justification for criminalizing images of consensual acts, as part of its proposed criminalization of possession of "extreme pornography". Another contrasting case was that of Stephen Lock in 2013, who was cleared of actual bodily harm on the grounds that the woman consented. In this case, the act was deemed to be sexual.
United States.
The United States Federal law does not list a specific criminal determination for consensual BDSM acts. Many BDSM practitioners cite the legal decision of People v. Jovanovic, 95 N.Y.2d 846 (2000), or the "Cybersex Torture Case" which was the first U.S. appellate decision to hold (in effect) one does not commit assault if victim consents. However, many individual states do criminalize specific BDSM actions within their state borders. Some states specifically address the idea of "consent to BDSM acts" within their assault laws such as the state of New Jersey which states "simple assault" to be defined as "a disorderly persons offense unless committed in a fight or scuffle "entered into by mutual consent", in which case it is a petty disorderly persons offense". BDSM activities including flagellation (spanking) specifically "without paying for such action" (i.e., prostitution) between consenting adults are specifically addressed as illegal in certain individual states.
Canada.
In 2004 a judge in Canada ruled that videos seized by the police featuring BDSM activities were not obscene, and did not constitute violence, but a "normal and acceptable" sexual activity between two consenting adults.
In 2011, the Supreme Court of Canada ruled in "R. v. J.A." that a person must have an active mind during the specific sexual activity in order to legally consent. The Court ruled that it is a criminal offence to perform a sexual act on an unconscious person—whether or not that person consented in advance.
Italy.
In Italian law BDSM is right on the border between crime and legality, and everything lies in the interpretation of the legal code by the judge. This concept is that anyone willingly causing "injury" to another person is to be punished. In this context though "injury" is legally defined as "anything causing a condition of illness", and "illness" is ill-defined itself in two different legal ways. The first is "any anatomical or functional alteration of the organism" (thus technically including little scratches and bruises too); The second is "a significant worsening of a previous condition relevant to organic and relational processes, requiring any kind of therapy". This could make it somewhat risky to play with someone as later the "victim" may call foul play citing even an insignificant mark as evidence against the partner. Also any injury requiring over 20 days of medical care must be denounced by the professional medic who discovers it, leading to automatic indictment of the person who caused it.
Austria.
§90 of the criminal code declares bodily injury (§§ 83, 84) or the endangerment of physical security (§89) to not be subject to penalty in cases in which the "victim" has consented and the injury or endangerment does not offend moral sensibilities. Case law from the Austrian Supreme Court has consistently shown that bodily injury is only offensive to moral sensibilities, thus it is only punishable when a "serious injury" (a damage to health or an employment disability lasting more than 24 days) or the death of the "victim" results. A "light injury" is generally considered "permissible" when the "victim" has consented to it. In cases of threats to bodily well-being the standard depends on the probability that an injury will actually occur. If serious injury or even death would be a likely result of a threat being carried out, then even the threat itself is considered punishable.
Switzerland.
The age of consent in Switzerland is 16 years which also applies for BDSM play. Minors (i.e. those under 16) are not subject to punishment for BDSM play as long as the age difference between them is less than three years. Certain practices however require granting consent for light injuries with only those over 18 permitted to give consent. On 1 April 2002 Articles 135 and 197 of the Swiss Criminal Code were tightened to make ownership of "objects or demonstrations [...] which depict sexual acts with violent content" a punishable offense. This law amounts to a general criminalization of sado-masochism since nearly every sado-masochist will have some kind of media which fulfills this criterion. Critics also object to the wording of the law which puts sado-masochists in the same category as pedophiles and pederasts.
Nordic countries.
In September 2010 a Swedish court acquitted a 32 year old man of assault for engaging in consensual BDSM play with a 16 year old woman (the age of consent in Sweden is 15). Norway's legal system has likewise taken a similar position, that safe and consensual BDSM play should not be subject to criminal prosecution. This parallels the stance of the mental health professions in the Nordic countries which have removed sadomasochism from their respective lists of psychiatric illnesses.
BDSM and culture.
Theatre.
Although it would be possible to establish certain elements related to BDSM in classical theater, not until the emergence of contemporary theatre could you see such topics as the main theme in the performing arts. Exemplifying this are two works: one Austrian, one German, in which BDSM is not only incorporated, but integral to the storyline of the play.
Film.
Apart from films that are part of the commercial pornography circuit, cinema has been treated since its inception with in depth BDSM relationships, from 1909 to the present decade. The current decade in particular has an abundance of examples: "Sade", "Quills", "The Piano Teacher", "Beyond Vanilla", "Secretary", "Wir leben", "Bitter Moon", and the forthcoming "Fifty Shades of Grey" among others. 
The first feature film shown around the world at film festivals and in cinemas that was principally set in the BDSM and fetish lifestyle was the 1997 UK production "Preaching to the Perverted". Filmed in a cartoon, high camp style, the film was also a political satire inspired by actual prosecutions of BDSM lifestyle practitioners such as Operation Spanner. It encountered censorship problems in the UK and USA and was the last film banned in Ireland in the twentieth century.
Naturally, there are thousands of movies that touch on or contain accents of BDSM or generic sadomasocism, or contain scenes of BDSM scenarios: "Last Tango in Paris", "Emmanuelle", "Personal Services", "Basic Instinct", "Eyes Wide Shut", not to mention many others, but these are not seen as part of the filmography of BDSM.
Literature.
Although examples of literature catering to BDSM and fetishistic taste was created in earlier periods, BDSM literature as it exists today cannot be found much earlier than World War II. However, such work as "The Trial of Gilles de Rais" that dates to 1440 with expressions that coincide with "S&M" refers as easily to "sadist and masochist" as to "slave and master"... a revealing and leveling equation: "sadist-slave," "masochist-master," both alternating as victims.
A central work in BDSM literature is undoubtedly the "Story of O" (1954) by Anne Desclos under the pseudonym Pauline Réage.
Other notable works include "9 ½ weeks" (1978) by Elizabeth McNeill, some works of the writer Anne Rice ("Exit to Eden", and her "Claiming of Sleeping Beauty" series of books), Jeanne de Berg ("L'Image" (1956) dedicated to Pauline Réage). More recent works include E L James' "Fifty Shades of Grey" series. Works from the Gor series by John Norman, and naturally all the works of Patrick Califia, Gloria Brame, the group Samois and many of the writer Georges Bataille (Histoire de l'oeil-Story of the Eye, Madame Edwarda, 1937), as well as Bob Flanagan: "Slave Sonnets" (1986), "Fuck Journal" (1987), "A Taste of Honey" (1990). A common part of many of the poems of Pablo Neruda is a reflection on feelings and sensations arising from the relations of EPE or erotic exchange of power.
Art.
As in the case of literature a list of BDSM art should not include works of the early history depicting sadomasochism, flogging, fetishism, and so on., but should start when the integrative aspects of BDSM become visible. In this sense figures such as those from Venus de Kostienki, Russia (3000 BC) or engraved on the tomb and the sarcophagus of the Egyptian aristocrat Bastret (1376 BC) are not considered BDSM art (as depicting BDSM was not their primary purpose).
For art which is undeniably BDSM art, it follows that the artist has spent most or part of their work dealing with BDSM, so an image of flogging by an artist who does many BDSM works is probably depicting BDSM, while the same image done by a historic maritime artist depicting life aboard a British vessel would probably not be considered BDSM art (although context, and the work itself would usually ultimately determine what the subject matter is discussing). Examples include:

</doc>
<doc id="4547" url="http://en.wikipedia.org/wiki?curid=4547" title="Bash (Unix shell)">
Bash (Unix shell)

Bash is a Unix shell written by Brian Fox for the GNU Project as a free software replacement for the Bourne shell (sh). Released in 1989, it has been distributed widely as the shell for the GNU operating system and as a default shell on Linux and . It has been ported to Microsoft Windows and distributed with Cygwin and MinGW, to DOS by the DJGPP project, to Novell NetWare and to Android via various terminal emulation applications. In the late 1990s, Bash was a minor player among multiple commonly used shells, unlike presently where Bash has overwhelming favor.
Bash is a command processor that typically runs in a text window, where the user types commands that cause actions. Bash can also read commands from a file, called a script. Like all Unix shells, it supports filename wildcarding, piping, here documents, command substitution, variables and control structures for condition-testing and iteration. The keywords, syntax and other basic features of the language were all copied from sh. Other features, e.g., history, were copied from csh and ksh. Bash is a POSIX shell, but with a number of extensions.
The name itself is an acronym, a pun, and a description. As an acronym, it stands for "Bourne-again shell", referring to its objective as a free replacement for the Bourne shell.
As a pun, it expressed that objective in a phrase that sounds similar to "born again", a term for spiritual rebirth. The name is also descriptive of what it did, "bashing together" the features of sh, csh, and ksh.
A security hole in Bash dubbed Shellshock, dating from version 1.03, was discovered in early September 2014.
History.
Brian Fox began coding Bash on January 10, 1988 after Richard Stallman became dissatisfied with the lack of progress being made by a prior developer. Stallman and the Free Software Foundation (FSF) considered a free shell that could run existing sh scripts so strategic to a completely free system built from BSD and GNU code that this was one of the few projects they funded themselves, with Fox undertaking the work as an employee of FSF. Fox released Bash as a beta, version .99, on June 7, 1989 and remained the primary maintainer until sometime between mid-1992 and mid-1994, when he was laid off from FSF and his responsibility was transitioned to another early contributor, Chet Ramey.
In September 2014, Stéphane Chazelas, a Unix/Linux, network and telecom specialist working in the UK, discovered a security bug in the program. The bug, first disclosed on September 24, was named Shellshock and assigned the numbers and . The bug was regarded as severe, since CGI scripts using Bash could be vulnerable, enabling arbitrary code execution. The bug is related to how Bash passes function definitions to subshells through environment variables.
Features.
The Bash command syntax is a superset of the Bourne shell command syntax. The vast majority of Bourne shell scripts can be executed by Bash without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. Bash command syntax includes ideas drawn from the Korn shell (ksh) and the C shell (csh) such as command line editing, command history, the directory stack, the $RANDOM and $PPID variables, and POSIX command substitution syntax $(…). When used as an interactive command shell and pressing the tab key, Bash automatically uses command line completion to match partly typed program names, filenames and variable names. The Bash command-line completion system is very flexible and customizable, and is often packaged with functions that complete arguments and filenames for specific programs and tasks.
Bash's syntax has many extensions lacking in the Bourne shell. Bash can perform integer calculations without spawning external processes. It uses the ((…)) command and the $((…)) variable syntax for this purpose. Its syntax simplifies I/O redirection. For example, it can redirect standard output (stdout) and standard error (stderr) at the same time using the &> operator. This is simpler to type than the Bourne shell equivalent 'command > file 2>&1'. Bash supports process substitution using the <(command)syntax, which substitutes the output of (or input to) a command where a filename is normally used.
When using the 'function' keyword, Bash function declarations are not compatible with Bourne/Korn/POSIX scripts (the Korn shell has the same problem when using 'function'), but Bash accepts the same function declaration syntax as the Bourne and Korn shells, and is POSIX-conformant. Because of these and other differences, Bash shell scripts are rarely runnable under the Bourne or Korn shell interpreters unless deliberately written with that compatibility in mind, which is becoming less common as Linux becomes more widespread. But in POSIX mode, Bash conformance with POSIX is nearly perfect.
Bash supports here documents. Since version 2.05b Bash can redirect standard input (stdin) from a "here string" using the «< operator.
Bash 3.0 supports in-process regular expression matching using a syntax reminiscent of Perl.
Bash 4.0 introduced support for associative arrays. Associative arrays allow a fake support for multi-dimensional (indexed) arrays, in a similar way to AWK:
Brace expansion.
Brace expansion, also called alternation, is a feature copied from the C shell. It generates a set of alternative combinations. Generated results need not exist as files. The results of each expanded string are not sorted and left to right order is preserved:
Users should not use brace expansions in portable shell scripts, because the Bourne shell does not produce the same output.
When brace expansion is combined with wildcards, the braces are expanded first, and then the resulting wildcards are substituted normally. Hence, a listing of JPEG and PNG images in the current directory could be obtained using:
Startup scripts.
When Bash starts it executes the commands in a variety of dot files. Though similar to Bash shell script commands, which have execute permission enabled and an [interpreter directive] like #!/bin/bash, the initialization files used by Bash require neither.
Execution order of startup files.
When started as an interactive login shell.
Bash reads and executes /etc/profile (if it exists). (Often this file calls /etc/bash.bashrc.)
After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile "in that order", and reads and executes the first one that exists and is readable.
When a login shell exits.
Bash reads and executes ~/.bash_logout (if it exists).
When started as an interactive shell (but not a login shell).
Bash reads and executes ~/.bashrc (if it exists). This may be inhibited by using the --norc option. The --rcfile file option forces Bash to read and execute commands from file instead of ~/.bashrc.
Comparison with the Bourne shell and csh startup sequences.
Elements of Bash were derived from the Bourne shell and csh, and allow limited startup file sharing with the Bourne shell and provide some startup features familiar to users of the csh.
Setting inheritable environment variables.
The Bourne shell uses the ~/.profile at login to set environment variables that subprocesses then inherit. Bash can use the ~/.profile in a compatible way, by executing it explicitly from the Bash-specific ~/.bash_profile or ~/.bash_login with the line below. Bash-specific syntax can be kept out of the ~/.profile to keep the latter compatible with the Bourne shell.
Aliases and Functions.
These two facilities, aliases from csh and the more general functions that largely supersede them from Bourne shell, were not typically inheritable from the login shell, and had to be redefined in each subshell spawned from the login shell. Although there is an ENV environment variable that could be applied to the problem, both csh and Bash support per-subshell startup files that address it directly. In Bash, the ~/.bashrc is called for interactive subshells. If user-defined functions from the ~/.bashrc are desired in the login shell as well, the ~/.bash_login can include the line below after any setting up of environment variables:
Commands performed only at login and logout.
The csh supports a ~/.login file for purposes of tasks performed only during initial login, such as displaying system load, disk status, whether email has come in, logging the login time, etc. The Bourne shell can emulate this in the ~/.profile, but doesn't predefine a file name. To achieve similar semantics to the csh model, the ~/.bash_profile can contain the line below, after the environment setup and function setup:
Likewise, the csh has a ~/.logout file run only when the login shell exits. The Bash equivalent is ~/.bash_logout, and requires no special setup. In the Bourne shell, the trap built-in can be used to achieve a similar effect. 
Legacy-compatible Bash startup example.
The skeleton ~/.bash_profile below is compatible with the Bourne shell and gives semantics similar to csh for the ~/.bashrc and ~/.bash_login. The [ -r "filename" ] are tests to see if the "filename" exists and is readable, simply skipping the part after the && if it's not.
Operating system issues in Bash startup.
Some versions of Unix and Linux contain Bash system startup scripts, generally under the /etc directories. Bash calls these as part of its standard initialization, but other startup files can read them in a different order than the documented Bash startup sequence. The default content of the root user's files may also have issues, as well as the skeleton files the system provides to new user accounts upon setup. The startup scripts that launch the X window system may also do surprising things with the user's Bash startup scripts in an attempt to set up user environment variables before launching the window manager. These issues can often be addressed using a ~/.xsession or ~/.xprofile file to read the ~/.profile—which provides the environment variables the Bash shell windows spawned from the window manager needs, such as xterm or Gnome Terminal.
Portability.
Invoking Bash with the codice_1 option or stating codice_2 in a script causes Bash to conform very closely to the POSIX 1003.2 standard.
Bash shell scripts intended for portability should at least take into account the Bourne shell it intends to replace. 
Bash has certain features that the traditional Bourne shell lacks. Among these are:
Keyboard shortcuts.
The following shortcuts work when using default (Emacs) key bindings. Vi-bindings can be enabled by running codice_3.
Note: For shortcuts involving , you may be able to use instead.
Note: Sometimes, you must use instead of , because the shortcut conflicts with another shortcut. For example, in Trisquel 5.0 (a distribution of Linux), pressing does not move the cursor forward one word, but activates "File" in the menu of the terminal window.
Process management.
The Bash shell has two modes of execution for commands: batch (Unix), and concurrent mode.
To execute commands in batch (i.e., in sequence) they must be separated by the character ";":
in this example, when command1 is finished, command2 is executed.
To have a concurrent execution of command1 and command2, they must be executed in the Bash shell in the following way:
In this case command1 is executed in background (symbol &), returning immediately the control to the shell that executes command2.
Summarizing:
Bug reporting.
An external command called "bashbug" reports Bash shell bugs. When the command is invoked, it brings up the user's default editor with a form to fill in. The form is mailed to the Bash maintainers or optionally to other email addresses.

</doc>
<doc id="4548" url="http://en.wikipedia.org/wiki?curid=4548" title="Blizzard">
Blizzard

Blizzard is a severe snowstorm characterized by strong sustained by winds of at least and lasting for a prolonged period of time – typically three hours or more. A ground blizzard is a weather condition where snow is not falling but loose snow on the ground is lifted and blown by strong winds.
Definition.
In the United States, the National Weather Service defines a blizzard as a severe snowstorm characterized by strong winds causing blowing snow that results in low visibilities. The difference between a blizzard and a snowstorm is the strength of the wind, not the amount of snow. To be a blizzard, a snow storm must have sustained winds or frequent gusts that are greater than or equal to with blowing or drifting snow which reduces visibility to or less and must last for a prolonged period of time – typically three hours or more.
 While severe cold and large amounts of drifting snow may accompany blizzards, they are not required. Blizzards can bring whiteout conditions, and can paralyze regions for days at a time, particularly where snowfall is unusual or rare.
A severe blizzard has winds over
, near zero visibility, and temperatures of or lower. In Antarctica, blizzards are associated with winds spilling over the edge of the ice plateau at an average velocity of .
Ground blizzard refers to a weather condition where loose snow or ice on the ground is lifted and blown by strong winds. The primary difference between a ground blizzard as opposed to a regular blizzard is that in a ground blizzard no precipitation is produced at the time, but rather all the precipitation is already present in the form of snow or ice at the surface.
The Australia Bureau of Meteorology describes a blizzard as, "Violent and very cold wind which is laden with snow, some part, at least, of which has been raised from snow covered ground."
Blizzard conditions of cold temperatures and strong winds can cause wind chill values that can result in hypothermia or frostbite. The wind chill factor is the amount of cooling the human body feels due to the combination of wind and temperature.
Blizzards in Australia.
Blizzards are not common in mainland Australia, but occur frequently in the Snowy Mountains in New South Wales and Victoria. When blizzards do occur, they can affect the Tasmanian Highlands and, particularly, Mount Wellington, which towers over the Tasmanian capital Hobart. Blizzards do not affect any major towns or cities, because there are no populated areas located in the mountains except for the ski resort towns of New South Wales and Victoria.
United States storm systems.
In the United States, storm systems powerful enough to cause blizzards usually form when the jet stream dips far to the south, allowing cold, dry polar air from the north to clash with warm, humid air moving up from the south. They are most common in the Great Plains, the Great Lakes states, and the northeastern states along the coast, and less common in the Pacific Northwest.
When cold, moist air from the Pacific Ocean moves eastward to the Rocky Mountains and the Great Plains, and warmer, moist air moves north from the Gulf of Mexico, all that is needed is a movement of cold polar air moving south to form potential blizzard conditions that may extend from the Texas panhandle to the Great Lakes.
Another storm system occurs when a cold core low over the Hudson Bay area in Canada is displaced southward over southeastern Canada, the Great Lakes, and New England. When the rapidly moving cold front collides with warmer air coming north from the Gulf of Mexico, strong surface winds, a lot of cold air advection, and extensive wintry precipitation occur.
Low pressure systems moving out of the Rocky Mountains onto the Great Plains, a broad expanse of flat land, much of it covered in prairie, steppe and grassland, can cause thunderstorms and rain to the south and heavy snows and strong winds to the north. With few trees or other obstructions to reduce wind and blowing, this part of the country is particularly vulnerable to blizzards with very low temperatures and whiteout conditions. In a true whiteout there is no visible horizon. People can become lost in their own front yards, when the door is only away, and they would have to feel their way back. Motorists have to stop their cars where they are, as the road is impossible to see.
Nor'easter blizzards.
A nor'easter is a macro-scale storm along the East Coast of the United States and Atlantic Canada; it gets its name from the direction the wind is coming from. The usage of the term in North America comes from the wind associated with many different types of storms some of which can form in the North Atlantic Ocean and some of which form as far south as the Gulf of Mexico. The term is most often used in the coastal areas of New England and Atlantic Canada. This type of storm has characteristics similar to a hurricane. More specifically it describes a low-pressure area whose center of rotation is just off the East Coast and whose leading winds in the left-forward quadrant rotate onto land from the northeast. High storm waves may sink ships at sea and cause coastal flooding and beach erosion. Notable nor'easters include The Great Blizzard of 1888, one of the worst blizzards in U.S. history. It dropped of snow and had sustained winds of more than 45 miles per hour (72 km/h) that produced snowdrifts in excess of 50 feet (15 m). Railroads were shut down and people were confined to their houses for up to a week. It killed 400 people, mostly in New York.
Historic events.
1972 Iran blizzard.
The 1972 Iran Blizzard, which caused approximately 4,000 deaths, was the deadliest blizzard in recorded history. Dropping as much as of snow, it completely covered 200 villages. After a snowfall lasting nearly a week, an area the size of Wisconsin was entirely buried in snow.
The Snow Winter of 1880–1881.
The winter of 1880–1881 is widely considered the most severe winter ever known in the United States. Many children – and their parents – learned of "The Snow Winter" through the children's book "The Long Winter" by Laura Ingalls Wilder, in which the author tells of her family's efforts to survive. The snow arrived in October 1880 and blizzard followed blizzard throughout the winter and into March 1881, leaving many areas snowbound throughout the entire winter. Accurate details in Wilder's novel include the blizzards' frequency and the deep cold, the Chicago and North Western Railway stopping trains until the spring thaw because the snow made the tracks impassable, the near-starvation of the townspeople, and the courage of her husband Almanzo and another man, who ventured out on the open prairie in search of a cache of wheat that no one was even sure existed.
The October blizzard brought snowfalls so deep that two-story homes had snow up to the second floor windows. No one was prepared for the deep snow so early in the season and farmers all over the region were caught before their crops had even been harvested, their grain milled, or with their fuel supplies for the winter in place. By January the train service was almost entirely suspended from the region. Railroads hired scores of men to dig out the tracks but it was a wasted effort: As soon as they had finished shoveling a stretch of line, a new storm arrived, filling up the line and leaving their work useless.
There were no winter thaws and on February 2, 1881, a second massive blizzard struck that lasted for nine days. In the towns the streets were filled with solid drifts to the tops of the buildings and tunneling was needed to secure passage about town. Homes and barns were completely covered, compelling farmers to tunnel to reach and feed their stock.
When the snow finally melted in late spring of 1881, huge sections of the plains were flooded. Massive ice jams clogged the Missouri River and when they broke the downstream areas were ravaged. Most of the town of Yankton, in what is now South Dakota, was washed away when the river overflowed its banks.
The Storm of the Century.
The Storm of the Century, also known as the Great Blizzard of 1993, was a large cyclonic storm that formed over the Gulf of Mexico on March 12, 1993, and dissipated in the North Atlantic Ocean on March 15. It is unique for its intensity, massive size and wide-reaching effect. At its height, the storm stretched from Canada towards Central America, but its main impact was on the Eastern United States and Cuba. The cyclone moved through the Gulf of Mexico, and then through the Eastern United States before moving into Canada. Areas as far south as central Alabama and Georgia received of snow and areas such as Birmingham, Alabama, received up to with isolated reports of . Even the Florida Panhandle reported up to , with hurricane-force wind gusts and record low barometric pressures. Between Louisiana and Cuba, hurricane-force winds produced high storm surges across northwestern Florida, which along with scattered tornadoes killed dozens of people. Record cold temperatures were seen across portions of the South and East in the wake of this storm. In the United States, the storm was responsible for the loss of electric power to over 10 million customers. It is purported to have been directly experienced by nearly 40 percent of the country's population at that time. A total of 310 people, including 10 from Cuba, perished during this storm.

</doc>
<doc id="4550" url="http://en.wikipedia.org/wiki?curid=4550" title="Bikini">
Bikini

A bikini is generally a two-piece swimsuit that comprises panties-style bottoms that cover at least a female's crotch and a bra-style top that covers at least her breasts, but which leaves her midriff exposed, and usually the navel and waist. The size of a bikini bottom can range from full pelvic coverage to a revealing thong or g-string design.
The modern bikini was popularised by French engineer Louis Réard and separately by fashion designer Jacques Heim in Paris in 1946. The take up of the style was controversial, and many western countries banned it from beaches and public places, with the Vatican declaring it sinful. Popularized by filmstars like Brigitte Bardot and Ursula Andress it became common in most western countries by the mid-1960s. Though widely popular, the bikini continues to be controversial with it being banned in parts of the world, and even in western countries it is banned in schools and commonly covered in places away from the beach or swimming pool.
The original bikini style has developed in popular culture a positive connotation, giving raise to variations of the term being used to describe stylistic variations. These forms are used predominantly for promotional purposes and as an industry classification, but which are not of importance to the general public, and are described as variants of the bikini. These variants often use endings such as "-kini"s and "-ini"s, such as microkini, tankini, trikini, pubikini, bandeaukini and skirtini. The term "bikini" and its derivatives have been liberally applied in contexts not related to its original use as a style of two-piece woman's swimwear. It may be used, for example, to describe a man's brief swimsuit or a style of men's and women's bikini-style underwear, to bikini waxing, and in other contexts. A "monokini" refers to women's one-piece topless swimwear, while a sling bikini is a one-piece swimsuit, but with large parts cut out. A "burqini" is a swimsuit that covers most of the body. Swimwear comprising pasties with a matching "maebari"-style bottom is sometimes called a strapless bikini or a no-string bikini.
Etymology.
While the two-piece swimsuit as a design existed in classical antiquity, the modern design first attracted public notice in Paris on July 5, 1946. French mechanical engineer Louis Réard introduced a design he named the "bikini," taking the name from the Bikini Atoll in the Pacific Ocean, where, four days earlier, the United States had initiated its first peace-time nuclear weapons test as part of Operation Crossroads. Réard hoped his swimsuit's revealing style would create an "explosive commercial and cultural reaction" similar to the explosion at Bikini Atoll. His name for the garment stuck with the media and the public.
Through analogy with words, like "bilingual" and "bilateral", containing the Latin prefix "bi-" (meaning "two" in Latin), the word "bikini" was first back-derived as consisting of two parts, ["bi" + "kini"] by Rudi Gernreich, who introduced the "mono"kini in 1964. Later swimsuit designs like the "tan"kini and "tri"kini further cemented this false assumption. Over time the ""–kini" family" (as dubbed by author William Safire), including the ""–ini" sisters" (as dubbed by designer Anne Cole), expanded into a variety of swimwear, often with an innovative lexicon, including the monokini (also numokini or unikini), seekini, tankini, camikini, hikini (also hipkini), minikini, and microkini.
History.
In antiquity.
The origins of the two-piece swimsuit can be traced to antiquity, in Çatalhöyük, where a mother goddes is depicted astride two leopards wearing a costume somewhat like a bikini, and the Greco-Roman world, where bikini-like garments worn by women athletes are depicted on urns and paintings dating back to 1400 BC. In "Coronation of the Winner," a mosaic in the floor of a Roman villa in Sicily that dates from the Diocletian period (286–305 AD), young women participate in weightlifting, discus throwing, and running ball games dressed in bikini-like garments (technically bandeaukinis in modern lexicon). The mosaic, found in the Sicilian Villa Romana del Casale, features ten maidens who have been anachronistically dubbed the "Bikini Girls". Other Roman archaeological finds depict the goddess Venus in a similar garment. In Pompeii, depictions of Venus wearing a bikini were discovered in the Casa della Venere, in the "tablinum" of the House of Julia Felix, and in an atrium garden of Via Dell'Abbondanza.
Bikini precursors.
Swimming or bathing outdoors were discouraged in the Christian West, and so there was little demand or need for swimming or bathing costume until the 18th century. The bathing gown of the 18th century was a loose ankle-length full-sleeve chemise-type gown made of wool or flannel, so that modesty or decency was not threatened. 
In 1907, Australian swimmer and performer Annette Kellerman was arrested on a Boston beach for wearing a form-fitting sleeveless one-piece knitted swimming tights that covered her from neck to toe, a costume she adopted from England, although it became accepted swimsuit attire for women in parts of Europe by 1910. In 1913, inspired by the introduction of females into Olympic swimming, the designer Carl Jantzen made the first functional two-piece swimwear, a close-fitting one-piece with shorts on the bottom and short sleeves on top.
During the 1920s and 1930s, people began to shift from "taking in the water" to "taking in the sun," at bathhouses and spas, and swimsuit designs shifted from functional considerations to incorporate more decorative features. Rayon was used in the 1920s in the manufacture of tight-fitting swimsuits, but its durability, especially when wet, proved problematic; jersey and silk were also sometimes used. By the 1930s, manufacturers had lowered necklines in the back, removed sleeves, and tightened the sides. With the development of new clothing materials, particularly latex and nylon, through the 1930s swimsuits gradually began hugging the body, with shoulder straps that could be lowered for tanning. 
Women's swimwear of the 1930s and 1940s incorporated increasing degrees of midriff exposure. Teen magazines of late 1940s and 1950s featured similar designs of midriff-baring suits and tops. However, midriff fashion was stated as only for beaches and informal events and considered indecent to be worn in public. Hollywood endorsed the new glamor in films like "Neptune's Daughter" in which Esther Williams wore provocatively named costumes such as "Double Entendre" and "Honey Child".
Wartime production during World War II required vast amounts of cotton, silk, nylon, wool, leather, and rubber. In 1942 the United States War Production Board issued Regulation L-85, cutting the use of natural fibers in clothing and mandating a 10% reduction in the amount of fabric in women's beachwear. To comply with the regulations, swimsuit manufacturers removed skirt panels and other superfluous material and increased production of the two-piece swimsuit with bare midriffs. At the same time, demand for all swimwear declined as there was not much interest in going to the beach, especially in Europe. The fabric shortage continued for some time after the end of the war.
Modern bikini.
With the fabric shortage still in place and in an endeavour to resurrect swimwear sales, two French designers – Jacques Heim and Louis Réard – almost simultaneously launched their new two-piece swimsuit ranges in 1946. Jacques Heim launched his two-piece swimsuit in Paris which he called the "atome", after the smallest known particle of matter, which he advertised as the world's "smallest bathing suit". Although briefer than the two-piece swimsuits of the 1930s, the bottom of Heim's new two-piece beach costume still covered the wearer's navel.
At about the same time, Louis Réard created a competing two-piece swimsuit design, which he called the "bikini". Réard's "bikini" topped Heim's "atome" in brevity. His costume was created in the form of a bra and two triangular pieces connected by strips of material, slicing the top off Heim's bottoms, with a total area of of cloth with newspaper-type print, which was advertised as "smaller than the smallest swimsuit". After not being able to find a model willing to showcase his revealing design, Réard hired Micheline Bernardini, a 19-year old nude dancer from the Casino de Paris. Bernardini received 50,000 fan letters, many of them from men. 
Réard said that "like the [atom] bomb, the bikini is small and devastating". Fashion writer Diana Vreeland described the bikini as the "atom bomb of fashion". In advertisements he declared the swimsuit couldn't be a genuine bikini "unless it could be pulled through a wedding ring." French newspaper "Le Figaro" wrote, "People were craving the simple pleasures of the sea and the sun. For women, wearing a bikini signaled a kind of second liberation. There was really nothing sexual about this. It was instead a celebration of freedom and a return to the joys in life." 
Heim's "atome" was a bigger hit than Réard's design, being more attune to the sense of propriety of the 1940s, but Réard's was the design that won the public's imagination over time. Both Heim's and Réard's businesses soared in France. According to WordIQ.com, it took fifteen years for Réard's bikini to be accepted in the United States. Though Heim's design was the first worn on the beach and sold more swimsuits, it was Réard's description of the two-piece swimsuit as a "bikini" that stuck. As the style gained world-wide acceptance, the term became a generic or common name for a two-piece swimsuit, instead of the original use as a brand name, and Réard's design became the standard for the two-piece swimsuit.
Social resistance.
Despite the garment's initial success in France, worldwide women still stuck to traditional one-piece swimsuits, and, his sales stalling, Réard went back to designing and selling orthodox knickers. In a 1950 "Time" magazine interview, American swimsuit mogul Fred Cole, owner of mass market swimwear firm Cole of California, stated that he had "little but scorn for France's famed Bikinis"; Réard himself would later describe it as a "two-piece bathing suit which reveals everything about a girl except for her mother's maiden name." Fashion magazine "Modern Girl Magazine" in 1957 stated that "it is hardly necessary to waste words over the so-called bikini since it is inconceivable that any girl with tact and decency would ever wear such a thing".
In 1951, Eric Morley organized the "Festival Bikini Contest", a beauty contest and swimwear advertising opportunity at that year's Festival of Britain. The press, welcoming the spectacle, referred to it as "Miss World", a name Morley registered as a trademark. The winner was Kiki Håkansson of Sweden who was crowned in a bikini. After the crowing Håkansson was condemned by the Pope, and some countries with religious traditions threatened to withdraw delegates from the pageant. In 1952, bikinis were banned from the pageant and replaced by evening gowns. As a result of the controversy the bikini was explicitly banned from many other beauty pageants worldwide. Though some regarded the bikini and beauty contests as freedom to women, they were opposed by some feminists as well as religious and cultural groups who objected to the degree of exposure of the female body.
The bikini was banned on the French Atlantic coastline, Spain, Italy, Belgium, Portugal and Australia, and was prohibited or discouraged in a number of US states. The United States Motion Picture Production Code, also known as the Hays Code, enforced from 1934, allowed two-piece gowns but prohibited the display of navels in Hollywood films. The National Legion of Decency, a Roman Catholic body guarding over American media content, also pressured Hollywood and foreign film producers to keep bikinis from being featured in Hollywood movies. As late as 1959, Anne Cole, one of the United State's largest swimsuit designers, said, "It's nothing more than a G-string. It's at the razor's edge of decency." The Hays Code was abandoned by the mid-1960s, and with it the prohibition of female navel exposure, as well as other restrictions. The influence of the National Legion of Decency had also waned by the 1960s.
Rise to popularity.
Increasingly common glamour shots of popular actresses and models on either side of the Atlantic played a large part in bringing the bikini into the mainstream. During the 1950s, Hollywood stars such as Ava Gardner, Rita Hayworth, Lana Turner, Elizabeth Taylor, Tina Louise, Marilyn Monroe, Esther Williams, and Betty Grable took advantage of the risqué publicity associated with the bikini by posing for photographs wearing them—pin-ups of Hayworth and Williams in costume were especially widely distributed in the United States. 
In Europe, 17-year-old Brigitte Bardot wore scanty bikinis (by contemporary standards) in the French film "Manina, la fille sans voiles" ("Manina, the girl unveiled"). The promotion for the film, released in France in March 1953, drew more attention to Bardot's bikinis than to the film itself. By the time the film was released in the United States in 1958 it was re-titled "Manina, the Girl in the Bikini". Bardot was also photographed wearing a bikini on the beach during the 1953 Cannes Film Festival. Working with her husband and agent Roger Vadim she garnered significant attention with photographs of her wearing a bikini on every beach in the south of France. Similar photographs were taken of Anita Ekberg and Sophia Loren, among others. According to "The Guardian", Bardot's photographs in particular turned Saint-Tropez into the bikini capital of the world, with Bardot identified as the original Cannes bathing beauty. Bardot's photography helped to enhance the public profile of the festival, and Cannes in turn played a crucial role in her career. 
Brian Hyland's novelty-song hit "Itsy Bitsy Teenie Weenie Yellow Polka Dot Bikini" became a "Billboard" No. 1 hit during the summer of 1960: the song tells a story about a young girl who's too shy to wear her new bikini on the beach, thinking it too risqué. "Playboy" first featured a bikini on its cover in 1962; the Sports Illustrated Swimsuit Issue debut two years later featured Babette March in a white bikini on the cover.
Ursula Andress, appearing as Honey Rider in the 1962 British James Bond film, "Dr. No", wore a white bikini, which became known as the "Dr. No bikini". It is cited as the most famous bikini of all time and an iconic moment in cinematic and fashion history. Andress said that she owed her career to that white bikini, remarking, "This bikini made me into a success. As a result of starring in "Dr. No" as the first Bond girl, I was given the freedom to take my pick of future roles and to become financially independent." 
The bikini finally caught on, and by 1963, the movie "Beach Party", starring Annette Funicello and Frankie Avalon, led a wave of films that made the bikini a pop-culture symbol, though Funicello was barred from wearing Réard's bikini unlike the other young females in the films. In 1965, a woman told "Time" that it was "almost square" not to wear a bikini; the magazine wrote two years later that "65% of the young set had already gone over", 
Raquel Welch wore a deer skin bikini in the British film "One Million Years B.C." (1966) that made her an instant pin-up girl. Her role wearing the fur bikini raised Welch to the status of a fashion icon and the photo of her in the bikini became a best-selling pinup poster. Welch was featured in the studio's advertising as "wearing mankind's first bikini", and the fur bikini was later described as a definitive look of the 1960s. In 2011, "Time" listed Welch's "B.C." bikini in the "Top Ten Bikinis in Pop Culture". 
The 1967 film "An Evening in Paris", is mostly remembered today because in it Bollywood actress Sharmila Tagore became the first Indian actress to wear a bikini on film. She also posed in a bikini for the glossy "Filmfare" magazine. The costume shocked a conservative Indian audience, but it also set in motion a trend carried forward by Zeenat Aman in "Heera Panna" (1973) and "Qurbani" (1980), Dimple Kapadia in "Bobby" (1973), and Parveen Babi in "Yeh Nazdeekiyan" (1982).
Mass acceptance.
Réard's company folded in 1988, four years after his death. By 1988 the bikini made up nearly 20% of swimsuit sales, more than any other model in the US, though one-piece suits made a comeback during the 1980s and early 1990s. By the end of the century, the bikini had become the most popular beachwear around the globe. According to French fashion historian Olivier Saillard, this was due to "the power of women, and not the power of fashion". As he explains, "The emancipation of swimwear has always been linked to the emancipation of women", though one survey indicates 85% of all bikinis never touch the water. 
In 1997, Miss Maryland Jamie Fox became the first contestant in 50 years to compete in a two-piece swimsuit at the Miss America Pageant. Actresses in action films like "Blue Crush" (2002) and "" (2003) made the two-piece "the millennial equivalent of the power suit", according to Gina Bellafonte of "The New York Times", 
Huludao City, Liaoning, China set the world record for the largest bikini parade in 2012, with 1,085 participants and a photo shoot involving 3,090 women. According to Beth Dincuff Charleston, research associate at the Costume Institute of the Metropolitan Museum of Art, "The bikini represents a social leap involving body consciousness, moral concerns, and sexual attitudes." By the early 2000s, bikinis had become a $811 million business annually, according to the NPD Group, a consumer and retail information company, and had boosted spin-off services like bikini waxing and the sun tanning industries.
Bikini variants.
While the name "bikini" was at first applied only to beachwear that revealed the wearer's navel, today the fashion industry considers any two-piece swimsuit a bikini. Modern bikini fashions are characterized by a simple, brief design: two triangles of fabric that form a bra and cover the woman's breasts and a third that forms a panty cut below the navel that cover the groin and the buttocks. The coverage offered can vary widely, from revealing pubikini, microkini and string bikinis to a fuller designs such as the tankinis, skirtinis or bandeaukinis.
Bikinis can and have been made out of almost every possible clothing material, and the fabrics and other materials used to make bikinis are an essential element of their design. The use of cotton makes the swimsuit more practical, and the increased reliance on stretch fabric after 1960 simplified construction. Modern bikinis were first made of cotton and jersey. DuPont's introduction of Lycra (spandex) in the 1960s completely changed how bikinis were designed and worn, as according to Kelly Killoren Bensimon, a former model and author of "The Bikini Book", "the advent of Lycra allowed more women to wear a bikini...it didn't sag, it didn't bag, and it concealed and revealed. It wasn't so much like lingerie anymore." Alternative swimwear fabrics such as velvet, leather, and crocheted squares surfaced in the early 1970s. 
Bikini variations has grown to include a large number of more or less revealing styles — string bikinis, monokinis (topless), seekinis (transparent), tankinis (tank top, bikini bottom), camikinis (camisole top, bikini bottom), hikinis (also hipkini), "granny bikini" (bikini top, boy shorts bottom), thong, minikinis, microkinis, miniminis, slingshot, tie-side and teardrop. In one major fashion show in 1985, there were two-piece suits with cropped tank tops instead of the usual skimpy bandeaux, suits that resembled bikinis in front and one-piece in back, suspender straps, ruffles, and deep navel-baring cutouts. Metal and stone jewelery pieces are now also often used to dress up look and style according to tastes, and to meet the fast pace of demands, some manufacturers now offer made-to-order bikinis ready in as few as seven minutes. The world's most expensive bikini was designed in February 2006 by Susan Rosen; containing of diamond, it was valued at £20 million.
Bikini in sport.
Bikinis have become a major component of marketing various women's sports and is an official uniform for beach volleyball and is widely worn in athletics and other sports. However, the trend has raised some criticism. Female swimmers do not normally wear bikinis in competitive swimming.
Beach volleyball.
In 1994, the bikini became the official uniform of women's Olympic beach volleyball, although some sports officials consider it exploitative and impractical in colder weather. Competitors such as Natalie Cook and Holly McPeak agree with the FIVB's statements that the uniforms are practical for a sport played on sand during the heat of summer, but British Olympian Denise Johns argues that the regulation uniform is intended to be "sexy" and to attract attention.
In 1999, the International Volleyball Federation (FIVB) standardized beach volleyball uniforms, with the bikini becoming the required uniform for women. This drew the ire of some athletes. According to FIVB rules, female beach volleyball players have the option of playing in shorts or a one-piece swimsuit, (with national associations deciding the uniform for the national squads) but most players prefer the bikini. In early 2012, FIVB announced it would allow shorts (maximum length above the knee) and sleeved tops at the London 2012 Olympics. Richard Baker, the federation spokesperson, said that "many of these countries have religious and cultural requirements so the uniform needed to be more flexible". At the time of the event, the weather at the evening games in London during 2012 was so cold that the players sometimes had to wear shirts and leggings. At the 2006 Asian Games at Doha, Qatar, only one Muslim country fielded a team in the beach volleyball competition because of concerns that the uniform was inappropriate. The Iraqi team refused to wear bikinis.
Beach volleyball became the fifth largest television audience of all the sports at the 2000 Summer Olympics at Bondi Beach in Australia owing, it is claimed, to the sex appeal of bikini-clad players as well as their athletic ability. Kimberly Bissell conducted a study on the camera angles used during the 2004 Summer Olympic Games beach volleyball games. Bissell found that 20% of the camera angles were focused on the women's chests, and 17% on their buttocks. Bissell theorized that the appearance of the players draws fans attention more than their actual athleticism. The popularity of "", a video game for Xbox, was attributed to the scantily clad women. In 2007, fans voted for contestants in the WWE Diva contest after watching them playing beach volleyball in skimpy bikinis.
During the 2004 Olympics, an exotic dance team from the Canary Islands entertained fans but drew some criticism from female competitors. During breaks in between points and matches, the group, wearing bikinis, raced on to the sand and danced to techno-pop music. Australian athlete Nicole Sanderson commented, "It's kind of disrespectful to the female players. I'm sure the male spectators love it, but I find it a little bit offensive."
Athletics.
Women in athletics often wear bikinis the same size as those worn in beach volleyball. Amy Acuff, a US high-jumper, wore a black leather bikini instead of a track suit at the 2000 Summer Olympics. Runner Florence Griffith-Joyner mixed bikini bottoms with one-legged tights at the 1988 Summer Olympics, earning her more attention than her record breaking performance in the women's 200 meters event.
In the 2007 South Pacific Games, the rules were adjusted to allow players to wear less revealing shorts and cropped sports tops instead of bikinis. At the West Asian Games in 2006, organizers banned bikini-bottoms for female athletes and asked them to wear long shorts.
Bodybuilding.
During 1950s to mid-1970s men's contest formats was often supplemented with women's beauty contests or bikini shows. The winners earned titles like Miss Body Beautiful, Miss Physical Fitness and Miss Americana, and also presented trophies to the winners of the men's contest. In the 1980s, the Ms Olympia competition started in the USA and in UK the NABBA (National Amateur Body Building Association) renamed Miss Bikini International to Ms Universe. In 1986, Ms Universe competition was divided in to two sections – "physique" (for a more muscular physique) and "figure" (traditional feminine presentation in high heels). In November 2010 IFBBF (International Federation of BodyBuilding & Fitness) introduced women's bikini contest for women who do not wish to build their muscles to figure competition levels.
Costumes are regulation "posing trunks" (bikini briefs) for both men and women. Female bodybuilders in America are prohibited from wearing thongs or T-back swimsuits in contests filmed for television, though they are allowed to do so by certain fitness organizations in closed events. For men, the dress code specifies "swim trunks only (no shorts, cut-off pants, or Speedos)." A similar policy by Virginia FCCLA bans "skimpy bikini or thong type suits" for women and specifies "swim trunks" for men ("no speedos").
Other sports.
The Bikini Basketball Association is an American women's basketball league, created by Cedric Mitchell and A. J. McArthur in 2012. The players wear sports bras and boy shorts, during games. Commentators found it variously funny, offensive, and smart business. String bikinis and other skimpy clothes are also common in surfing. In 2001, Vicky Botwright, then 16th seeded in women's squash circuit and dubbed as the 'Lancashire Hot Bot', was prohibited by Women's International Squash Players Association (WISPA) from wearing her trademark outfit, a thong and a sports bra, in the British Open Championships. In 2004, Alexander Putnam competed in the London Marathon in a green thong and painted as a tropical tree to protest against logging in Congo.
Bikini body.
In 1950 American swimsuit mogul Fred Cole, owner of Cole of California, told "Time magazine" that bikinis were designed for "diminutive Gallic women", as because "French girls have short legs...swimsuits have to be hiked up at the sides to make their legs look longer." The "New York Times" reported the opinion that the bikini is permissible for people are not "too fat or too thin." In the 1960s etiquette writer Emily Post decreed that "[A bikini] is for perfect figures only, and for the very young." In "The Bikini Book" by Kelly Killoren Bensimon, swimwear designer Norma Kamali says, "Anyone with a tummy" should not wear a bikini. Since then, a number of bikini designers including Malia Mills have encouraged women of all ages and body types to take up the style. The 1970s saw the rise of the lean ideal of female body and figures like Cheryl Tiegs, who possessed the figure that remains in vogue in the 21st century.
The fitness boom of the 1980s led to one of the biggest leaps in the evolution of the bikini. According to Mills, "The leg line became superhigh, the front was superlow, and the straps were superthin." Women's magazines used terms like "Bikini Belly", and workout programs were launched to develop a "bikini-worthy body". The tiny "fitness-bikinis" made of lycra were launched to cater to this hardbodied ideal, epitomized by six-time Sports Illustrated Swimsuit Issue cover model Elle Macpherson. Movies like "Blue Crush" and TV reality shows like "Surf Girls" has merged to concept of bikini model and athletes together, further accentuating the toned body ideal.
One survey commissioned by Diet Chef, an UK home delivery service, reported by "Daily Mail" and "The Today Show" and ridiculed by "More" magazine showed that women should stop wearing bikinis by the age of 47. Yearly Spring Break festivities, which mark the start of the bikini season, trigger many with eating disorders because of the over-promotion of the bikini body ideal.
In 1993, Suzy Menkes, then Fashion Editor of the "International Herald Tribune", suggested that women had begun to "revolt" against the "body ideal" and bikini "exposure." She wrote, "Significantly, on the beaches as on the streets, some of the youngest and prettiest women (who were once the only ones who dared to bare) seem to have decided that exposure is over." Nevertheless, professional beach volleyball player Gabrielle Reece, who competes in a bikini, claims that "confidence" alone can make a bikini sexy.
Bikini underwear.
Certain types of underwear worn by both men and women are identified as bikini-style underwear because they are similar in size and form to the bottom half of a bikini bathing suit. For women, bikini underwear can refer to virtually any tight, skimpy, or revealing undergarment that provides less coverage to the midsection than traditional underwear, panties or knickers. For men, a bikini is a type of undergarment that is smaller and more revealing than men's briefs. Bikini briefs can be low- or high-side bikini briefs but are usually lower than true waist, often at hips, and usually have no access pouch or flap, legs bands at tops of thighs. String bikini briefs have front and rear sections that meet in the crotch but not at the waistband, with no fabric on the side of the legs.
Swimwear and underwear have always had close design connections because of their shared proximity to the body; the primary distinguishing feature between them is that swimwear takes underwear to the public view. The swimsuit was and is closely aligned to underwear in terms of styling, and at about the same time that attitudes towards the bikini began to change, underwear underwent a redesign towards a minimal, unboned design that emphasized comfort first. 
History.
As the swimsuit was evolving, the underwear started to change. Between 1900 to 1940, swimsuit lengths followed the changes in underwear designs. In the 1920s women started discarding the corset, while the Cadole company of Paris started developing something they called the "breast girdle". During the Great Depression, panties and bras became softly constructed and were made of various elasticized yarns making underwear fit like a second skin. By 1930s underwear styles for both women and men were influenced by the new brief models of swimwear from Europe. Although the waistband was still above the navel, the leg openings of the panty brief were cut in an arc to rise from the crotch to the hip joint. The brief served as a template for most all variations of panties for the rest of the century. Warner standardized the concept of Cup size in 1935. The first underwire bra was developed in 1938. Beginning in the late thirties skants, a type of skanty men's briefs, were introduced, featuring very high-cut leg openings and a lower rise to the waistband. Howard Hughes designed the push-up bra worn by Jane Russell in "The Outlaw" in 1943. In 1950 Maidenform introduced the first official bust enhancing bra.
By the 1960s, the bikini swimsuit influenced panty styles and coincided with the cut of the new lower rise jeans and pants. In the seventies, with the emergence of skintight jeans, thong versions of the panty became mainstream, since the open, stringed back eliminated any tell-tale panty lines across the rear and hips. By the 1980s the design of the French-cut panty pushed the waistband back up to the natural waistline and the rise of the leg openings was nearly as high (French Cut panties come up to the waist, has a high cut leg, and usually are full in the rear). As with the bra and other type of lingerie, manufacturers of the last quarter of the century marketed panty styles that were designed primarily for their sexual allure. This decade marks the sexualization and eroticization of the male body through advertising campaigns for brands such as Calvin Klein, particularly by photographers Bruce Weber and Herb Ritts. Male bodies and men's undergarments were commodified and packaged for mass consumption, and swimwear and sportswear were influenced by sports photography and fitness. Over time, swimwear evolved from weighty wool to high-tech skin-tight garments, eventually cross-breeding with sportswear, underwear and exercise wear, resulting in the interchangeable fashions of the 1990s.
Men's bikini.
The term "men's bikini" is used to describe a specific type of men's swimsuit. Men's bikinis can have both high or low side panels, string sides or tie sides, and most lack a button or flap front. Unlike swim briefs, bikinis are not designed for drag reduction and generally lack a visible waistband. Suits less than 1.5 inches wide at the hips are less common for sporting purposes and are most often worn for recreation, fashion, and sun tanning. The posing brief standard to bodybuilding competitions is an example of this style. Male punk rock musicians have performed on the stage wearing women's bikini briefs. The 2000 Bollywood film "Hera Pheri" shows men sunbathing in bikinis, who were mistakenly believed to be girls from a distance.
Swimsuits shown in men's wear collections by Giorgio Armani, Dolce & Gabbana and Paul Smith have tended to be black and snug fitting, throwbacks to the designs of the 1930s and '40s. Gianni Versace's ads contain heroic depictions of Miami bathers in contrast to popular, sports-inspired beach wear—bright and baggy Bermudas or boxer shorts. The Greek designer Nikos Apostolopoulos put a different spin on his bathing suits (for both sexes, but with the focus on the male), making them anatomical creations, cut and stitched to outline the body and its sexual characteristics. Male bikini tops, a visual gag, also exist.
A mankini is not a form of a bikini, despite its name, but a type of sling swimsuit worn by men. It was popularized by Sacha Baron Cohen, who donned one in the film "Borat".
Bikini waxing.
Bikini waxing is the epilation of pubic hair in and around the pubic region (also known as the bikini line), commonly by women, by the use of wax. With certain styles of women's swimwear, pubic hair may become visible around the crotch area of a swimsuit. 
The bikini line delineates the part of a woman's pubic area which would normally be covered by the bottom part of a swimsuit. In the context of waxing, it is generally understood to describe any pubic hair visible beyond the boundaries of a swimsuit. Visible pubic hair is widely culturally disapproved of and, considered to be embarrassing, is often removed. With the reduction in the size of swimsuits, especially since the advent of the bikini after 1945, the practice of bikini waxing has also come into vogue.
Bikini tan.
Wearing a bikini in the sun results in the uncovered skin becoming sun tanned and creates tan lines. These tan lines separate pale breasts, crotch, and buttocks from otherwise tanned skin. 
Bikinis leave most of the body exposed to potentially dangerous UVB light. Overexposure to UVB radiation can cause sunburn and some forms of skin cancer, among other harmful effects. In humans, prolonged exposure to solar UV radiation may result in acute and chronic health effects on the skin, eye, and immune system. Moreover, UVC radiation can cause adverse effects that can be mutagenic or carcinogenic. As a result, medical organizations recommend that bikini-wearers protect themselves from UV radiation by using sunscreen, which contain ingredients that have been shown to protect mice against skin tumors. However, some sunscreen products can cause harm to the skin. 
Chemical company BASF has incorporated nanotechnology into bikinis for better UV protection as wet clothes have reduced protection against UV light. Made of Day-Glo leopard skin polyamide (nylon)-6 these bikinis have titanium dioxide embedded and provide a variable sunblock factor-80 for the beach and 15 for a spring day.

</doc>
<doc id="4551" url="http://en.wikipedia.org/wiki?curid=4551" title="Babur">
Babur

Zahir-ud-din Muhammad Babur (14 February 148326 December 1530; sometimes also spelt Baber or Babar) was a conqueror from Central Asia who, following a series of setbacks, finally succeeded in laying the basis for the Mughal dynasty in the Indian Subcontinent and became the first Mughal emperor. He was a direct descendant of Timur, from the Barlas clan, through his father, and also a descendant of Genghis Khan through his mother. Culturally, he was greatly influenced by the Persian culture and this affected both his own actions and those of his successors, giving rise to a significant expansion of the Persianate ethos in the Indian subcontinent.
Etymology.
He was born as Ẓahīr-ud-Dīn Muḥammad (), but was more commonly known by his nickname, "Bābur" (بابر). He had the royal titles "Padshah" and "al-ṣultānu 'l-ʿazam wa 'l-ḫāqān al-mukkarram pādshāh-e ġāzī". Ẓahīr-ud-Dīn Muḥammad ("Defender of the faith") was an Arabic name and difficult to pronounce for the Central Asian Turko-Mongols, therefore the name "Babur" was adopted. According to Babur's cousin, Mirzā Muḥammad Haydar:
According to Stephen Frederic Dale, the name "Babur" is derived from the Persian word "babr", meaning "tiger", a word that repeatedly appears in Firdawsī's "Shāhnāma" and had also been borrowed by the Turkic languages of Central Asia. This thesis is supported by the explanation that the Turko-Mongol name "Timur" underwent a similar evolution, from the Sanskrit word "cimara" ("iron") via a modified version "*čimr" to the final Turkicized version "timür", with "-ür" replacing "-r" due to need to provide vocalic support between "m" and "r". The choice of vowel would nominally be restricted to one of the four front vowels ("e", "i", "ö", "ü" per the Ottoman vowel harmony rule), hence "babr" → "babür", although the rule is routinely violated for words of Persian or Arabic derivation.
Contradicting these views, W.M. Thackston argues that the name cannot be taken from "babr" and instead must be derived from a word that has evolved out of the Indo-European word for "beaver", pointing to the fact that the name is pronounced "bāh-bor" in both Persian and Turkic, similar to the Russian word for beaver (бобр – "bobr").
Background.
Babur wrote his memoirs and these form the main source for details of his life. They are known as the "Baburnama" and were written in Chaghatai Turkic, his mother-tongue, though his prose was highly Persianized in its sentence structure, morphology and vocabulary. "Baburnama" was translated in Persian during the rule of Babur's grandson Akbar.
Babur was born on in the city of Andijan, Andijan Province, Fergana Valley, contemporary Uzbekistan. He was the eldest son of Umar Sheikh Mirza, ruler of the Fergana Valley, the son of Abū Saʿīd Mirza (and grandson of Miran Shah, who was himself son of Timur) and his wife Qutlugh Nigar Khanum, daughter of Yunus Khan, the ruler of Moghulistan (and great-great grandson of Tughlugh Timur, the son of Esen Buqa I, who was the great-great-great grandson of Chaghatai Khan, the second born son of Genghis Khan).
Although Babur hailed from the Barlas tribe which was of Mongol origin, his tribe had embraced Turkic and Persian culture, converted to Islam and resided in Turkestan and Khorasan. His mother tongue was the Chaghatai language (known to Babur as "Turkī", "Turkic") and he was equally fluent in Persian, the "lingua franca" of the Timurid elite.
Hence Babur, though nominally a Mongol (or "Moghul" in Persian language), drew much of his support from the local Turkic and Iranian people of Central Asia, and his army was diverse in its ethnic makeup. It included Persians (known to Babur as "Sarts" and "Tajiks"), ethnic Afghans, Arabs, as well as Barlas and Chaghatayid Turco-Mongols from Central Asia. Babur's army also included Qizilbāsh fighters, a militant religious order of Shi'a "Sufis" from Safavid Persia who later became one of the most influential groups in the Mughal court.
Rule in Central Asia.
As ruler of Farghana.
In 1495, at twelve years of age, Babur became the ruler of Farghana, present-day Uzbekistan, after Umar Sheikh Mirza died in a freak accident. During this time, two of his uncles from the neighbouring kingdoms, who were hostile to his father, and a group of nobles who wanted his younger brother Jahangir to be the ruler, threatened his succession to the throne. His uncles were relentless in their attempts to dislodge him from this position as well as many of his other territorial possessions to come. Babur was able to secure his throne partly due to help from his maternal grandmother, Aisan Daulat Begum.
Most territories around his kingdom were ruled by his relatives, who were descendants of either Timur or Genghis Khan, and were constantly in conflict. At that time, rival princes were fighting over the city of Samarkand to the west, which was ruled by his paternal cousin. Babur had a great ambition to capture it and in 1497, he besieged Samarkand for seven months before eventually gaining control over it. He was fifteen years old and for him, this campaign was a huge achievement. Babur was able to hold it despite desertions in his army but later fell seriously ill. Meanwhile, a rebellion amongst nobles who favoured his brother, back home approximately away robbed him of Farghana. As he was marching to recover it, he lost the Samarkand to a rival prince, leaving him with neither Farghana nor Samarkand. He had held Samarkand for 100 days and he considered this defeat as his biggest loss and would obsess over it even later in his life after his conquest of India.
In 1501, he laid siege on Samarkand once more, but was soon defeated by his most formidable rival, Muhammad Shaybani, khan of the Uzbeks. Samarkand, his lifelong obsession, was lost again. He tried to reclaim Farghana but lost it too and escaping with a small band of followers, he wandered to the mountains of central Asia and took refuge with hill tribes. Thus, during the ten years since becoming the ruler of Farghana, Babur suffered many short-lived victories and was without shelter and in exile, aided by friends and peasants. He finally stayed in Tashkent, which was ruled by his maternal uncle. Babur wrote, "During my stay in Tashkent, I endured much poverty and humiliation. No country, or hope of one!" For three years Babur concentrated on building up a strong army, recruiting widely amongst the Tajiks of Badakhshan in particular. By 1502, Babur had resigned all hopes of recovering Farghana, he was left with nothing and was forced to try his luck someplace else.
At Kabul.
Kabul was ruled by Ulugh Begh Mirza of the Arghun Dynasty who died leaving only an infant as heir. Thus, the city was claimed by Mukin Begh, who had a strong opposition from the local populace; they wanted this usurper to be dethroned. In 1504, by using the whole situation to his own advantage, Babur was able to cross the snowy Hindu Kush mountains and capture Kabul; the remaining Arghunids were forced to retreat to Kandahar. With this move, he gained a new kingdom, re-established his fortunes and would remain its ruler until 1526. In 1505, because of the low revenue his new mountain kingdom generated, Babur undertook his first expedition to India and had written before in his memoirs, "My desire for Hindustan had been constant. It was in the month of Shaban, the Sun being in Aquarius, that we rode out of Kabul for Hindustan"; it was a brief raid across the Khyber Pass.
In the same year, Babur united with Sultan Husayn Mirza Bayqarah of Herat, a fellow Timurid and distant relative, against their common enemy, the Uzbek Shaybani. However, this venture did not take place because Husayn Mirza died in 1506 and his two sons were reluctant to go to war. Babur instead stayed at Herat after being invited by the two Mirza brothers. It was then the cultural capital of the eastern Muslim world. Though he was disgusted by the vices and luxuries of the city, he marvelled at the intellectual abundance there, which he stated was "filled with learned and matched men". He became acquainted with the work of the Chagatai poet Mir Ali Shir Nava'i, who encouraged the use of Chagatai as a literary language. Nava'i's proficiency with the language, which he is credited with founding, may have influenced Babur in his decision to use it for his memoirs. He spend two months there before being forced to leave due to diminishing resources; it later was overrun by Shaybani and the Mirzas fled.
Babur became the only reigning ruler of the Timurid dynasty after the loss of Herat, and many princes sought refuge from him at Kabul because of Shaybani's invasion in the west. He thus assumed the title of "Padshah" (emperor) among the Timurids—though this tile was insignificant since most of his ancestral lands were taken, Kabul itself was in danger and Shaybani continued to be a threat. He prevailed during a potential rebellion in Kabul, but two years later a revolt among some of his leading generals drove him out of Kabul. Escaping with very few companions, Babur soon returned to the city, capturing Kabul again and regaining the allegiance of the rebels. Meanwhile, Shaybani was defeated and killed by Ismail I, Shah of Shia Safavid Persia, in 1510.
Babur and the remaining Timurids used this opportunity to reconquer their ancestral territories. Over the following few years, Babur and Shah Ismail would form a partnership in an attempt to take over parts of Central Asia. In return for Ismail's assistance, Babur permitted the Safavids to act as a suzerain over him and his followers. Thus, in 1513, after leaving his brother Nasir Mirza to rule Kabul, he managed to get Samarkand for the third time and Bokhara but lost both again to the Uzbeks. Shah Ismail reunited Babur with his sister Khānzāda, who had been imprisoned by and forced to marry the recently deceased Shaybani. He returned to Kabul after three years in 1514. The following 11 years of his rule mainly involved dealing with relatively insignificant rebellions from Afghan tribes, his nobles and relatives, in addition to conducting raids across the eastern mountains. Babur began to modernise and train his army despite it being, for him, relatively peaceful times.
Foreign relations.
Relations with the Safavids.
Babur's relations with the Safavids began when Ali Mirza Safavi ventured to meet Babur at Samarqand in order to maintain good relations that would last even after the Ottoman's reached out to Babur. The Safavids army led by Najm-e Sani massacred civilians in Central Asia and then sought the assistance of Babur, who advised the Safavids to withdraw. The Safavids however, refused and were defeated during the Battle of Ghazdewan by the warlord Ubaydullah Khan.
Relations with the Ottomans.
Babur's early relations with the Ottomans were poor because the Ottoman Sultan Selim I provided his rival Ubaydullah Khan with powerful matchlocks and cannons. In the year 1507, when ordered to accept Selim I as his rightful suzerain Babur refused, and gathered Qizilbash servicemen in order to counter the forces of Ubaydullah Khan during the Battle of Ghazdewan. In the year 1513, Ottoman Sultan Selim I reconciled with Babur (fearing that he would join the Safavids), dispatched Ustad Ali Quli the artilleryman and Mustafa Rumi, the matchlock marksman, and many other Ottoman Turks, in order to assist Babur in his conquests; this particular assistance proved to be the basis of future Mughal-Ottoman relations. From them, he also adopted the tactic of using matchlocks and cannons in field (rather than only in sieges), which would give him an important advantage in India.
Formation of the Mughal Empire in India.
Babur still wanted to escape from the Uzbeks, and finally chose India as a refuge instead of Badakhshan, which was to the north of Kabul. He wrote, "In the presence of such power and potency, we had to think of some place for ourselves and, at this crisis and in the crack of time there was, put a wider space between us and the strong foeman." After his third loss of Samarkand, Babur gave full attention on conquest of India, launching a campaign, he reached Chenab in 1519. Until 1524, his aim was to only expand his rule to Punjab, mainly to fulfil his ancestor Timur's legacy, since it used to be part of his empire. At the time India was under the rule of Ibrahim Lodi of Lodi dynasty, but the empire was crumbling and there were many defectors. He received invitations from Daulat Khan Lodi, Governor of Punjab and Ala-ud-Din, uncle of Ibrahim. He sent an ambassador to Ibrahim, claiming himself the rightful heir to the throne of the country, however the ambassador was detained at Lahore and released months later.
Babur started for Lahore, Punjab, in 1524 but found that Daulat Khan Lodi had been driven out by forces sent by Ibrahim Lodi. When Babur arrived at Lahore, the Lodi army marched out and was his army was routed. In response, Babur burned Lahore for two days, then marched to Dipalpur, placing Alam Khan, another rebel uncle of Lodi's, as governor. Alam Khan was quickly overthrown and fled to Kabul. In response, Babur supplied Alam Khan with troops who later joined up with Daulat Khan Lodi and together with about 30,000 troops, they besieged Ibrahim Lodi at Delhi. He easily defeated and drove off Alam's army and Babur realized Lodi would not allow him to occupy the Punjab.
First battle of Panipat.
Babur started his campaign in November 1525, when he reached Peshawar he got the news that Daulat Khan Lodi had switched sides and drove out Ala-ud-Din. Babur then marched onto Lahore to confront Daulat Khan Lodi, only to see Daulat's army melt away at their approach. Daulat surrendered and was pardoned, thus within three weeks of crossing the Indus Babur became the master of Punjab.
Babur marched onto Delhi via Sirhind, he reached the historical field of Panipat on 20 April 1526, where he met Ibrahim Lodi along with his numerically superior army of about 100,000 soldiers and 100 elephants. The battle began on morning of 21 April 1526, Babur utilised the tactic of "Tulugma", encircled the Ibrahim Lodi's army and forcing them to face artillery fire directly, and frightening the war elephants utilised by the Delhi's army.
Ibrahim Lodi died during the battle thus ending the Lodi Dynasty.
Babur wrote in his memoirs about his victory :
After the battle Babur occupied Delhi and Agra, seated himself on the throne of Lodi and laid the foundation of the Mughal Rule in India, but it was yet to be established and Babur was yet to become the ruler of India, as new contenders for the throne like, Rana Sanga, who rose to challenge his rule. However, Babur was able to take the fortress of Bayana, after sending the commander, Nizam Khan, a convincing poem in Persian:
Battles with the Rajputs.
Although master of Delhi and Agra, Babur records in his memoirs that he had sleepless nights because of continuing worries over Raja Hasan Khan, "Mewatpatti" (title, Lord of Mewat), the Khanzada ruler of Mewat, Rana Sanga, the Rajput ruler of Mewar.
In AD 1526 a new power appeared in India. Babur, who claimed to be the representative of Timur Lang, after winning the battle of Panipat, took possession of Delhi and Agra, and determined that his enterprise should not be a mere raid like Timur's, but the foundation of a new and
lasting empire. Then it was that the Rajputs made their last great struggle for independence. They were led by Rana Sanga, a chief of Mewar, who invited the Mewatti chief, Hasan Khan, to aid the nation from which he had sprung in resisting the new horde of Musalmans from the north.
The political position of Hasan Khan at this time was a very important one. Babur, in his autobiography, speaks of him as the prime mover in all the confusions and insurrections of the period. He had, he states, vainly shown Hasan Khan distinguished marks of favour, but the affections of the infidel lay all on the side of the natives, the Hindus (Indians), and the propinquity of his country to Delhi made his opposition especially dangerous. Hasan Khan's seat at this time was at Ulwur, but local tradition says that he was originally established at Bahadarpur, eight miles from Ulwur.
Babur says that the ancestors of his opponent Hasan Khan had governed Mewat in uninterrupted succession for nearly 200 years, and that Tejara was their capital. In another place he calls him Raja Hasan Khan Mewati, an infidel, who was the prime mover and agitator in the insurrection against the Mughals. The title of Raja and the term "infidel" show that Babur was aware of Hasan Khan's Hindu descent, and the period of "nearly 200 years" most probably refers to the date when his ancestor became a Muslim in the reign of Firoz Shah between AH 752 and 790.
The Rajput lords had, prior to Babur's intervention, succeeded in conquering some of the Sultanate's territory. They ruled an area directly to the southwest of Babur's new dominions, commonly known as Rajputana as well as fortified dominions in other parts of northern India. It was not a unified kingdom, but rather a confederacy of principalities, under the informal suzerainty of Rana Sanga, head of the senior Rajput dynasty.
The Rajputs had possibly heard word of the heavy casualties inflicted by Lodi on Babur's forces, and believed that they could capture Delhi, and possibly all Hindustan. They hoped to bring it back into Hindu Rajput hands for the first time in almost three hundred and fifty years since Sultan Shah-al Din Muhammad of Ghor defeated the Rajput Chauhan King Prithviraj III in 1192.
Furthermore, the Rajputs were well aware that there was dissent within the ranks of Babur's army. The hot Indian summer was upon them, and many troops wanted to return home to the cooler climes of Central Asia. The Rajputs' reputation for courage preceded them, and their superior numbers no doubt further contributed to the desire of Babur's army to retreat. According to Babur's own calculations the potential strength of the Rajput army was much larger than that deployed by the Lodis at Panipat. Babur resolved to make this an extended battle, and decided to push further into India, into lands never previously claimed by the Timurids. He needed his troops to defeat the Rajputs.
Despite the unwillingness of his troops to engage in further warfare, Babur was convinced he could overcome the Rajputs and gain complete control over Hindustan. He made great propaganda of the fact that for the first time he was to battle non-Muslims, the "Kafir", to the extent of taking a vow to abstain from drinking (a common fraction among his people) for the rest of his life to win divine favour, and declared the war against Rana Sanga.
Personal life and relationships.
There no descriptions about Babur's physical appearance, except the paintings from his memoirs which were made during the reign of his grandson Akbar, when he got it translated. Babur claimed to be strong and physically fit, saying to have swam across every major river he encountered, including twice across the Ganges River in North India. Unlike his father, he had ascetic tendencies and did not have any great interest in women. In his first marriage, he was "bashful" towards Aisha Sultan Begum later losing his affection for her. However, he acquired several more wives and concubines over the years, and as required for a prince, he was able to ensure the continuity of his line; Babur treated them and his other women relatives well. In his memoirs, there is a mention of his infatuation for a younger boy when Babur was 16 years old. According to Abraham Eraly, bisexuality and pederasty were common at that time among central Asian rulers.
Babur's first wife, Aisha Sultan Begum, was his cousin and daughter of Sultan Ahmad Mirza. She was betrothed to Babur when he was five years old and they married after eleven years. He had one daughter with her, Fakhr-un-Nissa, who died as an infant within a year in 1500 AD. Three years later and after his first defeat at Farghana, she left Babur. Babur then married Zainad Sultan Begum in 1504 and Mahim Begum in 1506. A year later, he married Maasuma Sultana Begum, Gulrukh Begum and Dildar Begum. Zainad died childless within two years. Babur had four children with Mahim, among which only Humayun survived. Maasuma died during childbirth—the year is disputed from 1508 to 1519. With Gulrukh, Babur had two sons, Kamran and Askari, and with Dildar Begum, Hindal. Babur later married Mubarika Yousefzai, who was an Afghan woman of the Yousefzai tribe. Gulnar Aghacha and Nargul Aghacha were two Circassian slaves given as gifts by Tahmasp Shah Safavi of Persia. They became "recognized ladies of the royal household."
During his rule in Kabul, when there was a relative time of peace, Babur pursued his interests in literature, art, music and gardening. Previously, he never drank alcohol and avoided it when he was in Herat. In Kabul, he first tasted it at the age of thirty. He then began to drink regularly, host wine parties and consume preparations made from opium. Though religion had a central place in his life, Babur also approvingly quoted a line of poetry by one of his contemporaries: "I am drunk, officer. Punish me when I am sober". He quit drinking for health reasons before the Battle of Khanwa, just two years before his death, and demanded that his court do the same. But he did not stop chewing narcotic preparations, and did not lose his sense of irony. He wrote, "Everyone regrets drinking and swears an oath (of abstinence); I swore the oath and regret that."
Death and legacy.
After Babur fell seriously ill, Humayun, his eldest son, was summoned from his "Jagir". He died at the age of 47 on , and was succeeded by Humayun. In accordance with his will, his body was moved to Kabul, Afghanistan there it lies in Bagh-e Babur (Babur Gardens).
It is generally agreed that, as a Timurid, Babur was not only significantly influenced by the Persian culture, but that his empire also gave rise to the expansion of the Persianate ethos in the Indian subcontinent.
For example, F. Lehmann states in the Encyclopædia Iranica: 
Since he was a patrilineal descendant of Timur, Babur regarded himself a Timurid and a Turk. Some sources therefore claim that Babur's empire was Turkic in nature. But, commenting on Babur's remarks about his father Omar Sheykh Mirzā enjoying the Persian poetry of Nezami, Rumi and especially the Shahnama of Ferdowsi, Svat Soucek points out that:
Although all applications of modern Central Asian ethnicities to people of Babur's time are anachronistic, Soviet and Uzbek sources regard Babur as an ethnic Uzbek. At the same time, during the Soviet Union Uzbek scholars were censored for idealizing and praising Babur and other historical figures such as Ali-Shir Nava'i.
Babur is considered a national hero in Uzbekistan. Many of Babur's poems have become popular Uzbek folk songs, especially by Sherali Jo‘rayev. Some sources claim that Babur is a national hero in Kyrgyzstan too. Babur is also held in high esteem in Afghanistan and Iran. In October 2005, Pakistan developed the Babur Cruise Missile, named in his honor.
Babri mosque.
Babur is popularly believed to have demolished the Rama Temple at Ayodhya and built Babri Mosque in Ayodhya, India. On 6 December 1992, Babri Masjid was demolished by Karsevaks of Ramajanmabhumi movement mobilised by the call given by organisations like VHP and Bajrang Dal. L K Advani of BJP party was a leading figure of the movement, along with several other leaders of Hindu organisations. Its destruction sparked off communal clashes around the country. resulting in the killing of thousands of Muslims and Hindus. However, from the three inscriptions which once adorned the surface of the mosque it becomes apparent that the mosque was constructed during his reign on the orders of Mir Baqi, who was one of the generals of Babur's forces sent towards this region. In 2003, The Archaeological Survey of India was asked to conduct a more in-depth study and an excavation to ascertain the type of structure that was beneath the rubble of Babri masjid. The summary of the ASI report indicated "no mention of a temple, only of evidence of a massive structure, fragments of which speak about their association with temple architecture of the Saivite style."

</doc>
<doc id="4552" url="http://en.wikipedia.org/wiki?curid=4552" title="Bernard of Clairvaux">
Bernard of Clairvaux

Bernard of Clairvaux (Latin: "Bernardus Claraevallensis"), O.Cist (1090 – August 20, 1153) was a French abbot and the primary builder of the reforming Cistercian order.
After the death of his mother, Bernard sought admission into the Cistercian order. "Three years later, he was sent to found a new abbey at an isolated clearing in a glen known as the "Val d'Absinthe", about 15 km southeast of Bar-sur-Aube. According to tradition, Bernard founded the monastery on 25 June 1115, naming it "Claire Vallée", which evolved into "Clairvaux." There Bernard would preach an immediate faith, in which the intercessor was the Virgin Mary." In the year 1128, Bernard assisted at the Council of Troyes, at which he traced the outlines of the Rule of the Knights Templar, who soon became the ideal of Christian nobility.
On the death of Pope Honorius II on 13 February 1130, a schism broke out in the Church. Louis VI of France convened a national council of the French bishops at Étampes in 1130, and Bernard was chosen to judge between the rivals for pope. After the council of Étampes, Bernard went to speak with the King of England, Henry I, also known as Henry Beauclerc, about the king's reservations regarding Pope Innocent II. Beauclerc was sceptical because most of the bishops of England supported Anacletus II; Bernard convinced him to support Innocent. Germany had decided to support Innocent through Norbert of Xanten, who was a friend of Bernard's. However, Innocent insisted on Bernard's company when he met with Lothair III of Germany. Lothair became Innocent's strongest ally among the nobility. Despite the councils of Étampes, Wurzburg, Clermont, and Rheims all supporting Innocent, there were still large portions of the Christian world supporting Anacletus. At the end of 1131, the kingdoms of France, England, Germany, Castile, and Aragon supported Innocent; however, most of Italy, southern France, and Sicily, with the patriarchs of Constantinople, Antioch, and Jerusalem, supported Anacletus. Bernard set out to convince these other regions to rally behind Innocent. The first person whom he went to was Gerard of Angoulême. He proceeded to write a letter known as Letter 126, which questioned Gerard's reasons for supporting Anacletus. Bernard would later comment that Gerard was his most formidable opponent during the whole schism. After convincing Gerard, Bernard traveled to visit the Count of Poitiers. He was the hardest for Bernard to convince. He did not pledge allegiance to Innocent until 1135. After that, Bernard spent most of his time in Italy convincing the Italians to pledge allegiance to Innocent. He traveled to Sicily in 1137 to convince the king of Sicily to follow Innocent. The whole conflict ended when Anacletus died on 25 January 1138. In 1139, Bernard assisted at the Second Council of the Lateran.
Bernard denounced the teachings of Peter Abelard to the pope, who called a council at Sens in 1141 to settle the matter. Bernard soon saw one of his disciples elected as Pope Eugenius III. Having previously helped end the schism within the church, Bernard was now called upon to combat heresy. In June 1145, Bernard traveled in southern France and his preaching there helped strengthen support against heresy.
Following the Christian defeat at the Siege of Edessa, the pope commissioned Bernard to preach the Second Crusade. The last years of Bernard's life were saddened by the failure of the crusaders, the entire responsibility for which was thrown upon him. Bernard died at age 63, after 40 years spent in the cloister. He was the first Cistercian placed on the calendar of saints, and was canonized by Pope Alexander III on 18 January 1174. In 1830 Pope Pius VIII bestowed upon Bernard the title "Doctor of the Church".
Early life (1090–1113).
Bernard's parents were Tescelin, Lord of Fontaines, and Aleth of Montbard, both belonging to the highest nobility of Burgundy. Bernard was the third of a family of seven children, six of whom were sons. At the age of nine years, he was sent to school at Châtillon-sur-Seine, run by the secular canons of Saint-Vorles. Bernard had a great taste for literature and devoted himself for some time to poetry. His success in his studies won the admiration of his teachers. He wanted to excel in literature in order to take up the study of the Bible. He had a special devotion to the Virgin Mary, and he would later write several works about the Queen of Heaven.
Bernard would expand upon Anselm of Canterbury's role in transmuting the sacramentally ritual Christianity of the Early Middle Ages into a new, more personally held faith, with the life of Christ as a model and a new emphasis on the Virgin Mary. In opposition to the rational approach to divine understanding that the scholastics adopted, Bernard would preach an immediate faith, in which the intercessor was the Virgin Mary.
Bernard was only nineteen years of age when his mother died. During his youth, he did not escape trying temptations and around this time he thought of retiring from the world and living a life of solitude and prayer.
In 1098 Saint Robert of Molesme had founded Cîteaux Abbey, near Dijon, with the purpose of restoring the Rule of St Benedict in all its rigour. Returning to Molesme, he left the government of the new abbey to Saint Alberic, who died in the year 1109. In 1113 Saint Stephen Harding had just succeeded him as third Abbot of Cîteaux when Bernard and thirty other young noblemen of Burgundy sought admission into the Cistercian order.
Abbot of Clairvaux (1115–28).
The little community of reformed Benedictines at Cîteaux, which would have so profound an influence on Western monasticism, grew rapidly. Three years later, Bernard was sent with a band of twelve monks to found a new house at Vallée d'Absinthe, in the Diocese of Langres. This Bernard named Claire Vallée, or Clairvaux, on 25 June 1115, and the names of Bernard and Clairvaux would soon become inseparable. During the absence of the Bishop of Langres, Bernard was blessed as abbot by William of Champeaux, Bishop of Châlons-sur-Marne. From that moment a strong friendship sprang up between the abbot and the bishop, who was professor of theology at Notre Dame of Paris, and the founder of the Abbey of St. Victor.
The beginnings of Clairvaux Abbey were trying and painful. The regime was so austere that Bernard became ill, and only the influence of his friend William of Champeaux and the authority of the general chapter could make him mitigate the austerities. The monastery, however, made rapid progress. Disciples flocked to it in great numbers and put themselves under the direction of Bernard. His father and all his brothers entered Clairvaux to pursue religious life, leaving only Humbeline, his sister, in the secular world. She, with the consent of her husband, soon took the veil in the Benedictine nunnery of Jully-les-Nonnains. Gerard of Clairvaux, Bernard's older brother, became the cellarer of Citeaux. The abbey became too small for its members and it was necessary to send out bands to found new houses. In 1118 Trois-Fontaines Abbey was founded in the diocese of Châlons; in 1119 Fontenay Abbey in the Diocese of Autun; and in 1121 Foigny Abbey near Vervins, in the diocese of Laon. In addition to these victories, Bernard also had his trials. During an absence from Clairvaux, the Grand Prior of Cluny went to Clairvaux and enticed away Bernard's cousin, Robert of Châtillon. This was the occasion of the longest and most emotional of Bernard's letters.
In the year 1119, Bernard was present at the first general chapter of the order convoked by Stephen of Cîteaux. Though not yet 30 years old, Bernard was listened to with the greatest attention and respect, especially when he developed his thoughts upon the revival of the primitive spirit of regularity and fervour in all the monastic orders. It was this general chapter that gave definitive form to the constitutions of the order and the regulations of the "Charter of Charity" which Pope Callixtus II confirmed 23 December 1119. In 1120, Bernard authored his first work, "De Gradibus Superbiae et Humilitatis", and his homilies which he entitled "De Laudibus Mariae." The monks of the abbey of Cluny were unhappy to see Cîteaux take the lead role among the religious orders of the Roman Catholic Church. For this reason, the Black Monks attempted to make it appear that the rules of the new order were impracticable. At the solicitation of William of St. Thierry, Bernard defended the order by publishing his "Apology" which was divided into two parts. In the first part, he proved himself innocent of the charges of Cluny and in the second he gave his reasons for his counterattacks. He protested his profound esteem for the Benedictines of Cluny whom he declared he loved equally as well as the other religious orders. Peter the Venerable, abbot of Cluny, answered Bernard and assured him of his great admiration and sincere friendship. In the meantime Cluny established a reform, and Abbot Suger, the minister of Louis VI of France, was converted by the "Apology" of Bernard. He hastened to terminate his worldly life and restore discipline in his monastery. The zeal of Bernard extended to the bishops, the clergy, and lay people. Bernard's letter to the archbishop of Sens was seen as a real treatise, "De Officiis Episcoporum." About the same time he wrote his work on "Grace and Free Will".
Doctor of the Church (1128–46).
In the year 1128 AD, Bernard participated in the Council of Troyes, which had been convoked by Pope Honorius II, and was presided over by Cardinal Matthew, Bishop of Albano. The purpose of this council was to settle certain disputes of the bishops of Paris, and regulate other matters of the Church of France. The bishops made Bernard secretary of the council, and charged him with drawing up the synodal statutes. After the council, the bishop of Verdun was deposed. It was at this council that Bernard traced the outlines of the Rule of the Knights Templar who soon became the ideal of Christian nobility. Around this time, he praised them in his "Liber ad milites templi de laude novae militiae".
Again reproaches arose against Bernard and he was denounced, even in Rome. He was accused of being a monk who meddled with matters that did not concern him. Cardinal Harmeric, on behalf of the pope, wrote Bernard a sharp letter of remonstrance stating, "It is not fitting that noisy and troublesome frogs should come out of their marshes to trouble the Holy See and the cardinals."
Bernard answered the letter by saying that, if he had assisted at the council, it was because he had been dragged to it by force. In his response Bernard wrote,
This letter made a positive impression on Harmeric, and in the Vatican.
Schism.
Bernard's influence was soon felt in provincial affairs. He defended the rights of the Church against the encroachments of kings and princes, and recalled to their duty Henri Sanglier, archbishop of Sens and Stephen of Senlis, bishop of Paris. On the death of Pope Honorius II, which occurred on 14 February 1130, a schism broke out in the Church by the election of two popes, Pope Innocent II and Pope Anacletus II. Innocent II, having been banished from Rome by Anacletus, took refuge in France. King Louis VI convened a national council of the French bishops at Étampes, and Bernard, summoned there by consent of the bishops, was chosen to judge between the rival popes. He decided in favour of Innocent II. This caused the pope to be recognized by all the great powers. He then went with him into Italy and reconciled Pisa with Genoa, and Milan with the pope. The same year Bernard was again at the Council of Reims at the side of Innocent II. He then went to Aquitaine where he succeeded for the time in detaching William X of Aquitaine, Count of Poitiers, from the cause of Anacletus.
In 1132, Bernard accompanied Innocent II into Italy, and at Cluny the pope abolished the dues which Clairvaux used to pay to that abbey. This action gave rise to a quarrel between the White Monks and the Black Monks which lasted 20 years. In May of that year, the pope, supported by the army of Emperor Lothair III, entered Rome, but Lothair, feeling himself too weak to resist the partisans of Anacletus, retired beyond the Alps, and Innocent sought refuge in Pisa in September 1133. Bernard had returned to France in June and was continuing the work of peacemaking which he had commenced in 1130. Towards the end of 1134, he made a second journey into Aquitaine, where William X had relapsed into schism. Bernard invited William to the Mass which he celebrated in the Church of La Couldre. At the Eucharist, he "admonished the Duke not to despise God as he did His servants". William yielded and the schism ended. Bernard went again to Italy, where Roger II of Sicily was endeavouring to withdraw the Pisans from their allegiance to Innocent. He recalled the city of Milan to obedience to the pope as they had followed the deposed Anselm V, Archbishop of Milan. For this, he was offered, and he refused, the archbishopric of Milan. He then returned to Clairvaux. Believing himself at last secure in his cloister, Bernard devoted himself with renewed vigour to the composition of the works which would win for him the title of "Doctor of the Church". He wrote at this time his sermons on the "Song of Songs." In 1137, he was again forced to leave his solitude by order of the pope to put an end to the quarrel between Lothair and Roger of Sicily. At the conference held at Palermo, Bernard succeeded in convincing Roger of the rights of Innocent II. He also silenced the final supporters who sustained the schism. Anacletus died of "grief and disappointment" in 1138, and with him the schism ended.
In 1139, Bernard assisted at the Second Council of the Lateran, in which the surviving adherents of the schism were definitively condemned. About the same time, Bernard was visited at Clairvaux by Saint Malachy, Primate of All Ireland, and a very close friendship formed between them. Malachy wanted to become a Cistercian, but the pope would not give his permission. Malachy would die at Clairvaux in 1148.
Contest with Abelard.
Towards the close of the 11th century, a spirit of independence flourished within schools of philosophy and theology. This led for a time to the exaltation of human reason and rationalism. The movement found an ardent and powerful advocate in Peter Abelard. Abelard's treatise on the Trinity had been condemned as heretical in 1121, and he was compelled to throw his own book into the fire. However, Abelard continued to develop his teachings, which were controversial in some quarters. Bernard, informed of this by William of St-Thierry, is said to have held a meeting with Abelard intending to persuade him to amend his writings, during which Abelard repented and promised to do so. But once out of Bernard's presence, he reneged. Bernard then denounced Abelard to the pope and cardinals of the Curia. Abelard sought a debate with Bernard, but Bernard initially declined, saying he did not feel matters of such importance should be settled by logical analyses. Bernard's letters to William of St-Thierry also express his apprehension about confronting the preeminent logician. Abelard continued to press for a public debate, and made his challenge widely known, making it hard for Bernard to decline. In 1141, at the urgings of Abelard, the archbishop of Sens called a council of bishops, where Abelard and Bernard were to put their respective cases so Abelard would have a chance to clear his name. Bernard lobbied the prelates on the evening before the debate, swaying many of them to his view. The next day, after Bernard made his opening statement, Abelard decided to retire without attempting to answer. The council found in favour of Bernard and their judgment was confirmed by the pope. Abelard submitted without resistance, and he retired to Cluny to live under the protection of Peter the Venerable, where he died two years later.
Cistercian Order and Heresy.
Bernard had occupied himself in sending bands of monks from his overcrowded monastery into Germany, Sweden, England, Ireland, Portugal, Switzerland, and Italy. Some of these, at the command of Innocent II, took possession of Trois-Fontaines Abbey, from which Pope Eugenius III would be chosen in 1145. Pope Innocent II died in the year 1143. His two successors, Pope Celestine II and Pope Lucius II, reigned only a short time, and then Bernard saw one of his disciples, Bernard of Pisa, and known thereafter as Eugenius III, raised to the Chair of Saint Peter. Bernard sent him, at the pope's own request, various instructions which comprise the "Book of Considerations," the predominating idea of which is that the reformation of the Church ought to commence with the sanctity of the pope. Temporal matters are merely accessories; the principles according to Bernard's work were that piety and meditation were to precede action.
Having previously helped end the schism within the Church, Bernard was now called upon to combat heresy. Henry of Lausanne, a former Cluniac monk, had adopted the teachings of the Petrobrusians, followers of Peter of Bruys and spread them in a modified form after Peter's death. Henry of Lausanne's followers became known as Henricians. In June 1145, at the invitation of Cardinal Alberic of Ostia, Bernard traveled in southern France. His preaching, aided by his ascetic looks and simple attire, helped doom the new sects. Both the Henrician and the Petrobrusian faiths began to die out by the end of that year. Soon afterwards, Henry of Lausanne was arrested, brought before the bishop of Toulouse, and probably imprisoned for life. In a letter to the people of Toulouse, undoubtedly written at the end of 1146, Bernard calls upon them to extirpate the last remnants of the heresy. He also preached against the Cathars.
Second Crusade (1146–49).
News came at this time from the Holy Land that alarmed Christendom. Christians had been defeated at the Siege of Edessa and most of the county had fallen into the hands of the Seljuk Turks. The Kingdom of Jerusalem and the other Crusader states were threatened with similar disaster. Deputations of the bishops of Armenia solicited aid from the pope, and the King of France also sent ambassadors. The pope commissioned Bernard to preach a Second Crusade and granted the same indulgences for it which Pope Urban II had accorded to the First Crusade.
There was at first virtually no popular enthusiasm for the crusade as there had been in 1095. Bernard found it expedient to dwell upon the taking of the cross as a potent means of gaining absolution for sin and attaining grace. On 31 March, with King Louis present, he preached to an enormous crowd in a field at Vézelay.
James Meeker Ludlow describes the scene in as follows:
 A large platform was erected on a hill outside the city. King and monk stood together, representing the combined will of earth and heaven. The enthusiasm of the assembly of Clermont in 1095, when Peter the Hermit and Urban II launched the first crusade, was matched by the holy fervor inspired by Bernard as he cried, "O ye who listen to me! Hasten to appease the anger of heaven, but no longer implore its goodness by vain complaints. Clothe yourselves in sackcloth, but also cover yourselves with your impenetrable bucklers. The din of arms, the danger, the labors, the fatigues of war, are the penances that God now imposes upon you. Hasten then to expiate your sins by victories over the Infidels, and let the deliverance of the holy places be the reward of your repentance." As in the olden scene, the cry ""Deus vult"! "Deus vult"!" rolled over the fields, and was echoed by the voice of the orator: "Cursed be he who does not stain his sword with blood."
When Bernard was finished the crowd enlisted en masse; they supposedly ran out of cloth to make crosses. Bernard is said to have given his own outer garments to be cut up to make more. Unlike the First Crusade, the new venture attracted royalty, such as Eleanor of Aquitaine, then Queen of France; Thierry of Alsace, Count of Flanders; Henry, the future Count of Champagne; Louis’ brother Robert I of Dreux; Alphonse I of Toulouse; William II of Nevers; William de Warenne, 3rd Earl of Surrey; Hugh VII of Lusignan; and numerous other nobles and bishops. But an even greater show of support came from the common people. Bernard wrote to the pope a few days afterwards, "Cities and castles are now empty. There is not left one man to seven women, and everywhere there are widows to still-living husbands."
Bernard then passed into Germany, and the reported miracles which multiplied almost at his every step undoubtedly contributed to the success of his mission. Conrad III of Germany and his nephew Frederick Barbarossa, received the cross from the hand of Bernard. Pope Eugenius came in person to France to encourage the enterprise. As in the First Crusade, the preaching inadvertently led to attacks on Jews; a fanatical French monk named Radulphe was apparently inspiring massacres of Jews in the Rhineland, Cologne, Mainz, Worms, and Speyer, with Radulphe claiming Jews were not contributing financially to the rescue of the Holy Land. The archbishop of Cologne and the archbishop of Mainz were vehemently opposed to these attacks and asked Bernard to denounce them. This he did, but when the campaign continued, Bernard traveled from Flanders to Germany to deal with the problems in person. He then found Radulphe in Mainz and was able to silence him, returning him to his monastery.
The last years of Bernard's life were saddened by the failure of the Second Crusade he had preached, the entire responsibility for which was thrown upon him. Bernard considered it his duty to send an apology to the Pope and it is inserted in the second part of his "Book of Considerations." There he explains how the sins of the crusaders were the cause of their misfortune and failures. When his attempt to call a new crusade failed, he tried to disassociate himself from the fiasco of the Second Crusade altogether.
Final years (1149–53).
The death of his contemporaries served as a warning to Bernard of his own approaching end. The first to die was Suger in 1152, of whom Bernard wrote to Eugenius III, "If there is any precious vase adorning the palace of the King of Kings it is the soul of the venerable Suger". Conrad III and his son Henry died the same year. From the beginning of the year 1153, Bernard felt his death approaching. The passing of Pope Eugenius had struck the fatal blow by taking from him one whom he considered his greatest friend and consoler. Bernard died at age sixty-three on 20 August 1153, after forty years spent in the cloister. He was buried at the Clairvaux Abbey, but after its dissolution in 1792 by the French revolutionary government, his remains were transferred to the Troyes Cathedral.
Theology.
St. Bernard of Clairvaux was named a Doctor of the Church in 1830. At the 800th anniversary of his death, Pope Pius XII issued an encyclical on Bernard, Doctor Mellifluus, in which he labeled him "The Last of the Fathers." Bernard did not reject human philosophy which is genuine philosophy, which leads to God; he differentiates between different kinds of knowledge, the highest being theological. Three central elements of Bernard's Mariology are how he explained the virginity of Mary, "the "Star of the Sea"", how the faithful should pray on the Virgin Mary, and how he relied on the Virgin Mary as Mediatrix.
Bernard, like Thomas Aquinas, denied the doctrine of the Immaculate Conception of Mary. Calvin quotes Bernard several times in support of the doctrine of "Sola Fide", which Luther described as the article upon which the church stands or falls. Calvin also quotes him in setting forth his doctrine of a forensic alien righteousness, or as it is commonly called imputed righteousness.
Spirituality.
Bernard was instrumental in re-emphasizing the importance of Lectio Divina and contemplation on Scripture within the Cistercian order. Bernard had observed that when Lectio Divina was neglected monasticism suffered. Bernard considered Lectio Divina and contemplation guided by the Holy Spirit the keys to nourishing Christian spirituality.
Works.
The works of Bernard include the following:
His sermons are also numerous:
Many letters, treatises, and other works, falsely attributed to him survive, and are now referred to as works by pseudo-Bernard. These include:
Legacy.
Bernard's theology and Mariology continue to be of major importance, particularly within the Cistercian and Trappist orders. Bernard led to the foundation of 163 monasteries in different parts of Europe. At his death, they numbered 343. His influence led Pope Alexander III to launch reforms that would lead to the establishment of canon law. He was the first Cistercian monk placed on the calendar of saints and was canonized by Pope Alexander III 18 January 1174. Pope Pius VIII bestowed on him the title of Doctor of the Church. He is fondly remembered as the "Mellifluous Doctor" (the Honey-Sweet(-voiced) Doctor) for his eloquence. The Cistercians honour him as only the founders of orders are honoured, because of the widespread activity which he gave to the order.
Saint Bernard's Prayer to the Shoulder Wound of Jesus is often published in Catholic prayer books.
Dante Alighieri's "Divine Comedy" places him as the last guide for Dante, as he travels through the Empyrean ("Paradiso", cantos XXXI–XXXIII). Dante's choice appears to be based on Bernard's contemplative mysticism, his devotion to Mary, and his reputation for eloquence.
He is also the attributed author of the poems often translated in English hymnals as "O Sacred Head, Now Wounded" and "Jesus the Very Thought of Thee".

</doc>
<doc id="4554" url="http://en.wikipedia.org/wiki?curid=4554" title="Bishkek">
Bishkek

Bishkek (in Kyrgyz and Russian: Бишкéк), formerly Pishpek and Frunze, is the capital and the largest city of the Kyrgyz Republic.
Bishkek is also the administrative centre of Chuy Province which surrounds the city, even though the city itself is not part of the province but rather a province-level unit of Kyrgyzstan.
According to the post-Soviet ideology, the name is thought to derive from a Kyrgyz word for a churn used to make fermented mare's milk (kumis), the Kyrgyz national drink, which is rather debatable.
Founded in 1825 as a Khokand fortress of "Pishpek" to control local caravan routes and to get tribute from Kyrgyz tribes, on 4 September 1860 the fortress was destroyed by Russian forces led by colonel Zimmermann, with approval of the Kyrgyz. 
In 1868 a Russian settlement was founded on the fortress's spot, adopting its original name - Pishpek, within the General Governorship of Russian Turkestan and its Semirechye Oblast.
In 1925 the Kara-Kirghiz Autonomous Oblast was created in Russian Turkestan, promoting Pishpek as its capital. In 1926 the city was given the name "Frunze", after the Bolshevik military leader Mikhail Frunze, who was born here. In 1936 the city of Frunze became the capital of the Kirghiz Soviet Socialist Republic during the final stages of the national delimitation in the Soviet Union.
In 1991, the Kyrgyz parliament changed the capital's name to Bishkek (without quorum though).
Bishkek is situated at about altitude just off the northern fringe of the Kyrgyz Ala-Too range, an extension of the Tian Shan mountain range, which rises up to and provides a spectacular backdrop to the city. North of the city, a fertile and gently undulating steppe extends far north into neighbouring Kazakhstan. The Chui River drains most of the area. Bishkek is connected to the Turkestan-Siberia Railway by a spur line.
Bishkek is a city of wide boulevards and marble-faced public buildings combined with numerous Soviet-style apartment blocks surrounding interior courtyards and, especially outside the city centre, thousands of smaller privately built houses. It is laid out on a grid pattern, with most streets flanked on both sides by narrow irrigation channels that water the innumerable trees which provide shade in the hot summers.
History.
Kokhand Rule.
Originally a caravan rest stop (possibly founded by the Sogdians) on one of the branches of the Silk Road through the Tian Shan range, the location was fortified in 1825 by the Uzbek khan of Kokhand with a mud fort. In the last years of Kokhand rule the fortress was led by Atabek, the Datka.
Tsarist Era.
In 1860, the fort was conquered and razed by the military forces of colonel Zimmermann when Tsarist Russia annexed the area. Colonel Zimmerman rebuilt the town over the destroyed fort and put field poruchik Titov as a head of new Russian . The site was redeveloped from 1877 onward by the Russian government, which encouraged the settlement of Russian peasants by giving them fertile land to develop.
Soviet Era.
In 1926, the city became the capital of the newly established Kirghiz ASSR and was renamed "Frunze" after Mikhail Frunze, Lenin's close associate who was born in Bishkek and played key roles during the revolutions of 1905 and 1917 and during the Russian civil war of the early 1920s.
Independence Era.
The early 1990s were tumultuous. In June 1990, a state of emergency was declared following severe ethnic riots in southern Kyrgyzstan which threatened to spread to the capital. The city was renamed Bishkek on 5 February 1991 and Kyrgyzstan achieved independence later that year during the breakup of the Soviet Union. Before independence, the majority of Bishkek's population was ethnic Russians. In 2004, Russians made up approximately 20% of the city's population, and about 7–8% in 2011.
Today, Bishkek is a modern city, with many restaurants and cafes and with many second-hand European and Japanese cars and minibuses crowding its streets. Streets and sidewalks have fallen into disrepair since the 1990s. At the same time Bishkek still preserves its former Soviet feel, with Soviet-period buildings and gardens prevailing over newer structures.
Bishkek is also the country's financial centre, with all of the country's 21 commercial banks keeping headquarters in the city. During the Soviet era, the city was home to a large number of industrial plants, but most have been shut down since 1991 or operate today on a much reduced scale. One of Bishkek's largest employment centres today is Dordoy Bazaar open market, which is one of the major sale place of Chinese goods imported to CIS countries.
Geography.
Orientation.
Though the city is relatively young, the surrounding area has some sites of interest dating from prehistory, the Greco-Buddhist period, the period of Nestorian influence, the era of the Central Asian "khanates", and the Soviet period.
The central part of the city is primarily built on a rectangular grid plan. The city's main street is the east–west Chui Avenue (Chuy Prospekti), named after the region's main river. In the Soviet era, it was called Lenin Avenue. Along, or within a block or two from it, many of the most important government buildings, universities, the Academy of Sciences compound, and so on, are to be found. The westernmost section of the avenue is known as Deng Xiaoping Avenue.
The main north–south axis is Yusup Abdrakhmanov Street, still commonly referred to by its old name, "Sovietskaya Street". Its northern and southern sections are called, respectively, Yelebesov and Baityk Batyr Streets. Several major shopping centres are located along it, and in the north it provides access to Dordoy Bazaar.
Erkindik ("Freedom") Boulevard runs from north to south, from the main railroad station (Bishkek II) south of Chui Avenue to the museum quarter and sculpture park just north of Chui Avenue, and further north toward the Ministry of Foreign Affairs. In the past, it was called Dzerzhinsky Boulevard—named after a Communist revolutionary, Felix Dzerzhinsky—and its northern continuation is still called Dzerzhinsky Street.
An important east–west street is Jibek Jolu ('Silk Road'). It runs parallel to Chui Avenue about a mile north of it, and is part of the main east–west road of Chui Province. Both the Eastern and Western bus terminals are located along Jibek Jolu.
There is a Roman Catholic church located at ul. Vasiljeva 197 (near Rynok Bayat). This is the only Catholic Cathedral in Kyrgyzstan.
Outer neighbourhoods.
The Dordoy Bazaar, just inside the bypass highway on the north-eastern edge of the city, is a major retail and wholesale market.
Outside the city.
The Kyrgyz Ala-Too mountain range, some away, provides a spectacular backdrop to the city; the Ala Archa National Park is only a 30 to 45 minutes drive away.
Climate.
Bishkek has a humid continental climate (Köppen climate classification "") averaging 322 clear days annually due to its mountainous location. Average precipitation is around per year. Average daily high temperatures range from in January to about during July. The summer months are dominated by dry periods experiencing the occasional thunderstorm which produces strong gusty winds and rare dust storms. The mountains to the south provide a natural boundary to provide protection from much of the damaging weather along with the smaller chain which runs NW to SE. In the winter months, sparse snow storms and frequent heavy fog are the dominating features. When an inversion sets up, the fog can last for days at a time.
Demographics.
Bishkek is the most populated city in Kyrgyzstan. Its population, according to the Population and Housing Census of 2012, was 874,400.
Ecology and environment.
Air quality.
Emissions of air pollutants in Bishkek amounted to 14,400 tons in 2010. Of all other cities of Kyrgyzstan, the level of air pollution in Bishkek is the highest with occasional exceeding maximum allowable concentrations by several times, especially in the city's center. For example, occasionally formaldehyde concentrations exceed maximum allowable ones by four times.
Responsibility for ambient air quality monitoring in Bishkek lies on Kyrgyz State Agency on Hydrometeorology. In total, there are seven air quality monitoring stations in Bishkek, measuring levels of sulfur dioxide, nitrogen oxides, formaldehyde, and ammonia.
Economy.
Bishkek uses the Kyrgyzstan currency, the som. The Som's value fluctuates regularly, but averages around 47 som per U.S. Dollar as of March 2011. The economy in Bishkek is primarily agricultural with the mass amounts of fruits, vegetables and livestock providing a co-existing system of bartering in the outlying regions. The streets of Bishkek are regularly lined with produce vendors in a market style venue. In the major portions of downtown there is a more urban cityscape with banks, stores, markets and malls. The most sought after of the goods are the prevalent hand-crafted artisan pieces; these include statues, carvings, paintings and many nature based sculptures.
Government.
Local government is administered by the Bishkek Mayor's Office. Askarbek Salymbekov was mayor until his resignation in August 2005, following which his deputy Arstanbek Nogoev took over the mayorship. Nogoev was in turn removed from his position in October 2007 through a decree of President Kurmanbek Bakiyev and replaced by businessman and former first deputy prime minister Daniar Usenov. In July 2008 former head of the Kyrgyz Railways Nariman Tuleyev was appointed mayor, who was dismissed by the interim government after 7 April 2010. Now interim mayor is Isa Omurkulov, also a former head of the Kyrgyz Railways.
Sports.
Bishkek is home to Spartak, the biggest football stadium in Kyrgyzstan and the only one eligible to host international matches. Several Bishkek-based football teams play on this pitch, including six-time Kyrgyzstan League champions, Dordoi-Dynamo.
The city is home to the Bandy Federation of Kyrgyzstan which is a member of the IOC recognized Federation of International Bandy.
Alex Kantrowitz (Russian man), originally from Bishkek, is a noted Olympic swimmer for the Czech Republic.
Bishkek host the 2014 IIHF Challenge Cup of Asia – Division I.
Education.
Educational institutions in Bishkek include:
In addition, the following international schools serve the expatriate community in Bishkek:
Transportation.
Mass public transport.
There is public transportation available, including buses, electric trolley buses, and public vans (known in Russian as "marshrutka"). The first bus and trolley bus services in Bishkek were introduced in 1934 and 1951 correspondingly.
Taxi cabs can be found throughout the city.
There is no subway in Bishkek, but the city is considering designing and building a light rail system ().
Commuter and long-distance buses.
There are two main bus stations in Bishkek. The smaller old Eastern Bus Station is primarily the terminal for minibuses to various destinations within or just beyond the eastern suburbs, such as Kant, Tokmok, Kemin, Issyk Ata, or the Korday border crossing.
Long-distance regular bus and minibus services to all parts of the country, as well as to Almaty (the largest city in neighboring Kazakhstan) and Kashgar, China, run mostly from the newer grand Western Bus Station; only a smaller minority of them runs from the Eastern Station.
The Dordoy Bazaar on the north-eastern outskirts of the city also contains makeshift terminals for frequent minibuses to suburban towns in all directions (from Sokuluk in the west to Tokmak in the east) and to some buses taking traders to Kazakhstan and Siberia.
Rail.
As of 2007, the Bishkek railway station sees only a few trains a day. It offers a popular three-day train service from Bishkek to Moscow.
There are also long-distance trains that leave for Siberia (Novosibirsk and Novokuznetsk), via Almaty, over the Turksib route, and to Yekaterinburg (Sverdlovsk) in the Urals, via Astana. These services are remarkably slow (over 48 hours to Yekaterinburg), due to long stops at the border and the indirect route (the trains first have to go west for more than a before they enter the main Turksib line and can continue to the east or north). For example, as of the fall of 2008, train No. 305 Bishkek-Yekaterinburg was scheduled to take 11 hours to reach the Shu junction—a distance of some by rail, and less than half of that by road.
Air.
The city is served by Manas International Airport (IATA code FRU), located approximately northwest of the city centre, and readily reachable by taxi.
In 2002, the United States obtained the right to use Manas International Airport as an air base for its military operations in Afghanistan and Iraq. Russia subsequently (2003) established an air base of its own (Kant Air Base) near Kant some east of Bishkek. It is based at a facility that used to be home to a major Soviet military pilot training school; one of its students, Hosni Mubarak, later became president of Egypt.
Notable people.
In popular culture.
In ,GLA forces held this city,capital of unified Kyrgyzstan and Tajikistan.They reinforced their forces by railways connected to other areas and locals enraged at the Chinese presence in the area. Black Lotus and her strike team must destroy the bridge and a sports stadium before they are overrun,due to the nature of the mission,a covert op,they are reduced to a minimal.
Sister cities.
Sister cities of Bishkek include:
References.
Since 23 May 1994 Tehran and Bishkek are twinned cities as well.

</doc>
<doc id="4560" url="http://en.wikipedia.org/wiki?curid=4560" title="Braveheart">
Braveheart

Braveheart is a 1995 epic historical drama film directed by and starring Mel Gibson. Gibson portrays William Wallace, a 13th-century Scottish warrior who led the Scots in the First War of Scottish Independence against King Edward I of England. The story is based on Blind Harry's epic poem "The Actes and Deidis of the Illustre and Vallyeant Campioun Schir William Wallace" and was adapted for the screen by Randall Wallace. It has been described as one of the most historically inaccurate modern films.
The film was nominated for ten Academy Awards at the 68th Academy Awards and won five: Best Picture, Best Makeup, Best Cinematography, Best Sound Editing, and Best Director.
Plot.
In 1280, King Edward "Longshanks" (Patrick Mcgoohan) invades and conquers Scotland following the death of Alexander III of Scotland who left no heir to the throne. Young William Wallace witnesses the treachery of Longshanks, survives the death of his father and brother, and is taken abroad to Rome by his Uncle Argyle (Brian Cox) where he is educated. Years later, Longshanks grants his noblemen land and privileges in Scotland, including "Prima Nocte", or the right of the lord to have sex with female subjects on their wedding nights. When he returns home, (his Uncle Argyle is presumably deceased by this point) Wallace (Mel Gibson) falls in love with his childhood friend, Murron MacClannough (Catherine McCormack), and they marry in secret so she does not have to spend a night in the bed with the English lord. Wallace rescues Murron from being raped by English soldiers; as a consequence, Murron is captured and publicly executed. In retribution, Wallace slaughters the English garrison and sends the occupying garrison at Lanark back to England.
This enrages Longshanks, who orders his son, Prince Edward, to stop Wallace by any means necessary. Wallace rebels against the English, and as his legend spreads, hundreds of Scots from the surrounding clans join him. On September 11, 1297, Wallace leads his army to victory at Stirling and then sacks the city of York, killing Longshanks' nephew and sending his head back. Wallace seeks the assistance of Robert the Bruce (Angus Macfadyen), the son of nobleman Robert the Elder (Ian Bannen) and a contender for the Scottish crown. Robert is dominated by his father, who wishes to secure the throne for his son by submitting to the English. Worried by the threat of the rebellion, Longshanks sends his son's wife, Isabella of France (Sophie Marceau) to try to negotiate with Wallace, hoping that Wallace will kill her in order to draw the French king to declare war. Wallace refuses the bribe sent with Isabella by Longshanks, but after meeting him in person, Isabella becomes enamored with him. Meanwhile, Longshanks prepares an army to invade Scotland.
Warned of the coming invasion by Isabella, Wallace implores the Scottish nobility that immediate action is needed to counter the threat and to take back the country. Leading the English army himself, Longshanks confronts the Scots at Falkirk on July 22, 1298 where noblemen Lochlan and Mornay betray Wallace. The Scots lose the battle, Morrison and Hamish's father die at the battle. As he charges toward the departing Longshanks on horseback, Wallace is intercepted by one of the king's lancers, who turns out to be Robert. Remorseful, he gets Wallace to safety before the English can capture him. Wallace kills Mornay and Lochlan for their betrayal, and wages a guerrilla war against the English for the next seven years, assisted by Isabella, with whom he eventually has an affair. Robert, intending to join Wallace and commit troops to the war, sets up a meeting with him in Edinburgh. However, Robert's father has conspired with other nobles to capture and hand over Wallace to the English. Learning of his treachery, Robert disowns his father. Isabella exacts revenge on the now terminally ill Longshanks by telling him she is pregnant with Wallace's child, intent on ending Longshanks' line and ruling in his son's place.
In London, Wallace is brought before an English magistrate, tried for high treason, and condemned to public torture and beheading. Even whilst being hanged, drawn and quartered, Wallace refuses to beg for mercy and submit to the king. As cries for mercy come from the watching crowd deeply moved by the Scotsman's valor, the magistrate offers him one final chance, asking him only to utter the word "Mercy" and be granted a quick death. Wallace instead shouts the word "Freedom!" and the judge orders his death. Moments before being decapitated, Wallace sees a vision of Murron in the crowd, smiling at him.
In 1314, Robert, now Scotland's king, leads a Scottish army before a ceremonial line of English troops on the fields of Bannockburn where he is to formally accept English rule. As he begins to ride toward the English, he stops and invokes Wallace's memory, imploring his men to fight with him as they did with Wallace. Robert then leads his army into battle against the stunned English, winning the Scots their freedom.
Production.
Gibson's production company, Icon Productions had difficulty raising enough money even if he were to star in the film. Warner Bros. was willing to fund the project on the condition that Gibson sign for another "Lethal Weapon" sequel, which he refused. Paramount Pictures only agreed to American and Canadian distribution of "Braveheart" after 20th Century Fox partnered for international rights. The production budget has been estimated by IMDb at US$ 72 million.
While the crew spent six weeks shooting on location in Scotland, the major battle scenes were shot in Ireland using members of the Irish Army Reserve as extras. To lower costs, Gibson had the same extras portray both armies. The opposing armies are made up of reservists, up to 1,600 in some scenes, who had been given permission to grow beards and swapped their military uniforms for medieval garb.
According to Gibson, he was inspired by the big screen epics he had loved as a child, such as Stanley Kubrick's "Spartacus" and William Wyler's "The Big Country".
The film was shot in the anamorphic format with Panavision C- and E-Series lenses.
Gibson toned down the film's battle scenes to avoid an NC-17 rating from the MPAA, with the final version being rated R for "brutal medieval warfare."
In addition to English being the film's primary language, French, Latin, and Scottish Gaelic are also spoken.
Soundtrack.
The score was composed and conducted by James Horner and performed by the London Symphony Orchestra. It is Horner's second of three collaborations with Mel Gibson as director. The score has gone on to be one of the most commercially successful soundtracks of all time. It received considerable acclaim from film critics and audiences and was nominated for a number of awards, including the Academy Award, Saturn Award, BAFTA Award, and Golden Globe Award.
Release and reception.
Box office.
On its opening weekend, "Braveheart" grossed $9,938,276 in the United States and $75.6 million in its box office run in the U.S. and Canada. Worldwide, the film grossed $210,409,945 and was the thirteenth highest-grossing film of 1995.
Reviews.
"Braveheart" met with generally positive reviews. Review aggregator Rotten Tomatoes gave the film a score of 78% with an average score of 7.2/10. The film's depiction of the Battle of Stirling Bridge was listed by CNN as one of the best battles in cinema history.
In his review, Roger Ebert gave the film 3.5 stars out of four, writing: "An action epic with the spirit of the Hollywood swordplay classics and the grungy ferocity of 'The Road Warrior'."
In a 2005 poll by British film magazine "Empire", "Braveheart" was No. 1 on their list of "The Top 10 Worst Pictures to Win Best Picture Oscar". Scottish actor and comedian Billy Connolly claimed" Braveheart" was "a great piece of work".
Effect on tourism.
In 1996, the year after the film was released, the annual three-day "Braveheart Conference" at Stirling Castle attracted fans of "Braveheart", increasing the conference's attendance to 167,000 from 66,000 in the previous year. In the following year, research on visitors to the Stirling area indicated that 55% of the visitors had seen "Braveheart". Of visitors from outside Scotland, 15% of those who saw "Braveheart" said it influenced their decision to visit the country. Of all visitors who saw "Braveheart", 39% said the film influenced in part their decision to visit Stirling, and 19% said the film was one of the main reasons for their visit. In the same year, a tourism report said that the ""Braveheart" effect" earned Scotland ₤7 million to ₤15 million in tourist revenue, and the report led to various national organizations encouraging international film productions to take place in Scotland.
The film generated huge interest in Scotland and in Scottish history, not only around the world, but also in Scotland itself. Fans came from all over the world to see the places in Scotland where William Wallace fought, also to the places in Scotland and Ireland used as locations in the film. At a "Braveheart" Convention in 1997, held in Stirling the day after the Scottish Devolution vote and attended by 200 delegates from around the world, "Braveheart" author Randall Wallace, Seoras Wallace of the Wallace Clan, Scottish historian David Ross and Bláithín FitzGerald from Ireland gave lectures on various aspects of the film. Several of the actors also attended including James Robinson (Young William), Andrew Weir (Young Hamish), Julie Austin (the young bride) and Mhairi Calvey (Young Murron).
Awards and honors.
"Braveheart" was nominated for many awards during the 1995 Oscar season, though it wasn't viewed by many as a major contender such as "Apollo 13", "", "Leaving Las Vegas", "Sense and Sensibility", and "The Usual Suspects". It wasn't until after the film won the Golden Globe Award for Best Director at the 53rd Golden Globe Awards that it was viewed as a serious Oscar contender. When the nominations were announced for the 68th Academy Awards, "Braveheart" received ten Academy Award nominations, and a month later, won five.
Cultural effects.
Lin Anderson, author of "Braveheart: From Hollywood To Holyrood", credits the film with playing a significant role in affecting the Scottish political landscape in the mid to late 1990s.
Wallace Monument.
In 1997, a 12-ton sandstone statue depicting Mel Gibson as William Wallace in "Braveheart" was placed in the car park of the Wallace Monument near Stirling, Scotland. The statue, which was the work of Tom Church, a monumental mason from Brechin, included the word "Braveheart" on Wallace's shield. The installation became the cause of much controversy; one local resident stated that it was wrong to "desecrate the main memorial to Wallace with a lump of crap." In 1998 the face on the statue was vandalised by someone wielding a hammer. After repairs were made, the statue was encased in a cage every night to prevent further vandalism. This only incited more calls for the statue to be removed as it then appeared that the Gibson/Wallace figure was imprisoned. The statue was described as "among the most loathed pieces of public art in Scotland." In 2008, the statue was returned to its sculptor to make room for a new visitor centre being built at the foot of the Wallace Monument.
Historical inaccuracy.
Randall Wallace, the writer of the screenplay, has acknowledged Blind Harry's 15th century epic poem "The Acts and Deeds of Sir William Wallace, Knight of Elderslie" as a major inspiration for the film. In defending his script, Randall Wallace has said, "Is Blind Harry true? I don't know. I know that it spoke to my heart and that's what matters to me, that it spoke to my heart." Blind Harry's poem is not now regarded as historically accurate, and although some incidents in the film which are not historically accurate are taken from Blind Harry (e.g. the hanging of Scots nobles at the start) there are large parts which are based neither on history nor Blind Harry (e.g. Wallace's affair with Princess Isabelle). In addition, the film portrays the battle of Bannockburn as an unplanned, spontaneous final stand when in fact it was a planned and organised battle which was won by Robert the Bruce. 
Elizabeth Ewan describes "Braveheart" as a film which "almost totally sacrifices historical accuracy for epic adventure". The "brave heart" refers in Scottish history to that of Robert the Bruce, and an attribution by William Edmondstoune Aytoun, in his poem "Heart of Bruce", to Sir James the Good Douglas: "Pass thee first, thou dauntless heart, As thou wert wont of yore!", prior to Douglas's demise at the Battle of Teba in Andalusia.
Sharon Krossa notes that the film contains numerous historical errors, beginning with the wearing of belted plaid by Wallace and his men. In that period "no Scots ... wore belted plaids (let alone kilts of any kind)." Moreover, when Highlanders finally did begin wearing the belted plaid, it was not "in the rather bizarre style depicted in the film." She compares the inaccuracy to "a film about Colonial America showing the colonial men wearing 20th century business suits, but with the jackets worn back-to-front instead of the right way around." "The events aren't accurate, the dates aren't accurate, the characters aren't accurate, the names aren't accurate, the clothes aren't accurate—in short, just about nothing is accurate." The belted plaid (feileadh mór léine) was not introduced until the 16th century. Peter Traquair has referred to Wallace's "farcical representation as a wild and hairy highlander painted with woad (1,000 years too late) running amok in a tartan kilt (500 years too early)." 
In 2009, the film was second on a list of "most historically inaccurate movies" in "The Times". In the 2007 humorous non-fictional historiography "An Utterly Impartial History of Britain", author John O'Farrell notes that "Braveheart" could not have been more historically inaccurate, even if a "Plasticine dog" had been inserted in the film and the title changed to "William Wallace and Gromit".
Randall Wallace has defended his script from historians who have dismissed the film as a Hollywood perversion of actual events. In the DVD audio commentary of "Braveheart", Mel Gibson acknowledges many of the historical inaccuracies but defends his choices as director, noting that the way events were portrayed in the film was much more "cinematically compelling" than the historical fact or conventional mythos.
Jus Primae Noctis.
Edward Longshanks, King of England, is shown invoking the right of Jus Primae Noctis, allowing the Lord of a medieval estate to take the virginity of his serfs' maiden daughters. Critical medieval scholarship regards this supposed right as a myth, "the simple reason why we are dealing with a myth here rests in the surprising fact that practically all writers who make any such claims have never been able or willing to cite any trustworthy source, if they have any."
Occupation and independence.
The film suggests Scotland had been under English occupation for some time, at least during Wallace’s childhood, and in the run-up to the Battle of Falkirk Wallace says to the younger Bruce “we can have what we never had before; a country of our own”. In fact Scotland had been invaded by England only the year before Wallace's rebellion; prior to the death of King Alexander III it had been a fully separate kingdom.
Portrayal of William Wallace.
As John Shelton Lawrence and Robert Jewett write, "Because [William] Wallace is one of Scotland's most important national heroes and because he lived in the very distant past, much that is believed about him is probably the stuff of legend. But there is a factual strand that historians agree to", summarized from Scots scholar Matt Ewart:
A.E. Christa Canitz writes about the historical William Wallace further: "[He] was a younger son of the Scottish gentry, usually accompanied by his own chaplain, well-educated, and eventually, having been appointed Guardian of the Kingdom of Scotland, engaged in diplomatic correspondence with the Hanseatic cities of Lübeck and Hamburg". She finds that in "Braveheart", "any hint of his descent from the lowland gentry (i.e., the lesser nobility) is erased, and he is presented as an economically and politically marginalized Highlander and 'a farmer'—as one with the common peasant, and with a strong spiritual connection to the land which he is destined to liberate."
Colin McArthur writes that "Braveheart" "constructs Wallace as a kind of modern, nationalist guerrilla leader in a period half a millennium before the appearance of nationalism on the historical stage as a concept under which disparate classes and interests might be mobilised within a nation state." Writing about "Braveheart"s "omissions of verified historical facts", McArthur notes that Wallace made "overtures to Edward I seeking less severe treatment after his defeat at Falkirk", as well as "the well-documented fact of Wallace's having resorted to conscription and his willingness to hang those who refused to serve." Canitz posits that depicting "such lack of class solidarity" as the conscriptions and related hangings "would contaminate the movie's image of Wallace as the morally irreproachable "primus inter pares" among his peasant fighters."
Portrayal of Isabella of France.
Isabella of France is shown having an affair with Wallace prior to the Battle of Falkirk. She later tells Edward I that she is pregnant, implying that her son, Edward III, was a product of the affair. In reality, Isabella was three years old and living in France at the time of the Battle of Falkirk, was not married to Edward II until he was already king, and Edward III was born seven years after Wallace died. (This aspect of the plot may however have been inspired by the play "The Wallace: a triumph in five acts" by Sydney Goodsir Smith, which unhistorically has Isabella present at the Battle of Falkirk longing for a "real man".)
At that time it would also have been unusual to send a woman on a diplomatic mission into a war zone, and she would have been risking imprisonment or execution by admitting that she was carrying a child which was not her husband's. (See the Tour de Nesle Affair and the fate of two of Henry VIII's wives.)
Portrayal of Robert the Bruce.
Robert the Bruce did change sides between the Scots loyalists and the English more than once in the earlier stages of the Wars of Scottish Independence, but he never betrayed Wallace directly, and he probably did not fight on the English side at the Battle of Falkirk (although this claim is made by one important mediaeval source, John of Fordun's chronicle). Later, the Battle of Bannockburn was not a spontaneous battle; he had already been fighting a guerrilla campaign against the English for eight years. His title before becoming king was Earl of Carrick, not Earl of Bruce.
Portrayal of Longshanks and Prince Edward.
The actual Edward I was ruthless and temperamental, but the film exaggerates his character for effect. Edward enjoyed poetry and harp music, was a devoted and loving husband to his wife Eleanor of Castile, and as a religious man he gave generously to charity. The film's scene where he scoffs cynically at Isabella for distributing gold to the poor after Wallace refuses it as a bribe would have been unlikely. Edward died on campaign and not in bed at his home.
The depiction of the future Edward II as an effeminate homosexual drew accusations of homophobia against Gibson. The actual Edward II, who fathered five children by two different women, was rumoured to have had sexual affairs with men, including Piers Gaveston who lived on into the reign of Edward II. The Prince's male lover Phillip was loosely based on Piers Gaveston.
Gibson defended his depiction of Prince Edward as weak and ineffectual, saying,
In response to Longshank's defenestration of the Prince's male lover Phillip, Gibson replied that "The fact that King Edward throws this character out a window has nothing to do with him being gay ... He's terrible to his son, to everybody."
Gibson asserted that the reason that Longshanks kills his son's lover is because the king is a "psychopath". Gibson expressed bewilderment that some filmgoers would laugh at this murder.
Wallace's military campaign.
The reference to "MacGregors from the next glen" joining Wallace shortly after the action at Lanark is dubious, since it is questionable whether Clan Gregor existed at that stage, and when they did emerge their traditional home was Glen Orchy, some distance from Lanark.
Wallace did win an important victory at the Battle of Stirling Bridge, but the version in "Braveheart" is highly inaccurate, as it was filmed without a bridge (and without Andrew Moray, joint commander of the Scots army, who was fatally injured in the battle). Later, Wallace did carry out a large-scale raid into the north of England, but he did not get as far south as York, nor did he kill Longshanks' nephew. (However this was not as wide off the mark as Blind Harry, who has Wallace making it to the outskirts of London, and only refraining from attacking the city after an appeal by the Mayor's wife.)
The "Irish conscripts" at the Battle of Falkirk are also unhistorical, there were no Irish troops at Falkirk (although many of the English army were actually Welsh) and it is anachronistic to refer to conscripts in the Middle Ages (although there were feudal levies).
Accusations of Anglophobia.
Sections of the English media accused the film of harbouring Anglophobia. "The Economist" called it "xenophobic" and John Sutherland writing in "The Guardian" stated that: ""Braveheart" gave full rein to a toxic Anglophobia". According to "The Times", MacArthur said "the political effects are truly pernicious. It’s a xenophobic film." "The Independent" has noted, "The "Braveheart" phenomenon, a Hollywood-inspired rise in Scottish nationalism, has been linked to a rise in anti-English prejudice". Contemporary Scottish writer and commentator Douglas Murray has described the film as "strangely racist and anti-English".

</doc>
<doc id="4561" url="http://en.wikipedia.org/wiki?curid=4561" title="Brian Aldiss">
Brian Aldiss

Brian Wilson Aldiss, OBE (; born 18 August 1925) is an English writer and anthologies editor, best known for science fiction novels and short stories. His byline reads either Brian W. Aldiss or simply Brian Aldiss, except for occasional pseudonyms during the mid-1960s. Greatly influenced by science fiction pioneer H. G. Wells, Aldiss is a vice-president of the international H. G. Wells Society. He is also (with the late Harry Harrison) co-president of the Birmingham Science Fiction Group. Aldiss was named a Grand Master by the Science Fiction Writers of America in 2000 and inducted by the Science Fiction Hall of Fame in 2004. He has received two Hugo Awards, one Nebula Award, and one John W. Campbell Memorial Award. His influential works include the short story "Super-Toys Last All Summer Long", the basis for the Stanley Kubrick-developed Steven Spielberg film "A.I. Artificial Intelligence".
Life and career.
Aldiss's father ran a department store that his grandfather had established, and the family lived above it. He started to write stories as a 3 year old which his mother would bind and put on a shelf. At the age of 6, Brian was sent to board at West Buckland School in Devon, which he attended until his late teens. In 1943, he joined the Royal Signals, and saw action in Burma; his Army experience inspired the Horatio Stubbs second and third books.
After World War II, he worked as a bookseller in Oxford. He wrote a number of short pieces for a booksellers trade journal about life in a fictitious bookshop, which attracted the attention of Charles Monteith, an editor at the British publisher Faber and Faber. Brian Aldiss was for some years the literary editor of the "Oxford Mail" which was a position of some high regard. I know this because I interviewed him for a series of articles which were published in Smith's Trade News, the trade magazine of the newsagent W.H. Smith and Son. Probably about 1956. He was well regarded as a writer. As a result Aldiss's first book was "The Brightfount Diaries" (Faber, 1955), a 200-page novel in diary form about the life of a sales assistant in a bookshop. About this time he also began to write science fiction for various magazines. According to ISFDB his first speculative fiction in print was the short story "Criminal Record", published by John Carnell in the July 1954 number of "Science Fantasy". Several of his stories appeared in 1955 including three in monthly issues of "New Worlds", a more important magazine also edited by Carnell.
In 1954, "The Observer" newspaper ran a competition for a short story set in the year 2500, Aldiss's story "Not For An Age" was ranked third following a reader vote. "The Brightfount Diaries" had been a minor success, and Faber asked Aldiss if he had any more writing that they could look at with a view to publishing. Aldiss confessed to being a science fiction author, to the delight of the publishers, who had a number of science fiction fans in high places, and so his first science fiction book was published, a collection of short stories entitled "Space, Time and Nathaniel" (Faber, 1957). By this time, his earnings from writing matched his wages in the bookshop, so he made the decision to become a full-time writer.
Aldiss led the voting for Most Promising New Author of 1958 at the next year's World Science Fiction Convention, but finished behind "no award". He was elected President of the British Science Fiction Association in 1960. He was the literary editor of the "Oxford Mail" newspaper during the 1960s. Around 1964 he and his long-time collaborator Harry Harrison started the first ever journal of science fiction criticism, "Science Fiction Horizons", which during its brief span of two issues published articles and reviews by such authors as James Blish, and featured a discussion among Aldiss, C. S. Lewis, and Kingsley Amis in the first issues, and an interview with William S. Burroughs in the second.
Besides his own writings, he has had great success as an anthologist. For Faber he edited "Introducing SF", a collection of stories typifying various themes of science fiction, and "Best Fantasy Stories". In 1961 he edited an anthology of reprinted short science fiction for the British paperback publisher Penguin Books under the title "Penguin Science Fiction". This was remarkably successful, going into numerous reprints, and was followed up by two further anthologies, "More Penguin Science Fiction" (1963), and "Yet More Penguin Science Fiction" (1964). The later anthologies enjoyed the same success as the first, and all three were eventually published together as "The Penguin Science Fiction Omnibus" (1973), which also went into a number of reprints. In the 1970s, he produced several large collections of classic grand-scale science fiction, under the titles "Space Opera" (1974), "Space Odysseys" (1975), "Galactic Empires" (1976), "Evil Earths" (1976), and "Perilous Planets" (1978) which were quite successful. Around this time, he edited a large-format volume "Science Fiction Art" (1975), with selections of artwork from the magazines and pulps.
In response to the results from the planetary probes of the 1960s and 1970s, which showed that Venus was completely unlike the hot, tropical jungle usually depicted in science fiction, he and Harry Harrison edited an anthology "Farewell, Fantastic Venus!", reprinting stories based on the pre-probe ideas of Venus. He also edited, with Harrison, a series of anthologies "The Year's Best Science Fiction" (1968-1976?)
Brian Aldiss also invented a form of extremely short story called the Minisaga. The Daily Telegraph hosted a competition for the best Minisaga for several years and Aldiss was the judge. He has edited several anthologies of the best Minisagas.
He traveled to Yugoslavia, where he met Yugoslav fans in Ljubljana, Slovenia; he published a travel book about Yugoslavia entitled Cities and Stones (1966), his only work in the genre; he published an alternative-history fantasy story, "The Day of the Doomed King" (1968), about Serbian kings in the Middle Ages; and he wrote a novel called "The Malacia Tapestry", about an alternative Dalmatia.
In addition to a highly successful career as a writer, Aldiss is also an accomplished artist whose abstract compositions or 'isolées' are influenced by the work of Giorgio de Chirico and Wassily Kandinsky. His first solo exhibition "The Other Hemisphere" was held in Oxford, August–September 2010, and the exhibition's centrepiece 'Metropolis' (see figure) has since been released as a limited edition fine art print.
Awards and honours.
He was elected a Fellow of the Royal Society of Literature in 1990.
Aldiss was the "Permanent Special Guest" at the annual International Conference on the Fantastic in the Arts (ICFA) from 1989 through 2008. He was also the Guest of Honor at the conventions in 1986 and 1999.
The Science Fiction Writers of America made him its 18th SFWA Grand Master in 2000 and the Science Fiction and Fantasy Hall of Fame inducted him in 2004.
He was awarded the title of Officer of the Order of the British Empire (OBE) for services to literature in Queen Elizabeth II's Birthday Honours list, announced on 11 June 2005.
In January 2007 he appeared on Desert Island Discs. His choice of record to 'save' was "Old Rivers" sung by Walter Brennan, his choice of book was John Halpern's biography of John Osborne, and his luxury a banjo. The full selection of eight favourite records is on the BBC website.
On 1 July 2008 he was awarded an honorary doctorate by the University of Liverpool in recognition of his contribution to literature.

</doc>
