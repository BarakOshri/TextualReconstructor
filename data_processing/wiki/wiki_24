<doc id="4010" url="http://en.wikipedia.org/wiki?curid=4010" title="Bing Crosby">
Bing Crosby

Harry Lillis “Bing” Crosby Jr. (May 3, 1903–October 14, 1977) was an American singer and actor. Crosby’s trademark bass-baritone voice made him one of the best-selling recording artists of the 20th century, with over half a billion records in circulation.
A multimedia star, from 1934 to 1954 Crosby was a leader in record sales, radio ratings, and motion picture grosses. His early career coincided with technical recording innovations; this allowed him to develop a laid-back, intimate singing style that influenced many of the popular male singers who followed him, including Perry Como, Frank Sinatra, and Dean Martin. "Yank" magazine recognized Crosby as the person who had done the most for American G.I. morale during World War II and, during his peak years, around 1948, American polls declared him the “most admired man alive,” ahead of Jackie Robinson and Pope Pius XII. Also in 1948, the "Music Digest" estimated that Crosby recordings filled more than half of the 80,000 weekly hours allocated to recorded radio music.
Crosby exerted an important influence on the development of the postwar recording industry. He worked for NBC at the time and wanted to record his shows; however, most broadcast networks did not allow recording. This was primarily because the quality of recording at the time was not as good as live broadcast sound quality. While in Europe performing during the war, Crosby had witnessed tape recording, on which The Crosby Research Foundation would come to have many patents. The company also developed equipment and recording techniques such as the laugh track, which are still in use today. In 1947 he invested $50,000 in the Ampex company, which built North America’s first commercial reel-to-reel tape recorder. He left NBC to work for ABC because NBC was not interested in recording at the time. This proved beneficial, because ABC accepted him and his new ideas. Crosby then became the first performer to pre-record his radio shows and master his commercial recordings onto magnetic tape. He gave one of the first Ampex Model 200 recorders to a friend, musician Les Paul, which led directly to Paul’s invention of multitrack recording. Along with Frank Sinatra, Crosby was one of the principal backers behind the famous United Western Recorders recording studio complex in Los Angeles.
During the “Golden Age of Radio,” performers often had to recreate their live shows a second time for the west coast time zone. Through the medium of recording, Crosby constructed his radio programs with the same directorial tools and craftsmanship (editing, retaking, rehearsal, time shifting) being used in motion picture production. This became the industry standard.
Crosby won an Academy Award for Best Actor for his role as Father Chuck O’Malley in the 1944 motion picture "Going My Way", and was nominated for his reprise of the role in "The Bells of St. Mary’s" opposite Ingrid Bergman the next year, becoming the first of four actors to be nominated twice for playing the same character. In 1963 Crosby received the first Grammy Global Achievement Award. Crosby is one of the 22 people to have three stars on the Hollywood Walk of Fame (a star for motion pictures, radio, and audio recording).
Childhood.
Crosby was born in Tacoma, Washington, on May 3, 1903, in a house his father built at 1112 North J Street. In 1906 Crosby’s family moved to Spokane, and in 1913, Crosby’s father built a house at 508 E. Sharp Ave. The house now sits on the campus of Crosby’s alma mater, Gonzaga University and formerly housed the Alumni Association.
He was the fourth of seven children: brothers Larry (1895–1975), Everett (1896–1966), Ted (1900–73), and Bob (1913–93); and two sisters, Catherine (1904–74) and Mary Rose (1906–90). His parents were Harry Lillis Crosby Sr. (1870–1950), a bookkeeper, and Catherine Helen (known as Kate) (née Harrigan; 1873–1964). Crosby’s mother was a second-generation Irish American. His father was of English descent; some of his ancestors had emigrated to America in the 17th century, and included "Mayflower" passenger William Brewster (c. 1567–April 10, 1644).
In 1910, six-year-old Harry Crosby Jr. was forever renamed. The Sunday edition of the "Spokesman-Review" published a feature called The Bingville Bugle. Written by humorist Newton Newkirk, "The Bingville Bugle" was a parody of a hillbilly newsletter filled with gossipy tidbits, minstrel quips, creative spelling, and mock ads. A neighbor, 15-year-old Valentine Hobart, shared Crosby’s enthusiasm for The Bugle and noting Crosby’s laugh, took a liking to him and called him Bingo from Bingville. Eventually the last vowel was dropped and the nickname stuck.
In 1917 Crosby took a summer job as property boy at Spokane’s “Auditorium,” where he witnessed some of the finest acts of the day, including Al Jolson, who held Crosby spellbound with his ad libbing and spoofs of Hawaiian songs. Crosby later described Jolson’s delivery as “electric.”
Crosby graduated from Gonzaga High School (today’s Gonzaga Prep) in 1920 and enrolled at Gonzaga University, but did not earn a bachelor’s degree. The university granted him an honorary doctorate in 1937.
Popular success.
Music.
In 1923 Crosby was invited to join a new band composed of high school students a few years younger than himself. Al Rinker, Miles Rinker, James Heaton, Claire Pritchard, and Robert Pritchard, along with drummer Crosby, formed the Musicaladers, who performed at dances both for high school students and club-goers. The group did perform on Spokane radio station KHQ but disbanded after two years.
By 1925 Crosby had formed a vocal duo with partner Al Rinker, brother of singer Mildred Bailey. Bailey introduced Rinker and Crosby to Paul Whiteman, who was at that time America’s most famous bandleader. Hired for $150 a week in 1926, they made their debut on December 6 at the Tivoli Theatre in Chicago. Their first recording was “I've Got The Girl,” with Don Clark’s Orchestra, but the Columbia-issued record did them no vocal favors, as it was inadvertently recorded at a speed slower than it should have been; this increased the singers’ pitch when played at 78 rpm. Throughout his career, Crosby often credited Mildred Bailey for getting him his first important job in the entertainment business.
Even as the Crosby and Rinker duo was increasing in popularity, Whiteman added a third member to the group. The threesome, now including pianist and aspiring songwriter Harry Barris, were dubbed The Rhythm Boys. They joined the Whiteman touring act, performing and recording with musicians Bix Beiderbecke, Jack Teagarden, Tommy Dorsey, Jimmy Dorsey, Eddie Lang, and Hoagy Carmichael, also appearing together in a Whiteman movie.
Crosby soon became the star attraction of the Rhythm Boys, and in 1928 he had his first No. one hit with the Whiteman orchestra, a jazz-influenced rendition of “Ol’ Man River.” However, Crosby’s reported taste for alcohol and his growing dissatisfaction with Whiteman led to his quitting the Rhythm Boys to join the Gus Arnheim Orchestra. During his time with Arnheim, the other two Rhythm Boys were increasingly pushed to the background as the emphasis was on Crosby. Harry Barris wrote several of Crosby’s subsequent hits, including “At Your Command,” “I Surrender Dear,” and “Wrap Your Troubles In Dreams.” But the members of the band had a falling out and split, setting the stage for Crosby’s solo career.
On September 2, 1931, Crosby made his solo radio debut. Before the end of the year, he signed with both Brunswick Records and CBS Radio. Doing a weekly 15-minute radio broadcast, Crosby quickly became a huge hit. His songs “Out of Nowhere,” “Just One More Chance,” “At Your Command,” and “I Found a Million Dollar Baby (in a Five and Ten Cent Store)” were all among the best selling songs of 1931.
As the 1930s unfolded, Crosby became the leading singer in America. Ten of the top 50 songs for 1931 featured Crosby, either solo or with others. A so-called Battle of the Baritones with singing star Russ Columbo proved short-lived, replaced with the slogan “Bing Was King.” Crosby played the lead in a series of sound-era musical comedy short films for Mack Sennett, signed with Paramount, and starred in his first full-length feature, 1932’s "The Big Broadcast", the first of 55 films in which he received top billing. He would appear in 79 pictures and signed a long-term deal with Jack Kapp’s new record company Decca in late 1934.
Around this time, Crosby co-starred on radio with The Carl Fenton Orchestra on a popular CBS radio show. By 1936 he'd replaced his former boss, Paul Whiteman, as the host of NBC’s "Kraft Music Hall", the weekly radio program where he remained for the next 10 years. “Where the Blue of the Night (Meets the Gold of the Day),” which showcased one of his then-trademark whistling interludes, became his theme song and signature tune.
Crosby’s much-imitated style helped take popular singing beyond the kind of “belting” associated with boisterous performers such as Al Jolson and Billy Murray, who had been obliged to reach the back seats in New York theaters without the aid of the microphone. As Henry Pleasants noted in "The Great American Popular Singers", something new had entered American music, a style that might be called “singing in American” with conversational ease. This new sound led to the popular epithet “crooner.”
Crosby made numerous live appearances before American troops fighting in the European theater. He also learned how to pronounce German from written scripts and would read propaganda broadcasts intended for the German forces. The nickname Der Bingle was common among Crosby’s German listeners and came to be used by his English-speaking fans. In a poll of U.S. troops at the close of World War II, Crosby topped the list as the person who had done the most for G.I. morale, ahead of President Franklin Delano Roosevelt, General Dwight Eisenhower, and Bob Hope.
‘White Christmas’.
The biggest hit song of Crosby’s career was his recording of Irving Berlin’s “White Christmas,” which he first introduced on a Christmas Day radio broadcast in 1941. (A copy of the recording from the radio program is owned by the estate of Bing Crosby and was loaned to "CBS Sunday Morning" for its December 25, 2011, program). The song then appeared soon after in his 1942 movie "Holiday Inn". Crosby’s recording hit the charts on October 3, 1942, and rose to No. 1 on October 31, where it stayed for 11 weeks. A holiday perennial, the song was repeatedly re-released by Decca, charting another 16 times. It topped the charts again in 1945, and for a third time in January 1947. The song remains the best-selling single of all time. According to "Guinness World Records", Crosby’s recording of “White Christmas” has “sold over 100 million copies around the world, with at least 50 million sales as singles.” Crosby’s recording was so popular that he was obliged to re-record it in 1947 using the same musicians and backup singers; the original 1942 master had become damaged due to its frequent use in pressing additional singles. Though the two versions are very similar, it is the 1947 recording that is most familiar today. Crosby was dismissive of his role in the song’s success, saying later that “a jackdaw with a cleft palate could have sung it successfully.”
Motion pictures.
With 1,077,900,000 movie tickets sold, Crosby is by that measure the third most popular actor of all time, behind Clark Gable (1,168,300,000) and John Wayne (1,114,000,000). The Quigley Publishing Company’s "International Motion Picture Almanac" lists Crosby in a tie for second on the All Time Number One Stars List with Clint Eastwood, Tom Hanks, and Burt Reynolds. Crosby's most popular film, "White Christmas", grossed $30 million in 1954 ($ million in current value). Crosby won an Academy Award for Best Actor for "Going My Way" in 1944 and was nominated for the 1945 sequel, "The Bells of Saint Mary’s". He received critical acclaim for his performance as an alcoholic entertainer in "The Country Girl" and received his third Academy Award nomination.
Crosby starred with Bob Hope and actress Dorothy Lamour in seven "Road to" musical comedies between 1940 and 1962, cementing the two entertainers as an on-and-off duo, despite never officially declaring themselves a “team” in the sense that Laurel and Hardy or Dean Martin and Jerry Lewis were teams. The series consists of "Road to Singapore" (1940), "Road to Zanzibar" (1941), "Road to Morocco" (1942), "Road to Utopia" (1946), "Road to Rio" (1947), "Road to Bali" (1952), and "The Road to Hong Kong" (1962). Appearing solo, Crosby and Hope frequently made note of the other during their various appearances, typically in a comically insulting fashion, and they appeared together countless times on stage, radio, and television over the decades as well as cameos in several additional films. In the 1949 Disney animated film "The Adventures of Ichabod and Mr. Toad", Crosby provided the narration and song vocals for "The Legend of Sleepy Hollow" segment.
By the late 1950s, Crosby had become seen as an avuncular elder statesman of music, and his albums "Bing Sings Whilst Bregman Swings" and "Bing With A Beat" sold reasonably well, even in the rock-and-roll era. In 1960 Crosby starred in "High Time", a collegiate comedy with Fabian Forte and Tuesday Weld that predicted the emerging gap between him and the new young generation of musicians and actors who had begun their careers after WWII. The following year, Crosby and Hope reunited for one more "Road" movie, "The Road to Hong Kong", which teamed them up with the much younger Joan Collins and Peter Sellers. Collins was used in place of their longtime partner Dorothy Lamour, whom Crosby felt was getting too old for the role, although Hope refused to do the movie without her so she instead made a lengthy cameo appearance. Not long before his death in 1977, Crosby had plans for yet another "Road" film in which the aging trio of himself, Hope, and Lamour search for the legendary Fountain of Youth. Ever media-savvy, he was alleged to have asked the scriptwriters to model "The Road To The Fountain Of Youth" on the "Monty Python" series so as to keep the humor fresh and contemporary for 1970s audiences.
Warner Bros. cartoons occasionally caricatured Crosby, alternately as an animal and as himself. His recognizable appearance popped up in "I've Got to Sing a Torch Song", "Hollywood Steps Out" and "What's Up, Doc?", while bird versions appeared in "The Woods Are Full of Cuckoos", "Swooner Crooner", and "Curtain Razor". "Bingo Crosbyana" had an insect version of him.
Television.
"The Fireside Theater" (1950) was Crosby’s first television production. The series of 26-minute shows was filmed at Hal Roach Studios rather than performed live on the air. The “telefilms” were syndicated to individual television stations.
Crosby was a frequent guest on the musical variety shows of the 1950s and 1960s. He was especially closely associated with ABC’s variety show "The Hollywood Palace". He was the show’s first and most frequent guest host, and appeared annually on its Christmas edition with his wife Kathryn and his younger children. In the early 1970s he made two famous late appearances on the "Flip Wilson Show", singing duets with the comedian. Crosby’s last TV appearance was a Christmas special filmed in London in September 1977 and aired just weeks after his death. It was on this special that Crosby recorded a duet of “The Little Drummer Boy” and “Peace on Earth” with the flamboyant rock star David Bowie. It was rush-released as a single 45-rpm record and has since become a staple of holiday radio, and the final popular hit of Crosby’s career. At the end of the century, "TV Guide" listed the Crosby–Bowie duet as one of the 25 most memorable musical moments of 20th-century television.
Bing Crosby Productions, affiliated with Desilu Studios and later CBS Television Studios, produced a number of television series, including Crosby’s own unsuccessful ABC sitcom "The Bing Crosby Show" in the 1964–65 season (with co-stars Beverly Garland and Frank McHugh). The company produced two ABC medical dramas, "Ben Casey" (1961–66) and "Breaking Point" (1963–64), the popular "Hogan's Heroes" (1965–71) military comedy on CBS, as well as the lesser known show "Slattery’s People" (1964–65). Another show that Crosby Productions produced was the game show "Beat the Odds".
Singing style and vocal characteristics.
Crosby was one of the first singers to exploit the intimacy of the microphone, rather than using the deep, loud “vaudeville style” associated with Al Jolson and others.Crosby’s love and appreciation of jazz music helped bring the genre to a wider mainstream audience. Within the framework of the novelty-singing style of the Rhythm Boys, Crosby bent notes and added off-tune phrasing, an approach that was firmly rooted in jazz. He had already been introduced to Louis Armstrong and Bessie Smith prior to his first appearance on record. Crosby and Armstrong would remain professionally friendly for decades, notably in the 1956 film "High Society", where they sang the duet “Now You Has Jazz.”
During the early portion of his solo career (about 1931–34), Crosby’s emotional, often pleading style of crooning was popular. But Jack Kapp, (manager of Brunswick and later Decca), talked Crosby into dropping many of his jazzier mannerisms, in favor of a straight-ahead clear vocal style. Crosby credited Kapp for diversifying his repertoire into various styles and genres, working with many other artists, and choosing hit songs.
Crosby also elaborated on a further idea of Al Jolson’s: phrasing, or the art of making a song’s lyric ring true. His success in doing so was influential. “I used to tell Sinatra over and over,” said Tommy Dorsey, “there's only one singer you ought to listen to and his name is Crosby. All that matters to him is the words, and that's the only thing that ought to for you, too.”
Vocal critic Henry Pleasants wrote:
[While] the octave B flat to B flat in Bing’s voice at that time [1930s] is, to my ears, one of the loveliest I have heard in 45 years of listening to baritones, both classical and popular, it dropped conspicuously in later years. From the mid-1950s, Bing was more comfortable in a bass range while maintaining a baritone quality, with the best octave being G to G, or even F to F. In a recording he made of 'Dardanella' with Louis Armstrong in 1960, he attacks lightly and easily on a low E flat. This is lower than most opera basses care to venture, and they tend to sound as if they were in the cellar when they get there.
Career statistics.
Crosby’s was among the most popular and successful musical acts of the 20th century. Although "Billboard" magazine operated under different methodologies for the bulk of Crosby’s career, his chart numbers remain astonishing: 383 chart singles, including 41 No. 1 hits. Crosby had separate charting singles in every calendar year between 1931 and 1954; the annual re-release of “White Christmas” extended that streak to 1957. He had 24 separate popular singles in 1939 alone. "Billboard"’s statistician Joel Whitburn determined Crosby to be America’s most successful recording act of the 1930s, and again in the 1940s.
For 15 years (1934, 1937, 1940, 1943–54), Crosby was among the top 10 in box-office drawing power, and for five of those years (1944–48) he topped the world. He sang four Academy Award–winning songs—“Sweet Leilani” (1937), “White Christmas” (1942), “Swinging on a Star” (1944), “In the Cool, Cool, Cool of the Evening” (1951)—and won the Academy Award for Best Actor for his role in "Going My Way" (1944).
He collected 23 gold and platinum records, according to the book "Million Selling Records". The Recording Industry Association of America did not institute its gold record certification program until 1958, by which point Crosby’s record sales were barely a blip; prior to that point, gold records were awarded by an artist’s own record company. Universal Music, current owner of Crosby’s Decca catalog, has never requested RIAA certification for any of his hit singles.
Although often overlooked in many biographies, Crosby charted 23 "Billboard" hits from 47 recorded songs with the Andrews Sisters, whose Decca record sales were second only to Crosby’s throughout the 1940s. Patty, Maxene, and LaVerne were his most frequent collaborators on disc from 1939 to 1952, a partnership that produced four million-selling singles: “Pistol Packin' Mama,” “Jingle Bells,” “Don’t Fence Me In,” and “South America, Take it Away.” They made one film appearance together in “Road to Rio,” singing “You Don't Have to Know the Language,” and they sang together countless times on radio shows throughout the 1940s and 1950s (appearing as guests on each other’s shows quite often, as well as on many shows for the Armed Forces Radio Service during and after World War II). The quartet’s Top-10 "Billboard" hits from 1943 to 1945 (including “The Vict’ry Polka,” “There’ll Be a Hot Time in the Town of Berlin (When the Yanks Go Marching In),” and “Is You Is or Is You Ain’t (Ma’ Baby?)”) were major morale-boosters for the American public during the war years.
In 1962 Crosby was given the Grammy Lifetime Achievement Award. He has been inducted into the halls of fame for both radio and popular music. In 2007 Crosby was inducted into the Hit Parade Hall of Fame, and in 2008 into the Western Music Hall of Fame.
Entrepreneurship.
Mass media.
Crosby’s radio career took a significant turn in 1945, when he clashed with NBC over his insistence that he be allowed to pre-record his radio shows. (The live production of radio shows was also reinforced by the musicians union and ASCAP, which wanted to ensure continued work for their members.) In "On the Air: The Encyclopedia of Old-Time Radio", historian John Dunning wrote about German engineers having developed a tape recorder with a near-professional broadcast quality standard:
[Crosby saw] an enormous advantage in prerecording his radio shows. The scheduling could now be done at the star’s convenience. He could do four shows a week, if he chose, and then take a month off. But the networks and sponsors were adamantly opposed. The public wouldn’t stand for “canned” radio, the networks argued. There was something magic for listeners in the fact that what they were hearing was being performed and heard everywhere, at that precise instant. Some of the best moments in comedy came when a line was blown and the star had to rely on wit to rescue a bad situation. Fred Allen, Jack Benny, Phil Harris, and also Crosby were masters at this, and the networks weren’t about to give it up easily.
Crosby’s insistence eventually factored into the further development of magnetic tape sound recording and the radio industry’s widespread adoption of it. He used his clout, both professional and financial, to innovate methods of reproducing audio of his performances. But NBC (and competitor CBS) were also insistent, refusing to air prerecorded radio programs. Crosby walked away from the network and stayed off the air for seven months, creating a legal battle with Kraft, his sponsor, that was settled out of court. Crosby returned to the air for the last 13 weeks of the 1945–46 season.
The Mutual network, on the other hand, had pre-recorded some of its programs as early as the 1938 run of "The Shadow" with Orson Welles. And the new ABC network, which had been formed out of the sale of the old NBC Blue Network in 1943 following a federal antitrust action, was willing to join Mutual in breaking the tradition. ABC offered Crosby $30,000 per week to produce a recorded show every Wednesday that would be sponsored by Philco. He would also get an additional $40,000 from 400 independent stations for the rights to broadcast the 30-minute show, which was sent to them every Monday on three 16-inch lacquer/aluminum discs that played 10 minutes per side at 33⅓ rpm.
Crosby wanted to change to recorded production for several reasons. The legend that has been most often told is that it would give him more time for his golf game. And he did record his first Philco program in August 1947 so he could enter the Jasper National Park Invitational Golf Tournament in September, just when the new radio season was to start. But golf was not the most important reason.
Though Crosby did want more time to tend his other business and leisure activities, he also sought better quality through recording, including being able to eliminate mistakes and control the timing of his show performances. Because his own Bing Crosby Enterprises produced the show, he could purchase the latest and best sound equipment and arrange the microphones his way; the logistics of microphone placement had long been a hotly debated issue in every recording studio since the beginning of the electrical era. No longer would he have to wear the hated toupee on his head previously required by CBS and NBC for his live audience shows (he preferred a hat). He could also record short promotions for his latest investment, the world’s first frozen orange juice, sold under the brand name Minute Maid. This investment allowed Crosby to make more money by finding a loophole whereby the IRS couldn’t tax him at a 77 percent rate.
The transcription method posed problems, though. The acetate surface coating of the aluminum discs was little better than the wax that Edison had used at the turn of the 20th century, with the same limited dynamic range and frequency response.
However, Murdo MacKenzie of Bing Crosby Enterprises had seen a demonstration of the German Magnetophon in June 1947—the same device that Jack Mullin had brought back from Radio Frankfurt, along with 50 reels of tape, at the end of the war. It was one of the magnetic tape recorders that BASF and AEG had built in Germany starting in 1935. The 6.5mm ferric oxide–coated tape could record 20 minutes per reel of high-quality sound. Alexander M. Poniatoff ordered his Ampex company, which he’d founded in 1944, to manufacture an improved version of the Magnetophone.
Crosby hired Mullin to start recording his "Philco Radio Time" show on his German-made machine in August 1947, using the same 50 reels of I.G. Farben magnetic tape that Mullin had found at a radio station at Bad Nauheim near Frankfurt while working for the U.S. Army Signal Corps. The crucial advantage was editing. As Crosby wrote in his autobiography:
By using tape, I could do a thirty-five or forty-minute show, then edit it down to the twenty-six or twenty-seven minutes the program ran. In that way, we could take out jokes, gags, or situations that didn’t play well and finish with only the prime meat of the show; the solid stuff that played big. We could also take out the songs that didn’t sound good. It gave us a chance to first try a recording of the songs in the afternoon without an audience, then another one in front of a studio audience. We’d dub the one that came off best into the final transcription. It gave us a chance to ad lib as much as we wanted, knowing that excess ad libbing could be sliced from the final product. If I made a mistake in singing a song or in the script, I could have some fun with it, then retain any of the fun that sounded amusing.
Mullin’s 1976 memoir of these early days of experimental recording agrees with Crosby's account:
In the evening, Crosby did the whole show before an audience. If he muffed a song then, the audience loved it—thought it was very funny—but we would have to take out the show version and put in one of the rehearsal takes. Sometimes, if Crosby was having fun with a song and not really working at it, we had to make it up out of two or three parts. This ad lib way of working is commonplace in the recording studios today, but it was all new to us.
Crosby invested $50,000 in Ampex with an eye toward producing more machines. In 1948 the second season of Philco shows was taped with the new Ampex Model 200 tape recorder using the new Scotch 111 tape from the Minnesota Mining and Manufacturing (3M) company. Mullin explained how one new broadcasting technique was invented on the Crosby show with these machines:
One time Bob Burns, the hillbilly comic, was on the show, and he threw in a few of his folksy farm stories, which of course were not in Bill Morrow’s script. Today they wouldn’t seem very off-color, but things were different on radio then. They got enormous laughs, which just went on and on. We couldn’t use the jokes, but Bill asked us to save the laughs. A couple of weeks later he had a show that wasn’t very funny, and he insisted that we put in the salvaged laughs. Thus the laugh track was born.
Crosby had launched the tape recorder revolution in America. In his 1950 film "Mr. Music", Crosby is seen singing into one of the new Ampex tape recorders that reproduced his voice better than anything else. Also quick to adopt tape recording was his friend Bob Hope.
Mullin continued to work for Crosby to develop a videotape recorder (VTR). Television production was mostly live television in its early years, but Crosby wanted the same ability to record that he had achieved in radio. 1950’s "The Fireside Theater", sponsored by Procter & Gamble, was his first television production. Mullin had not yet succeeded with video tape, so Crosby filmed the series of 26-minute shows at the Hal Roach Studios, and the “telefilms” were syndicated to individual television stations.
Crosby continued to finance the development of videotape. Bing Crosby Enterprises (BCE), gave the world’s first demonstration of videotape recording in Los Angeles on November 11, 1951. Developed by John T. Mullin and Wayne R. Johnson since 1950, the device aired what were described as “blurred and indistinct” images, using a modified Ampex 200 tape recorder and standard quarter-inch (6.3 mm) audio tape moving at 360 inches (9.1 m) per second.
TV stations.
A Crosby-led group purchased KCOP-TV station in 1954. NAFI Corporation and Crosby together purchased the television station KPTV for $4 million on September 1, 1959. In 1960, NAFI purchased KCOP from Crosby's group.
Thoroughbred horse racing.
Crosby was a fan of thoroughbred horse racing and bought his first racehorse in 1935. In 1937 he became a founding partner of the Del Mar Thoroughbred Club and a member of its board of directors. Operating from the Del Mar Racetrack at Del Mar, California, the group included millionaire businessman Charles S. Howard, who owned a successful racing stable that included Seabiscuit. His son, Lindsay Howard, became one of Crosby’s closest friends; Crosby named his son Lindsay after him, and would purchase his 40-room Hillsborough estate from Lindsay in 1965.
Crosby and Lindsay Howard formed Binglin Stable to race and breed thoroughbred horses at a ranch in Moorpark, in Ventura County, California. They also established the Binglin stock farm in Argentina, where they raced horses at Hipódromo de Palermo in Palermo, Buenos Aires. A number of Argentine-bred horses were purchased and shipped to race in the United States. On August 12, 1938, the Del Mar Thoroughbred Club hosted a $25,000 winner-take-all match race won by Charles S. Howard’s Seabiscuit over Binglin’s horse Ligaroti. In 1943 Binglin’s horse Don Bingo won the Suburban Handicap at Belmont Park in Elmont, New York.
The Binglin Stable partnership came to an end in 1953 as a result of a liquidation of assets by Crosby, who needed to raise enough funds to pay the hefty federal and state inheritance taxes on his deceased wife’s estate. The Bing Crosby Breeders' Cup Handicap at Del Mar Racetrack is named in his honor.
Crosby was also a co-owner of the British colt Meadow Court, with jockey Johnny Longden’s friend Max Bell. Meadow Court won the 1965 King George VI and Queen Elizabeth Stakes, and the Irish Derby. In the Irish Derby’s winner's circle at the Curragh, Crosby sang “When Irish Eyes Are Smiling.”
Though Crosby’s stables had some success, he often joked about his horseracing failures as part of his radio appearances. “Crosby’s horse finally came in” became a running gag.
Sports.
Crosby had an interest in sports. In the 1930s, a friend and former college classmate, Gonzaga head coach Mike Pecarovich, appointed Crosby as an assistant football coach. From 1946 until the end of his life, he was part owner of baseball’s Pittsburgh Pirates. Although he was passionate about his team, he was too nervous to watch the deciding Game 7 of the 1960 World Series, choosing to go to Paris with Kathryn and listen to the game on the radio. Crosby had the NBC telecast of the game recorded on kinescope. The game was one of the most famous in baseball history, capped off by Bill Mazeroski’s walk-off home run. He apparently viewed the complete film just once, and then stored it in his wine cellar, where it remained undisturbed until it was discovered in December 2009. The restored broadcast was shown on MLB Network in December 2010.
Crosby was also an avid golfer, and in 1978, he and Bob Hope were voted the Bob Jones Award, the highest honor given by the United States Golf Association in recognition of distinguished sportsmanship. He is a member of the World Golf Hall of Fame. In 1937 Bing Crosby hosted the first National Pro-Am Golf Championship, the “Crosby Clambake” as it was popularly known, at Rancho Santa Fe Golf Club in Rancho Santa Fe, California, the event’s location prior to World War II. Sam Snead won the first tournament, in which the first place check was for $500. After the war, the event resumed play in 1947 on golf courses in Pebble Beach, where it has been played ever since. Now the AT&T Pebble Beach National Pro-Am, it has been a leading event in the world of professional golf.
Crosby first took up golf at 12 as a caddy, dropped it, and started again in 1930 with some fellow cast members in Hollywood during the filming of "The King of Jazz". Crosby was accomplished at the sport, with a two handicap. He competed in both the British and U.S. Amateur championships, was a five-time club champion at Lakeside Golf Club in Hollywood, and once made a hole-in-one on the 16th at Cypress Point.
Personal life.
Crosby was married twice. His first wife was actress/nightclub singer Dixie Lee, to whom he was married from 1930 until her death from ovarian cancer in 1952; they had four sons: Gary, twins Dennis, and Phillip, and Lindsay. The 1947 film "" is indirectly based on Lee’s life. After her death, Crosby had relationships with model/Goldwyn Girl Pat Sheehan (who married his son Dennis in 1958) and actresses Inger Stevens and Grace Kelly before marrying the actress Kathryn Grant, who converted to Catholicism, in 1957. They had three children: Harry Lillis III (who played Bill in "Friday the 13th"), Mary (best known for portraying Kristin Shepard, who shot J. R. Ewing on TV's "Dallas"), and Nathaniel.
Crosby was a registered Republican and actively campaigned for Wendell Willkie in 1940 against President Roosevelt, arguing that no man should serve more than two terms in the White House. After Willkie lost, Crosby decreed that he would never again make any open political contributions.
Crosby reportedly had an alcohol problem in his youth, and may have been dismissed from Paul Whiteman’s orchestra because of it, but he later got a handle on his drinking. According to Giddins, Crosby told his son Gary to stay away from alcohol, adding, “It killed your mother.”
After Crosby’s death, his eldest son, Gary, wrote a highly critical memoir, "Going My Own Way", depicting his father as cruel, cold, remote, and both physically and psychologically abusive.
Gary Crosby wrote:
We had to keep a close watch on our actions... When one of us left a sneaker or pair of underpants lying around, he had to tie the offending object on a string and wear it around his neck until he went off to bed that night. Dad called it the Crosby lavalier. At the time the humor of the name escaped me...
“Satchel Ass” or “Bucket Butt” or “My Fat-assed Kid.” That’s how he introduced me to his cronies when he dragged me along to the studio or racetrack... By the time I was 10 or 11 he had stepped up his campaign by adding lickings to the regimen. Each Tuesday afternoon he weighed me in, and if the scale read more than it should have, he ordered me into his office and had me drop my trousers... I dropped my pants, pulled down my undershorts and bent over. Then he went at it with the belt dotted with metal studs he kept reserved for the occasion. Quite dispassionately, without the least display of emotion or loss of self-control, he whacked away until he drew the first drop of blood, and then he stopped. It normally took between 12 and 15 strokes. As they came down I counted them off one by one and hoped I would bleed early...
When I saw "Going My Way" I was as moved as they were by the character he played. Father O’Malley handled that gang of young hooligans in his parish with such kindness and wisdom that I thought he was wonderful too. Instead of coming down hard on the kids and withdrawing his affection, he forgave them their misdeeds, took them to the ball game and picture show, taught them how to sing. By the last reel, the sheer persistence of his goodness had transformed even the worst of them into solid citizens. Then the lights came on and the movie was over. All the way back to the house I thought about the difference between the person up there on the screen and the one I knew at home.
However, younger son Phillip vociferously disputed his brother Gary’s claims about their father. Around the time Gary made his claim, Phillip stated to the press that “Gary is a whining...crybaby, walking around with a 2-by-4 and just daring people to nudge it off.” However, Phillip did not deny that Crosby believed in corporal punishment. In an interview with People, Phillip stated that “we never got an extra whack or a cuff we didn’t deserve.” During a later interview conducted in 1999 by the Globe, Phillip said:
My dad was not the monster my lying brother said he was; he was strict, but my father never beat us black and blue, and my brother Gary was a vicious, no-good liar for saying so. I have nothing but fond memories of Dad, going to studios with him, family vacations at our cabin in Idaho, boating and fishing with him. To my dying day, I'll hate Gary for dragging Dad’s name through the mud. He wrote "Going My Own Way" out of greed. He wanted to make money and knew that humiliating our father and blackening his name was the only way he could do it. He knew it would generate a lot of publicity. That was the only way he could get his ugly, no-talent face on television and in the newspapers. My dad was my hero. I loved him very much. He loved all of us too, including Gary. He was a great father.
Dennis and Lindsay Crosby confirmed that their father was physically abusive. Lindsay added, “I’m glad [Gary] did it. I hope it clears up a lot of the old lies and rumors.” Unlike Gary, however, Lindsay said that he preferred to remember “all the good things I did with my dad and forget the times that were rough.” Dennis asserted that the book was “Gary’s business” and a result of his “anger,” but he would not deny the book’s claims. Bing’s younger brother, Bob Crosby, recalled at the time of Gary’s revelations that Bing was a “disciplinarian,” as their mother and father had been. He added, “We were brought up that way.” In an interview for the same article, Gary clarified that Bing was abusive as a means of administering punishment: “He was not out to be vicious, to beat children for his kicks.”
It was revealed that Crosby’s will had established a blind trust, with none of the sons receiving an inheritance until they reached the age of 65.
Lindsay Crosby died in 1989 and Dennis Crosby died in 1991, both suicides from a self-inflicted gunshot wound, aged 51 and 56, respectively. Gary Crosby died in 1995 at the age of 62 of lung cancer and 69-year-old Phillip Crosby died in 2004 of a heart attack.
Nathaniel Crosby, Crosby's youngest son from his second marriage, was a high-level golfer who won the U.S. Amateur at age 19 in 1981, at the time the youngest-ever winner of that event. Harry Crosby is an investment banker who occasionally makes singing appearances.
Widow Kathryn Crosby dabbled in local theater productions intermittently, and appeared in television tributes to her late husband. Denise Crosby, Dennis Crosby’s daughter, is also an actress and is known for her role as Tasha Yar on "" and for the recurring role of the Romulan Sela (daughter of Tasha Yar) after her withdrawal from the series as a regular cast member. She also appeared in the film adaptation of Stephen King’s novel "Pet Sematary". In 2006 Crosby’s niece Carolyn Schneider published the laudatory book "Me and Uncle Bing".
Disputes between Crosby’s two families continue. When Dixie died in 1952, her will provided that her share of the community property be distributed in trust to her sons. After Crosby’s death in 1977, he left the residue of his estate to a marital trust for the benefit of his widow, Kathryn, and HLC Properties, Ltd., was formed for the purpose of managing his interests, including his right of publicity. In 1996 Dixie’s trust sued HLC and Kathryn for declaratory relief as to the trust’s entitlement to interest, dividends, royalties, and other income derived from the community property of Crosby and Dixie. In 1999 the parties settled for approximately $1.5 million. Relying on a retroactive amendment to the California Civil Code, Dixie’s trust brought suit again, in 2010, alleging that Crosby’s right of publicity was community property, and that Dixie’s trust was entitled to a share of the revenue it produced. The trial court granted Dixie’s trust’s claim. The California Court of Appeal reversed, however, holding that the 1999 settlement barred the claim. In light of the court’s ruling, it was unnecessary for the court to decide whether a right of publicity can be characterized as community property under California law.
Failing health and death.
Following his recovery from a life-threatening fungal infection of his right lung in 1974, Crosby emerged from semiretirement to start a new spate of albums and concerts. In March 1977, after videotaping a concert at the Ambassador Theater in Pasadena for CBS to commemorate his 50th anniversary in show business, and with Bob Hope looking on, Crosby fell off the stage into an orchestra pit, rupturing a disc in his back requiring a month in the hospital. His first performance after the accident was his last American concert, on August 16, 1977; when the power went out during his performance, he continued singing without amplification.
In September, Crosby, his family, and singer Rosemary Clooney began a concert tour of Britain that included two weeks at the London Palladium. While in the UK, Crosby recorded his final album, "Seasons", and his final TV Christmas special with guest David Bowie (which aired several months after Crosby's death). His last concert was in the Brighton Centre on October 10, four days before his death, with British entertainer Dame Gracie Fields in attendance. The following day he made his final appearance in a recording studio and sang eight songs at the BBC Maida Vale studios for a radio program, which also included an interview with Alan Dell. Accompanied by the Gordon Rose Orchestra, Crosby’s last recorded performance was of the song “Once in a While.” Later that afternoon, he met with Chris Harding to take photographs for the "Seasons" album jacket.
On October 13, Crosby flew alone to Spain to play golf and hunt partridge. On October 14 at the La Moraleja Golf Course near Madrid, Crosby played 18 holes of golf. His partner was World Cup champion Manuel Piñero; their opponents were club president Cesar de Zulueta and Valentin Barrios. According to Barrios, Crosby was in good spirits throughout the day, and played with a new set of Ben Hogan golf clubs with an old Hogan putter. He was photographed several times during the round. At the ninth hole, construction workers building a house nearby recognized him, and when asked for a song, Crosby sang “Strangers in the Night.” Crosby completed the round with a score of 85; he and his partner won by one stroke (with Crosby’s 13 handicap). As Crosby and his party headed back to the clubhouse, Crosby said, “That was a great game of golf, fellas.” Some 20 yards from the clubhouse entrance at about 6:30 p.m., Crosby collapsed on the red-brick path and died instantly from a massive heart attack. At the clubhouse and later in the ambulance, house physician Dr. Laiseca tried to revive him but was unsuccessful. At Rena Victoria Hospital he was administered the last rites of the Catholic Church and was pronounced dead. On October 18, following a private funeral mass at St. Paul's Catholic Church in Westwood, Crosby was buried at Holy Cross Cemetery in Culver City, California.
Legacy.
He is a member of the National Association of Broadcasters Hall of Fame in the radio division.
The family launched an official website on October 14, 2007, the 30th anniversary of Crosby’s death.
In his autobiography, "Don't Shoot, It's Only Me!", (1990), Bob Hope wrote, “Dear old Bing. As we called him, the "Economy-sized Sinatra". And what a voice. God I miss that voice. I can’t even turn on the radio around Christmas time without crying anymore.”
Calypso musician Roaring Lion wrote a tribute song in 1939 titled “Bing Crosby,” in which he wrote: “Bing has a way of singing with his very heart and soul / Which captivates the world / His millions of listeners never fail to rejoice / At his golden voice...”
Bing Crosby Stadium in Front Royal, Virginia, was named after Crosby in honor of his fundraising efforts and direct cash contributions for its construction in the 1948 to 1950 timeframe.
Compositions.
Crosby wrote or co-wrote lyrics to 17 songs. His composition “At Your Command” was No.1 for three weeks on the U.S. pop singles chart beginning on August 8, 1931. “I Don’t Stand a Ghost of a Chance With You” was his most successful composition, recorded by Duke Ellington, Frank Sinatra, Thelonious Monk, Billie Holiday, and Mildred Bailey, among others. Songs co-written by Crosby include
Grammy Hall of Fame.
Bing Crosby was posthumously inducted into the Grammy Hall of Fame, a special Grammy award established in 1973 to honor recordings that are at least 25 years old and that have “qualitative or historical significance.”

</doc>
<doc id="4011" url="http://en.wikipedia.org/wiki?curid=4011" title="Base">
Base

Base or BASE may refer to:

</doc>
<doc id="4012" url="http://en.wikipedia.org/wiki?curid=4012" title="Basel Convention">
Basel Convention

The Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and Their Disposal, usually known as the Basel Convention, is an international treaty that was designed to reduce the movements of hazardous waste between nations, and specifically to prevent transfer of hazardous waste from developed to less developed countries (LDCs). It does not, however, address the movement of radioactive waste. The Convention is also intended to minimize the amount and toxicity of wastes generated, to ensure their environmentally sound management as closely as possible to the source of generation, and to assist LDCs in environmentally sound management of the hazardous and other wastes they generate.
The Convention was opened for signature on 22 March 1989, and entered into force on 5 May 1992. As of February 2014, 180 states and the European Union are parties to the Convention. Haiti and the United States have signed the Convention but not ratified it.
History.
With the tightening of environmental laws (for example, RCRA) in developed nations in the 1970s, disposal costs for hazardous waste rose dramatically. At the same time, globalization of shipping made transboundary movement of waste more accessible, and many LDCs were desperate for foreign currency. Consequently, the trade in hazardous waste, particularly to LDCs, grew rapidly.
One of the incidents which led to the creation of the Basel Convention was the "Khian Sea" waste disposal incident, in which a ship carrying incinerator ash from the city of Philadelphia in the United States dumped half of its load on a beach in Haiti before being forced away. It sailed for many months, changing its name several times. Unable to unload the cargo in any port, the crew was believed to have dumped much of it at sea.
Another is the 1988 Koko case in which 5 ships transported 8,000 barrels of hazardous waste from Italy to the small town of Koko in Nigeria in exchange for $100 monthly rent which was paid to a Nigerian for the use of his farmland.
These practices have been deemed "Toxic Colonialism" by many developing countries.
At its most recent meeting, 27 November – 1 December 2006, the Conference of the parties of the Basel Agreement focused on issues of electronic waste and the dismantling of ships.
According to Maureen Walsh, only around 4% of hazardous wastes that come from OECD countries are actually shipped across international borders. These wastes include, among others, chemical waste, radioactive waste, municipal solid waste, asbestos, incinerator ash, and old tires. Of internationally shipped waste that comes from developed countries, more than half is shipped for recovery and the remainder for final disposal. 
Increased trade in recyclable materials has led to an increase in a market for used products such as computers. This market is valued in billions of dollars. At issue is the distinction when used computers stop being a "commodity" and become a "waste".
As of 2014, there are 181 parties to the treaty. The UN member states that are not party to the treaty are Angola, Burma, East Timor, Fiji, Grenada, Haiti, San Marino, Sierra Leone, Solomon Islands, South Sudan, Tajikistan, Tuvalu, United States, and Vanuatu.
Definition of "hazardous waste".
A waste falls under the scope of the Convention if it is within the category of wastes listed in Annex I of the Convention and it exhibits one of the hazardous characteristics contained in Annex III. 
In other words it must both be listed and possess a characteristic such as being explosive, flammable, toxic, or corrosive. The other way that a waste may fall under the scope of the Convention is if it is defined as or considered to be a hazardous waste under the laws of either the exporting country, the importing country, or any of the countries of transit.
The definition of the term disposal is made in Article 2 al 4 and just refers to annex IV, which gives a list of operations which are understood as disposal or recovery. The examples of disposal are broad and include also recovery, recycling.
Annex II lists other wastes, such as household wastes and residue that comes from incinerating household waste. 
Radioactive waste that is covered under other international control systems and wastes from the normal operation of ships are not covered.
Annex IX attempts to define "commodities" which are not considered wastes and which would be excluded.
Obligations.
In addition to conditions on the import and export of the above wastes, there are stringent requirements for notice, consent and tracking for movement of wastes across national boundaries. It is of note that the Convention places a general prohibition on the exportation or importation of wastes between Parties and non-Parties. The exception to this rule is where the waste is subject to another treaty that does not take away from the Basel Convention. The United States is a notable non-Party to the Convention and has a number of such agreements for allowing the shipping of hazardous wastes to Basel Party countries. 
The OECD Council also has its own control system that governs the trans-boundary movement of hazardous materials between OECD member countries. This allows, among other things, the OECD countries to continue trading in wastes with countries like the United States that have not ratified the Basel Convention. 
Parties to the Convention must honor import bans of other Parties.
Article 4 of the Basel Convention calls for an overall reduction of waste generation. By encouraging countries to keep wastes within their boundaries and as close as possible to its source of generation, the internal pressures should provide incentives for waste reduction and pollution prevention. Parties are generally prohibited from exporting covered wastes to, or import covered waste from, non-parties to the convention. 
The Convention states that illegal hazardous waste traffic is criminal but contains no enforcement provisions.
According to Article 12, Parties are directed to adopt a protocol that establishes liability rules and procedures that are appropriate for damage that comes from the movement of hazardous waste across borders.
Basel Ban Amendment.
After the initial adoption of the Convention, some least developed countries and environmental organizations argued that it did not go far enough. Many nations and NGOs argued for a total ban on shipment of all hazardous waste to LDCs. In particular, the original Convention did not prohibit waste exports to any location except Antarctica but merely required a notification and consent system known as "prior informed consent" or PIC. Further, many waste traders sought to exploit the good name of recycling and begin to justify all exports as moving to recycling destinations. Many believed a full ban was needed including exports for recycling. These concerns led to several regional waste trade bans, including the Bamako Convention. 
Lobbying at the 1995 Basel conference by LDCs, Greenpeace and several European countries such as Denmark, led to the adoption of an amendment to the convention in 1995 termed the Basel Ban Amendment to the Basel Convention. The amendment has been accepted by 79 countries and the European Union, but has not entered into force (as that requires ratification by 3/4 of the member states to the Convention). The Amendment prohibits the export of hazardous waste from a list of developed (mostly OECD) countries to developing countries. The Basel Ban applies to export for any reason, including recycling. An area of special concern for advocates of the Amendment was the sale of ships for salvage, shipbreaking. The Ban Amendment was strenuously opposed by a number of industry groups as well as nations including Australia and Canada. The number of ratification for the entry-into force of the Ban Amendment is under debate: Amendments to the convention enter into force after ratification of "three-fourths of the Parties who accepted them" [Art. 17.5]; so far, the Parties of the Basel Convention could not yet agree whether this would be three fourth of the Parties that were Party to the Basel Convention when the Ban was adopted, or three fourth of the current Parties of the Convention[see Report of COP 9 of the Basel Convention]. The status of the amendment ratifications can be found on the Basel Secretariat's . The European Union fully implemented the Basel Ban in its Waste Shipment Regulation (EWSR), making it legally binding in all EU member states. Norway and Switzerland have similarly fully implemented the Basel Ban in their legislation.
In the light of the blockage concerning the entry into force of the Ban amendment, Switzerland and Indonesia have launched a “Country-led Initiative” (CLI) to discuss in an informal manner a way forward to ensure that the transboundary movements of hazardous wastes, especially to developing countries and countries with economies in transition, do not lead to an unsound management of hazardous wastes. This discussion aims at identifying and finding solutions to the reasons why hazardous wastes are still brought to countries that are not able to treat them in a safe manner. It is hoped that the CLI will contribute to the realization of the objectives of the Ban Amendment. The Basel Convention's website informs about the progress of this initiative 

</doc>
<doc id="4013" url="http://en.wikipedia.org/wiki?curid=4013" title="Bar Kokhba (album)">
Bar Kokhba (album)

Bar Kokhba is a double album by John Zorn, recorded between 1994 and 1996. It features music from Zorn's "Masada" project, rearranged for small ensembles.
Reception.
The AllMusic review by Marc Gilman awarded the album 4½ stars noting that "While some compositions retain their original structure and sound, some are expanded and probed by Zorn's arrangements, and resemble avant-garde classical music more than jazz. But this is the beauty of the album; the ensembles provide a forum for Zorn to expand his compositions. The album consistently impresses". 
Track listing.
"All compositions by John Zorn"
See also.
Masada (band)

</doc>
<doc id="4015" url="http://en.wikipedia.org/wiki?curid=4015" title="BASIC">
BASIC

BASIC (an acronym for Beginner's All-purpose Symbolic Instruction Code) is a family of general-purpose, high-level programming languages whose design philosophy emphasizes ease of use.
In 1964, John G. Kemeny and Thomas E. Kurtz designed the original BASIC language at Dartmouth College in New Hampshire. They wanted to enable students in fields other than science and mathematics to use computers. At the time, nearly all use of computers required writing custom software, which was something only scientists and mathematicians tended to learn.
Versions of BASIC became widespread on microcomputers in the mid-1970s and 1980s. Microcomputers usually shipped with BASIC, often in the machine's firmware. Having an easy-to-learn language on these early personal computers allowed small business owners, professionals, hobbyists, and consultants to develop custom software on computers they could afford.
BASIC remains popular in many dialects and in new languages influenced by BASIC, such as Microsoft's Visual Basic. In 2006, 59% of developers for the .NET Framework used Visual Basic .NET as their only programming language.
History.
Before the mid-1960s, computers were extremely expensive mainframe machines, usually requiring a dedicated computer room and air-conditioning, used by large organizations for scientific and commercial tasks. Users submitted jobs, on punched cards or similar media, to computer operators, and usually collected the output later. A simple batch processing arrangement ran only a single "job" at a time, one after another. During the 1960s faster and more affordable computers, still mainframes, became available, and time-sharing—a technique which allows multiple users or processes to share use of the CPU and memory—was developed. In such a system the operating system gives each of several processes time on the CPU, then pauses it and switches to another; each process behaves as if it had full use of the computer, although the time to complete its operation increases. Time-sharing was initially used to allow several batched processes to execute simultaneously.
Time-sharing also allowed several independent users to interact with a computer, working on terminals with keyboards and teletype printers, and later display screens. Computers were fast enough to respond quickly to each user.
The need to optimize interactive time-sharing, using command line interpreters and programming languages, was an area of intense research during the 1960s and 1970s.
Origin.
The original BASIC language was designed on May 1, 1964 by John Kemeny and Thomas Kurtz and implemented by a team of Dartmouth students under their direction. The acronym "BASIC" comes from the name of an unpublished paper by Thomas Kurtz. BASIC was designed to allow students to write mainframe computer programs for the Dartmouth Time-Sharing System. It was intended specifically for less technical users who did not have or want the mathematical background previously expected. Being able to use a computer to support teaching and research was quite novel at the time.
The language was based on FORTRAN II, with some influences from ALGOL 60 and with additions to make it suitable for timesharing. Initially, BASIC concentrated on supporting straightforward mathematical work, with matrix arithmetic support from its initial implementation as a batch language, and character string functionality being added by 1965.
The designers of the language decided to make the compiler available free of charge so that the language would become widespread. (In the 1960s software became a chargeable commodity; until then it was provided without charge as a service with the very expensive computers, usually available only to lease.) They also made it available to high schools in the Hanover area, and put a considerable amount of effort into promoting the language. In the following years, as other dialects of BASIC appeared, Kemeny and Kurtz's original BASIC dialect became known as "Dartmouth BASIC".
Spread on minicomputers.
Knowledge of the relatively simple BASIC became widespread for a computer language, and it was implemented by a number of manufacturers, becoming fairly popular on newer minicomputers such as the DEC PDP series and the Data General Nova. The BASIC language was also central to the HP Time-Shared BASIC system in the late 1960s and early 1970s, where the language was implemented as an interpreter. Also at this time it was ported into the Pick operating system where a compiler renders it into bytecode, able to be interpreted by a virtual machine.
During this period a number of simple computer games were written in BASIC, most notably Mike Mayfield's "Star Trek". A number of these were collected by DEC employee David H. Ahl and published in a newsletter he compiled. He later collected a number of these into book form, "101 BASIC Computer Games", which was first published in 1973. During the same period, Ahl was involved in the creation of a small computer for education use, an early personal computer. When management refused to support the concept, Ahl left DEC in 1974 to found the seminal computer magazine, "Creative Computing". The book remained popular, and was re-published on several occasions.
Explosive growth: the home computer era.
The introduction of the first microcomputers in the mid-1970s was the start of explosive growth for BASIC. It had the advantage that it was fairly well known to the young designers and computer hobbyists who took an interest in microcomputers.
One of the first to appear was Tiny BASIC, a simple BASIC variant designed by Dennis Allison at the urging of Bob Albrecht of the Homebrew Computer Club. He had seen BASIC on minicomputers and felt it would be the perfect match for new machines like the MITS Altair 8800. How to design and implement a stripped-down version of an interpreter for the BASIC language was covered in articles by Allison in the first three quarterly issues of the "People's Computer Company" newsletter published in 1975 and implementations with source code published in . Versions were written by Li-Chen Wang and Tom Pittman.
In 1975 MITS released Altair BASIC, developed by Bill Gates and Paul Allen as the company Micro-Soft, which eventually grew into corporate giant Microsoft. The first Altair version was co-written by Gates, Allen, and Monte Davidoff.
Almost universally, home computers of the 1980s had a ROM-resident BASIC interpreter, which the machines booted directly into. When the Apple II, PET 2001 and TRS-80 were all released in 1977, all three had BASIC as their primary programming language and operating environment. Upon boot, a BASIC interpreter in immediate mode was presented, not the command line interface used later. Commodore Business Machines included a version of Micro-Soft BASIC. The Apple II and TRS-80 each had two versions of BASIC, a smaller introductory version introduced with the initial releases of the machines and a more advanced version developed as interest in the platforms increased. As new companies entered the field, additional versions were added that subtly changed the BASIC family. The Atari 8-bit family had their own Atari BASIC that was modified in order to fit on an 8 kB ROM cartridge. The BBC published BBC BASIC, developed for them by Acorn Computers Ltd, incorporating many extra structuring keywords and advanced floating-point operation features.
As the popularity of BASIC grew in this period, magazines (such as "Creative Computing" in the U.S.) published complete source code in BASIC for games, utilities, and other programs. Given BASIC's straightforward nature, it was a simple matter to type in the code from the magazine and execute the program. Different magazines were published featuring programs for specific computers, though some BASIC programs were considered universal and could be used in machines running any variant of BASIC (sometimes with minor adaptations). Many books of type-in programs were also available, and in particular, Ahl published versions of the original 101 BASIC games converted into the Microsoft dialect and published it from "Creative Computing" as "BASIC Computer Games". This book, and its sequels, provided hundreds of ready-to-go programs that could be easily converted to practically any BASIC-running platform. The book reached the stores in 1978, just as the home computer market was starting off, and it became the first million-selling computer book. Later packages, such as Learn to Program BASIC would also have gaming as an introductory focus.
On the business-focused CP/M computers which soon became widespread in small business environments, Microsoft BASIC (MBASIC) was one of the leading applications.
IBM PC, and compatibles.
When IBM was designing the IBM PC they followed the paradigm of existing home-computers in wanting to have a built-in BASIC. They sourced this from Microsoft - IBM Cassette BASIC - but Microsoft also produced several other versions of BASIC for MS-DOS/PC DOS including IBM Disk BASIC (BASIC D), IBM BASICA (BASIC A), GW-BASIC (a BASICA-compatible version that did not need IBM's ROM) and QuickBASIC, all typically bundled with the machine. In addition they produced the Microsoft BASIC Compiler aimed at professional programmers.
Turbo Pascal-publisher Borland published Turbo Basic 1.0 in 1985 (successor versions are still being marketed by the original author under the name PowerBASIC). Microsoft wrote the windowing-based AmigaBASIC that was supplied with version 1.1 of the pre-emptive multitasking GUI Amiga computers (late 1985 / early 1986), although the product unusually did not bear any Microsoft marks.
These languages introduced many extensions to the original home-computer BASIC, such as improved string manipulation and graphics support, access to the file system and additional data types. More important were the facilities for structured programming, including additional control structures and proper subroutines supporting local variables.
However, by the latter half of the 1980s users were increasingly using applications written by others, rather than learning programming themselves, while professional programmers now had a wide range of more advanced languages available and BASIC tended to become the butt of derogatory comments such as Dijkstra's famous comment: "It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration."
Visual Basic.
BASIC's fortunes reversed once again with the introduction in 1991 of Visual Basic ("VB"), by Microsoft. This was an evolutionary development of QuickBasic, and included constructs from other languages such as block structured control statements including With and For Each, parameterized subroutines, optional static typing, and more recently a full object oriented language. But the language retains considerable links to its past, such as the Dim statement for declarations, Gosub/Return statements, and even line numbers which are still needed to report errors properly.
An important driver for the development of Visual Basic was as the new macro language for Excel.
Ironically, given the origin of BASIC as a "beginner's" language, and apparently even to the surprise of many at Microsoft who still initially marketed it as a language for hobbyists, the language had come into widespread use for small custom business applications shortly after the release of VB version 3.0, which is widely considered the first relatively stable version. While many advanced programmers still scoffed at its use, VB met the needs of small businesses efficiently wherever processing speed was less of a concern than ease of development. By that time, computers running Windows 3.1 had become fast enough that many business-related processes could be completed "in the blink of an eye" even using a "slow" language, as long as large amounts of data were not involved. Many small business owners found they could create their own small, yet useful applications in a few evenings to meet their own specialized needs. Eventually, during the lengthy lifetime of VB3, knowledge of Visual Basic had become a marketable job skill.
Microsoft also produced VBScript in 1996 and Visual Basic .NET in 2001. The latter has essentially the same power as C# and Java but with syntax that reflects the original Basic language.
Recent versions.
Many other BASIC dialects have also sprung up since 1990, including the open source QB64, Bywater BASIC, Gambas and FreeBASIC - and the commercial PureBasic, PowerBASIC, RealBasic, and True BASIC (the direct successor to Dartmouth BASIC from a company controlled by Kurtz).
Several web-based simple BASIC interpreters also now exist, including and Microsoft's Small Basic (educational software).
Versions of BASIC have been showing up for use on smart phones and tablets. Apple App Store contains such implementations of BASIC programming language as smart BASIC, Basic!, HotPaw Basic, BASIC-II, techBASIC and others. Android devices feature such implementations of BASIC as RFO BASIC and Mintoris Basic.
Applications for some mobile computers with proprietary OS (CipherLab) can be built with programming environment based on BASIC.
An application for the Nintendo 3DS and Nintendo DSi called Petit Computer allows for programming in a slightly modified version of BASIC with DSI button support.
Nostalgia.
The ubiquity of BASIC interpreters on personal computers was such that textbooks once included simple "Try It In BASIC" exercises that encouraged students to experiment with mathematical and computational concepts on classroom or home computers. Popular computer magazines of the day typically included type-in programs. Futurist and sci-fi writer David Brin mourned the loss of ubiquitous BASIC in a 2006 "Salon" article as have others who first used computers during this era. In turn, the article prompted Microsoft to develop and release Small Basic.
Syntax.
Data types and variables.
Minimal versions of BASIC had only integer variables and one- or two-letter variable names, which minimised requirements of limited and expensive memory (RAM). More powerful versions had floating-point arithmetic, and variables could be labelled with names six or more characters long. There were some problems and restrictions in early implementations; for example, Applesoft allowed variable names to be several characters long, but only the first two were significant, thus it was possible to inadvertently write a program with variables "LOSS" and "LOAN", which would be treated as being the same; assigning a value to "LOAN" would silently overwrite the value intended as "LOSS". Keywords could not be used in variables in many early BASICs; "SCORE" would be interpreted as "SC" OR "E", where OR was a keyword.
String variables are usually distinguished in many microcomputer dialects by having $ suffixed to their name, and values are often identified as strings by being delimited by "double quotation marks".
Arrays in BASIC could contain integers, floating point or string variables.
Some dialects of BASIC supported matrices and matrix operations, useful for the solution of sets of simultaneous linear algebraic equations. These dialects would directly support matrix operations such as assignment, addition, multiplication (of compatible matrix types), and evaluation of a determinant. Many microcomputer BASICs did not support this data type; matrix operations were still possible, but had to be programmed explicitly on array elements.
Examples.
The original Dartmouth Basic was unusual in having a matrix keyword, MAT. Although dropped by most later microprocessor derivatives it is used in this example from the 1968 manual which averages the numbers that are input:
5 LET S = 0 
10 MAT INPUT V 
20 LET N = NUM 
30 IF N = 0 THEN 99 
40 FOR I = 1 TO N 
45 LET S = S + V(I) 
50 NEXT I 
60 PRINT S/N 
70 GO TO 5 
99 END
New BASIC programmers on a home computer might start with a simple program, perhaps using the language's PRINT statement to display a message on the screen; a well-known and often-replicated example is Kernighan and Ritchie's Hello world program:
10 PRINT "Hello, World!"
140 END
An infinite loop could be used to fill the display with the message.
Most first-generation BASIC versions such as MSX BASIC and GW-BASIC supported simple data types, loop cycles and arrays. The following example is written for GW-BASIC, but will work in most versions of BASIC with minimal changes:
10 INPUT "What is your name: ", U$
20 PRINT "Hello "; U$
30 INPUT "How many stars do you want: ", N
40 S$ = ""
50 FOR I = 1 TO N
60 S$ = S$ + "*"
70 NEXT I
80 PRINT S$
90 INPUT "Do you want more stars? ", A$
100 IF LEN(A$) = 0 THEN GOTO 90
110 A$ = LEFT$(A$, 1)
120 IF A$ = "Y" OR A$ = "y" THEN GOTO 30
130 PRINT "Goodbye "; U$
140 END
The resulting dialog might resemble:
 What is your name: Mike
 Hello Mike
 How many stars do you want: 7
 Do you want more stars? yes
 How many stars do you want: 3
 Do you want more stars? no
 Goodbye Mike
Second-generation BASICs (for example, True BASIC, QuickBASIC and PowerBASIC) introduced a number of features into the language, primarily related to structured and procedure-oriented programming. Usually, line numbering is omitted from the language and replaced with labels (for GOTO) and procedures to encourage easier and more flexible design.
DECLARE SUB PrintSomeStars (StarCount!)
REM QuickBASIC example
INPUT "What is your name: ", UserName$
PRINT "Hello "; UserName$
DO
 INPUT "How many stars do you want: ", NumStars
 CALL PrintSomeStars(NumStars)
 DO
 INPUT "Do you want more stars? ", Answer$
 LOOP UNTIL Answer$ <> ""
 Answer$ = LEFT$(Answer$, 1)
LOOP WHILE UCASE$(Answer$) = "Y"
PRINT "Goodbye "; UserName$
SUB PrintSomeStars (StarCount)
 REM This procedure uses a local variable called Stars$
 Stars$ = STRING$(StarCount, "*")
 PRINT Stars$
END SUB
Third-generation BASIC dialects such as Visual Basic, REALbasic, StarOffice Basic and BlitzMax introduced features to support object-oriented and event-driven programming paradigm. Most built-in procedures and functions are now represented as "methods" of standard objects rather than "operators".
The following example is in Visual Basic .NET:
Public Class StarsProgram
 Public Shared Sub Main()
 Dim UserName, Answer, stars As String, NumStars As Integer
 Console.Write("What is your name: ")
 UserName = Console.ReadLine()
 Console.WriteLine("Hello {0}", UserName)
 Do
 Console.Write("How many stars do you want: ")
 NumStars = CInt(Console.ReadLine())
 stars = New String("*", NumStars)
 Console.WriteLine(stars)
 Do
 Console.Write("Do you want more stars? ")
 Answer = Console.ReadLine()
 Loop Until Answer <> ""
 Answer = Answer.Substring(0, 1)
 Loop While Answer.ToUpper() = "Y"
 Console.WriteLine("Goodbye {0}", UserName)
 End Sub
End Class

</doc>
<doc id="4016" url="http://en.wikipedia.org/wiki?curid=4016" title="List of Byzantine emperors">
List of Byzantine emperors

This is a list of the Byzantine emperors from the foundation of Constantinople in 330 AD, which marks the conventional start of the Eastern Roman or Byzantine Empire, to its fall to the Ottoman Empire in 1453 AD. Only the emperors who were recognized as legitimate rulers and exercised sovereign authority are included, to the exclusion of junior co-emperors ("symbasileis") who never attained the status of sole or senior ruler, as well as of the various usurpers or rebels who claimed the imperial title.
Traditionally, the line of Byzantine emperors is held to begin with the Roman Emperor Constantine the Great, the first Christian emperor, who rebuilt the city of Byzantium as an imperial capital, Constantinople, and who was regarded by the later Byzantine emperors as the model ruler. It was under Constantine that the major characteristics of what is considered the Byzantine state emerged: a Roman polity centered at Constantinople and culturally dominated by the Greek East, with Christianity as the state religion.
All Byzantine emperors considered themselves "Roman Emperors," the term "Byzantine" was coined by Western historiography only in the 16th century. The use of the title "Roman Emperor" was not contested until after the Papal coronation of the Frankish Charlemagne as "Holy Roman Emperor" (25 December 800 AD), done partly in response to the Byzantine coronation of Empress Irene, whose claim, as a woman, was not recognized by Pope Leo III.
The title of all Emperors preceding Heraclius was officially ""Augustus"," although other titles such as "Dominus" were also used. Their names were preceded by "Imperator Caesar" and followed by "Augustus". Following Heraclius, the title commonly became the Greek "Basileus" (Gr. Βασιλεύς), which had formerly meant sovereign but was then used in place of "Augustus". Following the establishment of the rival Holy Roman Empire in Western Europe, the title "Autokrator" (Gr. Αὐτοκράτωρ) was increasingly used. In later centuries, the Emperor could be referred to by Western Christians as the "Emperor of the Greeks." Towards the end of the Empire, they referred to themselves as "[Emperor's name] in Christ, Emperor and Autocrat of the Romans."
In the medieval period, dynasties were common, but the principle of hereditary succession was never formalized in the Empire, and hereditary succession was a custom rather than an inviolable principle.
Including the Palaiologan dynasty, claimed Byzantine Emperors in exile, there were a total of 99 Emperors of the thousand-year-old Eastern Roman Empire.

</doc>
<doc id="4024" url="http://en.wikipedia.org/wiki?curid=4024" title="Butterfly effect">
Butterfly effect

In chaos theory, the butterfly effect is the "sensitive dependency on initial conditions" in which a small change at one place in a deterministic nonlinear system can result in large differences in a later state. The name of the effect, coined by Edward Lorenz, is derived from the theoretical example of the details of a hurricane (exact time of formation, exact path taken) being influenced by minor perturbations equating to the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.
Although the butterfly effect may appear to be an unlikely behavior, it is exhibited by very simple systems. For example, a ball placed at the crest of a hill may roll into any surrounding valley depending on, among other things, slight differences in its initial position. Also, the randomness of throwing dice depends on this system's characteristic to amplify small differences in initial conditions - the throw - into significantly different dice paths and outcome, which makes it virtually impossible to throw dice exactly the same way twice. 
The butterfly effect is a common trope in fiction, especially in scenarios involving time travel. Additionally, works of fiction that involve points at which the storyline diverges during a seemingly minor event, resulting in a significantly different outcome than would have occurred without the divergence, are an example of the butterfly effect.
History.
Chaos theory and the sensitive dependence on initial conditions was described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.
In 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908. The idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events first appears in "A Sound of Thunder", a 1952 short story by Ray Bradbury about time travel (see Literature and print here).
In 1961, Lorenz was using a numerical computer model to rerun a weather prediction, when, as a shortcut on a number in the sequence, he entered the decimal 0.506 instead of entering the full 0.506127. The result was a completely different weather scenario. In 1963 Lorenz published a theoretical study of this effect in a well-known paper called "Deterministic Nonperiodic Flow". (As noted in the paper, the calculations were performed on a Royal McBee LGP-30 computing machine.) Elsewhere he said that "One meteorologist remarked that if the theory were correct, one flap of a sea gull's wings would be enough to alter the course of the weather forever. The controversy has not yet been settled, but the most recent evidence seems to favor the sea gulls." Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted "Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.
The phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. Note that the butterfly does not power or directly create the tornado. The Butterfly effect does not convey the notion - as is often misconstrued - that the flap of the butterfly's wings "causes" the tornado. The flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which causes a chain of events leading to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different - it's possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.
The butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.
Some scientists have since argued that the weather system is not as sensitive to initial condition as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.
Theory and mathematical definition.
Recurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.
A dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.
If "M" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in "M" and any δ > 0, there are y in "M", with formula_3 such that
The definition does not require that all points from a neighborhood separate from the base point "x", but it requires one positive Lyapunov exponent.
Examples.
The butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example.
The potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.
Other authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which "measures the rate at which identical initial states diverge when subjected to slightly different dynamics". They consider fidelity decay to be "the closest quantum analog to the (purely classical) butterfly effect". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.

</doc>
<doc id="4027" url="http://en.wikipedia.org/wiki?curid=4027" title="Borland">
Borland

Borland Software Corporation is a software company that facilitates software deployment projects. Borland was first headquartered in Scotts Valley, California, then in Cupertino, California, and now in Austin, Texas. It is now a Micro Focus subsidiary. It was founded in 1983 by Niels Jensen, Ole Henriksen, Mogens Glad and Philippe Kahn.
History.
The 1980s: Foundations.
Three Danish citizens, Niels Jensen, Ole Henriksen, and Mogens Glad, founded Borland Ltd. in August 1981 to develop products like Word Index for the CP/M operating system using an off-the-shelf company. However, response to the company's products at the CP/M-82 show in San Francisco showed that a U.S. company would be needed to reach the American market. They met Philippe Kahn, who had just moved to Silicon Valley, and who had been a key developer of the Micral. The three Danes had embarked, at first successfully, on marketing software first from Denmark, and later from Ireland, before running into some challenges at the time when they met Philippe Kahn. The partnership seems to have benefited all involved. Philippe Kahn was at all times Chairman, President, and CEO of Borland Inc. from its inception in 1983 until he left in 1995. Main shareholders at the incorporation of Borland were Niels Jensen (250,000 shares), Ole Henriksen (160,000), Mogens Glad (100,000), and Philippe Kahn (80,000).
Borland developed a series of well-regarded software development tools. Its first product was Turbo Pascal in 1983, developed by Anders Hejlsberg (who later developed .NET and C# for Microsoft) and before Borland acquired the product sold in Scandinavia under the name of Compas Pascal. 1984 saw the launch of Borland Sidekick, a time organization, notebook, and calculator utility that was an early and popular terminate and stay resident program (TSR) for DOS operating systems.
By 1985 the company had become so successful that it had the largest exhibit at the 1985 West Coast Computer Faire other than IBM or AT&T. After Turbo Pascal and Sidekick it successfully launched other applications such as SuperKey and Lightning, all developed in Denmark. While the Danes remained majority shareholders, board members included Philippe Kahn, Tim Berry, John Nash, and David Heller. With the assistance of John Nash and David Heller, both British members of the Borland Board, the company was taken public on London's Unlisted Securities Market (USM) in 1986. Schroders was the lead investment banker. According to the London IPO filings, the management team was Philippe Kahn as President, Spencer Ozawa as VP of Operations, Marie Bourget as CFO, and Spencer Leyton as VP of sales and business development, while all software development was continuing to take place in Denmark and later London as the Danish co-founders moved there. A first US IPO followed in 1989 after Ben Rosen joined the Borland board with Goldman Sachs as the lead banker and a second offering in 1991 with Lazard as the lead banker. All offerings were very successful and over-subscribed.
In 1985 Borland acquired Analytica and its Reflex database product. The engineering team of Analytica, managed by Brad Silverberg and including Reflex co-founder Adam Bosworth, became the core of Borland's engineering team in the USA. Brad Silverberg was VP of engineering until he left in early 1990 to head up the Personal Systems division at Microsoft. Adam Bosworth initiated and headed up the Quattro project until moving to Microsoft later in 1990 to take over the project which eventually became Access.
In 1987 Borland purchased Wizard Systems and incorporated portions of the Wizard C technology into Turbo C. Bob Jervis, the author of Wizard C became a Borland employee. Turbo C was released on May 18, 1987, and an estimated 100,000 copies were shipped in the first month of its release. This apparently drove a wedge between Borland and Niels Jensen and the other members of his team who had been working on a brand new series of compilers at their London development centre. An agreement was reached and they spun off a company called Jensen & Partners International(JPI), later TopSpeed. JPI first launched a MS-DOS compiler named JPI Modula-2, that later became TopSpeed Modula-2, and followed up with TopSpeed C, TopSpeed C++ and TopSpeed Pascal compilers for both the MS-DOS and OS/2 operating systems. The TopSpeed compiler technology exists today as the underlying technology of the Clarion 4GL programming language, a Windows development tool.
In September 1987 Borland purchased Ansa-Software, including their Paradox (version 2.0) database management tool. Richard Schwartz, a cofounder of Ansa, became Borland's CTO and Ben Rosen joined the Borland board.
The Quattro Pro spreadsheet was launched in 1989 with, at the time, a notable improvement and charting capabilities. Lotus Development, under the leadership of Jim Manzi sued Borland for copyright infringement (see Look and feel). The litigation, "Lotus Dev. Corp. v. Borland Int'l, Inc.", brought forward Borland's open standards position as opposed to Lotus' closed approach. Borland, under Kahn's leadership took a position of principle and announced that they would defend against Lotus' legal position and "fight for programmer's rights". After a decision in favor of Borland by the First Circuit Court of Appeals, the case went to the United States Supreme Court. Because Justice John Paul Stevens had recused himself, only eight Justices heard the case, and it ended in a 4–4 tie. As a result, the First Circuit decision remained standing, but the Supreme Court result, being a tie, did not bind any other court and set no national precedent.
Additionally, Borland was known for its practical and creative approach towards software piracy and intellectual property (IP), introducing its "Borland no-nonsense license agreement". This allowed the developer/user to utilize its products "just like a book"; he or she was allowed to make multiple copies of a program, as long as only one copy was in use at any point in time.
The 1990s: Rise and change.
In September 1991 Borland purchased Ashton-Tate, bringing the dBase and InterBase databases to the house, in an all stock transaction. Competition with Microsoft was fierce. Microsoft launched the competing database Microsoft Access and bought the dBase clone FoxPro in 1992, undercutting Borland's prices. During the early 1990s Borland's implementation of C and C++ outsold Microsoft's. Borland survived as a company, but no longer had the dominance in software tools that it once had. It has gone through a radical transition in products, financing, and staff, now a very different company from the one which challenged Microsoft and Lotus in the early 1990s.
The internal problems that arose with the Ashton-Tate merger were a large part of the fall. Ashton-Tate's product portfolio proved to be weak, with no provision for evolution into the GUI environment of Windows. Almost all product lines were discontinued. The consolidation of duplicate support and development offices was costly and disruptive. Worst of all, the highest revenue earner of the combined company was dBASE with no Windows version ready. Borland had an internal project to clone dBASE which was intended to run on Windows and was part of the strategy of the acquisition, but by late 1992 this was abandoned due to technical flaws and the company had to constitute a replacement team (the ObjectVision team, redeployed) headed by Bill Turpin to redo the job. Borland lacked the financial strength to project its marketing and move internal resources off other products to shore up the dBASE/W effort. Layoffs occurred in 1993 to keep the company afloat, the third instance of this in five years. By the time dBASE for Windows eventually shipped, the developer community had moved on to other products such as Clipper or FoxBase and dBASE never regained significant share of Ashton-Tate's former market. This happened against the backdrop of the rise in Microsoft's combined Office product marketing.
A change in market conditions also contributed to Borland's fall from prominence. In the 1980s, companies had few people who understood the growing personal computer phenomenon, and so most technical people were given free rein to purchase whatever software they thought they needed. Borland had done an excellent job marketing to those with a highly technical bent. By the mid-1990s, however, companies were beginning to ask what the return was on the investment they had made in this loosely controlled PC software buying spree. Company executives were starting to ask questions that were hard for technically minded staff to answer, and so corporate standards began to be created. This required new kinds of marketing and support materials from software vendors, but Borland remained focused on the technical side of its products.
During 1993 Borland explored ties with WordPerfect as a possible way to form a suite of programs to rival Microsoft's nascent integration strategy. WordPerfect itself was struggling with a late and troubled transition to Windows. The eventual joint company effort, named Borland Office for Windows (a combination of the WordPerfect word processor, Quattro Pro spreadsheet and Paradox database) was introduced at the 1993 Comdex computer show. Borland Office never made significant in-roads against Microsoft Office. WordPerfect was then bought by Novell. In October 1994, Borland sold Quattro Pro and rights to sell up to million copies of Paradox to Novell for $140 million in cash, repositioning the company on its core software development tools and the Interbase database engine and shifting toward client-server scenarios in corporate applications. This later proved a good foundation for the shift to web development tools.
Philippe Kahn and the Borland board came to a disagreement on how to focus the company, and Philippe Kahn resigned as Chairman, CEO and President of Borland, a position he had held for 12 years, in January 1995. However, the parting was amicable as Kahn remained on the Borland board until November 7, 1996, when he resigned from that position. Borland named Gary Wetsel as CEO, but he resigned in July 1996. William F. Miller was interim CEO until September of that year, when Whitney G. Lynn became interim president and CEO and then continued to have a succession of CEOs including Dale Fuller and Tod Nielsen.
The Delphi 1 rapid application development (RAD) environment was launched in 1995, under the leadership of Anders Hejlsberg.
In 1996 Borland acquired Open Environment Corp (OEC), a Cambridge based company founded by John J. Donovan.
The Inprise years, and name changes.
On November 25, 1996, Del Yocam was hired as Borland CEO and Chairman.
In 1997, Borland sold Paradox to Corel, but – importantly – retained all development rights for the core BDE. In November 1997, Borland acquired Visigenic, a middleware company that was focused on implementations of CORBA.
On April 29, 1998, Borland refocused its efforts on targeting enterprise applications development. Borland hired marketing firm Lexicon Branding to come up with a new name for the company. Borland CEO Del Yocam explained at the time that the new name, Inprise, was meant to evoke "integrating the enterprise". The idea was to integrate Borland's tools, Delphi, C++ Builder, and JBuilder with enterprise environment software, including Visigenic's implementations of CORBA, Visibroker for C++ and Java, and the new emerging product, Application Server.
For a number of years (both before and during the Inprise name) Borland suffered from serious financial losses and very poor public image. When the name was changed to Inprise, many thought Borland had gone out of business.
In March 1999, dBase was sold to KSoft, Inc. which was soon renamed to dBASE Inc. (In 2004 dBASE Inc. was renamed to dataBased Intelligence, Inc.).
In 1999, in the middle of Borland's identity crisis, Dale L. Fuller replaced Yocam. At this time Fuller's title was "interim president and CEO." The "interim" was dropped in December 2000. Keith Gottfried served in senior executive positions with the company from 2000 to 2004.
A proposed merger between Inprise and Corel was announced in February 2000, aimed at producing Linux based products. The scheme was abandoned when Corel's shares fell and it became clear that there was really no strategic fit.
InterBase 6.0 was made available as an open source product in July 2000.
Later Borland years.
In January 2001, the Inprise name was abandoned and the company became "Borland" once more.
Under the Borland name and a new management team headed by President and CEO Dale L. Fuller, a now-smaller and profitable Borland refocused on Delphi, and created a version of Delphi and C++ Builder for Linux, both under the name Kylix. This brought Borland's expertise in Integrated Development Environments to the Linux platform for the first time. Kylix was launched in 2001.
Plans to spin off the InterBase division as a separate company were abandoned after Borland and the people who were to run the new company could not agree on terms for the separation. Borland stopped open source releases of InterBase and has developed and sold new versions at a fast pace.
Delphi 6 became the first Integrated Development Environment to support web services. All of the company's development platforms now support web services.
C#Builder was released in 2003 as a native C# development tool, competing with Visual Studio .NET. As of the 2005 release, C#Builder, Delphi for Win32, and Delphi for .NET have been combined into a single IDE called "Borland Developer Studio" (though the combined IDE is still popularly known as "Delphi"). In late 2002 Borland purchased design tool vendor TogetherSoft and tool publisher Starbase, makers of the StarTeam configuration management tool and the CaliberRM requirements management tool. The latest releases of JBuilder and Delphi integrate these tools to give developers a broader set of tools for development.
Former CEO Dale Fuller quit in July 2005, but remained on the board of directors. Former COO Scott Arnold took the title of interim president and chief executive officer until November 8, 2005, when it was announced that Tod Nielsen would take over as CEO effective November 9, 2005. Nielsen remained with the company until January 2009, when he accepted the position of Chief Operating Officer at VMware; CFO Erik Prusch then took over as Acting President and CEO.
In October 2005, Borland acquired Legadero, in order to add its IT Management and Governance (ITM&G) suite, called Tempo, to the Borland product line.
On February 8, 2006, Borland announced the divestiture of their IDE division, including Delphi, JBuilder, and InterBase. At the same time they announced the planned acquisition of Segue Software, a maker of software test and quality tools, in order to concentrate on Application Lifecycle Management (ALM). The new spinoff is called CodeGear.
On March 20, 2006, Borland announced its acquisition of Gauntlet Systems, a provider of technology that screens software under development for quality and security.
On November 14, 2006, Borland announced its decision to separate the Developer Tools Group into a wholly owned subsidiary focused on maximizing developer productivity. The newly formed operation, CodeGear, will be responsible for advancing the four primary product lines formerly associated with Borland's Integrated Development Environment (IDE) business.
In early 2007 Borland rolled out a new company tagline, branding and go to market focus around Open Application Lifecycle Management (ALM) – defining it as the segment of the ALM market in which vendors' solutions are flexible enough to support a customer's specific processes, tools and platforms.
In April 2007, Borland announced that it would be relocating its headquarters and R&D facilities to Austin, Texas. It also has development centers at Singapore, Santa Ana, California, and Linz, Austria.
On May 7, 2008, Borland announced the sale of CodeGear division to Embarcadero Technologies for an expected $23 million price and $7 million in CodeGear accounts receivables retained by Borland.
On May 6, 2009, the company announced it was to be acquired by Micro Focus for $75 million. The transaction was approved by Borland shareholders on July 22, 2009, with Micro Focus acquiring the company for $1.50/share. Following Micro Focus shareholder approval and the required corporate filings, the transaction was completed in late July 2009. The "San Jose Mercury News" reported that, "Philippe Kahn, who founded the company in 1983 and headed it until 1994, called the deal a 'great fit and synergism for both companies and excellent outcome for employees, customers and shareholders.'"
Products.
Borland's current product line includes:
Old software, no longer actively sold by Borland, includes:

</doc>
<doc id="4031" url="http://en.wikipedia.org/wiki?curid=4031" title="Buckminster Fuller">
Buckminster Fuller

Richard Buckminster "Bucky" Fuller (; July 12, 1895 – July 1, 1983) was an American neo-futuristic architect, systems theorist, author, designer, and inventor.
Fuller published more than 30 books, coining or popularizing terms such as "Spaceship Earth", ephemeralization, and synergetic. He also developed numerous inventions, mainly architectural designs, and popularized the widely known geodesic dome. Carbon molecules known as fullerenes were later named by scientists for their resemblance to geodesic spheres.
Buckminster Fuller was the second president of Mensa from 1974 to 1983.
Biography.
Fuller was born on July 12, 1895, in Milton, Massachusetts, the son of Richard Buckminster Fuller and Caroline Wolcott Andrews, and also the grandnephew of the American Transcendentalist Margaret Fuller. He attended Froebelian Kindergarten. Spending much of his youth on Bear Island, in Penobscot Bay off the coast of Maine, he had trouble with geometry, being unable to understand the abstraction necessary to imagine that a chalk dot on the blackboard represented a mathematical point, or that an imperfectly drawn line with an arrow on the end was meant to stretch off to infinity. He often made items from materials he brought home from the woods, and sometimes made his own tools. He experimented with designing a new apparatus for human propulsion of small boats. By the age of 12 he had "invented" a 'push pull' system for propelling a row boat through the use of an inverted umbrella connected to the transom with a simple oar lock which allowed the user to face forward to point the boat toward its destination. Later in life Fuller took exception to the term "invention".
Years later, he decided that this sort of experience had provided him with not only an interest in design, but also a habit of being familiar with and knowledgeable about the materials that his later projects would require. Fuller earned a machinist's certification, and knew how to use the press brake, stretch press, and other tools and equipment used in the sheet metal trade.
Education.
Fuller attended Milton Academy in Massachusetts, and after that began studying at Harvard University, where he was affiliated with Adams House. He was expelled from Harvard twice: first for spending all his money partying with a vaudeville troupe, and then, after having been readmitted, for his "irresponsibility and lack of interest." By his own appraisal, he was a non-conforming misfit in the fraternity environment.
Wartime experience.
Between his sessions at Harvard, Fuller worked in Canada as a mechanic in a textile mill, and later as a laborer in the meat-packing industry. He also served in the U.S. Navy in World War I, as a shipboard radio operator, as an editor of a publication, and as a crash rescue boat commander. After discharge, he worked again in the meat packing industry, acquiring management experience. In 1917, he married Anne Hewlett. During the early 1920s, he and his father-in-law developed the Stockade Building System for producing light-weight, weatherproof, and fireproof housing—although the company would ultimately fail in 1927.
Depression and epiphany.
Buckminster Fuller recalled 1927 as a pivotal year of his life. Fuller was still feeling responsible for the death of his daughter Alexandra, who had died in 1922 from complications from polio and spinal meningitis just prior to her fourth birthday. Fuller felt a personal responsibility for her death, wondering if her death may have been caused by the damp and drafty home which the Fullers had been living in. This provided motivation for Fuller's involvement in Stockade Building Systems, a business which aimed to provide affordable, efficient housing.
In 1927 Fuller, then aged 32, lost his job as president of Stockade. The Fuller family had no savings to fall back upon, and the birth of their daughter Allegra in 1927 added to the financial challenges. Fuller was drinking heavily and reflecting upon the solution to his family's struggles on long walks around Chicago. During the autumn of 1927, Fuller contemplated suicide, so that his family could benefit from a life insurance payment.
Fuller said that he had experienced a profound incident which would provide direction and purpose for his life. He felt as though he was suspended several feet above the ground enclosed in a white sphere of light. A voice spoke directly to Fuller, and declared:
Fuller stated that this experience led to a profound re-examination of his life. He ultimately chose to embark on "an experiment, to find what a single individual [could] contribute to changing the world and benefiting all humanity."
Speaking to audiences later in life, Fuller would regularly recount the story of his Lake Michigan experience, and its transformative impact on his life. Historians have been unable to identify direct evidence for this experience within the 1927 papers of Fuller's Chronofile archives, housed at Stanford University. Stanford historian Barry Katz suggests that the suicide story may be a myth which Fuller constructed later in life, to summarize this formative period of his career.
Recovery.
In 1927 Fuller resolved to think independently which included a commitment to "the search for the principles governing the universe and help advance the evolution of humanity in accordance with them... finding ways of "doing more with less" to the end that all people everywhere can have more and more." By 1928, Fuller was living in Greenwich Village and spending much of his time at the popular café Romany Marie's, where he had spent an evening in conversation with Marie and Eugene O'Neill several years earlier. Fuller accepted a job decorating the interior of the café in exchange for meals, giving informal lectures several times a week, and models of the Dymaxion house were exhibited at the café. Isamu Noguchi arrived during 1929—Constantin Brâncuși, an old friend of Marie's, had directed him there—and Noguchi and Fuller were soon collaborating on several projects, including the modeling of the Dymaxion car based on recent work by Aurel Persu. It was the beginning of their lifelong friendship.
Geodesic domes.
Fuller taught at Black Mountain College in North Carolina during the summers of 1948 and 1949, serving as its Summer Institute director in 1949. There, with the support of a group of professors and students, he began reinventing a project that would make him famous: the geodesic dome. Although the geodesic dome had been created some 30 years earlier by Dr. Walther Bauersfeld, Fuller was awarded United States patents. He is credited for popularizing this type of structure.
One of his early models was first constructed in 1945 at Bennington College in Vermont, where he frequently lectured. In 1949, he erected his first geodesic dome building that could sustain its own weight with no practical limits. It was in diameter and constructed of aluminum aircraft tubing and a vinyl-plastic skin, in the form of an icosahedron. To prove his design, and to awe non-believers, Fuller suspended from the structure's framework several students who had helped him build it. The U.S. government recognized the importance of his work, and employed his firm Geodesics, Inc. in Raleigh, North Carolina to make small domes for the Marines. Within a few years there were thousands of these domes around the world.
His first "continuous tension – discontinuous compression" geodesic dome (full sphere in this case) was constructed at the University of Oregon Architecture School in 1959 with the help of students. These continuous tension – discontinuous compression structures featured single force compression members (no flexure or bending moments) that did not touch each other and were 'suspended' by the tensional members.
Best-known work.
For the next half-century, Fuller developed many ideas, designs and inventions, particularly regarding practical, inexpensive shelter and transportation. He documented his life, philosophy and ideas scrupulously by a daily diary (later called the "Dymaxion Chronofile"), and by twenty-eight publications. Fuller financed some of his experiments with inherited funds, sometimes augmented by funds invested by his collaborators, one example being the Dymaxion car project.
World stage.
International recognition began with the success of huge geodesic domes during the 1950s. Fuller lectured at NC State University in Raleigh in 1949, where he met James Fitzgibbon, who would become a close friend and colleague. Fitzgibbon was director of Geodesics, Inc. and Synergetics, Inc. the first licensees to design geodesic domes. Thomas C. Howard was lead designer, architect and engineer for both companies. From 1959 to 1970, Fuller taught at Southern Illinois University Carbondale (SIU). Beginning as an assistant professor, he gained full professorship in 1968, in the School of Art and Design. Working as a designer, scientist, developer, and writer, he lectured for many years around the world. He collaborated at SIU with the designer John McHale. In 1965, Fuller inaugurated the World Design Science Decade (1965 to 1975) at the meeting of the International Union of Architects in Paris, which was, in his own words, devoted to "applying the principles of science to solving the problems of humanity." Later in his SIU tenure, Fuller was also a visiting professor at SIU Edwardsville, where he designed the dome for the campus Religious Center.
Fuller believed human societies would soon rely mainly on renewable sources of energy, such as solar- and wind-derived electricity. He hoped for an age of "omni-successful education and sustenance of all humanity." Fuller referred to himself as "the property of universe" and during one radio interview he gave later in life, declared himself and his work "the property of all humanity". For his lifetime of work, the American Humanist Association named him the 1969 Humanist of the Year.
In 1976, Fuller was a key participant at UN Habitat I, the first UN forum on human settlements.
Honors.
Fuller was awarded 28 United States patents and many honorary doctorates. In 1960, he was awarded the Frank P. Brown Medal from The Franklin Institute. He was elected a Fellow of the American Academy of Arts and Sciences in 1968. In 1968 he was elected into the National Academy of Design as an Associate member, and became a full Academician in 1970. In 1970 he received the Gold Medal award from the American Institute of Architects. He also received numerous other awards, including the Presidential Medal of Freedom presented to him on February 23, 1983 by President Ronald Reagan.
Last filmed appearance.
Fuller's last filmed interview took place on April 3, 1983, in which he presented his analysis of Simon Rodia's Watts Towers as a unique embodiment of the structural principles found in nature. Portions of this interview appear in "I Build the Tower," a documentary film on Rodia's architectural masterpiece.
Death.
Fuller died on July 1, 1983, 11 days before his 88th birthday. During the period leading up to his death, his wife had been lying comatose in a Los Angeles hospital, dying of cancer. It was while visiting her there that he exclaimed, at a certain point: "She is squeezing my hand!" He then stood up, suffered a heart attack, and died an hour later, at age 87. His wife of 66 years died 36 hours later. They are buried in Mount Auburn Cemetery in Cambridge, Massachusetts.
Philosophy and worldview.
The grandson of Unitarian minister Arthur Buckminster Fuller, R. Buckminster Fuller was also Unitarian. Buckminster Fuller was an early environmental activist. He was very aware of the finite resources the planet has to offer, and promoted a principle that he termed "ephemeralization", which, in essence—according to futurist and Fuller disciple Stewart Brand—Fuller coined to mean "doing more with less". Resources and waste material from cruder products could be recycled into making more valuable products, increasing the efficiency of the entire process. Fuller also introduced synergetics, an encompassing term which he used broadly as a metaphoric language for communicating experiences using geometric concepts and, more specifically, to reference the empirical study of systems in transformation, with an emphasis on total system behavior unpredicted by the behavior of any isolated components. Fuller coined this term long before the term synergy became popular.
Fuller was a pioneer in thinking globally, and he explored principles of energy and material efficiency in the fields of architecture, engineering and design. He cited François de Chardenedes' opinion that petroleum, from the standpoint of its replacement cost out of our current energy "budget" (essentially, the net incoming solar flux), had cost nature "over a million dollars" per U.S. gallon (US$300,000 per litre) to produce. From this point of view, its use as a transportation fuel by people commuting to work represents a huge net loss compared to their earnings. An encapsulation quotation of his views might be, "There is no energy crisis, only a crisis of ignorance."
Fuller was concerned about sustainability and about human survival under the existing socio-economic system, yet remained optimistic about humanity's future. Defining wealth in terms of knowledge, as the "technological ability to protect, nurture, support, and accommodate all growth needs of life," his analysis of the condition of "Spaceship Earth" caused him to conclude that at a certain time during the 1970s, humanity had attained an unprecedented state. He was convinced that the accumulation of relevant knowledge, combined with the quantities of major recyclable resources that had already been extracted from the earth, had attained a critical level, such that competition for necessities was not necessary anymore. Cooperation had become the optimum survival strategy. "Selfishness," he declared, "is unnecessary and hence-forth unrationalizable... War is obsolete." He criticized previous utopian schemes as too exclusive, and thought this was a major source of their failure. To work, he thought that a utopia needed to include everyone.
So it is not surprising that he and others of his stature were attracted by Korzybski's ideas in "general semantics". General semantics is a discipline of mind that seeks to unify persons and nations by changing their worldview reaction and the philosophy of their expression. In the 1950s Fuller attended seminars and workshops organized by the Institute of General Semantics, and he delivered the annual Alfred Korzybski Memorial Lecture in 1955. Korzybski is mentioned in the Introduction of his book "Synergetics". The two gentlemen shared a remarkable amount of similarity in their formulations of general semantics.
In his 1970 book "I Seem To Be a Verb", he wrote: "I live on Earth at present, and I don't know what I am. I know that I am not a category. I am not a thing—a noun. I seem to be a verb, an evolutionary process—an integral function of the universe."
Fuller wrote that the natural analytic geometry of the universe was based on arrays of tetrahedra. He developed this in several ways, from the close-packing of spheres and the number of compressive or tensile members required to stabilize an object in space. One confirming result was that the strongest possible homogeneous truss is cyclically tetrahedral.
He had become a guru of the design, architecture, and 'alternative' communities, such as Drop City, the community of experimental artists to whom he awarded the 1966 "Dymaxion Award" for "poetically economic" domed living structures.
Major design projects.
The geodesic dome.
Fuller was most famous for his lattice shell structures – geodesic domes, which have been used as parts of military radar stations, civic buildings, environmental protest camps and exhibition attractions. An examination of the geodesic design by Walther Bauersfeld for the Zeiss-Planetarium, built some 20 years prior to Fuller's work, reveals that Fuller's Geodesic Dome patent (U.S. 2,682,235; awarded in 1954), follows the same design as Bauersfeld's.
Their construction is based on extending some basic principles to build simple "tensegrity" structures (tetrahedron, octahedron, and the closest packing of spheres), making them lightweight and stable. The geodesic dome was a result of Fuller's exploration of nature's constructing principles to find design solutions. The Fuller Dome is referenced in the Hugo Award-winning novel "Stand on Zanzibar" by John Brunner, in which a geodesic dome is said to cover the entire island of Manhattan, and it floats on air due to the hot-air balloon effect of the large air-mass under the dome (and perhaps its construction of lightweight materials).
Transportation.
In the 1930s, Fuller designed and built prototypes of what he hoped would be a safer, aerodynamic car, which he called the Dymaxion. ("Dymaxion" is said to be a syllabic abbreviation of "dynamic maximum tension", or possibly of "dynamic maximum ion".) Fuller worked with professional colleagues for three years beginning in 1932 on a design idea Fuller had derived from aircraft technologies. The three prototype cars were different from anything being sold at the time. They had three wheels: two front drive wheels and one rear, steered wheel. The engine was in the rear, and the chassis and body were original designs. The aerodynamic, somewhat tear-shaped body was large enough to seat eleven people and was about long, resembling a blend of a light aircraft (without wings) and a Volkswagen van of 1950s vintage. All three prototypes were essentially a mini-bus, and its concept long predated the Volkswagen Type 2 mini-bus conceived in 1947 by Ben Pon.
Despite its length, and due to its three-wheel design, the Dymaxion turned on a small radius and could easily be parked in a tight space. The prototypes were efficient in fuel consumption for their day, traveling about 30 miles per gallon. Fuller contributed a great deal of his own money to the project, in addition to funds from one of his professional collaborators. An industrial investor was also very interested in the concept. Fuller anticipated that the cars could travel on an open highway safely at up to about , but, in practice, they were difficult to control and steer above . Investors backed out and research ended after one of the prototypes was involved in a high-profile collision that resulted in a fatality. In 2007, "Time Magazine" reported on the Dymaxion as one of the "50 worst cars of all time".
In 1943, industrialist Henry J. Kaiser asked Fuller to develop a prototype for a smaller car, but Fuller's five-seater design was never developed further.
Housing.
Fuller's energy-efficient and inexpensive Dymaxion house garnered much interest, but has never been produced. Here the term "Dymaxion" is used in effect to signify a "radically strong and light tensegrity structure". One of Fuller's Dymaxion Houses is on display as a permanent exhibit at The Henry Ford in Dearborn, Michigan. Designed and developed during the mid-1940s, this prototype is a round structure (not a dome), shaped something like the flattened "bell" of certain jellyfish. It has several innovative features, including revolving dresser drawers, and a fine-mist shower that reduces water consumption. According to Fuller biographer Steve Crooks, the house was designed to be delivered in two cylindrical packages, with interior color panels available at local dealers. A circular structure at the top of the house was designed to rotate around a central mast to use natural winds for cooling and air circulation.
Conceived nearly two decades before, and developed in Wichita, Kansas, the house was designed to be lightweight and adapted to windy climates. It was to be inexpensive to produce and purchase, and assembled easily. It was to be produced using factories, workers, and technologies that had produced World War II aircraft. It was ultramodern-looking at the time, built of metal, and sheathed in polished aluminum. The basic model enclosed of floor area. Due to publicity, there were many orders during the early Post-War years, but the company that Fuller and others had formed to produce the houses failed due to management problems.
In 1969, Fuller began the Otisco Project, named after its location in Otisco, New York. The project developed and demonstrated concrete spray technology used in conjunction with mesh covered wireforms as a viable means of producing large scale, load bearing spanning structures built on site without the use of pouring molds, other adjacent surfaces or hoisting.
The initial construction method used a circular concrete footing in which anchor posts were set. Tubes cut to length and with ends flattened were then bolted together to form a duodeca-rhombicahedron (22-sided hemisphere) geodesic structure with spans ranging to . The form was then draped with layers of ¼-inch wire mesh attached by twist ties. Concrete was then sprayed onto the structure, building up a solid layer which, when cured, would support additional concrete to be added by a variety of traditional means. Fuller referred to these buildings as monolithic ferroconcrete geodesic domes. The tubular frame form proved too problematic when it came to setting windows and doors, and was abandoned. The second method used iron rebar set vertically in the concrete footing and then bent inward and welded in place to create the dome's wireform structure and performed satisfactorily. Domes up to three stories tall built with this method proved to be remarkably strong. Other shapes such as cones, pyramids and arches proved equally adaptable.
The project was enabled by a grant underwritten by Syracuse University and sponsored by US Steel (rebar), the Johnson Wire Corp, (mesh) and Portland Cement Company (concrete). The ability to build large complex load bearing concrete spanning structures in free space would open many possibilities in architecture, and is considered as one of Fuller's greatest contributions.
Dymaxion map and World Game.
Fuller also designed an alternative projection map, called the Dymaxion map. This was designed to show Earth's continents with minimum distortion when projected or printed on a flat surface. In the 1960s, Fuller developed the World Game, a collaborative simulation game played on a 70-by-35-foot Dymaxion map, in which players attempt to solve world problems. The object of the simulation game is, in Fuller's words, to “make the world work, for 100% of humanity, in the shortest possible time, through spontaneous cooperation, without ecological offense or the disadvantage of anyone.”
Quirks.
Fuller was a frequent flier, often crossing time zones. He famously wore three watches; one for the current zone, one for the zone he had departed, and one for the zone he was going to. Fuller also noted that a single sheet of newsprint, inserted over a shirt and under a suit jacket, provided completely effective heat insulation during long flights.
He experimented with polyphasic sleep, which he called Dymaxion sleep. In 1943, he told "Time Magazine" that he had slept only two hours a day for two years. He quit the schedule because it conflicted with his business associates' sleep habits, but stated that Dymaxion sleep could help the United States win World War II.
Fuller documented his life copiously from 1915 to 1983, approximately of papers in a collection called the Dymaxion Chronofile. He also kept copies of all incoming and outgoing correspondence. The enormous Fuller Collection is currently housed at Stanford University.
If somebody kept a very accurate record of a human being, going through the era from the Gay 90s, from a very different kind of world through the turn of the century—as far into the twentieth century as you might live. I decided to make myself a good case history of such a human being and it meant that I could not be judge of what was valid to put in or not. I must put everything in, so I started a very rigorous record.
In his youth, Fuller experimented with several ways of presenting himself: R. B. Fuller, Buckminster Fuller, but as an adult finally settled on R. Buckminster Fuller, and signed his letters as such. However, he preferred to be addressed as simply "Bucky".
Language and neologisms.
Buckminster Fuller spoke and wrote in a unique style and said it was important to describe the world as accurately as possible. Fuller often created long run-on sentences and used unusual compound words (omniwell-informed, intertransformative, omni-interaccommodative, omniself-regenerative) as well as terms he himself invented.
Fuller used the word "Universe" without the definite or indefinite articles ("the" or "a") and always capitalized the word. Fuller wrote that "by Universe I mean: the aggregate of all humanity's consciously apprehended and communicated (to self or others) Experiences."
The words "down" and "up", according to Fuller, are awkward in that they refer to a planar concept of direction inconsistent with human experience. The words "in" and "out" should be used instead, he argued, because they better describe an object's relation to a gravitational center, the Earth. "I suggest to audiences that they say, 'I'm going "outstairs" and "instairs."' At first that sounds strange to them; They all laugh about it. But if they try saying in and out for a few days in fun, they find themselves beginning to realize that they are indeed going inward and outward in respect to the center of Earth, which is our Spaceship Earth. And for the first time they begin to feel real 'reality.'"
"World-around" is a term coined by Fuller to replace "worldwide". The general belief in a flat Earth died out in classical antiquity, so using "wide" is an anachronism when referring to the surface of the Earth—a spheroidal surface has area and encloses a volume but has no width. Fuller held that unthinking use of obsolete scientific ideas detracts from and misleads intuition. Other neologisms collectively invented by the Fuller family, according to Allegra Fuller Snyder, are the terms "sunsight" and "sunclipse", replacing "sunrise" and "sunset" to overturn the geocentric bias of most pre-copernican celestial mechanics.
Fuller also invented the word "livingry," as opposed to weaponry (or "killingry"), to mean that which is in support of all human, plant, and Earth life. "The architectural profession—civil, naval, aeronautical, and astronautical—has always been the place where the most competent thinking is conducted regarding livingry, as opposed to weaponry."
As well as contributing significantly to the development of tensegrity technology, Fuller invented the term "tensegrity" from "tensional integrity". "Tensegrity describes a structural-relationship principle in which structural shape is guaranteed by the finitely closed, comprehensively continuous, tensional behaviors of the system and not by the discontinuous and exclusively local compressional member behaviors. Tensegrity provides the ability to yield increasingly without ultimately breaking or coming asunder."
"Dymaxion" is a portmanteau of "dynamic maximum tension". It was invented about 1929 by two admen at Marshall Field's department store in Chicago to describe Fuller's concept house, which was shown as part of a house of the future store display. They created the term utilizing three words that Fuller used repeatedly to describe his design – dynamic, maximum, and ion.
Fuller also helped to popularize the concept of Spaceship Earth: "The most important fact about Spaceship Earth: an instruction manual didn't come with it."
Concepts and buildings.
His concepts and buildings include:
Influence and legacy.
Among the many people who were influenced by Buckminster Fuller are:
Constance Abernathy,
Ruth Asawa,
J. Baldwin,
Michael Ben-Eli,
Pierre Cabrol,
John Cage,
Joseph Clinton,
Peter Floyd,
Medard Gabel,
Michael Hays,
David Johnston,
Robert Kiyosaki,
Peter Jon Pearce,
Shoji Sadao,
Edwin Schlossberg,
Kenneth Snelson,
Robert Anton Wilson and Stewart Brand.
An allotrope of carbon, fullerene—and a particular molecule of that allotrope C60 (buckminsterfullerene or buckyball) has been named after him. The Buckminsterfullerene molecule, which consists of 60 carbon atoms, very closely resembles a spherical version of Fuller's geodesic dome. The 1996 Nobel prize in chemistry was given to Kroto, Curl, and Smalley for their discovery of the fullerene.
He is quoted in the lyric of "The Tower Of Babble" in the musical "Godspell:" "Man is a complex of patterns and processes."
On July 12, 2004, the United States Post Office released a new commemorative stamp honoring R. Buckminster Fuller on the 50th anniversary of his patent for the geodesic dome and by the occasion of his 109th birthday.
Fuller was the subject of two documentary films: "The World of Buckminster Fuller" (1971) and "" (1996). Additionally, filmmaker Sam Green and the band Yo La Tengo collaborated on a 2012 "live documentary" about Fuller, "The Love Song of R. Buckminster Fuller".
In June 2008, the Whitney Museum of American Art presented "Buckminster Fuller: Starting with the Universe", the most comprehensive retrospective to date of his work and ideas. The exhibition traveled to the Museum of Contemporary Art, Chicago in 2009. It presented a combination of models, sketches, and other artifacts, representing six decades of the artist's integrated approach to housing, transportation, communication, and cartography. It also featured the extensive connections with Chicago from his years spent living, teaching, and working in the city.
In 2012, The San Francisco Museum of Modern Art hosted "The Utopian Impulse" – a show about Buckminster Fuller's influence in the Bay Area. Featured were concepts, inventions and designs for creating "free energy" from natural forces, and for sequestering carbon from the atmosphere. The show ran January through July.

</doc>
<doc id="4032" url="http://en.wikipedia.org/wiki?curid=4032" title="Bill Watterson">
Bill Watterson

William Boyd "Bill" Watterson II (born July 5, 1958) is an American artist and the author of the comic strip "Calvin and Hobbes", which was syndicated from 1985 to 1995. Watterson stopped drawing "Calvin and Hobbes" at the end of 1995 with a short statement to newspaper editors and his readers that he felt he had achieved all he could in the medium. Watterson is known for his views on licensing and comic syndication and his move back into private life after drawing "Calvin and Hobbes" came to a close.
Early life.
Bill Watterson was born in Washington, D.C., United States, where his father, James G. Watterson (born 1932) worked as a patent attorney.The family relocated to Chagrin Falls, Ohio in 1965 when Watterson was six years old because his mother, Kathryn, wanted to be closer to her family and felt the small town was a good place to raise her children, William and Thomas.
Watterson, who drew his first cartoon at age eight, spent much time in childhood alone, drawing and cartooning. This continued through his school years, during which time he discovered comic strips like "Pogo", "Krazy Kat", and Charles Schulz' "Peanuts" which subsequently inspired and influenced his desire to become a professional cartoonist. On one occasion, when he was in fourth grade, he wrote a letter to Charles Schulz, who—to Watterson's surprise—responded, making a big impression on him at the time. His parents encouraged him in his artistic pursuits. Later they would recall him as a "conservative child"—imaginative, but "not in a fantasy way", and certainly nothing like the character of Calvin he would later create. Watterson found avenues for his cartooning talents throughout primary and secondary school, creating high school-themed super hero comics with his friends and contributing cartoons and art to the school newspaper and yearbook.
From 1976 to 1980, Watterson attended Kenyon College and received a Bachelor of Arts degree in political science. Although he had already decided upon a career in cartooning, he felt his studies would help him move into editorial cartooning. While at college he continued to develop his art skills—during his sophomore year he painted Michelangelo's "Creation of Adam" on the ceiling of his dorm room. He also contributed cartoons to the college newspaper, some of which included the original "Spaceman Spiff" cartoons.
Later, when Watterson was creating names for the characters in his comic strip, he decided upon Calvin (after the Protestant reformer John Calvin) and Hobbes (after the social philosopher Thomas Hobbes), allegedly as a "tip of the hat" to the political science department at Kenyon. In "The Complete Calvin And Hobbes", Watterson stated that Calvin is named for "a 16th-century theologian who believed in predestination," and Hobbes for "a 17th-century philosopher with a dim view of human nature."
Career.
Early work.
Watterson was inspired by the work of Kenyon alum and "Cincinnati Enquirer" political cartoonist Jim Borgman, who currently draws "Zits", and decided to try to follow the same career path as Borgman, who in turn offered support and encouragement to the aspiring artist. Watterson graduated in 1980 and was hired on a trial basis at a competing paper of the "Enquirer", the "Cincinnati Post". Watterson quickly discovered that the job was full of unexpected challenges which prevented him from performing his duties to the standards set for him. Not the least of these challenges was his unfamiliarity with the Cincinnati political scene as he had never resided in or near the city, having grown up in the Cleveland area and attending college in central Ohio. The "Post" abruptly fired Watterson before his contract was up.
He then joined a small advertising agency and worked there for four years as a designer, creating grocery advertisements while also working on his own projects including development of his own cartoon strip and contributions to "Target: The Political Cartoon Quarterly".
Rise to success.
Watterson has said he works for personal fulfillment. As he told the graduating class of 1990 at Kenyon College, "It's surprising how hard we'll work when the work is done just for ourselves." "Calvin and Hobbes" was first published on November 18, 1985. In "Calvin and Hobbes Tenth Anniversary Book", he wrote that his influences included Charles Schulz for "Peanuts"; Walt Kelly for "Pogo" and George Herriman for "Krazy Kat". Watterson wrote the introduction to the first volume of "The Komplete Kolor Krazy Kat". Watterson's style also reflects the influence of Winsor McCay's "Little Nemo in Slumberland".
Like many artists, Watterson incorporated elements of his life, interests, beliefs and values into his work—for example, his hobby as a cyclist, memories of his own father's speeches about "building character", and his views on merchandising and corporations. Watterson's cat, Sprite, very much inspired the personality and physical features of Hobbes.
Watterson spent much of his career trying to change the climate of newspaper comics. He believed that the artistic value of comics was being undermined, and that the space they occupied in newspapers continually decreased, subject to arbitrary whims of shortsighted publishers. Furthermore, he opined that art should not be judged by the medium for which it is created (i.e., there is no "high" art or "low" art—just art).
Fight against merchandising the cartoon characters.
For years, Watterson battled against pressure from publishers to merchandise his work, something he felt would cheapen his comic. He refused to merchandise his creations on the grounds that displaying "Calvin and Hobbes" images on commercially sold mugs, stickers and T-shirts would devalue the characters and their personalities. Watterson said that Universal kept putting pressure on him and added that his contract, which he said he signed without fully perusing it because as a new artist he was happy to find a syndicate willing to give him a chance (two syndicates had denied Watterson), was so one-sided that if Universal really wanted to, they could license his characters against his will, and could even fire him but continue "Calvin and Hobbes" with a new artist. Watterson's position eventually won out and he was able to renegotiate his contract so that he would receive all rights to his work, but later added that the licensing fight exhausted him and contributed to the need for a nine-month sabbatical in 1991.
Despite Watterson's efforts, many unofficial knockoffs have been found, including items that depict Calvin and Hobbes consuming alcohol or Calvin urinating on a logo. Watterson has said 'Only thieves and vandals have made money on "Calvin and Hobbes" merchandise'.
End of "Calvin and Hobbes".
Watterson announced the end of "Calvin and Hobbes" on November 9, 1995, with the following letter to newspaper editors:
The last strip of "Calvin and Hobbes" was published on December 31, 1995.
After "Calvin and Hobbes".
Since the conclusion of "Calvin and Hobbes", Watterson has taken up painting, at one point drawing landscapes of the woods with his father. Watterson has kept away from the public eye and has given no indication of resuming the strip, creating new works based on the strip's characters, or embarking on other projects, though he has published several anthologies of "Calvin and Hobbes" strips. He will not sign autographs or license his characters, staying true to his stated principles.
In previous years, Watterson was known to sneak autographed copies of his books onto the shelves of the Fireside Bookshop, a family-owned bookstore in his hometown of Chagrin Falls, Ohio. However, after discovering that some were selling the autographed books online for high prices, he ended this practice as well. Valuing privacy, he is reluctant to give interviews or make public appearances. His lengthiest interview was featured as the cover story in "The Comics Journal" No. 127 in February 1989. He drew a new "Calvin and Hobbes" cover for that issue of the magazine as well.
On December 21, 1999, a short piece written by Watterson to mark the forthcoming end of the iconic comic strip "Peanuts" was published in the "Los Angeles Times".
In the years that followed the end of "Calvin and Hobbes", many attempts were made to locate Watterson in his hometown of Chagrin Falls. Both "The Plain Dealer" and the "Cleveland Scene" sent reporters in 1998 and 2003, respectively, but were unable to locate him.
In 2004, Watterson and his wife Melissa bought a home in the Cleveland suburb of Cleveland Heights, Ohio. In 2005, they completed the move from their home in Chagrin Falls to their new residence.
In 2005, Gene Weingarten of "The Washington Post" sent Watterson the first edition of the "Barnaby" book, as an incentive, hoping to land an interview. Weingarten passed the book, along with a message, to Watterson's parents, and declared he would wait in his hotel for as long as it took Watterson to contact him. Watterson's editor Lee Salem called the next day to tell Weingarten that the cartoonist would not be coming.
In October 2005, Watterson answered 15 questions submitted by readers.
In October 2007, Watterson wrote a review of "Schulz and Peanuts", a biography of Charles Schulz, in "The Wall Street Journal". In 2008, he provided a foreword for the first book collection of Richard Thompson's "Cul De Sac" comic strip.
In early 2010, Watterson was interviewed by "The Plain Dealer" on the 15th anniversary of the end of "Calvin and Hobbes". Explaining his decision to discontinue the strip, he said,
In April 2011, a representative for Andrews McMeel received a package from a "William Watterson in Cleveland Heights, Ohio", which contained a 6" x 8" oil-on-board painting of "Cul De Sac" character Petey Otterloop, done by Watterson for the "Team Cul de Sac" fundraising project for Parkinson's Disease. His syndicate, which has since become Universal Uclick, has said that the painting was the first new artwork from Watterson that the syndicate has seen since "Calvin and Hobbes" ended in 1995.
In October 2013, the magazine "Mental Floss" published an interview with Watterson, only the second since the strip ended. Watterson again confirmed that he would not be revisiting "Calvin and Hobbes", and that he was satisfied with his decision. Watterson also gave his opinion on the changes in the comic book industry and where it would be headed in future:
On February 26, 2014, Watterson published his first cartoon since the end of "Calvin and Hobbes": a poster for the documentary "Stripped".
In June 2014, three strips of "Pearls Before Swine" (published June 4, June 5, and June 6, 2014) featured guest illustrations by Watterson after cartoonist Stephan Pastis communicated with him via e-mail. Pastis likened this unexpected collaboration to getting "a glimpse of Bigfoot." "I thought maybe Stephan and I could do this goofy collaboration and then use the result to raise some money for Parkinson’s research in honor of Richard Thompson. It seemed like a perfect convergence," Watterson told the Washington Post.
Other work.
Watterson wrote a brief, tongue-in-cheek autobiography in the late 1980s.
Legacy.
Changing the format of the Sunday strip.
Watterson was critical of the prevailing format for the Sunday comic strip that was in place when he began drawing (and to varying degrees, still is). The typical layout consists of three rows with eight total squares, which takes up half a page if published with its normal size. Since some newspapers are restricted with space for their Sunday features, they often reduce the size of the strip. One of the more common ways is to cut the top two panels out, which Watterson believed forced him to waste the space on throwaway jokes that did not always fit the strip. While he was set to return from his first sabbatical, Watterson discussed with his syndicate a new format for "Calvin and Hobbes" that would enable him to use his space more efficiently and would almost require the papers his strip ran in to publish it as a half-page. Universal agreed that they would sell the strip as the half-page and nothing else, which garnered anger from papers and criticism for Watterson from both editors and some of his fellow cartoonists (whom he described as "unnecessarily hot tempered"). Eventually, Universal compromised and agreed to offer papers a choice between the full half-page or a reduced-sized version as to alleviate concerns about the size issue. Although Watterson conceded that this caused him to lose space in many papers, he said that in the end it was a benefit because he felt that he was giving the papers' readers a better strip for their money and editors were free not to run "Calvin and Hobbes" at their own risk; he added that he was not going to apologize for drawing a popular feature.
Awards and honors.
Watterson was awarded the National Cartoonists Society's Reuben Award in both 1986 and 1988. Watterson's second Reuben win made him the youngest cartoonist to be so honored, and only the fifth person to win twice, following Charles Schulz, Dik Browne, Chester Gould, and Jeff MacNelly. (Gary Larson is the only cartoonist to win a second Reuben since Watterson.) In 2014, Watterson was awarded the Grand Prix at the Angoulême International Comics Festival for his body of work, becoming just the fourth non-European cartoonist to be so honored in the first 41 years of the event.
Exhibitions.
In 2001, Watterson was contacted by the Billy Ireland Cartoon Library & Museum at Ohio State University about a possible exhibition of his work. Watterson agreed to participate and decided to highlight his Sunday strips. He chose thirty-six of his favorites, displaying them with both the original drawing and the colored finished product, with most pieces featuring personal annotations. Watterson also wrote an accompanying essay that served as the foreword for the exhibit. The exhibition was called "Calvin and Hobbes: Sunday Pages 1985-1995" and was opened on September 10, 2001. It was taken down in January 2002, and an accompanying published catalogue with the same title as the exhibit came out as well.
Beginning March 22, 2014, Watterson is exhibiting again at the Billy Ireland Cartoon Library & Museum at Ohio State University. In conjunction with this exhibition, Watterson also participated in an interview with the school.

</doc>
<doc id="4035" url="http://en.wikipedia.org/wiki?curid=4035" title="Black">
Black

Black is the color of coal, ebony, and of outer space. It is the darkest color, the result of the absence of or complete absorption of light. It is the opposite of white and often represents darkness in contrast with light.
Black was one of the first colors used by artists in neolithic cave paintings. In the 14th century, it began to be worn by royalty, the clergy, judges and government officials in much of Europe. It became the color worn by English romantic poets, businessmen and statesmen in the 19th century, and a high fashion color in the 20th century.
In the Roman Empire, it became the color of mourning, and over the centuries it was frequently associated with death, evil, witches and magic. As in the Western World today, it is also the color most commonly associated with mourning, evil, magic, the end, violence, power, secrets, and elegance.
Etymology and language.
The word "black" comes from Old English "blæc" ("black, dark", "also", "ink"), from Proto-Germanic *"blakkaz" ("burned"), from Proto-Indo-European *"bhleg-" ("to burn, gleam, shine, flash"), from base *"bhel-" ("to shine"), related to Old Saxon "blak" ("ink"), Old High German "blach" ("black"), Old Norse "blakkr" ("dark"), Dutch "blaken" ("to burn"), and Swedish "bläck" ("ink"). More distant cognates include Latin "flagrare" ("to blaze, glow, burn"), and Ancient Greek "phlegein" ("to burn, scorch").
The Ancient Greeks sometimes used the same word to name different colors, if they had the same intensity. "Kuanos"' could mean both dark blue and black.
The Ancient Romans had two words for black: "ater" was a flat, dull black, while "niger" was a brilliant, saturated black. "Ater" has vanished from the vocabulary, but "niger" was the source of the country name "Nigeria" the English word "Negro" and the word for "black" in most modern Romance languages (French: "noir"; Spanish: "negro"; Italian: "nero").
Old High German also had two words for black: "swartz" for dull black and "blach" for a luminous black. These are parallelled in Middle English by the terms "swart" for dull black and "blaek" for luminous black. "Swart" still survives as the word s"warthy", while "blaek" became the modern English "black".
In heraldry, the word used for the black color is sable, named for the black fur of the sable, an animal.
Black in history and art.
The Ancient World.
Black was one of the first colors used in art. The Lascaux Cave in France contains drawings of bulls and other animals drawn by paleolithic artists between 18,000 and 17,000 years ago. They began by using charcoal, and then made more vivid black pigments by burning bones or grinding a powder of manganese oxide.
For the ancient Egyptians, black had very positive associations. it was the color of the rich black soil flooded by the Nile. It was the color of Anubis, the god of the underworld, who took the form of a black jackal, and offered protection against evil to the dead.
For the ancient Greeks, black was also the color of the underworld, separated from the world of the living by the river Acheron, whose water was black. Those who had committed the worst sins were sent to Tartarus, the deepest and darkest level. In the center was the palace of Hades, the king of the underworld, where he was seated upon a black ebony throne.
Black was one of the most important colors used by ancient Greek artists. In the 6th century BC, they began making black-figure pottery and later red figure pottery, using a highly original technique. In black-figure pottery, the artist would paint figures with a glossy clay slip on a red clay pot. When the pot was fired, the figures painted with the slip would turn black, against a red background. Later they reversed the process, painting the spaces between the figures with slip. This created magnificent red figures against a glossy black background.
In the social hierarchy of ancient Rome, purple was the color reserved for the Emperor; red was the color worn by soldiers (red cloaks for the officers, red tunics for the soldiers); white the color worn by the priests, and black was worn by craftsmen and artisans. The black they wore was not deep and rich; the vegetable dyes used to make black were not solid or lasting, so the blacks often turned out faded gray or brown.
In Latin, the word for black, "ater" and to darken, "atere", were associated with cruelty, brutality and evil. They were the root of the English words "atrocious" and "atrocity".
Black was also the Roman color of death and mourning. In the 2nd century BC Roman magistrates began to wear a dark toga, called a "toga pulla", to funeral ceremonies. Later, under the Empire, the family of the deceased also wore dark colors for a long period; then, after a banquet to mark the end of mourning, exchanged the black for a white toga. In Roman poetry, death was called the "hora nigra", the black hour.
The German and Scandinavian peoples worshipped their own goddess of the night, Nótt, who crossed the sky in a chariot drawn by a black horse. They also feared Hel, the goddess of the kingdom of the dead, whose skin was black on one side and red on the other. They also held sacred the crow. They believed that Odin, the king of the Nordic pantheon, had two black crows, Huginn and Muninn, who served as his agents, traveling the world for him, watching and listening.
The Middle Ages.
In the early Middle Ages, black was commonly associated with darkness and evil. In Medieval paintings, the devil was usually depicted as having human form, but with wings and black skin or hair.
In fashion, black did not have the prestige of red, the color of the nobility. It was worn by Benedictine monks as a sign of humility and penitence. In the 12th century a famous theological dispute broke out between the Cistercian monks, who wore white, and the Benedictines, who wore black. A Benedictine abbot, Pierre the Venerable, accused the Cistercians of excessive pride in wearing white instead of black. Saint Bernard of Clairvaux, the founder of the Cistercians responded that black was the color of the devil, hell, "of death and sin," while white represented "purity, innocence and all the virtues".
Black symbolized both power and secrecy in the medieval world. The emblem of the Holy Roman Empire of Germany was a black eagle. The black knight in the poetry of the Middle Ages was an enigmatic figure, hiding his identity, usually wrapped in secrecy.
Obtaining a good quality black was an essential element of the most influential invention of the Middle Ages; the printing press. Traditional handwriting ink, invented in China and India in ancient times, blurred during printing. A new kind of ink, printer's ink, was created in the 15th century out of soot, turpentine and walnut oil. The new ink made it possible to spread ideas to a mass audience through printed books, and to popularize art through black and white engravings and prints.
Black becomes the color of fashion.
In the early Middle Ages, princes, nobles and the wealthy usually wore bright colors, particularly scarlet cloaks from Italy. Black was rarely part of the wardrobe of a noble family. The one exception was the fur of the sable. This glossy black fur, from an animal of the marten family, was the finest and most expensive fur in Europe. It was imported from Russia and Poland and used to trim the robes and gowns of royalty.
In the 14th century, the status of black began to change. First, high-quality black dyes began to arrive on the market, allowing garments of a deep, rich black. Magistrates and government officials began to wear black robes, as a sign of the importance and seriousness of their positions. A third reason was the passage of sumptuary laws in some parts of Europe which prohibited the wearing of costly clothes and certain colors by anyone except members of the nobility. The famous bright scarlet cloaks from Venice and the peacock blue fabrics from Florence were restricted to the nobility. The wealthy bankers and merchants of northern Italy responded by changing to black robes and gowns, made with the most expensive fabrics.
The change to the more austere but elegant black was quickly picked up by the kings and nobility. It began in northern Italy, where the Duke of Milan and the Count of Savoy and the rulers of Mantua, Ferrara, Rimini and Urbino began to dress in black. It then spread to France, led by Louis I, Duke of Orleans, younger brother of King Charles VI of France. It moved to England at the end of the reign of King Richard II (1377–1399), where all the court began to wear black. In 1419–20, black became the color of the powerful Duke of Burgundy, Philip the Good. It moved to Spain, where it became the color of the Spanish Habsburgs, of Charles V and of his son, Philip II of Spain (1527–1598). European rulers saw it as the color of power, dignity, humility and temperance. By the end of the 16th century, it was the color worn by almost all the monarchs of Europe and their courts.
Protestants and Puritans.
While black was the color worn by the Catholic rules of Europe, it was also the emblematic color of the Protestant Reformation in Europe and the Puritans in England and America. Jean Calvin, Melanchton and other Protestant theologians denounced the richly colored and decorated interiors of Roman Catholic churches. They saw the color red, worn by the Pope and his Cardinals, as the color of luxury, sin, and human folly. In some northern European cities, mobs attacked churches and cathedrals, smashed the stained glass windows and defaced the statues and decoration. In Protestant doctrine, clothing was required to be sober, simple and discreet. Bright colors were banished and replaced by blacks, browns and grays; women and children were recommended to wear white.
In the Protestant Netherlands, Rembrandt Van Rijn used this sober new palette of blacks and browns to create portraits whose faces emerged from the shadows expressing the deepest human emotions. The Catholic painters of the Counter-Reformation, like Rubens, went in the opposite direction; they filled their paintings with bright and rich colors. The new Baroque churches of the Counter-Reformation were usually shining white inside and filled with statues, frescoes, marble, gold and colorful paintings, to appeal to the public. But European Catholics of all classes, like Protestants, eventually adopted a sober wardrobe that was mostly black, brown and gray.
Witches and black cats.
In the second part of the 17th century, Europe and America experienced an epidemic of fear of witchcraft. People widely believed that the devil appeared at midnight in a ceremony called a black mass or black sabbath, usually in the form of a black animal, often a goat, a dog, a wolf, a bear, a deer or a rooster, accompanied by their familiar spirits, black cats, serpents and other black creatures. This was the origin of the widespread superstition about black cats and other black animals. In Medieval Flanders, in a ceremony called "Kattenstoet". black cats were thrown from the belfry of the Cloth Hall of Ypres to ward off witchcraft.
Witch trials were common in both Europe and America during this period. During the notorious Salem witch trials in New England in 1692–93, one of those on trial was accused of being able turn into a "black thing with a blue cap," and others of having familiars in the form of a black dog, a black cat and a black bird. Nineteen women and men were hanged as witches.
The 18th and 19th centuries.
In the 18th century, during the European Age of Enlightenment, black receded as a fashion color. Paris became the fashion capital, and pastels, blues, greens, yellow and white became the colors of the nobility and upper classes. But after the French Revolution, black again became the dominant color.
Black was the color of the industrial revolution, largely fueled by coal, and later by oil. Thanks to coal smoke, the buildings of the large cities of Europe and America gradually turned black. Charles Dickens and other writers described the dark streets and smoky skies of London, and they were vividly illustrated in the engravings of French artist Gustave Doré.
A different kind of black was an important part of the romantic movement in literature. Black was the color of melancholy, the dominant theme of romanticism. The novels of the period were filled with castles, ruins, dungeons, storms, and meetings at midnight. The leading poets of the movement were usually portrayed dressed in black, usually with a white shirt and open collar, and a scarf carelessly over their shoulder, Percy Bysshe Shelley and Lord Byron helped create the enduring stereotpype of the romantic poet.
The invention of new, inexpensive synthetic black dyes and the industrialization of the textile industry meant that good-quality black clothes were available for the first time to the general population. In the 19th century gradually black became the most popular color of business dress of the upper and middle classes in England, the Continent, and America.
Black dominated literature and fashion in the 19th century, and played a large role in painting. James McNeil Whistler made the color the subject of his most famous painting, "Arrangement in grey and black number one" (1871), better known as "Whistler's Mother".
Some 19th-century French painters had a low opinion of black: "Reject black," Paul Gauguin said, "and that mix of black and white they call gray. Nothing is black, nothing is gray." But Édouard Manet used blacks for their strength and dramatic effect. Manet's portrait of painter Berthe Morisot was a study in black which perfectly captured her spirit of independence. The black gave the painting power and immediacy; he even changed her eyes, which were green, to black to strengthen the effect. Henri Matisse quoted the French impressionist Pissarro telling him, "Manet is stronger than us all - he made light with black."
Auguste Renoir used luminous blacks, especially in his portraits. When someone told him that black was not a color, Renoir replied: "What makes you think that? Black is the queen of colors. I always detested Prussian blue. I tried to replace black with a mixure of red and blue, I tried using cobalt blue or ultramarine, but I always came back to ivory black."
Vincent van Gogh used black lines to outline many of the objects in his paintings, such as the bed in the famous painting of his bedroom. making them stand apart. His painting of black crows over a cornfield, painted shortly before he died, was particularly agitated and haunting.
In the late 19th century, black also became the color of anarchism. (See political movements.)
20th and 21st centuries.
In the 20th century, black was the color of Italian and German fascism. (See political movements.)
In art, black regained some of the territory that it had lost during the 19th century. The Russian painter Kasimir Malevich, a member of the Suprematist movement, created the Black Square in 1915, is widely considered the first purely abstract painting. He wrote, "The painted work is no longer simply the imitation of reality, but is this very reality ... It is not a demonstration of ability, but the materialization of an idea."
Black was also appreciated by Henri Matisse. "When I didn't know what color to put down, I put down black," he said in 1945. "Black is a force: I used black as ballast to simplify the construction ... Since the impressionists it seems to have made continuous progress, taking a more and more important part in color orchestration, comparable to that of the double bass as a solo instrument."
In the 1950s, black came to be a symbol of individuality and intellectual and social rebellion, the color of those who didn't accept established norms and values. In Paris, it was worn by Left-Bank intellectuals and performers such as Juliette Greco, and by some members of the Beat Movement in New York and San Francisco. Black leather jackets were worn by motorcycle gangs such as the Hells Angels and street gangs on the fringes of society in the United States. Black as a color of rebellion was celebrated in such films as The Wild One, with Marlon Brando. By the end of the 20th century, black was the emblematic color of the punk subculture punk fashion, and the goth subculture. Goth fashion, which emerged in England in the 1980s, was inspired by Victorian era mourning dress.
In men's fashion, black gradually ceded its dominance to navy blue, particularly in business suits. Black evening dress and formal dress in general were worn less and less. In 1960, John F. Kennedy was the last American President to be inaugurated wearing formal dress; President Lyndon Johnson and all his successors were inaugurated wearing business suits.
Women's fashion was revolutionized and simplified in 1926 by the French designer Coco Chanel, who published a drawing of a simple black dress in "Vogue" magazine. She famously said, "A woman needs just three things; a black dress, a black sweater, and, on her arm, a man she loves." Other designers contributed to the trend of the little black dress. The Italian designer Gianni Versace said, "Black is the quintessence of simplicity and elegance," and French designer Yves Saint Laurent said, "black is the liaison which connects art and fashion. One of the most famous black dresses of the century was designed by Hubert de Givenchy and was worn by Audrey Hepburn in the 1961 film "Breakfast at Tiffany's".
The American Civil Rights Movement in the 1950s was a struggle for the political equality of African Americans. It developed into the Black Power movement in the late 1960s and 1970s, and popularized the slogan "Black is Beautiful".
In the 1990s, the Black Standard became the banner of many Islamic groups that take militant interpretations of jihad. (See political movements.)
Science.
Optics.
In the visible spectrum, white reflects light and is a presence of all colors, but black absorbs light and is an absence of color.
Black can be defined as the visual impression experienced when no visible light reaches the eye. (This makes a contrast with whiteness, the impression of any combination of colors of light that equally stimulates all three types of color-sensitive visual receptors.)
Pigments or dyes that absorb light rather than reflect it back to the eye "look black". A black pigment can, however, result from a "combination" of several pigments that collectively absorb all colors. If appropriate proportions of three primary pigments are mixed, the result reflects so little light as to be called "black".
This provides two superficially opposite but actually complementary descriptions of black. Black is the lack of all colors of light, or an exhaustive combination of multiple colors of pigment. See also primary colors.
In physics, a black body is a perfect absorber of light, but, by a thermodynamic rule, it is also the best emitter. Thus, the best radiative cooling, out of sunlight, is by using black paint, though it is important that it be black (a nearly perfect absorber) in the infrared as well.
In elementary science, far ultraviolet light is called "black light" because, while itself unseen, it causes many minerals and other substances to fluoresce.
On January 16, 2008, researchers from Troy, New York's Rensselaer Polytechnic Institute announced the creation of the darkest material on the planet. The material, which reflects only .045 percent of light, was created from carbon nanotubes stood on end. This is 1/30 of the light reflected by the current standard for blackness, and one third the light reflected by the previous record holder for darkest substance.
A material is said to be black if most incoming light is absorbed equally in the material. Light (electromagnetic radiation in the visible spectrum) interacts with the atoms and molecules, which causes the energy of the light to be converted into other forms of energy, usually heat. This means that black surfaces can act as thermal collectors, absorbing light and generating heat(see Solar thermal collector).
Absorption of light is contrasted by transmission, reflection and diffusion, where the light is only redirected, causing objects to appear transparent, reflective or white respectively.
Chemistry of black pigments, dyes, and inks.
Pigments.
The earliest pigments used by Neolithic man were charcoal, red ocher and yellow ocher. The black lines of cave art were drawn with the tips of burnt torches made of a wood with resin.
Different charcoal pigments were made by burning different woods and animal products, each of which produced a different tone. The charcoal would be ground and then mixed with animal fat to make the pigment.
The 15th-century painter Cennino Cennini described how this pigment was made during the Renaissnace in his famous handbook for artists: "...There is a black which is made from vine twigs; these twigs are to be burned; and when they are burned, throw water on them, and quench them; and then work them up like an other black. And is a color both black, and lean; and it is one of the perfect colors which we eomploy."
Cennini also noted that "there is another black which is made from burnt almond shells or peach stones, and this is a perfect black, and fine." Similar fine blacks were made by burning the pits of the peach, cherry or apricot. The powdered charcoal was then mixed with gum arabic or the yellow of an egg to make a paint.
Different civilizations burned different plants to produce their charcoal pigments. The Inuit of Alaska used wood charcoal mixed with the blood of seals to paint masks and wooden objects. The Polynesians burned coconuts to produce their pigment.
Dyes.
Good-quality black dyes were not known until the middle of the 14th century. The most common early dyes were made from bark, roots or fruits of different trees; usually the walnut, chestnut, or certain oak trees. The blacks produced were often more gray, brown or bluish. The cloth had to be dyed several times to darken the color. One solution used by dyers was add to the dye some iron filings, rich in iron oxide, which gave a deeper black. Another was to first dye the fabric dark blue, and then to dye it black.
A much richer and deeper black dye was eventually found made from the Oak apple or gall-nut. The gall-nut is a small round tumor which grows on oak and other varieties of trees. They range in size from 2–5 cm, and are caused by chemicals injected by the larva of certain kinds of gall wasp in the family Cynipidae. The dye was very expensive; a great quantity of gall-nuts were needed for a very small amount of dye. The gall-nuts which made the best dye came from Poland, eastern Europe, the near east and North Africa. Beginning in about the 14th century, dye from gall-nuts was used for clothes of the kings and princes of Europe.
Another important source of natural black dyes from the 17th century onwards was the logwood tree, or Haematoxylum campechianum, which also produced reddish and bluish dyes. It is a species of flowering tree in the legume family, Fabaceae, that is native to southern Mexico and northern Central America. The modern nation of Belize grew from 17th century English logwood logging camps.
Since the mid-19th century, synthetic black dyes have largely replaced natural dyes. One of the important synthetic blacks is Nigrosin, a mixture of synthetic black dyes (CI 50415, Solvent black 5) made by heating a mixture of nitrobenzene, aniline and aniline hydrochloride in the presence of a copper or iron catalyst. Its main industrial uses are as a colorant for lacquers and varnishes and in marker-pen inks.
Inks.
The first known inks were made by the Chinese, and date back to the 23rd century B.C. They used natural plant dyes and minerals such as graphite ground with water and applied with an ink brush. Early Chinese inks similar to the modern inkstick have been found dating to about 256 BC at the end of the Warring States period. They were produced from soot, usually produced by burning pine wood, mixed with animal glue. To make ink from an inkstick, the stick is continuously ground against an inkstone with a small quantity of water to produce a dark liquid which is then applied with an ink brush. Artists and calligraphists could vary the thickness of the resulting ink by reducing or increasing the intensity and time of ink grinding. These inks produced the delicate shading and subtle or dramatic effects of Chinese brush painting.
India ink (or Indian ink in British English) is a black ink once widely used for writing and printing and now more commonly used for drawing, especially when inking comic books and comic strips. The technique of making it probably came from China. India ink has been in use in India since at least the 4th century BC, where it was called "masi". In India, the black color of the ink came from bone char, tar, pitch and other substances.
The Ancient Romans had a black writing ink they called "Atramentum librarium". Its name came from the Latin word "atrare", which meant to make something black. (This was the same root as the English word "atrocious".) It was usually made, like India ink, from soot, although one variety, called "atrementum elaphantinum", was made by burning the ivory of elephants.
Gall-nuts were also used for making fine black writing ink. Iron gall ink (also known as iron gall nut ink or oak gall ink) was a purple-black or brown-black ink made from iron salts and tannic acids from gall nut. It was the standard writing and drawing ink in Europe, from about the 12th century to the 19th century, and remained in use well into the 20th century.
Astronomy.
Black holes of stellar mass are expected to form when very massive stars collapse at the end of their life cycle. After a black hole has formed it can continue to grow by absorbing mass from its surroundings. By absorbing other stars and merging with other black holes, supermassive black holes of millions of solar masses may form. There is general consensus that supermassive black holes exist in the centers of most galaxies.
Why the night sky and space are black - Olbers' Paradox.
The fact that outer space is black is sometimes called Olbers' paradox. In theory, since the universe is full of stars, and is believed to be infinitely large, it would be expected that the light of an infinite number of stars would be enough to brilliantly light the whole universe all the time. However, the background color of outer space is black. This contradiction was first noted in 1823 by German astronomer Heinrich Wilhelm Matthias Olbers, who posed the question of why the night sky was black.
The current accepted answer is that, while the universe is infinitely large, it is not infinitely old. It is thought to be about 15 billion years old, so we can only see objects as far away as the distance light can travel in 15 billion years. Light from stars farther away has not reached Earth, and cannot contribute to making the sky bright. Also, as the universe is expanding, many stars are moving away from the Earth. As they move, the wavelength of their light becomes longer, through the Doppler effect, and shifts toward red, or even becomes invisible. As a result of these two phenomena, there is not enough starlight to make space anything but black.
The daytime sky on Earth is blue because the light from the Sun strikes molecules in the Earth's atmosphere and scatters in all directions. Blue light is scattered more than other colors, and reaches the eye in greater quantities, making the daytime sky look blue. This is known as Rayleigh scattering.
The nighttime sky seen from Earth is black because the part of Earth experiencing night is facing away from the Sun, the light of the Sun is blocked by the Earth, and there is no other bright nighttime source of light in the vicinity. Thus, there is not enough light to undergo Rayleigh scattering and make the sky blue. On the Moon, on the other hand, because there is no atmosphere to scatter the light, the sky is black both day and night. This also holds true for any other location without an atmosphere.
Political movements.
Anarchism is a political philosophy, most popular in the late 19th and early 20th centuries, which holds that governments and capitalism are harmful and undesirable. The symbols of anarchism was usually either a black flag or a black letter A. More recently it is usually represented with a bisected red and black flag, to emphasise the movement's socialist roots in the First International. Anarchism was most popular in Spain, France, Italy, Ukraine and Argentina. There were also small but influential movements in the United States and Russia. In the latter, the movement initially allied itself with the Bolsheviks.
The Black Army was a collection of anarchist military units which fought in the Russian Civil War, sometimes on the side of the Bolshevik Red Army, and sometimes for the opposing White Army. It was officially known as the Revolutionary Insurrectionary Army of Ukraine, and it was under the command of the famous anarchist Nestor Makhno.
Fascism. The Blackshirts () were Fascist paramilitary groups in Italy during the period immediately following World War I and until the end of World War II. The Blackshirts were officially known as the Voluntary Militia for National Security ("Milizia Volontaria per la Sicurezza Nazionale", or MVSN).
Inspired by the black uniforms of the Arditi, Italy's elite storm troops of World War I, the Fascist Blackshirts were organized by Benito Mussolini as the military tool of his political movement. They used violence and intimidation against Mussolini's opponents. The emblem of the Italian fascists was a black flag with fasces, an axe in a bundle of sticks, an ancient Roman symbol of authority. Mussolini came to power in 1922 through his March on Rome with the blackshirts.
Black was also adopted by Adolf Hitler and the Nazis in Germany. Red, white and black were the colors of the flag of the German Empire from 1870 to 1918. In "Mein Kampf", Hitler explained that they were "revered colors expressive of our homage to the glorious past." Hitler also wrote that "the new flag ... should prove effective as a large poster" because "in hundreds of thousands of cases a really striking emblem may be the first cause of awakening interest in a movement." The black swastika was meant to symbolize the Aryan race, which, according to the Nazis, "was always anti-Semitic and will always be anti-Semitic." Several designs by a number of different authors were considered, but the one adopted in the end was Hitler's personal design. Black became the color of the uniform of the SS, the "Schutzstaffel" or "defense corps", the paramilitary wing of the Nazi Party, and was worn by SS officers from 1932 until the end of World War II.
The Nazis used a black triangle to symbolize anti-social elements. The symbol originates from Nazi concentration camps, where every prisoner had to wear one of the Nazi concentration camp badges on their jacket, the color of which categorized them according to "their kind." Many Black Triangle prisoners were either mentally disabled or mentally ill. The homeless were also included, as were alcoholics, the Romani people, the habitually "work-shy," prostitutes, draft dodgers and pacifists. More recently the black triangle has been adopted as a symbol in lesbian culture and by disabled activists.
Black shirts were also worn by the British Union of Fascists before World War II, and members of fascist movements in the Netherlands and India.
Patriotic Resistance. The Lützow Free Corps, composed of volunteer German students and academics fighting against Napoleon in 1813, could not afford to make special uniforms and therefore adopted black, as the only color that could be used to dye their civilian clothing without the original color showing. In 1815 the students began to carry a red, black and gold flag, which they believed (incorrectly) had been the colors of the Holy Roman Empire (the imperial flag had actually been gold and black). In 1848, this banner became the flag of the German confederation. In 1866, Prussia unified Germany under its rule, and imposed the red, white and black of its own flag, which remained the colors of the German flag until the end of the Second World War. In 1949 the Federal Republic of Germany returned to the original flag and colors of the students and professors of 1815, which is the flag of Germany today.
Islamism. The Black Standard (, also known as "banner of the eagle" or simply as "the banner") is the historical flag flown by Muhammad in Islamic tradition, an eschatological symbol in Shi'a Islam (heralding the advent of the Mahdi), and a symbol used in Islamism and Jihadism.
Religion.
In Christianity, the Devil is considered the "prince of darkness." The term was used in John Milton's poem "Paradise Lost", published in 1667, referring to Satan, who is viewed as the embodiment of evil. It is an English translation of the Latin phrase "princeps tenebrarum", which occurs in the "Acts of Pilate", written in the fourth century, in the 11th-century hymn "Rhythmus de die mortis" by Pietro Damiani, and in a sermon by Bernard of Clairvaux from the 12th century. The phrase also occurs in "King Lear" by William Shakespeare (c. 1606), Act III, Scene IV, l. 14:
'The prince of darkness is a gentleman."
Priests and pastors of the Roman Catholic, Eastern Orthodox and Protestant churches commonly wear black, as do monks of the Benedictine Order, who consider it the color of humility and penitence.
Associations and symbolism.
Mourning.
In Europe and America, black is the color most commonly associated with mourning and bereavement. It is the color traditionally worn at funerals and memorial services. In some traditional societies, for example in Greece and Italy, some widows wear black for the rest of their lives. In contrast, across much of Africa and parts of Asia, white is a color of mourning and is worn during funerals.
In Victorian England, the colors and fabrics of mourning were specified in an unofficial dress code: "non-reflective black paramatta and crape for the first year of deepest mourning, followed by nine months of dullish black silk, heavily trimmed with crape, and then three months when crape was discarded. Paramatta was a fabric of combined silk and wool or cotton; crape was a harsh black silk fabric with a crimped appearance produced by heat. Widows were allowed to change into the colors of half-mourning, such as gray and lavender, black and white, for the final six months."
A "black day" (or week or month) usually refers to tragic date. The Romans marked "fasti" days with white stones and "nefasti" days with black. The term is often used to remember massacres. Black months include the Black September in Jordan, when large numbers of Palestinians were killed, and Black July in Sri Lanka, the killing of members of the Tamil population by the Sinhalese government.
In the financial world, the term often refers to a dramatic drop in the stock market. For example, the Wall Street Crash 1929, the stock market crash on October 29, 1929, which is the start of the Great Depression, is nicknamed Black Tuesday, and was preceded by Black Thursday, a downturn on October 24 the previous week.
Darkness and evil.
In western popular culture, black has long been associated with evil and darkness. It is the traditional color of witchcraft and black magic.
In the Book of Revelation, the last book in the New Testament of the BIble, the Four Horsemen of the Apocalypse are supposed to announce the Apocalypse before the Last Judgment. The horseman representing famine rides a black horse.
The vampire of literature and films, such as Count Dracula of the Bram Stoker novel, dressed in black, and could only move at night. The Wicked Witch of the West in the 1939 film "The Wizard of Oz", became the archetype of witches for generations of children. Whereas witches and sorcerers inspired real fear in the 17th century, in the 21st century children and adults dressed as witches for Halloween parties and parades.
Power, authority, and solemnity.
Black is frequently used as a color of power, law and authority. In many countries judges and magistrates wear black robes. That custom began in Europe in the 13th and 14th centuries. Jurists, magistrates and certain other court officials in France began to wear long black robes during the reign of Philip IV of France (1285–1314), and in England from the time of Edward I (1271–1307). The custom spread to the cities of Italy at about the same time, between 1300 and 1320. The robes of judges resembled those worn by the clergy, and represented the law and authority of the King, while those of the clergy represented the law of God and authority of the church.
Until the 20th century most police uniforms were black, until they were largely replaced by a less menacing blue in France, the U.S. and other countries. In both the United States and Britain, police cars are frequently black and white. Such British police cars are called panda cars and an American police car is often simply called a Black and whites. The riot control units of the Basque Autonomous Police in Spain are known as "beltzak" ("blacks") after their uniform.
Black today is the most common color for limousines and the official cars of government officials.
Black evening dress is still worn at many solemn occasions or ceremonies, from graduations to formal balls. Graduation gowns are copied from the gowns worn by university professors in the Middle Ages, which in turn were copied from the robes worn by judges and priests, who often taught at the early universities. The mortarboard hat worn by graduates is adapted from a square cap called a biretta worn by Medieval professors and clerics
Functionality.
In the 19th and 20th centuries, many machines and devices, large and small, were painted black, to stress their functionality. These included telephones, sewing machines, steamships, railroad locomotives, and automobiles. The Ford Model T, the first mass-produced car, was available only in black from 1914 to 1926. Of means of transportation, only airplanes were rarely ever painted black.
Race and color.
Black is also commonly used as a racial description in the United Kingdom, since ethnicity was first measured in the 2001 census. The 2011 British census asked residents to describe themselves, and categories offered included Black, African, Caribbean, or Black British. Other possible categories were African British, African Scottish, Caribbean British and Caribbean Scottish. Of the total UK population in 2001, 1.01 percent identified themselves as Black Caribbean, .8 percent as Black African, and .2 percent as Black (others).
In Canada, census respondents can identify themselves as Black. In the 2006 census, 2.5 percent of the population identified themselves as black.
In Australia, the term black is not used in the census. In the 2006 census, 2.3 percent of Australians identified themselves as Aboriginal and/or Torres Strait Islanders.
In Brazil, the Brazilian Institute of Geography and Statistics (IBGE) asks people to identify themselves as "branco" (white), "pardo" (brown), "preto" (black), or "amarelo" (yellow). In 2008 6.84 percent of the population identified themselves as "preto".
Black chambers and black ops.
Black is commonly associated with secrecy.
Elegance – black and fashion.
Black is the color most commonly associated with elegance in Europe and the United States, followed by silver, gold, and white.
Black first became a fashionable color for men in Europe in the 17th century, in the courts of Italy and Spain. (See history above). In the 19th century, it was the fashion for men both in business and for evening wear, in the form of a black coat whose tails came down the knees. In the evening it was the custom of the men to leave the women after dinner to go to a special smoking room to enjoy cigars or cigarettes. This meant that their tailcoats eventually smelled of tobacco. According to the legend, in 1865 Edward VII, then the Prince of Wales, had his tailor make a special short smoking jacket. The smoking jacket then evolved into the dinner jacket. Again according to legend, the first Americans to wear the jacket were members of the Tuxedo Club in New York State. Thereafter the jacket became known as a tuxedo in the U.S. The term "smoking" is still used today in Russia and other countries.
The tuxedo was always black until the 1930s, when the Duke of Windsor began to wear a tuxedo that was a very dark midnight blue. He did so because a black tuxedo looked greenish in artificial light, while a dark blue tuxedo looked blacker than black itself.
For women's fashion, the defining moment was the invention of the simple black dress by Coco Chanel in 1926. (See history.) Thereafter, a long black gown was used for formal occasions, while the simple black dress could be used for everything else. The designer Karl Lagerfeld, explaining why black was so popular, said: "Black is the color that goes with everything. If you're wearing black, you're on sure ground." Skirts have gone up and down and fashions have changed, but the black dress has not lost its position as the essential element of a woman's wardrobe. The fashion designer Christian Dior said, "elegance is a combination of distinction, naturalness, care and simplicity," and black exemplified elegance.
The expression "X is the new black" is a reference to the latest trend or fad that is considered a wardrobe basic for the duration of the trend, on the basis that black is always fashionable. The phrase has taken on a life of its own and has become a cliché.
Many performers of both popular and European classical music, including French singers Edith Piaf and Juliette Greco, and violinist Joshua Bell have traditionally worn black on stage during performances. A black costume was usually chosen as part of their image or stage persona, or because it did not distract from the music, or sometimes for a political reason. Country-western singer Johnny Cash always wore black on stage. In 1971, Cash wrote the song "Man in Black" to explain why he dressed in that color: "We're doing mighty fine I do suppose / In our streak of lightning cars and fancy clothes / But just so we're reminded of the ones who are held back / Up front there ought to be a man in black."
Black in Asian culture.
In China, the color black is associated with water, one of the five fundamental elements believed to compose all things; and with winter, cold, and the direction north, usually symbolized by a black tortoise. it is also associated with disorder, including the positive disorder which leads to change and new life. When the first Emperor of China Qin Shi Huang seized power from the Zhou Dynasty, he changed the Imperial color from red to black, saying that black extinguished red. Only when the Han Dynasty appeared in 206 A.D. was red restored as the imperial color.
The Chinese and Japanese character for black ("kuro" in Japanese), can, depending upon the context, also mean dark or evil.
In Japan, black is associated with mystery, the night, the unknown, the supernatural, the invisible and death. Combined with white, it can symbolize intuition.
In Japan in the 10th and 11th century, it was believed that wearing black could bring misfortune. It was worn at court by those who wanted to set themselves apart from the established powers or who had renounced material possessions.
In Japan black can also symbolize experience, as opposed to white, which symbolizes naiveté. The black belt in martial arts symbolizes experience, while a white belt is worn by novices. Japanese men traditionally wear a black kimono with some white decoration on their wedding day.
In Indonesia black is associated with depth, the subterranean world, demons, disaster, and the left hand. When black is combined with white, however, it symbolizes harmony and equilibrium.

</doc>
<doc id="4036" url="http://en.wikipedia.org/wiki?curid=4036" title="Black Flag">
Black Flag

Black Flag or black flag may refer to:

</doc>
<doc id="4037" url="http://en.wikipedia.org/wiki?curid=4037" title="Bletchley Park">
Bletchley Park

Bletchley Park, in Milton Keynes, Buckinghamshire, was the central site of the United Kingdom's Government Code and Cypher School (GC&CS), which during the Second World War regularly penetrated the secret communications of the Axis Powersmost importantly the German Enigma and Lorenz ciphers.
The official historian of World War II British Intelligence has written that the "Ultra" intelligence produced at Bletchley shortened the war by two to four years, and that without it the outcome of the war would have been uncertain.
The site is now an educational and historical attraction memorialising and celebrating those accomplishments.
Site.
Fifty miles (80 km) northwest of London, the site appears in the Domesday Book as part of the Manor of Eaton.
Browne Willis built a mansion there in 1711, but after Thomas Harrison purchased the property in 1793 this was pulled down.
It was first known as Bletchley Park after its purchase by Samuel Lipscomb Seckham in 1877.
The estate of was bought in 1883 by Sir Herbert Samuel Leon, who expanded the then-existing farmhouse into the present
"maudlin and monstrous pile"
combining Victorian Gothic, Tudor, and Dutch Baroque styles.
In 1938 the mansion and much of the site was bought by a builder planning a housing estate,
but in May 1938 Admiral Sir Hugh Sinclair, head of the Secret Intelligence Service (SIS or MI6) bought the mansion and for use by GC&CS and SIS in the event of war.
A key advantage seen by Sinclair and his colleagues (inspecting the site under the cover of "Captain Ridley's shooting party")
was Bletchley's geographical centrality.
It was almost immediately adjacent to Bletchley railway station, where the "Varsity Line" between Oxford and Cambridgewhose universities were expected to supply many of the code-breakersmet the main West Coast railway line connecting London, Birmingham, Manchester, Liverpool, Glasgow and Edinburgh.
Watling Street, the main road linking London to the north-west (now the A5) was close by, and high-volume communication links were available at the telegraph and telephone repeater station in nearby Fenny Stratford.
Bletchley Park was known as "B.P." to those who worked there.
"Station X", "London Signals Intelligence Centre", and "Government Communications Headquarters" were all cover names used during the war.
Personnel.
Commander Alastair Denniston was operational head of GC&CS from 1919 to 1942, beginning with its formation from the Admiralty's Room 40 (NID25) and the War Office's MI1b. Key GC&CS cryptanalysts who moved from London to Bletchley Park included John Tiltman, Dillwyn "Dilly" Knox, Josh Cooper and Nigel de Grey. These people had a variety of backgroundslinguists, chess champions, and crossword experts were common, and in Knox's case papyrology.
On the day Britain declared war on Germany, Denniston wrote to the Foreign Office about recruiting "men of the professor type". Personal networking drove early recruitments, particularly of men from the universities of Cambridge and Oxford. Trustworthy women were similarly recruited for administrative and clerical jobs. In one 1941 recruiting stratagem "The Daily Telegraph" was asked to organise a crossword competition, after which promising contestants were discreetly approached about "a particular type of work as a contribution to the war effort".
Denniston recognised, however, that the enemy's use of electromechanical cipher machines meant that formally trained mathematicians would be needed as well; Oxford's Peter Twinn joined GC&CS in February 1939; Cambridge's Alan Turing and Gordon Welchman began training in 1938 and reported to Bletchley the day after war was declared, along with John Jeffreys. Later-recruited cryptanalysts included the mathematicians Derek Taunt, Jack Good, Bill Tutte, and Max Newman; historian Harry Hinsley, and chess champions Hugh Alexander and Stuart Milner-Barry.
This eclectic staff of "Boffins and Debs" caused GC&CS to be whimsically dubbed the "Golf, Cheese and Chess Society", with the female staff in Dilwyn Knox's section sometimes termed "Dilly's Fillies". During a September 1941 morale-boosting visit, Winston Churchill reportedly remarked to Denniston: "I told you to leave no stone unturned to get staff, but I had no idea you had taken me so literally." Six weeks later, having failed to get sufficient typing and unskilled staff to achieve the productivity that was possible, Turing, Welchman, Alexander and Milner-Barry wrote directly to Churchill. His response was "Action this day make sure they have all they want on extreme priority and report to me that this has been done."
After initial training at the Inter-Service Special Intelligence School set up by John Tiltman (initially at an RAF depot in Buckingham and later in Bedfordwhere it was known locally as "the Spy School")
staff worked a six-day week, rotating through three shifts: 4p.m. to midnight, midnight to 8a.m. (the most disliked shift), and 8a.m. to 4p.m., each with a half-hour meal break. At the end of the third week a worker went off at 8a.m. and came back at 4p.m., thus putting in sixteen hours on that last day. The irregular hours affected workers' health and social life, as well as the routines of the nearby homes at which most staff lodged. The work was tedious and demanded intense concentration; staff got one week's leave four times a year, but some "girls" collapsed and required extended rest.
A small number of men (e.g. Post Office experts in Morse code or German) worked part-time.
In January 1945, at the peak of codebreaking efforts, some 9000 personnel were working at Bletchley; over 12,000 different persons (some 80% of them women) were assigned there at various points throughout the war.
Secrecy.
Properly used, the German Enigma and Lorenz ciphers should have been virtually unbreakable, but flaws in German cryptographic procedures, and poor discipline among the personnel carrying them out, created vulnerabilities which made Bletchley's attacks just barely feasible.
These vulnerabilities, however, could have been remedied by relatively simple improvements in enemy procedures,
and such changes would certainly have been implemented had Germany any hint of Bletchley's success.
Thus the intelligence Bletchley produced was considered wartime Britain's "Ultra secret"higher even than the normally highest classification "Most and security was paramount.
Few outside Bletchley knew its mission, and even fewer (inside or outside) understood the breadth of that mission and the extent of its success. All staff signed the Official Secrets Act (1939) and a 1942 security warning emphasised the importance of discretion even within Bletchley itself: "Do not talk at meals. Do not talk in the transport. Do not talk travelling. Do not talk in the billet. Do not talk by your own fireside. Be careful even in your Hut...
In addition, any commander in the field receiving Ultra intelligence was fed a cover story crediting a non-Ultra source; at times sham scouting missionsintentionally visible to the enemywere dispatched to "discover" German positions in fact already known from Ultra.
In some cases it was impossible to act on Ultra intelligence at all because to do so might suggest to the enemy that their communications had been penetrated,
though certain claims that the British authorities refused, for this reason, to take steps to protect civilians from imminent harm have been vigorously disputedsee, for example, Coventry Blitz.
Early work.
Turing, Knox, and Jeffreys did their early work in Number 3 Cottage.
The first personnel of the Government Code and Cypher School (GC&CS) moved to Bletchley Park on 15 August 1939.
The Naval, Military, and Air Sections were on the ground floor of the mansion, together with a telephone exchange, teleprinter room, kitchen, and dining room; the top floor was allocated to MI6.
Construction of the wooden huts began, and Elmers School, a neighbouring boarding school, was acquired for the Commercial and Diplomatic Sections.
After the United States joined the war a number of American cryptographers were posted to Hut 3, and from May 1943 onwards there was close co-operation between British and American intelligence.
In contrast the Soviet Union was never officially told of Bletchley Park and its activities
a reflection of Churchill's distrust of the Soviets even during the US-UK-USSR alliance imposed by the Nazi threat.
The only direct enemy damage to the site was done 2021November 1940 by three bombs likely intended for Bletchley railway station; Hut4, shifted two feet off its foundation, was winched back into place as
work inside continued.
Intelligence reporting.
Non-naval Enigma messages were deciphered in Hut 6, followed by translation, indexing and cross-referencing, in Hut 3.
Only then was it sent out to the Secret Intelligence Service (MI6), the intelligence chiefs in the relevant ministries, and later on to high-level commanders in the field.
Naval Enigma deciphering was in Hut 8, with translation in Hut 4.
Verbatim translations were sent only to the Naval Intelligence Division (NID) of the Admiralty's Operational Intelligence Centre (OIC), supplemented by information from indexes as to the meaning of technical terms and cross-references from a knowledge store of German naval technology.
Hut 4 also decoded a manual system known as the dockyard cipher, which sometimes carried messages that were also sent on an Enigma network.
Feeding these back to Hut8 provided excellent "cribs" for Known-plaintext attacks on the daily naval Enigma key.
Listening stations.
Initially, a wireless room was established at Bletchley Park.
It was set up in the mansion's water tower under the code name "Station X", a term now sometimes applied to the codebreaking efforts at Bletchley as a whole.
The "X" is the Roman numeral "ten", this being the Secret Intelligence Service's tenth such station.
Due to the long radio aerials stretching from the wireless room, the radio station was moved from Bletchley Park to nearby Whaddon Hall to avoid drawing attention to the site.
Subsequently, other listening stationsthe Y-stations, such as the ones at Chicksands in Bedfordshire, Beaumanor Hall, Leicestershire (where the headquarters of the War Office "Y" Group was located) and Beeston Hill Y Station in Norfolkgathered raw signals for processing at Bletchley.
Coded messages were taken down by hand and sent to Bletchley on paper by motorcycle despatch riders or (later) by teleprinter.
Bletchley Park is mainly remembered for breaking the German Enigma cypher, but its greatest cryptographic achievement may have been the breaking of the German on-line teleprinter Lorenz cipher (known at GC&CS as "Tunny").
Additional buildings.
The wartime needs required the building of additional accommodation.
Huts.
Often a hut's number became so strongly associated with the work performed inside that even when the work was moved to another building it was still referred to by the original "Hut" designation. are:
Blocks.
In addition to the wooden huts there were a number of brick-built "blocks".
Work on specific countries' signals.
German signals.
Most German messages decrypted at Bletchley were produced by one or another version of the Enigma cipher machine, but an important minority were produced by the even more complicated twelve-rotor Lorenz SZ42 on-line teleprinter cipher machine.
Five weeks before the outbreak of war, in Warsaw, Poland's Cipher Bureau revealed its achievements in breaking Enigma to astonished French and British personnel. The British used the Poles' information and techniques, and the Enigma clone sent to them in August 1939, which greatly increased their (previously very limited) success in decrypting Enigma messages.
The bombe was an electromechanical device whose function was to discover some of the daily settings of the Enigma machines on the various German military networks.
Its pioneering design was developed by Alan Turing (with an important contribution from Gordon Welchman) and the machine was engineered by Harold 'Doc' Keen of the British Tabulating Machine Company.
Each machine was about high and wide, deep and weighed about a ton.
At its peak, GC&CS was reading approximately 4,000 messages per day. 
As a hedge against enemy attack most bombes were dispersed to installations at Adstock and Wavendon (both later supplanted by installations at Stanmore and Eastcote), and Gayhurst.
Luftwaffe messages were the first to be read in quantity.
The German navy had much tighter procedures, and the capture of code books was needed before they could be broken.
When, in February 1942, the German navy introduced the four-rotor Enigma for communications with its Atlantic U-boats, this traffic became unreadable for a period of ten months.
Britain produced modified bombes, but it was the success of the US Navy bombe that was the main source of reading messages from this version of Enigma for the rest of the war.
Messages were sent to and fro across the Atlantic by enciphered teleprinter links.
The Lorenz messages were codenamed "Tunny" at Bletchley Park. They were only sent in quantity from mid-1942. The Tunny networks were used for high-level messages between German High Command and field commanders.
With the help of German operator errors, the cryptanalysts in the Testery (named after Ralph Tester, its head) worked out the logical structure of the machine despite not knowing its physical form.
They devised automatic machinery to help with decryption, which culminated in Colossus, the world's first programmable digital electronic computer.
This was designed and built by Tommy Flowers and his team at the Post Office Research Station at Dollis Hill.
The first was delivered to Bletchley Park in December 1943 and commissioned the following February.
Enhancements were developed for the Mark 2 Colossus, the first of which was working at Bletchley Park on the morning of D-day in June.
Flowers then produced one Colossus a month for the rest of the war, making a total of ten with an eleventh part-built.
The machines were operated mainly by Wrens in a section named the Newmanry after its head Max Newman.
Bletchley's work was essential to defeating the U-boats in the Battle of the Atlantic, and to the British naval victories in the Battle of Cape Matapan and the Battle of North Cape.
In 1941, Ultra exerted a powerful effect on the North African desert campaign against German forces under General Erwin Rommel.
General Sir Claude Auchinleck wrote that were it not for Ultra, "Rommel would have certainly got through to Cairo".
While not changing the events, "Ultra" decrypts featured prominently in the story of Operation SALAM, László Almásy's daring mission across the Libyan Desert behind enemy lines in 1942.
Prior to the Normandy landings on D-Day in June 1944, the Allies knew the locations of all but two of Germany's fifty-eight Western-front divisions.
Italian signals.
Italian signals had been of interest since Italy's attack on Abyssinia in 1935.
During the Spanish Civil War the Italian Navy used the K model of the commercial Enigma without a plugboard; this was solved by Knox in 1937.
When Italy entered the war in 1940 an improved version of the machine was used, though little traffic was sent by it and there were "wholesale changes" in Italian codes and cyphers.
Knox was given a new section for work on Enigma variations, which he staffed with women ("Dilly's girls") who included Margaret Rock, Jean Perrin, Clare Harding, Rachel Ronald, Elisabeth Granger; and Mavis Leverwho made the first break into the Italian naval traffic.
She solved the signals revealing the Italian Navy's operational plans before the Battle of Cape Matapan in 1941, leading to a British victory.
Although most Bletchley staff did not know the results of their work, Admiral Cunningham visited Bletchley in person a few weeks later to congratulate them.
On entering World War II in June 1940, the Italians were using book codes for most of their military messages.
The exception was the Italian Navy, which after the Battle of Cape Matapan started using the C-38 version of the Hagelin rotor-based cipher machine, particularly to route their navy and merchant marine convoys to the conflict in North Africa.
As a consequence, JRM Butler recruited his former student Bernard Willson to join a team with two others in Hut4. In June 1941, Willson became the first of the team to decode the Hagelin system, thus enabling military commanders to direct the Royal Navy and Royal Air Force to sink enemy ships carrying supplies from Europe to Rommel's Afrika Korps. This led to increased shipping losses and, from reading the intercepted traffic, the team learnt that between May and September 1941 the stock of fuel for the Luftwaffe in North Africa reduced by 90%.
After an intensive language course, in March 1944 Willson switched to Japanese language-based codes.
A Middle East Intelligence Centre (MEIC) was set up in Cairo in 1939.
When Italy entered the war in June 1940, delays in forwarding intercepts to Bletchley via congested radio links resulted in cryptanalysts being sent to Cairo.
A Combined Bureau Middle East (CBME) was set up in November, though the Middle East authorities made "increasingly bitter complaints" that GC&CS was giving too little priority to work on Italian cyphers.
However, the principle of concentrating high-grade cryptanalysis at Bletchley was maintained.
John Chadwick started cryptanalysis work in 1942 on Italian signals at the naval base 'HMS Nile' in Alexandria.
Later he was with GC&CS; in the Heliopolis Museum, Cairo and then in the Villa Laurens, Alexandria.
Soviet signals.
Soviet signals had been studied since the 1920s.
In 193940 John Tiltman (who had worked on Russian Army traffic from 1930) set up two Russian sections at Wavendon (a country house near Bletchley) and at Sarafand in Palestine.
Two Russian high-grade army and navy systems were broken.
Tiltman spent two weeks in Finland, where he obtained Russian traffic from Finland and Estonia in exchange for radio equipment.
In June 1941 when the Soviet Union became an ally, Churchill ordered a halt to intelligence operations against her.
In December 1941 the Russian section was closed down, but in late summer 1943 or late 1944 a small GC&CS Russian cypher section was set up in London overlooking Park Lane then in Sloane Square.
Japanese signals.
An outpost of the Government Code and Cypher School had been set up in Hong Kong in 1935, the Far East Combined Bureau (FECB). The FECB naval staff moved in 1940 to Singapore, then Colombo, Ceylon, then Kilindini, Mombasa, Kenya. They succeeded in deciphering Japanese codes with a mixture of skill and good fortune. The Army and Air Force staff went from Singapore to the Wireless Experimental Centre at Delhi, India.
In early 1942, a six-month crash course in Japanese, for 20 undergraduates from Oxford and Cambridge, was started by the Inter-Services Special Intelligence School in Bedford, in a building across from the main Post Office. This course was repeated every six months until war's end. 
Most of those completing these courses worked on decoding Japanese naval messages in Hut 7, under Col. J. Tiltman.
By mid-1945 well over 100 personnel were involved with this operation, which co-operated closely with the FECB and the US Signal intelligence Service at Arlington Hall, Virginia.
Because of these joint efforts, by August of that year the Japanese merchant navy was suffering 90% losses at sea. In 1999, Michael Smith wrote that: "Only now are the British codebreakers (like John Tiltman, Hugh Foss, and Eric Nave) beginning to receive the recognition they deserve for breaking Japanese codes and cyphers".
Post war.
Continued secrecy.
Much of Bletchley's equipment and documents were destroyed at the end of the war,
and the secrecy imposed on Bletchley staff remained in force, so that most relatives never knew more than that a child, spouse, or parent had done some kind of secret war work, or were told a cover story about clerical or statistical work.
Churchill referred to the Bletchley staff as "The geese that laid the golden eggs and never cackled".
With the publication of F.W. Winterbotham's "The Ultra Secret" (1974)
public discussion of Bletchley's work finally became possible (though even today some former staff still consider themselves bound to silence)
and in July 2009 the British government announced that Bletchley personnel would be recognised with a commemorative badge.
Site.
After the war the site passed through a succession of hands, and saw a number of uses, including as a teacher-training college and local GPO headquarters.
By 1991, the site was nearly empty and the buildings were at risk of demolition for redevelopment.
In February 1992 the Milton Keynes Borough Council declared most of the Park a conservation area, and the Bletchley Park Trust was formed to maintain the site as a museum. The site opened to visitors in 1993, and was formally inaugurated by HRH The Duke of Kent, as Chief Patron, in July 1994. In 1999 the Trust concluded an agreement with the landowner, giving control over much of the site to the Trust.
Museum.
Block B houses the main collection relating to the wartime codebreaking efforts, including the rebuilt bombe and the Enigma machine collection, extensive displays relating to wartime codebreaking and espionage generally, and
Stephen Kettle's life-size statue (2007) of Alan Turing,
As well as The National Museum of Computing, the park is also home to a number of other exhibits.
On 18 June 2014 the museum was officially re-opened by Catherine, Duchess of Cambridge when it was revealed that her grandmother had worked at Bletchley, Ms Glassborow, who married the duchess' grandfather Peter Middleton, was a civilian staff member at the centre where her twin sister Mary was also employed."
The National Museum of Computing.
This is housed in Block H, which is rented from the Bletchley Park Trust. Its Colossus and Tunny galleries tell an important part of allied breaking of German codes during World War II. There is a working reconstruction of a Colossus computer that was used on the high-level Lorenz cipher that was codenamed "Tunny" by the British.
The National Museum of Computing, which opened in 2007, is an independent voluntary organisation that is governed by its own board of trustees. The aim of the museum is "To collect and restore computer systems particularly those developed in Britain and to enable people to explore that collection for inspiration, learning and enjoyment." 
Through its many exhibits, the museum displays the story of computing through the mainframes of the 1960s and 1970s, and the rise of personal computing in the 1980s. It has a policy of having as many of the exhibits as possible in full working order. The Colossus and Tunny galleries are open daily. The rest of the Museum is open to the public every Thursday, Saturday and Sunday afternoons and most bank holidays, and by appointment for groups only, at other times. There are guided tours on Tuesday afternoons. There is a modest admission charge to the museum to help cover overheads.
RSGB National Radio Centre.
The Radio Society of Great Britain's National Radio Centre (including a library, radio station, museum and bookshop) are in a newly constructed building on Bletchley's grounds.
Funding.
In October 2005, American billionaire Sidney Frank donated £500,000 to Bletchley Park Trust to fund a new Science Centre dedicated to Alan Turing. In July 2008 a letter to The Times from more than a hundred academics condemned the neglect of the site. In September 2008, PGP, IBM, and other technology firms announced a fund-raising campaign to repair the facility. On 6 November 2008 it was announced that English Heritage would donate £300,000 to help maintain the buildings at Bletchley Park, and that they were in discussions regarding the donation of a further £600,000.
In October 2011, the Bletchley Park Trust received a £4.6m Heritage Lottery Fund grant to be used "to complete the restoration of the site, and to tell its story to the highest modern standards.", on the condition that £1.7m of 'match funding' is raised by the Bletchley Park Trust. By June 2012 it had successfully raised £2.4m to unlock the grants to restore Huts 3 and 6, as well as develop its exhibition centre in Block C.
Additional income is raised by renting Block H to the National Museum of Computing, and some office space in various parts of the park to private firms.
References.
</div class>

</doc>
<doc id="4041" url="http://en.wikipedia.org/wiki?curid=4041" title="Bede">
Bede

Bede ( ; ; 672/673 – 26 May 735), also referred to as Saint Bede or the Venerable Bede (), was an English monk at the monastery of Saint Peter at Monkwearmouth and its companion monastery, Saint Paul's, in modern Jarrow (see Monkwearmouth-Jarrow), Northeast England, both of which were located in the Kingdom of Northumbria. He is well known as an author and scholar, and his most famous work, "Historia ecclesiastica gentis Anglorum" ("The Ecclesiastical History of the English People") gained him the title "The Father of English History".
In 1899, Bede was made a Doctor of the Church by Leo XIII, a position of theological significance; he is the only native of Great Britain to achieve this designation (Anselm of Canterbury, also a Doctor of the Church, was originally from Italy). Bede was moreover a skilled linguist and translator, and his work made the Latin and Greek writings of the early Church Fathers much more accessible to his fellow Anglo-Saxons, contributing significantly to English Christianity. Bede's monastery had access to a superb library which included works by Eusebius and Orosius, among many others.
Life.
Almost everything that is known of Bede's life is contained in the last chapter of his "Historia ecclesiastica", a history of the church in England. It was completed in about 731, and Bede implies that he was then in his fifty-ninth year, which would give a likely birth date of about 672–673. A minor source of information is the letter by his disciple Cuthbert which relates Bede's death. Bede, in the "Historia", gives his birthplace as "on the lands of this monastery". He is referring to the twinned monasteries of Monkwearmouth and Jarrow, in modern-day Sunderland, claimed as his birthplace; there is also a tradition that he was born at Monkton, two miles from the monastery at Jarrow. Bede says nothing of his origins, but his connections with men of noble ancestry suggest that his own family was well-to-do. Bede's first abbot was Benedict Biscop, and the names "Biscop" and "Beda" both appear in a king list of the kings of Lindsey from around 800, further suggesting that Bede came from a noble family. The name "Bede" was not a common one at the time. The "Liber Vitae" of Durham Cathedral includes a list of priests; two are named Bede, and one of these is presumably Bede himself. Some manuscripts of the "Life of Cuthbert", one of Bede's works, mention that Cuthbert's own priest was named Bede; it is possible that this priest is the other name listed in the "Liber Vitae". These occurrences, along with a "Bieda" who is mentioned in the "Anglo-Saxon Chronicle" under the year 501, are the only appearances of the name in early sources. The name probably derives from the Old English "bēd", or prayer; if Bede was given the name at his birth, then his family had probably always planned for him to enter the clergy.
At the age of seven, he was sent to the monastery of Monkwearmouth by his family to be educated by Benedict Biscop and later by Ceolfrith. Bede does not say whether it was already intended at that point that he would be a monk. It was fairly common in Ireland at this time for young boys, particularly those of noble birth, to be fostered out; the practice was also likely to have been common among the Germanic peoples in England. Monkwearmouth's sister monastery at Jarrow was founded by Ceolfrith in 682, and Bede probably transferred to Jarrow with Ceolfrith that year. The dedication stone for the church has survived to the present day; it is dated 23 April 685, and as Bede would have been required to assist with menial tasks in his day-to-day life it is possible that he helped in building the original church. In 686, plague broke out at Jarrow. The "Life of Ceolfrith", written in about 710, records that only two surviving monks were capable of singing the full offices; one was Ceolfrith and the other a young boy, who according to the anonymous writer had been taught by Ceolfrith. The two managed to do the entire service of the liturgy until others could be trained. The young boy was almost certainly Bede, who would have been about 14.
When Bede was about 17 years old, Adomnan, the abbot of Iona Abbey, visited Monkwearmouth and Jarrow. Bede would probably have met the abbot during this visit, and it may be that Adomnan sparked Bede's interest in the Easter dating controversy. In about 692, in Bede's nineteenth year, Bede was ordained a deacon by his diocesan bishop, John, who was bishop of Hexham. The canonical age for the ordination of a deacon was 25; Bede's early ordination may mean that his abilities were considered exceptional, but it is also possible that the minimum age requirement was often disregarded. There might have been minor orders ranking below a deacon; but there is no record of whether Bede held any of these offices. In Bede's thirtieth year (about 702), he became a priest, with the ordination again performed by Bishop John.
In about 701 Bede wrote his first works, the "De Arte Metrica" and "De Schematibus et Tropis"; both were intended for use in the classroom. He continued to write for the rest of his life, eventually completing over 60 books, most of which have survived. Not all his output can be easily dated, and Bede may have worked on some texts over a period of many years. His last-surviving work is a letter to Ecgbert of York, a former student, written in 734. A 6th-century Greek and Latin manuscript of "Acts" that is believed to have been used by Bede survives and is now in the Bodleian Library at Oxford University; it is known as the Codex Laudianus. Bede may also have worked on one of the Latin bibles that were copied at Jarrow, one of which is now held by the Laurentian Library in Florence. Bede was a teacher as well as a writer; he enjoyed music, and was said to be accomplished as a singer and as a reciter of poetry in the vernacular. It is possible that he suffered a speech impediment of some kind, but this depends on a phrase in the introduction to his verse life of Saint Cuthbert. Translations of this phrase differ, and it is quite uncertain whether Bede intended to say that he was cured of a speech problem, or merely that he was inspired by the saint's works.
In 708, some monks at Hexham accused Bede of having committed heresy in his work "De Temporibus". The standard theological view of world history at the time was known as the six ages of the world; in his book, Bede calculated the age of the world for himself, rather than accepting the authority of Isidore of Seville, and came to the conclusion that Christ had been born 3,952 years after the creation of the world, rather than the figure of over 5,000 years that was commonly accepted by theologians. The accusation occurred in front of the bishop of Hexham, Wilfrid, who was present at a feast when some drunken monks made the accusation. Wilfrid did not respond to the accusation, but a monk present relayed the episode to Bede, who replied within a few days to the monk, writing a letter setting forth his defence and asking that the letter also be read to Wilfrid. Bede had another brush with Wilfrid, for the historian himself says that he met Wilfrid, sometime between 706 and 709, and discussed Æthelthryth, the abbess of Ely. Wilfrid had been present at the exhumation of her body in 695, and Bede questioned the bishop about the exact circumstances of the body and asked for more details of her life, as Wilfrid had been her advisor.
In 733, Bede travelled to York to visit Ecgbert, who was then bishop of York. The See of York was elevated to an archbishopric in 735, and it is likely that Bede and Ecgbert discussed the proposal for the elevation during his visit. Bede hoped to visit Ecgbert again in 734, but was too ill to make the journey. Bede also travelled to the monastery of Lindisfarne, and at some point visited the otherwise-unknown monastery of a monk named , a visit that is mentioned in a letter to that monk. Because of his widespread correspondence with others throughout the British Isles, and due to the fact that many of the letters imply that Bede had met his correspondents, it is likely that Bede travelled to some other places, although nothing further about timing or locations can be guessed. It seems certain that he did not visit Rome, however, as he would have mentioned it in the autobiographical chapter of his "Historia Ecclesiastica". Nothhelm, a correspondent of Bede's who assisted him by finding documents for him in Rome, is known to have visited Bede, though the date cannot be determined beyond the fact that it was after Nothhelm's visit to Rome.
Bede died on Thursday, 26 May 735 (Ascension Day) and was buried at Jarrow. Cuthbert, a disciple of Bede's, wrote a letter to a Cuthwin (of whom nothing else is known), describing Bede's last days and his death. According to Cuthbert, Bede fell ill, "with frequent attacks of breathlessness but almost without pain", before Easter. On the Tuesday, two days before Bede died, his breathing became worse and his feet swelled. He continued to dictate to a scribe, however, and despite spending the night awake in prayer he dictated again the following day. At three o'clock, according to Cuthbert, he asked for a box of his to be brought, and distributed among the priests of the monastery "a few treasures" of his: "some pepper, and napkins, and some incense". That night he dictated a final sentence to the scribe, a boy named Wilberht, and died soon afterwards. Cuthbert's letter also relates a five-line poem in the vernacular that Bede composed on his deathbed, known as "Bede's Death Song". It is the most-widely copied Old English poem, and appears in 45 manuscripts, but its attribution to Bede is not absolutely certain—not all manuscripts name Bede as the author, and the ones that do are of later origin than those that do not. Bede's remains may have been transferred to Durham Cathedral in the 11th century; his tomb there was looted in 1541, but the contents were probably re-interred in the Galilee chapel at the cathedral.
One further oddity in his writings is that in one of his works, the "Commentary on the Seven Catholic Epistles", he writes in a manner that gives the impression he was married. The section in question is the only one in that work that is written in first-person view. Bede says: "Prayers are hindered by the conjugal duty because as often as I perform what is due to my wife I am not able to pray." Another passage, in the "Commentary on Luke", also mentions a wife in the first person: "Formerly I possessed a wife in the lustful passion of desire and now I possess her in honourable sanctification and true love of Christ." The historian Benedicta Ward argues that these passages are Bede employing a rhetorical device.
Works.
Bede wrote scientific, historical and theological works, reflecting the range of his writings from music and metrics to exegetical Scripture commentaries. He knew patristic literature, as well as Pliny the Elder, Virgil, Lucretius, Ovid, Horace and other classical writers. He knew some Greek. His Latin is generally clear, but his Biblical commentaries are more technical.
Bede's scriptural commentaries employed the allegorical method of interpretation and his history includes accounts of miracles, which to modern historians has seemed at odds with his critical approach to the materials in his history. Modern studies have shown the important role such concepts played in the world-view of Early Medieval scholars.
He dedicated his work on the Apocalypse and the "De Temporum Ratione" to the successor of Ceolfrid as abbot, Hwaetbert.
Although Bede is mainly studied as a historian now, in his time his works on grammar, chronology, and biblical studies were as important as his historical and hagiographical works. The non-historical works contributed greatly to the Carolingian renaissance. He has been credited with writing a penitential, though his authorship of this work is still very much disputed.
"Historia ecclesiastica gentis Anglorum".
Bede's best-known work is the "Historia ecclesiastica gentis Anglorum", or "An Ecclesiastical History of the English People", completed in about 731. Bede was aided in writing this book by Albinus, abbot of St Augustine's Abbey, Canterbury. The first of the five books begins with some geographical background, and then sketches the history of England, beginning with Caesar's invasion in 55 BC. A brief account of Christianity in Roman Britain, including the martyrdom of St Alban, is followed by the story of Augustine's mission to England in 597, which brought Christianity to the Anglo-Saxons. The second book begins with the death of Gregory the Great in 604, and follows the further progress of Christianity in Kent and the first attempts to evangelise Northumbria. These ended in disaster when Penda, the pagan king of Mercia, killed the newly Christian Edwin of Northumbria at the Battle of Hatfield Chase in about 632. The setback was temporary, and the third book recounts the growth of Christianity in Northumbria under kings Oswald of Northumbria and Oswy. The climax of the third book is the account of the Council of Whitby, traditionally seen as a major turning point in English history. The fourth book begins with the consecration of Theodore as Archbishop of Canterbury, and recounts Wilfrid's efforts to bring Christianity to the kingdom of Sussex. The fifth book brings the story up to Bede's day, and includes an account of missionary work in Frisia, and of the conflict with the British church over the correct dating of Easter. Bede wrote a preface for the work, in which he dedicates it to Ceolwulf, king of Northumbria. The preface mentions that Ceolwulf received an earlier draft of the book; presumably Ceolwulf knew enough Latin to understand it, and he may even have been able to read it. The preface makes it clear that Ceolwulf had requested the earlier copy, and Bede had asked for Ceolwulf's approval; this correspondence with the king indicates that Bede's monastery had excellent connections among the Northumbrian nobility.
Sources.
The monastery at Wearmouth-Jarrow had an excellent library. Both Benedict Biscop and Ceolfrith had acquired books from the Continent, and in Bede's day the monastery was a renowned centre of learning. It has been estimated that there were about 200 books in the monastic library.
For the period prior to Augustine's arrival in 597, Bede drew on earlier writers, including Solinus. He had access to two works of Eusebius: the "Historia Ecclesiastica", and also the "Chronicon", though he had neither in the original Greek; instead he had a Latin translation of the "Historia", by Rufinus, and Saint Jerome's translation of the "Chronicon". He also knew Orosius's "Adversus Paganus", and Gregory of Tours' "Historia Francorum", both Christian histories, as well as the work of Eutropius, a pagan historian. He used Constantius's "Life of Germanus" as a source for Germanus's visits to Britain. Bede's account of the invasion of the Anglo-Saxons is drawn largely from Gildas's "De Excidio et Conquestu Britanniae". Bede would also have been familiar with more recent accounts such as Eddius Stephanus's "Life of Wilfrid", and anonymous "Lives" of Gregory the Great and Cuthbert. He also drew on Josephus's "Antiquities", and the works of Cassiodorus, and there was a copy of the "Liber Pontificalis" in Bede's monastery. Bede quotes from several classical authors, including Cicero, Plautus, and Terence, but he may have had access to their work via a Latin grammar rather than directly. However, it is clear he was familiar with the works of Virgil and with Pliny the Elder's "Natural History", and his monastery also owned copies of the works of Dionysius Exiguus. He probably drew his account of St. Alban from a life of that saint which has not survived. He acknowledges two other lives of saints directly; one is a life of Fursa, and the other of St. Æthelburh; the latter no longer survives. He also had access to a life of Ceolfrith. Some of Bede's material came from oral traditions, including a description of the physical appearance of Paulinus of York, who had died nearly 90 years before Bede's "Historia Ecclesiastica" was written.
Bede also had correspondents who supplied him with material. Albinus, the abbot of the monastery in Canterbury, provided much information about the church in Kent, and with the assistance of Nothhelm, at that time a priest in London, obtained copies of Gregory the Great's correspondence from Rome relating to Augustine's mission. Almost all of Bede's information regarding Augustine is taken from these letters. Bede acknowledged his correspondents in the preface to the "Historia Ecclesiastica"; he was in contact with Daniel, the Bishop of Winchester, for information about the history of the church in Wessex, and also wrote to the monastery at Lastingham for information about Cedd and Chad. Bede also mentions an Abbot Esi as a source for the affairs of the East Anglian church, and Bishop Cynibert for information about Lindsey.
The historian Walter Goffart argues that Bede based the structure of the "Historia" on three works, using them as the framework around which the three main sections of the work were structured. For the early part of the work, up until the Gregorian mission, Goffart feels that Bede used Gildas's "De excidio". The second section, detailing the Gregorian mission of Augustine of Canterbury was framed on the anonymous "Life of Gregory the Great" written at Whitby. The last section, detailing events after the Gregorian mission, Goffart feels were modelled on Stephen of Ripon's "Life of Wilfrid". Most of Bede's informants for information after Augustine's mission came from the eastern part of Britain, leaving significant gaps in the knowledge of the western areas, which were those areas likely to have a native Briton presence.
Models and style.
Bede's stylistic models included some of the same authors from whom he drew the material for the earlier parts of his history. His introduction imitates the work of Orosius, and his title is an echo of Eusebius's "Historia Ecclesiastica". Bede also followed Eusebius in taking the "Acts of the Apostles" as the model for the overall work: where Eusebius used the "Acts" as the theme for his description of the development of the church, Bede made it the model for his history of the Anglo-Saxon church. Bede quoted his sources at length in his narrative, as Eusebius had done. Bede also appears to have taken quotes directly from his correspondents at times. For example, he almost always uses the terms "Australes" and "Occidentales" for the South and West Saxons respectively, but in a passage in the first book he uses "Meridiani" and "Occidui" instead, as perhaps his informant had done. At the end of the work, Bede added a brief autobiographical note; this was an idea taken from Gregory of Tours' earlier "History of the Franks".
Bede's work as a hagiographer, and his detailed attention to dating, were both useful preparations for the task of writing the "Historia Ecclesiastica". His interest in computus, the science of calculating the date of Easter, was also useful in the account he gives of the controversy between the British and Anglo-Saxon church over the correct method of obtaining the Easter date.
Bede's Latin has been praised for its clarity, but his style in the "Historia Ecclesiastica" is not simple. He knew rhetoric, and often used figures of speech and rhetorical forms which cannot easily be reproduced in translation, depending as they often do on the connotations of the Latin words. However, unlike contemporaries such as Aldhelm, whose Latin is full of difficulties, Bede's own text is easy to read. In the words of Charles Plummer, one of the best-known editors of the "Historia Ecclesiastica", Bede's Latin is "clear and limpid ... it is very seldom that we have to pause to think of the meaning of a sentence ... Alcuin rightly praises Bede for his unpretending style."
Intent.
Bede's primary intention in writing the "Historia Ecclesiastica" was to show the growth of the united church throughout England. The native Britons, whose Christian church survived the departure of the Romans, earn Bede's ire for refusing to help convert the Saxons; by the end of the "Historia" the English, and their Church, are dominant over the Britons. This goal, of showing the movement towards unity, explains Bede's animosity towards the British method of calculating Easter: much of the "Historia" is devoted to a history of the dispute, including the final resolution at the Synod of Whitby in 664. Bede is also concerned to show the unity of the English, despite the disparate kingdoms that still existed when he was writing. He also wants to instruct the reader by spiritual example, and to entertain, and to the latter end he adds stories about many of the places and people about which he wrote.
Bede's extensive use of miracles can prove difficult for readers who consider him a more or less reliable historian, but do not accept the possibility of miracles.
Yet both reflect an inseparable integrity and regard for accuracy and truth, expressed in terms both of historical events and of a tradition of Christian faith that continues to the present day. Bede, like Gregory the Great whom Bede quotes on the subject in the "Historia", felt that faith brought about by miracles was a stepping stone to a higher, truer faith, and that as a result miracles had their place in a work designed to instruct.
Omissions and biases.
Bede is somewhat reticent about the career of Wilfrid, a contemporary and one of the most prominent clerics of his day. This may be because Wilfrid's opulent lifestyle was uncongenial to Bede's monastic mind; it may also be that the events of Wilfrid's life, divisive and controversial as they were, simply did not fit with Bede's theme of the progression to a unified and harmonious church.
Bede's account of the early migrations of the Angles and Saxons to England omits any mention of a movement of those peoples across the channel from Britain to Brittany described by Procopius, who was writing in the sixth century. Frank Stenton describes this omission as "a scholar's dislike of the indefinite"; traditional material that could not be dated or used for Bede's didactic purposes had no interest for him.
Bede was a Northumbrian, and this tinged his work with a local bias. The sources he had access to gave him less information about the west of England than for other areas. He says relatively little about the achievements of Mercia and Wessex, omitting, for example, any mention of Boniface, a West Saxon missionary to the continent of some renown and of whom Bede had almost certainly heard, though Bede does discuss Northumbrian missionaries to the continent. He also is parsimonious in his praise for Aldhelm, a West Saxon who had done much to convert the native Britons to the Roman form of Christianity. He lists seven kings of the Anglo-Saxons whom he regards as having held "imperium", or overlordship; only one king of Wessex, Ceawlin, is listed, and none from Mercia, though elsewhere he acknowledges the secular power several of the Mercians held. Historian Robin Fleming states that he was so hostile to Mercia because Northumbria had been diminished by Mercian power that he consulted no Mercian informants and included no stories about its saints.
Bede relates the story of Augustine's mission from Rome, and tells how the British clergy refused to assist Augustine in the conversion of the Anglo-Saxons. This, combined with Gildas's negative assessment of the British church at the time of the Anglo-Saxon invasions, led Bede to a very critical view of the native church. However, Bede ignores the fact that at the time of Augustine's mission, the history between the two was one of warfare and conquest, which, in the words of Barbara Yorke, would have naturally "curbed any missionary impulses towards the Anglo-Saxons from the British clergy."
Use of "Anno Domini".
At the time Bede wrote the "Historia Ecclesiastica", there were two common ways of referring to dates. One was to use indictions, which were 15-year cycles, counting from 312 AD. There were three different varieties of indiction, each starting on a different day of the year. The other approach was to use regnal years—the reigning Roman emperor, for example, or the ruler of whichever kingdom was under discussion. This meant that in discussing conflicts between kingdoms, the date would have to be given in the regnal years of all the kings involved. Bede used both these approaches on occasion, but adopted a third method as his main approach to dating: the "anno domini" method invented by Dionysius Exiguus. Although Bede did not invent this method, his adoption of it, and his promulgation of it in "De Temporum Ratione", his work on chronology, is the main reason why it is now so widely used.
Assessment.
The "Historia Ecclesiastica" was copied often in the Middle Ages, and about 160 manuscripts containing it survive. About half of those are located on the European continent, rather than on the British Isles. Most of the 8th- and 9th-century texts of Bede's "Historia" come from the northern parts of the Carolingian Empire. This total does not include manuscripts with only a part of the work, of which another 100 or so survive. It was printed for the first time between 1474 and 1482, probably at Strasbourg, France. Modern historians have studied the "Historia" extensively, and a number of editions have been produced. For many years, early Anglo-Saxon history was essentially a retelling of the "Historia", but recent scholarship has focused as much on what Bede did not write as what he did. The belief that the "Historia" was the culmination of Bede's works, the aim of all his scholarship, a belief common among historians in the past, is no longer accepted by most scholars.
Modern historians and editors of Bede have been lavish in their praise of his achievement in the "Historia Ecclesiastica". Stenton regarded it as one of the "small class of books which transcend all but the most fundamental conditions of time and place", and regarded its quality as dependent on Bede's "astonishing power of co-ordinating the fragments of information which came to him through tradition, the relation of friends, or documentary evidence ... In an age where little was attempted beyond the registration of fact, he had reached the conception of history." Patrick Wormald described him as "the first and greatest of England's historians".
The "Historia Ecclesiastica" has given Bede a high reputation, but his concerns were different from those of a modern writer of history. His focus on the history of the organisation of the English church, and on heresies and the efforts made to root them out, led him to exclude the secular history of kings and kingdoms except where a moral lesson could be drawn or where they illuminated events in the church. Besides the "Anglo-Saxon Chronicle", the medieval writers William of Malmesbury, Henry of Huntingdon, and Geoffrey of Monmouth used his works as sources and inspirations. Early modern writers, such as Polydore Vergil and Matthew Parker, the Elizabethan Archbishop of Canterbury, also utilised the "Historia", and his works were used by both Protestant and Catholic sides in the Wars of Religion.
Some historians have questioned the reliability of some of Bede's accounts. One historian, Charlotte Behr, thinks that the "Historia's" account of the arrival of the Germanic invaders in Kent should not be considered to relate what actually happened, but rather relates myths that were current in Kent during Bede's time.
It is likely that Bede's work, because it was so widely copied, discouraged others from writing histories and may even have led to the disappearance of manuscripts containing older historical works.
Other historical works.
Chronicles.
As Chapter 66 of his "On the Reckoning of Time", in 725 Bede wrote the "Greater Chronicle" ("chronica maiora"), which sometimes circulated as a separate work. For recent events the "Chronicle", like his "Ecclesiastical History", relied upon Gildas, upon a version of the Liber pontificalis current at least to the papacy of Pope Sergius I (687–701), and other sources. For earlier events he drew on Eusebius's "Chronikoi Kanones." The dating of events in the "Chronicle" is inconsistent with his other works, using the era of creation, the anno mundi.
Lives.
His other historical works included lives of the abbots of Wearmouth and Jarrow, as well as verse and prose lives of Saint Cuthbert of Lindisfarne, an adaptation of Paulinus of Nola's "Life of St Felix", and a translation of the Greek "Passion of St Anastasius". He also created a listing of saints, the "Martyrology".
Theological works.
In his own time, Bede was as well known for his biblical commentaries and exegetical, as well as other theological works. The majority of his writings were of this type, and covered the Old Testament and the New Testament. Most survived the Middle Ages, but a few were lost. It was for his theological writings that he earned the title of "Doctor Anglorum", and why he was made a saint.
Bede synthesised and transmitted the learning from his predecessors, as well as made careful, judicious innovation in knowledge (such as recalculating the age of the earth—for which he was censured before surviving the heresy accusations and eventually having his views championed by Archbishop Ussher in the sixteenth century—see below) that had theological implications. In order to do this, he learned Greek, and attempted to learn Hebrew. He spent time reading and rereading both the Old and the New Testaments. He mentions that he studied from a text of Jerome's Vulgate, which itself was from the Hebrew text. He also studied both the Latin and the Greek Fathers of the Church. In the monastic library at Jarrow were a number of books by theologians, including works by Basil, Cassian, John Chrysostom, Isidore of Seville, Origen, Gregory of Nazianzus, Augustine of Hippo, Jerome, Pope Gregory I, Ambrose of Milan, Cassiodorus, and Cyprian. He used these, in conjunction with the Biblical texts themselves, to write his commentaries and other theological works. He had a Latin translation by Evagrius of Athanasius's "Life of Antony", and a copy of Sulpicius Severus' "Life of St. Martin". He also used lesser known writers, such as Fulgentius, Julian of Eclanum, Tyconius, and Prosperius. Bede was the first to refer to Jerome, Augustine, Pope Gregory and Ambrose as the four Latin Fathers of the Church. It is clear from Bede's own comments that he felt his job was to explain to his students and readers the theology and thoughts of the Church Fathers.
Bede also wrote homilies, works written to explain theology used in worship services. Bede wrote homilies not only on the major Christian seasons such as Advent, Lent, or Easter, but on other subjects such as anniversaries of significant events.
Both types of Bede's theological works circulated widely in the Middle Ages. A number of his biblical commentaries were incorporated into the "Glossa Ordinaria", an 11th-century collection of biblical commentaries. Some of Bede's homilies were collected by Paul the Deacon, and they were used in that form in the Monastic Office. Saint Boniface used Bede's homilies in his missionary efforts on the continent.
Bede sometimes included in his theological books an acknowledgement of the predecessors on whose works he drew. In two cases he left instructions that his marginal notes, which gave the details of his sources, should be preserved by the copyist, and he may have originally added marginal comments about his sources to others of his works. Where he does not specify, it is still possible to identify books to which he must have had access by quotations that he uses. A full catalogue of the library available to Bede in the monastery cannot be reconstructed, but it is possible to tell, for example, that Bede was very familiar with the works of Virgil. There is little evidence that he had access to any other of the pagan Latin writers—he quotes many of these writers but the quotes are almost all to be found in the Latin grammars that were common in his day, one or more of which would certainly have been at the monastery. Another difficulty is that manuscripts of early writers were often incomplete: it is apparent that Bede had access to Pliny's "Encyclopedia", for example, but it seems that the version he had was missing book xviii, as he would almost certainly have quoted from it in his "De temporum ratione".
Works on the Old Testament.
The works dealing with the Old Testament included "Commentary on Samuel", "Commentary on Genesis", "Commentaries on Ezra and Nehemiah", "On the Temple", "On the Tabernacle", "Commentaries on Tobit", "Commentaries on Proverbs", "Commentaries on the Song of Songs", "Commentaries on the Canticle of Habakkuk", The works on Ezra, the Tabernacle and the Temple were especially influenced by Gregory the Great's writings.
Works on the New Testament.
Bede's works included "Commentary on Revelation", "Commentary on the Catholic Epistles", "Commentary on Acts", "Reconsideration on the Books of Acts", "On the Gospel of Mark", "On the Gospel of Luke", and "Homilies on the Gospels". At the time of his death he was working on a translation of the Gospel of St. John into English.
Works on historical and astronomical chronology.
"De temporibus", or "On Time", written in about 703, provides an introduction to the principles of Easter computus. This was based on parts of Isidore of Seville's "Etymologies", and Bede also included a chronology of the world which was derived from Eusebius, with some revisions based on Jerome's translation of the bible. In about 723, Bede wrote a longer work on the same subject, "On the Reckoning of Time", which was influential throughout the Middle Ages. He also wrote several shorter letters and essays discussing specific aspects of computus.
"On the Reckoning of Time" ("De temporum ratione") included an introduction to the traditional ancient and medieval view of the cosmos, including an explanation of how the spherical earth influenced the changing length of daylight, of how the seasonal motion of the Sun and Moon influenced the changing appearance of the New Moon at evening twilight, and a quantitative relation between the changes of the tides at a given place and the daily motion of the moon. Since the focus of his book was the computus, Bede gave instructions for computing the date of Easter and the related time of the Easter Full Moon, for calculating the motion of the Sun and Moon through the zodiac, and for many other calculations related to the calendar. He gives some information about the months of the Anglo-Saxon calendar in chapter XV. Any codex of Bede's Easter cycle is normally found together with a codex of his "De Temporum Ratione".
For calendric purposes, Bede made a new calculation of the age of the world since the creation, which he dated as 3952 BC. Due to his innovations in computing the age of the world, he was accused of heresy at the table of Bishop Wilfrid, his chronology being contrary to accepted calculations. Once informed of the accusations of these "lewd rustics," Bede refuted them in his Letter to Plegwin.
In addition to these works on astronomical timekeeping, he also wrote "De natura rerum", or "On the Nature of Things", modelled in part after the work of the same title by Isidore of Seville. His works were so influential that late in the 9th century Notker the Stammerer, a monk of the Monastery of St. Gall in Switzerland, wrote that "God, the orderer of natures, who raised the Sun from the East on the fourth day of Creation, in the sixth day of the world has made Bede rise from the West as a new Sun to illuminate the whole Earth".
Educational works.
Bede wrote some works designed to help teach grammar in the abbey school. One of these was his "De arte metrica", a discussion of the composition of Latin verse, drawing on previous grammarians work. It was based on Donatus' "De pedibus" and Servius' "De finalibus", and used examples from Christian poets as well as Virgil. It became a standard text for the teaching of Latin verse during the next few centuries. Bede dedicated this work to Cuthbert, apparently a student, for he is named "beloved son" in the dedication, and Bede says "I have laboured to educate you in divine letters and ecclesiastical statutes" Another textbook of Bede's is the "De orthographia", a work on orthography, designed to help a medieval reader of Latin with unfamiliar abbreviations and words from classical Latin works. Although it could serve as a textbook, it appears to have been mainly intended as a reference work. The exact date of composition for both of these works is unknown.
Another educational work is "De schematibus et tropis sacrae scripturae", which discusses the Bible's use of rhetoric. Bede was familiar with pagan authors such as Virgil, but it was not considered appropriate to teach biblical grammar from such texts, and in "De schematibus ..." Bede argues for the superiority of Christian texts in understanding Christian literature. Similarly, his text on poetic metre uses only Christian poetry for examples.
Vernacular poetry.
According to his disciple Cuthbert, Bede was also "doctus in nostris carminibus" ("learned in our songs"). Cuthbert's letter on Bede's death, the "Epistola Cuthberti de obitu Bedae", moreover, commonly is understood to indicate that Bede also composed a five line vernacular poem known to modern scholars as "Bede's Death Song"
As Opland notes, however, it is not entirely clear that Cuthbert is attributing this text to Bede: most manuscripts of the letter do not use a finite verb to describe Bede's presentation of the song, and the theme was relatively common in Old English and Anglo-Latin literature. The fact that Cuthbert's description places the performance of the Old English poem in the context of a series of quoted passages from Sacred Scripture, indeed, might be taken as evidence simply that Bede also cited analogous vernacular texts. On the other hand, the inclusion of the Old English text of the poem in Cuthbert's Latin letter, the observation that Bede "was learned in our song," and the fact that Bede composed a Latin poem on the same subject all point to the possibility of his having written it. By citing the poem directly, Cuthbert seems to imply that its particular wording was somehow important, either since it was a vernacular poem endorsed by a scholar who evidently frowned upon secular entertainment or because it is a direct quotation of Bede's last original composition.
Veneration.
There is no evidence for cult being paid to Bede in England in the 8th century. One reason for this may be that he died on the feast day of Augustine of Canterbury. Later, when he was venerated in England, he was either commemorated after Augustine on 26 May, or his feast was moved to 27 May. However, he was venerated outside England, mainly through the efforts of Boniface and Alcuin, both of whom promoted the cult on the Continent. Boniface wrote repeatedly back to England during his missionary efforts, requesting copies of Bede's theological works. Alcuin, who was taught at the school set up in York by Bede's pupil Egbert, praised Bede as an example for monks to follow and was instrumental in disseminating Bede's works to all of Alcuin's friends. Bede's cult became prominent in England during the 10th-century revival of monasticism, and by the 14th century had spread to many of the cathedrals of England. Wulfstan, Bishop of Worcester (c. 1008–1095) was a particular devotee of Bede's, dedicating a church to him in 1062, which was Wulfstan's first undertaking after his consecration as bishop.
His body was 'translated' (the ecclesiastical term for relocation of relics) from Jarrow to Durham Cathedral around 1020, where it was placed in the same tomb with Saint Cuthbert of Lindisfarne. Later Bede's remains were moved to a shrine in the Galilee Chapel at Durham Cathedral in 1370. The shrine was destroyed during the English Reformation, but the bones were reburied in the chapel. In 1831 the bones were dug up and then reburied in a new tomb, which is still there. Other relics were claimed by York, Glastonbury and Fulda.
His scholarship and importance to Catholicism were recognised in 1899 when he was declared a Doctor of the Church. He is the only Englishman named a Doctor of the Church. He is also the only Englishman in Dante's "Paradise" ("Paradiso" X.130), mentioned among theologians and doctors of the church in the same canto as Isidore of Seville and the Scot Richard of St. Victor.
His feast day was included in the General Roman Calendar in 1899, for celebration on 27 May rather than on his date of death, 26 May, which was then the feast day of Pope Saint Gregory VII. He is venerated in both the Anglican and Roman Catholic Church, with a feast day of 25 May, and in the Eastern Orthodox Church, with a feast day on 27 May.
Bede became known as "Venerable Bede" (Lat.: Beda Venerabilis) by the 9th century, but this was not linked to consideration for sainthood by the Roman Catholic Church. According to a legend the epithet was miraculously supplied by angels, thus completing his unfinished epitaph. It is first utilised in connection with Bede in the 9th century, where Bede was grouped with others who were called "venerable" at two ecclesiastical councils held at Aix in 816 and 836. Paul the Deacon then referred to him as venerable consistently. By the 11th and 12th century, it had become commonplace. However, there are no descriptions of Bede by that term right after his death.
Modern legacy.
Bede's reputation as a historian, based mostly on the "Historia Ecclesiastica", remains strong; historian Walter Goffart says of Bede that he "holds a privileged and unrivalled place among first historians of Christian Europe". His life and work are celebrated with the annual Jarrow Lecture, held at St. Paul's Church, Jarrow, since 1958.

</doc>
<doc id="4045" url="http://en.wikipedia.org/wiki?curid=4045" title="Bubble tea">
Bubble tea

Bubble tea, also known as pearl milk tea or boba milk tea, is a Taiwanese tea-based drink invented in Taichung in the 1980s. The term "bubble" is an Anglicized imitative form derived from the Chinese "bōbà" (波霸), meaning "large", slang for the large, chewy tapioca balls commonly added to the drink. These are (粉圓, "fěnyuán"), also called "pearls" (珍珠, "zhēnzhū"). Most bubble tea recipes contain a tea base mixed with fruit or milk. Ice-blended versions are usually mixed with fruit or syrup, resulting in a slushy consistency.
There are many variants of the drinks, and many kinds of ingredients may be added. The most popular bubble drinks are bubble milk tea with tapioca and bubble milk green tea with tapioca.
Description.
Bubble teas are typically of two distinct types: fruit-flavored teas and milk teas. However, some shops offer hybrid "fruit milk teas". Most milk teas include powdered dairy or non-dairy creamers, but some shops also offer fresh milk as an alternative. Other varieties are 100% crushed-fruit smoothies with tapioca pearls and signature ice cream shakes made from local ice cream sources. Many American bubble tea vendors sell "milk smoothies", which are similar to bubble tea but do not contain any tea ingredients. Some small cafés offer sweetener substitutes, such as honey, agave, stevia, and aspartame, upon special request.
The oldest known bubble tea consisted of a mixture of hot Taiwanese black tea, small tapioca pearls (粉圓), condensed milk, and syrup (糖漿) or honey. Many variations were created, the most common of which is served cold rather than hot. The tea type is frequently replaced. First was bubble green tea, which uses jasmine-infused green tea (茉香綠茶) instead of black tea. Big tapioca pearls (波霸/黑珍珠) were adapted and quickly replaced the small pearls. Peach or plum flavoring appeared, then more fruit flavors were added until, in some variations, the tea was removed entirely in favor of real fruit. These fruit versions sometimes contain colored pearls (and/or "jelly cubes" as in the related drink taho), the color chosen to match whatever fruit juice is used. Flavors may be added in the form of powder, fruit juice, pulp, or syrup to hot black or green tea, which is then shaken in a cocktail shaker or mixed with ice in a blender. Cooked tapioca pearls and other mix-ins (such as vanilla extract, honey, syrup, and sugar) are added at the end.
Today, one can find shops entirely devoted to bubble tea, similar to the juice bars of the early 1990s. Some cafés use plastic dome-shaped lids, while other bubble tea bars serve it using a machine to seal the top of the cup with plastic cellophane. The latter method allows the tea to be shaken in the serving cup and makes it spill-free until one is ready to drink it. The cellophane is then pierced with an oversized straw large enough to allow the pearls to pass through.
Today, in Taiwan, it's more common for people to refer to the drink as "pearl milk tea" ("zhēn zhū nǎi chá", or "zhēn nǎi" for short). "Pearl milk tea" is also used by English speakers and overseas Chinese and Taiwanese speakers, but it is usually called "bubble tea" or "boba tea" by English speakers, with the former seemingly more common in locations with less Chinese influence.
Variants.
Each of the ingredients of bubble tea can have many variations depending on the tea store. Typically, different types of black tea, green tea, or even coffee form the basis of this beverage. The most common black tea varieties are oolong and Earl Grey, while jasmine green tea is a mainstay at almost all tea stores. Another variation called yuanyang (鴛鴦, named after the Mandarin duck) originated in Hong Kong and consists of half black tea and half coffee. Some people add milk to the drink. Decaffeinated versions of teas are sometimes available when the tea house freshly brews the tea base.
The milk in bubble tea is optional, though many tea stores use it. Some cafés use a non-dairy creamer milk substitute instead of milk because many East Asians are lactose intolerant and because it is cheaper and easier to store and use than perishable milk. In Western countries, soy milk options are widely available for those who avoid dairy products. This adds a distinct flavor and consistency to the drink.
Different flavorings can be added to bubble tea. Some widely available fruit flavors include strawberry, green apple, passion fruit, mango, lemon, watermelon, grape, lychee, peach, pineapple, cantaloupe, honeydew, banana, avocado, coconut, kiwi, and jackfruit. Other popular non-fruit flavors include taro, pudding, chocolate, coffee, mocha, barley, sesame, almond, ginger, lavender, rose, caramel, and violet. Some of the sour fruit flavors are available in bubble tea without milk only as the acidity tends to curdle the milk.
Other varieties of the bubble tea drink can include blended drinks. Many stores in the US provide a list of choices to choose from. Some may include coffee-blended drinks or even smoothies.
Tapioca balls (boba) are the prevailing chewy tidbits in bubble tea, but a wide range of other options can be used to add similar texture to the drink. Green pearls have a small hint of green tea flavor and are chewier than the traditional tapioca balls. Jelly is also used in small cubes, stars, or rectangular strips, with flavors such as coconut jelly, konjac, lychee, grass, mango, and green tea often available at some shops. Rainbow, a fruit mix of konjac, has a pliant, almost crispy consistency. Azuki bean or mung bean paste, also typical toppings for Taiwanese shaved ice desserts, give the drinks an added subtle flavor as well as texture. Aloe, egg pudding (custard pudding), sago, and taro balls can also be found in most tea houses.
Due to its popularity, single-serving packets of black tea (with powdered milk and sugar included) are available as "Instant Boba Milk Tea" in some places.
Bubble tea cafés will also frequently serve drinks without coffee or tea in them. The base for these drinks is flavoring blended with ice, often called Snow Bubble. All mix-ins that can be added to the bubble tea can also be added to these slushie-like drinks. One drawback to them is that the coldness of the iced drink may cause the tapioca balls to harden, making them difficult to suck up through a straw and chew. To prevent this from happening, these slushies must be consumed more quickly than bubble tea.
Occasionally, nata de coco is used in mass-produced bubble tea drinks as a healthier alternative to tapioca starch. "Nata de coco" is high in dietary fiber and low in cholesterol and fat. The "nata de coco" is sliced into thin strips to make it easier to pass through a straw.
History.
Bubble tea was invented in Taiwan during the 1980s.
One possible origin is Chun Shui Tang teahouse in Taichung, where Ms. Lin Hsiu Hui (product development manager) poured sweetened balls into the tea during a meeting in 1988. The beverage was well received by the people at the meeting, leading to its inclusion on the menu, ultimately becoming the franchise's top-selling product. An alternative origin is the Hanlin teahouse in Tainan, Taiwan, owned by Tu Tsong-he. He made tea using traditional white tapioca, which has the appearance of pearls, supposedly resulting in the so-called "pearl tea". Shortly after, Hanlin changed the white tapioca balls to the black version that is seen most today. The drink became popular in most parts of East and Southeast Asia during the 1990s.
In June 2012, McDonald's McCafé locations in Germany and Austria began offering bubble tea. They offer black, green, or white tea, available with or without milk. Fruit syrups are also available, bringing the total number of possible flavor combinations to 250.
Health Concerns.
Tapioca pearls, milk powder, and juice syrups have in the past been found to contain banned chemical additives. In May 2011, a food scandal broke out in Taiwan where DEHP (a chemical plasticizer and potential carcinogen used to make plastic) was found as a stabilizer in drinks and juice syrups. Some of these products may have been exported and used in bubble tea shops around the world. DEHP can affect hormone balances. In June 2011, the Health Minister of Malaysia, Liow Tiong Lai, instructed companies selling "Strawberry Syrup", a material used in some bubble teas, to stop selling them after chemical tests showed they were tainted with a carcinogen identified as DEHP.
In August 2012, scientists from the Technical University of Aachen (RWTH) in Germany analyzed bubble tea samples within a research project to look for allergenic substances. The result indicated that the products contain styrene, acetophenone and brominated substances. The report was published by German newspaper "Rheinische Post" and caused Taiwan's representative office in Germany to issue a statement, saying food items in Taiwan are monitored. Taiwan's Food and Drug Administration confirmed in September that in a second round of tests conducted by German authorities, Taiwanese bubble tea was found to be free of cancer-causing chemicals. The products were also found to contain no excessive levels of heavy-metal contaminants or other health-threatening agents.
In May 2013 the Taiwan Food and Drug Administration issued an alert on the detection of maleic acid, an unapproved food additive, in some food products, including tapioca pearls. The Agri-Food & Veterinary Authority of Singapore conducted its own tests and found additional brands of tapioca pearls and some other starch-based products sold in Singapore were similarly affected.

</doc>
<doc id="4049" url="http://en.wikipedia.org/wiki?curid=4049" title="Battle of Blenheim">
Battle of Blenheim

The Battle of Blenheim (referred to in some countries as the Second Battle of Höchstädt), fought on 13 August 1704, was a major battle of the War of the Spanish Succession. Louis XIV of France sought to knock Emperor Leopold out of the war by seizing Vienna, the Habsburg capital, and gain a favourable peace settlement. The dangers to Vienna were considerable: the Elector of Bavaria and Marshal Marsin's forces in Bavaria threatened from the west, and Marshal Vendôme's large army in northern Italy posed a serious danger with a potential offensive through the Brenner Pass. Vienna was also under pressure from Rákóczi's Hungarian revolt from its eastern approaches. Realising the danger, the Duke of Marlborough resolved to alleviate the peril to Vienna by marching his forces south from Bedburg and help maintain Emperor Leopold within the Grand Alliance.
A combination of deception and brilliant administration – designed to conceal his true destination from friend and foe alike – enabled Marlborough to march unhindered from the Low Countries to the River Danube in five weeks. After securing Donauwörth on the Danube, Marlborough sought to engage the Elector's and Marsin's army before Marshal Tallard could bring reinforcements through the Black Forest. However, with the Franco-Bavarian commanders reluctant to fight until their numbers were deemed sufficient, the Duke enacted a policy of plundering in Bavaria designed to force the issue. The tactic proved unsuccessful, but when Tallard arrived to bolster the Elector's army, and Prince Eugene arrived with reinforcements for the Allies, the two armies finally met on the banks of the Danube in and around the small village of Blindheim.
Blenheim has gone down in history as one of the turning points of the War of the Spanish Succession. The overwhelming Allied victory ensured the safety of Vienna from the Franco-Bavarian army, thus preventing the collapse of the Grand Alliance. Bavaria was knocked out of the war, and Louis's hopes for a quick victory came to an end. France suffered over 30,000 casualties including the commander-in-chief, Marshal Tallard, who was taken captive to England. Before the 1704 campaign ended, the Allies had taken Landau, and the towns of Trier and Trarbach on the Moselle in preparation for the following year's campaign into France itself.
Background.
By 1704, the War of the Spanish Succession was in its fourth year. The previous year had been one of success for France and her allies, most particularly on the Danube, where Marshal Villars and the Elector of Bavaria had created a direct threat to Vienna, the Habsburg capital. Vienna had been saved by dissension between the two commanders, leading to the brilliant Villars being replaced by the less dynamic Marshal Marsin. Nevertheless, by 1704, the threat was still real: Rákóczi's Hungarian revolt was already threatening the Empire's eastern approaches, and Marshal Vendôme's forces threatened an invasion from northern Italy. In the courts of Versailles and Madrid, Vienna's fall was confidently anticipated, an event which would almost certainly have led to the collapse of the Grand Alliance.
To isolate the Danube from any Allied intervention, Marshal Villeroi's 46,000 troops were expected to pin the 70,000 Dutch and English troops around Maastricht in the Low Countries, while General de Coigny protected Alsace against surprise with a further corps. The only forces immediately available for Vienna's defence were Prince Louis of Baden's force of 36,000 stationed in the Lines of Stollhofen to watch Marshal Tallard at Strasbourg; there was also a weak force of 10,000 men under Field Marshal Count Limburg Styrum observing Ulm.
Both the Imperial Austrian Ambassador in London, Count Wratislaw, and the Duke of Marlborough realised the implications of the situation on the Danube. The Dutch, however, who clung to their troops for their country's protection, were against any adventurous military operation as far south as the Danube and would never willingly permit any major weakening of the forces in the Spanish Netherlands. Marlborough, realising the only way to ignore Dutch wishes was by the use of secrecy and guile, set out to deceive his Dutch allies by pretending to simply move his troops to the Moselle – a plan approved of by The Hague – but once there, he would slip the Dutch leash and link up with Austrian forces in southern Germany. "My intentions", wrote the Duke from The Hague on 29 April to his governmental confidant, Sidney Godolphin, "are to march with the English to Coblenz and declare that I intend to campaign on the Moselle. But when I come there, to write to the Dutch States that I think it absolutely necessary for the saving of the Empire to march with the troops under my command and to join with those that are in Germany ... in order to make measures with Prince Lewis of Baden for the speedy reduction of the Elector of Bavaria."
Prelude.
Protagonists march to the Danube.
Marlborough's march started on 19 May from Bedburg, north-west of Cologne. The army (assembled by the Duke's brother, General Charles Churchill) consisted of 66 squadrons, 31 battalions and 38 guns and mortars totalling 21,000 men (16,000 of whom were English troops). This force was to be augmented "en route" such that by the time Marlborough reached the Danube, it would number 40,000 (47 battalions, 88 squadrons). Whilst Marlborough led his army, General Overkirk would maintain a defensive position in the Dutch Republic in case Villeroi mounted an attack. The Duke had assured the Dutch that if the French were to launch an offensive he would return in good time, but Marlborough calculated that as he marched south, the French commander would be drawn after him. In this assumption Marlborough proved correct: Villeroi shadowed the Duke with 30,000 men in 60 squadrons and 42 battalions.
The military dangers in such an enterprise were numerous: Marlborough's lines of communication along the Rhine would be hopelessly exposed to French interference, for Louis' generals controlled the left bank of the river and its central reaches. Such a long march would almost certainly involve a high wastage of men and horses through exhaustion and disease. However, Marlborough was convinced of the urgency – "I am very sensible that I take a great deal upon me", he had earlier written to Godolphin, "but should I act otherwise, the Empire would be undone ..."
Whilst Allied preparations had progressed, the French were striving to maintain and re-supply Marshal Marsin. Marsin had been operating with the Elector of Bavaria against the Imperial commander, Prince Louis of Baden, and was somewhat isolated from France: his only lines of communication lay through the rocky passes of the Black Forest. However, on 14 May, with considerable skill Marshal Tallard managed to bring 10,000 reinforcements and vast supplies and munitions through the difficult terrain, whilst outmanoeuvring Baron Thüngen, the Imperial general who sought to block his path. Tallard then returned with his own force to the Rhine, once again side-stepping Thüngen's efforts to intercept him. The whole operation was an outstanding military achievement.
On 26 May, Marlborough reached Coblenz, where the Moselle meets the Rhine. If he intended an attack along the Moselle the Duke must now turn west, but, instead, the following day the army crossed to the right bank of the Rhine, (pausing to add 5,000 waiting Hanoverians and Prussians). "There will be no campaign on the Moselle", wrote Villeroi who had taken up a defensive position on the river, "the English have all gone up into Germany." A second possible objective now occurred to the French – an Allied incursion into Alsace and an attack on the city of Strasbourg. Marlborough skillfully encouraged this apprehension by constructing bridges across the Rhine at Philippsburg, a ruse that not only encouraged Villeroi to come to Tallard's aid in the defence of Alsace, but one that ensured the French plan to march on Vienna remained paralysed by uncertainty.
With Villeroi shadowing Marlborough's every move, Marlborough's gamble that the French would not move against the weakened Dutch position in the Netherlands paid off. In any case, Marlborough had promised to return to the Netherlands if a French attack developed there, transferring his troops down the Rhine on barges at a rate of a day. Encouraged by this promise (whatever it was worth) the States General agreed to release the Danish contingent of seven battalions and 22 squadrons as a reinforcement. Marlborough reached Ladenburg, in the plain of the Neckar and the Rhine, and there halted for three days to rest his cavalry and allow the guns and infantry to close up. On 6 June he arrived at Wiesloch, south of Heidelberg. The following day, the Allied army swung away from the Rhine towards the hills of the Swabian Jura and the Danube beyond. At last Marlborough's destination was established without doubt.
Strategy.
On 10 June, the Duke met for the first time the President of the Imperial War Council, Prince Eugene – accompanied by Count Wratislaw – at the village of Mundelsheim, half-way between the Danube and the Rhine. By 13 June, the Imperial Field Commander, Prince Louis of Baden, had joined them in Großheppach. The three generals commanded a force of nearly 110,000 men. At conference it was decided that Eugene would return with 28,000 men to the Lines of Stollhofen on the Rhine to keep an eye on Villeroi and Tallard, and prevent them going to the aid of the Franco-Bavarian army on the Danube. Meanwhile, Marlborough's and Baden's forces would combine, totalling 80,000 men, for the march on the Danube to seek out the Elector and Marsin before they could be reinforced.
Knowing Marlborough's destination, Tallard and Villeroi met at Landau in Alsace on 13 June to rapidly construct an action plan to save Bavaria, but the rigidity of the French command system was such that any variations from the original plan had to be sanctioned by Versailles. The Count of Mérode-Westerloo, commander of the Flemish troops in Tallard's army wrote – "One thing is certain: we delayed our march from Alsace for far too long and quite inexplicably." Approval from Louis arrived on 27 June: Tallard was to reinforce Marsin and the Elector on the Danube via the Black Forest, with 40 battalions and 50 squadrons; Villeroi was to pin down the Allies defending the Lines of Stollhofen, or, if the Allies should move all their forces to the Danube, he was to join with Marshal Tallard; and General de Coignies with 8,000 men, would protect Alsace. On 1 July Tallard's army of 35,000 re-crossed the Rhine at Kehl and began its march.
Meanwhile, on 22 June, Marlborough's forces linked up with Baden's Imperial forces at Launsheim. A distance of had been covered in five weeks. Thanks to a carefully planned time-table, the effects of wear and tear had been kept to a minimum. Captain Parker described the march discipline – "As we marched through the country of our Allies, commissars were appointed to furnish us with all manner of necessaries for man and horse ... the soldiers had nothing to do but pitch their tents, boil kettles and lie down to rest." In response to Marlborough's manoeuvres, the Elector and Marsin, conscious of their numerical disadvantage with only 40,000 men, moved their forces to the entrenched camp at Dillingen on the north bank of the Danube. Marlborough could not attack Dillingen because of a lack of siege guns – he was unable to bring any from the Low Countries, and Baden had failed to supply any despite assurances to the contrary.
The Allies, nevertheless, needed a base for provisions and a good river crossing. On 2 July, therefore, Marlborough stormed the key fortress of Schellenberg on the heights above the town of Donauwörth. Count Jean d'Arco had been sent with 12,000 men from the Franco-Bavarian camp to hold the town and grassy hill, but after a ferocious and bloody battle, inflicting enormous casualties on both sides, Schellenberg finally succumbed, forcing Donauwörth to surrender shortly afterwards. The Elector, knowing his position at Dillingen was now not tenable, took up a position behind the strong fortifications of Augsburg.
Tallard's march, meanwhile, presented a dilemma for Eugene. If the Allies were not to be outnumbered on the Danube, Eugene realised he must either try to cut Tallard off before he could get there, or, he must hasten to reinforce Marlborough. However, if he withdrew from the Rhine to the Danube, Villeroi might also make a move south to link up with the Elector and Marsin. Eugene compromised: leaving 12,000 troops behind guarding the Lines of Stollhofen, he marched off with the rest of his army to forestall Tallard.
Lacking in numbers, Eugene could not seriously disrupt Tallard's march; nevertheless, the French Marshal's progress was proving pitifully slow. Tallard's force had suffered considerably more than Marlborough's troops on their march – many of his cavalry horses were suffering from glanders, and the mountain passes were proving tough for the 2,000 wagons of provisions. Local German peasants, angry at French plundering, compounded Tallard's problems, leading Mérode-Westerloo to bemoan – "the enraged peasantry killed several thousand of our men before the army was clear of the Black Forest." Additionally, Tallard had insisted on besieging the little town of Villingen for six days (16–22 July), but abandoned the enterprise on discovering the approach of Eugene.
The Elector in Augsburg was informed on 14 July that Tallard was on his way through the Black Forest. This good news bolstered the Elector's policy of inaction, further encouraging him to wait for the reinforcements. But this reticence to fight induced Marlborough to undertake a controversial policy of spoliation in Bavaria, burning buildings and crops throughout the rich lands south of the Danube. This had two aims: firstly to put pressure on the Elector to fight or come to terms before Tallard arrived with reinforcements; and secondly, to ruin Bavaria as a base from which the French and Bavarian armies could attack Vienna, or pursue the Duke into Franconia if, at some stage, he had to withdraw northwards. But this destruction, coupled with a protracted siege of Rain (9–16 July), caused Prince Eugene to lament "... since the Donauwörth action I cannot admire their performances", and later to conclude "If he has to go home without having achieved his objective, he will certainly be ruined." Nevertheless, strategically the Duke had been able to place his numerically stronger forces between the Franco-Bavarian army and Vienna.
Final positioning.
Marshal Tallard, with 34,000 men, reached Ulm, joining with the Elector and Marsin in Augsburg on 5 August (although Tallard was not impressed to find that the Elector had dispersed his army in response to Marlborough's campaign of ravaging the region). Also on 5 August, Eugene reached Höchstädt, riding that same night to meet with Marlborough at Schrobenhausen. Marlborough knew it was necessary that another crossing point over the Danube would be required in case Donauwörth fell to the enemy. On 7 August, therefore, the first of Baden's 15,000 Imperial troops (the remainder following two days later) left Marlborough's main force to besiege the heavily defended city of Ingolstadt, farther down the Danube.
With Eugene's forces at Höchstädt on the north bank of the Danube, and Marlborough's at Rain on the south bank, Tallard and the Elector debated their next move. Tallard preferred to bide his time, replenish supplies and allow Marlborough's Danube campaign to flounder in the colder weeks of Autumn; the Elector and Marsin, however, newly reinforced, were keen to push ahead. The French and Bavarian commanders eventually agreed on a plan and decided to attack Eugene's smaller force. On 9 August, the Franco-Bavarian forces began to cross to the north bank of the Danube.
On 10 August, Eugene sent an urgent dispatch reporting that he was falling back to Donauwörth – "The enemy have marched. It is almost certain that the whole army is crossing the Danube at Lauingen ... The plain of Dillingen is crowded with troops ... Everything, milord, consists in speed and that you put yourself forthwith in movement to join me tomorrow, without which I fear it will be too late." By a series of brilliant marches Marlborough concentrated his forces on Donauwörth and, by noon 11 August, the link-up was complete.
During 11 August, Tallard pushed forward from the river crossings at Dillingen; by 12 August, the Franco-Bavarian forces were encamped behind the small river Nebel near the village of Blenheim on the plain of Höchstädt. That same day Marlborough and Eugene carried out their own reconnaissance of the French position from the church spire at Tapfheim, and moved their combined forces to Münster – five miles (8 km) from the French camp. A French reconnaissance under the Marquis de Silly went forward to probe the enemy, but were driven off by Allied troops who had deployed to cover the pioneers of the advancing army, labouring to bridge the numerous streams in the area and improve the passage leading westwards to Höchstädt. Marlborough quickly moved forward two brigades under the command of General Wilkes and Brigadier Rowe to secure the narrow strip of land between the Danube and the wooded Fuchsberg hill, at the Schwenningen defile.
Tallard's army numbered 56,000 men and 90 guns; the army of the Grand Alliance, 52,000 men and 66 guns. Some Allied officers who were acquainted with the superior numbers of the enemy, and aware of their strong defensive position, ventured to remonstrate with Marlborough about the hazards of attacking; but the Duke was resolute – "I know the danger, yet a battle is absolutely necessary, and I rely on the bravery and discipline of the troops, which will make amends for our disadvantages". Marlborough and Eugene decided to risk everything, and agreed to attack on the following day.
Battle.
The battlefield.
The battlefield stretched for nearly . The extreme right flank of the Franco-Bavarian army was covered by the Danube; to the extreme left flank lay the undulating pine-covered hills of the Swabian Jura. A small stream, the Nebel, (the ground either side of which was soft and marshy and only fordable intermittently), fronted the French line. The French right rested on the village of Blenheim near where the Nebel flows into the Danube; the village itself was surrounded by hedges, fences, enclosed gardens, and meadows. Between Blenheim and the next village of Oberglauheim the fields of wheat had been cut to stubble and were now ideal to deploy troops. From Oberglauheim to the next hamlet of Lutzingen the terrain of ditches, thickets and brambles was potentially difficult ground for the attackers.
Initial manoeuvres.
At 02:00 on 13 August, 40 squadrons were sent forward towards the enemy, followed at 03:00, in eight columns, by the main Allied force pushing over the Kessel. At about 06:00 they reached Schwenningen, two miles (3 km) from Blenheim. The English and German troops who had held Schwenningen through the night joined the march, making a ninth column on the left of the army. Marlborough and Eugene made their final plans. The Allied commanders agreed that Marlborough would command 36,000 troops and attack Tallard's force of 33,000 on the left (including capturing the village of Blenheim), whilst Eugene, commanding 16,000 men would attack the Elector and Marsin's combined forces of 23,000 troops on the right wing; if this attack was pressed hard the Elector and Marsin would have no troops to send to aid Tallard on their right. Lieutenant-General John Cutts would attack Blenheim in concert with Eugene's attack. With the French flanks busy, Marlborough could cross the Nebel and deliver the fatal blow to the French at their centre. However, Marlborough would have to wait until Eugene was in position before the general engagement could begin.
The last thing Tallard expected that morning was to be attacked by the Allies – deceived by intelligence gathered from prisoners taken by de Silly the previous day, and assured in their strong natural position, Tallard and his colleagues were convinced that Marlborough and Eugene were about to retreat north-eastwards towards Nördlingen. Tallard wrote a report to this effect to King Louis that morning, but hardly had he sent the messenger when the Allied army began to appear opposite his camp. "I could see the enemy advancing ever closer in nine great columns", wrote Mérode-Westerloo, " ... filling the whole plain from the Danube to the woods on the horizon." Signal guns were fired to bring in the foraging parties and picquets as the French and Bavarian troops tried to draw into battle-order to face the unexpected threat.
At about 08:00 the French artillery on their right wing opened fire, answered by Colonel Blood's batteries. The guns were heard by Baden in his camp before Ingolstadt, "The Prince and the Duke are engaged today to the westward", he wrote to the Emperor. "Heaven bless them." An hour later Tallard, the Elector, and Marsin climbed Blenheim's church tower to finalise their plans. It was settled that the Elector and Marsin would hold the front from the hills to Oberglauheim, whilst Tallard would defend the ground between Oberglauheim and the Danube. The French commanders were, however, divided as to how to utilise the Nebel: Tallard's tactic – opposed by Marsin and the Elector who felt it better to close their infantry right up to the stream itself – was to lure the allies across before unleashing their cavalry upon them, causing panic and confusion; whilst the enemy was struggling in the marshes, they would be caught in crossfire from Blenheim and Oberglauheim. The plan was sound if all its parts were implemented, but it allowed Marlborough to cross the Nebel without serious interference and fight the battle he had in mind.
Deployment.
The Franco-Bavarian commanders deployed their forces. In the village of Lutzingen, Count Maffei positioned five Bavarian battalions with a great battery of 16 guns at the village's edge. In the woods to the left of Lutzingen, seven French battalions under the Marquis de Rozel moved into place. Between Lutzingen and Oberglauheim the Elector placed 27 squadrons of cavalry – Count d'Arco commanded 14 Bavarian squadrons and Count Wolframsdorf had 13 more in support nearby. To their right stood Marsin's 40 French squadrons and 12 battalions. The village of Oberglauheim was packed with 14 battalions commanded by the Marquis de Blainville (including the effective Irish Brigade known as the 'Wild Geese'). Six batteries of guns were ranged alongside the village. On the right of these French and Bavarian positions, between Oberglauheim and Blenheim, Tallard deployed 64 French and Walloon squadrons (16 drawn from Marsin) supported by nine French battalions standing near the Höchstädt road. In the cornfield next to Blenheim stood three battalions from the Regiment de Roi. Nine battalions occupied the village itself, commanded by the Marquis de Clérambault. Four battalions stood to the rear and a further 11 were in reserve. These battalions were supported by Hautefeuille's 12 squadrons of dismounted dragoons. By 11:00 Tallard, the Elector, and Marsin were in place. Many of the Allied generals were hesitant to attack such a relatively strong position. The Earl of Orkney later confessed that, "had I been asked to give my opinion, I had been against it."
Prince Eugene was expected to be in position by 11:00, but due to the difficult terrain and enemy fire, progress was slow. Lord Cutts' column – who by 10:00 had expelled the enemy from two water mills upon the Nebel – had already deployed by the river against Blenheim, enduring over the next three hours severe fire from a heavy six-gun battery posted near the village. The rest of Marlborough's army, waiting in their ranks on the forward slope, were also forced to bear the cannonade from the French artillery, suffering 2,000 casualties before the attack could even be begun. Meanwhile engineers repaired a stone bridge across the Nebel, and constructed five additional bridges or causeways across the marsh between Blenheim and Oberglauheim. Marlborough's anxiety was finally allayed when, just past noon, Colonel Cadogan reported that Eugene's Prussian and Danish infantry were in place – the order for the general advance was given. At 13:00, Cutts was ordered to attack the village of Blenheim whilst Prince Eugene was requested to assault Lutzingen on the Allied right flank.
Blenheim.
Cutts ordered Brigadier-General Archibald Rowe's brigade to attack. The English infantry rose from the edge of the Nebel, and silently marched towards Blenheim, a distance of some . John Ferguson's English brigade supported Rowe's left, and moved in perfect order towards the barricades between the village and the river, defended by Hautefeuille's dragoons. As the range closed to within , the French fired a deadly volley. Rowe had ordered that there should be no firing from his men until he struck his sword upon the palisades, but as he stepped forward to give the signal, he fell mortally wounded. The survivors of the leading companies closed up the gaps in their torn ranks and rushed forward. Small parties penetrated the defences, but repeated French volleys forced the English back towards the Nebel, sustaining heavy casualties. As the attack faltered, eight squadrons of elite Gens d'Armes, commanded by the veteran Swiss officer, Beat-Jacques von Zurlauben, fell upon the English troops, cutting at the exposed flank of Rowe's own regiment. However, Wilkes' Hessian brigade, lying nearby in the marshy grass at the water's edge, stood firm and repulsed the Gens d'Armes with steady fire, enabling the English and Hessians to re-order and launch another attack.
Although the Allies were again repulsed, these persistent attacks on Blenheim eventually bore fruit, panicking Clérambault into making the worst French error of the day. Without consulting Tallard, Clérambault ordered his reserve battalions into the village, upsetting the balance of the French position and nullifying the French numerical superiority. "The men were so crowded in upon one another", wrote Mérode-Westerloo, "that they couldn't even fire – let alone receive or carry out any orders." Marlborough, spotting this error, now countermanded Cutts' intention to launch a third attack, and ordered him simply to contain the enemy within Blenheim; no more than 5,000 Allied soldiers were able to pen in twice the number of French infantry and dragoons.
Lutzingen.
On the Allied right, Eugene's Prussian and Danish forces were desperately fighting the numerically superior forces of the Elector and Marsin. The Prince of Anhalt-Dessau led forward four brigades across the Nebel to assault the well-fortified position of Lutzingen. Here, the Nebel was less of an obstacle, but the great battery positioned on the edge of the village enjoyed a good field of fire across the open ground stretching to the hamlet of Schwennenbach. As soon as the infantry crossed the stream, they were struck by Maffei's infantry, and salvoes from the Bavarian guns positioned both in front of the village and in enfilade on the wood-line to the right. Despite heavy casualties the Prussians attempted to storm the great battery, whilst the Danes, under Count Scholten, attempted to drive the French infantry out of the copses beyond the village.
With the infantry heavily engaged, Eugene's cavalry picked its way across the Nebel. After an initial success, his first line of cavalry, under the Imperial General of Horse, Prince Maximilian of Hanover, were pressed by the second line of Marsin's cavalry, and were forced back across the Nebel in confusion. Nevertheless, the exhausted French were unable to follow up their advantage, and the two cavalry forces tried to regroup and reorder their ranks. However, without cavalry support, and threatened with envelopment, the Prussian and Danish infantry were in turn forced to pull back across the Nebel. Panic gripped some of Eugene's troops as they crossed the stream. Ten infantry colours were lost to the Bavarians, and hundreds of prisoners taken; it was only through the leadership of Eugene and the Prussian Prince that the imperial infantry were prevented from abandoning the field.
After rallying his troops near Schwennenbach – well beyond their starting point – Eugene prepared to launch a second attack, led by the second-line squadrons under the Duke of Württemberg-Teck. Yet again they were caught in the murderous cross-fire from the artillery in Lutzingen and Oberglauheim, and were once again thrown back in disarray. The French and Bavarians, however, were almost as disordered as their opponents, and they too were in need of inspiration from their commander, the Elector, who was seen – " ... riding up and down, and inspiring his men with fresh courage." Anhalt-Dessau's Danish and Prussian infantry attacked a second time but could not sustain the advance without proper support. Once again they fell back across the stream.
Centre and Oberglauheim.
Whilst these events around Blenheim and Lutzingen were taking place, Marlborough was preparing to cross the Nebel. The centre, commanded by the Duke's brother, General Charles Churchill, consisted of 18 battalions of infantry arranged in two lines: seven battalions in the front line to secure a foothold across the Nebel, and 11 battalions in the rear providing cover from the Allied side of the stream. Between the infantry were placed two lines, 72 squadrons of cavalry. The first line of foot was to pass the stream first and march as far to the other side as could be conveniently done. This line would then form and cover the passage of the horse, leaving gaps in the line of infantry large enough for the cavalry to pass through and take their position in front.
Marlborough ordered the formation forward. Once again Zurlauben's Gens d'Armes charged, looking to rout Lumley's English cavalry who linked Cutts' column facing Blenheim with Churchill's infantry. As these elite French cavalry attacked, they were faced by five English squadrons under Colonel Francis Palmes. To the consternation of the French, the Gens d'Armes were pushed back in terrible confusion, pursued well beyond the Maulweyer stream that flows through Blenheim. "What? Is it possible?" exclaimed the Elector, "the gentlemen of France fleeing?" Palmes, however, attempted to follow up his success but was repulsed in some confusion by other French cavalry, and musketry fire from the edge of Blenheim.
Nevertheless, Tallard was alarmed by the repulse of the elite Gens d'Armes and urgently rode across the field to ask Marsin for reinforcements; but on the basis of being hard pressed by Eugene – whose second attack was in full flood – Marsin refused. As Tallard consulted with Marsin, more of his infantry was being taken into Blenheim by Clérambault. Fatally, Tallard, aware of the situation, did nothing to rectify this grave mistake, leaving him with just the nine battalions of infantry near the Höchstädt road
to oppose the massed enemy ranks in the centre. Zurlauben tried several more times to disrupt the Allies forming on Tallard's side of the stream; his front-line cavalry darting forward down the gentle slope towards the Nebel. But the attacks lacked co-ordination, and the Allied infantry's steady volleys disconcerted the French horsemen. During these skirmishes Zurlauben fell mortally wounded, and died two days later. The time was just after 15:00.
The Danish cavalry, under the Duke of Württemberg-Neuenstadt (not to be confused with the Duke of Württemberg who fought with Eugene), had made slow work of crossing the Nebel near Oberglau; harassed by Marsin's infantry near the village, the Danes were driven back across the stream. Count Horn's Dutch infantry managed to push the French back from the water's edge, but it was apparent that before Marlborough could launch his main effort against Tallard, Oberglauheim would have to be secured.
Count Horn directed the Prince of Holstein-Beck to take the village, but his two Dutch brigades were cut down by the French and Irish troops, capturing and badly wounding the Prince during the action. The battle was now in the balance. If Holstein-Beck's Dutch column were destroyed, the Allied army would be split in two: Eugene's wing would be isolated from Marlborough's, passing the initiative to the Franco-Bavarian forces now engaged across the whole plain. Seeing the opportunity, Marsin ordered his cavalry to change from facing Eugene, and turn towards their right and the open flank of Churchill's infantry drawn up in front of Unterglau. Marlborough (who had crossed the Nebel on a makeshift bridge to take personal control), ordered Hulsen's Hanoverian battalions to support the Dutch infantry. A Dutch cavalry brigade under Averock was also called forward but soon came under pressure from Marsin's more numerous squadrons.
Marlborough now requested Eugene to release Count Hendrick Fugger and his Imperial Cuirassier brigade to help repel the French cavalry thrust. Despite his own desperate struggle, the Imperial Prince at once complied, demonstrating the high degree of confidence and mutual co-operation between the two generals. Although the Nebel stream lay between Fugger's and Marsin's squadrons, the French were forced to change front to meet this new threat, thus forestalling the chance for Marsin to strike at Marlborough's infantry. Fugger's cuirassiers charged and, striking at a favourable angle, threw back Marsin's squadrons in disorder. With support from Colonel Blood's batteries, the Hessian, Hanoverian and Dutch infantry – now commanded by Count Berensdorf – succeeded in pushing the French and Irish infantry back into Oberglauheim so that they could not again threaten Churchill's flank as he moved against Tallard. The French commander in the village, the Marquis de Blainville, numbered amongst the heavy casualties.
Breakthrough.
By 16:00, with the enemy troops besieged in Blenheim and Oberglau, the Allied centre of 81 squadrons (nine squadrons had been transferred from Cutts' column), supported by 18 battalions was firmly planted amidst the French line of 64 squadrons and nine battalions of raw recruits. There was now a pause in the battle: Marlborough wanted to concert the attack upon the whole front, and Eugene, after his second repulse, needed time to reorganize.
Just after 17:00 all was ready along the Allied front. Marlborough's two lines of cavalry had now moved to the front of the Duke's line of battle, with the two supporting lines of infantry behind them. Mérode-Westerloo attempted to extricate some French infantry crowded in Blenheim, but Clérambault ordered the troops back into the village. The French cavalry exerted themselves once more against the first line – Lumley's English and Scots on the Allied left, and Hompesch's Dutch and German squadrons on the Allied right. Tallard's squadrons, lacking infantry support, were tired and ragged but managed to push the Allied first line back to their infantry support. With the battle still not won, Marlborough had to rebuke one of his cavalry officers who was attempting to leave the field – "Sir, you are under a mistake, the enemy lies that way ..." Now, at the Duke's command, the second Allied line under von Bulow and the Count of Ost-Friese was ordered forward, and, driving through the centre, the Allies finally put Tallard's tired horse to rout, not without cost. The Prussian Life Dragoons' Colonel, Ludwig von Blumenthal, and his 2nd in command, Lt. Col. von Hacke, fell next to each other. But the charge succeeded and with their cavalry in headlong flight, the remaining nine French infantry battalions fought with desperate valour, trying to form square. But it was futile. The French battalions were overwhelmed by Colonel Blood's close-range artillery and platoon fire. Mérode-Westerloo later wrote – "[They] died to a man where they stood, stationed right out in the open plain – supported by nobody."
The majority of Tallard's retreating troops headed for Höchstädt but most did not make the safety of the town, plunging instead into the Danube where upwards of 3,000 French horsemen drowned; others were cut down by the pursuing cavalry. The Marquis de Gruignan attempted a counter-attack, but he was easily brushed aside by the triumphant Allies. After a final rally behind his camp's tents, shouting entreaties to stand and fight, Marshal Tallard was caught up in the rout and pushed towards Sonderheim. Surrounded by a squadron of Hessian troops, Tallard surrendered to Lieutenant-Colonel de Boinenburg, the Prince of Hesse-Kassel's "aide-de-camp", and was sent under escort to Marlborough. The Duke welcomed the French commander – "I am very sorry that such a cruel misfortune should have fallen upon a soldier for whom I have the highest regard." With salutes and courtesies, the Marshal was escorted to Marlborough's coach.
Fall of Blenheim.
Meanwhile the Allies had once again attacked the Bavarian stronghold at Lutzingen.
Eugene, however, became exasperated with the performance of his Imperial cavalry whose third attack had failed: he had already shot two of his troopers to prevent a general flight. Then, declaring in disgust that he wished to "fight among brave men and not among cowards", Eugene went into the attack with the Prussian and Danish infantry, as did the Dessauer, waving a regimental colour to inspire his troops. This time the Prussians were able to storm the great Bavarian battery, and overwhelm the guns' crews. Beyond the village, Scholten's Danes defeated the French infantry in a desperate hand-to-hand bayonet struggle. When they saw that the centre had broken, the Elector and Marsin decided the battle was lost and, like the remnants of Tallard's army, fled the battlefield (albeit in better order than Tallard's men). Attempts to organise an Allied force to prevent Marsin's withdrawal failed owing to the exhaustion of the cavalry, and the growing confusion in the field.
Marlborough now had to turn his attention from the fleeing enemy to direct Churchill to detach more infantry to storm Blenheim. Orkney's infantry, Hamilton's English brigade and St Paul's Hanoverians moved across the trampled wheat to the cottages. Fierce hand-to-hand fighting gradually forced the French towards the village centre, in and around the walled churchyard which had been prepared for defence. Hay and Ross's dismounted dragoons were also sent, but suffered under a counter-charge delivered by the regiments of Artois and Provence under command of Colonel de la Silvière. Colonel Belville's Hanoverians were fed into the battle to steady the resolve of the dragoons, and once more went to the attack. The Allied progress was slow and hard, and like the defenders, they suffered many casualties.
Many of the cottages were now burning, obscuring the field of fire and driving the defenders out of their positions. Hearing the din of battle in Blenheim, Tallard sent a message to Marlborough offering to order the garrison to withdraw from the field.
"Inform Monsieur Tallard", replied the Duke, "that, in the position in which he is now, he has no command." Nevertheless, as dusk came the Allied commander was anxious for a quick conclusion. The French infantry fought tenaciously to hold on to their position in Blenheim, but their commander was nowhere to be found. Clérambault's insistence on confining his huge force in the village was to seal his fate that day. Realising his tactical mistake had contributed to Tallard's defeat in the centre, Clérambault deserted Blenheim and the 27 battalions defending the village, and reportedly drowned in the Danube while attempting to make his escape.
By now Blenheim was under assault from every side by three English generals: Cutts, Churchill, and Orkney. The French had repulsed every attack with heavy slaughter, but many had seen what had happened on the plain and what its consequences to them would be; their army was routed and they were cut off. Orkney, attacking from the rear, now tried a different tactic – "... it came into my head to beat parley", he later wrote, "which they accepted of and immediately their Brigadier de Nouville capitulated with me to be prisoner at discretion and lay down their arms." Threatened by Allied guns, other units followed their example. However, it was not until 21:00 that the Marquis de Blanzac, who had taken charge in Clérambault's absence, reluctantly accepted the inevitability of defeat, and some 10,000 of France's best infantry had laid down their arms.
During these events Marlborough was still in the saddle conducting the pursuit of the broken enemy. Pausing for a moment he scribbled on the back of an old tavern bill a note addressed to his wife, Sarah: "I have no time to say more but to beg you will give my duty to the Queen, and let her know her army has had a glorious victory."
Aftermath.
French losses were immense: over 30,000 killed, wounded and missing. Moreover, the myth of French invincibility had been destroyed and Louis's hopes of an early and victorious peace had been wrenched from his grasp. Mérode-Westerloo summarised the case against Tallard's army: "The French lost this battle for a wide variety of reasons. For one thing they had too good an opinion of their own ability ... Another point was their faulty field dispositions, and in addition there was rampant indiscipline and inexperience displayed ... It took all these faults to lose so celebrated a battle." It was a hard-fought contest, leading Prince Eugene to observe – "I have not a squadron or battalion which did not charge four times at least." Nevertheless, although the war dragged on for years, the Battle of Blenheim was probably its most decisive victory; Marlborough and Eugene, working indivisibly together, had saved the Habsburg Empire and thereby preserved the Grand Alliance from collapse. Munich, Augsburg, Ingolstadt, Ulm and all remaining territory of Bavaria soon fell to the Allies. By the Treaty of Ilbersheim, signed 7 November 1704, Bavaria was placed under Austrian military rule, allowing the Habsburgs to utilise its resources for the rest of the conflict.
The remnants of the Elector of Bavaria's and Marshal Marsin's wing limped back to Strasbourg, losing another 7,000 men through desertion. Despite being offered the chance to remain as ruler of Bavaria (under strict terms of an alliance with Austria), the Elector left his country and family in order to continue the war against the Allies from the Spanish Netherlands where he still held the post of governor-general. Their commander-in-chief that day, Marshal Tallard – who, unlike his subordinates, had not been ransomed or exchanged – was taken to England and imprisoned in Nottingham until his release in 1711.
The 1704 campaign lasted considerably longer than usual as the Allies sought to wring out maximum advantage. Realising that France was too powerful to be forced to make peace by a single victory, however, Eugene, Marlborough and Baden met to plan their next moves. For the following year the Duke proposed a campaign along the valley of the River Moselle to carry the war deep into France. This required the capture of the major fortress of Landau which guarded the Rhine, and the towns of Trier and Trarbach on the Moselle itself. Trier was taken on 26 October and Landau fell on 23 November to the Margrave of Baden and Prince Eugene; with the fall of Trarbach on 20 December, the campaign season for 1704 came to an end.
Marlborough returned to England on 14 December (O.S) to the acclamation of Queen Anne and the country. In the first days of January the 110 cavalry standards and the 128 infantry colours that were taken during the battle were borne in procession to Westminster Hall. In February 1705, Queen Anne, who had made Marlborough a Duke in 1702, granted him the Park of Woodstock and promised a sum of £240,000 to build a suitable house as a gift from a grateful crown in recognition of his victory – a victory which British historian Sir Edward Shepherd Creasy considered one of the pivotal battles in history, writing – "Had it not been for Blenheim, all Europe might at this day suffer under the effect of French conquests resembling those of Alexander in extent and those of the Romans in durability."

</doc>
<doc id="4050" url="http://en.wikipedia.org/wiki?curid=4050" title="Battle of Ramillies">
Battle of Ramillies

The Battle of Ramillies , fought on 23 May 1706, was a major engagement of the War of the Spanish Succession. For the Grand Alliance – Austria, England, and the Dutch Republic – the battle had followed an indecisive campaign against the Bourbon armies of King Louis XIV of France in 1705. Although the Allies had captured Barcelona that year, they had been forced to abandon their campaign on the Moselle, had stalled in the Spanish Netherlands, and suffered defeat in northern Italy. Yet despite his opponents' setbacks Louis XIV was desirous of peace – but he wanted it on reasonable terms. For this end, and in order to maintain their momentum, the French and their allies would swing over to the offensive in 1706.
The campaign began well for Louis XIV's generals: in Italy Marshal Vendôme had defeated the Austrians at the Battle of Calcinato in April, while in Alsace Marshal Villars had forced the Margrave of Baden back across the Rhine. Encouraged by these early gains Louis XIV urged Marshal Villeroi to go over to the offensive in the Spanish Netherlands and, with victory, gain a 'fair' peace. Accordingly, the French Marshal set off from Leuven ("Louvain") at the head of 60,000 men and marched towards Tienen ("Tirlemont"), as if to threaten Zoutleeuw ("Léau"). Also determined to fight a major engagement, the Duke of Marlborough, commander-in-chief of Anglo-Dutch forces, assembled his army – some 62,000 men – near Maastricht, and marched past Zoutleeuw. With both sides seeking battle, they soon stumbled upon one other on the dry ground between the Mehaigne and Petite Gheete rivers, close to the small village of Ramillies.
In less than four hours Marlborough's Dutch, English, and Danish forces overwhelmed Villeroi's and Max Emanuel's Franco-Spanish-Bavarian army. The Duke's subtle moves and changes in emphasis during the battle – something his opponents failed to realise until it was too late – caught the French in a tactical vice. The battle proved decisive. With their foe broken and routed, the Allies were able to fully exploit their victory. Town after town subsequently fell, including Brussels, Bruges, Antwerp; by the end of the campaign Villeroi's army had been driven from most of the Spanish Netherlands. With Prince Eugene's subsequent success at the Battle of Turin in northern Italy, the Allies had imposed the greatest loss of territory and resources that Louis XIV would suffer during the war. The year 1706 had indeed proved to be the Allies' "annus mirabilis".
Background.
After their disastrous defeat at Blenheim in 1704, the next year brought the French some respite. The Duke of Marlborough had intended the 1705 campaign – an invasion of France through the Moselle valley – to complete the work of Blenheim and persuade King Louis XIV to make peace, but the plan had been thwarted by both friend and foe alike. The reluctance of his Dutch allies to see their frontiers denuded of troops for another gamble in Germany had denied Marlborough the initiative, but of far greater importance was the Margrave of Baden’s pronouncement that he could not join the Duke in strength for the coming offensive. This was in part due to the sudden switching of troops from the Rhine to reinforce Prince Eugene in Italy, and part due to the deterioration of Baden’s health brought on by the re-opening of a severe foot wound he had received at the storming of the Schellenberg the previous year. Moreover, Marlborough had to cope with the death of Emperor Leopold I in May and the accession of Joseph I, which unavoidably complicated matters for the Grand Alliance.
The resilience of the French King, and the efforts of his generals, also added to Marlborough’s problems. Marshal Villeroi, exerting considerable pressure on the Dutch commander, Count Overkirk, along the Meuse, took Huy on 10 June before pressing on towards Liège. With Marshal Villars sitting strong on the Moselle, the Allied commander – whose supplies had by now become critical – was forced to call off his campaign on 16 June. "What a disgrace for Marlborough," exulted Villeroi, "to have made false movements without any result!" With Marlborough’s departure north, the French now transferred troops from the Moselle valley to reinforce Villeroi in Flanders, while Villars marched off to the Rhine.
The Anglo-Dutch forces gained minor compensation for the failed Moselle campaign with the success at Elixheim and the crossing of the Lines of Brabant in the Spanish Netherlands (Huy was also retaken on 11 July), but a chance to bring the French to a decisive engagement had eluded Marlborough. The year 1705 proved almost entirely barren for the Duke whose military disappointments were only partly compensated by efforts on the diplomatic front where, at the courts of Düsseldorf, Frankfurt, Vienna, Berlin and Hanover, Marlborough sought to bolster support for the Grand Alliance and extract promises of prompt assistance for the following year's campaign.
Prelude.
On 11 January 1706, Marlborough finally reached London at the end of his diplomatic tour, but he had already been planning his strategy for the coming season. The first option (although it is debatable to what extent the Duke was committed to such an enterprise) was a plan to transfer his forces from the Spanish Netherlands to northern Italy; once there, he intended linking up with Prince Eugene in order to defeat the French and safeguard Savoy from being overrun. Savoy would then serve as a gateway into France by way of the mountain passes or, alternatively, an invasion with naval support along the Mediterranean coast via Nice and Toulon in connection with redoubled Allied efforts in Spain. However, it seems that the Duke’s favoured scheme was to return to the Moselle valley (where Marshal Marsin had recently taken command of French forces) and once more attempt an advance into the heart of France. But these decisions soon became academic. Shortly after Marlborough landed in the Dutch Republic on 14 April, news arrived of major Allied setbacks in the wider war.
Determined to show the Grand Alliance that France was still resolute, Louis XIV prepared to launch a double surprise in Alsace and northern Italy. On the latter front Marshal Vendôme defeated the Imperial army at Calcinato on 19 April, pushing the Imperialists back in confusion (French forces were now in a position to prepare for the long-anticipated siege of Turin). In Alsace, Marshal Villars took Baden by surprise and captured Haguenau, driving him back across the Rhine in some disorder, thus creating a threat on Landau. With these reverses, the Dutch now refused to contemplate Marlborough’s ambitious march to Italy or, indeed, any plan that denuded their borders of the Duke and their army. Therefore, in the interest of coalition harmony, Marlborough prepared to campaign in the Low Countries.
On the move.
The Duke left The Hague on 9 May. "God knows I go with a heavy heart," he wrote six days later to his friend and political ally in England, Lord Godolphin, "for I have no hope of doing anything considerable, unless the French do what I am very confident they will not … " – in other words, court battle. On 17 May the Duke concentrated his Dutch and English troops at Tongeren, near Maastricht. The Hanoverians, Hessians and Danes, despite earlier undertakings, found, or invented, pressing reasons for withholding their support. Marlborough wrote an appeal to the Duke of Württemberg, the commander of the Danish contingent – "I send you this express to request your Highness to bring forward by a double march your cavalry so as to join us at the earliest moment …" Additionally, the King "in" Prussia, Frederick I, had kept his troops in quarters behind the Rhine while his personal disputes with Vienna and the States General at The Hague remained unresolved. Nevertheless, the Duke could think of no circumstances why the French would leave their strong positions and attack his army, even if Villeroi was first reinforced by substantial transfers from Marsin’s command. But in this he had miscalculated. Although Louis XIV wanted peace he wanted it on reasonable terms; for that, he needed victory in the field and to convince the Allies that his resources were by no means exhausted.
Following the successes in Italy and along the Rhine, Louis XIV was now hopeful of similar results in Flanders. Far from standing on the defensive therefore – and unbeknown to Marlborough – Louis XIV was persistently goading his marshal into action. "[Villeroi] began to imagine," wrote St Simon, "that the King doubted his courage, and resolved to stake all at once in an effort to vindicate himself." Accordingly, on 18 May, Villeroi set off from Leuven at the head of 70 battalions, 132 squadrons and 62 cannon – comprising an overall force of some 60,000 troops – and crossed the river Dyle to seek battle with the enemy. Spurred on by his growing confidence in his ability to out-general his opponent, and by Versailles’ determination to avenge Blenheim, Villeroi and his generals anticipated success.
Neither opponent expected the clash at the exact moment or place where it occurred. The French moved first to Tienen, (as if to threaten Zoutleeuw, abandoned by the French in October 1705), before turning southwards, heading for Jodoigne – this line of march took Villeroi’s army towards the narrow aperture of dry ground between the Mehaigne and Petite Gheete rivers close to the small villages of Ramillies and Taviers; but neither commander quite appreciated how far his opponent had travelled. Villeroi still believed (on 22 May) the Allies were a full day’s march away when in fact they had camped near Corswaren waiting for the Danish squadrons to catch up; for his part, Marlborough deemed Villeroi still at Jodoigne when in reality he was now approaching the plateau of Mont St. André with the intention of pitching camp near Ramillies (see map at right). However, the Prussian infantry was not there. Marlborough wrote to Lord Raby, the English resident at Berlin: "If it should please God to give us victory over the enemy, the Allies will be little obliged to the King [Frederick] for the success."
The following day, at 01:00, Marlborough dispatched Cadogan, his Quartermaster-General, with an advanced guard to reconnoitre the same dry ground that Villeroi’s army was now heading, country that was well known to the Duke from previous campaigns. Two hours later the Duke followed with the main body: 74 battalions, 123 squadrons, 90 pieces of artillery and 20 mortars, totalling 62,000 troops. At about 08:00, after Cadogan had just passed Merdorp, his force made brief contact with a party of French hussars gathering forage on the edge of the plateau of Jandrenouille. After a brief exchange of shots the French retired and Cadogan's dragoons pressed forward. With a short lift in the mist, Cadogan soon discovered the smartly ordered lines of Villeroi’s advance guard some four miles (6 km) off; a galloper hastened back to warn Marlborough. Two hours later the Duke, accompanied by the Dutch field commander Field Marshal Overkirk, General Daniel Dopff, and the Allied staff, rode up to Cadogan where on the horizon to the westward he could discern the massed ranks of the French army deploying for battle along the four mile (6.4 km) front. Marlborough later told Bishop Burnet that, ‘the French army looked the best of any he had ever seen’.
Battle.
Battlefield.
The battlefield of Ramillies is very similar to that of Blenheim, for here too there is an immense area of arable land unimpeded by woods or hedges. Villeroi’s right rested on the villages of Franquenée and Taviers, with the river Mehaigne protecting his flank. A large open plain, just over 1 mile (~2 km) wide, lay between Taviers and Ramillies, but unlike Blenheim, there was no stream to hinder the cavalry. His centre was secured by Ramillies itself, lying on a slight eminence which gave distant views to the north and east. The French left flank was protected by broken country, and by a stream, the Petite Gheete, which runs deep between steep and slippery slopes. On the French side of the stream the ground rises to Offus, the village which, together with Autre-Eglise farther north, anchored Villeroi’s left flank. To the west of the Petite Gheete rises the plateau of Mont St. André; a second plain, the plateau of Jandrenouille – upon which the Anglo-Dutch army amassed – rises to the east.
Initial dispositions.
At 11:00, the Duke ordered the army to take standard battle formation. On the far right, towards Foulz, the British battalions and squadrons took up their posts in a double line near the Jeuche stream. The centre was formed by the mass of Dutch, German, Protestant Swiss and Scottish infantry – perhaps 30,000 men – facing Offus and Ramillies. Also facing Ramillies Marlborough placed a powerful battery of thirty 24-pounders, dragged into position by a team of oxen; further batteries were positioned overlooking the Petite Gheete. On their left, on the broad plain between Taviers and Ramillies – and where Marlborough thought the decisive encounter must take place – Overkirk drew the 69 squadrons of the Dutch and Danish horse, supported by 19 battalions of Dutch infantry and two artillery pieces. 
Meanwhile Villeroi deployed his forces. In Taviers on his right, he placed two battalions of the Greder Suisse Régiment, with a smaller force forward in Franquenée; the whole position was protected by the boggy ground of the Mehaigne river, thus preventing an Allied flanking movement. In the open country between Taviers and Ramillies, he placed 82 squadrons under General de Guiscard supported by several interleaved brigades of French, Swiss and Bavarian infantry. Along the Ramillies–Offus–Autre Eglise ridge-line, Villeroi positioned Walloon and Bavarian infantry, supported by the Elector of Bavaria's 50 squadrons of Bavarian and Walloon cavalry placed behind on the plateau of Mont St. André. Ramillies, Offus and Autre-Eglise were all packed with troops and put in a state of defence, with alleys barricaded and walls loop-holed for muskets. Villeroi also positioned powerful batteries near Ramillies. These guns (some of which were of the three barrelled kind first seen at Elixheim the previous year) enjoyed good arcs of fire, able to fully cover the approaches of the plateau of Jandrenouille over which the Allied infantry would have to pass.
Marlborough, however, noticed several important weaknesses in the French dispositions. Tactically, it was imperative for Villeroi to occupy Taviers on his right and Autre-Eglise on his left, but by adopting this posture he had been forced to over-extend his forces. Moreover, this disposition – concave in relation to the Allied army – gave Marlborough the opportunity to form a more compact line, drawn up in a shorter front between the ‘horns’ of the French crescent; when the Allied blow came it would be more concentrated and carry more weight. Additionally, the Duke’s disposition facilitated the transfer of troops across his front far more easily than his foe, a tactical advantage that would grow in importance as the events of the afternoon unfolded. Although Villeroi had the option of enveloping the flanks of the Allied army as they deployed on the plateau of Jandrenouille – threatening to encircle their army – the Duke correctly gauged that the characteristically cautious French commander was intent on a defensive battle along the ridge-line.
Taviers.
At 13:00 the batteries went into action; a little later two Allied columns set out from the extremities of their line and attacked the flanks of the Franco-Bavarian army. To the south the Dutch Guards, under the command of Colonel Wertmüller, came forward with their two field guns to seize the hamlet of Franquenée. The small Swiss garrison in the village, shaken by the sudden onslaught and unsupported by the battalions to their rear, were soon compelled back towards the village of Taviers. Taviers was of particular importance to the Franco-Bavarian position: it protected the otherwise unsupported flank of General de Guiscard’s cavalry on the open plain, while at the same time, it allowed the French infantry to pose a threat to the flanks of the Dutch and Danish squadrons as they came forward into position. But hardly had the retreating Swiss rejoined their comrades in that village when the Dutch Guards renewed their attack. The fighting amongst the alleys and cottages soon deteriorated into a fierce bayonet and clubbing "mêlée", but the superiority in Dutch firepower soon told. The accomplished French officer, Colonel de la Colonie, standing on the plain nearby remembered – "this village was the opening of the engagement, and the fighting there was almost as murderous as the rest of the battle put together." By about 15:00 the Swiss had been pushed out of the village into the marshes beyond.
Villeroi’s right flank fell into chaos and was now open and vulnerable. Alerted to the situation de Guiscard ordered an immediate attack with 14 squadrons of French dragoons currently stationed in the rear. Two other battalions of the Greder Suisse Régiment were also sent, but the attack was poorly co-ordinated and consequently went in piecemeal. The Anglo-Dutch commanders now sent dismounted Dutch dragoons into Taviers, which, together with the Guards and their field guns, poured concentrated musketry- and canister-fire into the advancing French troops. Colonel d’Aubigni, leading his regiment, fell mortally wounded.
As the French ranks wavered, the leading squadrons of Württemberg’s Danish horse – now unhampered by enemy fire from either village – were also sent into the attack and fell upon the exposed flank of the Franco-Swiss infantry and dragoons. De la Colonie, with his Grenadiers Rouge regiment, together with the Cologne Guards who were brigaded with them, was now ordered forward from his post south of Ramillies to support the faltering counter-attack on the village. But on his arrival, all was chaos – "Scarcely had my troops got over when the dragoons and Swiss who had preceded us, came tumbling down upon my battalions in full flight … My own fellows turned about and fled along with them." De La Colonie managed to rally some of his grenadiers, together with the remnants of the French dragoons and Greder Suisse battalions, but it was an entirely peripheral operation, offering only fragile support for Villeroi’s right flank.
Offus and Autre-Eglise.
While the attack on Taviers went in the Earl of Orkney launched his first line of English across the Petite Gheete in a determined attack against the barricaded villages of Offus and Autre-Eglise on the Allied right. Villeroi, posting himself near Offus, watched anxiously the redcoats' advance, mindful of the counsel he had received on 6 May from Louis XIV – "Have particular care to that part of the line which will endure the first shock of the English troops." Heeding this advice the French commander began to transfer battalions from his centre to reinforce the left, drawing more foot from the already weakened right to replace them.
As the English battalions descended the gentle slope of the Petite Gheete valley, struggling through the boggy stream, they were met by Major General de la Guiche’s disciplined Walloon infantry sent forward from around Offus. After concentrated volleys, exacting heavy casualties on the redcoats, the Walloons reformed back to the ridgeline in good order. The English took some time to reform their ranks on the dry ground beyond the stream and press on up the slope towards the cottages and barricades on the ridge. The vigour of the English assault, however, was such that they threatened to break through the line of the villages and out onto the open plateau of Mont St André beyond. This was potentially dangerous for the Allied infantry who would then be at the mercy of the Elector’s Bavarian and Walloon squadrons patiently waiting on the plateau for the order to move.
Although Henry Lumley’s British cavalry had managed to cross the marshy ground around the Petite Gheete, it was soon evident to Marlborough that sufficient cavalry support would not be practicable and that the battle could not be won on the Allied right. The Duke, therefore, called off the attack against Offus and Autre-Eglise. To make sure that Orkney obeyed his order to withdraw, Marlborough sent his Quartermaster-General in person with the command. Despite Orkney’s protestations, Cadogan insisted on compliance and, reluctantly, Orkney gave the word for his troops to fall back to their original positions on the edge of the plateau of Jandrenouille. It is still not clear how far Orkney’s advance was planned only as a feint; according to historian David Chandler it is probably more accurate to surmise that Marlborough launched Orkney in a serious probe with a view to sounding out the possibilities of the sector. Nevertheless, the attack had served its purpose. Villeroi had given his personal attention to that wing and strengthened it with large bodies of horse and foot that ought to have been taking part in the decisive struggle south of Ramillies.
Ramillies.
Meanwhile, the Dutch assault on Ramillies was gaining pace. Marlborough’s younger brother, General of Infantry, Charles Churchill, ordered four brigades of foot to attack the village. The assault consisted of 12 battalions of Dutch infantry commanded by Major Generals Schultz and Spaar; two brigades of Saxons under Count Schulenburg; a Scottish brigade in Dutch service led by the 2nd Duke of Argyle; and a small brigade of Protestant Swiss. The 20 French and Bavarian battalions in Ramillies, supported by the Irish dragoons who had left Ireland in the Flight of the Wild Geese to join Clare's Dragoons and a small brigade of Cologne and Bavarian Guards under the Marquis de Maffei, put up a determined defence, initially driving back the attackers with severe losses as commemorated in the song "Clare's Dragoons":
When on Ramillies' bloody field <br>
The baffled French were forced to yield,<br>
The victor Saxon backward reeled <br>
Before the charge of Clare's Dragoons. <br>
"Viva là, the new brigade!"<br>
"Viva là the old one too!"<br>
"Viva là, the Rose shall fade"<br>
"The Shamrock shine forever new!"<br>
Seeing that Schultz and Spaar were faltering, Marlborough now ordered Orkney’s second-line British and Danish battalions (who had not been used in the assault on Offus and Autre-Eglise) to move south towards Ramillies. Shielded as they were from observation by a slight fold in the land, their commander, Brigadier-General Van Pallandt, ordered the regimental colours to be left in place on the edge of the plateau to convince their opponents they were still in their initial position. Therefore, unbeknown to the French who remained oblivious to the Allies’ real strength and intentions on the opposite side of the Petite Gheete, Marlborough was throwing his full weight against Ramillies and the open plain to the south. Villeroi meanwhile, was still moving more reserves of infantry in the opposite direction towards his left flank; crucially, it would be some time before the French commander noticed the subtle change in emphasis of the Allied dispositions.
At around 15:30, Overkirk advanced his massed squadrons on the open plain in support of the infantry attack on Ramillies. Overkirk's squadrons – 48 Dutch, supported on their left by 21 Danish – steadily advanced towards the enemy (taking care not to prematurely tire the horses), before breaking into a trot to gain the impetus for their charge. The Marquis de Feuquières writing after the battle described the scene – "They advanced in four lines … As they approached they advanced their second and fourth lines into the intervals of their first and third lines; so that when they made their advance upon us, they formed only one front, without any intermediate spaces."
The initial clash favoured the Dutch and Danish squadrons. The disparity of numbers – exacerbated by Villeroi stripping their ranks of infantry to reinforce his left flank – enabled Overkirk's cavalry to throw the first line of French horse back in some disorder towards their second-line squadrons. This line also came under severe pressure and, in turn, was forced back to their third-line of cavalry and the few battalions still remaining on the plain. But these French horsemen were amongst the best in Louis XIV’s army – the "Maison du Roi", supported by four elite squadrons of Bavarian Cuirassiers. Ably led by de Guiscard, the French cavalry rallied, thrusting back the Allied squadrons in successful local counterattacks. On Overkirk’s right flank, close to Ramillies, ten of his squadrons suddenly broke ranks and were scattered, riding headlong to the rear to recover their order, leaving the left flank of the Allied assault on Ramillies dangerously exposed. Notwithstanding the lack of infantry support, de Guiscard threw his cavalry forward in an attempt to split the Allied army in two. 
A crisis threatened the centre, but from his vantage point Marlborough was at once aware of the situation. The Allied commander now summoned the cavalry on the right wing to reinforce his centre, leaving only the English squadrons in support of Orkney. Thanks to a combination of battle-smoke and favourable terrain, his redeployment went unnoticed by Villeroi who made no attempt to transfer any of his own 50 unused squadrons. While he waited for the fresh reinforcements to arrive, Marlborough flung himself into the "mêlée", rallying some of the Dutch cavalry who were in confusion. But his personal involvement nearly led to his undoing. A number of French horsemen, recognising the Duke, came surging towards his party. Marlborough’s horse tumbled and the Duke was thrown – "Milord Marlborough was rid over," wrote Orkney some time later. It was a critical moment of the battle. "Major-General Murray," recalled one eye witness, " … seeing him fall, marched up in all haste with two Swiss battalions to save him and stop the enemy who were hewing all down in their way." Fortunately Marlborough’s newly appointed aide-de-camp, Richard Molesworth, galloped to the rescue, mounted the Duke on his horse and made good their escape, before Murray’s disciplined ranks threw back the pursuing French troopers. 
After a brief pause, Marlborough’s equerry, Colonel Bringfield (or Bingfield), led up another of the Duke’s spare horses; but while assisting him onto his mount, the unfortunate Bringfield was hit by an errant cannonball that sheared off his head. One account has it that the cannonball flew between the Captain-General’s legs before hitting the unfortunate colonel, whose torso fell at Marlborough’s feet – a moment subsequently depicted in a lurid set of contemporary playing cards. Nevertheless the danger passed, enabling the Duke to attend to the positioning of the cavalry reinforcements feeding down from his right flank – a change of which Villeroi remained blissfully unaware.
Breakthrough.
The time was about 16:30, and the two armies were in close contact across the whole four-mile (6 km) front, from the skirmishing in the marshes in the south, through the vast cavalry battle on the open plain; to the fierce struggle for Ramillies at the centre, and to the north, where, around the cottages of Offus and Autre-Eglise, Orkney and de la Guiche faced each other across the Petite Gheete ready to renew hostilities.
The arrival of the transferring squadrons now began to tip the balance in favour of the Allies. Tired, and suffering a growing list of casualties, the numerical inferiority of Guiscard’s squadrons battling on the plain at last began to tell. After earlier failing to hold or retake Franquenée and Taviers, Guiscard’s right flank had become dangerously exposed and a fatal gap had opened on the right of their line. Taking advantage of this breach, Württemberg’s Danish cavalry now swept forward, wheeling to penetrate the flank of the Maison du Roi whose attention was almost entirely fixed on holding back the Dutch. Sweeping forwards, virtually without resistance, the 21 Danish squadrons reformed behind the French around the area of the Tomb of Ottomond, facing north across the plateau of Mont St André towards the exposed flank of Villeroi’s army.
The final Allied reinforcements for the cavalry contest to the south were at last in position; Marlborough’s superiority on the left could no longer be denied, and his fast-moving plan took hold of the battlefield. Now, far too late, Villeroi tried to redeploy his 50 unused squadrons, but a desperate attempt to form line facing south, stretching from Offus to Mont St André, floundered amongst the baggage and tents of the French camp carelessly left there after the initial deployment. The Allied commander ordered his cavalry forward against the now heavily outnumbered French and Bavarian horsemen. De Guiscard’s right flank, without proper infantry support, could no longer resist the onslaught and, turning their horses northwards, they broke and fled in complete disorder. Even the squadrons currently being scrambled together by Villeroi behind Ramillies could not withstand the onslaught. "We had not got forty yards on our retreat," remembered Captain Peter Drake, the Irish mercenary serving with the French – "when the words "sauve qui peut" went through the great part, if not the whole army, and put all to confusion"
In Ramillies the Allied infantry, now reinforced by the English troops brought down from the north, at last broke through. The Régiment de Picardie stood their ground but were caught between Colonel Borthwick’s Scots-Dutch regiment and the English reinforcements. Borthwick was killed, as was Charles O’Brien, the Irish Viscount Clare in French service, fighting at the head of his regiment. The Marquis de Maffei attempted one last stand with his Bavarian and Cologne Guards, but it proved in vain. Noticing a rush of horsemen fast approaching from the south, he later recalled – " … I went towards the nearest of these squadrons to instruct their officer, but instead of being listened to [I] was immediately surrounded and called upon to ask for quarter."
Pursuit.
The roads leading north and west were choked with fugitives. Orkney now sent his English troops back across the Petite Gheete stream to once again storm Offus where de la Guiche’s infantry had begun to drift away in the confusion. To the right of the infantry Lord John Hay’s ‘Scots Greys’ also picked their way across the stream and charged the Régiment du Roi within Autre-Eglise. "Our dragoons," wrote John Deane, "pushing into the village … made terrible slaughter of the enemy." The Bavarian Horse Grenadiers and the Electoral Guards withdrew and formed a shield about Villeroi and the Elector but were scattered by Lumley’s cavalry. Stuck in the mass of fugitives fleeing the battlefield, the French and Bavarian commanders narrowly escaped capture by General Cornelius Wood who, unaware of their identity, had to content himself with the seizure of two Bavarian Lieutenant-Generals. Far to the south, the remnants of de la Colonie’s brigade headed in the opposite direction towards the French held fortress of Namur."
The retreat became a rout. Individual Allied commanders drove their troops forward in pursuit, allowing their beaten enemy no chance to recover. Soon the Allied infantry could no longer keep up, but their cavalry were off the leash, heading through the gathering night for the crossings on the Dyle river. At last, however, Marlborough called a halt to the pursuit shortly after midnight near Meldert, from the field. "It was indeed a truly shocking sight to see the miserable remains of this mighty army," wrote Captain Drake, "… reduced to a handful."
Aftermath.
What was left of Villeroi’s army was now broken in spirit; the imbalance of the casualty figures amply demonstrates the extent of the disaster for Louis XIV’s army: ("see below"). In addition, hundreds of French soldiers were fugitives, many of whom would never remuster to the colours. Villeroi also lost 52 artillery pieces and his entire engineer pontoon train. In the words of Marshal Villars, the French defeat at Ramillies was – "The most shameful, humiliating and disastrous of routs." 
Town after town now succumbed to the Allies. Leuven fell on 25 May 1706; three days later, the Allies entered Brussels, the capital of the Spanish Netherlands. Marlborough realised the great opportunity created by the early victory of Ramillies: "We now have the whole summer before us," wrote the Duke from Brussels to Robert Harley, "and with the blessing of God I shall make the best use of it." Malines, Lierre, Ghent, Alost, Damme, Oudenaarde, Bruges, and on 6 June Antwerp, all subsequently fell to Marlborough’s victorious army and, like Brussels, proclaimed the Austrian candidate for the Spanish throne, the Archduke Charles, as their sovereign. Villeroi was helpless to arrest the process of collapse. When Louis XIV learnt of the disaster he recalled Marshal Vendôme from northern Italy to take command in Flanders; but it would be weeks before the command changed hands.
As news spread of the Allies’ triumph, the Prussians, Hessians and Hanoverian contingents, long delayed by their respective rulers, eagerly joined the pursuit of the broken French and Bavarian forces. "This," wrote Marlborough wearily, "I take to be owing to our late success." Meanwhile, Overkirk took the port of Ostend on 4 July thus opening a direct route to the English Channel for communication and supply, but the Allies were making scant progress against Dendermonde whose governor, the Marquis de Valée, was stubbornly resisting. Only later when Cadogan and Churchill went to take charge did the town’s defences begin to fail. 
Vendôme formally took over command in Flanders on 4 August; Villeroi would never again receive a major command – "I cannot foresee a happy day in my life save only that of my death." Louis XIV was more forgiving to his old friend – "At our age, Marshal, we must no longer expect good fortune." In the mean time, Marlborough invested the elaborate fortress of Menin which, after a costly siege, capitulated on 22 August. Dendermonde finally succumbed on 6 September followed by Ath – the last conquest of 1706 – on 2 October. By the time Marlborough had closed down the Ramillies campaign he had denied the French most of the Spanish Netherlands west of the Meuse and north of the Sambre – it was an unsurpassed operational triumph for the English Duke but once again it was not decisive as these gains did not defeat France. 
The immediate question for the Allies now was how to deal with the Spanish Netherlands, a subject which the Austrians and the Dutch were diametrically opposed. Emperor Joseph I, acting on behalf of his younger brother King ’Charles III’, absent in Spain, claimed that reconquered Brabant and Flanders should be put under immediate possession of a governor named by himself. The Dutch, however, who had supplied the major share of the troops and money to secure the victory (the Austrians had produced nothing of either) claimed the government of the region till the war was over, and that after the peace they should continue to garrison Barrier Fortresses stronger than those which had fallen so easily to Louis XIV’s forces in 1701. Marlborough mediated between the two parties but favoured the Dutch position. To sway the Duke’s opinion, the Emperor offered Marlborough the governorship of the Spanish Netherlands. It was a tempting offer, but in the name of Allied unity, it was one he refused. In the end England and the Dutch Republic took control of the newly won territory for the duration of the war; after which it was to be handed over to the direct rule of ‘Charles III’, subject to the reservation of a Dutch Barrier, the extent and nature of which had yet to be settled.
Meanwhile, on the Upper Rhine, Villars had been forced onto the defensive as battalion after battalion had been sent north to bolster collapsing French forces in Flanders; there was now no possibility of his undertaking the re-capture of Landau. Further good news for the Allies arrived from northern Italy where, on 7 September, Prince Eugene had routed a French army before the Piedmontese capital, Turin, driving the Franco-Spanish forces from northern Italy. Only from Spain did Louis XIV receive any good news where Das Minas and Galway had been forced to retreat from Madrid towards Valencia, allowing Philip V to re-enter his capital on 4 October. All in all though, the situation had changed considerably and Louis XIV began to look for ways to end what was fast becoming a ruinous war for France. For Queen Anne also, the Ramillies campaign had one overriding significance – "Now we have God be thanked so hopeful a prospect of peace." Instead of continuing the momentum of victory, however, cracks in Allied unity would enable Louis XIV to reverse some of the major setbacks suffered at Turin and Ramillies.
Casualties.
The total number of French casualties cannot be calculated precisely, so complete was the collapse of the Franco-Bavarian army that day. David G. Chandler’s "Marlborough as Military Commander" and "A Guide to the Battlefields of Europe" are consistent with regards to French casualty figures i.e., 12,000 dead and wounded plus some 7,000 taken prisoner. James Falkner, in "Ramillies 1706: Year of Miracles," also notes 12,000 dead and wounded and states ‘up to 10,000’ taken prisoner. In "The Collins Encyclopaedia of Military History", Dupuy puts Villeroi’s dead and wounded at 8,000, with a further 7,000 captured. John Millner’s memoirs – "Compendious Journal" (1733) – is more specific, recording 12,087 of Villeroi’s army were killed or wounded, with another 9,729 taken prisoner. In "Marlborough", however, Correlli Barnett puts the total casualty figure as high as 30,000 – 15,000 dead and wounded with an additional 15,000 taken captive. Trevelyan estimates Villeroi’s casualties at 13,000, but adds, ‘his losses by desertion may have doubled that number’. La Colonie omits a casualty figure in his "Chronicles of an old Campaigner"; but Saint-Simon in his "Memoirs" states 4,000 killed, adding 'many others were wounded and many important persons were taken prisoner'. Voltaire, however, in "Histoire du siecle du Louis XIV" records, 'the French lost there twenty thousand men'.

</doc>
<doc id="4051" url="http://en.wikipedia.org/wiki?curid=4051" title="Brian Kernighan">
Brian Kernighan

Brian Wilson Kernighan (; born January 1, 1942) is a Canadian computer scientist who worked at Bell Labs alongside Unix creators Ken Thompson and Dennis Ritchie and contributed to the development of Unix. He is also coauthor of the AWK and AMPL programming languages. The "K" of K&R C and the "K" in AWK both stand for "Kernighan". Since 2000 Brian Kernighan has been a Professor at the Computer Science Department of Princeton University, where he is also the Undergraduate Department Representative.
Kernighan's name became widely known through co-authorship of the first book on the C programming language with Dennis Ritchie. Kernighan affirmed that he had no part in the design of the C language ("it's entirely Dennis Ritchie's work"). He authored many Unix programs, including ditroff, and cron for Version 7 Unix.
In collaboration with Shen Lin he devised well-known heuristics for two NP-complete optimization problems: graph partitioning and the travelling salesman problem. (In a display of authorial equity, the former is usually called the "Kernighan–Lin algorithm", while the latter is styled "Lin–Kernighan".)
Kernighan was the software editor for Prentice Hall International. His "Software Tools" series spread the essence of "C/Unix thinking" with makeovers for BASIC, FORTRAN, and Pascal, and most notably his "Ratfor" (rational FORTRAN) was put in the public domain.
He has said that if stranded on an island with only one programming language it would have to be C.
Kernighan coined the term Unix in the 1970s. The original term he coined was Unics (for Uniplexed Information and Computing Service, a play on Multics), which was later changed to Unix. Kernighan is also known as a coiner of the expression "What You See Is All You Get" (WYSIAYG), which is a sarcastic variant of the original "What You See Is What You Get" (WYSIWYG). Kernighan's term is used to indicate that WYSIWYG systems might throw away information in a document that could be useful in other contexts.
Early life and education.
Born in Toronto, Kernighan attended the University of Toronto between 1960 and 1964, earning his Bachelor's degree in engineering physics. He received his PhD in electrical engineering from Princeton University, where he has held a professorship in the department of computer science since 2000. Each fall he teaches a course called "Computers in Our World", which introduces the fundamentals of computing to non-majors.

</doc>
<doc id="4052" url="http://en.wikipedia.org/wiki?curid=4052" title="BCPL">
BCPL

BCPL (Basic Combined Programming Language) is a procedural, imperative, and structured computer programming language designed by Martin Richards of the University of Cambridge in 1966.
Design.
Originally intended for writing compilers for other languages, BCPL is no longer in common use. However, its influence is still felt because a stripped down and syntactically changed version of BCPL, called B, was the language on which the C programming language was based. This led many C programmers to give BCPL the humorous backronym Before C Programming Language.
BCPL was the first brace programming language, and the braces survived the syntactical changes and have become a common means of denoting program source code statements. In practice, on limited keyboards of the day, source programs often used the sequences $( and $) in place of the symbols { and }.
The single-line '//' comments of BCPL, which were not adopted by C, reappeared in C++, and later in C99.
BCPL was a response to difficulties with its predecessor Combined Programming Language (CPL), created during the early 1960s. Richards created BCPL by "removing those features of the full language which make compilation difficult". The first compiler implementation, for the IBM 7094 under Compatible Time-Sharing System (CTSS), was written while Richards was visiting Project MAC at the Massachusetts Institute of Technology (MIT) in the spring of 1967. The language was first described in a paper presented to the 1969 Spring Joint Computer Conference.
It was designed so that small and simple compilers could be written for it; reputedly some compilers could be run in 16 kilobytes. Further, the Richards compiler, itself written in BCPL, was easily portable. BCPL was thus a popular choice for bootstrapping a system.
A major reason for the compiler's portability lay in its structure. It was split into two parts: the front end parsed the source and generated O-code for a virtual machine, and the back end took the O-code and translated it into the code for the target machine. Only 1/5 of the compiler's code needed to be rewritten to support a new machine, a task that usually took between 2 and 5 man-months. This approach became common practice later, e.g., Pascal or Java, but the Richards BCPL compiler was the first to define a virtual machine for this purpose.
The language is unusual in having only one data type: a word, a fixed number of bits, usually chosen to align with the architecture's machine word and of adequate capacity to represent any valid storage address. For many machines of the time, this data type was a 16-bit word. This choice later proved to be a significant problem when BCPL was used on machines in which the smallest addressable item was not a word, but a byte or on machines with larger word sizes: 32-bit and 64-bit words, which allowed them to manage large address spaces.
The interpretation of any value was determined by the operators used to process the values. (For example, + added two values together treating them as integers; ! indirected through a value, effectively treating it as a pointer.) In order for this to work, the implementation provided no type checking. The Hungarian notation was developed to help programmers avoid inadvertent type errors.
The mismatch between BCPL's word orientation and byte-oriented hardware was addressed in several ways. One was providing standard library routines for packing and unpacking words into byte strings. Later, two language features were added: the bit-field selection operator and the infix byte indirection operator (denoted by the '%' character).
BCPL handles bindings spanning separate compilation units in a unique way. There are no user-declarable global variables; instead there is a global vector, which is similar to "blank common" in Fortran. All data shared between different compilation units comprises scalars and pointers to vectors stored in a pre-arranged place in the global vector. Thus the header files (files included during compilation using the "GET" directive) become the primary means of synchronizing global data between compilation units, containing "GLOBAL" directives that present lists of symbolic names, each paired with a number that associates the name with the corresponding numerically addressed word in the global vector. As well as variables, the global vector also contains bindings for external procedures. This makes dynamic loading of compilation units very simple to achieve. Instead of relying on the link loader of the underlying implementation, effectively BCPL gives the programmer control of the linking process.
The global vector also made it very simple to replace or augment standard library routines. A program could save the pointer from the global vector to the original routine and replace it with a pointer to an alternative version. The alternative might call the original as part of its processing. This could be used as a quick ad-hoc debugging aid.
The philosophy of BCPL can be summarised by quoting from the book "BCPL, the language and its compiler":
The design, and philosophy, of BCPL strongly influenced B, which in turn influenced C.
There are rumours that BCPL actually stood for "Bootstrap Cambridge Programming Language", however CPL was never created since development stopped at BCPL, and the acronym was reinterpreted for the BCPL book.
Uses and implementations.
BCPL is the language in which the original hello world program was written. The first MUD was also written in BCPL (MUD1).
Several operating systems were written partially or wholly in BCPL (for example, TRIPOS and the earliest versions of AmigaDOS, a part of AmigaOS). BCPL was also the initial language used in the seminal Xerox PARC Alto project, the first modern personal computer; among other projects, the Bravo document preparation system was written in BCPL.
An early compiler, bootstrapped in 1969 by starting with a paper tape of the O-code of Martin Richards's Atlas 2 compiler, targeted the ICT 1900 series. The two machines had different word-lengths (48 vs 24 bits), different character encodings, and different packed string representations—and the successful bootstrapping increased confidence in the practicality of the method.
By late 1970, implementations existed for the Honeywell 635 and Honeywell 645, the IBM 360, the PDP-10, the TX-2, the CDC 6400, the UNIVAC 1108, the PDP-9, the KDF 9 and the Atlas 2. In 1974 a dialect of BCPL was implemented at BBN without using the intermediate O-code. The initial implementation was a cross-compiler hosted on BBN's Tenex PDP-10s, and directly targeted the PDP-11s used in BBN's implementation of the second generation IMPs used in the Arpanet.
There was also a version produced for the BBC Micro in the mid-1980s by Richards Computer Products, a company started by John Richards, the brother of Dr. Martin Richards. The BBC Domesday Project made use of the language. Versions of BCPL for the Amstrad CPC and Amstrad PCW computers were also released in 1986 by UK software house Arnor Ltd. MacBCPL was released for the Apple Macintosh in 1985 by Topexpress Ltd, of Kensington, England.
In 1979 implementations of BCPL existed for at least 25 architectures; by 2001 it saw little use, though.
Examples.
These complete and compilable examples are from Martin Richards′ BCPL distribution.
Print factorials:
Count solutions to the N queens problem:

</doc>
<doc id="4054" url="http://en.wikipedia.org/wiki?curid=4054" title="Battleship">
Battleship

A battleship is a large armored warship with a main battery consisting of heavy caliber guns. During the late 19th and early 20th centuries the battleship was the most powerful type of warship, and a fleet of battleships was vital for any nation which desired to maintain command of the sea. During World War II, aircraft carriers overtook battleships in power projection. Some battleships remained in service during the Cold War and the last were decommissioned in the 1990s.
The word "battleship" was coined around 1794 and is a contraction of the phrase "line-of-battle ship," the dominant wooden warship during the Age of Sail. The term came into formal use in the late 1880s to describe a type of ironclad warship, now referred to by historians as pre-dreadnought battleships. In 1906, the commissioning of heralded a revolution in battleship design. Following battleship designs, influenced by HMS "Dreadnought", were referred to as "dreadnoughts".
Battleships were a symbol of naval dominance and national might, and for decades the battleship was a major factor in both diplomacy and military strategy. The global arms race in battleship construction began in Europe, following the 1890 publication of Alfred Thayer Mahan's "The Influence of Sea Power upon History, 1660–1783". This arms race culminated at the decisive Battle of Tsushima in 1905; the outcome of which significantly influenced the design of HMS "Dreadnought". The launch of "Dreadnought" in 1906 commenced a new naval arms race which was widely considered to have been an indirect cause of World War I. The Naval Treaties of the 1920s and 1930s limited the number of battleships, though technical innovation in battleship design continued. Both the Allies and the Axis powers deployed battleships during World War II.
The value of the battleship has been questioned, even during the period of their prominence. In spite of the immense resources spent on battleships, there were few pitched battleship clashes. Even with their enormous firepower and protection, battleships were increasingly vulnerable to much smaller, cheaper weapons: initially the torpedo and the naval mine, and later aircraft and the guided missile. The growing range of naval engagements led to the aircraft carrier replacing the battleship as the leading capital ship during World War II, with the last battleship to be launched being in 1944. Battleships were retained by the United States Navy into the Cold War for fire support purposes before being stricken from the U.S. Naval Vessel Register in the 2000s.
Ships of the line.
A ship of the line was a large, unarmored wooden sailing ship on which was mounted a battery of up to 120 smoothbore guns and carronades. It was a gradual evolution of a basic design that dates back to the 15th century, and, apart from growing in size, it changed little between the adoption of line of battle tactics in the early 17th century and the end of the sailing battleship's heyday in the 1830s. From 1794, the alternative term 'line of battle ship' was contracted (informally at first) to 'battle ship' or 'battleship'.
The sheer number of guns fired broadside meant a sail battleship could wreck any wooden enemy, holing her hull, knocking down masts, wrecking her rigging, and killing her crew. However, the effective range of the guns was as little as a few hundred yards, so the battle tactics of sailing ships depended in part on the wind.
The first major change to the ship of the line concept was the introduction of steam power as an auxiliary propulsion system. Steam power was gradually introduced to the navy in the first half of the 19th century, initially for small craft and later for frigates. The French Navy introduced steam to the line of battle with the 90-gun in 1850—the first true steam battleship. "Napoléon" was armed as a conventional ship-of-the-line, but her steam engines could give her a speed of , regardless of the wind conditions: a potentially decisive advantage in a naval engagement. The introduction of steam accelerated the growth in size of battleships. France and the United Kingdom were the only countries to develop fleets of wooden steam screw battleships, although several other navies operated small numbers of screw battleships, including Russia (9), Turkey (3), Sweden (2), Naples (1), Denmark (1) and Austria (1).
Ironclads.
The adoption of steam power was only one of a number of technological advances which revolutionized warship design in the 19th century. The ship of the line was overtaken by the ironclad: powered by steam, protected by metal armor, and armed with guns firing high-explosive shells.
Explosive shells.
Guns which fired explosive or incendiary shells were a major threat to wooden ships, and these weapons quickly became widespread after the introduction of 8 inch shell guns as part of the standard armament of French and American line-of-battle ships in 1841. In the Crimean War, six line-of-battle ships and two frigates of the Russian Black Sea Fleet destroyed seven Turkish frigates and three corvettes with explosive shells at the Battle of Sinop in 1853. Later in the war, French ironclad floating batteries used similar weapons against the defenses at the Battle of Kinburn.
Nevertheless wooden-hulled ships stood up comparatively well to shells, as shown in the 1866 Battle of Lissa, where the modern Austrian steam two-decker ranged across a confused battlefield, rammed an Italian ironclad and took 80 hits from Italian ironclads, many of which were shells, but including at least one 300 pound shot at point blank range. Despite losing her bowsprit and her foremast, and being set on fire, she was ready for action again the very next day.
Iron armor and construction.
The development of high-explosive shells made the use of iron armor plate on warships necessary. In 1859 France launched , the first ocean-going ironclad warship. She had the profile of a ship of the line, cut to one deck due to weight considerations. Although made of wood and reliant on sail for most journeys, "Gloire" was fitted with a propeller, and her wooden hull was protected by a layer of thick iron armor. "Gloire" prompted further innovation from the Royal Navy, anxious to prevent France from gaining a technological lead.
The superior armored frigate followed "Gloire" by only 14 months, and both nations embarked on a program of building new ironclads and converting existing screw ships of the line to armored frigates. Within two years, Italy, Austria, Spain and Russia had all ordered ironclad warships, and by the time of the famous clash of the and the at the Battle of Hampton Roads at least eight navies possessed ironclad ships.
Navies experimented with the positioning of guns, in turrets (like the USS "Monitor"), central-batteries or barbettes, or with the ram as the principal weapon. As steam technology developed, masts were gradually removed from battleship designs. By the mid-1870s steel was used as a construction material alongside iron and wood. The French Navy's , laid down in 1873 and launched in 1876, was a central battery and barbette warship which became the first battleship in the world to use steel as the principal building material.
Pre-dreadnought battleship.
The term "battleship" was officially adopted by the Royal Navy in the re-classification of 1892. By the 1890s, there was an increasing similarity between battleship designs, and the type that later became known as the 'pre-dreadnought battleship' emerged. These were heavily armored ships, mounting a mixed battery of guns in turrets, and without sails. The typical first-class battleship of the pre-dreadnought era displaced 15,000 to 17,000 tons, had a speed of , and an armament of four guns in two turrets fore and aft with a mixed-caliber secondary battery amidships around the superstructure. An early design with superficial similarity to the pre-dreadnought is the British of 1871.
The slow-firing main guns were the principal weapons for battleship-to-battleship combat. The intermediate and secondary batteries had two roles. Against major ships, it was thought a 'hail of fire' from quick-firing secondary weapons could distract enemy gun crews by inflicting damage to the superstructure, and they would be more effective against smaller ships such as cruisers. Smaller guns (12-pounders and smaller) were reserved for protecting the battleship against the threat of torpedo attack from destroyers and torpedo boats.
The beginning of the pre-dreadnought era coincided with Britain reasserting her naval dominance. For many years previously, Britain had taken naval supremacy for granted. Expensive naval projects were criticised by political leaders of all inclinations. However, in 1888 a war scare with France and the build-up of the Russian navy gave added impetus to naval construction, and the British Naval Defence Act of 1889 laid down a new fleet including eight new battleships. The principle that Britain's navy should be more powerful than the two next most powerful fleets combined was established. This policy was designed to deter France and Russia from building more battleships, but both nations nevertheless expanded their fleets with more and better pre-dreadnoughts in the 1890s.
In the last years of the 19th century and the first years of the 20th, the escalation in the building of battleships became an arms race between Britain and Germany. The German naval laws of 1890 and 1898 authorised a fleet of 38 battleships, a vital threat to the balance of naval power. Britain answered with further shipbuilding, but by the end of the pre-dreadnought era, British supremacy at sea had markedly weakened. In 1883, the United Kingdom had 38 battleships, twice as many as France and almost as many as the rest of the world put together. By 1897, Britain's lead was far smaller due to competition from France, Germany, and Russia, as well as the development of pre-dreadnought fleets in Italy, the United States and Japan. Turkey, Spain, Sweden, Denmark, Norway, the Netherlands, Chile and Brazil all had second-rate fleets led by armored cruisers, coastal defence ships or monitors.
Pre-dreadnoughts continued the technical innovations of the ironclad. Turrets, armor plate, and steam engines were all improved over the years, and torpedo tubes were introduced. A small number of designs, including the American and es, experimented with all or part of the 8-inch intermediate battery superimposed over the 12-inch primary. Results were poor: recoil factors and blast effects resulted in the 8-inch battery being completely unusable, and the inability to train the primary and intermediate armaments on different targets led to significant tactical limitations. Even though such innovative designs saved weight (a key reason for their inception), they proved too cumbersome in practice.
Dreadnought era.
In 1906, the British Royal Navy launched the revolutionary . Created as a result of pressure from Admiral Sir John ("Jackie") Fisher, HMS "Dreadnought" made existing battleships obsolete. Combining an "all-big-gun" armament of ten 12-inch (305 mm) guns with unprecedented speed (from steam turbine engines) and protection, she prompted navies worldwide to re-evaluate their battleship building programmes. While the Japanese had laid down an all-big-gun battleship, in 1904, and the concept of an all-big-gun ship had been in circulation for several years, it had yet to be validated in combat. "Dreadnought" sparked a new arms race, principally between Britain and Germany but reflected worldwide, as the new class of warships became a crucial element of national power.
Technical development continued rapidly through the dreadnought era, with steep changes in armament, armor and propulsion. Ten years after "Dreadnought"s commissioning, much more powerful ships, the super-dreadnoughts, were being built.
Origin.
In the first years of the 20th century, several navies worldwide experimented with the idea of a new type of battleship with a uniform armament of very heavy guns.
Admiral Vittorio Cuniberti, the Italian Navy's chief naval architect, articulated the concept of an all-big-gun battleship in 1903. When the "Regia Marina" did not pursue his ideas, Cuniberti wrote an article in "Janes" proposing an "ideal" future British battleship, a large armored warship of 17,000 tons, armed solely with a single calibre main battery (twelve 12-inch {305 mm} guns), carrying belt armor, and capable of 24 knots (44 km/h).
The Russo-Japanese War provided operational experience to validate the 'all-big-gun' concept. At the Yellow Sea and Tsushima, pre-dreadnoughts exchanged volleys at ranges of 7,600–12,000 yd (7 to 11 km), beyond the range of the secondary batteries. It is often held that these engagements demonstrated the importance of the gun over its smaller counterparts, though some historians take the view that secondary batteries were just as important as the larger weapons.
In Japan, the two battleships of the 1903-4 Programme were the first to be laid down as all-big-gun designs, with eight 12-inch guns. However, the design had armor which was considered too thin, demanding a substantial redesign. The financial pressures of the Russo-Japanese War and the short supply of 12-inch guns which had to be imported from Britain meant these ships were completed with a mixed 10- and 12-inch armament. The 1903-4 design also retained traditional triple-expansion steam engines.
As early as 1904, Jackie Fisher had been convinced of the need for fast, powerful ships with an all-big-gun armament. If Tsushima influenced his thinking, it was to persuade him of the need to standardise on guns. Fisher's concerns were submarines and destroyers equipped with torpedoes, then threatening to outrange battleship guns, making speed imperative for capital ships. Fisher's preferred option was his brainchild, the battlecruiser: lightly armored but heavily armed with eight 12-inch guns and propelled to by steam turbines.
It was to prove this revolutionary technology that "Dreadnought" was designed in January 1905, laid down in October 1905 and sped to completion by 1906. She carried ten 12-inch guns, had an 11-inch armor belt, and was the first large ship powered by turbines. She mounted her guns in five turrets; three on the centerline (one forward, two aft) and two on the wings, giving her at her launch twice the broadside of any other warship. She retained a number of 12-pound (3-inch, 76 mm) quick-firing guns for use against destroyers and torpedo-boats. Her armor was heavy enough for her to go head-to-head with any other ship in a gun battle, and conceivably win.
"Dreadnought" was to have been followed by three s, their construction delayed to allow lessons from "Dreadnought" to be used in their design. While Fisher may have intended "Dreadnought" to be the last Royal Navy battleship, the design was so successful he found little support for his plan to switch to a battlecruiser navy. Although there were some problems with the ship (the wing turrets had limited arcs of fire and strained the hull when firing a full broadside, and the top of the thickest armor belt lay below the waterline at full load), the Royal Navy promptly commissioned another six ships to a similar design in the and es.
An American design, , authorized in 1905 and laid down in December 1906, was another of the first dreadnoughts, but she and her sister, , were not launched until 1908. Both used triple-expansion engines and had a superior layout of the main battery, dispensing with "Dreadnought"s wing turrets. They thus retained the same broadside, despite having two fewer guns.
Arms race.
In 1897, before the revolution in design brought about by HMS "Dreadnought", the Royal Navy had 62 battleships in commission or building, a lead of 26 over France and 50 over Germany. In 1906, the Royal Navy owned the field with "Dreadnought". The new class of ship prompted an arms race with major strategic consequences. Major naval powers raced to build their own dreadnoughts. Possession of modern battleships was not only vital to naval power, but also, as with nuclear weapons today, represented a nation's standing in the world. Germany, France, Japan, Italy, Austria, and the United States all began dreadnought programmes; while Ottoman Turkey, Argentina, Russia, Brazil, and Chile commissioned dreadnoughts to be built in British and American yards.
World War I.
The battleship, particularly the dreadnought, was the dominant naval weapon of the World War I era. There were few serious challenges at that time. The most significant naval battles of World War I, such as Jutland (May 31, 1916 – June 1, 1916), were fought by battleships and their battlecruiser cousins.
By virtue of geography, the Royal Navy was able to use her imposing battleship and battlecruiser fleet to impose a strict and successful naval blockade of Germany and kept Germany's smaller battleship fleet bottled up in the North Sea: only narrow channels led to the Atlantic Ocean and these were guarded by British forces. Both sides were aware that, because of the greater number of British dreadnoughts, a full fleet engagement would be likely to result in a British victory. The German strategy was therefore to try to provoke an engagement on their terms: either to induce a part of the Grand Fleet to enter battle alone, or to fight a pitched battle near the German coastline, where friendly minefields, torpedo-boats and submarines could be used to even the odds. Germany's submarines were able to break out and raid commerce, but even though they sank many merchant ships, they could not successfully blockade Great Britain – in contrast to Britain's successful battleship blockade of Germany, which was a major cause of Germany's economic collapse in 1918. The Royal Navy were able to adopt convoy tactics to combat the submarine blockade, and eventually defeated it.
The first two years of war saw the Royal Navy's battleships and battlecruisers regularly "sweep" the North Sea making sure that no German ships could get in or out. Only a few German surface ships that were already at sea, such as the famous light cruiser Emden, were able to raid commerce. Even some of those that did manage to get out were hunted down by battlecruisers, as in the Battle of the Falklands, December 7, 1914. The results of sweeping actions in the North Sea were battles such as the Heligoland Bight and Dogger Bank and German raids on the English coast, all of which were attempts by the Germans to lure out portions of the Grand Fleet in an attempt to defeat the Royal Navy in detail. On May 31, 1916, a further attempt to draw British ships into battle on German terms resulted in a clash of the battlefleets in the Battle of Jutland. The German fleet withdrew to port after two short encounters with the British fleet. Less than two months later, the Germans once again attempted to draw portions of the Grand Fleet into battle. The resulting Action of 19 August 1916 proved inconclusive. This reinforced German determination not to engage in a fleet to fleet battle.
In the other naval theatres there were no decisive pitched battles. In the Black Sea, engagement between Russian and Turkish battleships was restricted to skirmishes. In the Baltic Sea, action was largely limited to the raiding of convoys, and the laying of defensive minefields; the only significant clash of battleship squadrons there was the Battle of Moon Sound at which one Russian pre-dreadnought was lost. The Adriatic was in a sense the mirror of the North Sea: the Austro-Hungarian dreadnought fleet remained bottled up by the British and French blockade. And in the Mediterranean, the most important use of battleships was in support of the amphibious assault on Gallipoli.
Torpedo boats did have some successes against battleships in World War I, sinking the Austro-Hungarian dreadnought Szent Istvan and the British pre-dreadnought Goliath. In large fleet actions, however, destroyers and torpedo boats were usually unable to get close enough to the battleships to damage them. The only battleship sunk in a fleet action by either torpedo boats or destroyers was the obsolescent German pre-dreadnought Pommern. She was sunk by destroyers during the night phase of the Battle of Jutland.
Submarines did prove to be an incredibly dangerous threat to older pre-dreadnought battleships, as shown by examples such as the sinking of the , which was caught in the Dardanelles by a British submarine and the and were torpedoed by "U-21" as well as , , etc. As for dreadnoughts however, not a single dreadnought battleship was sunk by submarine torpedoes in World War I. In September 1914, the threat posed to surface ships by German U-boats was confirmed by successful attacks on British cruisers, including the sinking of three British armored cruisers by the German submarine in less than an hour, but the only casualty of a dreadnought to a submarine during the war was the British battleship HMS Audacious, which struck a mine laid by a German U-boat in October 1914 and sank. She was also the only dreadnought to be sunk by mine strike in WWI.
Threats to British battleships was enough to cause the Royal Navy to change their strategy and tactics in the North Sea to reduce the risk of U-boat attack. Whilst the escape of the German fleet from the superior British firepower at Jutland was effected by the German cruisers and destroyers successfully turning away the British battleships, the German attempt to rely on U-boat attacks on the British fleet failed. Further near-misses from submarine attacks on battleships and casualties amongst cruisers led to growing concern in the Royal Navy about the vulnerability of battleships.
The German High Seas Fleet, for their part, were determined not to engage the British without the assistance of submarines; and since the submarines were needed more for raiding commercial traffic, the fleet stayed in port for much of the war. Other theatres equally showed the role of small craft in damaging or destroying dreadnoughts: of the Austro-Hungarian Navy was sunk by Italian motor torpedo boats in June 1918, while her sister ship, , was sunk by frogmen. The Allied capital ships lost in Gallipoli were sunk by mines and torpedo, while a Turkish pre-dreadnought, , was caught in the Dardanelles by a British submarine.
Inter-war period.
For many years, Germany simply had no battleships. The Armistice with Germany required that most of the High Seas Fleet be disarmed and interned in a neutral port; largely because no neutral port could be found, the ships remained in British custody in Scapa Flow, Scotland. The Treaty of Versailles specified that the ships should be handed over to the British. Instead, most of them were scuttled by their German crews on June 21, 1919 just before the signature of the peace treaty. The treaty also limited the German Navy, and prevented Germany from building or possessing any capital ships.
The inter-war period saw the battleship subjected to strict international limitations to prevent a costly arms race breaking out.
While the victors were not limited by the Treaty of Versailles, many of the major naval powers were crippled after the war. Faced with the prospect of a naval arms race against the United Kingdom and Japan, which would in turn have led to a possible Pacific war, the United States was keen to conclude the Washington Naval Treaty of 1922. This treaty limited the number and size of battleships that each major nation could possess, and required Britain to accept parity with the U.S. and to abandon the British alliance with Japan. The Washington treaty was followed by a series of other naval treaties, including the First Geneva Naval Conference (1927), the First London Naval Treaty (1930), the Second Geneva Naval Conference (1932), and finally the Second London Naval Treaty (1936), which all set limits on major warships. These treaties became effectively obsolete on September 1, 1939 at the beginning of World War II, but the ship classifications that had been agreed upon still apply. The treaty limitations meant that fewer new battleships were launched in 1919–39 than in 1905–14. The treaties also inhibited development by putting maximum limits on the weights of ships. Designs like the projected British , the first American , and the Japanese —all of which continued the trend to larger ships with bigger guns and thicker armor—never got off the drawing board. Those designs which were commissioned during this period were referred to as treaty battleships.
Rise of air power.
As early as 1914, the British Admiral Percy Scott predicted that battleships would soon be made irrelevant by aircraft. By the end of World War I, aircraft had successfully adopted the torpedo as a weapon. In 1921 the Italian general and air theorist Giulio Douhet completed a hugely influential treatise on strategic bombing titled "The Command of the Air", which foresaw the dominance of air power over naval units.
In the 1920s, General Billy Mitchell of the United States Army Air Corps, believing that air forces had rendered navies around the world obsolete, testified in front of Congress that "1,000 bombardment airplanes can be built and operated for about the price of one battleship" and that a squadron of these bombers could sink a battleship, making for more efficient use of government funds. This infuriated the U.S. Navy, but Mitchell was nevertheless allowed to conduct a careful series of bombing tests alongside Navy and Marine bombers. In 1921, he bombed and sank numerous ships, including the "unsinkable" German World War I battleship and the American pre-dreadnought .
Although Mitchell had required "war-time conditions", the ships sunk were obsolete, stationary, defenseless and had no damage control. The sinking of "Ostfriesland" was accomplished by violating an agreement that would have allowed Navy engineers to examine the effects of various munitions: Mitchell's airmen disregarded the rules, and sank the ship within minutes in a coordinated attack. The stunt made headlines, and Mitchell declared, "No surface vessels can exist wherever air forces acting from land bases are able to attack them." While far from conclusive, Mitchell's test was significant because it put proponents of the battleship against naval aviation on the back foot. Rear Admiral William A. Moffett used public relations against Mitchell to make headway toward expansion of the U.S. Navy's nascent aircraft carrier program.
Rearmament.
The Royal Navy, United States Navy, and Imperial Japanese Navy extensively upgraded and modernized their World War I–era battleships during the 1930s. Among the new features were an increased tower height and stability for the optical rangefinder equipment (for gunnery control), more armor (especially around turrets) to protect against plunging fire and aerial bombing, and additional anti-aircraft weapons. Some British ships received a large block superstructure nicknamed the "Queen Anne's castle", such as in the and , which would be used in the new conning towers of the fast battleships. External bulges were added to improve both buoyancy to counteract weight increase and provide underwater protection against mines and torpedoes. The Japanese rebuilt all of their battleships, plus their battlecruisers, with distinctive "pagoda" structures, though the received a more modern bridge tower that would influence the new s. Bulges were fitted, including steel tube arrays to improve both underwater and vertical protection along the waterline. The U.S. experimented with cage masts and later tripod masts, though after Pearl Harbor some of the most severely damaged ships such as and were rebuilt to a similar appearance to their contemporaries (called tower masts). Radar, which was effective beyond visual contact and was effective in complete darkness or adverse weather conditions, was introduced to supplement optical fire control.
Even when war threatened again in the late 1930s, battleship construction did not regain the level of importance which it had held in the years before World War I. The "building holiday" imposed by the naval treaties meant that the building capacity of dockyards worldwide was relatively reduced, and the strategic position had changed.
In Germany, the ambitious Plan Z for naval rearmament was abandoned in favour of a strategy of submarine warfare supplemented by the use of battlecruisers and s as commerce raiders. In Britain, the most pressing need was for air defenses and convoy escorts to safeguard the civilian population from bombing or starvation, and re-armament construction plans consisted of five ships of the . It was in the Mediterranean that navies remained most committed to battleship warfare. France intended to build six battleships of the and es, and the Italians two ships. Neither navy built significant aircraft carriers. The U.S. preferred to spend limited funds on aircraft carriers until the . Japan, also prioritising aircraft carriers, nevertheless began work on three mammoth ships (although the third, , was later completed as a carrier) and a planned fourth was cancelled.
At the outbreak of the Spanish Civil War, the Spanish navy consisted of only two small dreadnought battleships, and . "España" (originally named "Alfonso XIII"), by then in reserve at the northwestern naval base of El Ferrol, fell into Nationalist hands in July 1936. The crew aboard "Jaime I" murdered their officers, mutinied, and joined the Republican Navy. Thus each side had one battleship; however, the Republican Navy generally lacked experienced officers. The Spanish battleships mainly restricted themselves to mutual blockades, convoy escort duties, and shore bombardment, rarely in direct fighting against other surface units. In April 1937, "España" ran into a mine laid by friendly forces, and sank with little loss of life. In May 1937, "Jaime I" was damaged by Nationalist air attacks and a grounding incident. The ship was forced to go back to port to be repaired. There she was again hit by several aerial bombs. It was then decided to tow the battleship to a more secure port, but during the transport she suffered an internal explosion that caused 300 deaths and her total loss. Several Italian and German capital ships participated in the non-intervention blockade. On May 29, 1937, two Republican aircraft managed to bomb the German pocket battleship outside Ibiza, causing severe damage and loss of life. retaliated two days later by bombarding Almería, causing much destruction, and the resulting "Deutschland" incident meant the end of German and Italian support for non-intervention.
World War II.
The —an obsolete pre-dreadnought—fired the first shots of World War II with the bombardment of the Polish garrison at Westerplatte; and the final surrender of the Japanese Empire took place aboard a United States Navy battleship, . Between those two events, it had become clear that aircraft carriers were the new principal ships of the fleet and that battleships now performed a secondary role.
Battleships played a part in major engagements in Atlantic, Pacific and Mediterranean theatres; in the Atlantic, the Germans used their battleships as independent commerce raiders. However, clashes between battleships were of little strategic importance. The Battle of the Atlantic was fought between destroyers and submarines, and most of the decisive fleet clashes of the Pacific war were determined by aircraft carriers.
In the first year of the war, armored warships defied predictions that aircraft would dominate naval warfare. and surprised and sank the aircraft carrier off western Norway in June 1940. This engagement marked the last time a fleet carrier was sunk by surface gunnery. In the attack on Mers-el-Kébir, British battleships opened fire on the French battleships in the harbour near Oran in Algeria with their heavy guns, and later pursued fleeing French ships with planes from aircraft carriers.
The subsequent years of the war saw many demonstrations of the maturity of the aircraft carrier as a strategic naval weapon and its potential against battleships. The British air attack on the Italian naval base at Taranto sank one Italian battleship and damaged two more. The same Swordfish torpedo bombers played a crucial role in sinking the German commerce-raider .
On December 7, 1941, the Japanese launched a surprise attack on Pearl Harbor. Within a short time five of eight U.S. battleships were sunk or sinking, with the rest damaged. The American aircraft carriers were out to sea, however, and evaded detection. They took up the fight, and eventually turned the tide of the war in the Pacific. The sinking of the British battleship and her escort, the battlecruiser , demonstrated the vulnerability of a battleship to air attack while at sea without sufficient air cover, settling the argument begun by Mitchell in 1921. Both warships were under way and en route to attack the Japanese amphibious force that had invaded Malaya when they were caught by Japanese land-based bombers and torpedo bombers on December 10, 1941.
At many of the early crucial battles of the Pacific, for instance Coral Sea and Midway, battleships were either absent or overshadowed as carriers launched wave after wave of planes into the attack at a range of hundreds of miles. In later battles in the Pacific, battleships primarily performed shore bombardment in support of amphibious landings and provided anti-aircraft defense as escort for the carriers. Even the largest battleships ever constructed, Japan's , which carried a main battery of nine 18-inch (46 cm) guns and were designed as a principal strategic weapon, were never given a chance to show their potential in the decisive battleship action that figured in Japanese pre-war planning.
The last battleship confrontation in history was the Battle of Surigao Strait, on October 25, 1944, in which a numerically and technically superior American battleship group destroyed a lesser Japanese battleship group by gunfire after it had already been devastated by destroyer torpedo attacks. All but one of the American battleships in this confrontation had previously been sunk by the attack on Pearl Harbor and subsequently raised and repaired. When fired the last salvo of this battle, the last salvo fired by a battleship against another heavy ship, she was "firing a funeral salute to a finished era of naval warfare." In April 1945, during the battle for Okinawa, the world's most powerful battleship, the "Yamato", was sent out against a massive U.S. force on a suicide mission and sunk by overwhelming carrier aircraft with nearly all hands.
Cold War.
After World War II, several navies retained their existing battleships, but they were no longer strategically dominant military assets. Indeed, it soon became apparent that they were no longer worth the considerable cost of construction and maintenance and only one new battleship was commissioned after the war, . During the war it had been demonstrated that battleship-on-battleship engagements like Leyte Gulf or the sinking of were the exception and not the rule, and with the growing role of aircraft engagement ranges were becoming longer and longer, making heavy gun armament irrelevant. The armor of a battleship was equally irrelevant in the face of a nuclear attack as tactical missiles with a range of or more could be mounted on the Soviet and s. By the end of the 1950s, minor vessel classes which formerly offered no noteworthy opposition now were capable of eliminating battleships at will.
The remaining battleships met a variety of ends. and were sunk during the testing of nuclear weapons in Operation Crossroads in 1946. Both battleships proved resistant to nuclear air burst but vulnerable to underwater nuclear explosions. The was taken by the Soviets as reparations and renamed "Novorossiysk"; she was sunk by a leftover German mine in the Black Sea on October 29, 1955. The two ships were scrapped in 1956. The French was scrapped in 1954, in 1968, and in 1970.
The United Kingdom's four surviving ships were scrapped in 1957, and followed in 1960. All other surviving British battleships had been sold or broken up by 1949. The Soviet Union's was scrapped in 1953, in 1957 and (back under her original name, , since 1942) in 1956-7. Brazil's was scrapped in Genoa in 1953, and her sister ship sank during a storm in the Atlantic "en route" to the breakers in Italy in 1951.
Argentina kept its two ships until 1956 and Chile kept (formerly ) until 1959. The Turkish battlecruiser (formerly , launched in 1911) was scrapped in 1976 after an offer to sell her back to Germany was refused. Sweden had several small coastal-defense battleships, one of which, , survived until 1970. The Soviets scrapped four large incomplete cruisers in the late 1950s, whilst plans to build a number of new s were abandoned following the death of Joseph Stalin in 1953. The three old German battleships , , and all met similar ends. "Hessen" was taken over by the Soviet Union and renamed "Tsel". She was scrapped in 1960. "Schleswig-Holstein" was renamed "Borodino", and was used as a target ship until 1960. "Schlesien", too, was used as a target ship. She was broken up between 1952 and 1957.
The s gained a new lease of life in the U.S. Navy as fire support ships. Radar and computer-controlled gunfire could be aimed with pinpoint accuracy to target. The U.S. recommissioned all four "Iowa"-class battleships for the Korean War and the for the Vietnam War. These were primarily used for shore bombardment, "New Jersey" firing nearly 6,000 rounds of 16 inch shells and over 14,000 rounds of 5 inch projectiles during her tour on the gunline, seven times more rounds against shore targets in Vietnam than she had fired in the Second World War.
As part of Navy Secretary John F. Lehman's effort to build a 600-ship Navy in the 1980s, and in response to the commissioning of "Kirov" by the Soviet Union, the United States recommissioned all four "Iowa"-class battleships. On several occasions, battleships were support ships in carrier battle groups, or led their own battleship battle group. These were modernized to carry Tomahawk missiles, with "New Jersey" seeing action bombarding Lebanon in 1983 and 1984, while and fired their 16 inch (406 mm) guns at land targets and launched missiles during Operation Desert Storm in 1991. "Wisconsin" served as the TLAM strike commander for the Persian Gulf, directing the sequence of launches that marked the opening of "Desert Storm", firing a total of 24 TLAMs during the first two days of the campaign. The primary threat to the battleships were Iraqi shore based surface-to-surface missiles; "Missouri" was targeted by two Iraqi Silkworm missiles, with one missing and another being intercepted by the British destroyer .
Modern times.
All four "Iowa" ships were decommissioned in the early 1990s, making them the last battleships to see active service. USS and USS were maintained to a standard where they could be rapidly returned to service as fire support vessels, pending the development of a superior fire support vessel. These last two battleships were finally stricken from the U.S. Naval Vessel Register in 2006. The Military Balance and Russian states the U.S. Navy listed one battleship in the reserve (Naval Inactive Fleet/Reserve 2nd Turn) in 2010. The U.S. Marine Corps believes that the current naval surface fire support gun and missile programs will not be able to provide adequate fire support for an amphibious assault or onshore operations.
With the decommissioning of the last "Iowa"-class ships, no battleships remain in service or in reserve with any navy worldwide. A number are preserved as museum ships, either afloat or in drydock. The U.S. has eight battleships on display: , , , , , , and . "Missouri" and "New Jersey" are now museums at Pearl Harbor and Camden, New Jersey, respectively. "Iowa" is now on display as an educational attraction at the Los Angeles Waterfront in San Pedro, California. "Wisconsin" was removed from the Naval Vessel Register in 2006 and now serves as a museum ship in Norfolk, Virginia. "Massachusetts", which owns the distinction of never having lost a man while in active service, was acquired by the Battleship Cove naval museum in Fall River, Massachusetts in 1965. "Texas", the first battleship turned into a museum, is on display at the San Jacinto Battleground State Historic Site, near Houston. "North Carolina" is on display in Wilmington, North Carolina. "Alabama" is on display in Mobile, Alabama. The only other 20th-century battleship on display is the Japanese pre-dreadnought .
Owing to geography, "Iowa", "Missouri" and "Wisconsin" are the only museum battleships not enshrined in their namesake states.
Strategy and doctrine.
Doctrine.
Battleships were the embodiment of sea power. For Alfred Thayer Mahan and his followers, a strong navy was vital to the success of a nation, and control of the seas was vital for the projection of force on land and overseas. Mahan's theory, proposed in "The Influence of Sea Power Upon History, 1660–1783" of 1890, dictated the role of the battleship was to sweep the enemy from the seas. While the work of escorting, blockading, and raiding might be done by cruisers or smaller vessels, the presence of the battleship was a potential threat to any convoy escorted by any vessels other than capital ships. (This concept came to be known as a "fleet in being".) Mahan went on to say victory could only be achieved by engagements between battleships, which came to be known as the "decisive battle" doctrine in some navies, while targeting merchant ships (commerce raiding or "guerre de course", as posited by the "Jeune École") could never succeed.
Mahan was highly influential in naval and political circles throughout the age of the battleship, calling for a large fleet of the most powerful battleships possible. Mahan's work developed in the late 1880s, and by the end of the 1890s it had a massive international impact, in the end adopted by many major navies (notably the British, American, German, and Japanese). The strength of Mahanian opinion was important in the development of the battleships arms races, and equally important in the agreement of the Powers to limit battleship numbers in the interwar era.
The "fleet in being" suggested battleships could simply by their existence tie down superior enemy resources. This in turn was believed to be able to tip the balance of a conflict even without a battle. This suggested even for inferior naval powers a battleship fleet could have important strategic impact.
Tactics.
While the role of battleships in both World Wars reflected Mahanian doctrine, the details of battleship deployment were more complex. Unlike the ship of the line, the battleships of the late 19th and early 20th centuries had significant vulnerability to torpedoes and mines, weapons which could be used by relatively small and inexpensive craft. The "Jeune École" school of thought of the 1870s and 1880s recommended placing torpedo boats alongside battleships; these would hide behind the battleships until gun-smoke obscured visibility enough for them to dart out and fire their torpedoes. While this tactic was vitiated by the development of smokeless propellant, the threat from more capable torpedo craft (later including submarines) remained. By the 1890s the Royal Navy had developed the first destroyers, which were initially designed to intercept and drive off any attacking torpedo boats. During the First World War and subsequently, battleships were rarely deployed without a protective screen of destroyers.
Battleship doctrine emphasised the concentration of the battlegroup. In order for this concentrated force to be able to bring its power to bear on a reluctant opponent (or to avoid an encounter with a stronger enemy fleet), battlefleets needed some means of locating enemy ships beyond horizon range. This was provided by scouting forces; at various stages battlecruisers, cruisers, destroyers, airships, submarines and aircraft were all used. (With the development of radio, direction finding and traffic analysis would come into play, as well, so even shore stations, broadly speaking, joined the battlegroup.) So for most of their history, battleships operated surrounded by squadrons of destroyers and cruisers. The North Sea campaign of the First World War illustrates how, despite this support, the threat of mine and torpedo attack, and the failure to integrate or appreciate the capabilities of new techniques, seriously inhibited the operations of the Royal Navy Grand Fleet, the greatest battleship fleet of its time.
Strategic and diplomatic impact.
The presence of battleships had a great psychological and diplomatic impact. Similar to possessing nuclear weapons today, the ownership of battleships served to enhance a nation's force projection.
Even during the Cold War, the psychological impact of a battleship was significant. In 1946, USS "Missouri" was dispatched to deliver the remains of the ambassador from Turkey, and her presence in Turkish and Greek waters staved off a possible Soviet thrust into the Balkan region. In September 1983, when Druze militia in Lebanon's Shouf Mountains fired upon U.S. Marine peacekeepers, the arrival of USS "New Jersey" stopped the firing. Gunfire from "New Jersey" later killed militia leaders.
Value for money.
Battleships were the largest and most complex, and hence the most expensive warships of their time; as a result, the value of investment in battleships has always been contested. As the French politician Etienne Lamy wrote in 1879, "The construction of battleships is so costly, their effectiveness so uncertain and of such short duration, that the enterprise of creating an armored fleet seems to leave fruitless the perseverance of a people". The "Jeune École" school of thought of the 1870s and 1880s sought alternatives to the crippling expense and debatable utility of a conventional battlefleet. It proposed what would nowadays be termed a sea denial strategy, based on fast, long-ranged cruisers for commerce raiding and torpedo boat flotillas to attack enemy ships attempting to blockade French ports. The ideas of the "Jeune Ecole" were ahead of their time; it was not until the 20th century that efficient mines, torpedoes, submarines, and aircraft were available that allowed similar ideas to be effectively implemented.
The determination of powers such as Germany to build battlefleets with which to confront much stronger rivals has been criticised by historians, who emphasise the futility of investment in a battlefleet which has no chance of matching its opponent in an actual battle. According to this view, attempts by a weaker navy to compete head-to-head with a stronger one in battleship construction simply wasted resources which could have been better invested in attacking the enemy's points of weakness. In Germany's case, the British dependence on massive imports of food and raw materials proved to be a near-fatal weakness, once Germany had accepted the political risk of unrestricted submarine warfare against commercial shipping. Although the U-boat offensive in 1917–18 was ultimately defeated, it was successful in causing huge material loss and forcing the Allies to divert vast resources into anti-submarine warfare. This success, though not ultimately decisive, was nevertheless in sharp contrast to the inability of the German battlefleet to challenge the supremacy of Britain's far stronger fleet.

</doc>
<doc id="4055" url="http://en.wikipedia.org/wiki?curid=4055" title="Bifröst">
Bifröst

In Norse mythology, Bifröst or sometimes Bilröst or Bivrost, is a burning rainbow bridge that reaches between Midgard (the world) and Asgard, the realm of the gods. The bridge is attested as "Bilröst" in the "Poetic Edda"; compiled in the 13th century from earlier traditional sources, and as "Bifröst" in the "Prose Edda"; written in the 13th century by Snorri Sturluson, and in the poetry of skalds. Both the "Poetic Edda" and the "Prose Edda" alternately refer to the bridge as Asbrú (Old Norse "Æsir's bridge").
According to the "Prose Edda", the bridge ends in heaven at Himinbjörg, the residence of the god Heimdallr, who guards it from the jötnar. The bridge's destruction at Ragnarök by the forces of Muspell is foretold. Scholars have proposed that the bridge may have originally represented the Milky Way and have noted parallels between the bridge and another bridge in Norse mythology, Gjallarbrú.
Etymology.
Scholar Andy Orchard posits that "Bifröst" may mean "shimmering path." He notes that the first element of "Bilröst"—"bil" (meaning "a moment")—"suggests the fleeting nature of the rainbow," which he connects to the first element of "Bifröst"—the Old Norse verb "bifa" (meaning "to shimmer" or "to shake")—noting that the element provokes notions of the "lustrous sheen" of the bridge. Austrian Germanist Rudolf Simek says that "Bifröst" either means "the swaying road to heaven" (also citing "bifa") or, if "Bilröst" is the original form of the two (which Simek says is likely), "the fleetingly glimpsed rainbow" (possibly connected to "bil", perhaps meaning "moment, weak point").
Attestations.
Two poems in the "Poetic Edda" and two books in the "Prose Edda" provide information about the bridge:
"Poetic Edda".
In the "Poetic Edda", the bridge is mentioned in the poems "Grímnismál" and "Fáfnismál", where it is referred to as "Bilröst". In one of two stanzas in the poem "Grímnismál" that mentions the bridge, Grímnir (the god Odin in disguise) provides the young Agnarr with cosmological knowledge, including that Bilröst is the best of bridges. Later in "Grímnismál", Grímnir notes that Asbrú "burns all with flames" and that, every day, the god Thor wades through the waters of Körmt and Örmt and the two Kerlaugar:
In "Fáfnismál", the dying wyrm Fafnir tells the hero Sigurd that, during the events of Ragnarok, bearing spears, gods will meet at Óskópnir. From there, the gods will cross Bilröst, which will break apart as they cross over it, causing their horses to dredge through an immense river.
"Prose Edda".
The bridge is mentioned in the "Prose Edda" books "Gylfaginning" and "Skáldskaparmál", where it is referred to as "Bifröst". In chapter 13 of "Gylfaginning", Gangleri (King Gylfi in disguise) asks the enthroned figure of High what way exists between heaven and earth. Laughing, High replies that the question isn't an intelligent one, and goes on to explain that the gods built a bridge from heaven and earth. He incredulously asks Gangleri if he has not heard the story before. High says that Gangleri must have seen it, and notes that Gangleri may call it a rainbow. High says that the bridge consists of three colors, has great strength, "and is built with art and skill to a greater extent than other constructions."
High notes that, although the bridge is strong, it will break when "Muspell's lads" attempt to cross it, and their horses will have to make do with swimming over "great rivers." Gangleri says that it doesn't seem that the gods "built the bridge in good faith if it is liable to break, considering that they can do as they please." High responds that the gods do not deserve blame for the breaking of the bridge, for "there is nothing in this world that will be secure when Muspell's sons attack."
In chapter 15 of "Gylfaginning", Just-As-High says that Bifröst is also called "Asbrú", and that every day the gods ride their horses across it (with the exception of Thor, who instead wades through the boiling waters of the rivers Körmt and Örmt) to reach Urðarbrunnr, a holy well where the gods have their court. As a reference, Just-As-High quotes the second of the two stanzas in "Grímnismál" that mention the bridge (see above). Gangleri asks if fire burns over Bifröst. High says that the red in the bridge is burning fire, and, without it, the frost jotnar and mountain jotnar would "go up into heaven" if anyone who wanted could cross Bifröst. High adds that, in heaven, "there are many beautiful places" and that "everywhere there has divine protection around it."
In chapter 17, High tells Gangleri that the location of Himinbjörg "stands at the edge of heaven where Bifrost reaches heaven." While describing the god Heimdallr in chapter 27, High says that Heimdallr lives in Himinbjörg by Bifröst, and guards the bridge from mountain jotnar while sitting at the edge of heaven. In chapter 34, High quotes the first of the two "Grímnismál" stanzas that mention the bridge. In chapter 51, High foretells the events of Ragnarök. High says that, during Ragnarök, the sky will split open, and from the split will ride forth the "sons of Muspell". When the "sons of Muspell" ride over Bifröst it will break, "as was said above."
In the "Prose Edda" book "Skáldskaparmál", the bridge receives a single mention. In chapter 16, a work by the 10th century skald Úlfr Uggason is provided, where Bifröst is referred to as "the powers' way."
Theories.
In his translation of the "Prose Edda", Henry Adams Bellows comments that the "Grímnismál" stanza mentioning Thor and the bridge stanza may mean that "Thor has to go on foot in the last days of the destruction, when the bridge is burning. Another interpretation, however, is that when Thor leaves the heavens (i.e., when a thunder-storm is over) the rainbow-bridge becomes hot in the sun."
John Lindow points to a parallel between Bifröst, which he notes is "a bridge between earth and heaven, or earth and the world of the gods", and the bridge Gjallarbrú, "a bridge between earth and the underworld, or earth and the world of the dead." Several scholars have proposed that Bifröst may represent the Milky Way.

</doc>
<doc id="4057" url="http://en.wikipedia.org/wiki?curid=4057" title="Battlecruiser">
Battlecruiser

A battlecruiser, or battle cruiser, was a large capital ship built in the first half of the 20th century. They were similar in size and cost to a battleship, and typically carried the same kind of heavy guns, but generally carried less armour and were faster. The first battlecruisers were designed in the United Kingdom in the first decade of the century, as a development of the armoured cruiser, at the same time the dreadnought succeeded the pre-dreadnought battleship. The original aim of the battlecruiser was to hunt down slower, older armoured cruisers and destroy them with heavy gunfire. However, as more and more battlecruisers were built, they increasingly became used alongside the better-protected battleships.
Battlecruisers served in the navies of Britain, Germany, Ottoman Empire, Australia and Japan during World War I, most notably at the Battle of the Falkland Islands and in the several raids and skirmishes in the North Sea which culminated in a pitched fleet battle, the Battle of Jutland. British battlecruisers in particular suffered heavy losses at Jutland, where their light armour made them very vulnerable to battleship shells. By the end of the war, capital ship design had developed with battleships becoming faster and battlecruisers becoming more heavily armoured, blurring the distinction between a battlecruiser and a fast battleship. The Washington Naval Treaty, which limited capital ship construction from 1922 onwards, treated battleships and battlecruisers identically, and the new generation of battlecruisers planned was scrapped under the terms of the treaty.
From the 1930s, only the Royal Navy continued to use 'battlecruiser' as a classification for warships, for the WWI-era capital ships that remained in the fleet. (While Japan's battlecruisers remained in service, they were significantly reconstructed and re-rated as battleships.) Nevertheless, the fast, light capital ships developed by Germany and France of the and classes, respectively, are sometimes referred to as battlecruisers.
World War II saw the modernized battlecruisers in action again, only one of which survived the war. There was also renewed interest in large "cruiser-killer" type warships, but few were ever begun, as construction of capital ships was curtailed in favor of more-needed convoy escorts, aircraft carriers, and cargo ships. In the post–Cold War era, the Soviet of large guided missile cruisers have also been termed "battlecruisers".
Background.
The battlecruiser was developed by the Royal Navy in the first years of the 20th century as an evolution of the armoured cruiser.
The first armoured cruisers had been built in the 1870s, as an attempt to give armour protection to ships fulfilling the typical cruiser roles of patrol, trade protection and power projection. However, the results were rarely satisfactory, as the weight of armour required for any meaningful protection usually meant that the ship became almost as slow as a battleship. As a result, navies preferred to build protected cruisers with an armoured deck protecting their engines, or simply no armour at all.
In the 1890s, technology began to change this balance. New Krupp steel armour meant that it was now possible to give a cruiser side armour which would protect it against the quick-firing guns of enemy battleships and cruisers alike. In 1896-7 France and Russia, who were regarded as likely allies in the event of war, started to build large, fast armoured cruisers taking advantage of this. In the event of a war between Britain and France or Russia, or both, these cruisers threatened to cause serious difficulties for the British Empire's worldwide trade.
Britain, which had concluded in 1892 that it needed twice as many cruisers as any potential enemy to adequately protect its empire's sea lanes, responded to the perceived threat by laying down its own large armoured cruisers. Between 1899 and 1905, it completed or laid down seven classes of this type, a total of 35 ships. This building program, in turn, prompted the French and Russians to increase their own construction. The Imperial German Navy began to build large armoured cruisers for use on their overseas stations, laying down eight between 1897 and 1906.
The cost of this cruiser arms race was significant. In the period 1889–96, the Royal Navy spent £7.3 million on new large cruisers. From 1897−1904, it spent £26.9 million. Many armoured cruisers of the new kind were just as large and expensive as the equivalent battleship.
The increasing size and power of the armoured cruiser led to suggestions in British naval circles that cruisers should displace battleships entirely. The battleship's main advantage was its 12-inch heavy guns, and heavier armour designed to protect from shells of similar size. However, for a few years after 1900 it seemed that those advantages were of little practical value. The torpedo now had a range of 2,000 yards, and it seemed unlikely that a battleship would engage within torpedo range. However, at ranges of more than 2,000 yards it became increasingly unlikely that the heavy guns of a battleship would score any hits, as the heavy guns relied on primitive aiming techniques. The secondary batteries of 6-inch quick-firing guns, firing more plentiful shells, were more likely to hit the enemy. As naval expert Fred T. Jane wrote in June 1902,
In 1904, Admiral John "Jacky" Fisher became First Sea Lord, the senior officer of the Royal Navy. He had for some time thought about the development of a new fast armoured ship. He was very fond of the "second-class battleship" , a faster, more lightly armoured battleship. As early as 1901, there is confusion in Fisher's writing about whether he saw the battleship or the cruiser as the model for future developments. This did not stop him from commissioning designs from naval architect W. H. Gard for an armoured cruiser with the heaviest possible armament for use with the fleet. The design Gard submitted was for a ship between , capable of , armed with four 9.2-inch and twelve guns in twin gun turrets and protected with six inches of armour along her belt and 9.2-inch turrets, on her 7.5-inch turrets, 10 inches on her conning tower and up to on her decks. However, mainstream British naval thinking between 1902 and 1904 was clearly in favour of heavily armoured battleships, rather than the fast ships that Fisher favoured.
The Battle of Tsushima proved conclusively the effectiveness of heavy guns over intermediate ones and the need for a uniform main caliber on a ship for fire control. Even before this, the Royal Navy had begun to consider a shift away from the mixed-calibre armament of the 1890s pre-dreadnought to an "all-big-gun" design, and preliminary designs circulated for battleships with all 12-inch or all 10-inch guns and armoured cruisers with all 9.2-inch guns. In late 1904, not long after the Royal Navy had decided to use 12-inch guns for its next generation of battleships because of their superior performance at long range, Fisher began to argue that big-gun cruisers could replace battleships altogether. The continuing improvement of the torpedo meant that submarines and destroyers would be able to destroy battleships; this in Fisher's view heralded the end of the battleship or at least compromised the validity of heavy armour protection. Nevertheless, armoured cruisers would remain vital for commerce protection.
Of what use is a battle fleet to a country called (A) at war with a country called (B) possessing no battleships, but having fast armoured cruisers and clouds of fast torpedo craft? What damage would (A's) battleships do to (B)? Would (B) wish for a few battleships or for more armoured cruisers? Would not (A) willingly exchange a few battleships for more fast armoured cruisers? In such a case, neither side wanting battleships is presumptive evidence that they are not of much value" – Fisher to Lord Selborne (First Lord of the Admiralty), 20 October 1904
Fisher's views were very controversial within the Royal Navy, and even given his position as First Sea Lord, he was not in a position to insist on his own approach. Thus he assembled a "Committee on Designs", consisting of a mixture of civilian and naval experts, to determine the approach to both battleship and armoured cruiser construction in the future. While the stated purpose of the Committee was to investigate and report on future requirements of ships, Fisher and his associates had already made key decisions. The terms of reference for the Committee were for a battleship capable of with 12-inch guns and no intermediate calibres, capable of docking in existing drydocks; and a cruiser capable of , also with 12-inch guns and no intermediate armament, armoured like , the most recent armoured cruiser, and also capable of using existing docks.
First battlecruisers.
Under the Selborne plan of 1902, the Royal Navy intended to start three new battleships and four armoured cruisers each year. However, in late 1904 it became clear that the 1905–06 programme would have to be considerably smaller, because of lower than expected tax revenue and the need to buy out two Chilean battleships under construction in British yards, lest they be purchased by the Russians for use against the Japanese, Britain's ally. These economies meant that the 1905–06 programme consisted only of one battleship, but three armoured cruisers. The battleship became the revolutionary battleship , and the cruisers became the three ships of the . Fisher later claimed, however, that he had argued during the Committee for the cancellation of the remaining battleship.
The construction of the new class were begun in 1906 and completed in 1908, delayed perhaps to allow their designers to learn from any problems with "Dreadnought". The ships fulfilled the design requirement quite closely. On a displacement similar to "Dreadnought", the "Invincible"s were longer to accommodate additional boilers and more powerful turbines to propel them at . Moreover, the new ships could maintain this speed for days, whereas pre-dreadnought battleships could not generally do so for more than an hour. Armed with eight 12-inch Mk X guns, compared to ten on "Dreadnought", they had of armour protecting the hull and the gun turrets. ("Dreadnought"s armour, by comparison, was at its thickest.) The class had a very marked increase in speed, displacement and firepower compared to the most recent armoured cruisers but no more armour.
While the "Invincible"s were to fill the same role as the armoured cruisers they succeeded, they were expected to do so more effectively. Specifically their roles were:
Confusion about how to refer to these new battleship-size armoured cruisers set in almost immediately. Even in late 1905, before work was begun on the "Invincible"s, a Royal Navy memorandum refers to "large armoured ships" meaning both battleships and large cruisers. In October 1906, the Admiralty began to classify all post-Dreadnought battleships and armoured cruisers as "capital ships", while Fisher used the term "dreadnought" to refer either to his new battleships or the battleships and armoured cruisers together. At the same time, the "Invincible" class themselves were referred to as "cruiser-battleships", "dreadnought cruisers"; the term "battlecruiser" was first used by Fisher in 1908. Finally, on 24 November 1911, Admiralty Weekly Order No. 351 laid down that "All cruisers of the “Invincible” and later types are for the future to be described and classified as “battle cruisers” to distinguish them from the armoured cruisers of earlier date."
Along with questions over the new ships' nomenclature came uncertainty about their actual role due to their lack of protection. If they were primarily to act as scouts for the battle fleet and hunter-killers of enemy cruisers and commerce raiders, then the seven inches of belt armour with which they had been equipped would be adequate. If, on the other hand, they were expected to reinforce a battle line of dreadnoughts with their own heavy guns, they were too thin-skinned to be safe from an enemy's heavy guns. The "Invincible"s were essentially extremely large, heavily armed, fast armoured cruisers. However, the viability of the armoured cruiser was already in doubt. A cruiser that could have worked with the Fleet might have been a more viable option for taking over that role.
Because of the "Invincible"s size and armament, naval authorities considered them capital ships almost from their inception—an assumption that might have been inevitable. Complicating matters further was that many naval authorities, including Lord Fisher, had made overoptimistic assessments from the Battle of Tsushima in 1905 about the armoured cruiser's ability to survive in a battle line against enemy capital ships due to their superior speed. These assumptions had been made without taking into account the Russian Baltic Fleet's inefficiency and tactical ineptitude. By the time the term "battlecruiser" had been given to the "Invincible"s, the idea of their parity with battleships had been fixed in many people's minds.
Not everyone was so convinced. "Brasseys Naval Annual", for instance, stated that with vessels as large and expensive as the "Invincible"s, an admiral "will be certain to put them in the line of battle where their comparatively light protection will be a disadvantage and their high speed of no value." Those in favor of the battlecruiser countered with two points—first, since all capital ships were vulnerable to new weapons such as the torpedo, armour had lost some of its validity; and second, because of its greater speed, the battlecruiser could control the range at which it engaged an enemy.
Battlecruisers in the dreadnought arms race.
Between the launching of the "Invincible"s to just after the outbreak of the First World War, the battlecruiser played a junior role in the developing dreadnought arms race, as it was never wholeheartedly adopted as the key weapon in British imperial defence, as Fisher had presumably desired. The biggest factor for this lack of acceptance was the marked change in Britain's strategic circumstances between their conception and the commissioning of the first ships. The prospective enemy for Britain had shifted from a Franco-Russian alliance with many armoured cruisers to a resurgent and increasingly belligerent Germany. Diplomatically, Britain had entered the Entente cordiale in 1904 and the Anglo-Russian Entente. Neither France nor Russia posed a particular naval threat; the Russian navy had largely been sunk or captured in the Russo-Japanese War of 1904–5, while the French were in no hurry to adopt the new dreadnought-type design. Britain also boasted very cordial relations with two of the significant new naval powers, Japan (bolstered by the Anglo-Japanese Alliance, signed in 1902 and renewed in 1905), and the USA. These changed strategic circumstances, and the great success of the "Dreadnought", ensured that she rather than the "Invincible" became the new model capital ship. Nevertheless, battlecruiser construction played a part in the renewed naval arms-race sparked by the "Dreadnought".
For their first few years of service, the "Invincible"s entirely fulfilled Fisher's vision of being able to sink any ship fast enough to catch them, and run from any ship capable of sinking them. An "Invincible" would also, in many circumstances, be able to take on an enemy pre-dreadnought battleship. Naval circles concurred that the armoured cruiser in its current form had come to the logical end of its development and the "Invincible"s were so far ahead of any enemy armoured cruiser in firepower and speed that it proved difficult to justify building more or bigger cruisers. This lead was extended by the surprise both "Dreadnought" and "Invincible" produced by having been built in secret; this prompted most other navies to delay their building programmes and radically revise their designs. This was particularly true for cruisers, because the details of the "Invincible" class were kept secret for longer; this meant that the last German armoured cruiser, , was armed with only guns, and was no match for the new battlecruisers.
The Royal Navy's early superiority in capital ships led to the rejection of a 1905–06 design that would, essentially, have fused the battlecruiser and battleship concepts into what would eventually become the fast battleship. The 'X4' design combined the full armour and armament of "Dreadnought" with the 25 knot speed of "Invincible". The additional cost could not be justified given the existing British lead and the new Liberal government's need for economy; the slower and cheaper , a relatively close copy of "Dreadnought", was adopted instead. The X4 concept would eventually be fulfilled in the and later by other navies.
The next British battlecruisers were the three , slightly improved "Invincible"s built to fundamentally the same specification, partly due to political pressure to limit costs and partly due to the secrecy surrounding German battlecruiser construction, particularly about the heavy armour of . This class came to be widely seen as a mistake and the next generation of British battlecruisers were markedly more powerful. By 1909–10 a sense of national crisis about rivalry with Germany outweighed cost-cutting, and a naval panic resulted in the approval of a total of eight capital ships in 1909–10. Fisher pressed for all eight to be battlecruisers, but was unable to have his way; he had to settle for six battleships and two battlecruisers of the . The "Lion"s carried eight 13.5-inch guns, the now-standard caliber of the British "super-dreadnought" battleships. Speed increased to and armour protection, while not as good as in German designs, was better than in previous British battlecruisers, with armour belt and barbettes. The two "Lion"s were followed by the very similar .
By 1911 Germany had built battlecruisers of her own, and the superiority of the British ships could no longer be assured. Moreover, the German Navy did not share Fisher's view of the battlecruiser. In contrast to the British focus on increasing speed and firepower, Germany progressively improved the armour and staying power of their ships to better the British battlecruisers. "Von der Tann", begun in 1908 and completed in 1910, carried eight 11.1-inch guns, but with 11.1-inch (283 mm) armour she was far better protected than the "Invincible"s. The two s were quite similar but carried ten 11.1-inch guns of an improved design. , designed in 1909 and finished in 1913, was a modified "Moltke"; speed increased by one knot to , while her armour had a maximum thickness of 12 inches, equivalent to the s of a few years earlier. "Seydlitz" was Germany's last battlecruiser completed before World War I.
The next step in battlecruiser design came from Japan. The Imperial Japanese Navy had been planning the ships from 1909, and was determined that, since the Japanese economy could support relatively few ships, each would be more powerful than its likely competitors. Initially the class was planned with the "Invincible"s as the benchmark. On learning of the British plans for "Lion", and the likelihood that new U.S. Navy battleships would be armed with guns, the Japanese decided to radically revise their plans and go one better. A new plan was drawn up, carrying eight 14-inch guns, and capable of , thus marginally having the edge over the "Lion"s in speed and firepower. The heavy guns were also better-positioned, being superfiring both fore and aft with no turret amidships. The armour scheme was also marginally improved over the "Lion"s, with nine inches of armour on the turrets and on the barbettes. The first ship in the class was built in Britain, and a further three constructed in Japan. The Japanese also re-classified their powerful armoured cruisers of the "Tsukuba" and "Ibuki" classes, carrying four 12-inch guns, as battlecruisers; nonetheless, their armament was weaker and they were slower than any battlecruiser.
The next British battlecruiser, , was intended initially as the fourth ship in the "Lion" class, but was substantially redesigned. She retained the eight 13.5-inch guns of her predecessors, but they were positioned like those of "Kongō" for better fields of fire. She was faster (making on sea trials), and carried a heavier secondary armament. "Tiger" was also more heavily armoured on the whole; while the maximum thickness of armour was the same at nine inches, the height of the main armour belt was increased. Not all the desired improvements for this ship were approved, however. Her designer, Sir Eustace Tennyson d'Eyncourt, had wanted small-bore water-tube boilers and geared turbines to give her a speed of , but he received no support from the authorities and the engine makers refused his request.
1912 saw work begin on three more German battlecruisers of the , the first German battlecruisers to mount 12-inch guns. These excellent ships, like "Tiger" and the "Kongō"s, had their guns arranged in superfiring turrets for greater efficiency. Their armour and speed was similar to the previous "Seydlitz" class. In 1913, the Russian Empire also began the construction of the four-ship , which were designed for service in the Baltic Sea. These ships were designed to carry twelve 14-inch guns, with armour up to 12 inches thick, and a speed of . The heavy armour and relatively slow speed of these ships made them more similar to German designs than to British ships; construction of the "Borodino"s was halted by the First World War and all were scrapped after the end of the Russian Civil War.
World War I.
Construction.
For most of the combatants, capital ship construction was very limited during the war. Germany finished the "Derfflinger" class and began work on the . The "Mackensen"s were a development of the "Derfflinger" class, with 13.8-inch guns and a broadly similar armour scheme, designed for .
In Britain, Jackie Fisher returned to the office of First Sea Lord in October 1914. His enthusiasm for big, fast ships was unabated, and he set designers to producing a design for a battlecruiser with 15-inch guns. Because Fisher expected the next German battlecruiser to steam at 28 knots, he required the new British design to be capable of 32 knots. He planned to reorder two s, which had been approved but not yet laid down, to a new design. Fisher finally received approval for this project on 28 December 1914 and they became the . With six 15-inch guns but only 6-inch armour they were a further step forward from "Tiger" in firepower and speed, but returned to the level of protection of the first British battlecruisers.
At the same time, Fisher resorted to subterfuge to obtain another three fast, lightly armoured ships that could use several spare gun turrets left over from battleship construction. These ships were essentially light battlecruisers, and Fisher occasionally referred to them as such, but officially they were classified as "large light cruisers". This unusual designation was required because construction of new capital ships had been placed on hold, while there were no limits on light cruiser construction. They became and her sisters and , and there was a bizarre imbalance between their main guns of 15 inches (or in "Furious") and their armour, which at thickness was on the scale of a light cruiser. The design was generally regarded as a failure (nicknamed in the Fleet "Outrageous", "Uproarious" and "Spurious"), though the later conversion of the ships to aircraft carriers was very successful. Fisher also speculated about a new mammoth, but lightly built battlecruiser, that would carry guns, which he termed ; this never got beyond the concept stage.
It is often held that the "Renown" and "Courageous" classes were designed for Fisher's plan to land troops (possibly Russian) on the German Baltic coast. Specifically, they were designed with a reduced draught, which might be important in the shallow Baltic. This is not clear-cut evidence that the ships were designed for the Baltic: it was considered that earlier ships had too much draught and not enough freeboard under operational conditions. Roberts argues that the focus on the Baltic was probably unimportant at the time the ships were designed, but was inflated later, after the disastrous Dardanelles Campaign.
The final British battlecruiser design of the war was the , which was born from a requirement for an improved version of the "Queen Elizabeth" battleship. The project began at the end of 1915, after Fisher's final departure from the Admiralty. While initially envisaged as a battleship, senior sea officers felt that Britain had enough battleships, but that new battlecruisers might be required to combat German ships being built (the British overestimated German progress on the "Mackensen" class as well as their likely capabilities). A battlecruiser design with eight 15-inch guns, 8 inches of armour and capable of 32 knots was decided on. The experience of battlecruisers at the Battle of Jutland meant that the design was radically revised and transformed again into a fast battleship with armour up to 12 inches thick, but still capable of . The first ship in the class, , was built according to this design to counter the possible completion of any of the Mackensen-class ship. The plans for her three sisters, on which little work had been done, were revised once more later in 1916 and in 1917 to improve protection.
The Admiral class would have been the only British ships capable of taking on the German "Mackensen" class; nevertheless, German shipbuilding was drastically slowed by the war, and while two "Mackensen"s were launched, none were ever completed. The Germans also worked briefly on a further three ships, of the , which were modified versions of the "Mackensen"s with 15-inch guns. Work on the three additional Admirals was suspended in March 1917 to enable more escorts and merchant ships to be built to deal with the new threat from U-boats to trade. They were finally cancelled in February 1919.
Battlecruisers in action.
The first combat involving battlecruisers during World War I was the Battle of Heligoland Bight in August 1914. A force of British light cruisers and destroyers entered the Heligoland Bight (the part of the North Sea closest to Hamburg) to attack German shipping. When they met opposition from German cruisers, Vice Admiral David Beatty took his squadron of five battlecruisers into the Bight and turned the tide of the battle, ultimately sinking three German light cruisers and killing their commander, Rear Admiral Leberecht Maass.
The German battlecruiser perhaps made the most impact early in the war. Stationed in the Mediterranean, she and the escorting light cruiser evaded British and French ships on the outbreak of war, and steamed to Constantinople (Istanbul) with two British battlecruisers in hot pursuit. The two German ships were handed over to the Ottoman Navy, and this was instrumental in bringing the Ottoman Empire into the war as one of the Central Powers. "Goeben" herself, renamed "Yavuz Sultan Selim", fought engagements against the Imperial Russian Navy in the Black Sea and against the British in the Aegean Sea.
The original battlecruiser concept proved successful in December 1914 at the Battle of the Falkland Islands. The British battlecruisers and did precisely the job for which they were intended when they chased down and annihilated the German East Asia Squadron, centered on the armoured cruisers and , along with three light cruisers, commanded by Admiral Maximilian Graf Von Spee, in the South Atlantic Ocean. Prior to the battle, the Australian battlecruiser had unsuccessfully searched for the German ships in the Pacific.
During the Battle of Dogger Bank in 1915, the aftermost barbette of the German flagship "Seydlitz" was struck by a British 13.5-inch shell from "HMS Lion". The shell did not penetrate the barbette, but it dislodged a piece of the barbette armour that allowed the flame from the shell's detonation to enter the barbette. The propellant charges being hoisted upwards were ignited, and the fireball flashed up into the turret and down into the magazine, setting fire to charges removed from their brass cartridge cases. The gun crew tried to escape into the next turret, which allowed the flash to spread into that turret as well, killing the crews of both turrets. "Seydlitz" was saved from near-certain destruction only by emergency flooding of her after magazines, which had been effected by Wilhelm Heidkamp. This near-disaster was due to the way that ammunition handling was arranged and was common to both German and British battleships and battlecruisers, but the lighter protection on the latter made them more vulnerable to the turret or barbette being penetrated. The Germans learned from investigating the damaged "Seydlitz" and instituted measures to ensure that ammunition handling minimised any possible exposure to flash.
Apart from the cordite handling, the battle was mostly inconclusive, though both the British flagship "Lion" and "Seydlitz" were severely damaged. "Lion" lost speed, causing her to fall behind the rest of the battleline, and Beatty was unable to effectively command his ships for the remainder of the engagement. A British signalling error allowed the German battlecruisers to withdraw, as most of Beatty's squadron mistakenly concentrated on the crippled armoured cruiser "Blücher", sinking her with great loss of life. The British blamed their failure to win a decisive victory on their poor gunnery and attempted to increase their rate of fire by stockpiling unprotected cordite charges in their ammunition hoists and barbettes.
At the Battle of Jutland on 31 May 1916, both British and German battlecruisers were employed as fleet units. The British battlecruisers became engaged with both their German counterparts, the battlecruisers, and then German battleships before the arrival of the battleships of the British Grand Fleet. The result was a disaster for the Royal Navy's battlecruiser squadrons: "Invincible", "Queen Mary", and exploded with the loss of all but a handful of their crews. The exact reason why the ships' magazines detonated is not known, but the plethora of exposed cordite charges stored in their turrets, ammunition hoists and working chambers in the quest to increase their rate of fire undoubtedly contributed to their loss. Beatty's flagship "Lion" herself was almost lost in a similar manner, save for the heroic actions of Major Francis Harvey.
The better-armoured German battlecruisers fared better, in part due to the poor performance of British fuzes (the British shells tended to explode or break up on impact with the German armour). —the only German battlecruiser lost at Jutland—had only 128 killed, for instance, despite receiving more than thirty hits. The other German battlecruisers, , "Von der Tann", "Seydlitz", and , were all heavily damaged and required extensive repairs after the battle, "Seydlitz" barely making it home, for they had been the focus of British fire for much of the battle.
Interwar period.
In the years immediately after World War I, Britain, Japan and the USA all began design work on a new generation of ever more powerful battleships and battlecruisers. The new burst of shipbuilding that each nation's navy desired was politically controversial and potentially economically crippling. This nascent arms race was prevented by the Washington Naval Treaty of 1922, where the major naval powers agreed to limits on capital ship numbers. The German navy was not represented at the talks; under the terms of the Treaty of Versailles, Germany was not allowed any modern capital ships at all.
Through the 1920s and 1930s only Britain and Japan retained battlecruisers, often modified and rebuilt from their original designs. The line between the battlecruiser and the modern fast battleship became blurred; indeed, the Japanese "Kongō"s were formally redesignated as battleships.
Plans in the aftermath of World War I.
"Hood", launched in 1918, was the last World War I battlecruiser to be completed. Owing to lessons from Jutland, the ship was modified during construction; the thickness of her belt armour was increased by an average of 50 percent and extended substantially, she was given heavier deck armour, and the protection of her magazines was improved to guard against the ignition of ammunition. This was hoped to be capable of resisting her own weapons—the classic measure of a "balanced" battleship. "Hood" was the largest ship in the Royal Navy when completed; thanks to her great displacement, in theory she combined the firepower and armour of a battleship with the speed of a battlecruiser, causing some to refer to her as a fast battleship. However her protection was markedly less than that of the British battleships built immediately after World War I, the .
The navies of Japan and the United States, not being affected immediately by the war, had time to develop new heavy guns for their latest designs and to refine their battlecruiser designs in light of combat experience in Europe. The Imperial Japanese Navy began four s. These vessels would have been of unprecedented size and power, as fast and well armoured as "Hood" whilst carrying a main battery of ten 16-inch guns, the most powerful armament ever proposed for a battlecruiser. They were, for all intents and purposes, fast battleships—the only differences between them and the s which were to precede them were less side armour and a increase in speed. The United States Navy, which had worked on its battlecruiser designs since 1913 and watched the latest developments in this class with great care, responded with the . If completed as planned, they would have been exceptionally fast and well armed with eight 16-inch guns, but carried armour little better than the "Invincible"s—this after an increase in protection following Jutland. The final stage in the post-war battlecruiser race came with the British response to the "Amagi" and "Lexington" types: four G3 battlecruisers. Royal Navy documents of the period often described any battleship with a speed of over about as a battlecruiser, regardless of the amount of protective armour, although the G3 was considered by most to be a well-balanced fast battleship.
The Washington Naval Treaty meant that none of these designs came to fruition. Ships that had been started were either broken up on the slipway or converted to aircraft carriers. In Japan, "Amagi" and were selected for conversion. "Amagi" was damaged beyond repair by the 1923 Great Kantō earthquake and was broken up for scrap; the hull of one of the proposed "Tosa"-class battleships, , was converted in her stead. The United States Navy also converted two battlecruiser hulls into aircraft carriers in the wake of the Washington Treaty: and , although this was only considered marginally preferable to scrapping the hulls outright (the remaining four: "Constellation", "Ranger", "Constitution" and "United States" were scrapped). In Britain, Fisher's "large light cruisers," were converted to carriers. "Furious" had already been partially converted during the war and "Glorious" and "Courageous" were similarly converted.
Rebuilding programmes.
In total, nine battlecruisers survived the Washington Naval Treaty, although HMS "Tiger" later became a victim of the London Naval Conference of 1930 and was scrapped. Because their high speed made them valuable surface units in spite of their weaknesses, most of these ships were significantly updated before World War II. and were modernized significantly in the 1920s and 1930s. Between 1934 and 1936, "Repulse" was partially modernized and had her bridge modified, an aircraft hangar, catapult and new gunnery equipment added and her anti-aircraft armament increased. "Renown" underwent a more thorough reconstruction between 1937 and 1939. Her deck armour was increased, new turbines and boilers were fitted, an aircraft hangar and catapult added and she was completely rearmed aside from the main guns which had their elevation increased to +30 degrees. The bridge structure was also removed and a large bridge similar to that used in the battleships installed in its place. While conversions of this kind generally added weight to the vessel, "Renown"s tonnage actually decreased due to a substantially lighter power plant. Similar thorough rebuildings planned for "Repulse" and "Hood" were cancelled due to the advent of World War II.
Unable to build new ships, the Imperial Japanese Navy also chose to improve its existing battlecruisers of the "Kongō" class (initially the , , and —the only later as it had been disarmed under the terms of the Washington treaty) in two substantial reconstructions (one for "Hiei"). During the first of these, elevation of their main guns was increased to +40 degrees, anti-torpedo bulges and of horizontal armour added, and a "pagoda" mast with additional command positions built up. This reduced the ships' speed to . The second reconstruction focused on speed as they had been selected as fast escorts for aircraft carrier task forces. Completely new main engines, a reduced number of boilers and an increase in hull length by allowed them to reach up to 30 knots once again. They were reclassified as "fast battleships," although their armour and guns still fell short compared to surviving World War I–era battleships in the American or the British navies, with dire consequences during the Pacific War, when "Hiei" and "Kirishima" were easily crippled by US gunfire during actions off Guadalcanal, forcing their scuttling shortly afterwards. Perhaps most tellingly, "Hiei" was crippled by medium-caliber gunfire from heavy and light cruisers in a close-range night engagement.
There were two exceptions: Turkey's "Yavuz Sultan Selim" and the Royal Navy's "Hood". The Turkish Navy made only minor improvements to the ship in the interwar period, which primarily focused on repairing wartime damage and the installation of new fire control systems and anti-aircraft batteries. "Hood" was in constant service with the fleet and could not be withdrawn for an extended reconstruction. She received minor improvements over the course of the 1930s, including modern fire control systems, increased numbers of anti-aircraft guns, and in March 1941, radar.
Naval rearmament.
In the late 1930s navies began to build capital ships again, and during this period a number of large commerce raiders and small, fast battleships were built that are sometimes referred to as battlecruisers. Germany and Russia designed new battlecruisers during this period, though only the latter laid down two of the 35,000-ton . They were still on the slipways when the Germans invaded in 1941 and construction was suspended. Both ships were scrapped after the war.
The Germans planned three battlecruisers of the as part of the expansion of the Kriegsmarine (Plan Z). With six 15-inch guns, high speed, excellent range, but very thin armour, they were intended as commerce raiders. Only one was ordered shortly before World War II; no work was ever done on it. No names were assigned, and they were known by their contract names: 'O', 'P', and 'Q'. The new class was not universally welcomed in the Kriegsmarine. Their abnormally-light protection gained it the derogatory nickname "Ohne Panzer Quatsch" (without armour nonsense) within certain circles of the Navy.
World War II.
The Royal Navy deployed some of its battlecruisers during the Norwegian Campaign in April 1940. The and the were engaged during the Action off Lofoten by "Renown" in very bad weather and disengaged after "Gneisenau" was damaged. One of "Renown"s 15-inch shells passed through "Gneisenau"s director-control tower without exploding, severing electrical and communication cables as it went and destroyed the rangefinders for the forward 150 mm (5.9 in) turrets. Main-battery fire control had to be shifted aft due to the loss of electrical power. Another shell from "Renown" knocked out "Gneisenau"s aft turret. The British ship was struck twice by German shells that failed to inflict any significant damage. She was the only modernized battlecruiser to survive the war.
In the early years of the war various German ships had a measure of success hunting merchant ships in the Atlantic. Allied battlecruisers such as "Renown", "Repulse", and the fast battleships "Dunkerque" and were employed on operations to hunt down the commerce-raiding German ships, but they never got close to their targets. The one stand-up fight occurred when the battleship and the heavy cruiser sortied into the North Atlantic to attack British shipping and were intercepted by "Hood" and the battleship in May 1941 in the Battle of the Denmark Strait. The elderly British battlecruiser was no match for the modern German battleship: within minutes, the "Bismarck"s 15-inch shells caused a magazine explosion in "Hood" reminiscent of the Battle of Jutland. Only three men survived.
The first battlecruiser to see action in the Pacific War was "Repulse" when she was sunk by Japanese torpedo bombers north of Singapore on 10 December 1941 whilst in company with "Prince of Wales". She was lightly damaged by a single bomb and near-missed by two others in the first Japanese attack. Her speed and agility enabled her avoid the other attacks by level bombers and dodge 33 torpedoes. The last group of torpedo bombers attacked from multiple directions and "Repulse" was struck by five torpedoes. She quickly capsized with the loss of 27 officers and 486 crewmen; 42 officers and 754 enlisted men were rescued by the escorting destroyers. The loss of "Repulse" and "Prince of Wales" conclusively proved the vulnerability of capital ships to aircraft without air cover of their own.
The Japanese "Kongō"-class battlecruisers were extensively used as carrier escorts for most of their wartime career due to their high speed. Their World War I-era armament was weaker and their upgraded armour was still thin compared to contemporary battleships. On 13 November 1942, during the First Naval Battle of Guadalcanal, "Hiei" stumbled across American cruisers and destroyers at point-blank range. The ship was badly damaged in the encounter and had to be towed by her sister ship "Kirishima". Both were spotted by American aircraft the following morning and "Kirishima" was forced to cast off her tow because of repeated aerial attacks. "Hiei"s captain ordered her crew to abandon ship after further damage and scuttled "Hiei" in the early evening of 14 November. On the night of 14/15 November during the Second Naval Battle of Guadalcanal, "Kirishima" returned to Ironbottom Sound, but encountered the American battleships and . While failing to detect "Washington", "Kirishima" engaged "South Dakota" with some effect. "Washington" opened fire a few minutes later at short range and badly damaged "Kirishima", knocking out her aft turrets, jamming her rudder, and hitting the ship below the waterline. The flooding proved to be uncontrollable and "Kirishima" capsized three and a half hours later.
Returning to Japan after the Battle of Leyte Gulf, "Kongō" was torpedoed and sunk by the American submarine on 21 November 1944. "Haruna" was moored at Kure, Japan when the naval base was attacked by American carrier aircraft on 24 and 28 July. The ship was only lightly damaged by a single bomb hit on 24 July, but was hit a dozen more times on 28 July and sank at her pier. She was refloated after the war and scrapped in early 1946.
Large cruisers or "cruiser killers".
A late renaissance in popularity of ships between battleships and cruisers in size occurred on the eve of World War II. Described by some as battlecruisers, but never classified as capital ships, they were variously described as "super cruisers", "large cruisers" or even "unrestricted cruisers". The Dutch, American, and Japanese navies all planned these new classes specifically to counter the heavy cruisers, or their counterparts, being built by their naval rivals.
The first such battlecruisers were the Dutch Design 1047, desired to protect their colonies in the East Indies in the face of Japanese aggression. Never officially assigned names, these ships were designed with German and Italian assistance. While they broadly resembled the German "Scharnhorst" class and had the same main battery, they would have been more lightly armoured and only protected against eight-inch gunfire. Although the design was mostly completed, work on the vessels never commenced as the Germans overran the Netherlands in May 1940. The first ship would have been laid down in June of that year.
The only class of these late battlecruisers actually built were the United States Navy's "large cruisers". Two of them were completed, and ; a third, , was cancelled while under construction and three others, to be named "Philippines", "Puerto Rico" and "Samoa", were cancelled before they were laid down. They were classified as "large cruisers" instead of battlecruisers, and their status as non-capital ships evidenced by their being named for territories or protectorates. (Battleships, in contrast, were named after states and cruisers after cities). With a main armament of nine 12-inch guns in three triple turrets and a displacement of , the "Alaska"s were twice the size of s and had guns some 50% larger in diameter. They lacked the thick armoured belt and intricate torpedo defence system of true capital ships. However, unlike most battlecruisers, they were considered a balanced design according to cruiser standards as their protection could withstand fire from their own caliber of gun, albeit only in a very narrow range band. They were designed to hunt down Japanese heavy cruisers, though by the time they entered service most Japanese cruisers had been sunk by American aircraft or submarines. Like the contemporary fast battleships, their speed ultimately made them more useful as carrier escorts and bombardment ships than as the surface combatants they were developed to be.
The Japanese started designing the B64 class, which was similar to the "Alaska" but with guns. News of the "Alaska"s led them to upgrade the design, creating the B65. Armed with 356 mm guns, the B65's would have been the best armed of the new breed of battlecruisers, but they still would have had only sufficient protection to keep out eight-inch shells. Much like the Dutch, the Japanese got as far as completing the design for the B65s, but never laid them down. By the time the designs were ready the Japanese Navy recognized that they had little use for the vessels and that their priority for construction should lie with aircraft carriers. Like the "Alaska"s, the Japanese did not call these ships battlecruisers, referring to them instead as super-heavy cruisers.
Cold War-era designs.
In spite of the fact that after World War II, most navies abandoned the battleship and battlecruiser concepts, Joseph Stalin's fondness for big-gun-armed warships caused the Soviet Union to plan a large cruiser class in the late 1940s. In the Soviet Union they were termed "heavy cruisers" ("tjazholyj krejser"). The fruits of this program were the Project 82 ("Stalingrad") cruisers, of standard load, nine 305 mm guns and a speed of . Three ships were laid down in 1951–52, but they were cancelled in April 1953 after Stalin's death. Only the central armoured hull section of the first ship, "Stalingrad", was launched in 1954 and then used as a target.
The Soviet is sometimes referenced as a battlecruiser. This classification arises from their over displacement, which is roughly equal to that of a First World War battleship and more than twice the displacement of contemporary cruisers. The "Kirov" class lacks the armour that distinguishes battlecruisers from ordinary cruisers and they are classified as "Tyazholyy Atomnyy Raketny Kreyser" (Heavy Nuclear-powered Missile Cruiser) by Russia. Four members of the class were completed during the 1980s and 1990s, but due to budget constraints only the is operational with the Russian Navy, though plans were announced in 2010 to return the other three ships to service. As of 2012 one ship was being refitted, but the other two ships are reportedly beyond economical repair.

</doc>
<doc id="4059" url="http://en.wikipedia.org/wiki?curid=4059" title="Bob Hawke">
Bob Hawke

Robert James Lee "Bob" Hawke (born 9 December 1929) is an Australian politician who was the 23rd Prime Minister of Australia and the Leader of the Labor Party from 1983 to 1991. After a decade as President of the Australian Council of Trade Unions, he was elected to the House of Representatives as the Labor MP for Wills in 1980. Three years later, he led Labor to a landslide election victory and was sworn in as Prime Minister. He led Labor to victory at three more elections in 1984, 1987 and 1990, thus making him the most successful Labor Leader in history. Hawke was eventually replaced by Paul Keating at the end of 1991. He remains to date Labor's longest-serving Prime Minister, and is Australia's third-longest-serving Prime Minister.
Early life and education.
Hawke was born in Bordertown, South Australia to Clem, a Congregationalist minister, and his wife Edith (known as Ellie), a schoolteacher. His uncle, Albert, was the Labor Premier of Western Australia between 1953 and 1959, and was also a close friend of Prime Minister John Curtin, who was in many ways Bob Hawke's role model. Ellie Hawke had an almost messianic belief in her son's destiny, and this contributed to his supreme self-confidence throughout his career. Both his parents were of Cornish origin, and he himself has stated that his background is Cornish. This led the Cornish writer and historian A.L. Rowse to write, "Bob Hawke's characteristics are as Cornish as Australian. I know them well; the aggressive individualism, the egoism, the touchiness, the liability to resentment, even a touch of vindictiveness." Hawke says, while attending the 1952 World Christian Youth Conference in India, "there were all these poverty stricken kids at the gate of this palatial place where we were feeding our face and I just had this struck by this enormous sense of irrelevance of religion to the needs of people". He subsequently abandoned his Christian beliefs. By the time he entered politics he was a self-described agnostic. Hawke told Andrew Denton in 2008 that his father's Christian faith continued to influence his outlook however: "[My father] said if you believe in the fatherhood of God you must necessarily believe in the brotherhood of man, it follows necessarily, and even though I left the church and was not religious, that truth remained with me."
Hawke was educated at Perth Modern School and the University of Western Australia where he graduated with a Bachelor of Arts and Bachelor of Laws. At the age 15, he accurately boasted that he would one day become Prime Minister of Australia. He joined the Labor Party in 1947 at the age of 18, and successfully applied for a Rhodes Scholarship at the end of 1952. In 1953, Hawke attended University College, Oxford to commence a Bachelor of Arts in Philosophy, Politics and Economics (PPE). He soon found he was covering much the same ground as he did in his education at the University of Western Australia. Hawke then transferred to a Bachelor of Letters, with a thesis on wage-fixing in Australia which was successfully presented in January 1956.
His academic achievements were complemented by setting a new world speed record for beer drinking; he downed - equivalent to a yard of ale - from a sconce pot in 11 seconds as part of a college penalty. In his memoirs, Hawke suggested that this single feat may have contributed to his political success more than any other, by endearing him to a voting population with a strong beer culture.
At the age of 17, Hawke had a serious accident on his black Panther motorcycle that left him in a critical condition for several days. His brother Neil had died at the same age. It was this near-death experience that was his catharsis and drove him to make the most of his talents and not let his abilities go to waste.
In March 1956, Hawke married Hazel Masterson at Perth Trinity Church. They would go on to have three children: Susan Pieters-Hawke (born 1957), Stephen (born 1959) and Roslyn (born 1960). Their fourth child, Robert Jr, died in his early infancy in 1963. Hawke would later be named Victorian Father of the Year in 1971. In the same year, Hawke accepted a scholarship to undertake doctoral studies in the area of arbitration law in the law department at the Australian National University in Canberra. Soon after arrival at ANU, Hawke became the students' representative on the University Council.
In 1957, Hawke was recommended to the President of the Australian Council of Trade Unions (ACTU), Albert Monk, to become a research officer, replacing Harold Souter who had become ACTU Secretary. The recommendation was made by Hawke's mentor at ANU, H.P. Brown, who for a number of years had assisted the ACTU in national wage cases. Hawke decided to abandon his doctoral studies and accept the offer, moving to Melbourne.
ACTU President.
Not long after Hawke began work at the ACTU, he became responsible for the presentation of its annual case for higher wages to the national wages tribunal, the Conciliation and Arbitration Commission. He was first appointed as an ACTU advocate in 1959. The 1958 case, under advocate R.L. Eggleston, had yielded only a five-shilling increase. The 1959 case found for a fifteen-shilling increase, and was regarded as a personal triumph for Hawke. He went on to attain such success and prominence in his role as an ACTU advocate that, in 1969, he was encouraged to run for ACTU President, despite the fact that he had never held elected office in a trade union.
He was elected ACTU President in 1969 on a modernising platform, by a narrow margin of 399 to 350, with the support of the left of the union movement, including some associated with the Communist Party. He later credited Ray Gietzelt, General Secretary of the FMWU, as the single most significant union figure in helping him achieve this outcome.
Hawke declared publicly that "socialist is not a word I would use to describe myself", and his approach to government was pragmatic. He concerned himself with making improvements to workers' lives from within the traditional institutions of government, rather than by using any ideological theory. He opposed the Vietnam War, but was a strong supporter of the US-Australian alliance, and also an emotional supporter of Israel. It was his commitment to the cause of Jewish Refuseniks that led to a planned assassination attempt on Hawke by the Popular Front for the Liberation of Palestine, and its Australian operative Munif Mohammed Abou Rish.
In 1971, Hawke along with other members of the ACTU requested that South Africa send a non-racially biased team for the Rugby Union tour, with the intention of unions agreeing not to serve the team in Australia. Prior to arrival, the Western Australian branch of the Transport Workers Union, and the Barmaids’ and Barmens’ Union announced that they would serve the team, which allowed the Springboks to land in Perth. The tour commenced on June 26 and riots occurred as anti-apartheid protesters disrupted games. Hawke and his family started to receive malicious mail and phone calls from people who thought that sport and politics should not mix. The harassment continued from anti-Semites, for his relationship with Israel. Hawke remained committed to the ban on apartheid teams and that same year, the South African cricket team was successfully denied and no apartheid team was to ever come to Australia again. It was this ongoing dedication to racial equality in South Africa that earned Hawke the respect and friendship of Nelson Mandela.
In industrial matters, Hawke continued to demonstrate a preference for, and considerable skill at, negotiation, and was generally liked and respected by employers as well as the unions he advocated for. As early as 1972, speculation began that he would seek to enter Parliament and eventually run to become the Leader of the Labor Party. But while his professional career continued successfully, his heavy drinking and his notorious womanising placed considerable strains on his family life.
In 1973, Hawke was elected as the Labor Party's Federal President. Two years later, when the Whitlam Government was controversially dismissed by the Governor-General, Hawke showed an initial keenness to enter Parliament at the ensuing election. Harry Jenkins, the MP for Scullin, came under pressure to step down to allow Hawke to stand in his place, but he strongly resisted this push. Hawke eventually decided not to attempt to enter Parliament at that time, a decision he soon regretted. After Labor was defeated at the election, Whitlam initially offered the Labor leadership to Hawke, although it was not within Whitlam's power to decide who would succeed him. Despite not taking on the offer, Hawke remained influential, playing a key role in averting national strike action. The strain of this period took its toll, and in 1979 he suffered a physical collapse.
This shock led Hawke to make a sustained and ultimately successful effort to conquer his alcoholism – John Curtin was his inspiration in this, as in many other things. He was helped through this period by the relationship that he had established with the writer Blanche d'Alpuget, who in 1982 published an admiring biography of Hawke. His popularity with the public was unaffected by this period of rehabilitation, and opinion polling suggested that he was a far more popular public figure than either Labor Leader Bill Hayden or Liberal Prime Minister Malcolm Fraser.
Member of Parliament.
Hawke's first attempt to enter Parliament came during the 1963 federal election. He stood in the seat of Corio and managed to achieve a 3.1% swing against the national trend, although he fell short of winning the seat. Hawke passed up several opportunities to enter Parliament throughout the 1970s, something he later wrote that he "regretted". He eventually stood for election to the House of Representatives at the 1980 election for Wills, Melbourne, winning comfortably. Immediately upon his election to Parliament, Hawke was appointed to the Shadow Cabinet by Labor Leader Bill Hayden as Shadow Minister for Industrial Relations, Employment and Youth. Throughout that time, opinion polls continually indicated that, in contrast to Hayden, Hawke was regarded as "a certain election winner". After losing the 1980 election, Hayden's position as leader was never completely secure. In order to quell this constant speculation over his position, Hayden eventually called a leadership ballot for 16 July 1982, believing that if he won he would be able to lead Labor into the next election. Hawke duly challenged Hayden, but Hayden was able to defeat him and remain in his position, although his five-vote victory over the former ACTU President was not large enough to dispel doubts that he could lead the Labor Party to victory at an election.
Despite being defeated, Hawke continued to agitate behind the scenes for a change in leadership, with opinion polls continuing to show that Hawke was a far more popular figure than both Hayden and Prime Minister Malcolm Fraser. Hayden's leadership position was thrown into further doubt after Labor performed poorly in a by-election in December 1982 for the Victorian seat of Flinders, following the resignation of the former Liberal Minister Sir Phillip Lynch. Labor needed a swing of 5.5% to win the seat, and had been predicted by the media to win, but could only achieve a swing of 3%. This convinced many Labor MPs that only Hawke would be able to lead Labor to victory at the upcoming election. Labor Party power-brokers, such as Graham Richardson and Barrie Unsworth, now openly switched their allegiance from Hayden to Hawke. More significantly, Hayden's staunch friend and political ally, Labor's Senate Leader John Button, eventually became convinced that Hawke's chances of victory at an election were greater than Hayden's. Having initially believed that he could carry on, Button's defection proved to be the final straw in convincing Hayden that he would have to resign as Labor Leader. Less than two months after the disastrous showing in Flinders, Hayden announced his resignation as Labor Leader to the caucus on 3 February 1983. Hawke was subsequently named Acting Leader—and hence Leader of the Opposition—pending a party-room ballot at which he was elected unopposed. By a remarkable coincidence, on the same day that Hawke became Leader, Fraser called a snap election for 5 March 1983, hoping to both capitalise on Labor's feuding before it could replace Hayden with Hawke. Fraser initially believed that he had caught Labor out, thinking that they would be forced to fight the election with Hayden as Leader. However, he was surprised to find out that Hayden had already resigned that morning, literally hours before the writs were issued. In the election held a month later, Hawke led Labor to a landslide election victory, achieving a 24-seat swing—still the worst defeat that a sitting non-Labor government has ever suffered—and ending seven years of Liberal rule.
Prime Minister.
After Labor's landslide win, Hawke was sworn in as the 23rd Prime Minister of Australia by the Governor-General on 11 March 1983. The inaugural days of the Hawke Government were distinctly different from those of the Whitlam Government. Rather than immediately initiating extensive reform programmes as Whitlam had, Hawke announced that Malcolm Fraser's pre-election concealment of the budget deficit meant that many of Labor's election commitments would have to be deferred. As part of his internal reforms package, Hawke divided the Government into two tiers, with only the most important ministers attending regular meetings of the Cabinet. The Labor caucus were still given the authority to determine who would make up the Ministry, but gave Hawke unprecedented powers for a Labor Prime Minister to select which individual ministers would comprise the 13-strong Cabinet. Hawke said that he did this in order to avoid what he viewed as the unwieldy nature of the Whitlam Cabinet, which had 27 members. Caucus under Hawke also exhibited a much more formalised system of parliamentary factions, which significantly altered the dynamics of caucus operations.
Unlike his predecessor as Labor Leader, Hawke's authority within the Labor Party was absolute. This enabled him to persuade his MPs to support a substantial set of policy changes. Individual accounts from ministers indicate that while Hawke was not usually the driving force for economic reform – that impetus instead coming from Treasurer Paul Keating and Industry Minister John Button – he took on the role of achieving consensus and providing political guidance on what was electorally feasible and how best to sell it to the public, tasks at which he proved highly successful. Hawke took on a very public role as Prime Minister, proving to be incredibly popular with the Australian electorate; to this date he still holds the highest ever AC Nielsen approval rating.
The political partnership between Hawke and his Treasurer, Paul Keating, provided essential to their success in government. The two men proved a study in contrasts: Hawke was a Rhodes Scholar; Keating left high school early. Hawke's enthusiasms were cigars, horse racing and all forms of sport; Keating preferred classical architecture, Mahler symphonies and collecting English Regency and French Empire antiques. Hawke was consensus-driven; Keating revelled in aggressive debate. Hawke was a lapsed Protestant; Keating was a practising Catholic. These differences, however, seemed only to increase the effectiveness of their partnership, as they oversaw sweeping economic and social changes throughout Australia.
According to political commentator Paul Kelly, "the most influential economic decisions of the 1980s were the floating of the Australian dollar and the deregulation of the financial system". Although the Fraser Government had played a part in the process of financial deregulation by commissioning the 1981 Campbell Report, opposition from Fraser himself had stalled the deregulation process. When the Hawke Government implemented a comprehensive program of financial deregulation and reform, it "transformed economics and politics in Australia". The Australian economy became significantly more integrated with the global economy as a result, which completely transformed its relationship with Asia, Europe and the United States. Both Hawke and Keating would claim the credit for being the driving force behind the success of the Australian Dollar float.
Among other reforms, the Hawke Government dismantled the tariff system, privatised state sector industries, ended the subsidisation of loss-making industries, and sold off the state-owned Commonwealth Bank of Australia. The tax system was reformed, with the introduction of a fringe benefits tax and a capital gains tax, reforms strongly opposed by the Liberal Party at the time, but not ones that they reversed when they eventually returned to office. Partially offsetting these imposts upon the business community – the "main loser" from the 1985 Tax Summit according to Paul Kelly – was the introduction of full dividend imputation, a reform insisted upon by Keating. Funding for schools was also considerably increased, while financial assistance was provided for students to enable them to stay at school longer. Considerable progress was also made in directing assistance "to the most disadvantaged recipients over the whole range of welfare benefits."
Hawke benefited greatly from the disarray into which the Liberal Party fell after the resignation of Malcolm Fraser. The Liberals were divided between supporters of the dour, socially conservative John Howard and the more liberal, urbane Andrew Peacock. The arch-conservative Premier of Queensland, Joh Bjelke-Petersen, added to the Liberals' problems with his "Joh for Canberra" campaign, which proved highly damaging. Exploiting these divisions, Hawke led the Labor Party to landslide election victories in a snap 1984 election and the 1987 election.
Hawke's time as Prime Minister saw considerable friction develop between himself and the grassroots of the Labor Party, who were unhappy at what they viewed as Hawke's iconoclasm and willingness to cooperate with business interests. All Labor Prime Ministers have at times engendered the hostility of the organisational wing of the Party, but none more so than Hawke, who regularly expressed his willingness to cull Labor's "sacred cows". The Socialist Left faction, as well as prominent Labor figure Barry Jones, offered severe criticism of a number of government decisions. He also received criticism for his "confrontationalist style" in siding with the airlines in the 1989 Australian pilots' strike.
In spite of the criticisms levelled against the Hawke Government, it succeeded in enacting a wide range of social reforms during its time in office. Deflecting arguments that the Hawke Government had failed as a reform government, Neville Wran, John Dawkins, Bill Hayden and Paul Keating made a number of speeches throughout the 1980s arguing that the Hawke Government had been a recognisably reformist government, drawing attention to Hawke's achievements as Prime Minister during his first five years in office. As well as the reintroduction of Medibank, under the new name Medicare, these included the doubling of child care places, the introduction of occupational superannuation, a boost in school retention rates, a focus on young people's job skills, a doubling of subsidised home care services, the elimination of poverty traps in the welfare system, a 50% increase in public housing funds, an increase in the real value of the old-age pension, the development of a new youth support program, the re-introduction of six-monthly indexation of single adult unemployment benefits, and significant improvements in social security provisions. As pointed out by John Dawkins, the proportion of total government outlays allocated to families, the sick, single parents, widows, the handicapped, and veterans was significantly higher under the Hawke Government than under the Whitlam Government.
Another notable success for which Hawke's response is given considerable credit was Australia's public health campaign regarding AIDS. In the later years of the Hawke Government, Aboriginal affairs also saw considerable attention, with an investigation of the idea of a treaty between Aborigines and the Government, although this idea would be overtaken by events, notably the Mabo court decision.
The Hawke Government also made some notable environmental decisions. In its first months in office it halted the construction of the Franklin Dam in Tasmania, responding to a groundswell of protest about the issue. In 1990, with an election looming, tough political operator Graham Richardson was appointed Environment Minister, and was given the task of attracting second-preference votes from the Australian Democrats and other environmental parties. Richardson claimed this as a major factor in the government's narrow re-election at the 1990 election.
Richardson felt that the importance of his contribution to Labor's victory would automatically entitle him to the ministerial portfolio of his choice, which was Transport and Communications. He was shocked, however, at what he perceived as Hawke's ingratitude in allocating him Social Security instead. He later vowed in a telephone conversation with Peter Barron, a former Hawke staffer, to do "whatever it takes" to "get" Hawke. He immediately transferred his allegiance to Paul Keating, who after seven years as Treasurer was openly coveting the leadership.
The late 1980s recession and accompanying high interest rates had seen the government in considerable electoral trouble, with many doubting if Hawke could win in 1990. Although Keating was the main architect of the government's economic policies, he took advantage of Hawke's declining popularity to plan a leadership challenge. In 1988, in the wake of poorer opinion polls, Keating put pressure on Hawke to step down immediately. Hawke responded by agreeing a secret deal with Keating, the so-called "Kirribilli Agreement", that he would stand down in Keating's favour shortly after the 1990 election, which he convinced Keating he could win. Hawke duly won the 1990 election, albeit by a very tight margin, and subsequently appointed Keating as Deputy Prime Minister to replace the retiring Lionel Bowen, and to prepare Keating to assume the leadership.
Not long after becoming Deputy Prime Minister, frustrated at the lack of any indication from Hawke as to when he might step down, Keating made a provocative speech to the Federal Parliamentary Press Gallery. Hawke considered the speech extremely disloyal, and subsequently indicated to Keating that he would renege on the Kirribilli Agreement as a result. After this disagreement tensions between the two men reached an all-time high, and after a turbulent year, Keating finally resigned as Deputy Prime Minister and Treasurer in June 1991, to challenge Hawke for the leadership. Hawke comfortably defeated Keating, and in a press conference after the result Keating declared that with regards the leadership, he had fired his "one shot". Hawke appointed John Kerin to replace Keating as Treasurer, but Kerin quickly proved to be unequal to the job. In spite of his convincing win over Keating, Hawke was seen after the result as a "wounded" leader; he had now lost his long-term political partner, his rating in opinion polls began to decrease, and after nearly nine years as Prime Minister, many were openly speculating that he was "tired", and that it was time for somebody new.
Hawke's leadership was finally irrevocably damaged towards the end of 1991, as new Liberal Leader John Hewson released 'Fightback!', a detailed proposal for sweeping economic change, including the introduction of a goods and services tax and deep cuts to government spending and personal income tax. The package appeared to take Hawke by complete surprise, and his response to it was judged to be extremely ineffective. Many within the Labor Party appeared to lose faith in him over this, and Keating duly challenged for the leadership a second time on 19 December 1991, this time narrowly defeating Hawke by 56 votes to 51. In a speech to the House of Representatives the following day, Hawke declared that his nine years as Prime Minister had left Australia a better country than he found, and he was given a standing ovation by those present. He subsequently tendered his resignation as Prime Minister to the Governor-General. Hawke briefly returned to the backbenches before resigning from Parliament on 20 February 1992, sparking a by-election which was won by independent Phil Cleary from a record field of 22 candidates.
Hawke wrote that he had very few regrets over his time in office; although his bitterness towards Keating surfaced in his earlier memoirs, by 2010, Hawke said that he and Keating had long since buried their differences, and that they regularly dined together and considered each other friends.
Retirement and later life.
After leaving Parliament, Hawke entered the business world, taking on a number of directorships and consultancy positions, roles in which he was able to achieve considerable success. He and Hazel Hawke divorced in 1995; she had tolerated his open relationship with Blanche d'Alpuget whilst he was Prime Minister, but the marriage ended three years after his retirement from politics. Shortly afterwards, Hawke married d'Alpuget. Hazel Hawke died on 23 May 2013 following complications of Alzheimer's disease.
Hawke deliberately had little involvement with the Labor Party during Keating's time as Prime Minister, not wanting to overshadow his successor, although he did occasionally criticise some of Keating's policies publicly. After Keating's defeat and the election of the Howard Government at the 1996 election, he began to be more involved with Labor again, and he has since regularly appeared at a number of official Labor launches and campaigns, often alongside Keating.
In the run up to the 2007 election, Hawke made a considerable personal effort to support Kevin Rudd, making speeches at a large number of campaign office openings across Australia. As well as campaigning against WorkChoices, Hawke also attacked John Howard's record as Treasurer, stating "it was the judgement of every economist and international financial institution that it was the restructuring reforms undertaken by my government, with the full cooperation of the trade union movement, which created the strength of the Australian economy today". Similarly, in the 2010 and 2013 campaigns, Hawke leant considerable support to Julia Gillard and Kevin Rudd respectively. Hawke also maintained an involvement in Labor politics at a state level; in 2011, Hawke publicly supported NSW Premier Kristina Keneally, who was facing almost certain defeat, in her campaign against Liberal Barry O'Farrell, describing her campaign as "gutsy".
In February 2008, Hawke joined former Prime Ministers Gough Whitlam, Malcolm Fraser and Paul Keating in Parliament House to witness Prime Minister Kevin Rudd deliver the long anticipated apology to the Stolen Generations.
In 2009, Hawke helped establish the Centre for Muslim and Non-Muslim Understanding at the University of South Australia. Interfaith dialogue was an important issue for Hawke, who told the "Adelaide Review" that he is "convinced that one of the great potential dangers confronting the world is the lack of understanding in regard to the Muslim world. Fanatics have misrepresented what Islam is. They give a false impression of the essential nature of Islam."
Film.
A biographical television film, "Hawke", premiered on the Ten Network in Australia on 18 July 2010, with Richard Roxburgh playing the title character. Rachael Blake and Felix Williamson portrayed Hazel Hawke and Paul Keating respectively. Patrick Brammall starred as then Deputy Prime Minister Kim Beazley.

</doc>
<doc id="4060" url="http://en.wikipedia.org/wiki?curid=4060" title="Baldr">
Baldr

Baldr (also Balder, Baldur) is a god of light and purity in Norse mythology, and a son of the god Odin and the goddess Frigg. He has numerous brothers, such as Thor and Váli.
In the 12th century, Danish accounts by Saxo Grammaticus and other Danish Latin chroniclers recorded a euhemerized account of his story. Compiled in Iceland in the 13th century, but based on much older Old Norse poetry, the Poetic Edda and the Prose Edda contain numerous references to the death of Baldr as both a great tragedy to the Æsir and a harbinger of Ragnarök.
According to "Gylfaginning", a book of Snorri Sturluson's Prose Edda, Baldr's wife is Nanna and their son is Forseti. In "Gylfaginning", Snorri relates that Baldr had the greatest ship ever built, named Hringhorni, and that there is no place more beautiful than his hall, Breidablik.
Name.
Jacob Grimm in his "Teutonic Mythology" (ch. 11) identifies Old Norse "Baldr" with the Old High German "Baldere" (2nd Merseburg Charm, Thuringia), "Palter" (theonym, Bavaria), "Paltar" (personal name) and with Old English "bealdor, baldor" "lord, prince, king" (used always with a genitive plural, as in "gumena baldor" "lord of men", "wigena baldor" "lord of warriors", et cetera). Old Norse shows this usage of the word as an honorific in a few cases, as in "baldur î brynju" (Sæm. 272b) and "herbaldr" (Sæm. 218b), both epithets of heroes in general.
Grimm traces the etymology of the name to *"balþaz", whence Gothic "balþs", Old English "bald", Old High German "pald", all meaning "bold, brave".
But the interpretation of Baldr as "the brave god" may be secondary. Baltic (cf. Lithuanian "baltas", Latvian "balts") has a word meaning "the white, the good", and Grimm speculates that the name may originate as a Baltic loan into Proto-Germanic.
In continental Saxon and Anglo-Saxon tradition, the son of Woden is called not "Bealdor" but "Baldag" (Sax.) and "Bældæg, Beldeg" (AS.), which shows association with "day", possibly with Day personified as a deity which, Grimm points out, would agree with the meaning "shining one, white one, a god" derived from the meaning of Baltic "baltas", further adducing Slavic "Belobog" and German "Berhta".
Attestations.
Merseburg Incantation.
One of the two Merseburg Incantations names Balder, and mentions a figure named "Phol", considered to be another name for Baldr (as in Scandinavian, "Falr", "Fjalarr"; (in Saxo) "Balderus" : "Fjallerus").
"Poetic Edda".
In the Poetic Edda the tale of Baldr's death is referred to rather than recounted at length. Among the visions which the Völva sees and describes in the prophecy known as the "Völuspá" is one of the fatal mistletoe, the birth of Váli and the weeping of Frigg (stanzas 31-33). Yet looking far into the future the Völva sees a brighter vision of a new world, when both Höðr and Baldr will come back (stanza 62). The Eddic poem "Baldr's Dreams" mentions that Baldr has bad dreams which the gods then discuss. Odin rides to Hel and awakens a seeress, who tells him Höðr will kill Baldr but Vali will avenge him (stanzas 9, 11).
"Prose Edda".
In "Gylfaginning", Baldur is described as follows:
Apart from this description Baldr is known primarily for the story of his death. His death is seen as the first in the chain of events which will ultimately lead to the destruction of the gods at Ragnarök. Baldr will be reborn in the new world, according to "Völuspá".
He had a dream of his own death and his mother had the same dreams. Since dreams were usually prophetic, this depressed him, so his mother Frigg made every object in every realm vow never to hurt Baldr. All objects made this vow except mistletoe. Frigg had thought it too young to swear—a detail which has traditionally been explained with the idea that it was too unimportant and unthreatening to bother asking it to make the vow, but which Merrill Kaplan has instead argued echoes that fact that young people were not eligible to swear legal oaths, which could make them a threat later in life.
When Loki, the mischief-maker, heard of this, he made a magical spear from this plant (in some later versions, an arrow). He hurried to the place where the gods were indulging in their new pastime of hurling objects at Baldr, which would bounce off without harming him. Loki gave the spear to Baldr's brother, the blind god Höðr, who then inadvertently killed his brother with it (other versions suggest that Loki guided the arrow himself). For this act, Odin and the giantess Rindr gave birth to Váli who grew to adulthood within a day and slew Höðr.
Baldr was ceremonially burnt upon his ship, Hringhorni, the largest of all ships. As he was carried to the ship, Odin whispered in his ear. The question of what he said was to be a key riddle asked by Odin (in disguise) of the giant Vafthrudnir (and which was, of course, unanswerable) in the poem "Vafthrudnismal". The riddle also appears in the riddles of Gestumblindi in "Hervarar saga".
The dwarf Litr was kicked by Thor into the funeral fire and burnt alive. Nanna, Baldr's wife, also threw herself on the funeral fire to await Ragnarök when she would be reunited with her husband (alternatively, she died of grief). Baldr's horse with all its trappings was also burned on the pyre. The ship was set to sea by Hyrrokin, a giantess, who came riding on a wolf and gave the ship such a push that fire flashed from the rollers and all the earth shook.
Upon Frigg's entreaties, delivered through the messenger Hermod, Hel promised to release Baldr from the underworld if all objects alive and dead would weep for him. All did, except a giantess, Þökk often presumed to be the god Loki in disguise, who refused to mourn the slain god. Thus Baldr had to remain in the underworld, not to emerge until after Ragnarök, when he and his brother Höðr would be reconciled and rule the new earth together with Thor's sons.
"Gesta Danorum".
Writing at about the end of the 12th century, the Danish historian Saxo Grammaticus tells the story of Baldr (recorded as "Balderus") in a form which professes to be historical. According to him, Balderus and Høtherus were rival suitors for the hand of Nanna, daughter of Gewar, King of Norway. Balderus was a demigod and common steel could not wound his sacred body. The two rivals encountered each other in a terrific battle. Though Odin and Thor and the rest of the gods fought for Balderus, he was defeated and fled away, and Høtherus married the princess.
Nevertheless Balderus took heart of grace and again met Høtherus in a stricken field. But he fared even worse than before. Høtherus dealt him a deadly wound with a magic sword, named Mistletoe, which he had received from Miming, the satyr of the woods; after lingering three days in pain Balderus died of his injury and was buried with royal honours in a barrow.
"Chronicon Lethrense" and "Annales Lundenses".
There are also two lesser known Danish Latin chronicles, the "Chronicon Lethrense" and the "Annales Lundenses" of which the latter is included in the former. These two sources provide a second euphemerized account of Höðr's slaying of Baldr.
It relates that Hother was the king of the Saxons and son of Hothbrod and the daughter of Hadding. Hother first slew Othen's (i.e. Odin) son Balder in battle and then chased Othen and Thor. Finally, Othen's son Both killed Hother. Hother, Balder, Othen and Thor were incorrectly considered to be gods.
Utrecht Inscription.
A Latin votive inscription from Utrecht, from the 3rd or 4th century C.E., has been theorized as containing the dative form "Baldruo", pointing to a Latin nominative singular *"Baldruus", which some have identified with the Norse/Germanic god, although both the reading and this interpretation have been questioned.
Eponyms.
Plants.
As referenced in "Gylfaginning", in Sweden and Norway, the scentless mayweed ("Matricaria perforata") and the similar sea mayweed ("Matricaria maritima") are both called "baldursbrá" "Balder's brow" and regionally in northern England ("baldeyebrow"). In Iceland only the former is found. In Germany lily-of-the-valley is known as "weisser Baldrian"; variations using or influenced by reflexes of "Phol" include "Faltrian" (upper Austria), Villum"fallum" (Salzburg), and "Fildron" or "Faldron" (Tyrol).
Toponyms.
There are a few old place names in Scandinavia that contain the name "Baldr". The most certain and notable one is the (former) parish name Balleshol in Hedmark county, Norway: "a Balldrshole" 1356 (where the last element is "hóll" m "mound; small hill"). Others may be (in Norse forms) "Baldrsberg" in Vestfold county, "Baldrsheimr" in Hordaland county "Baldrsnes" in Sør-Trøndelag county — and (very uncertain) the Balsfjorden fjord and Balsfjord municipality in Troms county.
In Copenhagen, there is also a Baldersgade, or "Balder's Street." A street in downtown Reykjavík is called Baldursgata (Baldur's Street).
In Belgium, the name "Balder" is also used in dialect for a village called Berlaar and in another village (Tielen), the "Balderij" is a street and a swampy area next to it.
In Yorkshire there are Baldersby and Pule Hill (from Phol).
In Nottinghamshire there is a village called Balderton, originally a vineyard. This is also mentioned also in the Doomsday book.
Baldur, Manitoba is a village in southern Manitoba, Canada. About 1890, Sigurdur Christopherson could not find a suitable flower in the district to name the town after, so he suggested the name of a beautiful Nordic God, namely Baldur, son of Odin.

</doc>
<doc id="4061" url="http://en.wikipedia.org/wiki?curid=4061" title="Breidablik">
Breidablik

In Norse mythology, Breiðablik ("Broad-gleaming") is the home of Baldr. It is briefly described in Snorri Sturluson's "Gylfaginning" as one of the halls of Asgard:
Later in the work, when Snorri describes Baldr, he gives a longer description, citing "Grímnismál", though he does not name the poem:
Breiðablik is not otherwise mentioned in the Eddic sources.

</doc>
<doc id="4062" url="http://en.wikipedia.org/wiki?curid=4062" title="Bilskirnir">
Bilskirnir

Bilskirnir (Old Norse "lightning-crack") is the hall of the god Thor in Norse mythology. Here he lives with his wife Sif and their children. According to "Grímnismál", the hall is the greatest of buildings and contains 540 rooms, located in Asgard, as are all the dwellings of the gods, in the kingdom of Þrúðheimr (or Þrúðvangar according to "Gylfaginning" and "Ynglinga saga").

</doc>
<doc id="4063" url="http://en.wikipedia.org/wiki?curid=4063" title="Brísingamen">
Brísingamen

In Norse mythology, Brísingamen (or Brísinga men) is the torc or necklace of the goddess Freyja. The name is an Old Norse compound "brísinga-men" whose second element is "men" "(ornamental) neck-ring (of precious metal), torc". The etymology of the first element is uncertain. It has been derived from Old Norse "brísingr", a poetic term for "fire" mentioned in the anonymous versified word-lists ("þulur") appended to many manuscripts of the Prose Edda, making Brísingamen "gleaming torc", "sunny torc", or the like. However, "Brísingr" can also be an ethnonym, in which case "Brísinga men" is "torque of the Brísings"; the Old English parallel in "Beowulf" supports this derivation, though who the Brísings (Old Norse "Brísingar") may have been remains unknown.
Attestations.
"Beowulf".
Brísingamen is referred to in the Anglo-Saxon epic "Beowulf" as "Brosinga mene". The brief mention in "Beowulf" is as follows (trans. by Howell Chickering, 1977):
"...since Hama bore off<br>to the shining city the Brosings' necklace, <br>Gem-figured filigree. He gained the hatred <br>Of Eormanric the Goth, chose eternal reward."
This seems to confuse two different stories as the "Beowulf" poet is clearly referring to the "Dietrich Cycle". The "Þiðrekssaga" tells that the warrior Heime ("Hama" in Old English) takes sides against Eormanric, king of the Goths, and has to flee his kingdom after robbing him; later in life, Hama enters a monastery and gives them all his stolen treasure. However, this saga makes no mention of the great necklace. Possibly the "Beowulf" poet was confused, or invented the addition of the necklace to give him an excuse to drag in a mention of Eormanric. In any case, the necklace given to Beowulf in the story is not the Brísingamen itself; it is only being compared to it.
"Poetic Edda".
In the poem "Þrymskviða" of the "Poetic Edda", Thrymr, the King of the jötuns, steals Thor's hammer, Mjölnir. Freyja lends Loki her falcon cloak to search for it; but upon returning, Loki tells Freyja that Thrymr has hidden the hammer and demanded to marry her in return. Freyja is so wrathful that all the Æsir’s halls beneath her are shaken and the necklace Brísingamen breaks off from her neck. Later Thor borrows Brísingamen when he dresses up as Freyja to go to the wedding at Jötunheim.
This myth is also recorded in an 18th-century Swedish folksong called "Hammar-Hemtningen" (the taking of the hammer), where Freyja is called Miss Frojenborg, "den väna solen" (the fair sun).
"Prose Edda".
"Húsdrápa", a skaldic poem partially preserved in the "Prose Edda", relates the story of the theft of Brísingamen by Loki. One day when Freyja wakes up and finds Brísingamen missing, she enlists the help of Heimdall to help her search for it. Eventually they find the thief, who turns out to be Loki who has transformed himself into a seal. Heimdall turns into a seal as well and fights Loki. After a lengthy battle at Singasteinn, Heimdall wins and returns Brísingamen to Freyja.
Snorri Sturluson quoted this old poem in "Skáldskaparmál", saying that because of this legend Heimdall is called "Seeker of Freyja's Necklace" ("Skáldskaparmál", section 8) and Loki is called "Thief of Brísingamen" ("Skáldskaparmál", section 16). A similar story appears in the later "Sörla þáttr", where Heimdall does not appear.
"Sörla þáttr".
Sörla þáttr is a short story in the later and extended version of the "Saga of Olaf Tryggvason" in the manuscript of the "Flateyjarbók", which was written and compiled by two Christian priests, Jon Thordson and Magnus Thorhalson, in the late 14th century. In the end of the story, the arrival of Christianity dissolves the old curse that traditionally was to endure until Ragnarök.
Freyja was a human in Asia and was the favorite concubine of Odin, King of Asialand. When this woman wanted to buy a golden necklace (no name given) forged by four dwarves (named Dvalinn, Alfrik, Berling, and Grer), she offered them gold and silver but they replied that they would only sell it to her if she would lie a night by each of them. She came home afterward with the necklace and kept silent as if nothing happened. But a man called Loki somehow knew it, and came to tell Odin. King Odin commanded Loki to steal the necklace, so Loki turned into a fly to sneak into Freyja's bower and stole it. When Freyja found her necklace missing, she came to ask king Odin. In exchange for it, Odin ordered her to make two kings, each served by twenty kings, fight forever unless some christened men so brave would dare to enter the battle and slay them. She said yes, and got that necklace back. Under the spell, king Högni and king Heðinn battled for one hundred and forty-three years, as soon as they fell down they had to stand up again and fight on. But in the end, the Christian lord Olaf Tryggvason, who has a great fate and luck, arrived with his christened men, and whoever slain by a Christian would stay dead. Thus the pagan curse was finally dissolved by the arrival of Christianity. After that, the noble man, king Olaf, went back to his realm.
The battle of Högni and Heðinn is recorded in several medieval sources, including the skaldic poem "Ragnarsdrápa", "Skáldskaparmál" (section 49), and "Gesta Danorum": king Högni's daughter, Hildr, is kidnapped by king Heðinn. When Högni comes to fight Heðinn on an island, Hildr comes to offer her father a necklace on behalf of Heðinn for peace; but the two kings still battle, and Hildr resurrects the fallen to make them fight until Ragnarök. None of these earlier sources mentions Freyja or king Olaf Tryggvason, the historical figure who Christianized Norway and Iceland in the 10th Century.
Archaeological record.
A pagan völva was buried with considerable splendour in Hagebyhöga in Östergötland. In addition to being buried with her wand, she had received great riches which included horses, a wagon and an Arabian bronze pitcher. There was also a silver pendant, which represents a woman with a broad necklace around her neck. This kind of necklace was only worn by the most prominent women during the Iron Age and some have interpreted it as Freyja's necklace Brísingamen. The pendant may represent Freyja herself.
Modern influence.
Alan Garner wrote a children's fantasy novel called "The Weirdstone of Brisingamen" about an enchanted teardrop bracelet.
Diana Paxson's novel "Brisingamen" features Freyja and her bracelet.
Black Phoenix Alchemy Lab has a perfumed oil scent named Brisingamen.
Freyja's necklace Brisingamen features prominently in Betsy Tobin's novel "Iceland", where the necklace is seen to have significant protective powers.
J.R.R. Tolkien's "Silmarillion" includes a treasure called The Nauglamir which was made by the dwarves of Ered Luin for the Elvish King Finrod Felagund. However, the necklace was brought out a dragon's hoard by Turin Turambar and given to King Thingol of Doriath. This king asks a group of dwarves to set a Silmaril into the necklace for his wife Melian to wear. The dwarves fall under the spell of the Silmaril and they claim the Nauglamir as their own – with the Silmaril attached. They kill Thingol and make off with the necklace. It is eventually recovered and is an heirloom of Thingol's descendants, eventually leading Earendil to Valinor and resulting in the return of the Valar into the affairs of Middle Earth. This is clearly intended to be the equivalent in his mythology to the Brisingamen.
In Christopher Paolini's "Inheritance Cycle", the word "brisingr" means fire. This is probably a distillation of the word "brisinga".
Brisingamen is represented as a card in the Yu-Gi-Oh Trading Card Game, "Nordic Relic Brisingamen".
Brisingamen was part of MMORPG Ragnarok Online lore, which is ranked as "God item". The game is heavily based from Norse mythology.

</doc>
<doc id="4064" url="http://en.wikipedia.org/wiki?curid=4064" title="Borsuk–Ulam theorem">
Borsuk–Ulam theorem

In mathematics, the Borsuk–Ulam theorem, named after Stanislaw Ulam and Karol Borsuk, states that every continuous function from an "n"-sphere into Euclidean "n"-space maps some pair of antipodal points to the same point.
Here, two points on a sphere are called antipodal if they are in exactly opposite directions from the sphere's center.
According to (), the first historical mention of the statement of this theorem appears in (). The first proof was given by (), where the formulation of the problem was attributed to Ulam. Since then, many alternative proofs have been found by various authors, as collected in ().
Theorem.
We use the stronger statement that every odd (antipodes-preserving) mapping "h" : S"n−1" → S"n−1" has odd degree.
Using the above theorem it is easy to see that the original Borsuk Ulam statement is correct since if we take a map "f" : S"n" → R"n" that does not equalize on any antipodes then we can construct a map "g" : S"n" → S"n−1" by the formula
since "f" never equalizes antipodes the denominator never vanishes. Note that "g" is an antipode preserving map. Now let "h" : S"n−1" → S"n−1" be the restriction of "g" to the equator. By construction, "h" is antipode-preserving, and thus has non-zero degree. By construction, "h" extends to the whole upper hemisphere of S"n", and as such is null-homotopic. A null-homotopic map has degree zero, contradicting our only assumption, namely that "f" exists.

</doc>
<doc id="4067" url="http://en.wikipedia.org/wiki?curid=4067" title="Bragi">
Bragi

Bragi is the skaldic god of poetry in Norse mythology.
Etymology.
"Bragi" is generally associated with "bragr", the Norse word for poetry. The name of the god may have been derived from "bragr", or the term "bragr" may have been formed to describe 'what Bragi does'. A connection between the name Bragi and Old English "brego" 'chieftain' has been suggested but is generally now discounted. A connection between Bragi and the "bragarfull" 'promise cup' is sometimes suggested, as "bragafull", an alternate form of the word, might be translated as 'Bragi's cup'. See Bragarfull.
Attestations.
Snorri Sturluson writes in the "Gylfaginning" after describing Odin, Thor, and Baldr:
In "Skáldskaparmál" Snorri writes:
That Bragi is Odin's son is clearly mentioned only here and in some versions of a list of the sons of Odin (see Sons of Odin). But "wish-son" in stanza 16 of the "Lokasenna" could mean "Odin's son" and is translated by Hollander as "Odin's kin". Bragi's mother is never named. If Bragi's mother is Frigg, then Frigg is somewhat dismissive of Bragi in the "Lokasenna" in stanza 27 when Frigg complains that if she had a son in Ægir's hall as brave as Baldr then Loki would have to fight for his life.
In that poem Bragi at first forbids Loki to enter the hall but is overruled by Odin. Loki then gives a greeting to all gods and goddesses who are in the hall save to Bragi. Bragi generously offers his sword, horse, and an arm ring as peace gift but Loki only responds by accusing Bragi of cowardice, of being the most afraid to fight of any of the Æsir and Elves within the hall. Bragi responds that if they were outside the hall, he would have Loki's head, but Loki only repeats the accusation. When Bragi's wife Iðunn attempts to calm Bragi, Loki accuses her of embracing her brother's slayer, a reference to matters that have not survived. It may be that Bragi had slain Iðunn's brother.
A passage in the "Poetic Edda" poem "Sigrdrífumál" describes runes being graven on the sun, on the ear of one of the sun-horses and on the hoofs of the other, on Sleipnir's teeth, on bear's paw, on eagle's beak, on wolf's claw, and on several other things including on Bragi's tongue. Then the runes are shaved off and the shavings are mixed with mead and sent abroad so that Æsir have some, Elves have some, Vanir have some, and Men have some, these being beech runes and birth runes, ale runes, and magic runes. The meaning of this is obscure.
The first part of Snorri Sturluson's "Skáldskaparmál" is a dialogue between Ægir and Bragi about the nature of poetry, particularly skaldic poetry. Bragi tells the origin of the mead of poetry from the blood of Kvasir and how Odin obtained this mead. He then goes on to discuss various poetic metaphors known as "kennings".
Snorri Sturluson clearly distinguishes the god Bragi from the mortal skald Bragi Boddason whom he often mentions separately. Bragi Boddason is discussed below. The appearance of Bragi in the "Lokasenna" indicates that if these two Bragis were originally the same, they have become separated for that author also, or that chronology has become very muddled and Bragi Boddason has been relocated to mythological time. Compare the appearance of the Welsh Taliesin in the second branch of the Mabinogi. Legendary chronology sometimes does become muddled. Whether Bragi the god originally arose as a deified version of Bragi Boddason was much debated in the 19th century, especially by the German scholars Eugen Mogk and Sophus Bugge. The debate remains undecided.
In the poem "Eiríksmál" Odin, in Valhalla, hears the coming of the dead Norwegian king Eric Bloodaxe and his host, and bids the heroes Sigmund and Sinfjötli rise to greet him. Bragi is then mentioned, questioning how Odin knows that it is Eric and why Odin has let such a king die. In the poem "Hákonarmál", Hákon the Good is taken to Valhalla by the valkyrie Göndul and Odin sends Hermóðr and Bragi to greet him. In these poems Bragi could be either a god or a dead hero in Valhalla. Attempting to decide is further confused because "Hermóðr" also seems to be sometimes the name of a god and sometimes the name of a hero. That Bragi was also the first to speak to Loki in the "Lokasenna" as Loki attempted to enter the hall might be a parallel. It might have been useful and customary that a man of great eloquence and versed in poetry should greet those entering a hall.
Skalds named Bragi.
Bragi Boddason.
In the "Prose Edda" Snorri Sturluson quotes many stanzas attributed to Bragi Boddason the old ("Bragi Boddason inn gamli"), a Norwegian court poet who served several Swedish kings, Ragnar Lodbrok, Östen Beli and Björn at Hauge who reigned in the first half of the 9th century. This Bragi was reckoned as the first skaldic poet, and was certainly the earliest skaldic poet then remembered by name whose verse survived in memory.
Snorri especially quotes passages from Bragi's "Ragnarsdrápa", a poem supposedly composed in honor of the famous legendary Viking Ragnar Lodbrók ('Hairy-breeches') describing the images on a decorated shield which Ragnar had given to Bragi. The images included Thor's fishing for Jörmungandr, Gefjun's ploughing of Zealand from the soil of Sweden, the attack of Hamdir and Sorli against King Jörmunrekk, and the never-ending battle between Hedin and Högni.
Bragi son of Hálfdan the Old.
Bragi son of Hálfdan the Old is mentioned only in the "Skjáldskaparmál". This Bragi is the sixth of the second of two groups of nine sons fathered by King Hálfdan the Old on Alvig the Wise, daughter of King Eymund of Hólmgard. This second group of sons are all eponymous ancestors of legendary families of the north. Snorri says:
Bragi, from whom the Bragnings are sprung (that is the race of Hálfdan the Generous).
Of the Bragnings as a race and of Hálfdan the Generous nothing else is known. However, "Bragning" is often, like some others of these dynastic names, used in poetry as a general word for 'king' or 'ruler'.
Bragi Högnason.
In the eddic poem "Helgakviða Hundingsbana II", Bragi Högnason, his brother Dag, and his sister Sigrún were children of Högne, the king of East Götaland. The poem relates how Sigmund's son Helgi Hundingsbane agreed to take Sigrún daughter of Högni as his wife against her unwilling betrothal to Hodbrodd son of Granmar the king of Södermanland. In the subsequent battle of Frekastein (probably one of the 300 hill forts of Södermanland, as "stein" meant "hill fort") against Högni and Grammar, all the chieftains on Granmar's side are slain, including Bragi, except for Bragi's brother Dag.

</doc>
<doc id="4068" url="http://en.wikipedia.org/wiki?curid=4068" title="Blaise Pascal">
Blaise Pascal

 Blaise Pascal (; 19 June 1623 – 19 August 1662) was a French mathematician, physicist, inventor, writer and Christian philosopher. He was a child prodigy who was educated by his father, a tax collector in Rouen. Pascal's earliest work was in the natural and applied sciences where he made important contributions to the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli. Pascal also wrote in defense of the scientific method.
In 1642, while still a teenager, he started some pioneering work on calculating machines. After three years of effort and fifty prototypes, he was one of the first two inventors of the mechanical calculator. He built 20 of these machines (called Pascal's calculators and later Pascalines) in the following ten years. Pascal was an important mathematician, helping create two major new areas of research: he wrote a significant treatise on the subject of projective geometry at the age of 16, and later corresponded with Pierre de Fermat on probability theory, strongly influencing the development of modern economics and social science. Following Galileo and Torricelli, in 1646, he refuted Aristotle's followers who insisted that nature abhors a vacuum. Pascal's results caused many disputes before being accepted.
In 1646, he and his sister Jacqueline identified with the religious movement within Catholicism known by its detractors as Jansenism. His father died in 1651. Following a religious experience in late 1654, he began writing influential works on philosophy and theology. His two most famous works date from this period: the "Lettres provinciales" and the "Pensées", the former set in the conflict between Jansenists and Jesuits. In that year, he also wrote an important treatise on the arithmetical triangle. Between 1658 and 1659 he wrote on the cycloid and its use in calculating the volume of solids.
Pascal had poor health, especially after his 18th year, and his death came just two months after his 39th birthday.
Early life and education.
Pascal was born in Clermont-Ferrand; he lost his mother, Antoinette Begon, at the age of three. His father, Étienne Pascal (1588–1651), who also had an interest in science and mathematics, was a local judge and member of the "Noblesse de Robe". Pascal had two sisters, the younger Jacqueline and the elder Gilberte.
In 1631, five years after the death of his wife, Étienne Pascal moved with his children to Paris. The newly arrived family soon hired Louise Delfault, a maid who eventually became an instrumental member of the family. Étienne, who never remarried, decided that he alone would educate his children, for they all showed extraordinary intellectual ability, particularly his son Blaise. The young Pascal showed an amazing aptitude for mathematics and science.
Particularly of interest to Pascal was a work of Desargues on conic sections. Following Desargues' thinking, the 16-year-old Pascal produced, as a means of proof, a short treatise on what was called the "Mystic Hexagram", "Essai pour les coniques" ("Essay on Conics") and sent it—his first serious work of mathematics—to Père Mersenne in Paris; it is known still today as Pascal's theorem. It states that if a hexagon is inscribed in a circle (or conic) then the three intersection points of opposite sides lie on a line (called the Pascal line).
Pascal's work was so precocious that Descartes was convinced that Pascal's father had written it. When assured by Mersenne that it was, indeed, the product of the son not the father, Descartes dismissed it with a sniff: "I do not find it strange that he has offered demonstrations about conics more appropriate than those of the ancients," adding, "but other matters related to this subject can be proposed that would scarcely occur to a 16-year-old child."
In France at that time offices and positions could be—and were—bought and sold. In 1631 Étienne sold his position as second president of the "Cour des Aides" for 65,665 livres. The money was invested in a government bond which provided if not a lavish then certainly a comfortable income which allowed the Pascal family to move to, and enjoy, Paris. But in 1638 Richelieu, desperate for money to carry on the Thirty Years' War, defaulted on the government's bonds. Suddenly Étienne Pascal's worth had dropped from nearly 66,000 livres to less than 7,300.
Like so many others, Étienne was eventually forced to flee Paris because of his opposition to the fiscal policies of Cardinal Richelieu, leaving his three children in the care of his neighbor Madame Sainctot, a great beauty with an infamous past who kept one of the most glittering and intellectual salons in all France. It was only when Jacqueline performed well in a children's play with Richelieu in attendance that Étienne was pardoned. In time Étienne was back in good graces with the cardinal, and in 1639 had been appointed the king's commissioner of taxes in the city of Rouen — a city whose tax records, thanks to uprisings, were in utter chaos.
In 1642, in an effort to ease his father's endless, exhausting calculations, and recalculations, of taxes owed and paid (into which work the young Pascal had been recruited), Pascal, not yet 19, constructed a mechanical calculator capable of addition and subtraction, called Pascal's calculator or the Pascaline. Of the eight Pascalines known to have survived, four are held by the Musée des Arts et Métiers in Paris and one more by the Zwinger museum in Dresden, Germany, exhibit two of his original mechanical calculators. Though these machines are pioneering forerunners to a further 400 years of development of mechanical methods of calculation, and in a sense to the later field of computer engineering, the calculator failed to be a great commercial success. Partly because it was still quite cumbersome to use in practice, but probably primarily because it was extraordinarily expensive the Pascaline became little more than a toy, and status symbol, for the very rich both in France and elsewhere in Europe. Pascal continued to make improvements to his design through the next decade and he refers to some 50 machines that were built to his design.
Contributions to mathematics.
Pascal continued to influence mathematics throughout his life. His "Traité du triangle arithmétique" ("Treatise on the Arithmetical Triangle") of 1653 described a convenient tabular presentation for binomial coefficients, now called Pascal's triangle. The triangle can also be represented:
He defines the numbers in the triangle by recursion: Call the number in the ("m" + 1)th row and ("n" + 1)th column "t""mn". Then "t""mn" = "t""m"–1,"n" + "t""m","n"–1, for "m" = 0, 1, 2, ... and "n" = 0, 1, 2, ... The boundary conditions are "t""m",−1 = 0, "t"−1,"n" = 0 for "m" = 1, 2, 3, ... and "n" = 1, 2, 3, ... The generator "t"00 = 1. Pascal concludes with the proof,
In 1654, prompted by his friend the Chevalier de Méré, he corresponded with Pierre de Fermat on the subject of gambling problems, and from that collaboration was born the mathematical theory of probabilities. The specific problem was that of two players who want to finish a game early and, given the current circumstances of the game, want to divide the stakes fairly, based on the chance each has of winning the game from that point. From this discussion, the notion of expected value was introduced. Pascal later (in the "Pensées") used a probabilistic argument, Pascal's Wager, to justify belief in God and a virtuous life. The work done by Fermat and Pascal into the calculus of probabilities laid important groundwork for Leibniz' formulation of the calculus.
After a religious experience in 1654, Pascal mostly gave up work in mathematics.
Philosophy of mathematics.
Pascal's major contribution to the philosophy of mathematics came with his "De l'Esprit géométrique" ("Of the Geometrical Spirit"), originally written as a preface to a geometry textbook for one of the famous ""Petites-Ecoles de Port-Royal" ("Little Schools of Port-Royal")". The work was unpublished until over a century after his death. Here, Pascal looked into the issue of discovering truths, arguing that the ideal of such a method would be to found all propositions on already established truths. At the same time, however, he claimed this was impossible because such established truths would require other truths to back them up—first principles, therefore, cannot be reached. Based on this, Pascal argued that the procedure used in geometry was as perfect as possible, with certain principles assumed and other propositions developed from them. Nevertheless, there was no way to know the assumed principles to be true.
Pascal also used "De l'Esprit géométrique" to develop a theory of definition. He distinguished between definitions which are conventional labels defined by the writer and definitions which are within the language and understood by everyone because they naturally designate their referent. The second type would be characteristic of the philosophy of essentialism. Pascal claimed that only definitions of the first type were important to science and mathematics, arguing that those fields should adopt the philosophy of formalism as formulated by Descartes.
In "De l'Art de persuader" ("On the Art of Persuasion"), Pascal looked deeper into geometry's axiomatic method, specifically the question of how people come to be convinced of the axioms upon which later conclusions are based. Pascal agreed with Montaigne that achieving certainty in these axioms and conclusions through human methods is impossible. He asserted that these principles can be grasped only through intuition, and that this fact underscored the necessity for submission to God in searching out truths.
Contributions to the physical sciences.
Pascal's work in the fields of the study of hydrodynamics and hydrostatics centered on the principles of hydraulic fluids. His inventions include the hydraulic press (using hydraulic pressure to multiply force) and the syringe. He proved that hydrostatic pressure depends not on the weight of the fluid but on the elevation difference. He demonstrated this principle by attaching a thin tube to a barrel full of water and filling the tube with water up to the level of the third floor of a building. This caused the barrel to leak, in what became known as Pascal's barrel experiment.
By 1646, Pascal had learned of Evangelista Torricelli's experimentation with barometers. Having replicated an experiment that involved placing a tube filled with mercury upside down in a bowl of mercury, Pascal questioned what force kept some mercury in the tube and what filled the space above the mercury in the tube. At the time, most scientists contended that, rather than a vacuum, some invisible matter was present. This was based on the Aristotelian notion that creation was a thing of substance, whether visible or invisible; and that this substance was forever in motion. Furthermore, "Everything that is in motion must be moved by something," Aristotle declared. Therefore, to the Aristotelian trained scientists of Pascal's time, a vacuum was an impossibility. How so? As proof it was pointed out:
Following more experimentation in this vein, in 1647 Pascal produced "Experiences nouvelles touchant le vide" ("New Experiments with the Vacuum"), which detailed basic rules describing to what degree various liquids could be supported by air pressure. It also provided reasons why it was indeed a vacuum above the column of liquid in a barometer tube.
On 19 September 1648, after many months of Pascal's friendly but insistent prodding, Florin Périer, husband of Pascal's elder sister Gilberte, was finally able to carry out the fact-finding mission vital to Pascal's theory. The account, written by Périer, reads:
"The weather was chancy last Saturday...[but] around five o'clock that morning...the Puy-de-Dôme was visible...so I decided to give it a try. Several important people of the city of Clermont had asked me to let them know when I would make the ascent...I was delighted to have them with me in this great work...
"...at eight o'clock we met in the gardens of the Minim Fathers, which has the lowest elevation in town...First I poured 16 pounds of quicksilver...into a vessel...then took several glass tubes...each four feet long and hermetically sealed at one end and opened at the other...then placed them in the vessel [of quicksilver]...I found the quick silver stood at 26" and 3½ lines above the quicksilver in the vessel...I repeated the experiment two more times while standing in the same spot...[they] produced the same result each time...
"I attached one of the tubes to the vessel and marked the height of the quicksilver and...asked Father Chastin, one of the Minim Brothers...to watch if any changes should occur through the day...Taking the other tube and a portion of the quick silver...I walked to the top of Puy-de-Dôme, about 500 fathoms higher than the monastery, where upon experiment...found that the quicksilver reached a height of only 23" and 2 lines...I repeated the experiment five times with care...each at different points on the summit...found the same height of quicksilver...in each case..."
Pascal replicated the experiment in Paris by carrying a barometer up to the top of the bell tower at the church of Saint-Jacques-de-la-Boucherie, a height of about fifty meters. The mercury dropped two lines.
In the face of criticism that some invisible matter must exist in Pascal's empty space, Pascal, in his reply to Estienne Noel, gave one of the 17th century's major statements on the scientific method, which is a striking anticipation of the idea popularised by Karl Popper that scientific theories are characterised by their falsifiability: "In order to show that a hypothesis is evident, it does not suffice that all the phenomena follow from it; instead, if it leads to something contrary to a single one of the phenomena, that suffices to establish its falsity." His insistence on the existence of the vacuum also led to conflict with other prominent scientists, including Descartes.
Pascal introduced a primitive form of roulette and the roulette wheel in his search for a perpetual motion machine.
Adult life, religion, philosophy, and literature.
Religious conversion.
In the winter of 1646, Pascal's 58-year-old father broke his hip when he slipped and fell on an icy street of Rouen; given the man's age and the state of medicine in the 17th century, a broken hip could be a very serious condition, perhaps even fatal. Rouen was home to two of the finest doctors in France: Monsieur Doctor Deslandes and Monsieur Doctor de La Bouteillerie. The elder Pascal "would not let anyone other than these men attend him...It was a good choice, for the old man survived and was able to walk again..." But treatment and rehabilitation took three months, during which time La Bouteillerie and Deslandes had become household guests.
Both men were followers of Jean Guillebert, proponent of a splinter group from Catholic teaching known as Jansenism. This still fairly small sect was making surprising inroads into the French Catholic community at that time. It espoused rigorous Augustinism. Blaise spoke with the doctors frequently, and upon his successful treatment of Étienne, borrowed from them works by Jansenist authors. In this period, Pascal experienced a sort of "first conversion" and began to write on theological subjects in the course of the following year.
Pascal fell away from this initial religious engagement and experienced a few years of what some biographers have called his "worldly period" (1648–54). His father died in 1651 and left his inheritance to Pascal and Jacqueline, for whom Pascal acted as her conservator. Jacqueline announced that she would soon become a postulant in the Jansenist convent of Port-Royal. Pascal was deeply affected and very sad, not because of her choice, but because of his chronic poor health; he too needed her.
"Suddenly there was war in the Pascal household. Blaise pleaded with Jacqueline not to leave, but she was adamant. He commanded her to stay, but that didn't work, either. At the heart of this was...Blaise's fear of abandonment...if Jacqueline entered Port-Royal, she would have to leave her inheritance behind...[but] nothing would change her mind."
By the end of October in 1651, a truce had been reached between brother and sister. In return for a healthy annual stipend, Jacqueline signed over her part of the inheritance to her brother. Gilberte had already been given her inheritance in the form of a dowry. In early January, Jacqueline left for Port-Royal. On that day, according to Gilberte concerning her brother, "He retired very sadly to his rooms without seeing Jacqueline, who was waiting in the little parlor..."
In early June 1653, after what must have seemed like endless badgering from Jacqueline,
Pascal formally signed over the whole of his sister's inheritance to Port-Royal, which, to him, "had begun to smell like a cult." With two thirds of his father's estate now gone, the 29-year-old Pascal was now consigned to genteel poverty.
For a while, Pascal pursued the life of a bachelor. During visits to his sister at Port-Royal in 1654, he displayed contempt for affairs of the world but was not drawn to God.
Brush with death.
On 23 November 1654, between 10:30 and 12:30 at night, Pascal had an intense religious vision and immediately recorded the experience in a brief note to himself which began: "Fire. God of Abraham, God of Isaac, God of Jacob, not of the philosophers and the scholars..." and concluded by quoting Psalm 119:16: "I will not forget thy word. Amen." He seems to have carefully sewn this document into his coat and always transferred it when he changed clothes; a servant discovered it only by chance after his death. This piece is now known as the "Memorial". The story of the carriage accident as having led to the experience described in the "Memorial" is disputed by some scholars.
His belief and religious commitment revitalized, Pascal visited the older of two convents at Port-Royal for a two-week retreat in January 1655. For the next four years, he regularly travelled between Port-Royal and Paris. It was at this point immediately after his conversion when he began writing his first major literary work on religion, the "Provincial Letters".
The "Provincial Letters".
Beginning in 1656, Pascal published his memorable attack on casuistry, a popular ethical method used by Catholic thinkers in the early modern period (especially the Jesuits, and in particular Antonio Escobar). Pascal denounced casuistry as the mere use of complex reasoning to justify moral laxity and all sorts of sins. The 18-letter series was published between 1656 and 1657 under the pseudonym Louis de Montalte and incensed Louis XIV. The king ordered that the book be shredded and burnt in 1660. In 1661, in the midsts of the formulary controversy, the Jansenist school at Port-Royal was condemned and closed down; those involved with the school had to sign a 1656 papal bull condemning the teachings of Jansen as heretical. The final letter from Pascal, in 1657, had defied Alexander VII himself. Even Pope Alexander, while publicly opposing them, nonetheless was persuaded by Pascal's arguments.
Aside from their religious influence, the "Provincial Letters" were popular as a literary work. Pascal's use of humor, mockery, and vicious satire in his arguments made the letters ripe for public consumption, and influenced the prose of later French writers like Voltaire and Jean-Jacques Rousseau.
The "Provincial Letters" also received praise. For example, Charles Perrault wrote of the "Letters": "Everything is there—purity of language, nobility of thought, solidity in reasoning, finesse in raillery, and throughout an "agrément" not to be found anywhere else."
The "Pensées".
Pascal's most influential theological work, referred to posthumously as the "Pensées" ("Thoughts"), was not completed before his death. It was to have been a sustained and coherent examination and defense of the Christian faith, with the original title "Apologie de la religion Chrétienne" ("Defense of the Christian Religion"). The first version of the numerous scraps of paper found after his death appeared in print as a book in 1669 titled "Pensées de M. Pascal sur la religion, et sur quelques autres sujets" ("Thoughts of M. Pascal on religion, and on some other subjects") and soon thereafter became a classic. One of the "Apologie"'s main strategies was to use the contradictory philosophies of skepticism and stoicism, personalized by Montaigne on one hand, and Epictetus on the other, in order to bring the unbeliever to such despair and confusion that he would embrace God.
Pascal's "Pensées" is widely considered to be a masterpiece, and a landmark in French prose. When commenting on one particular section (Thought #72), Sainte-Beuve praised it as the finest pages in the French language. Will Durant hailed it as "the most eloquent book in French prose." In "Pensées", Pascal surveys several philosophical paradoxes: infinity and nothing, faith and reason, soul and matter, death and life, meaning and vanity—seemingly arriving at no definitive conclusions besides humility, ignorance, and grace. Rolling these into one he develops Pascal's Wager.
Last works and death.
T. S. Eliot described him during this phase of his life as "a man of the world among ascetics, and an ascetic among men of the world." Pascal's ascetic lifestyle derived from a belief that it was natural and necessary for a person to suffer. In 1659, Pascal fell seriously ill. During his last years, he frequently tried to reject the ministrations of his doctors, saying, "Sickness is the natural state of Christians."
Louis XIV suppressed the Jansenist movement at Port-Royal in 1661. In response, Pascal wrote one of his final works, "Écrit sur la signature du formulaire" ("Writ on the Signing of the Form"), exhorting the Jansenists not to give in. Later that year, his sister Jacqueline died, which convinced Pascal to cease his polemics on Jansenism. Pascal's last major achievement, returning to his mechanical genius, was inaugurating perhaps the first bus line, moving passengers within Paris in a carriage with many seats.
In 1662, Pascal's illness became more violent, and his emotional condition had severely worsened since his sister's death. Aware that his health was fading quickly, he sought a move to the hospital for incurable diseases, but his doctors declared that he was too unstable to be carried. In Paris on 18 August 1662, Pascal went into convulsions and received extreme unction. He died the next morning, his last words being "May God never abandon me," and was buried in the cemetery of Saint-Étienne-du-Mont.
An autopsy performed after his death revealed grave problems with his stomach and other organs of his abdomen, along with damage to his brain. Despite the autopsy, the cause of his poor health was never precisely determined, though speculation focuses on tuberculosis, stomach cancer, or a combination of the two. The headaches which afflicted Pascal are generally attributed to his brain lesion.
Legacy.
In honor of his scientific contributions, the name Pascal has been given to the SI unit of pressure, to a programming language, and Pascal's law (an important principle of hydrostatics), and as mentioned above, Pascal's triangle and Pascal's wager still bear his name.
Pascal's development of probability theory was his most influential contribution to mathematics. Originally applied to gambling, today it is extremely important in economics, especially in actuarial science. John Ross writes, "Probability theory and the discoveries following it changed the way we regard uncertainty, risk, decision-making, and an individual's and society's ability to influence the course of future events." However, it should be noted that Pascal and Fermat, though doing important early work in probability theory, did not develop the field very far. Christiaan Huygens, learning of the subject from the correspondence of Pascal and Fermat, wrote the first book on the subject. Later figures who continued the development of the theory include Abraham de Moivre and Pierre-Simon Laplace.
In literature, Pascal is regarded as one of the most important authors of the French Classical Period and is read today as one of the greatest masters of French prose. His use of satire and wit influenced later polemicists. The content of his literary work is best remembered for its strong opposition to the rationalism of René Descartes and simultaneous assertion that the main countervailing philosophy, empiricism, was also insufficient for determining major truths.
In France, prestigious annual awards, Blaise Pascal Chairs are given to outstanding international scientists to conduct their research in the Ile de France region. One of the Universities of Clermont-Ferrand, France – Université Blaise Pascal – is named after him. The University of Waterloo, Ontario, Canada, holds an annual math contest named in his honour.
Roberto Rossellini directed a filmed biopic (entitled "Blaise Pascal") which originally aired on Italian television in 1971. Pascal was a subject of the first edition of the 1984 BBC Two documentary, "Sea of Faith", presented by Don Cupitt.

</doc>
<doc id="4069" url="http://en.wikipedia.org/wiki?curid=4069" title="Brittonic languages">
Brittonic languages

The Brittonic, Brythonic or British Celtic languages (, , ) form one of the two branches of the Insular Celtic language family; the other is Goidelic. The name "Brythonic" was derived by Welsh Celticist John Rhys from the Welsh word "Brython", meaning an indigenous Briton as opposed to an Anglo-Saxon or Gael. The name "Brittonic" derives ultimately from the name "Prettanike", recorded by Greek authors for the British Isles. Some authors reserve the term "Brittonic" for the modified later Brittonic languages after about AD 600.
The Brittonic languages derive from the Common Brittonic language, spoken throughout Great Britain south of the Firth of Forth during the Iron Age and Roman period. North of the Forth, the Pictish language is considered to be related; it is possible it was a Brittonic language, but it may have been a sister language. In the 5th and 6th centuries emigrating Britons also took Brittonic speech to the continent, most significantly in Brittany. During the next few centuries the language began to split into several dialects, eventually evolving into Welsh, Cornish, Breton, and Cumbric. Welsh and Breton continue to be spoken as native languages, while a revival in Cornish has led to an increase in speakers of that language. Cumbric is extinct, having been replaced by Goidelic and English speech. The Isle of Man may also have originally spoken a Brittonic language, later replaced with a Goidelic one. Due to emigration, there are also communities of Brittonic language speakers in England, France, and Y Wladfa (the Welsh settlement in Patagonia).
Name.
The names "Brittonic" and "Brythonic" are scholarly conventions referring to the Celtic languages of Britain and to the ancestral language they originated from, designated Common Brittonic, in contrast to the Goidelic languages originating in Ireland. Both were created in the 19th century to avoid the ambiguity of earlier terms such as "British" and "Cymric". "Brythonic" was coined in 1879 by the Celticist John Rhys from the Welsh word "Brython". "Brittonic", derived from "Briton" and also earlier spelled "Britonic" and "Britonnic", emerged later in the 19th century. It became more prominent through the 20th century, and was used in Kenneth H. Jackson's highly influential 1953 work on the topic, "Language and History in Early Britain". Jackson noted that by that time "Brythonic" had become a dated term, and that "of late there has been an increasing tendency to use Brittonic instead." Today, "Brittonic" often replaces "Brythonic" in the literature. Rudolf Thurneysen used "Britannic" in his influential "A Grammar of Old Irish", though this never became popular among subsequent scholars.
Comparable historical terms include the Medieval Latin "lingua Britannica" and "sermo Britannicus" and the Welsh "Brythoneg". Some writers use "British" for the language and its descendants, though due to the risk of confusion, others avoid it or use it only in a restricted sense. Jackson, and later John T. Koch, use "British" only for the early phase of the Common Brittonic language.
Prior to Jackson's work, "Brittonic" (and "Brythonic") were often used for all the P-Celtic languages, including not just the varieties in Britain but those Continental Celtic languages that similarly experienced the evolution of the Proto-Celtic language element /kʷ/ to /p/. However, subsequent writers have tended to follow Jackson's scheme, rendering this use obsolete.
Evidence.
Knowledge of the Brittonic languages comes from a variety of sources. For the early languages information is obtained from coins, inscriptions and comments by classical writers as well as place names and personal names recorded by them. For later languages there is information from medieval writers and modern native speakers, together with place names. The names recorded in the Roman period are given in Rivet and Smith.
Characteristics.
The Brittonic branch is also referred to as P-Celtic because linguistic reconstruction of the Brittonic reflex of the Proto-Indo-European phoneme *"kw" is "p" as opposed to Goidelic "c". Such nomenclature usually implies an acceptance of the P-Celtic and Q-Celtic hypothesis rather than the Insular Celtic hypothesis because the term includes certain Continental Celtic languages as well. (for a discussion, see Celtic languages).
Other major characteristics include:
Classification.
The family tree of the Brittonic languages is as follows:
Brittonic languages in use today are Welsh, Cornish and Breton. Welsh and Breton have been spoken continuously since they formed. For all practical purposes Cornish died out during the 18th or 19th centuries, but a revival movement has more recently created small numbers of new speakers. Also notable are the extinct language Cumbric, and possibly the extinct Pictish although this may be best considered to be a sister of the Brittonic languages. The late Kenneth H. Jackson argued during the 1950s, from some of the few remaining examples of stone inscriptions, that the Picts may have also used a non-Indo-European language, but some modern scholars of Pictish do not agree.
History and origins.
The modern Brittonic languages are generally considered to all derive from a common ancestral language termed "Brittonic", "British", "Common Brittonic", "Old Brittonic" or "Proto-Brittonic", which is thought to have developed from Proto-Celtic or early Insular Celtic by the 6th century BC.
Brittonic languages were probably spoken prior to the Roman invasion at least in the majority of Great Britain south of the rivers Forth and Clyde, though the Isle of Man later had a Goidelic language, Manx. Northern Scotland mainly spoke Pritennic, which became the Pictish language, which may have been a Brittonic language like that of its neighbors. The theory has been advanced (notably by T. F. O'Rahilly) that part of Ireland spoke a Brittonic language, usually termed "Ivernic", before it was displaced by Primitive Irish, although the authors Dillon and Chadwick reject this theory as being implausible.
During the period of the Roman occupation of England and Wales (AD 43 to c. 410), Common Brittonic borrowed a large stock of Latin words, both for concepts unfamiliar in the pre-urban society of Celtic Britain such as urbanisation and new tactics of warfare as well as for rather more mundane words which displaced native terms (most notably, the word for "fish" in all the Brittonic languages derives from the Latin "piscis" rather than the native *"ēskos" - which may survive, however, in the Welsh name of the River Usk, ). Approximately 800 of these Latin loan-words have survived in the three modern Brittonic languages.
It is probable that at the start of the Post-Roman period "Common Brittonic" was differentiated into at least two major dialect groups – Southwestern and Western (in addition we may posit additional dialects, such as Eastern Brittonic, spoken in what is now the East of England, which have left little or no evidence). Between the end of the Roman occupation and the mid 6th century the two dialects began to diverge into recognisably separate languages, the Western into Cumbric and Welsh and the Southwestern into Cornish and its closely related sister language Breton, which was carried to continental Armorica. Jackson showed that a few of the dialect distinctions between West and Southwest Brittonic go back a long way. New divergencies began around AD 500 but other changes which were shared occurred in the 6th century. Other common changes occurred in the 7th century onward and are possibly due to inherent tendencies. Thus the concept of a common Brittonic language ends by AD 600. Substantial numbers of Britons certainly remained in the expanding area controlled by Anglo-Saxons, but over the fifth and sixth centuries they mostly adopted the English language.
The Brittonic languages spoken in what is now Scotland, the Isle of Man and what is now England began to be displaced in the 5th century through the settlement of Irish-speaking Gaels and Germanic peoples. The displacement of the languages of Brittonic descent was probably complete in all of Britain except Cornwall and Wales and the English counties bordering these areas such as Devon by the 11th century.
The regular consonantal sound changes from Proto-Celtic to Welsh, Cornish and Breton are summarised in the following table. Where the graphemes have a different value from the corresponding IPA symbols, the IPA equivalent is indicated between slashes. V represents a vowel; C represents a consonant.
Remnants in England, Scotland and Ireland.
Place names and river names.
The principal legacy left behind in those territories from which the Brittonic languages were displaced is that of toponyms (place names) and hydronyms (river names). There are many Brittonic place names in lowland Scotland and in the parts of England where it is agreed that substantial Brittonic speakers remained (Brittonic names, apart from those of the former Romano-British towns, are scarce over most of England). Names derived (sometimes indirectly) from Brittonic include London, Penicuik, Perth, Aberdeen, York, Dorchester, Dover and Colchester. Brittonic elements found in England include "bre-" and "bal-" for hills, while some such as combe or coomb(e) for a small deep valley and tor for a hill are examples of Brittonic words that were borrowed into English. Others reflect the presence of Britons such as Dumbarton – from the Scottish Gaelic "Dùn Breatainn" meaning "Fort of the Britons", or Walton meaning a "tun" or settlement where the "Wealh" "Britons" still lived.
The number of Celtic river names in England generally increases from east to west, a map showing these being given by Jackson. These names include ones such as Avon, Chew, Frome, Axe, Brue and Exe, but also river names containing the elements "der-/dar-/dur-" and "-went" e.g. "Derwent, Darwen, Deer, Adur, Dour, Darent, Went". In fact these names exhibit multiple different Celtic roots. One is *dubri- "water" [Bret. "dour", C. "dowr", W. "dŵr"], also found in the place-name "Dover" (attested in the Roman period as "Dubrīs"); this is the original source of rivers named "Dour". Another is *deru̯o- "oak" or "true" [Bret. "derv", C. "derow", W. "derw"), coupled with two agent suffixes, *-ent- and *-iū; this is the origin of "Derwent", " Darent" and "Darwen" (attested in the Roman period as "Deru̯entiō"). The final root to be examined is "went". In Roman Britain there were three tribal capitals named "U̯entā" (modern Winchester, Caerwent and Caistor St Edmunds), however the meaning is unknown. It may mean "the favoured/chosen" or "(place) of arrivals/comings".
Brittonicisms in English.
Some have argued that Celtic has acted as a substrate to English for both the lexicon and syntax. It is generally accepted that linguistic effects on English were lexically rather poor aside from toponyms, consisting of a few domestic words, which may include hubbub, dad, peat, bucket, crock, crumpet (cf. Br. "krampouz"), noggin, gob (cf. Gaelic "gob"), nook; and the dialectal term for a badger, i.e. "brock" (cf. Welsh "broch", C. "brogh" and Gaelic "broc"). Another legacy may be the sheep-counting system Yan Tan Tethera in the west, in the traditionally Celtic areas of England such as Cumbria. Several Cornish mining words are still in use in English language mining terminology, such as costean, gunnies, and vug.
Those who argue against the theory of a Brittonic substratum and heavy influence point out that many toponyms have no semantic continuation from the Brittonic language. A notable example is "Avon" which comes from the Celtic term for river "abona" or the Welsh term for river "afon" but was used by the English as a personal name. Likewise the River Ouse, Yorkshire contains the word "usa" which merely means water and the name of the river Trent simply comes from the Welsh word for a trepasser ("an over-flowing river")
It has been argued that the use of periphrastic constructions (using auxiliary verbs like "do" and "be") in the English verb (which is more widespread than in the other Germanic languages) is traceable to Brittonic influence although some find this very unlikely and prefer a hypothesis of North Germanic influence rather than Celtic. For example in literary Welsh we can have "Caraf" = "I love" and "Yr wyf yn caru" = "I am loving" where the Brittonic syntax is exactly mirrored in English, whereas in the Germanic sister languages of English we only have one form, for example "Ich liebe" in German. (Note that in "colloquial" German, a progressive tense has evolved which is formally very similar to that of English and those found in Celtic languages, e.g. "Ich bin am Arbeiten" "I am working", literally: "I am at the working". A similar structure is also found in modern Dutch. These parallel developments suggest that the English progressive is not necessarily due to Celtic influence.)
Some researchers (Filppula "et al.", 2001) argue that English syntax reflects more extensive Brittonic influences. For instance, in English tag questions, the form of the tag depends on the verb form in the main statement ("aren't I?", "isn't he?", "won't we?" etc.). The German "nicht wahr?" and the French "n'est-ce pas?", by contrast, are fixed forms which can be used with almost any main statement. It has been claimed that the English system has been borrowed from Brittonic, since Welsh tag questions vary in almost exactly the same way.
Brittonic effect on the Goidelic languages.
Far more notable, but less well known, are the many Brittonic influences on Scottish Gaelic. Like English, periphrastic constructions have come to the fore, but to a much greater degree. Scottish Gaelic contains a number of apparently P-Celtic loanwords, but as there is a far greater overlap in terms of Celtic vocabulary, than with English, it is not always possible to disentangle P- and Q-Celtic words. However some common words such as "monadh" = Welsh "mynydd" Cumbric "*monidh" are particularly evident.
Often the Brittonic influence on Scots Gaelic is indicated by considering Irish language usage, which is not likely to have been influenced so much by Brittonic. In particular, the word "srath" (Anglicised as "Strath") is a native Goidelic word, but its usage appears to have been modified by the Brittonic cognate "ystrad" whose meaning is slightly different. The effect on Irish has been the loan from British of many Latin-derived words. This has been associated with the Christianisation of Ireland from Britain.

</doc>
<doc id="4071" url="http://en.wikipedia.org/wiki?curid=4071" title="Bronski Beat">
Bronski Beat

Bronski Beat was a popular British synthpop trio who achieved success in the mid-1980s, particularly with the 1984 chart hit "Smalltown Boy". All members of the group were openly gay and their songs reflected this, often containing political commentary on gay-related issues. At the height of their popularity the band consisted of singer Jimmy Somerville backed by Steve Bronski and Larry Steinbachek, both of whom played keyboards and percussion. Somerville went on to have success as lead singer of The Communards and as a solo artist.
History.
1983–85: Early years and "The Age of Consent".
Bronski Beat formed in 1983 when Somerville, Steinbachek, and Bronski shared a three-bedroom flat at Lancaster House in Brixton.
Bronski Beat signed a recording contract with London Records in 1984 after doing only nine live gigs. The band's debut single, "Smalltown Boy" (about a young man leaving from a railway station after being teased and bullied for being a gay teen in Glasgow) was a hit, peaking at No 3 in the UK Singles Chart, and topping charts in Italy, Belgium, and the Netherlands. The single was accompanied by a promotional video directed by Bernard Rose, showing Somerville trying to befriend an attractive diver at a swimming pool, then being attacked by the diver's homophobic mates, being returned to his family by the police and having to leave home. (The police officer was played by Colin Bell, then the marketing manager of London Records). "Smalltown Boy" reached #48 in the U.S. chart and peaked at #7 in Australia. The single featured vocals from session singer Kevin Glancy.
The follow-up single, "Why?", while focusing on a Hi-NRG musical formula, was more lyrically focused on anti-gay prejudice. It also achieved Top 10 status in the UK, reaching #6, and was a Top 10 hit for the band in Australia.
At the end of 1984, the trio released an album entitled "The Age of Consent". The inner sleeve listed the varying ages of consent for consensual gay sex in different nations around the world. At the time, the age of consent for sexual acts between men in the UK was 21 compared with 16 for heterosexual acts, with several other countries having more liberal laws on gay sex. The album peaked at #4 in the UK Albums Chart, #36 in the U.S., and #12 in Australia.
Around the same time, the band headlined "Pits and Perverts", a concert at the Electric Ballroom in London to raise funds for the Lesbians and Gays Support the Miners campaign.
A third single was released, before Christmas 1984 was a revival of "It Ain't Necessarily So", the George and Ira Gershwin classic (from "Porgy and Bess"). The song questions the authenticity of biblical tales. It also reached the UK Top 20.
In 1985, the trio joined up with Marc Almond to record a version of Donna Summer's "I Feel Love". The full version was actually a medley, also incorporating snippets of Summer's "Love to Love You Baby" and John Leyton's "Johnny Remember Me". It was a success, reaching #3 in the UK, equalling the chart achievement of "Smalltown Boy". Although the original had been one of Marc Almond's all-time favourite songs, he had never read the lyrics and thus incorrectly sang "What'll it be, what'll it be, you and me" instead of "Falling free, falling free, falling free".
The band and their producer Mike Thorne had gone back into the studio in early 1985 to record a new single, "Run From Love". PolyGram (London Records' parent company at that time) had pressed a number of promo singles and 12" versions of the song, sending them out to both radio and record stores in the UK. However, the single was shelved as tensions in the band, both personal and political, resulted in Somerville leaving Bronski Beat in the summer of that year.
"Run From Love" was subsequently released in a remix form on the Bronski Beat album "Hundreds & Thousands", a collection of mostly remixes (LP) and b-sides (as bonus tracks on the CD version) as well as the hit "I Feel Love". Somerville went on to form The Communards with Richard Coles while the remaining members of Bronski Beat searched for a new vocalist.
1985–present: Post-Jimmy Somerville.
Bronski Beat recruited John Foster as Somerville's replacement (Foster is credited as "Jon Jon"). A single, "Hit That Perfect Beat", was released in November 1985, reaching #3 in the UK. It repeated this success in the Australian charts and was also featured in the film, "Letter to Brezhnev". A second single, "C'mon C'mon", also charted in the UK Top 20 and an album, "Truthdare Doubledare", released in May 1986, peaked at #18. The film "Parting Glances" (1986) included Bronski Beat songs "Love and Money", "Smalltown Boy" and "Why?". During this period, the band teamed up with producer Mark Cunningham on the first-ever BBC Children In Need single, a cover of David Bowie's "Heroes", released in 1986 under the name of The County Line.
Foster left the band in 1987. Following Foster's departure, Bronski Beat began work on their next album, "Out and About". The tracks were recorded at Berry Street studios in London with engineer Brian Pugsley. Some of the song titles were "The Final Spin" and "Peace And Love". The latter track featured Strawberry Switchblade vocalist Rose McDowell and appeared on several internet sites in 2006. One of the other songs from the project called "European Boy" was recorded in 1987 by disco group Splash. The lead singer of Splash was former Tight Fit singer Steve Grant. Steinbachek and Bronski toured extensively with the new material and got great reviews, however the project was abandoned as the group were dropped by London Records. Also in 1987, Bronski Beat and Somerville did a reunion concert for "International AIDS Day", supported by New Order, at the Brixton Academy, London.
In 1989, Jonathan Hellyer became lead singer, and the band extensively toured the U.S. and Europe with back-up vocalist Annie Conway and had one minor hit with the song "Cha Cha Heels", a one-off collaboration sung by American actress and singer Eartha Kitt. The song was originally written for movie and recording star Divine, who was unable to record the song before his death in 1988. 1990–91 saw Bronski Beat release three further singles on the Zomba record label, "I'm Gonna Run Away", "One More Chance" and "What More Can I Say". The singles were produced by Mike Thorne.
Foster and Bronski Beat teamed up again in 1994, and released a techno "Tell Me Why '94" and an acoustic "Smalltown Boy '94" on the German record label, ZYX Music. The album "Rainbow Nation" was released the following year with Hellyer returning as lead vocalist, as Foster had dropped out of the project. Bronski Beat then dissolved with Steve Bronski going on to become a producer for other artists. Larry Steinbachek became the musical director for Michael Laub's theatre company, 'Remote Control Productions'.
In 2007, Bronski remixed the song "Stranger To None" by the UK alternative rock band, All Living Fear. Four different mixes were done, with one appearing on their retrospective album, "Fifteen Years After". Bronski also remixed the track "Flowers in the Morning" by Northern Irish electronic band, Electrobronze in 2007, changing the style of the song from classical to Hi-NRG disco.

</doc>
<doc id="4074" url="http://en.wikipedia.org/wiki?curid=4074" title="Barrel (disambiguation)">
Barrel (disambiguation)

A barrel is a cylindrical container. 
Barrel may also refer to:

</doc>
<doc id="4077" url="http://en.wikipedia.org/wiki?curid=4077" title="Binary prefix">
Binary prefix

A binary prefix is a prefix attached before a unit symbol to multiply it by a power of 2. In computing, such a prefix is seen in combination with a unit of information (bit, byte, etc.), to indicate a power of 1024.
The computer industry has historically used the units "kilobyte", "megabyte", and "gigabyte", and the corresponding symbols KB, MB, and GB, in at least two slightly different measurement systems. In citations of main memory (RAM) capacity, "gigabyte" customarily means bytes. As this is the third power of 1024, and 1024 is a power of two (210), this usage is therefore referred to as a binary prefix.
In most other contexts, the industry uses the multipliers "kilo", "mega", "giga", etc., in a manner consistent with their meaning in the International System of Units (SI), namely as powers of 1000. For example, a 500 gigabyte hard disk holds bytes, and a 100 megabit per second Ethernet connection transfers data at bit/s. In contrast with the "binary prefix" usage, this use is described as a "decimal prefix", as 1000 is a power of 10.
The ambiguity of using the same unit prefixes for two different representations within the same industry has caused some confusion. Starting around 1998, the International Electrotechnical Commission (IEC) and several other standards and trade organizations approved standards and recommendations for a new set of binary prefixes that refer unambiguously to powers of 1024. Accordingly, the National Institute of Standards and Technology requires that SI prefixes only be used in the decimal sense: kilobyte and megabyte denote one thousand bytes and one million bytes respectively (consistent with SI), while new terms such as kibibyte, mebibyte and gibibyte, having the symbols KiB, MiB, and GiB, denote 1024 bytes, bytes, and bytes, respectively. In 2008, the IEC prefixes were incorporated into the International System of Quantities.
History.
Main memory.
Early computers used one of two addressing methods to access the system memory; binary (base 2) or decimal (base 10).
For example, the IBM 701 (1952) used binary and could address 2048 36-bit words, while the IBM 702 (1953) used decimal and could address 7-bit words.
By the mid-1960s, binary addressing had become the standard architecture in most computer designs, and main memory sizes were most commonly powers of two. This is the most natural configuration for memory, as all combinations of their address lines map to a valid address, allowing easy aggregation into a larger block of memory with contiguous addresses.
Early computer system documentation would specify the memory size with an exact number such as 4096, 8192, or 16384 words of storage. These are all powers of two, and furthermore are small multiples of 210, or 1024. As storage capacities increased, several different methods were developed to abbreviate these quantities.
The method most commonly used today uses prefixes such as kilo, mega, giga, and corresponding symbols K, M, and G, which the computer industry originally adopted from the metric system. The prefixes "kilo-" and "mega-", meaning 1000 and respectively, were commonly used in the electronics industry before World War II. 
Along with "giga-" or G-, meaning , they are now known as SI prefixes after the International System of Units (SI), introduced in 1960 to formalize aspects of the metric system. (Note that K is the SI unit for temperature (kelvin) and should not be confused with k, the SI prefix for kilo.)
The International System of Units does not define units for digital information but notes that the SI prefixes may be applied outside the contexts where base units or derived units would be used. But as computer main memory in a 
binary-addressed system is manufactured in sizes that were easily expressed as multiples of 1024, "kilobyte", when applied to computer memory, came to be used to mean 1024 bytes instead of 1000. (This usage is not consistent with the SI. Compliance with the SI requires that the prefixes take their 1000-based meaning, and cannot be used as placeholders for other numbers, like 1024.)
The use of K in the binary sense as in a "32K core" meaning 32×1024 words, i.e., words, can be found as early as 1959.
Gene Amdahl's seminal 1964 article on IBM System/360 used 1K to mean 1024.
This style was used by other computer vendors, the CDC 7600 "System Description" (1968) made extensive use of K as 1024.
Thus the first binary prefix was born.
Another style was to truncate the last three digits and append K, essentially using K as a decimal prefix similar to SI, but always truncating to the next lower whole number instead of rounding to the nearest. The exact values words, words and words would then be described as "32K", "65K" and "131K".
This style was used from about 1965 to 1975.
These two styles (K = 1024 and truncation) were used loosely around the same time, sometimes by the same company. In discussions of binary-addressed memories, the exact size was evident from context. (For memory sizes of "41K" and below, there is no difference between the two styles.) The HP 21MX real-time computer (1974) denoted (which is 192×1024) as "196K" and as "1M",
while the HP 3000 business computer (1973) could have "64K", "96K", or "128K" bytes of memory.
The "truncation" method gradually waned. Capitalization of the letter K became the "de facto" standard for binary notation, although this could not be extended to higher powers. Nevertheless, the practice of using the SI-inspired "kilo" to indicate 1024 was later extended to "megabyte" meaning 10242 () bytes, and later "gigabyte" for 10243 () bytes. For example, a "512 megabyte" RAM module is 512×10242 bytes (512×, or ), rather than .
The symbols Kbit, Kbyte, Mbit and Mbyte started to be used as "binary units"—"bit" or "byte" with a multiplier that is a power of 1024—in the early 1970s.
For a time, memory capacities were often expressed in K, even when M could have been used: The IBM System/370 Model 158 brochure (1972) had the following: "Real storage capacity is available in 512K increments ranging from 512K to 2,048K bytes."
Megabyte was used to describe the 22-bit addressing of DEC PDP-11/70 (1975)
and gigabyte the 30-bit addressing DEC VAX-11/780 (1977).
In 1998, the International Electrotechnical Commission IEC introduced the binary prefixes kibi, mebi, gibi ... to mean 1024, 10242, 10243 etc., so that 1048576 bytes could be referred to unambiguously as 1 mebibyte. The IEC prefixes were incorporated into the International System of Quantities (ISQ) in 2008.
Disk drives.
The disk drive industry followed a different pattern. Industry practice, more thoroughly documented at Timeline of binary prefixes and continuing today, is to specify hard drives using SI prefixes and symbols in their SI or "decimal" interpretation. Unlike binary-addressed computer main memory, there is nothing in a disk drive that influences it to have a total capacity easily expressed using a power of 1024. The first commercially sold disk drive, the IBM 350, had 50 (not 32 or 64) physical disk "platters" containing a total of 50,000 sectors of 100 characters each, for a total quoted capacity of "5 million characters." It was introduced in September 1956.
In the 1960s most disk drives used IBM's variable block length format (called Count Key Data or "CKD").
Any block size could be specified up to the maximum track length. Since the block headers occupied space, the usable capacity of the drive was dependent on the block size. Blocks ("records" in IBM's terminology) of 88, 96, 880 and 960 were often used because they related to the fixed block size of punch cards. The drive capacity was usually stated under conditions of full track record blocking. For example, the 100-megabyte 3336 disk pack only achieved that capacity with a full track block size of 13,030 bytes.
Hard disk drive manufacturers used "megabytes" or "MB", meaning 106 bytes, to characterize their products as early as 1974. By 1977, in its first edition, Disk/Trend, a leading hard disk drive industry marketing consultancy segmented the industry according to MBs (decimal sense) of capacity.
One of the earliest hard disk drives in personal computing history,
the Seagate ST-412, was specified as "Formatted: 10.0 Megabytes". The specification of 4 heads or active surfaces (tracks per cylinder), 306 cylinders and when formatted with a sector size of 256 bytes and 32 sectors/track results in a capacity of bytes. This drive was one of several types installed into the IBM PC/XT and extensively advertised and reported as a "10 MB" (formatted) hard disk drive.
Operating systems and programs using the customary binary prefixes show this as "9.5625 MB".
The hard drive industry continues to use decimal prefixes for drive capacity. Today, for example, a "300 GB" hard drive offers slightly more than 300×109, or , bytes, not 300×230 (which would be about 322×109). Operating systems such as Microsoft Windows that display hard drive sizes using the customary binary prefix "GB" (as it is used for RAM) would display this as "279.4 GB" (meaning 279.4×10243, or 279.4×). On the other hand, Mac OS X has since version 10.6 shown hard drive size using decimal prefixes (thus matching the drive makers' packaging). (Previous versions of Mac OS used binary prefixes.)
However, other usages still occur. For example, in one document, Seagate specifies data transfer rates of some of its hard drives in "both" IEC and decimal units. 
"Advanced Format" drives using 4096-byte sectors are described as having "4K sectors."
Information transfer and clock rates.
Like the hard drive, there is nothing in a computer clock circuit or data transfer path that demands or even encourages that things happen at rates easily expressed using powers of 1024, or even using powers of 2.
Computer clock frequencies are always quoted using SI prefixes in their decimal sense. For example, the internal clock frequency of the original IBM PC was 4.77 MHz, that is, . 
Similarly, digital information transfer rates are mostly quoted using decimal prefixes:
Standardization of dual definitions.
By the mid-1970s it was common to see K meaning 1024 and the occasional M meaning for words or bytes of main memory (RAM) while K and M were commonly used with their decimal meaning for disk storage. In the 1980s, as capacities of both types of devices increased, the SI prefix G, with SI meaning, was commonly applied to disk storage, while M in its binary meaning, became common for computer memory. In the 1990s, the prefix G, in its binary meaning, became commonly used for computer memory capacity. The first terabyte (SI prefix, bytes) hard disk drive was introduced in 2007.
The dual usage of the kilo, mega, and giga prefixes and their corresponding symbols K, M, and G as both powers of 1000 and powers of 1024 was recorded in standards and dictionaries. For example, the 1986 ANSI/IEEE Std 1084-1986
defined dual uses for kilo and mega.
 The binary units Kbyte and Mbyte were formally defined in ANSI/IEEE Std 1212-1991.
Many dictionaries have noted the practice of using traditional prefixes to indicate binary multiples.
Oxford online dictionary defines, for example, megabyte as: "Computing: a unit of information equal to one million or (strictly) bytes."
The units Kbyte, Mbyte, and Gbyte are found in the trade press and in IEEE journals. Gigabyte was formally defined in IEEE Std 610.10-1994 as either or 230 bytes.
Kilobyte, Kbyte, and KB are equivalent units and all are defined in the obsolete standard, IEEE 100-2000.
Byte multiples using powers of 1024 up to yottabyte are given by the on-line computing dictionary FOLDOC (Free On-Line Dictionary of Computing).
The hardware industry has coped with the dual definitions because of relative consistency:
system memory (RAM) typically uses the binary meaning while magnetic disk storage uses the SI meaning.
There are, however, exceptions and special cases.
Diskettes use yet another "megabyte" equal to 1024×1000 bytes.
In optical disks, Compact Disks use MB to mean 10242 bytes while DVDs use GB to mean 10003 bytes.
Inconsistent use of units.
Deviation between powers of 1024 and powers of 1000.
Computer storage has become cheaper per unit and thereby larger, by many orders of magnitude since "K" was first used to mean 1024. 
Because both the SI and "binary" meanings of kilo, mega, etc., are based on powers of 1000 or 1024 rather than simple multiples, the difference between 1M "binary" and 1M "decimal" is proportionally larger than that between 1K "binary" and 1k "decimal," and so on up the scale.
The relative difference between the values in the binary and decimal interpretations increases, when using the SI prefixes as the base, from 2.4% for kilo to nearly 21% for the yotta prefix.
Consumer confusion.
In the early days of computers there was little or no consumer confusion because of the sophisticated nature of the consumers and the practice of computer manufacturers to specify their products with capacities in full precision.
For example, in 1965 IBM stated about the System/360 Model 75 that "Its main memory operated at 750 nanoseconds and was available in three sizes up to 1,048,576 characters of information."
One source of consumer confusion is the difference in the way many operating systems display hard drive sizes, compared to the way hard drive manufacturers describe them.
As noted previously, hard drives are described and sold using "GB" or "TB" in their SI meaning: one billion and one trillion bytes. Many operating systems and other software, however, display hard drive and file sizes using "MB", "GB" or other SI-looking prefixes in their "binary" meaning, just as they do for displays of RAM capacity. For example, many such systems display a hard drive marketed as "160 GB" as 149.05 GB. The earliest known presentation of hard disk drive capacity by an operating system using "KB" or "MB" in a binary sense is 1984; earlier operating systems generally presented hard disk drive capacity in decimal digits with no prefix of any sort, for example, in the output of the MS-DOS or PC DOS CHKDSK command.
Legal disputes.
The different interpretations of disk size prefixes has led to three significant class action lawsuits against digital storage manufacturers.
One case involved flash memory and the other two involved hard disk drives.
Two of these were settled with the manufacturers admitting no wrongdoing but agreeing to clarify the storage capacity of their products on the consumer packaging.
Flash memory and hard disk manufacturers now have disclaimers on their packaging and web sites clarifying the formatted capacity of the devices
or defining MB as 1 million bytes and 1 GB as 1 billion bytes.
Willem Vroegh v. Eastman Kodak Company.
On 20 February 2004, Willem Vroegh filed a lawsuit against Lexar Media, Dane–Elec Memory, Fuji Photo Film USA, Eastman Kodak Company, Kingston Technology Company, Inc., Memorex Products, Inc.; PNY Technologies Inc., SanDisk Corporation, Verbatim Corporation, and Viking Interworks alleging that their descriptions of the capacity of their flash memory cards were false and misleading.
Vroegh claimed that a 256 MB Flash Memory Device had only 244 MB of accessible memory. "Plaintiffs allege that Defendants marketed the memory capacity of their products by assuming that one megabyte equals one million bytes and one gigabyte equals one billion bytes."
The plaintiffs wanted the defendants to use the traditional values of 10242 for megabyte and 10243 for gigabyte.
The plaintiffs acknowledged that the IEC and IEEE standards define a MB as one million bytes but stated that the industry has largely ignored the IEC standards.
The manufacturers agreed to clarify the flash memory card capacity on the packaging and web sites. The consumers could apply for "a discount of ten percent off a future online purchase from Defendants' Online Stores Flash Memory Device".
Orin Safier v. Western Digital Corporation.
On 7 July 2005, an action entitled "Orin Safier v. Western Digital Corporation, et al." was filed in the Superior Court for the City and County of San Francisco, Case No. CGC-05-442812.
The case was subsequently moved to the Northern District of California, Case No. 05-03353 BZ.
Although Western Digital maintained that their usage of units is consistent with "the indisputably correct industry standard for measuring and describing storage capacity", and that they "cannot be expected to reform the software industry", they agreed to settle in March 2006 with 14 June 2006 as the Final Approval hearing date.
Western Digital offered to compensate customers with a free download of backup and recovery software valued at US$30. They also paid $500,000 in fees and expenses to San Francisco lawyers Adam Gutride and Seth Safier, who filed the suit.
The settlement called for Western Digital to add a disclaimer to their later packaging and advertising.
Cho v. Seagate Technology (US) Holdings, Inc..
A lawsuit (Cho v. Seagate Technology (US) Holdings, Inc., San Francisco Superior Court, Case No. CGC-06-453195) was filed against Seagate Technology, alleging that Seagate overrepresented the amount of usable storage by 7% on hard drives sold between March 22, 2001 and September 26, 2007. The case was settled without Seagate admitting wrongdoing, but agreeing to supply those purchasers with free backup software or a 5% refund on the cost of the drives.
Unique binary prefixes.
Early suggestions.
While early computer scientists typically used k to mean 1000, some recognized the convenience that would result from working with multiples of 1024 and the confusion that resulted from using the same prefixes for two different meanings.
Several proposals for unique binary prefixes were made in 1968. Donald Morrison proposed to use the Greek letter kappa (κ) to denote 1024, κ2 to denote 1024×1024, and so on.
Wallace Givens responded with a proposal to use bK as an abbreviation for 1024 and bK2 or bK2 for 1024×1024, though he noted that neither the Greek letter nor lowercase letter b would be easy to reproduce on computer printers of the day.
Bruce A. Martin further proposed that the prefixes be abandoned altogether, and the letter B be used as a binary exponent, similar to E notation, to create shorthands like 3B20 for 3×220
None of these gained much acceptance, and capitalization of the letter K became the "de facto" standard for indicating a factor of 1024 instead of 1000, although this could not be extended to higher powers.
As the discrepancy between the two systems increased in the higher order powers, more proposals for unique prefixes were made.
In 1996, Markus Kuhn proposed a system with "di" prefixes, like the "dikilobyte" (K₂B or K2B). Donald Knuth, who uses decimal notation like 1 MB = 1000 kB, expressed "astonishment" that the IEC proposal was adopted, calling them "funny-sounding" and opining that proponents were assuming "that standards are automatically adopted just because they are there." Knuth proposed that the powers of 1024 be designated as "large kilobytes" and "large megabytes" (abbreviated KKB and MMB, as "doubling the letter connotes both binary-ness and large-ness"). Double prefixes were already abolished from SI, however, having a multiplicative meaning ("MMB" would be equivalent to "TB"), and this proposed usage never gained any traction.
IEC prefixes.
The set of binary prefixes that were eventually adopted, now referred to as the "IEC prefixes", were first proposed by the International Union of Pure and Applied Chemistry's (IUPAC) Interdivisional Committee on Nomenclature and Symbols (IDCNS) in 1995. At that time, it was proposed that the terms kilobyte and megabyte be used only for 103 bytes and 106 bytes, respectively. The new prefixes "kibi" (kilobinary), "mebi" (megabinary), "gibi" (gigabinary) and "tebi" (terabinary) were also proposed at the time, and the proposed symbols for the prefixes were kb, Mb, Gb and Tb respectively, rather than Ki, Mi, Gi and Ti. The proposal was not accepted at the time.
The Institute of Electrical and Electronic Engineers (IEEE) began to collaborate with the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC) to find acceptable names for binary prefixes. Under the chairmanship of Anders Thor, IEC proposed "kibi", "mebi", "gibi" and "tebi", with the symbols Ki, Mi, Gi and Ti respectively, in 1996.
The names for the new prefixes are derived from the original SI prefixes combined with the term "binary", but contracted, by taking the first two letters of the SI prefix and "bi" from binary. The first letter of each such prefix is therefore identical to the corresponding SI prefixes, except for "K", which is used interchangeably with "k", whereas in SI, only the lower-case k represents 1000.
The IEEE decided that their standards would use the prefixes "kilo", etc. with their metric definitions, but allowed the binary definitions to be used in an interim period as long as such usage was explicitly pointed out on a case-by-case basis.
Adoption by IEC, NIST and ISQ.
In January 1999, the IEC published the first international standard (IEC 60027-2 Amendment 2) with the new prefixes, extended up to "pebi" (Pi) and "exbi" (Ei).
The IEC 60027-2 Amendment 2 also states that the IEC position is the same as that of BIPM (the body that regulates the SI system); the SI prefixes retain their definitions in powers of 1000 and are never used to mean a power of 1024.
In usage, products and concepts typically described using powers of 1024 would continue to be, but with the new IEC prefixes. For example, a memory module of bytes (512×) would be referred to as 512 MiB or 512 mebibytes instead of 512 MB or 512 megabytes. Conversely, since hard drives have historically been marketed using the SI convention that "giga" means , a "500 GB" hard drive would still be labeled as such. According to these recommendations, operating systems and other software would also use binary and SI prefixes in the same way, so the purchaser of a "500 GB" hard drive would find the operating system reporting either "500 GB" or "466 GiB", while bytes of RAM would be displayed as "512 MiB".
The second edition of the standard, published in 2000, defined them only up to "exbi", but in 2005, the third edition added prefixes "zebi" and "yobi", thus matching all SI prefixes with binary counterparts.
The harmonized ISO/IEC IEC 80000-13:2008 standard,
part of the International System of Quantities (ISQ), cancels and replaces subclauses 3.8 and 3.9 of IEC 60027-2:2005 (those defining prefixes for binary multiples). The only significant change is the addition of explicit definitions for some quantities.
The BIPM standard JCGM 200:2012 "International vocabulary of metrology - Basic and general concepts and associated terms (VIM), 3rd edition" lists the IEC binary prefixes and states "SI prefixes refer strictly to powers of 10, and should not be used for powers of 2. For example, 1 kilobit should not be used to represent bits (210 bits), which is 1 kibibit." 
Other standards bodies and organizations.
The IEC standard binary prefixes are now supported by other standardization bodies and technical organizations.
The United States National Institute of Standards and Technology (NIST) supports the ISO/IEC standards for
"Prefixes for binary multiples" and has a documenting them, describing and justifying their use. NIST suggests that in English, the first syllable of the name of the binary-multiple prefix should be pronounced in the same way as the first syllable of the name of the corresponding SI prefix, and that the second syllable should be pronounced as "bee". NIST has stated the SI prefixes "refer strictly to powers of 10" and that the binary definitions "should not be used" for them.
In December 2002, JEDEC, a leading standards organization in the microelectronics industry, mentioned the IEC prefixes in their "Terms, Definitions, and Letter Symbols for Microcomputers, Microprocessors, and Memory Integrated Circuits" document. This document defines "kilo", "mega", and "giga" with binary multipliers. A "Note" to this definition then states that that definition is only presented "to reflect common usage", and quotes the IEC in describing the binary prefixes as "an alternative system". However, subsequent memory standards published by JEDEC still define and use the prefixes kilo, mega, and giga as binary multipliers.
On 19 March 2005, the IEEE standard IEEE 1541-2002 ("Prefixes for Binary Multiples") was elevated to a full-use standard by the IEEE Standards Association after a two-year trial period. However, , the IEEE Publications division does not require the use of IEC prefixes in its major magazines such as "Spectrum" or "Computer". 
The International Bureau of Weights and Measures (BIPM), which maintains the International System of Units (SI), expressly prohibits the use of SI prefixes to denote binary multiples, and recommends the use of the IEC prefixes as an alternative since units of information are not included in SI.
The Society of Automotive Engineers (SAE) prohibits the use of SI prefixes with anything but a power-of-1000 meaning, but does not recommend or otherwise cite the IEC binary prefixes.
The European Committee for Electrotechnical Standardization (CENELEC) adopted the IEC-recommended binary prefixes via the harmonization document HD 60027-2:2003-03.
The European Union (EU) has required the use of the IEC binary prefixes since 2007.
Current practice.
Most computer hardware uses SI prefixes to state capacity and define other performance parameters such as data rate. Main and cache memories are notable exceptions.
Capacities of main memory and cache memory are usually expressed with customary binary prefixes
On the other hand, flash memory, like that found in solid state drives, mostly uses SI prefixes to state capacity.
Some operating systems and other software continue to use the customary binary prefixes in displays of memory, disk storage capacity, and file size, but SI prefixes in other areas such as network communication speeds and processor speeds.
In the following subsections, unless otherwise noted, examples are first given using the common prefixes used in each case, and then followed by interpretation using other notation where appropriate.
Operating systems.
Prior to the release of Macintosh System Software (1984), file sizes were typically reported by the operating system without any prefixes. Today, most operating systems report file sizes with prefixes.
Software.
, most software does not distinguish symbols for binary and decimal prefixes.
The IEC binary naming convention has been adopted by a few, but this is not used universally.
One of the stated goals of the introduction of the IEC prefixes was "to preserve the SI prefixes as unambiguous decimal multipliers." Programs such as fdisk/cfdisk, parted, and apt-get use SI prefixes with their decimal meaning.
Example of the use of IEC binary prefixes in the Linux operating system displaying traffic volume on a network interface in kibibytes (KiB) and mebibytes (MiB), as obtained with the ifconfig utility:
Software that uses standard SI prefixes for powers of 1000, but "not" IEC binary prefixes for powers of 1024, includes:
Software that uses IEC binary prefixes for powers of 1024 "and" uses standard SI prefixes for powers of 1000 includes:
Computer hardware.
Hardware types that use powers-of-1024 multipliers, such as memory, continue to be marketed with customary binary prefixes.
Computer memory.
Measurements of most types of electronic memory such as RAM and ROM are given using customary binary prefixes (kilo, mega, and giga). This includes some flash memory, like EEPROMs. For example, a "512-megabyte" memory module is 512×220 bytes (512×, or ).
JEDEC Solid State Technology Association, the semiconductor engineering standardization body of the Electronic Industries Alliance (EIA), continues to include the customary binary definitions of kilo, mega and giga in their "Terms, Definitions, and Letter Symbols" document,
and uses those definitions in later memory standards
Many computer programming tasks reference memory in terms of powers of two because of the inherent binary design of current hardware addressing systems. For example, a 16-bit processor register can reference at most 65,536 items (bytes, words, or other objects); this is conveniently expressed as "64K" items. An operating system might map memory as 4096-byte pages, in which case exactly 8192 pages could be allocated within bytes of memory: "8K" (8192 bytes) pages of "4 kilobytes" (4096 bytes) each within "32 megabytes" (32 MiB) of memory.
Hard disk drives.
All hard disk drive manufacturers state capacity using SI prefixes.
Flash drives.
USB flash drives, flash-based memory cards like CompactFlash or Secure Digital, and flash-based SSDs use SI prefixes;
for example, a "256 MB" flash card provides at least 256 million bytes (), not 256×1024×1024 ().
The flash memory chips inside these devices contain considerably more than the quoted capacities, but much like a traditional hard drive, some space is reserved for internal functions of the flash drive. These include wear leveling, error correction, sparing, and metadata needed by the device's internal firmware.
Floppy drives.
Floppy disks have existed in numerous physical and logical formats, and have been sized inconsistently. In part, this is because the end user capacity of a particular disk is a function of the controller hardware, so that the same disk could be formatted to a variety of capacities. In many cases, the media are marketed without any indication of the end user capacity, as for example, DSDD, meaning double-sided double-density.
The last widely adopted diskette was the 3½-inch high density. This has a formatted capacity of bytes or 1440 KB (1440×1024, using "KB" in the customary binary sense). These are marketed as "HD", or "1.44 MB" or both. This usage creates a third definition of "megabyte" as 1000×1024 bytes.
Most operating systems display the capacity using "MB" in the customary binary sense, resulting in a display of "1.4 MB" (. Some users have noticed the missing 0.04 MB and both Apple and Microsoft have support bulletins referring to them as 1.4 MB.
The earlier "1200 KB" (1200×1024 bytes) 5¼-inch diskette sold with the IBM PC AT was marketed as "1.2 MB" (). The largest 8-inch diskette formats could contain more than a megabyte, and the capacities of those devices were often irregularly specified in megabytes, also without controversy.
Older and smaller diskette formats were usually identified as an accurate number of (binary) KB, for example the Apple Disk II described as "140KB" had a 140×1024-byte capacity, and the original "360KB" double sided, double density disk drive used on the IBM PC had a 360×1024-byte capacity.
In many cases diskette hardware was marketed based on unformatted capacity, and the overhead required to format sectors on the media would reduce the nominal capacity as well (and this overhead typically varied based on the size of the formatted sectors), leading to more irregularities.
Optical discs.
The capacities of most optical disc storage media like DVD, Blu-ray Disc, HD DVD and magneto-optical (MO) are given using SI decimal prefixes.
A "4.7 GB" DVD has a nominal capacity of about 4.38 GiB. However, CD capacities are always given using customary binary prefixes. Thus a "700-MB" (or "80-minute") CD has a nominal capacity of about 700 MiB (approx 730 MB).
Tape drives and media.
Tape drive and media manufacturers use SI decimal prefixes to identify capacity.
Data transmission and clock rates.
Certain units are always used with SI decimal prefixes even in computing contexts.
Two examples are hertz (Hz), which is used to measure the clock rates of electronic components, and bit/s, used to measure data transmission speed.
Bus clock speeds and therefore bandwidths are both quoted using SI decimal prefixes.
Use by industry.
IEC prefixes are used by IBM, HP and Toshiba to advertise or describe some of their products. According to one HP brochure, "[t]o reduce confusion, vendors are pursuing one of two remedies: they are changing SI prefixes to the new binary prefixes, or they are recalculating the numbers as powers of ten." The IBM Data Center also uses IEC prefixes to reduce confusion. The IBM Style Guide reads "To help avoid inaccuracy (especially with the larger prefixes) and potential ambiguity, the International Electrotechnical Commission (IEC) in 2000 adopted a set of prefixes specifically for binary multipliers (See IEC 60027-2). Their use is now supported by the United States National Institute of Standards and Technology (NIST) and incorporated into ISO 80000. They are also required by EU law and in certain contexts in the US.
However, most documentation and products in the industry continue to use SI prefixes when referring to binary multipliers. In product documentation, follow the same standard that is used in the product itself (for example, in the interface or firmware). Whether you choose to use IEC prefixes for powers of 2 and SI prefixes for powers of 10, or use SI prefixes for a dual purpose ... be consistent in your usage and explain to the user your adopted system." 

</doc>
<doc id="4078" url="http://en.wikipedia.org/wiki?curid=4078" title="National Baseball Hall of Fame and Museum">
National Baseball Hall of Fame and Museum

The National Baseball Hall of Fame and Museum is an American history museum and hall of fame, located at 25 Main Street in Cooperstown, New York, and operated by private interests. It serves as the central point for the study of the history of baseball in the United States and beyond, displays baseball-related artifacts and exhibits, and honors those who have excelled in playing, managing, and serving the sport. The Hall's motto is "Preserving History, Honoring Excellence, Connecting Generations."
The word Cooperstown is often used as shorthand (or a metonym) for the National Baseball Hall of Fame and Museum.
History.
The Hall of Fame was founded in 1939 by Stephen Carlton Clark, the owner of a local hotel. Clark sought to bring tourists to a city hurt by the Great Depression, which reduced the local tourist trade, and Prohibition, which devastated the local hops industry. The Hall of Fame was dedicated on June 12, 1939. (Clark's granddaughter, Jane Forbes Clark, is the current chairman of the Board of Directors.)
The erroneous claim that U.S. Civil War hero Abner Doubleday invented baseball in Cooperstown, a claim made by former National League president Abraham G. Mills and his 1905 Mills Commission, was instrumental in the early marketing of the Hall.
An expanded library and research facility opened in 1994. Dale Petroskey became the organization's president in 1999.
In 2002, the Hall launched "Baseball As America", a traveling exhibit that toured ten American museums over six years. The Hall of Fame has also sponsored educational programming on the Internet to bring the Hall of Fame to schoolchildren who might not visit. The Hall and Museum completed a series of renovations in spring 2005. The Hall of Fame also presents an annual exhibit at FanFest at the Major League Baseball All-Star Game.
Jeff Idelson replaced Petroskey as president on April 16, 2008. He had been acting as president since March 25, 2008, when Petroskey was forced to resign for having "failed to exercise proper fiduciary responsibility" and making "judgments that were not in the best interest of the National Baseball Hall of Fame and Museum."
In 2012, Congress passed and President Barack Obama signed a law ordering the United States Mint to produce and sell commemorative, non-circulating coins to benefit the private, non-profit Hall. The bill, , was introduced in the United States House of Representatives by Rep. Richard Hanna, a Republican from New York, and passed the House on October 26, 2011. The coins, which depict baseball gloves and balls, are the first concave designs produced by the Mint. The mintage included 50,000 gold coins, 400,000 silver coins, and 750,000 clad (Nickel-Copper) coins. The Mint released them on March 27, 2014, and the gold and silver editions quickly sold out. The Hall receives money from surcharges included in the sale price: a total of $9.5 million if all the coins are sold.
Inductees.
Among baseball fans, "Hall of Fame" means not only the museum and facility in Cooperstown, New York, but the pantheon of players, managers, umpires, executives, and pioneers who have been enshrined in the Hall. The first five men elected were Ty Cobb, Babe Ruth, Honus Wagner, Christy Mathewson and Walter Johnson, chosen in 1936; roughly 20 more were selected before the entire group was inducted at the Hall's 1939 opening. , 306 people had been elected to the Hall of Fame, including 211 former Major League Baseball players, 35 Negro league baseball players and executives, 22 managers, 10 umpires, and 28 pioneers, executives, and organizers. 114 members of the Hall of Fame have been inducted posthumously, including four who died after their selection was announced. Of the 35 Negro league members, 29 were inducted posthumously, including all 24 selected since the 1990s. The Hall of Fame includes one female member, Effa Manley.
The newest members, forming the class of , are players Tom Glavine, Greg Maddux, and Frank Thomas, plus managers Bobby Cox, Tony La Russa, and Joe Torre. In addition to honoring Hall of Fame inductees, the National Baseball Hall of Fame has presented 38 men with the Ford C. Frick Award for excellence in broadcasting, 65 with the J. G. Taylor Spink Award for excellence in baseball writing, and three with the Buck O'Neil Lifetime Achievement Award for contributions to baseball. While Frick and Spink Award honorees are not members of the Hall of Fame, they are recognized in an exhibit in the Hall of Fame's library. O'Neil Award honorees are also not Hall of Fame members, but are listed alongside a permanent statue of the award's namesake and first recipient, Buck O'Neil, that stands at the Hall.
Selection process.
Players are currently inducted into the Hall of Fame through election by either the Baseball Writers Association of America (or BBWAA), or the Veterans Committee, which now consists of three subcommittees, each of which considers and votes for candidates from a separate era of baseball. Five years after retirement, any player with 10 years of major league experience who passes a screening committee (which removes from consideration players of clearly lesser qualification) is eligible to be elected by BBWAA members with 10 years' membership or more. From a final ballot typically including 25–40 candidates, each writer may vote for up to 10 players; until the late 1950s, voters were advised to cast votes for the maximum 10 candidates. Any player named on 75% or more of all ballots cast is elected. A player who is named on fewer than 5% of ballots is dropped from future elections. In some instances, the screening committee had restored their names to later ballots, but in the mid-1990s, dropped players were made permanently ineligible for Hall of Fame consideration, even by the Veterans Committee. A 2001 change in the election procedures restored the eligibility of these dropped players; while their names will not appear on future BBWAA ballots, they may be considered by the Veterans Committee. Players receiving 5% or more of the votes but fewer than 75% are reconsidered annually until a maximum of ten years of eligibility (lowered from fifteen years for the 2015 election).
Under special circumstances, certain players may be deemed eligible for induction even though they have not met all requirements. Addie Joss was elected in 1978, despite only playing nine seasons before he died of meningitis. Additionally, if an otherwise eligible player dies before his fifth year of retirement, then that player may be placed on the ballot at the first election at least six months after his death. Roberto Clemente's induction in 1973 set the precedent when the writers chose to put him up for consideration after his death on New Year's Eve, 1972.
The five-year waiting period was established in 1954 after an evolutionary process. In 1936 all players were eligible, including active ones. From the 1937 election until the 1945 election, there was no waiting period, so any retired player was eligible, but writers were discouraged from voting for current major leaguers. Since there was no formal rule preventing a writer from casting a ballot for an active player, the scribes did not always comply with the informal guideline; Joe DiMaggio received a vote in 1945, for example. From the 1946 election until the 1954 election, an official one-year waiting period was in effect. (DiMaggio, for example, retired after the 1951 season and was first eligible in the 1953 election.) The modern rule establishing a wait of five years was passed in 1954, although an exception was made for Joe DiMaggio because of his high level of previous support, thus permitting him to be elected within four years of his retirement. Contrary to popular belief, no formal exception was made for Lou Gehrig, other than to hold a special one-man election for him. There was no waiting period at that time and Gehrig met all other qualifications, so he would have been eligible for the next regular election after he retired during the 1939 season, but the BBWAA decided to hold a special election at the 1939 Winter Meetings in Cincinnati, specifically to elect Gehrig (most likely because it was known that he was terminally ill, making it uncertain that he would live long enough to see another election). Nobody else was on that ballot, and the numerical results have never been made public. Since no elections were held in 1940 or 1941, the special election permitted Gehrig to enter the Hall while still alive.
If a player fails to be elected by the BBWAA within 20 years of his retirement from active play, he may be selected by the Veterans Committee. Following the most recent changes to the election process for that body made in 2010, it is now responsible for electing all otherwise eligible candidates who are not eligible for the BBWAA ballot—both long-retired players and non-playing personnel (managers, umpires, and executives). With these changes, each candidate can now be considered once every three years. A more complete discussion of the new process is available below.
From 2008 to 2010, following changes made by the Hall in July 2007, the main Veterans Committee, then made up of living Hall of Famers, voted only on players whose careers began in 1943 or later. These changes also established three separate committees to select other figures:
Players of the Negro Leagues have also been considered at various times, beginning in 1971. In 2005 the Hall completed a study on African American players between the late 19th century and the integration of the major leagues in 1947, and conducted a special election for such players in February 2006; seventeen figures from the Negro Leagues were chosen in that election, in addition to the eighteen previously selected. Following the 2010 changes, Negro Leagues figures will primarily be considered for induction alongside other figures from the 1871–1946 era, called the "Pre-Integration Era" by the Hall.
Predictably, the selection process catalyzes endless debate among baseball fans over the merits of various candidates. Even players elected years ago remain the subjects of discussions as to whether they deserved election. For example, Bill James' book "Whatever Happened to the Hall of Fame?" goes into detail about who he believes does and does not belong in the Hall of Fame.
Changes to Veterans Committee process.
The actions and composition of the Veterans Committee have been at times controversial, with occasional selections of contemporaries and teammates of the committee members over seemingly more worthy candidates.
In 2001, the Veterans Committee was reformed to comprise the living Hall of Fame members and other honorees. The revamped Committee held three elections—in 2003 and 2007 for both players and non-players, and in 2005 for players only. No individual was elected in that time, sparking criticism among some observers who expressed doubt whether the new Veterans Committee would ever elect a player. The Committee members – most of whom were Hall members – were accused of being reluctant to elect new candidates in the hope of heightening the value of their own selection. After no one was selected for the third consecutive election in 2007, Hall of Famer Mike Schmidt noted, "The same thing happens every year. The current members want to preserve the prestige as much as possible, and are unwilling to open the doors." In 2007, the committee and its selection processes were again reorganized; the main committee then included all living members of the Hall, and voted on a reduced number of candidates from among players whose careers began in 1943 or later. Separate committees, including sportswriters and broadcasters, would select umpires, managers and executives, as well as players from earlier eras.
In the first election to be held under the 2007 revisions, two managers and three executives were elected in December 2007 as part of the 2008 election process. The next Veterans Committee elections for players were held in December 2008 as part of the 2009 election process; the main committee did not select a player, while the panel for pre–World War II players elected Joe Gordon in its first and ultimately only vote. The main committee voted as part of the election process for inductions in odd-numbered years, while the pre-WWII panel would vote every five years, and the panel for umpires, managers, and executives voted as part of the election process for inductions in even-numbered years.
Further changes to the Veterans Committee process were announced by the Hall on July 26, 2010, effective with the 2011 election.
All individuals eligible for induction but not eligible for BBWAA consideration are now considered on a single ballot, grouped by the following eras in which they made their greatest contributions:
The Hall is using the BBWAA's Historical Overview Committee to formulate the ballots for each era, consisting of 12 individuals for the Expansion Era and 10 for the other eras. The Hall's board of directors selects a committee of 16 voters for each era, made up of Hall of Famers, executives, baseball historians, and media members. Each committee meets and votes at the Baseball Winter Meetings once every three years. The Expansion Era committee held its first vote in 2010 for 2011 induction, with longtime general manager Pat Gillick becoming the first individual elected under the new procedure. The Golden Era committee voted in 2011 for the induction class of 2012, with Ron Santo becoming the first player elected under the new procedure. The Pre-Integration Era committee voted in 2012 for the induction class of 2013, electing three figures. Subsequent elections rotate among the three committees in that order.
Players and managers with multiple teams.
While the text on a player's or manager's plaque lists all teams for which the inductee was a member in that specific role, inductees are usually depicted wearing the cap of a specific team, though in a few cases, like umpires, they wear caps without logos. (Executives are not depicted wearing caps.) The Hall selects the logo "based on where that player makes his most indelible mark."
Although the Hall always made the final decision on which logo was shown, until 2001 the Hall deferred to the wishes of players or managers whose careers were linked with multiple teams. Some examples of honorees associated with multiple teams are the following:
In 2001, the Hall of Fame decided to change the policy on cap logo selection, as a result of rumors that some teams were offering compensation, such as number retirement, money, or organizational jobs, in exchange for the cap designation. (For example, though Wade Boggs denied the claims, some media reports had said that his contract with the Tampa Bay Devil Rays required him to request depiction in the Hall of Fame as a Devil Ray.) The Hall decided that it would no longer defer to the inductee, though the player's wishes would be considered, when deciding on the logo to appear on the plaque. Newly elected members affected by the change include the following:
The museum.
According to the Hall of Fame, approximately 300,000 visitors enter the museum each year, and the running total has surpassed 14 million. These visitors see only a fraction of its 38,000 artifacts, 2.6 million library items (such as newspaper clippings and photos) and 130,000 baseball cards.
However, the Hall has seen a noticeable decrease in attendance in recent years. A 2013 story on "ESPN.com" about the village of Cooperstown and its relation to the game partially linked the reduced attendance with Cooperstown Dreams Park, a youth baseball complex about 5 miles (8 km) away in the town of Hartwick. The 22 fields at Dreams Park currently draw 17,000 players each summer for a week of intensive play; while the complex includes housing for the players, their parents and grandparents must stay elsewhere. According to the story,Prior to Dreams Park, a room might be filled for a week by several sets of tourists. Now, that room will be taken by just one family for the week, and that family may only go into Cooperstown and the Hall of Fame once. While there are other contributing factors (the recession and high gas prices among them), the Hall's attendance has tumbled since Dreams Park opened. The Hall drew 383,000 visitors in 1999. It drew 262,000 last year.
Unauthorized sale of items in collection.
A controversy erupted in 1982, when it emerged that some historic items given to the Hall had been sold on the collectibles market. The items had been lent to the Baseball Commissioner's office, gotten mixed up with other property owned by the Commissioner's office and employees of the office, and moved to the garage of Joe Reichler, an assistant to Commissioner Bowie Kuhn, who sold the items to resolve his personal financial difficulties. Under pressure from the New York Attorney General, the Commissioner's Office made reparations, but the negative publicity damaged the Hall of Fame's reputation, and made it more difficult for it to solicit donations.
Non-induction of banned players.
Following the banning of Pete Rose from MLB, the selection rules for the Baseball Hall of Fame were modified to prevent the induction of anyone on Baseball's permanent suspension list, such as Rose or Shoeless Joe Jackson. Many others have been barred from participation in MLB, but none have Hall of Fame qualifications on the level of Jackson or Rose.
Jackson and Rose were both banned from MLB for life for actions related to gambling on their own teams—Jackson was determined to have cooperated with those who conspired to lose the 1919 World Series intentionally, and Rose voluntarily accepted a permanent spot on the ineligible list in return for MLB's promise to make no official finding in relation to alleged betting on the Cincinnati Reds when he was their manager in the 1980s. (Baseball's Rule 21, prominently posted in every clubhouse locker room, mandates permanent banishment from the MLB for having a gambling interest of any sort on a game in which a player or manager is directly involved.) Rose later admitted that he bet on the Reds in his 2004 autobiography. Baseball fans are deeply split on the issue of whether these two should remain banned or have their punishment revoked. Writer Bill James, though he advocates Rose eventually making it into the Hall of Fame, compared the people who want to put Jackson in the Hall of Fame to "those women who show up at murder trials wanting to marry the cute murderer".
References.
Notes

</doc>
<doc id="4079" url="http://en.wikipedia.org/wiki?curid=4079" title="BPP (complexity)">
BPP (complexity)

In computational complexity theory, BPP, which stands for bounded-error probabilistic polynomial time is the class of decision problems solvable by a probabilistic Turing machine in polynomial time with an error probability bounded away from 1/2 for all instances.
BPP is one of the largest "practical" classes of problems, meaning most problems of interest in BPP have efficient probabilistic algorithms that can be run quickly on real modern machines. BPP also contains P, the class of problems solvable in polynomial time with a deterministic machine, since a deterministic machine is a special case of a probabilistic machine.
Informally, a problem is in BPP if there is an algorithm for it that has the following properties:
Definition.
A language "L" is in BPP if and only if there exists a probabilistic Turing machine "M", such that
Unlike the complexity class ZPP, the machine "M" is required to run for polynomial time on all inputs, regardless of the outcome of the random coin flips.
Alternatively, BPP can be defined using only deterministic Turing machines. A language "L" is in BPP if and only if there exists a polynomial "p" and deterministic Turing machine "M", such that
In this definition, the string "y" corresponds to the output of the random coin flips that the probabilistic Turing machine would have made. For some applications this definition is preferable since it does not mention probabilistic Turing machines.
In practice, an error probability of 1/3 might not be acceptable, however, the choice of 1/3 in the definition is arbitrary. It can be any constant between 0 and 1/2 (exclusive) and the set BPP will be unchanged. It does not even have to be constant: the same class of problems is defined by allowing error as high as 1/2 − "n"−"c" on the one hand, or requiring error as small as 2−"nc" on the other hand, where "c" is any positive constant, and "n" is the length of input. The idea is that there is a probability of error, but if the algorithm is run many times, the chance that the majority of the runs are wrong drops off exponentially as a consequence of the Chernoff bound. This makes it possible to create a highly accurate algorithm by merely running the algorithm several times and taking a "majority vote" of the answers. For example, if one defined the class with the restriction that the algorithm can be wrong with probability at most 1/2100, this would result in the same class of problems.
Problems.
Besides the problems in P, which are obviously in BPP, many problems were known to be in BPP but not known to be in P. The number of such problems is decreasing, and it is conjectured that P = BPP.
For a long time, one of the most famous problems that was known to be in BPP but not known to be in P was the problem of determining whether a given number is prime. However, in the 2002 paper "PRIMES is in P", Manindra Agrawal and his students Neeraj Kayal and Nitin Saxena found a deterministic polynomial-time algorithm for this problem, thus showing that it is in P.
An important example of a problem in BPP (in fact in co-RP) still not known to be in P is polynomial identity testing, the problem of determining whether a polynomial is identically equal to the zero polynomial. In other words, is there an assignment of variables such that when the polynomial is evaluated the result is nonzero? It suffices to choose each variable's value uniformly at random from a finite subset of at least "d" values to achieve bounded error probability, where "d" is the total degree of the polynomial.
Related classes.
If the access to randomness is removed from the definition of BPP, we get the complexity class P. In the definition of the class, if we replace the ordinary Turing machine with a quantum computer, we get the class BQP.
Adding postselection to BPP, or allowing computation paths to have different lengths, gives the class BPPpath. BPPpath is known to contain NP, and it is contained in its quantum counterpart PostBQP.
A Monte Carlo algorithm is a randomized algorithm which is likely to be correct. Problems in the class BPP have Monte Carlo algorithms with polynomial bounded running time. This is compared to a Las Vegas algorithm which is a randomized algorithm which either outputs the correct answer, or outputs "fail" with low probability. Las Vegas algorithms with polynomial bound running times are used to define the class ZPP. Alternatively, ZPP contains probabilistic algorithms that are always correct and have expected polynomial running time. This is weaker than saying it is a polynomial time algorithm, since it may run for super-polynomial time, but with very low probability.
Complexity-theoretic properties.
It is known that BPP is closed under complement; that is, BPP = co-BPP. BPP is low for itself, meaning that a BPP machine with the power to solve BPP problems instantly (a BPP oracle machine) is not any more powerful than the machine without this extra power. In symbols, BPPBPP = BPP.
The relationship between BPP and NP is unknown: it is not known whether BPP is a subset of NP, NP is a subset of BPP or neither. If NP is contained in BPP, which is considered unlikely since it would imply practical solutions for NP-complete problems, then NP = RP and PH ⊆ BPP.
It is known that RP is a subset of BPP, and BPP is a subset of PP. It is not known whether those two are strict subsets, since we don't even know if P is a strict subset of PSPACE. BPP is contained in the second level of the polynomial hierarchy and therefore it is contained in PH. More precisely, the Sipser–Lautemann theorem states that formula_1. As a result, P = NP leads to P = BPP since PH collapses to P in this case. Thus either P = BPP or P ≠ NP or both.
Adleman's theorem states that membership in any language in BPP can be determined by a family of polynomial-size Boolean circuits, which means BPP is contained in P/poly. Indeed, as a consequence of the proof of this fact, every BPP algorithm operating on inputs of bounded length can be derandomized into a deterministic algorithm using a fixed string of random bits. Finding this string may be expensive, however.
Relative to oracles, we know that there exist oracles A and B, such that PA = BPPA and PB ≠ BPPB. Moreover, relative to a random oracle with probability 1, P = BPP and BPP is strictly contained in NP and co-NP.
Derandomization.
The existence of certain strong pseudorandom number generators is conjectured by most experts of the field. This conjecture implies that randomness does not give additional computational power to polynomial time computation, that is, P = RP = BPP. Note that ordinary generators are not sufficient to show this result; any probabilistic algorithm implemented using a typical random number generator will always produce incorrect results on certain inputs irrespective of the seed (though these inputs might be rare).
László Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson showed that unless EXPTIME collapses to MA, BPP is contained in 
The class i.o.-SUBEXP, which stands for infinitely often SUBEXP, contains problems which have sub-exponential time algorithms for infinitely many input sizes. They also showed that P = BPP if the exponential-time hierarchy, which is defined in terms of the polynomial hierarchy and E as EPH, collapses to E; however, note that the exponential-time hierarchy is usually conjectured "not" to collapse.
Russell Impagliazzo and Avi Wigderson showed that if any problem in E, where 
has circuit complexity 2Ω("n") then P = BPP.

</doc>
<doc id="4080" url="http://en.wikipedia.org/wiki?curid=4080" title="BQP">
BQP

In computational complexity theory, BQP (bounded error quantum polynomial time) is the class of decision problems solvable by a quantum computer in polynomial time, with an error probability of at most 1/3 for all instances. It is the quantum analogue of the complexity class BPP.
In other words, there is an algorithm for a quantum computer (a quantum algorithm) that solves the decision problem with "high" probability and is guaranteed to run in polynomial time. On any given run of the algorithm, it has a probability of at most 1/3 that it will give the wrong answer.
Similarly to other "bounded error" probabilistic classes the choice of 1/3 in the definition is arbitrary. We can run the algorithm a constant number of times and take a majority vote to achieve any desired probability of correctness less than 1, using the Chernoff bound. Detailed analysis shows that the complexity class is unchanged by allowing error as high as 1/2 − "n"−"c" on the one hand, or requiring error as small as 2−"nc" on the other hand, where "c" is any positive constant, and "n" is the length of input.
Definition.
BQP can also be viewed as a bounded-error uniform family of quantum circuits. A language "L" is in BQP if and only if there exists a polynomial-time uniform family of quantum circuits formula_1, such that
Quantum computation.
The number of qubits in the computer is allowed to be a polynomial function of the instance size. For example, algorithms are known for factoring an "n"-bit integer using just over 2"n" qubits (Shor's algorithm).
Usually, computation on a quantum computer ends with a measurement. This leads to a collapse of quantum state to one of the basis states. It can be said that the quantum state is measured to be in the correct state with high probability.
Quantum computers have gained widespread interest because some problems of practical interest are known to be in BQP, but suspected to be outside P. Some prominent examples are:
Relationship to other complexity classes.
This class is defined for a quantum computer and its natural corresponding class for an ordinary computer (or a Turing machine plus a source of randomness) is BPP. Just like P and BPP, BQP is low for itself, which means BQPBQP = BQP. Informally, this is true because polynomial time algorithms are closed under composition. If a polynomial time algorithm calls as a subroutine polynomially many polynomial time algorithms, the resulting algorithm is still polynomial time.
BQP contains P and BPP and is contained in AWPP, PP and PSPACE.
In fact, BQP is low for PP, meaning that a PP machine achieves no benefit from being able to solve BQP problems instantly, an indication of the possible difference in power between these similar classes.
As the problem of P ≟ PSPACE has not yet been solved, the proof of inequality between BQP and classes mentioned above is supposed to be difficult. The relation between BQP and NP is not known.
Adding postselection to BQP results in the complexity class PostBQP which is equal to PP.

</doc>
<doc id="4086" url="http://en.wikipedia.org/wiki?curid=4086" title="Brainfuck">
Brainfuck

Brainfuck is an esoteric programming language noted for its extreme minimalism. The language consists of only eight simple commands and an instruction pointer. It is designed to challenge and amuse programmers, and was not made to be suitable for practical use. It was created in 1993 by Urban Müller.
Language design.
Urban Müller created brainfuck in 1993 with the intention of designing a language which could be implemented with the smallest possible compiler, inspired by the 1024-byte compiler for the FALSE programming language. Several brainfuck compilers have been made smaller than 200 bytes. One compiler of 100 bytes is known to exist. The classic distribution is Müller's version 2, containing a compiler for the Amiga, an interpreter, example programs, and a readme document.
The language consists of eight commands, listed below. A brainfuck program is a sequence of these commands, possibly interspersed with other characters (which are ignored). The commands are executed sequentially, with some exceptions: an instruction pointer begins at the first command, and each command it points to is executed, after which it normally moves forward to the next command. The program terminates when the instruction pointer moves past the last command.
The brainfuck language uses a simple machine model consisting of the program and instruction pointer, as well as an array of at least 30,000 byte cells initialized to zero; a movable data pointer (initialized to point to the leftmost byte of the array); and two streams of bytes for input and output (most often connected to a keyboard and a monitor respectively, and using the ASCII character encoding).
Commands.
The eight language commands, each consisting of a single character:
codice_2 and codice_1 match as parentheses usually do: each codice_2 matches exactly one codice_1 and vice versa, the codice_2 comes first, and there can be no unmatched codice_2 or codice_1 between the two.
Brainfuck programs can be translated into C using the following substitutions, assuming codice_10 is of type codice_11 and has been initialized to point to an array of zeroed bytes:
As the name suggests, brainfuck programs tend to be difficult to comprehend. This is partly because any mildly complex task requires a long sequence of commands; partly it is because the program's text gives no direct indications of the program's state. These, as well as brainfuck's inefficiency and its limited input/output capabilities, are some of the reasons it is not used for serious programming. Nonetheless, like any Turing-complete language, brainfuck is theoretically capable of computing any computable function or simulating any other computational model, if given access to an unlimited amount of memory. A variety of brainfuck programs have been written. Although brainfuck programs, especially complicated ones, are difficult to write, it is quite trivial to write an interpreter for brainfuck in a more typical language such as C due to its simplicity. There even exists a brainfuck interpreter written in the brainfuck language itself.
Brainfuck's formal "parent language".
Except for its two I/O commands, brainfuck is a minor variation of the formal programming language P′′ created by Corrado Böhm in 1964. In fact, using six symbols equivalent to the respective brainfuck commands codice_12, codice_13, codice_14, codice_15, codice_2, codice_1, Böhm provided an explicit program for each of the basic functions that together serve to compute any computable function. So in a very real sense, the first "brainfuck" programs appear in Böhm's 1964 paper – and they were programs sufficient to prove Turing-completeness.
Examples.
Hello World!
The following program prints "Hello World!" and a newline to the screen:
For readability, this code has been spread across many lines and blanks and comments have been added. Brainfuck ignores all characters except the eight commands codice_18 so no special syntax for comments is needed (as long as the comments don't contain the command characters). The code could just as well have been written as:
ROT13.
This program enciphers its input with the ROT13 cipher. To do this, it must map characters A-M (ASCII 65-77) to N-Z (78-90), and vice versa. Also it must map a-m (97-109) to n-z (110-122) and vice versa. It must map all other characters to themselves; it reads characters one at a time and outputs their enciphered equivalents until it reads an EOF (here assumed to be represented as either -1 or "no change"), at which point the program terminates.
The basic approach used is as follows. Calling the input character "x", divide "x"-1 by 32, keeping quotient and remainder. Unless the quotient is 2 or 3, just output "x", having kept a copy of it during the division. If the quotient is 2 or 3, divide the remainder (("x"-1) modulo 32) by 13; if the quotient here is 0, output "x"+13; if 1, output "x"-13; if 2, output "x".
Regarding the division algorithm, when dividing "y" by "z" to get a quotient "q" and remainder "r", there is an outer loop which sets "q" and "r" first to the quotient and remainder of 1/"z", then to those of 2/"z", and so on; after it has executed "y" times, this outer loop terminates, leaving "q" and "r" set to the quotient and remainder of "y"/"z". (The dividend "y" is used as a diminishing counter that controls how many times this loop is executed.) Within the loop, there is code to increment "r" and decrement "y", which is usually sufficient; however, every "z"th time through the outer loop, it is necessary to zero "r" and increment "q". This is done with a diminishing counter set to the divisor "z"; each time through the outer loop, this counter is decremented, and when it reaches zero, it is refilled by moving the value from "r" back into it.
Portability issues.
Partly because Urban Müller did not write a thorough language specification, the many subsequent brainfuck interpreters and compilers have come to use slightly different dialects of brainfuck.
Cell size.
In the classic distribution, the cells are of 8-bit size (cells are bytes), and this is still the most common size. However, to read non-textual data, a brainfuck program may need to distinguish an end-of-file condition from any possible byte value; thus 16-bit cells have also been used. Some implementations have used 32-bit cells, 64-bit cells, or bignum cells with practically unlimited range, but programs that use this extra range are likely to be slow, since storing the value "n" into a cell requires Ω("n") time as a cell's value may only be changed by incrementing and decrementing.
In all these variants, the codice_19 and codice_20 commands still read and write data in bytes. In most of them, the cells wrap around, i.e. incrementing a cell which holds its maximal value (with the codice_12 command) will bring it to its minimal value and vice versa. The exceptions are implementations which are distant from the underlying hardware, implementations that use bignums, and implementations that try to enforce portability.
Fortunately, it is usually easy to write brainfuck programs that do not ever cause integer wraparound or overflow, and therefore don't depend on cell size. Generally this means avoiding increment of +255 (unsigned 8-bit wraparound), or avoiding overstepping the boundaries of [-128, +127] (signed 8-bit wraparound) (since there are no comparison operators, a program cannot distinguish between a signed and unsigned two's complement fixed-bit-size cell and negativeness of numbers is a matter of interpretation). For more details on integer wraparound, see the Integer overflow article.
Array size.
In the classic distribution, the array has 30,000 cells, and the pointer begins at the leftmost cell. Even more cells are needed to store things like the millionth Fibonacci number, and the easiest way to make the language Turing-complete is to make the array unlimited on the right.
A few implementations extend the array to the left as well; this is an uncommon feature, and therefore portable brainfuck programs do not depend on it.
When the pointer moves outside the bounds of the array, some implementations will give an error message, some will try to extend the array dynamically, some will not notice and will produce undefined behavior, and a few will move the pointer to the opposite end of the array. Some tradeoffs are involved: expanding the array dynamically to the right is the most user-friendly approach and is good for memory-hungry programs, but it carries a speed penalty. If a fixed-size array is used it is helpful to make it very large, or better yet let the user set the size. Giving an error message for bounds violations is very useful for debugging but even that carries a speed penalty unless it can be handled by the operating system's memory protections.
End-of-line code.
Different operating systems (and sometimes different programming environments) use subtly different versions of ASCII. The most important difference is in the code used for the end of a line of text. MS-DOS and Microsoft Windows use a CRLF, i.e. a 13 followed by a 10, in most contexts. UNIX and its descendants, including Linux and Mac OS X, use just 10, and older Macs use just 13. It would be unfortunate if brainfuck programs had to be rewritten for different operating systems. Fortunately, a unified standard is easy to find. Urban Müller's compiler and his example programs use 10, on both input and output; so do a large majority of existing brainfuck programs; and 10 is also more convenient to use than CRLF. Thus, brainfuck implementations should make sure that brainfuck programs that assume newline=10 will run properly; many do so, but some do not.
This assumption is also consistent with most of the world's sample code for C and other languages, in that they use '\n', or 10, for their newlines. On systems that use CRLF line endings, the C standard library transparently remaps "\n" to "\r\n" on output and "\r\n" to "\n" on input for streams not opened in binary mode.
End-of-file behavior.
The behavior of the "codice_19" command when an end-of-file condition has been encountered varies. Some implementations set the cell at the pointer to 0, some set it to the C constant EOF (in practice this is usually -1), some leave the cell's value unchanged. There is no real consensus; arguments for the three behaviors are as follows.
Setting the cell to 0 avoids the use of negative numbers, and makes it marginally more concise to write a loop that reads characters until EOF occurs. This is a language extension devised by Panu Kalliokoski.
Setting the cell to -1 allows EOF to be distinguished from any byte value (if the cells are larger than bytes), which is necessary for reading non-textual data; also, it is the behavior of the C translation of "codice_19" given in Müller's readme file. However, it is not obvious that those C translations are to be taken as normative.
Leaving the cell's value unchanged is the behavior of Urban Müller's brainfuck compiler. This behavior can easily coexist with either of the others; for instance, a program that assumes EOF=0 can set the cell to 0 before each "codice_19" command, and will then work correctly on implementations that do either EOF=0 or EOF="no change". It is so easy to accommodate the "no change" behavior that any brainfuck programmer interested in portability should do so.
Derivatives.
Many people have created brainfuck equivalents (languages with commands that directly map to brainfuck) or brainfuck derivatives (languages that extend its behavior or map it into new semantic territory).
Some examples:
However, there are also unnamed minor variants (or dialects), formed possibly as a result of inattention, of which some of the more common are:

</doc>
<doc id="4091" url="http://en.wikipedia.org/wiki?curid=4091" title="Bartolomeo Ammannati">
Bartolomeo Ammannati

Bartolomeo Ammannati (18 June 1511 – 13 April 1592) was an Italian architect and sculptor, born at Settignano, near Florence. He studied under Baccio Bandinelli and Jacopo Sansovino (assisting on the Library of St. Mark's, the "Biblioteca Marciana", Venice) and closely imitated the style of Michelangelo.
He was more distinguished in architecture than in sculpture. He designed many buildings in Rome, which included work at the Villa Giulia complex (in collaboration with Vignola and Vasari), also at Lucca and Florence. His work at the completion of Pitti Palace, commissioned by Eleonora of Toledo, wife of Cosimo I, is one of his most celebrated achievements (1558–1570), respecting the original style of Filippo Brunelleschi. He was also named "Console" of the prestigious Accademia delle Arti del Disegno of Florence, founded by the Duke Cosimo I, at 13 January 1563, under the influence of Vasari.
He was then employed in 1569 to build the beautiful bridge over the Arno, known as Ponte Santa Trinita and one of his most celebrated works. The three arches are elliptic, and though very light and elegant, have resisted the fury of the river, which has swept away several other bridges at different times. It was destroyed in 1944, during World War II, and rebuilt in 1957.
Another of his most important works was the marble and bronze "Fountain of Neptune" ("Fontana del Nettuno") for the Piazza della Signoria. The assignment was originally given to the ageing Bartolommeo Bandinelli. On his death, Ammannati won the competition for the continuing of this assignment over other famous sculptors, such as Benvenuto Cellini and Vincenzo Danti. He worked between 1563 and 1565 on the original block of marble (chosen by Bandinelli), together with his assistants, among which Giambologna. He took Grand Duke Cosimo I as model for Neptune's face. When the work on the ungainly sea god was finished, Michelangelo scoffed at Ammannati that he had ruined a beautiful piece of marble: "Ammannati, Ammanato, che bell' marmo hai rovinato!" Ammannati continued working on this fountain for another ten years, adding, in a mannerist style, around the perimeter suave bronze reclining river gods, laughing satyrs and marble sea horses emerging from the water. The whole gives nevertheless a coherent impression. The fountain served as an example for future fountain-makers.
Other famous sculptures by Ammannati include:
In 1550 Ammannati married Laura Battiferri, an elegant poet and an accomplished woman. Later in his life he had a religious crisis, influenced by Counter-Reformation piety, which resulted in condemning his own works depicting nudity, and he left all his possessions to the Jesuits.
He died in Florence in 1592.

</doc>
<doc id="4092" url="http://en.wikipedia.org/wiki?curid=4092" title="Bishop">
Bishop

A bishop (English derivation from the New Testament Greek , "epískopos", "overseer", "guardian") is an ordained or consecrated member of the Christian clergy who is generally entrusted with a position of authority and oversight.
Within the Roman Catholic, Eastern Orthodox, Oriental Orthodox, Anglican, Old Catholic and Independent Catholic churches and in the Assyrian Church of the East, bishops claim apostolic succession, a direct historical lineage dating back to the original Twelve Apostles. Within these churches, bishops are seen as those who possess the full priesthood and can ordain clergy – including other bishops. Some Protestant churches including the Lutheran and Methodist churches have bishops serving similar functions as well, though not always understood to be within apostolic succession in the same way. One who has been ordained deacon, priest, and then bishop is understood to hold the fullness of the (ministerial) priesthood, given responsibility by Christ to govern, teach and sanctify the Body of Christ, members of the Faithful. Priests, deacons and lay ministers cooperate and assist their bishop(s) in shepherding a flock.
Term.
The term "epískopos" meaning "bishop" in Greek, the early language of the Christian Church, was not from the earliest times clearly distinguished from the term presbýteros (literally: "elder" or "senior", origin of the modern English word priest), but the term was already clearly used in the sense of the order or office of bishop, distinct from that of presbyter in the writings of Ignatius of Antioch (died c. 108), and sources from the middle of the 2nd century undoubtedly set forth that all the chief centres of Christianity recognized and had the office of bishop, using a form of organization that remained universal until the Protestant Reformation.
History.
The earliest organization of the Church in Jerusalem was, according to most scholars, similar to that of Jewish synagogues, but it had a council or college of ordained presbyters ( "elders, priests"). In ( and ), we see a collegiate system of government in Jerusalem chaired by James the Just, according to tradition the first bishop of the city. In (), the Apostle Paul ordains presbyters in churches in Anatolia.
Often, the word "presbyter" was not yet distinguished from "overseer" (ἐπίσκοπος "episkopos", later used exclusively to mean "bishop"), as in (, and ) The earliest writings of the Apostolic Fathers, the Didache and the First Epistle of Clement, for example, show the church used two terms for local church offices—presbyters (seen by many as an interchangeable term with "episcopos" or overseer) and deacon.
This does not mean that the episcopate, in the sense of the holder of the order or office of bishop, must have developed only later, or have been plural, because in each church the college or presbyter-overseers (also called "presbyter-bishops") did not exercise an independent supreme power; it was subject to the Apostles or to their delegates. An explanation suggests that the delegates were bishops in the actual sense of the term, but that they did not possess fixed sees nor had they a special title. Since they were essentially itinerant, they confided to the care of some of the better educated and highly respected converts the fixed necessary functions relating to the daily life of the community.
In Timothy and Titus in the New Testament a more clearly defined episcopate can be seen. We are told that Paul had left Timothy in Ephesus and Titus in Crete to oversee the local church ( and ). Paul commands them to ordain presbyters/bishops and to exercise general oversight, telling Titus to "rebuke with all authority".()
Early sources are unclear but various groups of Christian communities may have had the bishop surrounded by a group or college functioning as leaders of the local churches. Eventually the head or "monarchic" bishop came to rule more clearly, and all local churches would eventually follow the example of the other churches and structure themselves after the model of the others with the one bishop in clearer charge, though the role of the body of priests remained important.
Eventually, as Christendom grew, bishops no longer directly served individual congregations. Instead, the Metropolitan bishop (the bishop in a large city) appointed priests to minister each congregation, acting as the bishop's delegate.
Apostolic Fathers.
Around the end of the 1st century, the church's organization becomes clearer in historical documents. In the works of the Apostolic Fathers, and Ignatius of Antioch in particular, the role of the episkopos, or bishop, became more important or, rather, already was very important and being clearly defined. While Ignatius of Antioch offers the earliest clear description of monarchial bishops (a single bishop over all house churches in a city) he is an advocate of monepiscopal structure rather than describing an accepted reality. To the bishops and house churches to which he writes, he offers strategies on how to pressure house churches who don't recognize the bishop into compliance. Other contemporary Christian writers do not describe monarchial bishops, either continuing to equate them with the presbyters or speaking of episkopoi (bishops, plural) in a city.
"Plainly therefore we ought to regard the bishop as the Lord Himself" — Epistle of Ignatius to the Ephesians 6:1.
"your godly bishop" — Epistle of Ignatius to the Magnesians 2:1.
"the bishop presiding after the likeness of God and the presbyters after the likeness of the council of the Apostles, with the deacons also who are most dear to me, having been entrusted with the diaconate of Jesus Christ" — Epistle of Ignatius to the Magnesians 6:1.
"Therefore as the Lord did nothing without the Father, [being united with Him], either by Himself or by the Apostles, so neither do ye anything without the bishop and the presbyters." — Epistle of Ignatius to the Magnesians 7:1.
"Be obedient to the bishop and to one another, as Jesus Christ was to the Father [according to the flesh], and as the Apostles were to Christ and to the Father, that there may be union both of flesh and of spirit." — Epistle of Ignatius to the Magnesians 13:2.
"In like manner let all men respect the deacons as Jesus Christ, even as they should respect the bishop as being a type of the Father and the presbyters as the council of God and as the college of Apostles. Apart from these there is not even the name of a church." — Epistle of Ignatius to the Trallesians 3:1.
"follow your bishop, as Jesus Christ followed the Father, and the presbytery as the Apostles; and to the deacons pay respect, as to God's commandment" — Epistle of Ignatius to the Smyrnans 8:1.
"He that honoureth the bishop is honoured of God; he that doeth aught without the knowledge of the bishop rendereth service to the devil" — Epistle of Ignatius to the Smyrnans 9:1.
— Lightfoot translation.
It is clear that a single bishop was expected to lead the church in each centre of Christian mission, supported by a council of presbyters (a distinct and subordinate position) with a pool of deacons. As the Church continued to expand, new churches in important cities gained their own bishop. Churches in the regions outside an important city were served by Chorbishop, an official rank of bishops. However, soon, presbyters and deacons were sent from bishop of a city church. Gradually priests replaced the chorbishops. Thus, in time, the bishop changed from being the leader of a single church confined to an urban area to being the leader of the churches of a given geographical area.
Clement of Alexandria (end of the 2nd century) writes about the ordination of a certain Zachæus as bishop by the imposition of Simon Peter Bar-Jonah's hands. The words bishop and ordination are used in their technical meaning by the same Clement of Alexandria. The bishops in the 2nd century are defined also as the only clergy to whom the ordination to priesthood (presbyterate) and diaconate is entrusted: "a priest (presbyter) lays on hands, but does not ordain." ("cheirothetei ou cheirotonei")
At the beginning of the 3rd century, Hippolytus of Rome describes another feature of the ministry of a bishop, which is that of the "Spiritum primatus sacerdotii habere potestatem dimittere peccata": the primate of sacrificial priesthood and the power to forgive sins.
Bishops and civil government.
The efficient organization of the Roman Empire became the template for the organisation of the church in the 4th century, particularly after Constantine's Edict of Milan. As the church moved from the shadows of privacy into the public forum it acquired land for churches, burials and clergy. In 391, Theodosius I decreed that any land that had been confiscated from the church by Roman authorities be returned.
The most usual term for the geographic area of a bishop's authority and ministry, the diocese, began as part of the structure of the Roman Empire under Diocletian. As Roman authority began to fail in the western portion of the empire, the church took over much of the civil administration. This can be clearly seen in the ministry of two popes: Pope Leo I in the 5th century, and Pope Gregory I in the 6th century. Both of these men were statesmen and public administrators in addition to their role as Christian pastors, teachers and leaders. In the Eastern churches, latifundia entailed to a bishop's see were much less common, the state power did not collapse the way it did in the West, and thus the tendency of bishops acquiring secular power was much weaker than in the West. However, the role of Western bishops as civil authorities, often called prince bishops, continued throughout much of the Middle Ages.
Bishops holding political office.
As well as being arch chancellors of the Holy Roman Empire after the 9th century, bishops generally served as chancellors to medieval monarchs, acting as head of the "justiciary" and chief chaplain. The Lord Chancellor of England was almost always a bishop up until the dismissal of Cardinal Thomas Wolsey by Henry VIII. Similarly, the position of Kanclerz in the Polish kingdom was always a bishop until the 16th century.
In France before the French Revolution, representatives of the clergy — in practice, bishops and abbots of the largest monasteries — comprised the First Estate of the Estates-General, until their role was abolished during the French Revolution.
In the 21st century, the more senior bishops of the Church of England continue to sit in the House of Lords of the Parliament of the United Kingdom, as representatives of the established church, and are known as Lords Spiritual. The Bishop of Sodor and Man, whose diocese lies outside of the United Kingdom, is an ex officio member of the Legislative Council of the Isle of Man. In the past, the Bishop of Durham, known as a prince bishop, had extensive viceregal powers within his northern diocese — the power to mint money, collect taxes and raise an army to defend against the Scots.
Eastern Orthodox bishops, along with all other members of the clergy, are canonically forbidden to hold political office. Occasional exceptions to this rule are tolerated when the alternative is political chaos. In the Ottoman Empire, the Patriarch of Constantinople, for example, had de facto administrative, fiscal, cultural and legal jurisdiction, as well as spiritual, over all the Christians of the empire. More recently, Archbishop Makarios III of Cyprus, served as President of the Republic of Cyprus from 1960 to 1977.
In 2001, Peter Hollingworth, AC, OBE – then the Anglican Archbishop of Brisbane – was controversially appointed Governor-General of Australia. Although Hollingworth gave up his episcopal position to accept the appointment, it still attracted considerable opposition in a country which maintains a formal separation between Church and State.
Episcopacy during the English Civil War.
During the period of the English Civil War, the role of bishops as wielders of political power and as upholders of the established church became a matter of heated political controversy. Indeed, Presbyterianism was the polity of most Reformed Churches in Europe, and had been favored by many in England since the English Reformation. Since in the primitive church the offices of "presbyter" and "episkopos" were identical, many Puritans held that this was the only form of government the church should have. The Anglican divine, Richard Hooker, objected to this claim in his famous work "Of the Laws of Ecclesiastic Polity" while, at the same time, defending Presbyterian ordination as valid (in particular Calvin's ordination of Beza). This was the official stance of the English Church until the Commonwealth, during which time, the views of Presbyterians and Independents (Congregationalists) were more freely expressed and practiced.
Churches.
Catholic Church, Orthodox churches and Anglican churches.
Bishops form the leadership in the Roman Catholic Church, the Eastern Orthodox Church, the Oriental Orthodox Churches, the Anglican Communion, the Lutheran Church, the Independent Catholic Churches, the Independent Anglican Churches, and certain other, smaller, denominations.
The traditional role of a bishop is as pastor of a diocese (also called a bishopric, synod, eparchy or see), and so to serve as a "diocesan bishop," or "eparch" as it is called in many Eastern Christian churches. Dioceses vary considerably in size, geographically and population-wise. Some dioceses around the Mediterranean Sea which were Christianised early are rather compact, whereas dioceses in areas of rapid modern growth in Christian commitment—as in some parts of Sub-Saharan Africa, South America and the Far East—are much larger and more populous.
As well as traditional diocesan bishops, many churches have a well-developed structure of church leadership that involves a number of layers of authority and responsibility.
Duties.
In Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, and Anglicanism, only a bishop can ordain other bishops, priests, and deacons.
In the Eastern liturgical tradition, a priest can celebrate the Divine Liturgy only with the blessing of a bishop. In Byzantine usage, an antimension signed by the bishop is kept on the altar partly as a reminder of whose altar it is and under whose omophorion the priest at a local parish is serving. In Syriac Church usage, a consecrated wooden block called a thabilitho is kept for the same reasons.
The pope, in addition to being the Bishop of Rome and spiritual head of the Catholic Church, is also the Patriarch of the Latin Rite. Each bishop within the Latin Rite is answerable directly to the Pope and not any other bishop except to metropolitans in certain oversight instances. The pope previously used the title "Patriarch of the West", but this title was dropped from use in 2006 a move which caused some concern within the Orthodox Communion as, to them, it implied wider papal jurisdiction.
In Catholic, Eastern Orthodox, Oriental Orthodox and Anglican cathedrals there is a special chair set aside for the exclusive use of the bishop. This is the bishop's "cathedra" and is often called the throne. In some Christian denominations, for example, the Anglican Communion, parish churches may maintain a chair for the use of the bishop when he visits; this is to signify the parish's union with the bishop.
The bishop is the ordinary minister of the sacrament of confirmation in the Latin Rite Catholic Church, and in the Anglican and Old Catholic communion only a bishop may administer this sacrament. However, in the Byzantine and other Eastern rites, whether Eastern or Oriental Orthodox or Eastern Catholic, chrismation is done immediately after baptism, and thus the priest is the one who confirms, using chrism blessed by a bishop.
Ordination of Catholic, Orthodox and Anglican bishops.
Bishops in all of these communions are ordained by other bishops through the laying on of hands. While traditional teaching maintains that any bishop with apostolic succession can validly perform the ordination of another bishop, some churches require two or three bishops participate, either to ensure sacramental validity or to conform with church law. Roman Catholic doctrine holds that one bishop can validly ordain another male (priest) as a bishop. Though a minimum of three bishops participating is desirable (there are usually several more) in order to demonstrate collegiality, canonically only one bishop is necessary. The practice of only one bishop ordaining was normal in countries where the Church was persecuted under Communist rule.
The title of archbishop or metropolitan may be granted to a senior bishop, usually one who is in charge of a large ecclesiastical jurisdiction. He may, or may not, have provincial oversight of suffragan bishops and may possibly have auxiliary bishops assisting him.
Ordination of a bishop, and thus continuation of apostolic succession, takes place through a ritual centred on the imposition of hands and prayer.
Apart from the ordination, which is always done by other bishops, there are different methods as to the actual selection of a candidate for ordination as bishop. In the Catholic Church the Congregation for Bishops oversees the selection of new bishops with the approval of the pope. The papal nuncio usually solicits names from the bishops of a country, and then selects three to be forwarded to the Holy See. Most Eastern Orthodox churches allow varying amounts of formalised laity and/or lower clergy influence on the choice of bishops. This also applies in those Eastern churches which are in union with the pope, though it is required that he give assent.
Catholic, Orthodox, Anglican, Old Catholic and some Lutheran bishops claim to be part of the continuous sequence of ordained bishops since the days of the apostles referred to as apostolic succession. Since Pope Leo XIII issued the bull "Apostolicae Curae" in 1896, the Catholic Church has insisted that Anglican orders are invalid because of changes in the Anglican ordination rites of the 16th century and divergence in understanding of the theology of priesthood, episcopacy and Eucharist. However, since the 1930s, Utrecht Old Catholic bishops (recognised by the Holy See as validily ordained) have sometimes taken part in the ordination of Anglican bishops. According to the writer Timothy Dufort, by 1969, all Church of England bishops had acquired Old Catholic lines of apostolic succession recognised by the Holy See. This development has muddied the waters somewhat as it could be argued that the strain of apostolic succession has been re-introduced into Anglicanism, at least within the Church of England.
The Catholic Church does recognise as valid (though illicit) ordinations done by breakaway Catholic, Old Catholic or Oriental bishops, and groups descended from them; it also regards as both valid and licit those ordinations done by bishops of the Eastern churches, so long as those receiving the ordination conform to other canonical requirements (for example, is an adult male) and an orthodox rite of episcopal ordination, expressing the proper functions and sacramental status of a bishop, is used; this has given rise to the phenomenon of "episcopi vagantes" (for example, clergy of the Independent Catholic groups which claim apostolic succession, though this claim is rejected by both Orthodoxy and Catholicism).
The Orthodox Churches would not accept the validity of any ordinations performed by the Independent Catholic groups, as Orthodoxy considers to be spurious any consecration outside of the Church as a whole. Orthodoxy considers apostolic succession to exist only within the Universal Church, and not through any authority held by individual bishops; thus, if a bishop ordains someone to serve outside of the (Orthodox) Church, the ceremony is ineffectual, and no ordination has taken place regardless of the ritual used or the ordaining prelate's position within the Orthodox Churches.
The position of Roman Catholicism is slightly different. Whilst it does recognise the validity of the orders of certain groups which separated from communion with Holy See. The Holy See accepts as valid the ordinations of the Old Catholics in communion with Utrecht, as well as the Polish National Catholic Church (which received its orders directly from Utrecht, and was—until recently—part of that communion); but Roman Catholicism does not recognise the orders of any group whose teaching is at variance with what they consider the core tenets of Christianity; this is the case even though the clergy of the Independent Catholic groups may use the proper ordination ritual. There are also other reasons why the Holy See does not recognise the validity of the orders of the Independent clergy:
Whilst members of the Independent Catholic movement take seriously the issue of valid orders, it is highly significant that the relevant Vatican Congregations tend not to respond to petitions from Independent Catholic bishops and clergy who seek to be received into communion with the Holy See, hoping to continue in some sacramental role. In those instances where the pope does grant reconciliation, those deemed to be clerics within the Independent Old Catholic movement are invariably admitted as laity and not priests or bishops.
There is a mutual recognition of the validity of orders amongst Roman Catholic, Eastern Orthodox, Old Catholic, Oriental Orthodox and Assyrian Nestorian churches.
Some provinces of the Anglican Communion have begun ordaining women as bishops in recent decades for example, the United States, New Zealand, Canada and Cuba. The first woman bishop within Anglicanism was Barbara Clementine Harris, who was ordained in the United States in 1989. In 2006, Katharine Jefferts Schori, the Episcopal Bishop of Nevada, became the first woman to become the Presiding Bishop of the Episcopal Church.
Lutheranism.
In the Evangelical Lutheran Church in America (ELCA) and the Evangelical Lutheran Church in Canada (ELCIC), the largest Lutheran Church bodies in the United States and Canada respectively and roughly based on the Nordic Lutheran state churches (similar to that of the Church of England), bishops are elected by Synod Assemblies, consisting of both lay members and clergy, for a term of 6 years, which can be renewed, depending upon the local synod's "constitution" (which is mirrored on either the ELCA or ELCIC's national constitution). Since the implementation of concordats between the ELCA and the Episcopal Church of the United States and the ELCIC and the Anglican Church of Canada, all bishops, including the Presiding Bishop (ELCA) or the National Bishop (ELCIC), have been consecrated using the historic succession, with at least one Anglican bishop serving as co-consecrator.
Since going into ecumenical communion with their respective Anglican body, bishops in the ELCA or the ELCIC not only approve the "rostering" of all ordained pastors, diaconal ministers, and associates in ministry, but they serve as the principal celebrant of all pastoral ordination and installation ceremonies, diaconal consecration ceremonies, as well as serving as the "chief pastor" of the local synod, upholding the teachings of Martin Luther as well as the documentations of the Ninety-Five Theses and the Augsburg Confession. Unlike their counterparts in the United Methodist Church, ELCA and ELCIC synod bishops do not appoint pastors to local congregations (pastors, like their counterparts in the Episcopal Church, are called by local congregations). The Presiding Bishop of the ELCA and the National Bishop of the ELCIC, the national bishops of their respective bodies, is elected for a single 6-year term and may be elected to an additional term.
Although ELCA agreed with the Episcopal Church to limit ordination to the bishop "ordinarily", ELCA pastor-"ordinators" are given permission to perform the rites in "extraordinary" circumstance. In practice, "extraordinary" circumstance have included disagreeing with Episcopalian views of the episcopate, and as a result, ELCA pastors ordained by other pastors are not permitted to be deployed to Episcopal Churches (they can, however, serve in Presbyterian Church USA, United Methodist Church, Reformed Church in America, and Moravian Church congregations, as the ELCA is in full communion with these denominations). The Lutheran Church–Missouri Synod (LCMS) and the Wisconsin Evangelical Lutheran Synod (WELS), the second and third largest Lutheran bodies in the United States and the two largest Confessional Lutheran bodies in North America, do not have a bishop as the head of the church or middle jurisdiction, practicing a form of congregationalism similar to the United Church of Christ. It should also be noted that the second largest of the three predecessor bodies of the ELCA, the American Lutheran Church, was a congregationalist body, with national and synod presidents before they were re-titled as bishops (borrowing from the Lutheran churches in Germany) in the 1980s.
Methodism.
United Methodist Church.
In the United Methodist Church (the largest branch of Methodism in the world) bishops serve as administrative and pastoral superintendents of the church. They are elected for life from among the ordained elders (presbyters) by vote of the delegates in regional (called jurisdictional) conferences, and are consecrated by the other bishops present at the conference through the laying on of hands. In the United Methodist Church bishops remain members of the "Order of Elders" while being consecrated to the "Office of the Episcopacy". Within the United Methodist Church only bishops are empowered to consecrate bishops and ordain clergy. Among their most critical duties is the ordination and appointment of clergy to serve local churches as pastor, presiding at sessions of the Annual, Jurisdictional, and General Conferences, providing pastoral ministry for the clergy under their charge, and safeguarding the doctrine and discipline of the Church. Furthermore, individual bishops, or the Council of Bishops as a whole, often serve a prophetic role, making statements on important social issues and setting forth a vision for the denomination, though they have no legislative authority of their own. In all of these areas, bishops of the United Methodist Church function very much in the historic meaning of the term. According to the "Book of Discipline of the United Methodist Church", a bishop's responsibilities are 
In each Annual Conference, United Methodist bishops serve for four-year terms, and may serve up to three terms before either retirement or appointment to a new Conference. United Methodist bishops may be male or female, with the Rev. Marjorie Matthews being the first woman to be consecrated a bishop in 1980.
The collegial expression of episcopal leadership in the United Methodist Church is known as the Council of Bishops. The Council of Bishops speaks to the Church and through the Church into the world and gives leadership in the quest for Christian unity and interreligious relationships. The "Conference of Methodist Bishops" includes the United Methodist "Council of Bishops" plus bishops from affiliated autonomous Methodist or United Churches.
John Wesley consecrated Thomas Coke a "General Superintendent," and directed that Francis Asbury also be consecrated for the United States of America in 1784, where the Methodist Episcopal Church first became a separate denomination apart from the Church of England. Coke soon returned to England, but Asbury was the primary builder of the new church. At first he did not call himself bishop, but eventually submitted to the usage by the denomination.
Notable bishops in United Methodist history include Coke, Asbury, Richard Whatcoat, Philip William Otterbein, Martin Boehm, Jacob Albright, John Seybert, Matthew Simpson, John S. Stamm, William Ragsdale Cannon, Marjorie Matthews, Leontine T. Kelly, William B. Oden, Ntambo Nkulu Ntanda, Joseph Sprague, William Henry Willimon, and Thomas Bickerton.
Christian Methodist Episcopal Church.
In the Christian Methodist Episcopal Church in the United States, bishops are administrative superintendents of the church; they are elected by "delegate" votes for as many years deemed until the age of 74, then he/she must retire. Among their duties, are responsibility for appointing clergy to serve local churches as pastor, for performing ordinations, and for safeguarding the doctrine and discipline of the Church. The General Conference, a meeting every four years, has an equal number of clergy and lay delegates. In each Annual Conference, CME bishops serve for four-year terms. CME Church bishops may be male or female.
The Church of Jesus Christ of Latter-day Saints.
In The Church of Jesus Christ of Latter-day Saints, the Bishop is the leader of a local congregation, called a ward. As with most LDS priesthood holders, the bishop is a part-time lay minister and earns a living through other employment; in all cases, he is a married man. As such, it is his duty to preside at services, call local leaders, and judge the worthiness of members for service. The bishop does not deliver sermons at every service (generally asking members to do so), but is expected to be a spiritual guide for his congregation. It is therefore believed that he has both the right and ability to receive divine inspiration (through the Holy Spirit) for the ward under his direction. Because it is a part-time position, all able members are expected to assist in the management of the ward by holding delegated lay positions (for example, women's and youth leaders, teachers) referred to as 'callings.' Although members are asked to confess serious sins to him, unlike the Roman Catholic Church, he is not the instrument of divine forgiveness, merely a guide through the repentance process (and a judge in case transgressions warrant excommunication or other official discipline). The bishop is also responsible for the physical welfare of the ward, and thus collects tithing and fast offerings and distributes financial assistance where needed.
A bishop is the president of the Aaronic priesthood in his ward (and is thus a form of Mormon Kohen; in fact, a literal descendant of Aaron has "legal right" to act as a Bishop after being found worthy and ordained by the First Presidency). In the absence of a literal descendant of Aaron, a High priest in the Melchizedek priesthood is called to be a Bishop. Each bishop is selected from resident members of the ward by the stake presidency with approval of the First Presidency, and chooses two "counselors" to form a "bishopric". In special circumstances (such as a ward consisting entirely of young university students), a bishop may be chosen from outside the ward. A bishop is typically released after about five years and a new bishop is called to the position. Although the former bishop is released from his duties, he continues to hold the Aaronic priesthood office of Bishop. Church members frequently refer to a former bishop as "Bishop" as a sign of respect and affection.
Latter-day Saint bishops do not wear any special clothing or insignia the way clergy in many other churches do, but are expected to dress and groom themselves neatly and conservatively per their local culture, especially when performing official duties. Bishops (as well as other members of the priesthood) can trace their line of authority back to Joseph Smith, Jr., who, according to church doctrine, was ordained to lead the Church in modern times by the ancient apostles Peter, James, and John, who were ordained to lead the Church by Jesus Christ.
The Presiding Bishop oversees the temporal affairs (buildings, properties, commercial corporations, and so on) of the worldwide Church, including the Church's massive global humanitarian aid and social welfare programs. The Presiding Bishop has two counselors; the three together form the Presiding Bishopric.
New Apostolic Church.
The New Apostolic Church (NAC) knows three classes of ministries: Deacons, Priests and Apostles. The Apostles, who are all included in the apostolate with the Chief Apostle as head, are the highest ministries.
Of the several kinds of priest...ministries, the bishop is the highest. Nearly all bishops are set in line directly from the chief apostle. They support and help their superior apostle.
Church of God (Cleveland, Tennessee).
In the polity of the Church of God (Cleveland, Tennessee), the international leader is the Presiding Bishop, and the members of the Executive Committee are Executive Bishops. Collectively, they supervise and appoint national and state leaders across the world. Leaders of individual states and regions are Administrative Bishops, who have jurisdiction over local churches in their respective states and are vested with appointment authority for local pastorates. All ministers are credentialed at one of three levels of licensure, the most senior of which is the rank of Ordained Bishop. To be eligible to serve in state, national, or international positions of authority, a minister must hold the rank of Ordained Bishop.
Pentecostal Church of God.
In 2002, the general convention of the Pentecostal Church of God came to a consensus to change the title of their overseer from General Superintendent to Bishop. The change was brought on because internationally, the term Bishop is more commonly related to religious leaders than the previous title.
The title Bishop is used for both the General (International leader) and the district (state) leaders. The title is sometimes used in conjunction with the previous thus becoming General (District) Superintendent/Bishop.
Others.
Some Baptists have begun taking on the title of Bishop.
In some smaller Protestant denominations and independent churches the term bishop is used in the same way as pastor, to refer to the leader of the local congregation, and may be male or female. This usage is especially common in African American churches in the USA. In the Church of Scotland, which has a Presbyterian church structure, the word "bishop" refers to an ordained person, usually a normal parish minister, who has temporary oversight of a trainee minister. In the Presbyterian Church (U.S.A.), the term bishop is an expressive name for a Minister of Word and Sacrament who serves a congregation and exercises "the oversight of the flock of Christ." The term is traceable to the 1789 Form of Government of the PC(U.S.A.) and the Presbyterian understanding of the pastoral office.
While not considered orthodox Christian, the Ecclesia Gnostica Catholica uses roles and titles derived from Christianity for its clerical hierarchy, including bishops who have much the same authority and responsibilities as in Roman Catholicism.
The Salvation Army does not have bishops but have appointed leaders of geographical areas known as Divisional Commanders. Larger geographical areas, called Territories, are led by a Territorial Commander, who is the highest-ranking officer in that Territory.
Dress and insignia.
Traditionally, a number of items are associated with the office of a bishop, most notably the mitre, crosier, and ecclesiastical ring. Other vestments and insignia vary between Eastern and Western Christianity.
In the Latin Rite of the Catholic Church, the choir dress of a bishop includes the purple cassock with amaranth trim, rochet, purple zucchetto (skull cap), purple biretta, and pectoral cross. The cappa magna may be worn, but only within the bishop's own diocese and on especially solemn occasions. The mitre, zuchetto, and stole are generally worn by bishops when presiding over liturgical functions. For liturgical functions other than the Mass the bishop typically wears the cope. Within his own diocese and when celebrating solemnly elsewhere with the consent of the local ordinary, he also uses the crosier. When celebrating Mass, a bishop, like a priest, wears the chasuble. The Caeremoniale Episcoporum recommends, but does not impose, that in solemn celebrations a bishop should also wear a dalmatic, which can always be white, beneath the chasuble, especially when administering the sacrament of holy orders, blessing an abbot or abbess, and dedicating a church or an altar. The Caeremoniale Episcoporum no longer makes mention of episcopal gloves, episcopal sandals, liturgical stockings (also known as buskins), or the accoutrements that it once prescribed for the bishop's horse. The coat of arms of a Latin Rite Catholic bishop usually displays a galero with a cross and crosier behind the escutcheon; the specifics differ by location and ecclesiastical rank (see Ecclesiastical heraldry).
Anglican bishops generally make use of the mitre, crosier, ecclesiastical ring, purple cassock, purple zucchetto, and pectoral cross. However, the traditional choir dress of Anglican bishops retains its late mediaeval form, and looks quite different from that of their Catholic counterparts; it consists of a long rochet which is worn with a chimere.
In the Eastern Churches (Eastern Orthodox, Eastern Rite Catholic) a bishop will wear the mandyas, panagia (and perhaps an enkolpion), sakkos, omophorion and an Eastern-style mitre. Eastern bishops do not normally wear an episcopal ring; the faithful kiss (or, alternatively, touch their forehead to) the bishop's hand. To seal official documents, he will usually use an inked stamp. An Eastern bishop's coat of arms will normally display an Eastern-style mitre, cross, eastern style crosier and a red and white (or red and gold) mantle. The arms of Oriental Orthodox bishops will display the episcopal insignia (mitre or turban) specific to their own liturgical traditions. Variations occur based upon jurisdiction and national customs.

</doc>
<doc id="4093" url="http://en.wikipedia.org/wiki?curid=4093" title="Bertrand Andrieu">
Bertrand Andrieu

Bertrand Andrieu (November 4, 1761 – December 10, 1822) was a French engraver of medals from Bordeaux. In France he was considered as the restorer of the art, which had declined after the time of Louis XIV, and during the last twenty years of his life the French government commissiond him to undertake several works. Many of his medals are figured in the "Medallic History of Napoleon".

</doc>
<doc id="4097" url="http://en.wikipedia.org/wiki?curid=4097" title="Bordeaux">
Bordeaux

Bordeaux (; Gascon: "Bordèu"; ) is a port city on the Garonne River in the Gironde department in southwestern France.
The small municipality (commune) of Bordeaux proper has a population of 239,399 (January 2011), but together with its suburbs and satellite towns Bordeaux forms the 6th-largest metropolitan area in France with a population of 1,140,668 at the January 2011 census. It is the capital of the Aquitaine region, as well as the prefecture of the Gironde department. Its inhabitants are called "Bordelais" (for men) or "Bordelaises" (women). The term "Bordelais" may also refer to the city and its surrounding region.
The city's nicknames are "La perle d'Aquitaine" (The Pearl of Aquitaine), and "La Belle Endormie" (Sleeping Beauty) in reference to the old center which had black walls due to pollution. Nowadays, this is not the case. In fact, a part of the city, Le Port de La Lune, was almost completely renovated.
Bordeaux is the world's major wine industry capital. It is home to the world's main wine fair, , while the wine economy in the metro area takes in 14.5 billion euros each year. Bordeaux wine has been produced in the region since the 8th century. The historic part of the city is on the UNESCO World Heritage List as "an outstanding urban and architectural ensemble" of the 18th century.
History.
 
In historical times, around 300 BCE it was the settlement of a Celtic tribe, the Bituriges Vivisci, who named the town "Burdigala", probably of Aquitainian origin. The name Bourde is still the name of a river south of the city.
In 107 BCE, the Battle of Burdigala was fought by the Romans who were defending the Allobroges, an allied Roman tribe, and the Tigurini led by Divico. The Romans were defeated and their commander, the consul Lucius Cassius Longinus was killed in the action.
The city fell under Roman rule around 60 BC, its importance lying in the commerce of tin and lead towards Rome. Later it became capital of Roman Aquitaine, flourishing especially during the Severan dynasty (3rd century). In 276 it was sacked by the Vandals. Further ravage was brought by the same Vandals in 409, the Visigoths in 414 and the Franks in 498, beginning a period of obscurity for the city.
In the late 6th century, the city re-emerged as the seat of a county and an archdiocese within the Merovingian kingdom of the Franks, but royal Frankish power was never strong. The city started to play a regional role as a major urban center on the fringes of the newly founded Frankish Duchy of Vasconia. Circa 585, a certain Gallactorius is cited as count of Bordeaux and fighting the Basques.
The city was plundered by the troops of Abd er Rahman in 732, after he had defeated Duke Eudes in the Battle of the River Garonne near Bordeaux and before the former was killed during the Battle of Tours on 10 October. After Duke Eudes's defeat, Aquitaine pledged allegiance formally to the new rising Carolingian dynasty, but still remained out of Frankish central rule until 768 (Duke Waifer defeated). In 736, the Aquitanian duke Hunald led a rebellion after his father Eudes's death, at which Charles responded by sending an expedition that captured and plundered Bordeaux again, while the Frankish commander didn't retain it for long, since he left south-east to wage war in Narbonnaise.
In 778, Seguin (or Sihimin) was appointed count of Bordeaux, probably undermining the power of the Duke Lupo, and possibly leading to the Battle of Roncevaux Pass that very year. In 814, Seguin was made Duke of Vasconia, but he was deposed in 816 for failing to suppress or sympathise with a Basque rebellion. Under the Carolingians, sometimes the Counts of Bordeaux held the title concomitantly with that of Duke of Vasconia. They were meant to keep in check the Basques and defend the mouth of the Garonne from the Vikings when the latter appeared c. 844 in the region of Bordeaux. In Autumn 845, count Seguin II marched on the Vikings assaulting Bordeaux and Saintes but was captured and put to death. There are no bishops mentioned during the whole 8th century and part of the 9th in Bordeaux.
From the 12th to the 15th century, Bordeaux regained importance following the marriage of Duchess Eleanor of Aquitaine with the French-speaking Count Henri Plantagenet, born in Le Mans, who became, within months of their wedding, King Henry II of England. The city flourished, primarily due to wine trade, and the cathedral of St. André was built. It was also the capital of an independent state under Edward, the Black Prince (1362–1372), but in the end, after the Battle of Castillon (1453) it was annexed by France which extended its territory. The "Château Trompette" (Trumpet Castle) and the "Fort du Hâ", built by Charles VII of France, were the symbols of the new domination, which however deprived the city of its richness by halting the wine commerce with England.
In 1462, Bordeaux obtained a parliament, but regained importance only in the 16th century when it became the center of the distribution of sugar and slaves from the West Indies along with the traditional wine.
Bordeaux adhered to the Fronde, being effectively annexed to the Kingdom of France only in 1653, when the army of Louis XIV entered the city.
The 18th century was the golden age of Bordeaux. Many downtown buildings (about 5,000), including those on the quays, are from this period. Victor Hugo found the town so beautiful he once said: "take Versailles, add Antwerp, and you have Bordeaux". Baron Haussmann, a long-time prefect of Bordeaux, used Bordeaux's 18th-century big-scale rebuilding as a model when he was asked by Emperor Napoleon III to transform a then still quasi-medieval Paris into a "modern" capital that would make France proud.
In 1870, at the beginning of the Franco-Prussian war against Prussia, the French government relocated to Bordeaux from Paris. This happened again during the First World War and again very briefly during the Second World War, when it became clear that Paris would soon fall into German hands. However, on the last of these occasions the French capital was soon moved again to Vichy. In May and June 1940, Bordeaux was the site of the life-saving actions of the Portuguese consul-general, Aristides de Sousa Mendes, who illegally granted thousands of Portuguese Visas, which were needed to pass the Spanish Border, to refugees fleeing the German Occupation.
From 1940 to 1943, the Italian Royal Navy ("Regia Marina Italiana") established BETASOM, a submarine base at Bordeaux. Italian submarines participated in the Battle of the Atlantic from this base which was also a major base for German U-boats as headquarters of 12th U-boat Flotilla. The massive, reinforced concrete U-boat pens have proved impractical to demolish and are now partly used as a cultural center for exhibitions.
Geography.
Bordeaux is located close to the European Atlantic coast, in the southwest of France and in the north of the Aquitaine region. It is around southwest of Paris. The city is built on a bend of the river Garonne, and is divided into two parts: the right bank to the east and left bank in the west. Historically the left bank is more developed because when flowing outside the bend, the water makes a furrow of the required depth to allow the passing of merchant ships, which used to offload on this side of the river. In Bordeaux, the Garonne River is accessible to ocean liners. The right bank of the Garonne is a low-lying, often marshy plain.
Climate.
Bordeaux's climate is usually classified as an oceanic climate (Köppen climate classification "Cfb"); however, the summers tend to be warmer and the winters milder than most areas of similar classification. Substantial summer rainfall prevents its climate from being classified as Mediterranean.
Winters are mild because of the prevalence of westerly winds from the Atlantic. Summers are warm and long due to the influence from the Bay of Biscay (surface temperature reaches . The average seasonal winter temperature is , but recent winters have been warmer than this. Frosts in the winter are commonplace, occurring several times during a winter, but snowfall is very rare, occurring only once every three years. The average summer seasonal temperature is . The summer of 2003 set a record with an average temperature of .
Economy.
Wine.
The vine was introduced to the Bordeaux region by the Romans, probably in the mid-1st century, to provide wine for local consumption, and wine production has been continuous in the region since then.
Bordeaux now has about of vineyards, 57 appellations, 10,000 wine-producing châteaux and 13,000 grape growers. With an annual production of approximately 960 million bottles, Bordeaux produces large quantities of everyday wine as well as some of the most expensive wines in the world. Included among the latter are the area's five "premier cru" (first growth) red wines (four from Médoc and one, Château Haut-Brion, from Graves), established by the Bordeaux Wine Official Classification of 1855:
The first growths are:
Both red and white wines are made in Bordeaux. Red Bordeaux is called claret in the United Kingdom. Red wines are generally made from a blend of grapes, and may be made from Cabernet Sauvignon, Merlot, Cabernet Franc, Petit verdot, Malbec, and, less commonly in recent years, Carménère. White Bordeaux is made from Sauvignon blanc, Sémillon, and Muscadelle. Sauternes is a subregion of Graves known for its intensely sweet, white, dessert wines such as Château d'Yquem.
Because of a wine glut (wine lake) in the generic production, the price squeeze induced by an increasingly strong international competition, and vine pull schemes, the number of growers has recently dropped from 14,000 and the area under vine has also decreased significantly. In the meanwhile however, the global demand for the first growths and the most famous labels markedly increased and their prices skyrocketed.
Others.
The Laser Mégajoule will be one of the most powerful lasers in the world, allowing fundamental research and the development of the laser and plasma technologies. This project, carried by the French Ministry of Defence, involves an investment of 2 billion euros. The "Road of the lasers", a major project of regional planning, promotes regional investment in optical and laser related industries leading to the Bordeaux area having the most important concentration of optical and laser expertise in Europe.
20,000 people work for the aeronautic industry in Bordeaux. The city has some of the biggest companies including Dassault, EADS Sogerma, Snecma, Thales, SNPE, and others. The Dassault Falcon private jets are built there as well as the military aircraft Rafale and Mirage 2000, the Airbus A380 cockpit, the boosters of Ariane 5, and the M51 SLBM missile.
Tourism, especially wine tourism, is a major industry.
Access to the port from the Atlantic is via the Gironde estuary. Almost 9 million tons of goods arrive and leave each year.
Population.
At the January 2011 census, there were 239,399 inhabitants in the city proper (commune) of Bordeaux. The majority of the population is French, but there are sizable groups of Italians, Spaniards (Up to 20% of the Bordeaux population claim some degree of Spanish heritage), Portuguese, Turks, Germans, North Africans, Afro-Caribbean, Sub-Saharan Afican and Asian (mostly from China and Vietnam).. 
The built-up area has grown for more than a century beyond the municipal borders of Bordeaux due to urban sprawl, so that by the January 2011 census there were 1,140,668 people living in the overall metropolitan area of Bordeaux, only a fifth of whom lived in the city proper.
Education.
University.
The university was created by the archbishop Pey Berland in 1441 and was abolished in 1793, during the French Revolution, before reappearing in 1808 with Napoleon I. Bordeaux accommodates approximately 70,000 students on one of the largest campuses of Europe (235 ha).
The University of Bordeaux is divided into four:
Schools.
Bordeaux has numerous public and private schools offering undergraduate and postgraduate programs.
Engineering schools:
Business and management schools:
Other:
Main sights.
Bordeaux is classified "City of Art and History". The city is home to 362 "monuments historiques" (only Paris has more in France) with some buildings dating back to Roman times. Bordeaux has been inscribed on UNESCO World Heritage List as "an outstanding urban and architectural ensemble".
Bordeaux is home to one of Europe's biggest 18th-century architectural urban areas, making it a sought-after destination for tourists and cinema production crews. It stands out as one of the first French cities, after Nancy, to have entered an era of urbanism and metropolitan big scale projects, with the team Gabriel father and son, architects for King Louis XV, under the supervision of two intendants (Governors), first Nicolas-François Dupré de Saint-Maur then the Marquis (Marquess) de Tourny.
Buildings.
Main sights include:
Saint-André Cathedral, Saint-Michel Basilica and Saint-Seurin Basilica are part of the World Heritage Sites of the Routes of Santiago de Compostela in France.
Parks and gardens.
"Le Jardin Public" is a park in the heart of the city.
Pont Jacques Chaban-Delmas.
Europe’s largest lift bridge has been opened in Bordeaux spanning over the River Garonne. The central lift span is 117m long and can be lifted vertically up to 53m to let tall ships pass underneath. The €160 million bridge was inaugurated by President François Hollande and Mayor Alain Juppé on 16 March 2013. The Bridge was named after the late Jacques Chaban-Delmas, who was a former Prime Minister and Mayor of Bordeaux.
Shopping.
Bordeaux has many shopping options. In the heart of Bordeaux is "Rue Sainte-Catherine". This pedestrian only shopping street has of shops, restaurants and cafés; it is also one of the longest shopping streets in Europe. "Rue Sainte-Catherine" starts at "Place de la Victoire" and ends at "Place de la Comédie" by the "Grand Théâtre". The shops become progressively more upmarket as one moves towards "Place de la Comédie" and the nearby "Cours de l'Intendance" is where one finds the more exclusive shops and boutiques.
Culture.
Bordeaux is also the first city in France to have created, in the 1980s, an architecture exhibition and research center, "Arc en rêve".
Bordeaux offers a large number of cinemas, theatres and is the home of the Opéra national de Bordeaux. There are many music venues of varying capacity. The city also offers several festivals throughout the year.
Transport.
Road.
Bordeaux is an important road and motorway junction. The city is connected to Paris by the A10 motorway, with Lyon by the A89, with Toulouse by the A62, and with Spain by the A63. There is a ring road called the "Rocade" which is often very busy. Another ring road is under consideration.
Bordeaux has five road bridges that cross the Garonne, the Pont de pierre built in the 1820s and three modern bridges built after 1960: the Pont Saint Jean, just south of the Pont de pierre (both located downtown), the Pont d'Aquitaine, a suspended bridge downstream from downtown, and the Pont François Mitterrand, located upstream of downtown. These two bridges are part of the ring road around Bordeaux. A fifth bridge, the Pont Jacques-Chaban-Delmas, was constructed in 2009–2012 and opened to traffic in March 2013. Located halfway between the Pont de pierre and the Pont d'Aquitaine and serving downtown rather than highway traffic, it is a vertical-lift bridge with a height comparable to the Pont de pierre in closed position, and to the Pont d'Aquitaine in open position. All five road bridges, including the two highway bridges, are open to cyclists and pedestrians as well.
Another bridge, the Pont Jean-Jacques Bosc, is to be built in 2018.
Lacking any steep hills, Bordeaux is relatively friendly to cyclists. Cycle paths (separate from the roadways) exist on the highway bridges, along the riverfront, on the university campuses, and incidentally elsewhere in the city. Cycle lanes and bus lanes that explicitly allow cyclists exist on many of the city's boulevards. A paid Bicycle sharing system with automated stations has been established in 2010.
Rail.
The main railway station, Gare de Bordeaux Saint-Jean, near the center of the city, has 4 million passengers a year. It is served by the French national (SNCF) railway's high speed train, the TGV, that gets to Paris in three hours, with connections to major European centers such as Lille, Brussels, Amsterdam, Cologne, Geneva and London. The TGV also serves Toulouse and Irun from Bordeaux. A regular train service is provided to Nantes, Nice, Marseille and Lyon. The Gare Saint-Jean is the major hub for regional trains (TER) operated by the SNCF to Arcachon, Limoges, Agen, Périgueux, Pau, Le Médoc, Angoulême and Bayonne.
Historically the train line used to terminate at a station on the right bank of the river Garonne near the Pont de Pierre, and passengers crossed the bridge to get into the city. Subsequently a double-track steel railway bridge was constructed in the 1850s, by Gustave Eiffel, to bring trains across the river direct into Gare de Bordeaux Saint-Jean. The old station was later converted and in 2010 comprised a cinema and restaurants.
The two-track Eiffel bridge with a speed limit of became a bottleneck and a new bridge was built, opening in 2009. The new bridge has 4 tracks and allows trains to pass at . During the planning there was much lobbying by the Eiffel family and other supporters to preserve the old bridge as a footbridge across the Garonne, with possibly a museum to document the history of the bridge and Gustave Eiffel's contribution. The decision was taken to save the bridge, but by early 2010 no plans had been announced as to its future use. The bridge remains intact, but unused and without any means of access.
Air.
Bordeaux is served by an international airport, Aéroport de Bordeaux Mérignac, located from the city center in the suburban city of Mérignac.
Trams, buses and boats.
Bordeaux has an important public transport system called Tram et Bus de la CUB (TBC). This company is run by the Keolis group. The network consists of:
This network is operated from 5 am to 1 am.
There had been several plans for a subway network to be set up, but they stalled for both geological and financial reasons. Work on the Tramway de Bordeaux system was started in the autumn of 2000, and services started in December 2003 connecting Bordeaux with its suburban areas. The tram system uses ground-level power supply technology (APS), a new cable-free technology developed by French company Alstom and designed to preserve the aesthetic environment by eliminating overhead cables in the historic city. Conventional overhead cables are used outside the city. The system was controversial for its considerable cost of installation, maintenance and also for the numerous initial technical problems that paralysed the network. Many streets and squares along the tramway route became pedestrian areas, with limited access for cars.
Taxi.
There are more than 400 taxicabs in Bordeaux.
Sport.
The 35,000-capacity Stade Chaban-Delmas is the largest stadium in Bordeaux. It was a venue for the FIFA World Cup in 1938 and 1998, as well as the 2007 Rugby World Cup. In the 1938 FIFA World Cup, it hosted a violent quarter-final known as the Battle of Bordeaux. The ground was formerly known as the "Stade du Parc Lescure" until 2001, when it was renamed in honour of the city's long-time mayor, Jacques Chaban-Delmas.
There are two major sport teams in Bordeaux, both playing at the Stade Chaban-Delmas. Girondins de Bordeaux is the football team, currently playing in Ligue 1 in the French football championship. Union Bordeaux Bègles is a rugby team in the Top 14 in the Ligue Nationale de Rugby.
Skateboarding, rollerblading, and BMX biking are activities enjoyed by many young inhabitants of the city. Bordeaux is home to a beautiful quai which runs along the Gironde river. On the Quai there is a skatepark divided into three sections. One section is for Vert tricks, one for street style tricks, and one for little action sports athletes with easier features and softer materials. The skatepark is very well maintained by the municipal workers.
A new stadium will open in 2015 to replace the Chaban-Delmas stadium. It will have a capacity of 42500 people.
Bordeaux is also the home of one of the strongest cricket teams in France and are the current champions of the South West League.
There is a wooden velodrome, Vélodrome du Lac, in Bordeaux which hosts international cycling competition in the form of UCI Track Cycling World Cup events.
People.
Bordeaux was the birthplace of:
International relationship.
Twin towns and sister cities.
Bordeaux is twinned with:

</doc>
<doc id="4098" url="http://en.wikipedia.org/wiki?curid=4098" title="Puzzle Bobble">
Puzzle Bobble

, also known as Bust-a-Move, is a 1994 tile-matching arcade puzzle video game for one or two players created by Taito Corporation. It is based on Taito's popular 1986 arcade game "Bubble Bobble", featuring characters and themes from that game. Its characteristically "cute" Japanese animation and music, along with its play mechanics and level designs, made it successful as an arcade title and spawned several sequels and ports to home gaming systems.
Versions.
Two different versions of the original game were released. "Puzzle Bobble" was originally released in Japan only in June 1994 by Taito Corporation, running on Taito's B System hardware (with the preliminary title "Bubble Buster"). Then, 6 months later in December, the international Neo Geo version of "Puzzle Bobble" was released. It was almost identical aside from being in stereo and having some different sound effects and translated text.
When set to the US region, the Neo-Geo version displays the alternative title "Bust a Move" and features anti-drugs and anti-littering messages in the title sequence. The Bust-a-Move title was used for all subsequent games in the series in the United States and Canada, as well as for some (non-Taito published) console releases in Europe.
Gameplay.
At the start of each round, the rectangular playing arena contains a prearranged pattern of coloured "bubbles". (These are actually referred to in the translation as "balls"; however, they were clearly intended to be bubbles, since they pop, and are taken from "Bubble Bobble".) At the bottom of the screen, the player controls a device called a "pointer", which aims and fires bubbles up the screen. The colour of bubbles fired is randomly generated and chosen from the colors of bubbles still left on the screen.
The fired bubbles travel in straight lines (possibly bouncing off the side walls of the arena), stopping when they touch other bubbles or reach the top of the arena. If a bubble touches identically-colored bubbles, forming a group of three or more, those bubbles—as well as any bubbles hanging from them—are removed from the field of play, and points are awarded.
After every few shots, the "ceiling" of the playing arena drops downwards slightly, along with all the bubbles stuck to it. The number of shots between each drop of the ceiling is influenced by the number of bubble colors remaining. The closer the bubbles get to the bottom of the screen, the faster the music plays and if they cross the line at the bottom then the game is over.
The objective of the game is to clear all the bubbles from the arena without any bubble crossing the bottom line. Bubbles will fire automatically if the player remains idle. After clearing the arena, the next round begins with a new pattern of bubbles to clear.
Scoring system.
As with many popular arcade games, experienced players (who can complete the game relatively easily) become much more interested in the secondary challenge of obtaining a high score (which involves a lot more skill and strategy). "Puzzle Bobble" caters to this interest very well, featuring an exponential scoring system which allows extremely high scores to be achieved.
"Popped" bubbles (that is, bubbles of the same color which disappear) are worth 10 points each. However, "dropped" bubbles (that is, bubbles that were hanging from popped bubbles), are worth far more: one dropped bubble scores 20 points; two score 40; three score 80. This figure continues doubling for each bubble dropped, up to 17 or more bubbles which scores 1,310,720 points. It is possible to achieve this maximum on most rounds (sometimes twice or more), resulting in a potential total score of 30 million and beyond.
Bonus points are also awarded for completing a round quickly. The maximum 50,000-point bonus is awarded for clearing a round in 5 seconds or less; this bonus then drops down to zero over the next minute, after which no bonus is awarded.
Two player mode.
There are no rounds in the two player game. Both players have an arena each (both visible on screen) and an identical arrangement of colored bubbles in each arena. When a player removes a large group (four bubbles or more) some of those removed are transferred to the opponent's arena, usually delaying their efforts to remove all the bubbles from their individual arena. In some versions, the two player game can also be played by one player against a computer opponent.
Connections with Bubble Bobble.
The characters and theme of the game are based on the 1986 platform arcade game "Bubble Bobble". An arrangement of the original "Bubble Bobble" BGM is played in the game's end credits.
The two dinosaurs operating the pointer are called "Bub" and "Bob" (or "Bubblun" and "Bobblun" in Japan). Their graphics and animation are based directly on the original "Bubble Bobble", only larger (very similar to Bubble Symphony which was released less than a month later). Less obvious is the fact that "Puzzle Bobble" also features all the enemies from "Bubble Bobble", which are trapped inside the bubbles and fly out when the bubbles pop. Inspecting the bubbles closely, one can see the enemies twitching inside the bubbles.
Clones.
Many popular clones of "Puzzle Bobble" have been produced, including:

</doc>
<doc id="4099" url="http://en.wikipedia.org/wiki?curid=4099" title="Bone">
Bone

Bones are rigid organs that constitute part of the endoskeleton of vertebrates. They support and protect the various organs of the body, produce red and white blood cells and store minerals. Bone tissue is a type of dense connective tissue. Bones come in a variety of shapes and have a complex internal and external structure, are lightweight yet strong and hard, and serve multiple functions. One of the types of tissue that makes up bone is the mineralized osseous tissue, also called bone tissue, that gives it rigidity and a coral-like three-dimensional internal structure. Other types of tissue found in bones include marrow, endosteum, periosteum, nerves, blood vessels and cartilage. At birth, there are over 270 bones in an infant human's body, but many of these fuse together as the child grows, leaving a total of 206 separate bones in a typical adult, not counting numerous small sesamoid bones and ossicles. The largest bone in the human body is the femur and the smallest bone of the 206 is the stapes.
Functions.
Bones have eleven main functions:
Mechanical properties.
The primary tissue of bone, osseous tissue, is a relatively hard and lightweight composite material. It is mostly made up of a composite material incorporating the mineral calcium phosphate in the chemical arrangement termed calcium hydroxylapatite (this is the osseous tissue that gives bones their rigidity) and collagen, an elastic protein which improves fracture resistance. It has relatively high compressive strength of about 170 MPa (1800 kgf/cm²) but poor tensile strength of 104–121 MPa and very low shear stress strength (51.6 MPa), meaning it resists pushing forces well, but not pulling or torsional forces. While bone is essentially brittle, it does have a significant degree of elasticity, contributed chiefly by collagen. All bones consist of living cells embedded in the mineralized organic matrix that makes up the osseous tissue.
Structure.
Bone structure.
Bone is not a uniformly solid material, but is rather a complex set of materials.
Compact (cortical) bone.
The hard outer layer of bones is composed of compact bone tissue, so-called due to its minimal gaps and spaces. Its porosity is 5–30%. This tissue gives bones their smooth, white, and solid appearance, and accounts for 80% of the total bone mass of an adult skeleton. Compact bone may also be referred to as dense bone.
Trabecular (cancellous or spongy) bone.
Filling the interior of the bone is the trabecular bone tissue (an open cell porous network also called cancellous or spongy bone), which is composed of a network of rod- and plate-like elements that make the overall organ lighter and allow room for blood vessels and marrow. Trabecular bone accounts for the remaining 20% of total bone mass but has nearly ten times the surface area of compact bone. Its porosity is 30–90%. If, for any reason, there is an alteration in the strain the cancellous is subjected to, there is a rearrangement of the trabeculae. The microscopic difference between compact and cancellous bone is that compact bone consists of haversian sites and osteons, while cancellous bones do not. Also, bone surrounds blood in the compact bone, while blood surrounds bone in the cancellous bone.
In birds.
It is a common misconception that bird skeletons are lightweight. While their bones are smaller and thinner, bird bone tissue is generally denser than mammalian bone tissue, allowing them to be stronger and offer more support with less volume and weight. Among mammals, bats come closest to birds in terms of bone density, suggesting that small dense bones are a flight adaptation. Many bird bones have little marrow due to being hollow, though not all bird bones are hollow.
Cellular structure.
There are several types of cells constituting the typical bone:
Molecular structure.
Matrix.
The majority of bone is made of the bone matrix. It is composed primarily of inorganic hydroxyapatite and organic collagen. Bone is formed by the hardening of this matrix around entrapped cells. When these cells become entrapped from osteoblasts they become osteocytes.
Inorganic.
The inorganic composition of bone (bone mineral) is formed from carbonated hydroxyapatite (Ca10(PO4)6(OH)2) with lower crystallinity.
The matrix is initially laid down as unmineralised osteoid (manufactured by osteoblasts). Mineralisation involves osteoblasts secreting vesicles containing alkaline phosphatase. This cleaves the phosphate groups and acts as the foci for calcium and phosphate deposition. The vesicles then rupture and act as a centre for crystals to grow on.
More particularly, bone mineral is formed from globular and plate structures, distributed among the collagen fibrils of bone and forming yet larger structure.
Organic.
The organic part of matrix is mainly composed of Type I collagen. This is synthesised intracellularly as tropocollagen and then exported, forming fibrils. The organic part is also composed of various growth factors, the functions of which are not fully known. Factors present include glycosaminoglycans, osteocalcin, osteonectin, bone sialo protein, osteopontin and Cell Attachment Factor.
Woven vs. lamellar bone.
Two types of bone can be identified microscopically according to the pattern of collagen forming the osteoid (collagenous support tissue of type I collagen embedded in glycosaminoglycan gel):
Woven bone is produced when osteoblasts produce osteoid rapidly, which occurs initially in all fetal bones (but is later replaced by more resilient lamellar bone). In adults woven bone is created after fractures or in Paget's disease. Woven bone is weaker, with a smaller number of randomly oriented collagen fibers, but forms quickly; it is for this appearance of the fibrous matrix that the bone is termed "woven". It is soon replaced by lamellar bone, which is highly organized in concentric sheets with a much lower proportion of osteocytes to surrounding tissue. Lamellar bone, which makes its first appearance in the fetus during the third trimester, is stronger and filled with many collagen fibers parallel to other fibers in the same layer (these parallel columns are called osteons). In cross-section, the fibers run in opposite directions in alternating layers, much like in plywood, assisting in the bone's ability to resist torsion forces. After a fracture, woven bone forms initially and is gradually replaced by lamellar bone during a process known as "bony substitution." Compared to woven bone, lamellar bone formation takes place more slowly. The orderly deposition of collagen fibers restricts the formation of osteoid to about 1 to 2 µm per day. Lamellar bone also requires a relatively flat surface to lay the collagen fibers in parallel or concentric layers.
These terms are histologic, in that a microscope is necessary to differentiate between the two.
Types.
There are five types of bones in the human body: long, short, flat, irregular, and sesamoid.
Formation.
The formation of bone during the fetal stage of development occurs by two processes: Intramembranous ossification and endochondral ossification.
Intramembranous ossification.
Intramembranous ossification mainly occurs during formation of the flat bones of the skull but also the mandible, maxilla, and clavicles; the bone is formed from connective tissue such as mesenchyme tissue rather than from cartilage. The steps in intramembranous ossification are:
Endochondral ossification.
Endochondral ossification, on the other hand, occurs in long bones and most of the rest of the bones in the body; it involves an initial hyaline cartilage that continues to grow. The steps in endochondral ossification are:
Endochondral ossification begins with points in the cartilage called "primary ossification centers." They mostly appear during fetal development, though a few short bones begin their primary ossification after birth. They are responsible for the formation of the diaphyses of long bones, short bones and certain parts of irregular bones. Secondary ossification occurs after birth, and forms the epiphyses of long bones and the extremities of irregular and flat bones. The diaphysis and both epiphyses of a long bone are separated by a growing zone of cartilage (the epiphyseal plate). When the child reaches skeletal maturity (18 to 25 years of age), all of the cartilage is replaced by bone, fusing the diaphysis and both epiphyses together (epiphyseal closure).
In the upper limbs, only the diaphyses of the long bones and scapula are ossified. The epiphyses, carpal bones, coracoid process, medial border of the scapula, and acromion are still cartilaginous. The following steps are followed in the conversion of cartilage to bone:
1. Zone of reserve cartilage. This region, farthest from the marrow cavity, consists of typical hyaline cartilage that as yet shows no sign of transforming into bone.
2. Zone of cell proliferation. A little closer to the marrow cavity, chondrocytes multiply and arrange themselves into longitudinal columns of flattened lacunae.
3. Zone of cell hypertrophy. Next, the chondrocytes cease to divide and begin to hypertrophy (enlarge), much like they do in the primary ossification center of the fetus. The walls of the matrix between lacunae become very thin.
4. Zone of calcification. Minerals are deposited in the matrix between the columns of lacunae and calcify the cartilage. These are not the permanent mineral deposits of bone, but only a temporary support for the cartilage that would otherwise soon be weakened by the breakdown of the enlarged lacunae.
5. Zone of bone deposition. Within each column, the walls between the lacunae break down and the chondrocytes die. This converts each column into a longitudinal channel, which is immediately invaded by blood vessels and marrow from the marrow cavity. Osteoblasts line up along the walls of these channels and begin depositing concentric lamellae of matrix, while osteoclasts dissolve the temporarily calcified cartilage.
Bone marrow.
Bone marrow can be found in almost any bone that holds cancellous tissue. In newborns, all such bones are filled exclusively with red marrow, but as the child ages it is mostly replaced by yellow, or "fatty" marrow. In adults, red marrow is mostly found in the marrow bones of the femur, the ribs, the vertebrae and pelvic bones.
Remodeling.
"Remodeling" or "bone turnover" is the process of resorption followed by replacement of bone with little change in shape and occurs throughout a person's life. Osteoblasts and osteoclasts, coupled together via paracrine cell signalling, are referred to as bone remodeling unit. Approximately 10% of the skeletal mass of an adult is remodelled each year.
Purpose.
The purpose of remodeling is to regulate calcium homeostasis, repair micro-damaged bones (from everyday stress) but also to shape and sculpt the skeleton during growth.
Calcium balance.
The process of bone resorption by the osteoclasts releases stored calcium into the systemic circulation and is an important process in regulating calcium balance. As bone formation actively "fixes" circulating calcium in its mineral form, removing it from the bloodstream, resorption actively "unfixes" it thereby increasing circulating calcium levels. These processes occur in tandem at site-specific locations.
Bone volume.
Bone volume is determined by the rates of bone formation and bone resorption. Recent research has suggested that certain growth factors may work to locally alter bone formation by increasing osteoblast activity. Numerous bone-derived growth factors have been isolated and classified via bone cultures. These factors include insulin-like growth factors I and II, transforming growth factor-beta, fibroblast growth factor, platelet-derived growth factor, and bone morphogenetic proteins. Evidence suggests that bone cells produce growth factors for extracellular storage in the bone matrix. The release of these growth factors from the bone matrix could cause the proliferation of osteoblast precursors. Essentially, bone growth factors may act as potential determinants of local bone formation. Research has suggested that trabecular bone volume in postemenopausal osteoporosis may be determined by the relationship between the total bone forming surface and the percent of surface resorption.
Repair.
Repeated stress, such as weight-bearing exercise or bone healing, results in the bone thickening at the points of maximum stress (Wolff's law). It has been hypothesized that this is a result of bone's piezoelectric properties, which cause bone to generate small electrical potentials under stress.
Paracrine cell signalling.
The action of osteoblasts and osteoclasts are controlled by a number of chemical factors that either promote or inhibit the activity of the bone remodeling cells, controlling the rate at which bone is made, destroyed, or changed in shape. The cells also use paracrine signalling to control the activity of each other.
Osteoblast stimulation.
Osteoblasts can be stimulated to increase bone mass through increased secretion of osteoid and by inhibiting the ability of osteoclasts to break down osseous tissue.
Bone building through increased secretion of osteoid is stimulated by the secretion of growth hormone by the pituitary, thyroid hormone and the sex hormones (estrogens and androgens). These hormones also promote increased secretion of osteoprotegerin. Osteoblasts can also be induced to secrete a number of cytokines that promote reabsorbtion of bone by stimulating osteoclast activity and differentiation from progenitor cells. Vitamin D, parathyroid hormone and stimulation from osteocytes induce osteoblasts to increase secretion of RANK-ligand and interleukin 6, which cytokines then stimulate increased reabsorption of bone by osteoclasts. These same compounds also increase secretion of macrophage colony-stimulating factor by osteoblasts, which promotes the differentiation of progenitor cells into osteoclasts, and decrease secretion of osteoprotegerin.
Osteoclast inhibition.
The rate at which osteoclasts resorb bone is inhibited by calcitonin and osteoprotegerin. Calcitonin is produced by parafollicular cells in the thyroid gland, and can bind to receptors on osteoclasts to directly inhibit osteoclast activity. Osteoprotegerin is secreted by osteoblasts and is able to bind RANK-L, inhibiting osteoclast stimulation.
Disorders.
There are many disorders of the skeleton. One of the more prominent is osteoporosis.
Osteoporosis.
Osteoporosis is a disease of bone, leading to an increased risk of fracture. In osteoporosis, the bone mineral density (BMD) is reduced, bone microarchitecture is disrupted, and the amount and variety of non-collagenous proteins in bone is altered. Osteoporosis is defined by the World Health Organization (WHO) in women as a bone mineral density 2.5 standard deviations below peak bone mass (same age and sex-matched healthy person average) as measured by DEXA; the term "established osteoporosis" includes the presence of a fragility fracture. Osteoporosis is most common in women after the menopause, when it is called postmenopausal osteoporosis, but may develop in men and premenopausal women in the presence of particular hormonal disorders and other chronic diseases or as a result of smoking and medications, specifically glucocorticoids, when the disease is called steroid- or glucocorticoid-induced osteoporosis (SIOP or GIOP).. Osteoporosis usually has no symptoms until a fracture occurs, this is the reason why regular DEXA scan are important in early detection and prevention.
Osteoporosis can be prevented with lifestyle advice and medication, and preventing falls in people with known or suspected osteoporosis is an established way to prevent fractures. Osteoporosis can be treated with bisphosphonates and various other medical treatments.
Other.
Other disorders of bone include:
Osteology.
The study of bones and teeth is referred to as osteology. It is frequently used in anthropology, archeology and forensic science for a variety of tasks. This can include determining the nutritional, health, age or injury status of the individual the bones were taken from. Preparing fleshed bones for these types of studies can involve maceration – boiling fleshed bones to remove large particles, then hand-cleaning.
Typically anthropologists and archeologists study bone tools made by "Homo sapiens" and "Homo neanderthalensis". Bones can serve a number of uses such as projectile points or artistic pigments, and can be made from endoskeletal or external bones such as antler or tusk.
Alternatives to bony endoskeletons.
There are several evolutionary alternatives to mammillary bone; though they have some similar functions, they are not completely functionally analogous to bone.
Exposed bone.
Bone penetrating the skin and being exposed to the outside can be both a natural process in some animals, and due to injury:
Uses.
Bones from slaughtered animals have a number of uses. They have been used as crafting materials for buttons, handles, ornaments etc. A special genre is scrimshaw. Ground bones are used as an organic phosphorus-nitrogen fertilizer and as additive in animal feed. Bones, in particular after calcination to bone ash is used as source of calcium phosphate for the production of bone china and previously also phosphorus chemicals.
Terminology.
Several terms are used to refer to features and components of bones throughout the body:
Several terms are used to refer to specific features of long bones:

</doc>
<doc id="4100" url="http://en.wikipedia.org/wiki?curid=4100" title="Bretwalda">
Bretwalda

Bretwalda (also brytenwalda and bretenanwealda) is an Old English word, the first record of which comes from the late 9th century "Anglo-Saxon Chronicle". It is given to some of the rulers of Anglo-Saxon kingdoms from the 5th century onwards who had achieved overlordship of some or all of the other Anglo-Saxon kingdoms. It is unclear whether the word dates back to the 5th century and was used by the kings themselves, or whether it is a later, 9th-century, invention. The term "bretwalda" also appears in a charter of Æthelstan.
The rulers of Mercia were generally the most powerful of the Anglo-Saxon kings from the mid-7th to the early 9th centuries, but are not accorded the title of bretwalda by the "Chronicle", which is generally thought to be because of the anti-Mercian bias of the Chroniclers. The "Annals of Wales" continued to recognise the kings of Northumbria as 'Kings of the Saxons' until the death of Osred I of Northumbria in 716.
Etymology.
The first syllable of the term "bretwalda" may be related to 'Briton' or 'Britain' and would thus mean 'sovereign of Britain' or 'wielder of Britain'. The word may be a compound containing the Old English adjective "brytten" (from the verb "breotan" meaning 'to break' or 'to disperse'), an element also found in the terms "bryten rice" ('kingdom'), "bryten-grund" ('the wide expanse of the earth') and "bryten cyning" ('king whose authority was widely extended'). Though the origin is ambiguous, the draughtsman of the charter issued by Æthelstan used the term in a way that can only mean 'wide ruler'.
The latter etymology was first suggested by John Mitchell Kemble who alluded that "of six manuscripts in which this passage occurs, one only reads "Bretwalda": of the remaining five, four have "Bryten-walda" or "-wealda", and one "Breten-anweald", which is precisely synonymous with Brytenwealda"; that Æthelstan was called "brytenwealda ealles ðyses ealondes", which Kemble translates as "ruler of all these islands"; and that "bryten-" is a common prefix to words meaning 'wide or general dispersion' and that the similarity to the word "bretwealh" ('Briton') is "merely accidental".
Contemporary use.
The first recorded use of the term "Bretwalda" comes from a West Saxon chronicle of the late 9th century that applied the term to Ecgberht, who ruled from 802 to 839. The chronicler also wrote down the names of seven kings that Bede listed in his "Historia ecclesiastica gentis Anglorum" in 731. All subsequent manuscripts of the "Chronicle" use the term "Brytenwalda", which may have represented the original term or derived from a common error.
There is no evidence that the term was a title that had any practical use, with implications of formal rights, powers and office, or even that it had any existence before the 9th-century. Bede wrote in Latin and never used the term and his list of kings holding "imperium" should be treated with caution, not least in that he overlooks kings such as Penda of Mercia, who clearly held some kind of dominance during his reign. Similarly, in his list of bretwaldas, the West Saxon chronicler ignored such Mercian kings as Offa.
The use of the term "Bretwalda" was the attempt by a West Saxon chronicler to make some claim of West Saxon kings to the whole of Great Britain. The concept of the overlordship of the whole of Britain was at least recognised in the period, whatever was meant by the term. Quite possibly it was a survival of a Roman concept of "Britain": it is significant that, while the hyperbolic inscriptions on coins and titles in charters often included the title "rex Britanniae", when England was unified the title used was "rex Angulsaxonum", ('king of the Anglo-Saxons'.)
Modern interpretation by historians.
For some time the existence of the word "bretwalda" in the "Anglo-Saxon Chronicle", which was based in part on the list given by Bede in his "Historia Ecclesiastica", led historians to think that there was perhaps a 'title' held by Anglo-Saxon overlords. This was particularly attractive as it would lay the foundations for the establishment of an English monarchy. The 20th-century historian Frank Stenton said of the Anglo-Saxon chronicler that "his inaccuracy is more than compensated by his preservation of the English title applied to these outstanding kings". He argued that the term "bretwalda" "falls into line with the other evidence which points to the Germanic origin of the earliest English institutions".
Over the later 20th century this assumption was increasingly challenged. Patrick Wormald interpreted it as "less an objectively realized office than a subjectively perceived status" and emphasised the partiality of its usage in favour of Southumbrian rulers. In 1991, Steven Fanning argued that "it is unlikely that the term ever existed as a title or was in common usage in Anglo-Saxon England". The fact that Bede never mentioned a special title for the kings in his list implies that he was unaware of one. In 1995, Simon Keynes observed that "if Bede's concept of the Southumbrian overlord, and the chronicler's concept of the 'Bretwalda', are to be regarded as artificial constructs, which have no validity outside the context of the literary works in which they appear, we are released from the assumptions about political development which they seem to involve... we might ask whether kings in the eighth and ninth centuries were quite so obsessed with the establishment of a pan-Southumbrian state".
Modern interpretations view the concept of bretwaldaship as complex and an important indicator of how a 9th-century chronicler interpreted history and attempted to insert the increasingly powerful Saxon kings into that history.
Overlordship.
A complex array of dominance and subservience existed during the Anglo-Saxon period. A king who used charters to grant land in another kingdom indicated such a relationship. If a king held sway over a large kingdom, such as when the Mercians dominated the East Anglians, the relationship would have been more equal than in the case of the Mercian dominance of the Hwicce, which was a comparatively small kingdom. Mercia was arguably the most powerful Anglo-Saxon kingdom for much of the late 7th and 8th centuries, though Mercian kings are missing from the two main 'lists'. For Bede, Mercia was a traditional enemy of his native Northumbria and he regarded powerful kings such as the pagan Penda as standing in the way of the Christian conversion of the Anglo-Saxons. Bede omits them from his list, even though it is evident that Penda held a considerable degree of power. Similarly powerful Mercia kings such as Offa are missed out of the West Saxon "Anglo-Saxon Chronicle", which sought to demonstrate the legitimacy of their kings to rule over other Anglo-Saxon peoples.

</doc>
<doc id="4101" url="http://en.wikipedia.org/wiki?curid=4101" title="Brouwer fixed-point theorem">
Brouwer fixed-point theorem

Brouwer's fixed-point theorem is a fixed-point theorem in topology, named after Luitzen Brouwer. It states that for any continuous function "f" mapping a compact convex set into itself there is a point "x"0 such that "f"("x"0) = "x"0. The simplest forms of Brouwer's theorem are for continuous functions "f" from a closed interval "I" in the real numbers to itself or from a closed disk "D" to itself. A more general form than the latter is for continuous functions from a convex compact subset "K" of Euclidean space to itself.
Among hundreds of fixed-point theorems, Brouwer's is particularly well known, due in part to its use across numerous fields of mathematics.
In its original field, this result is one of the key theorems characterizing the topology of Euclidean spaces, along with the Jordan curve theorem, the hairy ball theorem and the Borsuk–Ulam theorem.
This gives it a place among the fundamental theorems of topology. The theorem is also used for proving deep results about differential equations and is covered in most introductory courses on differential geometry.
It appears in unlikely fields such as game theory. In economics, Brouwer's fixed-point theorem and its extension, the Kakutani fixed-point theorem, play a central role in the proof of existence of general equilibrium in market economies as developed in the 1950s by economics Nobel prize winners Gérard Debreu and Kenneth Arrow.
The theorem was first studied in view of work on differential equations by the French mathematicians around Poincaré and Picard.
Proving results such as the Poincaré–Bendixson theorem requires the use of topological methods.
This work at the end of the 19th century opened into several successive versions of the theorem. The general case was first proved in 1910 by Jacques Hadamard and by Luitzen Egbertus Jan Brouwer.
Statement.
The theorem has several formulations, depending on the context in which it is used and its degree of generalization.
The simplest is sometimes given as follows:
This can be generalized to an arbitrary finite dimension:
A slightly more general version is as follows:
An even more general form is better known under a different name:
Notes.
The continuous function in this theorem is not required to be bijective or even surjective. Since any closed ball in Euclidean "n"-space is homeomorphic to the closed unit ball "D" "n", the theorem also has equivalent formulations that only state it for "D" "n".
Because the properties involved (continuity, being a fixed point) are invariant under homeomorphisms, the theorem is equivalent to forms in which the domain is required to be a closed unit ball "D" "n". For the same reason it holds for every set that is homeomorphic to a closed ball (and therefore also closed, bounded, connected, without holes, etc.).
The statement of the theorem is false if formulated for the "open" unit disk, the set of points with distance strictly less than 1 from the origin. Consider for example the function
which is a continuous function from the open interval (-1,1) to itself. As it shifts every point to the right, it cannot have a fixed point. (But it does have a fixed point for the closed interval [-1,1], namely f(x) = x = 1).
Illustrations.
The theorem has several "real world" illustrations. Here are some examples.
1. Take two sheets of graph paper of equal size with coordinate systems on them, lay one flat on the table and crumple up (without ripping or tearing) the other one and place it, in any fashion, on top of the first so that the crumpled paper does not reach outside the flat one. There will then be at least one point of the crumpled sheet that lies directly above its corresponding point (i.e. the point with the same coordinates) of the flat sheet. This is a consequence of the "n" = 2 case of Brouwer's theorem applied to the continuous map that assigns to the coordinates of every point of the crumpled sheet the coordinates of the point of the flat sheet immediately beneath it.
2. Take an ordinary map of a country, and suppose that that map is laid out on a table inside that country. There will always be a "You are Here" point on the map which represents that same point in the country.
3. In three dimensions the consequence of the Brouwer fixed-point theorem is that, no matter how much you stir a cocktail in a glass, when the liquid has come to rest some point in the liquid will end up in exactly the same place in the glass as before you took any action, assuming that the final position of each point is a continuous function of its original position, and that the liquid after stirring is contained within the space originally taken up by it.
Intuitive approach.
Explanations attributed to Brouwer.
The theorem is supposed to have originated from Brouwer's observation of a cup of coffee.
If one stirs to dissolve a lump of sugar, it appears there is always a point without motion.
He drew the conclusion that at any moment, there is a point on the surface that is not moving.
The fixed point is not necessarily the point that seems to be motionless, since the centre of the turbulence moves a little bit.
The result is not intuitive, since the original fixed point may become mobile when another fixed point appears.
Brouwer is said to have added: "I can formulate this splendid result different, I take a horizontal sheet, and another identical one which I crumple, flatten and place on the other. Then a point of the crumpled sheet is in the same place as on the other sheet."
Brouwer "flattens" his sheet as with a flat iron, without removing the folds and wrinkles.
One-dimensional case.
In one dimension, the result is intuitive and easy to prove. The continuous function "f" is defined on a closed interval ["a", "b"] and takes values in the same interval. Saying that this function has a fixed point amounts to saying that its graph (dark green in the figure on the right) intersects that of the function defined on the same interval ["a", "b"] which maps "x" to "x" (light green).
Intuitively, any continuous line from the left edge of the square to the right edge must necessarily intersect the green diagonal. Proof: consider the function "g" which maps "x" to "f"("x") - "x". It is ≥ 0 on "a" and ≤ 0 on "b". By the intermediate value theorem, "g" has a zero in ["a", "b"]; this zero is a fixed point.
Brouwer is said to have expressed this as follows: "Instead of examining a surface, we will prove the theorem about a piece of string. Let us begin with the string in an unfolded state, then refold it. Let us flatten the refolded string. Again a point of the string has not changed its position with respect to its original position on the unfolded string."
History.
The Brouwer fixed point theorem was one of the early achievements of algebraic topology, and is the basis of more general fixed point theorems which are important in functional analysis. The case "n" = 3 first was proved by Piers Bohl in 1904 (published in "Journal für die reine und angewandte Mathematik"). It was later proved by L. E. J. Brouwer in 1909. Jacques Hadamard proved the general case in 1910, and Brouwer found a different proof in the same year. Since these early proofs were all non-constructive indirect proofs, they ran contrary to Brouwer's intuitionist ideals. Methods to construct (approximations to) fixed points guaranteed by Brouwer's theorem are now known, however; see for example (Karamadian 1977) and (Istrăţescu 1981).
Prehistory.
To understand the prehistory of Brouwer's fixed point theorem one needs to pass through differential equations. At the end of the 19th century, the old problem of the stability of the solar system returned into the focus of the mathematical community.
Its solution required new methods. As noted by Henri Poincaré, who worked on the three-body problem, there is no hope to find an exact solution: "Nothing is more proper to give us an idea of the hardness of the three-body problem, and generally of all problems of Dynamics where there is no uniform integral and the Bohlin series diverge."
He also noted that the search for an approximate solution is no more efficient:
"the more we seek to obtain precise approximations, the more the result will diverge towards an increasing imprecision.".
He studied a question analogous to that of the surface movement in a cup of coffee. What can we say, in general, about the trajectories on a surface animated by a constant flow? Poincaré discovered that the answer can be found in what we now call the topological properties in the area containing the trajectory. If this area is compact, i.e. both closed and bounded, then the trajectory either becomes stationary, or it approaches a limit cycle. Poincaré went further; if the area is of the same kind as a disk, as is the case for the cup of coffee, there must necessarily be a fixed point. This fixed point is invariant under all functions which associate to each point of the original surface its position after a short time interval "t". If the area is a circular band, or if it is not closed, then this is not necessarily the case.
To understand differential equations better, a new branch of mathematics was born. Poincaré called it "analysis situs". The French Encyclopædia Universalis defines it as the branch which "treats the properties of an object that are invariant if it is deformed in any continuous way, without tearing". In 1886, Poincaré proved a result that is equivalent to Brouwer's fixed-point theorem, although the connection with the subject of this article was not yet apparent. A little later, he developed one of the fundamental tools for better understanding the analysis situs, now known as the fundamental group or sometimes the Poincaré group. This method can be used for a very compact proof of the theorem under discussion.
Poincaré's method was analogous to that of Émile Picard, a contemporary mathematician who generalized the Cauchy–Lipschitz theorem. Picard's approach is based on a result that would later be formalised by another fixed-point theorem, named after Banach. Instead of the topological properties of the domain, this theorem uses the fact that the function in question is a contraction.
First proofs.
At the dawn of the 20th century, the interest in analysis situs did not stay unnoticed. However, the necessity of a theorem equivalent to the one discussed in this article was not yet evident. Piers Bohl, a Latvian mathematician, applied topological methods to the study of differential equations. In 1904 he proved the three-dimensional case of our theorem, but his publication was not noticed.
It was Brouwer, finally, who gave the theorem its first patent of nobility. His goals were different from those of Poincaré. This mathematician was inspired by the foundations of mathematics, especially mathematical logic and topology. His initial interest lay in an attempt to solve Hilbert's fifth problem. In 1909, during a voyage to Paris, he met Poincaré, Hadamard and Borel. The ensuing discussions convinced Brouwer of the importance of a better understanding of Euclidean spaces, and were the origin of a fruitful exchange of letters with Hadamard. For the next four years, he concentrated on the proof of certain great theorems on this question. In 1912 he proved the hairy ball theorem for the two-dimensional sphere, as well as the fact that every continuous map from the two-dimensional ball to itself has a fixed point. These two results in themselves were not really new. As Hadamard observed, Poincaré had shown a theorem equivalent to the hairy ball theorem. The revolutionary aspect of Brouwer's approach was his systematic use of recently developed tools such as homotopy, the underlying concept of the Poincaré group. In the following year, Hadamard generalised the theorem under discussion to an arbitrary finite dimension, but he employed different methods. H. Freudenthal comments on the respective roles as follows: "Compared to Brouwer's revolutionary methods, those of Hadamard were very traditional, but Hadamard's participation in the birth of Brouwer's ideas resembles that of a midwife more than that of a mere spectator."
Brouwer's approach yielded its fruits, and in 1910 he also found a proof that was valid for any finite dimension, as well as other key theorems such as the invariance of dimension. In the context of this work, Brouwer also generalized the Jordan curve theorem to arbitrary dimension and established the properties connected with the degree of a continuous mapping. This branch of mathematics, originally envisioned by Poincaré and developed by Brouwer, changed its name. In the 1930s, analysis situs became algebraic topology.
Brouwer's celebrity is not exclusively due to his topological work. He was also the originator and zealous defender of a way of formalising mathematics that is known as intuitionism, which at the time made a stand against set theory. While Brouwer preferred constructive proofs, ironically, the original proofs of his great topological theorems were not constructive, and it took until 1967 for constructive proofs to be found.
Reception.
The theorem proved its worth in more than one way. During the 20th century numerous fixed-point theorems were developed, and even a branch of mathematics called fixed-point theory.
Brouwer's theorem is probably the most important. It is also among the foundational theorems on the topology of topological manifolds and is often used to prove other important results such as the Jordan curve theorem.
Besides the fixed-point theorems for more or less contracting functions, there are many that have emerged directly or indirectly from the result under discussion. A continuous map from a closed ball of Euclidean space to its boundary cannot be the identity on the boundary. Similarly, the Borsuk–Ulam theorem says that a continuous map from the "n"-dimensional sphere to Rn has a pair of antipodal points that are mapped to the same point. In the finite-dimensional case, the Lefschetz fixed-point theorem provided from 1926 a method for counting fixed points. In 1930, Brouwer's fixed-point theorem was generalized to Banach spaces. This generalization is known as Schauder's fixed-point theorem, a result generalized further by S. Kakutani to multivalued functions. One also meets the theorem and its variants outside topology. It can be used to prove the Hartman-Grobman theorem, which describes the qualitative behaviour of certain differential equations near certain equilibria. Similarly, Brouwer's theorem is used for the proof of the Central Limit Theorem. The theorem can also be found in existence proofs for the solutions of certain partial differential equations.
Other areas are also touched. In game theory, John Nash used the theorem to prove that in the game of Hex there is a winning strategy for white. In economy, P. Bich explains that certain generalizations of the theorem show that its use is helpful for certain classical problems in game theory and generally for equilibria (Hotelling's law), financial equilibria and incomplete markets.
Proof outlines.
A proof using homology.
The proof uses the observation that the boundary of "D" "n" is "S" "n" − 1, the ("n" − 1)-sphere.
The argument proceeds by contradiction, supposing that a continuous function "f" : "D" "n" → "D" "n" has "no" fixed point, and then attempting to derive an inconsistency, which proves that the function must in fact have a fixed point. For each "x" in "D" "n", there is only one straight line that passes through "f"("x") and "x", because it must be the case that "f"("x") and "x" are distinct by hypothesis (recall that "f" having no fixed points means that "f"("x") ≠ "x"). Following this line from "f"("x") through "x" leads to a point on "S" "n" − 1, denoted by "F"("x"). This defines a continuous function "F" : "D" "n" → "S" "n" − 1, which is a special type of continuous function known as a retraction: every point of the codomain (in this case "S" "n" − 1) is a fixed point of the function.
Intuitively it seems unlikely that there could be a retraction of "D" "n" onto "S" "n" − 1, and in the case "n" = 1 it is obviously impossible because "S" 0 (i.e., the endpoints of the closed interval "D" 1) is not even connected. The case "n" = 2 is less obvious, but can be proven by using basic arguments involving the fundamental groups of the respective spaces: the retraction would induce an injective group homomorphism from the fundamental group of "S" 1 to that of "D" 2, but the first group is isomorphic to Z while the latter group is trivial, so this is impossible. The case "n" = 2 can also be proven by contradiction based on a theorem about non-vanishing vector fields.
For "n" > 2, however, proving the impossibility of the retraction is more difficult. One way is to make use of homology groups: the homology "H""n" − 1("D" "n") is trivial, while "H""n" − 1("S" "n" − 1) is infinite cyclic. This shows that the retraction is impossible, because again the retraction would induce an injective group homomorphism from the latter to the former group.
A proof using Stokes's theorem.
To prove that a map has fixed points, one can assume that it is smooth, because if a map has no fixed points then convolving it with a smooth function of sufficiently small support produces a smooth function with no fixed points. As in the proof using homology, one is reduced to proving that there is no smooth retraction "f" from the ball "B" onto its boundary "∂B". If ω is a volume form on the boundary then by Stokes Theorem,
giving a contradiction.
More generally, this shows that there is no smooth retraction from any non-empty smooth orientable compact manifold onto its boundary. The proof using Stokes's theorem is closely related to the proof using homology (or rather cohomology), because the form ω generates the de Rham cohomology group "H""n"−1("∂B") used in the cohomology proof.
A combinatorial proof.
There is also a more elementary combinatorial proof, whose main step consists in establishing Sperner's lemma in "n" dimensions.
A proof by Hirsch.
There is also a quick proof, by Morris Hirsch, based on the impossibility of a differentiable retraction. The indirect proof starts by noting that the map "f" can be approximated by a smooth map retaining the property of not fixing a point; this can be done by using the Weierstrass approximation theorem, for example. One then defines a retraction as above which must now be differentiable. Such a retraction must have a non-singular value, by Sard's theorem, which is also non-singular for the restriction to the boundary (which is just the identity). Thus the inverse image would be a 1-manifold with boundary. The boundary would have to contain at least two end points, both of which would have to lie on the boundary of the original ball—which is impossible in a retraction.
Kellogg, Li, and Yorke turned Hirsch's proof into a constructive proof by observing that the retract is in fact defined everywhere except at the fixed points. For almost any point, q, on the boundary, (assuming it is not a fixed point) the one manifold with boundary mentioned above does exist and the only possibility is that it leads from q to a fixed point. It is an easy numerical task to follow such a path from q to the fixed point so the method is essentially constructive. Chow, Mallet-Paret, and Yorke gave a conceptually similar path-following version of the homotopy proof which extends to a wide variety of related problems.
A proof using the "oriented area".
A variation of the preceding proof does not employ the Sard's theorem, and goes as follows. If "r" : "B"→∂"B"   is a smooth retraction, one considers the smooth deformation "gt(x) := t r(x) + (1-t)x," and the smooth function
Differentiating under the sign of integral it is not difficult to check that "φ′(t)=0" for all "t", so "φ" is a constant function, which is a contradiction because "φ(0)" is the "n"-dimensional volume of the ball, while "φ(1)" is zero. The geometric idea is that "φ(t)" is the oriented area of "gt(B)" (that is, the Lebesgue measure of the image of the ball via "gt", taking into account multiplicity and orientation), and should remain constant (as it is very clear in the one-dimensional case). On the other hand, as the parameter "t" passes form "0" to "1" the map "gt" transforms continuously from the identity map of the ball, to the retraction "r", which is a contradiction since the oriented area of the identity coincides with the volume of the ball, while the oriented area of "r" is necessarily "0", as its image is the boundary of the ball, a set of null measure.
A proof using the game hex.
A quite different proof given by David Gale is based on the game of Hex. The basic theorem about Hex is that no game can end in a draw. This is equivalent to the Brouwer fixed-point theorem for dimension 2. By considering "n"-dimensional versions of Hex, one can prove in general that Brouwer's theorem is equivalent to the determinacy theorem for Hex.
A proof using the Lefschetz fixed-point theorem.
The Lefschetz fixed-point theorem says that if a continuous map "f" from a finite simplicial complex "B" to itself has only isolated fixed points, then the number of fixed points counted with multiplicities (which may be negative) is equal to the Lefschetz number
and in particular if the Lefschetz number is nonzero then "f" must have a fixed point. If "B" is a ball (or more generally is contractible) then the Lefschetz number is one because the only non-zero homology group is :formula_5, so "f" has a fixed point.
A proof in a weak logical system.
In reverse mathematics, Brouwer's theorem can be proved in the system WKL0, and conversely over the base system RCA0 Brouwer's theorem for a square implies the weak König's lemma, so this gives a precise description of the strength of Brouwer's theorem.
Generalizations.
The Brouwer fixed-point theorem forms the starting point of a number of more general fixed-point theorems.
The straightforward generalization to infinite dimensions, i.e. using the unit ball of an arbitrary Hilbert space instead of Euclidean space, is not true. The main problem here is that the unit balls of infinite-dimensional Hilbert spaces are not compact. For example, in the Hilbert space ℓ2 of square-summable real (or complex) sequences, consider the map "f" : ℓ2 → ℓ2 which sends a sequence ("x""n") from the closed unit ball of ℓ2 to the sequence ("y""n") defined by
It is not difficult to check that this map is continuous, has its image in the unit sphere of ℓ 2, but does not have a fixed point.
The generalizations of the Brouwer fixed-point theorem to infinite dimensional spaces therefore all include a compactness assumption of some sort, and in addition also often an assumption of convexity. See fixed-point theorems in infinite-dimensional spaces for a discussion of these theorems.
There is also finite-dimensional generalization to a larger class of spaces: If formula_7 is a product of finitely many chainable continua, then every continuous function formula_8 has a fixed point, where a chainable continuum is a (usually but in this case not necessarily metric) compact Hausdorff space of which every open cover has a finite open refinement formula_9, such that formula_10 if and only if formula_11. Examples of chainable continua include compact connected linearly ordered spaces and in particular closed intervals of real numbers.
The Kakutani fixed point theorem generalizes the Brouwer fixed-point theorem in a different direction: it stays in R"n", but considers upper hemi-continuous correspondences (functions that assign to each point of the set a subset of the set). It also requires compactness and convexity of the set.
The Lefschetz fixed-point theorem applies to (almost) arbitrary compact topological spaces, and gives a condition in terms of singular homology that guarantees the existence of fixed points; this condition is trivially satisfied for any map in the case of "D" "n".

</doc>
<doc id="4106" url="http://en.wikipedia.org/wiki?curid=4106" title="Benzoic acid">
Benzoic acid

Benzoic acid , C7H6O2 (or C6H5COOH), is a colorless crystalline solid and a simple aromatic carboxylic acid. The name is derived from gum benzoin, which was for a long time its only known source. Benzoic acid occurs naturally in many plants and it serves as an intermediate in the biosynthesis of many secondary metabolites. Salts of benzoic acid are used as food preservatives and benzoic acid is an important precursor for the industrial synthesis of many other organic substances. The salts and esters of benzoic acid are known as benzoates .
History.
Benzoic acid was discovered in the sixteenth century. The dry distillation of gum benzoin was first described by Nostradamus (1556), and then by Alexius Pedemontanus (1560) and Blaise de Vigenère (1596).
Pioneer work in 1830 through a variety of experiences based on amygdalin, obtained from bitter almonds (the fruit of Prunus dulcis) oil by Pierre Robiquet and Antoine Boutron-Charlard, two French chemists, had produced benzaldehyde but they failed in working out a proper interpretation of the structure of amygdalin that would account for it, and thus missed the identification of the benzoyl radical C7H5O.
This last step was achieved some few months later (1832) by Justus von Liebig and Friedrich Wöhler, who determined the composition of benzoic acid. These latter also investigated how hippuric acid is related to benzoic acid.
In 1875 Salkowski discovered the antifungal abilities of benzoic acid, which was used for a long time in the preservation of benzoate-containing cloudberry fruits.
It is also one of the chemical compounds found in castoreum. This compound is gathered from the beaver plant food.
Production.
Industrial preparations.
Benzoic acid is produced commercially by partial oxidation of toluene with oxygen. The process is catalyzed by cobalt or manganese naphthenates. The process uses cheap raw materials, and proceeds in high yield.
U.S. production capacity is estimated to be 126,000 tonnes per year (139,000 tons), much of which is consumed domestically to prepare other industrial chemicals.
Laboratory synthesis.
Benzoic acid is cheap and readily available, so the laboratory synthesis of benzoic acid is mainly practiced for its pedagogical value. It is a common undergraduate preparation.
Benzoic acid can be purified by recrystallization from water because of its high solubility in hot water and poor solubility in cold water. The avoidance of organic solvents for the recrystallization makes this experiment particularly safe. The solubility of benzoic acid in over 40 solvents with references to original sources can be found as part of the Open Notebook Science Challenge
By hydrolysis.
Like other nitriles and amides, benzonitrile and benzamide can be hydrolyzed to benzoic acid or its conjugate base in acid or basic conditions.
From benzaldehyde.
The base-induced disproportionation of benzaldehyde, the Cannizzaro reaction, affords equal amounts of benzoate and benzyl alcohol; the latter can be removed by distillation.
From bromobenzene.
Bromobenzene can be converted to benzoic acid by "carbonation" of the intermediate phenylmagnesium bromide. This synthesis offers a convenient exercise for students to carry out a Grignard reaction, an important class of carbon–carbon bond forming reaction in organic chemistry.
From benzyl alcohol.
Benzyl alcohol is refluxed with potassium permanganate or other oxidizing reagents in water. The mixture is hot filtered to remove manganese dioxide and then allowed to cool to afford benzoic acid.
From benzyl chloride.
Benzoic acid can be prepared by oxidation of benzyl chloride in the presence of alkaline KMnO4:
Historical preparation.
The first industrial process involved the reaction of benzotrichloride (trichloromethyl benzene) with calcium hydroxide in water, using iron or iron salts as catalyst. The resulting calcium benzoate is converted to benzoic acid with hydrochloric acid. The product contains significant amounts of chlorinated benzoic acid derivatives. For this reason, benzoic acid for human consumption was obtained by dry distillation of gum benzoin. Food-grade benzoic acid is now produced synthetically.
Uses.
Benzoic acid is mainly consumed in the production of phenol by oxidative decarboxylation at 300−400 °C:
The temperature required can be lowered to 200 °C by the addition of catalytic amounts of copper(II) salts. The phenol can be converted to cyclohexanol, which is a starting material for nylon synthesis.
Precursor to plasticizers.
Benzoate plasticizers, such as the glycol-, diethylenegylcol-, and triethyleneglycol esters, are obtained by transesterification of methyl benzoate with the corresponding diol. Alternatively these species arise by treatment of benzoylchloride with the diol. These plasticizers are used similarly to those derived from terephthalic acid ester.
Precursor to sodium benzoate and related preservatives.
Benzoic acid and its salts are used as a food preservatives, represented by the E-numbers E210, E211, E212, and E213. Benzoic acid inhibits the growth of mold, yeast and some bacteria. It is either added directly or created from reactions with its sodium, potassium, or calcium salt. The mechanism starts with the absorption of benzoic acid into the cell. If the intracellular pH changes to 5 or lower, the anaerobic fermentation of glucose through phosphofructokinase is decreased by 95%. The efficacy of benzoic acid and benzoate is thus dependent on the pH of the food. Acidic food and beverage like fruit juice (citric acid), sparkling drinks (carbon dioxide), soft drinks (phosphoric acid), pickles (vinegar) or other acidified food are preserved with benzoic acid and benzoates.
Typical levels of use for benzoic acid as a preservative in food are between 0.05–0.1%. Foods in which benzoic acid may be used and maximum levels for its application are controlled by international food law.
Concern has been expressed that benzoic acid and its salts may react with ascorbic acid (vitamin C) in some soft drinks, forming small quantities of benzene.
Medicinal.
Benzoic acid is a constituent of Whitfield's ointment which is used for the treatment of fungal skin diseases such as tinea, ringworm, and athlete's foot. As the principal component of benzoin resin, benzoic acid is also a major ingredient in both tincture of benzoin and Friar's balsam. Such products have a long history of use as topical antiseptics and inhalant decongestants.
Benzoic acid was used as an expectorant, analgesic, and antiseptic in the early 20th century.
Benzoyl chloride.
Benzoic acid is a precursor to benzoyl chloride, C6H5C(O)Cl by treatment with thionyl chloride, phosgene or one of the chlorides of phosphorus. is an important starting material for several benzoic acid derivates like benzyl benzoate, which is used in artificial flavours and insect repellents.
Niche and laboratory uses.
In teaching laboratories, benzoic acid is a common standard for calibrating a bomb calorimeter.
Biology and health effects.
Benzoic acid is relatively nontoxic. It is excreted as hippuric acid. Benzoic acid is metabolized by butyrate-CoA ligase into an intermediate product, benzoyl-CoA, which is then metabolized by glycine N-acyltransferase into hippuric acid.
Benzoic acid occurs naturally as do its esters in many plant and animal species. Appreciable amounts have been found in most berries (around 0.05%). Ripe fruits of several "Vaccinium" species (e.g., cranberry, "V. vitis macrocarpon"; bilberry, "V. myrtillus") contain as much as 0.03–0.13% free benzoic acid. Benzoic acid is also formed in apples after infection with the fungus "Nectria galligena". Among animals, benzoic acid has been identified primarily in omnivorous or phytophageous species, e.g., in viscera and muscles of the Rock Ptarmigan ("Lagopus muta") as well as in gland secretions of male muskoxen ("Ovibos moschatus") or Asian bull elephants ("Elephas maximus").
Gum benzoin contains up to 20% of benzoic acid and 40% benzoic acid esters.
"Cryptanaerobacter phenolicus" is a bacterium species that produces benzoate from phenol via 4-hydroxybenzoate
Benzoic acid is present as part of hippuric acid ("N"-benzoylglycine) in urine of mammals, especially herbivores (Gr. "hippos" = horse; "ouron" = urine). Humans produce about 0.44 g/L hippuric acid per day in their urine, and if the person is exposed to toluene or benzoic acid, it can rise above that level.
For humans, the World Health Organization's International Programme on Chemical Safety (IPCS) suggests a provisional tolerable intake would be 5 mg/kg body weight per day. Cats have a significantly lower tolerance against benzoic acid and its salts than rats and mice. Lethal dose for cats can be as low as 300 mg/kg body weight. The oral for rats is 3040 mg/kg, for mice it is 1940–2263 mg/kg.
In Taipei, Taiwan, a city health survey in 2010 found that 30% of dried and pickled food products had too much benzoic acid, which may affect the liver and kidney, along with more serious issues like excessive cyclamate.
Reactions.
Reactions of benzoic acid can occur at either the aromatic ring or at the carboxyl group:
Aromatic ring.
Electrophilic aromatic substitution reaction will take place mainly in 3-position due to the electron-withdrawing carboxylic group; i.e. benzoic acid is "meta" directing.
The second substitution reaction (on the right) is slower because the first nitro group is deactivating. Conversely, if an activating group (electron-donating) was introduced (e.g., alkyl), a second substitution reaction would occur more readily than the first and the disubstituted product might accumulate to a significant extent.
Carboxyl group.
All the reactions mentioned for carboxylic acids are also possible for benzoic acid.

</doc>
<doc id="4107" url="http://en.wikipedia.org/wiki?curid=4107" title="Boltzmann distribution">
Boltzmann distribution

In statistical mechanics and mathematics, a Boltzmann distribution (also called Gibbs distribution) is a probability distribution, probability measure, or frequency distribution over various possible states of a system, with the form
where formula_2 is state energy (which varies from state to state), and formula_3 (a constant of the distribution) is the product of Boltzmann's constant and thermodynamic temperature.
The "ratio" of a Boltzmann distribution computed for two states is known as the Boltzmann factor and characteristically only depends on the states' energy difference.
The Boltzmann distribution is named after Ludwig Boltzmann who first formulated it in 1868 during his studies of the statistical mechanics of gases in thermal equilibrium. The distribution was later investigated extensively, in its modern generic form, by Josiah Willard Gibbs in 1902.
In statistical mechanics.
The Boltzmann distribution appears in statistical mechanics when considering isolated (or nearly-isolated) systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble, but also some special cases (derivable from the canonical ensemble) also show the Boltzmann distribution in different aspects:
Although these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed:
In mathematics.
In more general mathematical settings, the Boltzmann distribution is also known as the Gibbs measure. In statistics and machine learning it is called a log-linear model.

</doc>
<doc id="4109" url="http://en.wikipedia.org/wiki?curid=4109" title="Leg theory">
Leg theory

Leg theory is a bowling tactic in the sport of cricket. The term "leg theory" is somewhat archaic and seldom used any more, but the basic tactic still plays a part in modern cricket.
Simply put, leg theory involves concentrating the bowling attack at or near the line of leg stump. This may or may not be accompanied by a concentration of fielders on the leg side. The line of attack aims to cramp the batsman, making him play the ball with the bat close to the body. This makes it difficult to hit the ball freely and score runs, especially on the off side. Since a leg theory attack means the batsman is more likely to hit the ball on the leg side, additional fielders on that side of the field can be effective in preventing runs and taking catches.
Stifling the batsman in this manner can lead to impatience and frustration, resulting in rash play by the batsman which in turn can lead to a quick dismissal.
Leg theory can be a moderately successful tactic when used with both fast bowling and spin bowling, particularly leg spin to right-handed batsmen or off spin to left-handed batsmen. However, because it relies on lack of concentration or discipline by the batsman, it can be risky against patient and skilled players, especially batsmen who are strong on the leg side. The English opening bowlers Sydney Barnes and Frank Foster used leg theory with some success in Australia in 1911-12. In England, at around the same time Fred Root was one of the main proponents of the same tactic.
Concentrating attack on the leg stump is considered by many cricket fans and commentators to lead to boring play, as it stifles run scoring and encourages batsmen to play conservatively.
Fast leg theory.
In 1930, England captain Douglas Jardine, together with Nottinghamshire's captain Arthur Carr and his bowlers Harold Larwood and Bill Voce, developed a variant of leg theory in which the bowlers bowled fast, short-pitched balls that would rise into the batsman's body, together with a heavily stacked ring of close fielders on the leg side. The idea was that when the batsman defended against the ball, he would be likely to deflect the ball into the air for a catch.
Jardine called this modified form of the tactic "fast leg theory". On the 1932-33 English tour of Australia, Larwood and Voce bowled fast leg theory at the Australian batsmen. It turned out to be extremely dangerous, and most Australian players sustained injuries from being hit by the ball. Wicket-keeper Bert Oldfield's skull was fractured by a ball hitting his head (although the ball had first glanced off the bat and Larwood had an orthodox field), almost precipitating a riot by the Australian crowd.
The Australian press dubbed the tactic "Bodyline", and claimed it was a deliberate attempt by the English team to intimidate and injure the Australian players. Reports of the controversy reaching England at the time described the bowling as "fast leg theory", which sounded to many people to be a harmless and well-established tactic. This led to a serious misunderstanding amongst the English public and the Marylebone Cricket Club - the administrators of English cricket - of the dangers posed by Bodyline. The English press and cricket authorities declared the Australian protests to be a case of sore losing and "squealing".
It was only with the return of the English team and the subsequent use of Bodyline against English players in England by the touring West Indian cricket team in 1933 that demonstrated to the country the dangers it posed. The MCC subsequently revised the Laws of Cricket to prevent the use of "fast leg theory" tactics in future, also limiting the traditional tactic.

</doc>
<doc id="4110" url="http://en.wikipedia.org/wiki?curid=4110" title="Blythe Danner">
Blythe Danner

Blythe Katherine Danner (born February 3, 1943) is an American actress. She is known for her portrayal as Will Truman's mother, Marilyn, in the sitcom "Will & Grace" and for her role in the 2000 comedy hit "Meet the Parents", and its sequels, "Meet the Fockers" and "Little Fockers". She is the mother of actress Gwyneth Paltrow and director Jake Paltrow.
Early life.
Danner was born in Philadelphia, Pennsylvania, the daughter of Katharine (née Kile) and Harry Earl Danner, a bank executive. She has a brother, opera singer/actor Harry Danner, a sister, performer-turned-director Dorothy (Dottie) Danner, and a half-brother, violin maker William Moennig. Danner has Pennsylvania Dutch (German), and some English and Irish, ancestry; her maternal grandmother was a German immigrant, and one of her paternal great-grandmothers was born in Barbados (a White Barbadian).
Career.
A graduate of Bard College, Danner's first roles included the 1967 musical "Mata Hari" (closed out of town), and the 1968 off-Broadway production, "Summertree". Her early Broadway appearances included roles in "Cyrano de Bergerac" (1968) and "The Miser" (1969). She won a Best Supporting Actress Tony playing a free-spirited divorcee in "Butterflies Are Free" (1969).
In 1972, Danner portrayed Martha Jefferson opposite Ken Howard's Thomas Jefferson in the movie version of "1776". That same year, she played a cuckolded wife opposite Peter Falk and John Cassavetes in the "Columbo" episode "Etude in Black".
Her earliest starring film role was opposite Alan Alda in "To Kill a Clown" (1972). Danner appeared in the episode of "M*A*S*H" entitled "The More I See You", playing the love interest of Alda's character Hawkeye Pierce. She played lawyer Amanda Bonner in television's "Adam's Rib", also opposite Ken Howard as Adam Bonner. She played Zelda Fitzgerald in "The Last of the Belles" (1974). She was the eponymous heroine in the film "Lovin' Molly" (1974) (directed by Sidney Lumet). She appeared in "Futureworld", playing Tracy Ballard with co-star Peter Fonda (1976). In the 1982 TV movie "Inside the Third Reich", she played the wife of Albert Speer. In the film version of Neil Simon's semi-autobiographical play "Brighton Beach Memoirs" (1986), she portrayed a middle-aged Jewish mother. She has appeared in two films based on the novels of Pat Conroy, "The Great Santini" (1979) and "The Prince of Tides" (1991), as well as two television movies adapted from books by Anne Tyler, "Saint Maybe" and "Back When We Were Grownups", both for the Hallmark Hall of Fame.
Danner appeared opposite Robert De Niro in the 2000 comedy hit "Meet the Parents", and its sequels, "Meet the Fockers" and "Little Fockers".
From 2001 to 2006, she regularly appeared on "Will & Grace" as Will Truman's mother Marilyn. From 2004 to 2006, she starred in the TV series "Huff". In 2005, she was nominated for three Emmy Awards: for her work on "Will & Grace", "Huff" and "Back When We Were Grownups". Emmy host Ellen DeGeneres poked fun at Danner during the award ceremony, saying that Danner should not be nervous because she was almost certain to win at least one Emmy, which she did, for "Huff". In July 2006, she won a second consecutive Emmy award for "Huff". For 25 years, she has been a regular performer at the Williamstown Summer Theater Festival, where she also serves on the Board of Directors.
In 2006, Danner was awarded an inaugural Katharine Hepburn Medal by Bryn Mawr College's Katharine Houghton Hepburn Center.
Environmental activism.
Danner has been involved in environmental issues such as recycling and conservation for over 30 years. She has been active with INFORM, Inc., is on the Board of Environmental Advocates of New York and the Board of Directors of the Environmental Media Association, and won the 2002 EMA Board of Directors Ongoing Commitment Award. In 2011, Danner joined Moms Clean Air Force, to help call on parents to join in the fight against toxic air pollution.
Health care activism.
After the death of her husband Bruce Paltrow from oral cancer, she became involved with the Oral Cancer Foundation, a national 501(c)3 non profit charity. In 2005, she filmed a public service announcement that played on TV stations around the country about the risks associated with oral cancer, and through that shared the personal pain associated with the loss of her husband publicly to further awareness of the disease and the need for early detection. She continues to donate her time to the foundation, and has appeared on morning talk shows, and has done interviews in high profile magazines such as "People" to further public awareness of the disease and its risk factors. Through The Bruce Paltrow Oral Cancer Fund, administered by the Oral Cancer Foundation, she continues to raise awareness and funding for oral cancer issues, particularly those involving communities in which disparities in health care exist. She is now appearing in commercials for Prolia.
Personal life.
Danner is the widow of producer/director Bruce Paltrow, who died from complications of pneumonia while battling oral cancer in 2002, and the mother of actress Gwyneth Paltrow and director Jake Paltrow. Danner first co-starred with her daughter in 1992 in the TV movie "Cruel Doubt" and then again in the 2003 film "Sylvia" playing Aurelia Plath, mother to Gwyneth Paltrow's title role as Sylvia Plath.

</doc>
<doc id="4111" url="http://en.wikipedia.org/wiki?curid=4111" title="Bioleaching">
Bioleaching

Bioleaching is the extraction of metals from their ores through the use of living organisms. This is much cleaner than the traditional heap leaching using cinema. Bioleaching is one of several applications within biohydrometallurgy and several methods are used to recover copper, zinc, lead, arsenic, antimony, nickel, molybdenum, gold, silver, and cobalt.
Process.
Bioleaching can involve numerous ferrous iron and sulfur oxidizing bacteria, including "Acidithiobacillus ferrooxidans" and "Acidithiobacillus " (formerly known as "Thiobacillus"). As a general principle, Fe3+ ions are used to oxidize the ore. This step is entirely independent of microbes. The role of the bacteria is the further oxidation of the ore, but also the regeneration of the chemical oxidant Fe3+ from Fe2+. For example, bacteria catalyse the breakdown of the mineral pyrite (FeS2) by oxidising the sulfur and metal (in this case ferrous iron, (Fe2+)) using oxygen. This yields soluble products that can be further purified and refined to yield the desired metal.
Pyrite leaching (FeS2):
In the first step, disulfide is spontaneously oxidized to thiosulfate by ferric ion (Fe3+), which in turn is reduced to give ferrous ion (Fe2+):
The ferrous ion is then oxidized by bacteria using oxygen:
Thiosulfate is also oxidized by bacteria to give sulfate:
The ferric ion produced in reaction (2) oxidized more sulfide as in reaction (1), closing the cycle and given the net reaction:
The net products of the reaction are soluble ferrous sulfate and sulfuric acid.
The microbial oxidation process occurs at the cell membrane of the bacteria. The electrons pass into the cells and are used in biochemical processes to produce energy for the bacteria while reducing oxygen to water. The critical reaction is the oxidation of sulfide by ferric iron. The main role of the bacterial step is the regeneration of this reactant.
The process for copper is very similar, but the efficiency and kinetics depend on the copper mineralogy. The most efficient minerals are supergene minerals such as chalcocite, Cu2S and covellite, CuS. The main copper mineral chalcopyrite (CuFeS2) is not leached very efficiently, which is why the dominant copper-producing technology remains flotation, followed by smelting and refining. The leaching of CuFeS2 follows the two stages of being dissolved and then further oxidised, with Cu2+ ions being left in solution.
Chalcopyrite leaching:
net reaction:
In general, sulfides are first oxidized to elemental sulfur, whereas disulfides are oxidized to give thiosulfate, and the processes above can be applied to other sulfidic ores. Bioleaching of non-sulfidic ores such as pitchblende also uses ferric iron as an oxidant (e.g., UO2 + 2 Fe3+ ==> UO22+ + 2 Fe2+). In this case, the sole purpose of the bacterial step is the regeneration of Fe3+. Sulfidic iron ores can be added to speed up the process and provide a source of iron. Bioleaching of non-sulfidic ores by layering of waste sulfides and elemental sulfur, colonized by "Acidithiobacillus" spp., has been accomplished, which provides a strategy for accelerated leaching of materials that do not contain sulfide minerals.
Further processing.
The dissolved copper (Cu2+) ions are removed from the solution by ligand exchange solvent extraction, which leaves other ions in the solution. The copper is removed by bonding to a ligand, which is a large molecule consisting of a number of smaller groups, each possessing a lone electron pair. The ligand-copper complex is extracted from the solution using an organic solvent such as kerosene:
The ligand donates electrons to the copper, producing a complex - a central metal atom (copper) bonded to the ligand. Because this complex has no charge, it is no longer attracted to polar water molecules and dissolves in the kerosene, which is then easily separated from the solution. Because the initial reaction is reversible, it is determined by pH. Adding concentrated acid reverses the equation, and the copper ions go back into an aqueous solution.
Then the copper is passed through an electro-winning process to increase its purity: An electric current is passed through the resulting solution of copper ions. Because copper ions have a 2+ charge, they are attracted to the negative cathodes and collect there.
The copper can also be concentrated and separated by displacing the copper with Fe from scrap iron:
The electrons lost by the iron are taken up by the copper. Copper is the oxidising agent (it accepts electrons), and iron is the reducing agent (it loses electrons).
Traces of precious metals such as gold may be left in the original solution. Treating the mixture with sodium cyanide in the presence of free oxygen dissolves the gold. The gold is removed from the solution by adsorbing (taking it up on the surface) to charcoal.
Bioleaching with fungi.
Several species of fungi can be used for bioleaching. Fungi can be grown on many different substrates, such as electronic scrap, catalytic converters, and fly ash from municipal waste incineration. Experiments have shown that two fungal strains ("Aspergillus niger, Penicillium simplicissimum") were able to mobilize Cu and Sn by 65%, and Al, Ni, Pb, and Zn by more than 95%."Aspergillus niger" can produce some organic acids such as citric acid. This form of leaching does not rely on microbial oxidation of metal but rather uses microbial metabolism as source of acids that directly dissolve the metal.
Compared with other extraction techniques.
Extractions involve many expensive steps such as roasting and smelting, which require sufficient concentrations of elements in ores and are environmentally unfriendly. Low concentrations are not a problem for bacteria because they simply ignore the waste that surrounds the metals, attaining extraction yields of over 90% in some cases. These microorganisms actually gain energy by breaking down minerals into their constituent elements. The company simply collects the ions out of the solution after the bacteria have finished. There is a limited amount of ores.
Disadvantages of bioleaching.
At the current time, it is more economical to smelt copper ore rather than to use bioleaching, since the concentration of copper in its ore is in general quite high. The profit obtained from the speed and yield of smelting justifies its cost. Nonetheless, at the largest copper mine of the world, Escondida in Chile the process seems to be favorable. 
However, the concentration of gold in its ore is in general very low. In this case, the lower cost of bacterial leaching outweighs the time it takes to extract the metal.
Economically it is also very expensive and many companies once started can not keep up with the demand and end up in debt. Projects like Finnish Talvivaara proved to be environmentally and economically disastrous.

</doc>
<doc id="4113" url="http://en.wikipedia.org/wiki?curid=4113" title="Bouldering">
Bouldering

Bouldering is a form of rock climbing that is performed without the use of ropes or harnesses. While it can be done without any equipment whatsoever, most climbers use climbing shoes to help secure footholds, chalk to keep their hands dry, and bouldering mats to prevent injuries from falls. Unlike free solo climbing, which is also performed without ropes, bouldering problems (the path that a climber takes in order to complete the climb) are usually less than 20 feet tall. Artificial climbing walls allow boulderers to train indoors in areas without natural boulders. Bouldering competitions, which employ a variety of formats, take place in both indoor and outdoor settings.
The sport originated as a method of training for roped climbs and mountaineering. Bouldering enabled top rope climbers to practice specific moves at a safe distance from the ground. Additionally, the sport served to build stamina and increase finger strength. Throughout the 1900s, bouldering evolved into a separate discipline. Individual problems are assigned ratings based on their difficulty. There have been many different rating systems used throughout the history of the sport, but modern problems usually use either the V-scale or the Fontainebleau scale.
The growing popularity of the sport has caused several environmental concerns, including soil erosion and trampled vegetation as climbers hike off-trail to reach bouldering sites. This has caused some landowners to restrict access or prohibit bouldering altogether.
Overview.
Bouldering is a form of rock climbing which takes place on boulders and other small rock formations, usually measuring less than from ground to top. Unlike top rope climbing and lead climbing, no ropes are used to protect or aid the climber. Bouldering routes or "problems" require the climber to reach the top of a boulder, usually from a specified start position. Some boulder problems, known as "traverses," require the climber to climb horizontally from one position to another.
The characteristics of boulder problems depend largely on the type of rock being climbed. Granite, for example, often features long cracks and slabs. Sandstone rocks are known for their steep overhangs and frequent horizontal breaks. Other common bouldering rocks include limestone and volcanic rock.
There are many prominent bouldering areas throughout the United States, including Hueco Tanks in Texas and Mount Evans in Colorado. Squamish, British Columbia is one of the most popular bouldering areas in Canada. Europe also hosts a number of bouldering sites, such as Fontainebleau in France, Albarracín in Spain, and various mountains throughout Switzerland.
Indoor bouldering.
Artificial climbing walls are used to simulate boulder problems in an indoor environment, usually at climbing gyms. These walls are constructed with wooden panels, polymer cement panels, concrete shells, or precast molds of actual rock walls. Holds, usually made of plastic, are then bolted onto the wall to create problems. The walls often feature steep overhanging surfaces, forcing the climber to employ highly technical movements while supporting much of their weight with their upper body strength.
Climbing gyms often feature multiple problems within the same section of wall. In the US the most common method Routesetters use to designate the intended route for a particular problem is by placing colored tape next to each hold—for example, holds with red tape would indicate one bouldering problem, while green tape would be used to set off a different problem in the same area. Across much of the rest of the world problems and grades are usually designated by using a set color of plastic hold to indicate a particular problem. For example, green may be v0-v1, blue may be v2-v3 and so on. Setting via color is advantageous for a few reasons, most notably due to it being more obvious where the holds for a problem are. This contrasts with taped problems in that tape can often be accidentally kicked off - especially on footholds.
Competitions.
Bouldering competitions occur in both indoor and outdoor settings. The International Federation of Sport Climbing (IFSC) employs an indoor format that breaks the competition into three rounds: qualifications, semi-finals, and finals. The rounds feature different sets of four or five boulder problems, and each competitor has a fixed amount of time to attempt each problem. At the end of each round, competitors are ranked by the number of completed problems, with ties settled by the total number of attempts taken to solve the problems.
There are several other formats used for bouldering competitions. Some competitions give climbers a fixed number of attempts at each problem with a timed rest period in between each attempt, unlike the IFSC format, in which competitors can use their allotted time however they choose. In an open-format competition, all climbers compete simultaneously, and are given a fixed amount of time to complete as many problems as possible. More points are awarded for more difficult problems, while points are deducted for multiple attempts on the same problem.
In 2012, the IFSC submitted a proposal to the International Olympic Committee (IOC) to include lead climbing in the 2020 Summer Olympics. The proposal was later revised to an "overall" competition, which would feature bouldering, lead climbing, and speed climbing. In May 2013, the IOC announced that climbing would not be added to the 2020 Olympic program.
History.
Rock climbing first emerged as a sport in the mid-1800s. Early records describe climbers engaging in what is now referred to as bouldering, not as a separate discipline, but as a form of training for larger ascents. In the early 20th century, the Fontainebleau area of France established itself as a prominent climbing area, where some of the first dedicated "bleausards" (or "boulderers") emerged. The specialized rock climbing shoe was invented by one such athlete, Pierre Allain.
In the 1960s, the sport was pushed forward by American mathematician John Gill, who contributed several important innovations. Gill's previous athletic pursuit was gymnastics, a sport which had an established scale of difficulty for particular movements and body positions. He applied this idea to bouldering, which shifted the focus from reaching a summit to navigating a specific sequence of holds. Gill developed a closed-ended rating system: B1 problems were as difficult as the most challenging roped routes of the time, B2 problems were more difficult, and B3 problems were those that had only been completed once.
Gill introduced chalk as a method of keeping the climber's hands dry. He also emphasized the importance of strength training to complement technical skill. Neither of these practices had been popular among climbers, but as Gill's ability level and influence grew, his ideas became the norm.
Two important training tools emerged in the 1980s: Bouldering mats and artificial climbing walls. The former, also referred to as "crash pads", prevented injuries from falling, and enabled boulderers to climb in areas that would have been too dangerous to attempt otherwise. Indoor climbing walls helped spread the sport to areas without outdoor climbing, and allowed serious climbers to train year-round regardless of weather conditions.
As the sport grew in popularity, new bouldering areas were developed throughout Europe and the United States, and more athletes began participating in bouldering competitions. The visibility of the sport greatly increased in the early 2000s, as YouTube videos and climbing blogs helped boulderers around the world to quickly learn techniques, find hard problems, and announce newly completed projects.
In early 2010, two American climbers claimed first ascents on boulder problems that have come to be regarded as the most difficult in the world: "The Game" near Boulder, Colorado, established by Daniel Woods; and "Lucid Dreaming" near Bishop, California, established by Paul Robinson. The following year, fellow American Carlo Traversi claimed the second ascent of "The Game" and in January 2014, American Daniel Woods completed the second ascent of "Lucid Dreaming." In 2011, Czech climber Adam Ondra claimed the second ascent of "Gioia", originally established three years earlier by Italian boulderer Christian Core, and suggested that it was among the world's most challenging boulder problems.
Equipment.
Unlike other climbing sports, bouldering can be performed safely and effectively with very little equipment, an aspect which makes the discipline highly appealing to many climbers. Bouldering pioneer John Sherman asserted that "The only gear really needed to go bouldering is boulders". Others suggest the use of climbing shoes and a chalkbag as the bare minimum, while more experienced boulderers typically bring multiple pairs of shoes, chalk, brushes, crash pads, and a skincare kit.
Of the aforementioned equipment, climbing shoes have the most direct impact on performance. Besides protecting the climber's feet from rough surfaces, climbing shoes are designed to help the climber secure and maintain footholds. Climbing shoes typically fit much tighter than other athletic footwear, and often curl the toes downwards to enable precise footwork. They are manufactured in a variety of different styles in order to perform well in different situations: High-top shoes, for example, provide better protection for the ankle, while low-top shoes provide greater flexibility and freedom of movement. Stiffer shoes excel at securing small edges, whereas softer shoes provide greater sensitivity. The front of the shoe, called the "toe box", can be asymmetric, which performs well on overhanging rocks, or symmetric, which is better suited for vertical problems and slabs. 
Most boulderers use gymnastics chalk on their hands to absorb sweat. It is stored in a small chalkbag which can be tied around the waist, allowing the climber to reapply chalk during the climb. Brushes are used to remove excess chalk and other debris from boulders in between climbs; they are often attached to the end of a stick, pipe, or other straight object in order to reach higher holds. Crash pads, also referred to as bouldering mats, are foam cushions placed on the ground to protect climbers from falls.
Safety.
Boulder problems are generally shorter than from ground to top. This makes the sport significantly safer than free solo climbing, which is also performed without ropes, but with no upper limit on the height of the climb. However, minor injuries are common in bouldering, particularly sprained ankles and wrists. Two factors contribute to the frequency of injuries in bouldering: first, boulder problems typically feature more difficult moves than other climbing disciplines, making falls more common. Second, without ropes to arrest the climber's descent, every fall will cause the climber to hit the ground.
To prevent injuries, boulderers position crash pads near the boulder to provide a softer landing, as well as one or more spotters to help redirect the climber towards the pads. Upon landing, boulderers employ falling techniques similar to those used in gymnastics: spreading the impact across the entire body to avoid bone fractures, and positioning limbs to allow joints to move freely throughout the impact.
Technique.
As with other forms of climbing, bouldering technique is largely centered around proper footwork. Leg muscles are significantly stronger than arm muscles; as such, proficient boulderers use their arms primarily to maintain balance and body positioning, relying on their legs to push them up the boulder. Boulderers also keep their arms straight whenever possible, allowing their bones to support their body weight rather than their muscles.
Bouldering movements are described as either "static" or "dynamic". Static movements are those that are performed slowly, with the climber's position controlled by maintaining contact on the boulder with the other three limbs. Dynamic movements use the climber's momentum to reach holds that would be difficult or impossible to secure statically, with an increased risk of falling if the movement is not performed accurately.
Grading.
Bouldering problems are assigned numerical difficulty ratings by routesetters and climbers. The two most widely used rating systems are the V-scale and the Fontainebleau system. 
The V-scale, which originated in the United States, is an open-ended rating system with higher numbers indicating a higher degree of difficulty. The V1 rating indicates that a problem can be completed by a novice climber in good physical condition after several attempts. The scale begins at V0, and as of 2013, the highest V rating that has been assigned to a bouldering problem is V16. Some climbing gyms also use a VB grade to indicate beginner problems.
The Fontainebleau scale follows a similar system, with each numerical grade divided into three ratings with the letters "a", "b", and "c". For example, Fontainebleau 7A roughly corresponds with V6, while Fontainebleau 7C+ is equivalent to V10. In both systems, grades are further differentiated by appending "+" to indicate a small increase in difficulty. Despite this level of specificity, ratings of individual problems are often controversial, as ability level is not the only factor that affects how difficult a problem will be for a particular climber. Height, arm length, flexibility, and other body characteristics can also be relevant.
Environmental impact.
Bouldering can damage vegetation that grows on rocks, such as mosses and lichens. This can occur as a result of the climber intentionally cleaning the boulder, or unintentionally from repeated use of handholds and footholds. Vegetation on the ground surrounding the boulder can also be damaged from overuse, particularly by climbers laying down crash pads. Soil erosion can occur when boulderers trample vegetation while hiking off of established trails, or when they unearth small rocks near the boulder in an effort to make the landing zone safer. Other environmental concerns include littering, improperly disposed feces, and graffiti. These issues have caused some land managers to prohibit bouldering, as was the case in Tea Garden, a popular bouldering area in Rocklands, South Africa.

</doc>
<doc id="4115" url="http://en.wikipedia.org/wiki?curid=4115" title="Boiling point">
Boiling point

The boiling point of a substance is the temperature at which the vapor pressure of the liquid equals the pressure surrounding the liquid and the liquid changes into a vapor.
A liquid at high-pressure has a higher boiling point than when that liquid is at atmospheric pressure. In other words, the boiling point of a liquid varies depending upon the surrounding environmental pressure. For a given pressure, different liquids boil at different temperatures.
The normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, 1 atmosphere. At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of 1 bar.
The heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).
Liquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.
Saturation temperature and pressure.
A "saturated liquid" contains as much thermal energy as it can without boiling (or conversely a "saturated vapor" contains as little thermal energy as it can without condensing).
Saturation temperature means "boiling point". The saturation temperature is the temperature for a corresponding saturation pressure at which a liquid boils into its vapor phase. The liquid can be said to be saturated with thermal energy. Any addition of thermal energy results in a phase transition.
If the pressure in a system remains constant (isobaric), a vapor at saturation temperature will begin to condense into its liquid phase as thermal energy (heat) is removed. Similarly, a liquid at saturation temperature and pressure will boil into its vapor phase as additional thermal energy is applied.
The boiling point corresponds to the temperature at which the vapor pressure of the liquid equals the surrounding environmental pressure. Thus, the boiling point is dependent on the pressure. Boiling points may be published with respect to the NIST, USA standard pressure of 101.325 kPa (or 1 atm), or the IUPAC standard pressure of 100.000 kPa. At higher elevations, where the atmospheric pressure is much lower, the boiling point is also lower. The boiling point increases with increased pressure up to the critical point, where the gas and liquid properties become identical. The boiling point cannot be increased beyond the critical point. Likewise, the boiling point decreases with decreasing pressure until the triple point is reached. The boiling point cannot be reduced below the triple point.
If the heat of vaporization and the vapor pressure of a liquid at a certain temperature is known, the boiling point can be calculated by using the Clausius–Clapeyron equation thus:
formula_1
Saturation pressure is the pressure for a corresponding saturation temperature at which a liquid boils into its vapor phase. Saturation pressure and saturation temperature have a direct relationship: as saturation pressure is increased so is saturation temperature.
If the temperature in a system remains constant (an "isothermal" system), vapor at saturation pressure and temperature will begin to condense into its liquid phase as the system pressure is increased. Similarly, a liquid at saturation pressure and temperature will tend to flash into its vapor phase as system pressure is decreased.
There are two conventions regarding the "standard boiling point of water": The "normal boiling point" is at a pressure of 1 atm (i.e., 101.325 kPa). The IUPAC recommended "standard boiling point of water" at a standard pressure of 100 kPa (1 bar) is . For comparison, on top of Mount Everest, at elevation, the pressure is about and the boiling point of water is .
Relation between the normal boiling point and the vapor pressure of liquids.
The higher the vapor pressure of a liquid at a given temperature, the lower the normal boiling point (i.e., the boiling point at atmospheric pressure) of the liquid.
The vapor pressure chart to the right has graphs of the vapor pressures versus temperatures for a variety of liquids. As can be seen in the chart, the liquids with the highest vapor pressures have the lowest normal boiling points.
For example, at any given temperature, methyl chloride has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (−24.2 °C), which is where the vapor pressure curve of methyl chloride (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.
Properties of the elements.
The element with the lowest boiling point is helium. Both the boiling points of rhenium and tungsten exceed 5000 K at standard pressure; because it is difficult to measure extreme temperatures precisely without bias, both have been cited in the literature as having the higher boiling point.
Boiling point as a reference property of a pure compound.
As can be seen from the above plot of the logarithm of the vapor pressure vs. the temperature for any given pure chemical compound, its normal boiling point can serve as an indication of that compound's overall volatility. A given pure compound has only one normal boiling point, if any, and a compound's normal boiling point and melting point can serve as characteristic physical properties for that compound, listed in reference books. The higher a compound's normal boiling point, the less volatile that compound is overall, and conversely, the lower a compound's normal boiling point, the more volatile that compound is overall. Some compounds decompose at higher temperatures before reaching their normal boiling point, or sometimes even their melting point. For a stable compound, the boiling point ranges from its triple point to its critical point, depending on the external pressure. Beyond its triple point, a compound's normal boiling point, if any, is higher than its melting point. Beyond the critical point, a compound's liquid and vapor phases merge into one phase, which may be called a superheated gas. At any given temperature, if a compound's normal boiling point is lower, then that compound will generally exist as a gas at atmospheric external pressure. If the compound's normal boiling point is higher, then that compound can exist as a liquid or solid at that given temperature at atmospheric external pressure, and will so exist in equilibrium with its vapor (if volatile) if its vapors are contained. If a compound's vapors are not contained, then some volatile compounds can eventually evaporate away in spite of their higher boiling points.
In general, compounds with ionic bonds have high normal boiling points, if they do not decompose before reaching such high temperatures. Many metals have high boiling points, but not all. Very generally—with other factors being equal—in compounds with covalently bonded molecules, as the size of the molecule (or molecular mass) increases, the normal boiling point increases. When the molecular size becomes that of a macromolecule, polymer, or otherwise very large, the compound often decomposes at high temperature before the boiling point is reached. Another factor that affects the normal boiling point of a compound is the polarity of its molecules. As the polarity of a compound's molecules increases, its normal boiling point increases, other factors being equal. Closely related is the ability of a molecule to form hydrogen bonds (in the liquid state), which makes it harder for molecules to leave the liquid state and thus increases the normal boiling point of the compound. Simple carboxylic acids dimerize by forming hydrogen bonds between molecules. A minor factor affecting boiling points is the shape of a molecule. Making the shape of a molecule more compact tends to lower the normal boiling point slightly compared to an equivalent molecule with more surface area.
Most volatile compounds (anywhere near ambient temperatures) go through an intermediate liquid phase while warming up from a solid phase to eventually transform to a vapor phase. By comparison to boiling, a sublimation is a physical transformation in which a solid turns directly into vapor, which happens in a few select cases such as with carbon dioxide at atmospheric pressure. For such compounds, a sublimation point is a temperature at which a solid turning directly into vapor has a vapor pressure equal to the external pressure.
Impurities and mixtures.
In the preceding section, boiling points of pure compounds were covered. Vapor pressures and boiling points of substances can be affected by the presence of dissolved impurities (solutes) or other miscible compounds, the degree of effect depending on the concentration of the impurities or other compounds. The presence of non-volatile impurities such as salts or compounds of a volatility far lower than the main component compound decreases its mole fraction and the solution's volatility, and thus raises the normal boiling point in proportion to the concentration of the solutes. This effect is called boiling point elevation. As a common example, salt water boils at a higher temperature than pure water.
In other mixtures of miscible compounds (components), there may be two or more components of varying volatility, each having its own pure component boiling point at any given pressure. The presence of other volatile components in a mixture affects the vapor pressures and thus boiling points and dew points of all the components in the mixture. The dew point is a temperature at which a vapor condenses into a liquid. Furthermore, at any given temperature, the composition of the vapor is different from the composition of the liquid in most such cases. In order to illustrate these effects between the volatile components in a mixture, a boiling point diagram is commonly used. Distillation is a process of boiling and [usually] condensation which takes advantage of these differences in composition between liquid and vapor phases.

</doc>
<doc id="4116" url="http://en.wikipedia.org/wiki?curid=4116" title="Big Bang">
Big Bang

The Big Bang theory is the prevailing cosmological model for the early development of the universe. The key idea is that the universe is expanding. Consequently, the universe was denser and hotter in the past. Moreover, the Big Bang model suggests that at some moment all matter in the universe was contained in a single point, which is considered the beginning of the universe. Modern measurements place this moment at approximately 13.8 billion years ago, which is thus considered the age of the universe. After the initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, including protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.
The Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background, large scale structure, and Hubble's Law. As the distance between galaxies increases today, in the past galaxies were closer together. The known laws of nature can be used to calculate the characteristics of the universe in detail back in time to extreme densities and temperatures. While large particle accelerators can replicate such conditions, resulting in confirmation and refinement of the details of the Big Bang model, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is poorly understood and still an area of open investigation. The Big Bang theory does not provide any explanation for the initial conditions of the universe; rather, it describes and explains the general evolution of the universe going forward from that point on.
Belgian catholic priest and scientist Georges Lemaître proposed what became the Big Bang theory in 1927. Over time, scientists built on his initial idea of cosmic expansion, which, his theory went, could be traced back to the origin of the cosmos and which led to formation of the modern universe. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. In 1929, Edwin Hubble discovered that the distances to faraway galaxies were strongly correlated with their redshifts. Hubble's observation was taken to indicate that all distant galaxies and clusters have an apparent velocity directly away from our vantage point: that is, the farther away, the higher the apparent velocity, regardless of direction. Assuming that we are not at the center of a giant explosion, the only remaining interpretation is that all observable regions of the universe are receding from each other.
While the scientific community was once divided between supporters of two different expanding universe theories—the Big Bang and the Steady State theory, observational confirmation of the Big Bang scenario came with the discovery of the cosmic microwave background radiation in 1964, and later when its spectrum (i.e., the amount of radiation measured at each wavelength) was found to match that of thermal radiation from a black body. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology.
Overview.
Timeline of the Big Bang.
Singularity.
Extrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity signals the breakdown of general relativity. How closely we can extrapolate towards the singularity is debated—certainly no closer than the end of the Planck epoch. This singularity is sometimes called "the Big Bang", but the term can also refer to the early hot, dense phase itself, which can be considered the "birth" of our universe. Based on measurements of the expansion using Type Ia supernovae, measurements of temperature fluctuations in the cosmic microwave background, and measurements of the correlation function of galaxies, the universe has a calculated age of 13.798 ± 0.037 billion years. The agreement of these three independent measurements strongly supports the ΛCDM model that describes in detail the contents of the universe.
Inflation and baryogenesis.
The earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with an incredibly high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10−37 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially. After inflation stopped, the universe consisted of a quark–gluon plasma, as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.
Cooling.
The universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10−11 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle physics experiments. At about 10−6 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 1010 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).
A few minutes into the expansion, when the temperature was about a billion (one thousand million; 109; SI prefix giga-) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei. As the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation.
Structure formation.
Over a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available (from WMAP) show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an "extended model" which includes hot dark matter in the form of neutrinos, then if the "physical baryon density" Ωbh2 is estimated at about 0.023 (this is different from the 'baryon density' Ωb expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density Ωch2 is about 0.11, the corresponding neutrino density Ωvh2 is estimated to be less than 0.0062.
Cosmic acceleration.
Independent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate. Dark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both observationally and theoretically.
All of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. As noted above, there is no well-supported model describing the action prior to 10−15 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.
Underlying assumptions.
The Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.
These ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10−5. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.
If the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10−5 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.
Expansion of space.
General relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, themselves are specified using a coordinate chart or "grid" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). This metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system the grid expands along with the universe, and objects that are moving only due to the expansion of the universe remain at fixed points on the grid. While their "coordinate" distance (comoving distance) remains constant, the "physical" distance between two such comoving points expands proportionally with the scale factor of the universe.
The Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.
Horizons.
An important feature of the Big Bang spacetime is the presence of horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a "past horizon" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never "catch up" to very distant objects. This defines a "future horizon", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe. Our understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.
History.
Etymology.
English astronomer Fred Hoyle is credited with coining the term "Big Bang" during a 1949 BBC radio broadcast. It is popularly reported that Hoyle, who favored an alternative "steady state" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.
Development.
The Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a "spiral nebula" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were "island universes" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist and Roman Catholic priest, proposed that the inferred recession of the nebulae was due to the expansion of the universe.
In 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a "primeval atom" where and when the fabric of time and space came into existence.
Starting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the Cosmological Principle.
In the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Monsignor Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, viz., that matter is eternal. A beginning in time was "repugnant" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.
During the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.
After World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the cosmic microwave background radiation (CMB). Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as "this "big bang" idea" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the cosmic microwave background radiation in 1964 secured the Big Bang as the best theory of the origin and evolution of the cosmos. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.
Significant progress in Big Bang cosmology have been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.
Observational evidence.
The earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the "four pillars" of the Big Bang theory.
Precise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.
Hubble's law and the expansion of space.
Observations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:
where
Hubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.
The theory requires the relation "v" = "HD" to hold at all times, where "D" is the comoving distance, "v" is the recessional velocity, and "v", "H", and "D" vary as the universe expands (hence we write "H"0 to denote the present-day Hubble "constant"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity "v". However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.
That space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.
Measurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the cosmic microwave background over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.
Cosmic microwave background radiation.
In 1964 Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the general CMB predictions: the radiation was found to be consistent with an almost perfect black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.
The "surface of last scattering" corresponding to emission of the CMB occurs shortly after "recombination", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.
In 1989 NASA launched the Cosmic Background Explorer satellite (COBE). Its findings were consistent with predictions regarding the CMB, finding a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.725 K) and providing the first evidence for fluctuations (anisotropies) in the CMB, at a level of about one part in 105. John C. Mather and George Smoot were awarded the Nobel Prize for their leadership in this work. During the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.
In early 2003 the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.
Abundance of primordial elements.
Using the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for /, about 10−3 for /, about 10−4 for / and about 10−9 for /.
The measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for , and off by a factor of two ; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to "tune" the Big Bang to produce much more or less than 20–30% helium. Indeed there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than , and in constant ratios, too.
Galactic evolution and distribution.
Detailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then larger structures have been forming, such as galaxy clusters and superclusters. Populations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures agree well with Big Bang simulations of the formation of structure in the universe and are helping to complete details of the theory.
Primordial gas clouds.
In 2011 astronomers found what they believe to be pristine clouds of primordial gas, by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis. Their composition matches the composition predicted from Big Bang nucleosynthesis. This provides direct evidence that there was a period in the history of the universe before the formation of the first stars, when most ordinary matter existed in the form of clouds of neutral hydrogen.
Other lines of evidence.
The age of universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.
The prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.
On 17 March 2014, astronomers at the Harvard-Smithsonian Center for Astrophysics announced the apparent detection of primordial gravitational waves, which, if confirmed, may provide strong evidence for inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the findings was reported; and on 19 September 2014, even more lowered confidence.
Related issues in physics.
Baryon asymmetry.
It is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot, it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effect is not strong enough to explain the present baryon asymmetry.
Dark energy.
Measurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed "dark energy". Dark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the "missing" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.
Negative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Possible candidates include a cosmological constant and quintessence. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.
Dark matter.
During the 1970s and 80s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.
Indirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.
Globular cluster age.
In the mid-1990s observations of globular clusters appeared to be inconsistent with the Big Bang theory. Computer simulations that matched the observations of the stellar populations of globular clusters suggested that they were about 15 billion years old, which conflicted with the 13.8 billion year age of the universe. This issue was partially resolved in the late 1990s when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. There remain some questions as to how accurately the ages of the clusters are measured, but it is clear that observations of globular clusters no longer appear inconsistent with the Big Bang theory.
Problems.
There are generally considered to be three outstanding problems with the Big Bang theory: the horizon problem, the flatness problem, and the magnetic monopole problem. The most common answer to these problems is inflationary theory; however, since this creates new problems, other options have been proposed, such as the Weyl curvature hypothesis.
Horizon problem.
The horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.
A resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.
Heisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.
If inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.
Flatness problem.
The flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric. The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density, positive if greater, and zero at the critical density, in which case space is said to be "flat". The problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10−43 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes
(the time of nucleosynthesis), the universe density must have been within one part in 1014 of its critical value, or it would not exist as it does today.
A resolution to this problem is offered by inflationary theory. During the inflationary period, spacetime expanded to such an extent that its curvature would have been smoothed out. Thus, it is theorized that inflation drove the universe to a very nearly spatially flat state, with almost exactly the critical density.
Magnetic monopoles.
The magnetic monopole objection was raised in the late 1970s. Grand unification theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.
The future according to the Big Bang theory.
Before observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch. Alternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.
Modern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.
Speculative physics beyond the Big Bang theory.
While the Big Bang model is well established in cosmology, it is likely to be refined. The equations of classical general relativity indicate a singularity at the origin of cosmic time, although this conclusion depends on several assumptions and the equations break down at any time before the universe reached the Planck temperature. A correct treatment of quantum gravity may avoid the would-be singularity.
It is not known what could have caused the singularity to come into existence (if it had a cause), or how and why it originated, though speculation abounds in the field of cosmogony.
Some proposals, each of which entails untested hypotheses, are:
Proposals in the last two categories, see the Big Bang as an event in either a much larger and older universe or in a multiverse.
Religious and philosophical interpretations.
As a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, while others argue that Big Bang cosmology makes the notion of a creator superfluous.

</doc>
<doc id="4119" url="http://en.wikipedia.org/wiki?curid=4119" title="Bock">
Bock

Bock is a strong lager of German origin. Several substyles exist, including maibock (helles bock, heller bock), a paler, more hopped version generally made for consumption at spring festivals; doppelbock (double bock), a stronger and maltier version; and eisbock, a much stronger version made by partially freezing the beer and removing the ice that forms.
Originally a dark beer, a modern bock can range from light copper to brown in colour. The style is very popular, with many examples brewed internationally.
History.
The style known now as "bock" was a dark, malty, lightly hopped ale first brewed in the 14th century by German brewers in the Hanseatic town of Einbeck. The style from Einbeck was later adopted by Munich brewers in the 17th century and adapted to the new lager style of brewing. Due to their Bavarian accent, citizens of Munich pronounced "Einbeck" as "ein Bock" ("a billy goat"), and thus the beer became known as "bock". To this day, as a visual pun, a goat often appears on bock labels.
Bock is historically associated with special occasions, often religious festivals such as Christmas, Easter or Lent (the latter as ""). Bocks have a long history of being brewed and consumed by Bavarian monks as a source of nutrition during times of fasting.
Bockfest, in Cincinnati, Ohio, is the longest running and largest bock beer festival in the world. The festival celebrates bock beer, the Cincinnati neighborhood of Over-the-Rhine, the city's German heritage, and the coming of spring.
The styles of bock.
Traditional bock.
Traditional bock is a sweet, relatively strong (6.3%–7.2% by volume), lightly hopped (20-27 IBUs) lager. The beer should be clear, and colour can range from light copper to brown, with a bountiful and persistent off-white head. The aroma should be malty and toasty, possibly with hints of alcohol, but no detectable hops or fruitiness. The mouthfeel is smooth, with low to moderate carbonation and no astringency. The taste is rich and toasty, sometimes with a bit of caramel. Again, hop presence is low to undetectable, providing just enough bitterness so that the sweetness is not cloying and the aftertaste is muted. The following commercial products are indicative of the style: Einbecker Ur-Bock Dunkel, Pennsylvania Brewing St. Nick Bock, Aass Bock, Great Lakes Rockefeller Bock, Stegmaier Brewhouse Bock.
Maibock.
The maibock style, also known as helles bock or heller bock, is a helles lager brewed to bock strength, therefore still as strong as traditional bock, but lighter in colour and with more hop presence. It is a fairly recent development compared to other styles of bock beers, frequently associated with springtime and the month of May. Colour can range from deep gold to light amber with a large, creamy, persistent white head, and moderate to moderately high carbonation, while alcohol content ranges from 6.3% to 7.4% by volume. The flavour is typically less malty than a traditional bock, and may be drier, hoppier, and more bitter, but still with a relatively low hop flavour, with a mild spicy or peppery quality from the hops, increased carbonation and alcohol content. The following commercial products are indicative of the style: Ayinger Maibock, Mahr’s Bock, Hacker-Pschorr Hubertus Bock, Capital Maibock, Einbecker Mai-Urbock, Hofbräu Maibock, Victory St. Boisterous, Gordon Biersch Blonde Bock, Smuttynose Maibock, Old Dominion Brewing Company Big Thaw Bock, Brewery 85's Quittin' Time, and Rogue Dead Guy Ale.
Doppelbock.
"Doppelbock" or "double bock" is a stronger version of traditional bock that was first brewed in Munich by the Paulaner Friars, a Franciscan order founded by St. Francis of Paula. Historically, doppelbock was high in alcohol and sweet, thus serving as "liquid bread" for the Friars during times of fasting, when solid food was not permitted. Today, doppelbock is still strong—ranging from 7%–12% or more by volume. It is clear, with colour ranging from dark gold, for the paler version, to dark brown with ruby highlights for darker version. It has a large, creamy, persistent head (although head retention may be impaired by alcohol in the stronger versions). The aroma is intensely malty, with some toasty notes, and possibly some alcohol presence as well; darker versions may have a chocolate-like or fruity aroma. The flavour is very rich and malty, with toasty notes and noticeable alcoholic strength, and little or no detectable hops (16–26 IBUs). Paler versions may have a drier finish. The monks who originally brewed doppelbock named their beer "Salvator" ("Savior"), which today is trademarked by Paulaner. Brewers of modern doppelbocks often add "-ator" to their beer's name as a signpost of the style; there are 200 "-ator" doppelbock names registered with the German patent office. The following are representative examples of the style: Paulaner Salvator, Ayinger Celebrator, Weihenstephaner Korbinian, Andechser Doppelbock Dunkel, Spaten Optimator, Tucher Bajuvator, Weltenburger Kloster Asam-Bock, Capital Autumnal Fire, EKU 28, Eggenberg Urbock 23º, Bell's Consecrator, Moretti La Rossa, Samuel Adams Double Bock, Troegs Troegenator Double Bock, Devastator, Great Lakes Doppelrock, Abita Andygator.
Eisbock.
Eisbock is a traditional specialty beer of the Kulmbach district of Germany that is made by partially freezing a doppelbock and removing the water ice to concentrate the flavour and alcohol content, which ranges from 9% to 13% by volume. It is clear, with a colour ranging from deep copper to dark brown in colour, often with ruby highlights. Although it can pour with a thin off-white head, head retention is frequently impaired by the higher alcohol content. The aroma is intense, with no hop presence, but frequently can contain fruity notes, especially of prunes, raisins, and plums. Mouthfeel is full and smooth, with significant alcohol, although this should not be hot or sharp. The flavour is rich and sweet, often with toasty notes, and sometimes hints of chocolate, always balanced by a significant alcohol presence. The following are representative examples of the style: Kulmbacher Reichelbräu Eisbock, Eggenberg Urbock Dunkel Eisbock, Capital Eisphyre, Southampton Eisbock.
International variations.
Europe.
In Austria, bockbier is traditionally brewed only around Christmas and Easter, when nearly every brewery brews its own bock.
A number of bock beers are produced, including Brasserie d'Achouffe Bok and "Leute Bok" from the Van Steenberge brewer, brewed since 1927. Belgium-based InBev produces Artois Bock, which is exported internationally and can be found in areas where bock is not traditionally available.
Zagorka Brewery produces "Stolichno Bock Beer", a 6.5% abv beer.
A Spanish brewery Damm produces Bock Damm (5,4%)
Bock is an unusual style in the UK, but a few examples exist. The Robert Cains brewery in Liverpool brews Cains Double Bock beer at 8% abv, and the Dark Star Brewery, West Sussex, produce a 5.6% abv Maibock.
Dreher Brewery sells a rather strong (7.3% ABV) bock beer. It is called Bak, the name for billy goat in Hungarian.
A variation of bock called 'bokbier' is also brewed extensively in the Netherlands and occasionally in Belgium. Most larger Dutch breweries, such as Heineken International, Grolsch, Amstel, Alfa Brouwerij, Brand and Dommelsch, market at least one variety. Most bokbiers tend to be seasonal beers (traditionally autumn, although there are currently also spring, summer and winter boks). They are among the only few specialty beers that existed besides lager for a long time. Microbreweries may prefer to seasonally brew a bokbier, such as the eco-beer biobok, made in autumn by Brouwerij 't IJ in Amsterdam. The consumers' organization PINT holds a bok festival every autumn at the Beurs van Berlage in Amsterdam.
Bocks are also brewed in Norway, where they are known as "bokkøl" (bockbeers) and available during the whole year. Notable examples of bock brands are Aass, Borg, Frydenlund and Mack.
Bocks are also brewed in Poland, where they are known as "Koźlak" and available during the whole year. Notable examples of bock brands are Koźlak Amber, Miłosław Koźlak, Cornelius Kożlak.
One of the main beer brands in Portugal is Super Bock.
Only one Bock beer is brewed in Sweden; Mariestad's Old Ox, with an alcoholic percentage of 6,9%.
South America.
There are several beers brewed in Argentina which are termed bock, including "Araucana Negra Bock", "Quilmes Bock", and "Blest Bock".
In Bolivia, the Cerveceria Boliviana Nacional brews a beer called simply "Cerveza Bock," advertised primarily for its 7% alcohol by volume strength.
In Brazil, Kaiser is one of the breweries that sells bock beer, called "Kaiser Bock". This beer is available only in the months of fall and winter (April to September). Usually Brazilian bocks are produced by local breweries or craft breweries, especially in the cities of German settlement in Paraná/Santa Catarina States and also in Petrópolis, state of Rio de Janeiro. Kaiser Bock is made in Ponta Grossa, from Curitiba, capital of Paraná.
Kunstmann Brewery from Valdivia produces a dark, bittersweet version of bock. Kross brewery from Curacavi is producing a maibock (6.3% abv)
Inducerv S.A.S. brews a Bock (6.0% vol) under their Apóstol brand with German ingredients. The brewery is located in the city of Sabaneta, near Medellín.
North America.
Bock is a popular style, made by breweries across the country, including:
Cayman Island Brewing (George Town, Cayman Islands): Ironshore Bock (7.5% abv)
Bock beer is produced in Mexico around Christmas season, under the Noche Buena label with 5.9% abv
Bock and its substyles are popular in all parts of the country. The city of Cincinnati, Ohio has hosted a celebration called Bockfest since 1992 that promotes its German-style brewing history and the German culture of its Over-the-Rhine neighbourhood. A short list of American bocks include:
Africa.
Bock beer is produced and distributed under the Urbock label by Namibian Breweries. Like other Namibian Breweries beers, it is available in some of the neighbouring countries in Southern Africa, especially South Africa. The brewery also produces a maibock sporadically.
Oceania.
"Smokin' Bishop"; a bock-style beer that is brewed at the Invercargill Brewery
Doppel-bock by Monteiths; 6% alcohol

</doc>
<doc id="4124" url="http://en.wikipedia.org/wiki?curid=4124" title="Bantu languages">
Bantu languages

The Bantu languages (), technically the Narrow Bantu languages, constitute a traditional sub-branch of the Niger–Congo languages. There are about 250 Bantu languages by the criterion of mutual intelligibility, though the distinction between language and dialect is often unclear, and "Ethnologue" counts 535 languages. Bantu languages are spoken largely east and south of present-day Cameroon; i.e. in the regions commonly known as Central Africa, Southeast Africa, and Southern Africa. Parts of the Bantu area include languages from other language families (see map).
The Bantu language with the largest total number of speakers is Swahili; however, nearly all speakers know it as a second language. According to Ethnologue, there are over 40 million L2 (second-language) speakers, but only about 2 million native speakers.
According to Ethnologue, Shona is the most widely spoken as a first language, with 10.8 million speakers, followed closely by Zulu, with 10.3 million. Ethnologue also lists Manyika and Ndau as separate languages, though Shona speakers consider them to be two of the five main dialects of Shona. If the 3.4 million Manyika and Ndau speakers are included among the Shona, then Shona totals 14.2 million first-language speakers.
Ethnologue separates Kinyarwanda and Kirundi, which are largely mutually intelligible and hence often considered dialects of a single Rwanda-Rundi language. These two, grouped together, have 12.4 million speakers according to Ethnologue. Other estimates may be significantly larger or smaller. Estimates of number of speakers of most languages vary widely, due both to the lack of accurate statistics in most third-world countries and the difficulty in defining exactly where the boundaries of a language lie, particularly in the presence of a dialect continuum.
Origin.
The Bantu languages descend from a common Proto-Bantu language, which is believed to have been spoken in what is now Cameroon in West Africa. An estimated 2,500–3,000 years ago (500 BC to 1000 BC), although other sources put the start of the Bantu Expansion closer to 3000 BC, speakers of the Proto-Bantu language began a series of migrations eastward and southward, carrying agriculture with them. This Bantu expansion came to dominate Sub-Saharan Africa east of Cameroon, an area where Bantu peoples now constitute nearly the entire population.
The technical term Bantu, meaning "human beings" or simply "people", was first used by Wilhelm Bleek (1827–1875), as this is reflected in many of the languages of this group. A common characteristic of Bantu languages is that they use words such as "muntu" or "mutu" for "human being" or in simplistic terms "person", and the plural prefix for human nouns starting with "mu-" (class 1) in most languages is "ba-" (class 2), thus giving "bantu" for "people". Bleek, and later Carl Meinhof, pursued extensive studies comparing the grammatical structures of Bantu languages.
Classification.
The term 'narrow Bantu' was coined by the "Benue–Congo Working Group" to distinguish Bantu as recognized by Malcolm Guthrie in his seminal 1948 classification of the Bantu languages, from the Bantoid languages not recognized as Bantu by Guthrie (1948). In recent times, the distinctiveness of Narrow Bantu as opposed to the other Southern Bantoid groups has been called into doubt (cf. Piron 1995, Williamson & Blench 2000, Blench 2011), but the term is still widely used. A coherent classification of Narrow Bantu will likely need to exclude many of the Zone A and perhaps Zone B languages.
There is no true genealogical classification of the (Narrow) Bantu languages. Most attempted classifications are problematic in that they consider only languages which happen to fall within traditional Narrow Bantu, rather than South Bantoid, which has been established as a unit by the comparative method. The most widely used classification, the alphanumeric coding system developed by Guthrie, is mainly geographic. At a broader level, the family is commonly split in two depending on the reflexes of proto-Bantu tone patterns: Many Bantuists group together parts of zones A through D (the extent depending on the author) as "Northwest Bantu" or "Forest Bantu", and the remainder as "Central Bantu" or "Savanna Bantu". The two groups have been described as having mirror-image tone systems: Where Northwest Bantu has a high tone in a cognate, Central Bantu languages generally have a low tone, . Northwest Bantu is more divergent internally than Central Bantu, and perhaps less conservative due to contact with non-Bantu Niger–Congo languages; Central Bantu is likely the innovative line cladistically. Northwest Bantu is clearly not a coherent family, but even for Central Bantu the evidence is lexical, with little evidence that it is a historically valid group.
The only attempt at a detailed genetic classification to replace the Guthrie system is the 1999 "Tervuren" proposal of Bastin, Coupez, and Mann. However, it relies on lexicostatistics, which, because of its reliance on similarity rather than shared innovations, may predict spurious groups of conservative languages which are not closely related. Meanwhile, "Ethnologue" has added languages to the Guthrie classification that Guthrie overlooked, while removing the Mbam languages (much of zone A), and shifting some languages between groups (much of zones D and E to a new zone J, for example, and part of zone L to K, and part of M to F) in an apparent effort at a semi-genetic, or at least semi-areal, classification. This has been criticized for sowing confusion in one of the few unambiguous ways to distinguish Bantu languages. Nurse & Philippson (2006) evaluate many proposals for low-level groups of Bantu languages, but the result is not a complete portrayal of the family. "Glottolog" has incorporated many of these into their classification.
Nonetheless, some version of zone S (Southern Bantu) does appear to be a coherent group. The languages which share Dahl's Law may also form a valid group, Northeast Bantu. The infobox at right lists these together with various low-level groups that are fairly uncontroversial, though they continue to be revised. The development of a rigorous genealogical classification of many branches of Niger–Congo, not just Bantu, is hampered by insufficient data.
Language structure.
Guthrie reconstructed both the phonemic inventory and the core vocabulary of Proto-Bantu.
The most prominent grammatical characteristic of Bantu languages is the extensive use of affixes (see Sotho grammar and Ganda noun classes for detailed discussions of these affixes). Each noun belongs to a class, and each language may have several numbered classes, somewhat like genders in European languages. The class is indicated by a prefix that is part of the noun, as well as agreement markers on verb and qualificative roots connected with the noun. Plural is indicated by a change of class, with a resulting change of prefix.
The verb has a number of prefixes, though in the western languages these are often treated as independent words. In Swahili, for example, "Mtoto mdogo amekisoma" (also "Kamwana kadoko kariverenga" in Shona language) means 'The small child has read it [a book]'. "Mtoto" 'child' governs the adjective prefix "m-" and the verb subject prefix "a-". Then comes perfect tense "-me-" and an object marker "-ki-" agreeing with implicit "kitabu" 'book'. Pluralizing to 'children' gives "Watoto wadogo wamekisoma" ("Vana vadoko variverenga" in Shona), and pluralizing to 'books' ("vitabu") gives "Watoto wadogo wamevisoma".
Bantu words are typically made up of open syllables of the type CV (consonant-vowel) with most languages having syllables exclusively of this type. The Bushong language recorded by Vansina, however, has final consonants, while slurring of the final syllable (though written) is reported as common among the Tonga of Malawi. The morphological shape of Bantu words is typically CV, VCV, CVCV, VCVCV, etc.; that is, any combination of CV (with possibly a V- syllable at the start). In other words, a strong claim for this language family is that almost all words end in a vowel, precisely because closed syllables (CVC) are not permissible in most of the documented languages, as far as is understood. This tendency to avoid consonant clusters in some positions is important when words are imported from English or other non-Bantu languages. An example from Chewa: the word "school", borrowed from English, and then transformed to fit the sound patterns of this language, is "sukulu". That is, "sk-" has been broken up by inserting an epenthetic "-u-"; "-u" has also been added at the end of the word. Another example is "buledi" for "bread". Similar effects are seen in loanwords for other non-African CV languages like Japanese. However, a clustering of sounds at the beginning of a syllable can be readily observed in such languages as Shona, and the Makua variants.
Reduplication.
Reduplication is a common morphological phenomenon in Bantu languages and is usually used to indicate frequency or intensity of the action signalled by the (unreduplicated) verb stem.
Well-known words and names that have reduplication include
Repetition emphasizes the repeated word in the context that it is used. For instance, "Mwenda pole hajikwai," while, "Pole pole ndio mwendo," has two to emphasize the consistency of slowness of the pace. The meaning of the former in translation is, "He who goes slowly doesn't trip," and that of the latter is, "A slow but steady pace wins the race." Haraka haraka would mean hurrying just for the sake of hurrying, reckless hurry, as in "Njoo! Haraka haraka" [come here! Hurry, hurry].
On the contrary to the above definition, there are some words in some of the languages in which reduplication has the opposite meaning. It usually denotes short durations, and or lower intensity of the action and also means a few repetitions or a little bit more.
Notable Bantu languages.
Following are the principal Bantu languages of each country. Included are those languages that constitute at least 1% of the population and have at least 10% the number of speakers of the largest Bantu language in the country.
Most languages are best known in English without the class prefix ("Swahili", "Tswana", "Ndebele"), but are sometimes seen with the (language-specific) prefix ("Kiswahili", "Setswana", "Sindebele"). In a few cases prefixes are used to distinguish languages with the same root in their name, such as Tshiluba and Kiluba (both "Luba"), Umbundu and Kimbundu (both "Mbundu"). The bare (prefixless) form typically does not occur in the language itself, but is the basis for other words based on the ethnicity. So, in the country of Botswana the people are the "Batswana", one person is a "Motswana", and the language is "Setswana"; and in Uganda, centred on the kingdom of "Buganda", the dominant ethnicity are the "Baganda" (sg. "Muganda"), whose language is "Luganda".
Lingua franca
Angola
Botswana
Burundi
Cameroon
Central African Republic
Democratic Republic of the Congo
Equatorial Guinea
Kenya
Lesotho
Malawi
Mozambique
Nigeria
Namibia
Republic of the Congo (Congo-Brazzaville)
Rwanda
South Africa
Swaziland
Tanzania
Uganda
Zambia
Zimbabwe
This list is incomplete; an attempt at a full list of Bantu languages (with various conflations and a puzzlingly diverse nomenclature) was found in "The Bantu Languages of Africa", 1959.
Bantu words popularised in western cultures.
Some words from various Bantu languages have been borrowed into western languages. These include: 
A case has been made out for borrowings of many place-names and even misremembered rhymes – chiefly from one of the Luba varieties – in the USA.

</doc>
<doc id="4127" url="http://en.wikipedia.org/wiki?curid=4127" title="Bearing">
Bearing

Bearing may refer to:

</doc>
<doc id="4130" url="http://en.wikipedia.org/wiki?curid=4130" title="CIM-10 Bomarc">
CIM-10 Bomarc

The Boeing CIM-10 Bomarc (IM-99 Weapon System prior to September 1962) was a supersonic interceptor for Cold War air defense of North America which, in addition to being the first long-range anti-aircraft missile (cf. proposed WIZARD predecessor), was the only SAM deployed by the United States Air Force.
Stored horizontally in a launcher shelter with movable roof, the aircraft was erected to fire vertically using rocket boosters and then was ramjet-powered during midcourse command guidance to a dive point. During the "homing dive", the missile's onboard AN/DPN-34 radar allowed the BOMARC to guide itself to the target (e.g., enemy bomber or formation) and a radar proximity fuze detonated the warhead (conventional or 10 kiloton nuclear W-40). After the May 17, 1957, $7 million initial contract for operational aircraft; the "Interceptor Missile" was deployed at launch areas in Canada and the United States (e.g., the 1960 Fort Dix IM-99 accident contaminated a launch area.) Boeing indicated: "Differences in the Langley Base layout are due to planning for accommodation of the advanced missile system [(IM-99B) ground equipment with equipment for] the IM-99A system".
Launches were from Florida test range sites (AFMTC & Eglin's Santa Rosa Island) and controlled by AN/GPA-35 and/or AN/FSQ-7 computers, e.g., the Montgomery SAGE Center commanded three BOMARCs simultaneously in-flight during a May 1960 test. BOMARC launches as target drones began at Vandenberg Air Force Base on 25 August 1966 (the 1st Vandenberg BOMARC launch was 14 October 1964).
Design and development.
In 1946, Boeing started to study surface-to-air guided missiles under the United States Army Air Forces project MX-606. By 1950, Boeing had launched more than 100 test rockets in various configurations, all under the designator XSAM-A-1 GAPA (Ground-to-Air Pilotless Aircraft). Because these tests were very promising, Boeing received a USAF contract in 1949 to develop a pilotless interceptor (a term then used by the USAF for air-defense guided missiles) under project MX-1599. The MX-1599 missile was to be a ramjet-powered, nuclear-armed long-range surface-to-air missile to defend the Continental United States from high-flying bombers. The Michigan Aerospace Research Center (MARC) was added to the project soon afterward, and this gave the new missile its name Bomarc (for Boeing and MARC). In 1951, the USAF decided to emphasize its point of view that missiles were nothing else than pilotless aircraft by assigning aircraft designators to its missile projects, and anti-aircraft missiles received F-for-Fighter designations. The Bomarc became the F-99.
Test flights of XF-99 test vehicles began in September 1952 and continued through early 1955. The XF-99 tested only the liquid-fueled booster rocket, which would accelerate the missile to ramjet ignition speed. In February 1955, tests of the XF-99A propulsion test vehicles began. These included live ramjets, but still had no guidance system or warhead. The designation YF-99A had been reserved for the operational test vehicles. In August 1955, the USAF discontinued the use of aircraft-like type designators for missiles, and the XF-99A and YF-99A became XIM-99A and YIM-99A, respectively. Originally the USAF had allocated the designation IM-69, but this was changed (possibly at Boeing's request to keep number 99) to IM-99 in October 1955. In October 1957, the first YIM-99A production-representative prototype flew with full guidance, and succeeded to pass the target within destructive range. In late 1957, Boeing received the production contract for the IM-99A Bomarc A interceptor missile, and in September 1959, the first IM-99A squadron became operational.
The IM-99A had an operational radius of and was designed to fly at Mach 2.5–2.8 at a cruising altitude of . It was long and weighed . Its armament was either a conventional warhead or a W40 nuclear warhead (7–10 kiloton yield). A liquid-fuel rocket engine boosted the Bomarc to Mach 2, when its Marquardt RJ43-MA-3 ramjet engines, fueled by 80-octane gasoline, would take over for the remainder of the flight.
The operational IM-99A missiles were based horizontally in semi-hardened shelters, nicknamed "coffins". After the launch order, the shelter's roof would slide open, and the missile raised to the vertical. After the missile was supplied with fuel for the booster rocket, it would be launched by the Aerojet General LR59-AJ-13 booster. After sufficient speed was reached, the Marquardt RJ43-MA-3 ramjets would ignite and propel the missile to its cruise speed and altitude of Mach 2.8 at .
When the Bomarc was within of the target, its own Westinghouse AN/DPN-34 radar guided the missile to the interception point. The maximum range of the IM-99A was , and it was fitted with either a conventional high-explosive or a 10 kiloton W-40 nuclear fission warhead.
The Bomarc relied on the Semi-Automatic Ground Environment (SAGE), an automated control system used by NORAD for detecting, tracking and intercepting enemy bomber aircraft. SAGE allowed for remote launching of the Bomarc missiles, which were housed in a constant combat-ready basis in individual launch shelters in remote areas. At the height of the program, there were 14 Bomarc sites located in the United States and two in Canada.
The liquid-fuel booster of the Bomarc A was no optimal solution. It took two minutes to fuel before launch, which could be a long time in high-speed intercepts, and its hypergolic propellants (hydrazine and nitric acid) were very dangerous to handle, leading to several serious accidents.
As soon as high-thrust solid-fuel rockets became a reality in the mid-1950s, the USAF began to develop a new solid-fueled Bomarc variant, the IM-99B Bomarc B. It used a Thiokol XM51 booster, and also had improved Marquardt RJ43-MA-7 (and finally the RJ43-MA-11) ramjets. The first IM-99B was launched in May 1959, but problems with the new propulsion system delayed the first fully successful flight until July 1960, when a supersonic KD2U-1/MQM-15A Regulus II drone was intercepted. Because the new booster took up less space in the missile, more ramjet fuel could be carried, increasing the range to . The terminal homing system was also improved, using the world's first pulse Doppler search radar, the Westinghouse AN/DPN-53. All Bomarc Bs were equipped with the W-40 nuclear warhead. In June 1961, the first IM-99B squadron became operational, and Bomarc B quickly replaced most Bomarc A missiles. On March 23, 1961, a Bomarc B successfully intercepted a Regulus II cruise missile flying at 100,000 ft, thus achieving the highest interception in the world up to that date.
Boeing built 570 Bomarc missiles between 1957 and 1964, 269 CIM-10A, 301 CIM-10B.
In September 1958 Air Research & Development Command decided to transfer the Bomarc program from its testing at Cape Canaveral Air Force Station to a new facility on Santa Rosa Island, immediately south of Eglin AFB Hurlburt Field on the Gulf of Mexico. To operate the facility and to provide training and operational evaluation in the missile program, Air Defense Command established the 4751st Air Defense Wing (Missile) (4751st ADW) on 15 January 1958. The first launch from Santa Rosa took place on 15 January 1959.
Operational history.
The first USAF operational Bomarc squadron was the 46th Air Defense Missile Squadron (ADMS), organized on 1 January 1959 and activated on 25 March. The 46th ADMS was assigned to the New York Air Defense Sector at McGuire Air Force Base, New Jersey. The training program, under the 4751st ADW used technicians acting as instructors and was established for a four-month duration. Training included missile maintenance; SAGE operations and launch procedures, including the launch of an unarmed missile at Eglin. In September 1959 the squadron assembled at their permanent station, the Bomarc site near McGuire AFB, and trained for operational readiness. The first Bomarc-A went operational at McGuire on 19 September 1959 with Kincheloe AFB getting the first operational IM-99Bs. While several of the squadrons replicated earlier fighter interceptor unit numbers, they were all new organizations with no previous historical counterpart.
ADC's initial plans called for some 52 Bomarc sites around the United States with 120 missiles each but as defense budgets decreased during the 1950s the number of sites dropped substantially. Ongoing development and reliability problems didn't help, nor did Congressional debate over the missile's usefulness and necessity. In June 1959, the Air Force authorized 16 Bomarc sites with 56 missiles each; the initial five would get the IM-99A with the remainder getting the IM-99B. However, in March 1960, HQ USAF cut deployment to eight sites in the United States and two in Canada.
Within a year of becoming operational, a Bomarc-A with a nuclear warhead caught fire at McGuire AFB on 7 June 1960 following the explosive rupture of its onboard helium tank. While the missile's explosives didn't detonate, the heat melted the warhead, releasing plutonium, which the fire crews spread. The Air Force and the Atomic Energy Commission cleaned up the site and covered it with concrete. This was the only major incident involving the weapons system. The site remained in operation for several years following the fire. After its closure in 1972, the accident resulted in the area remaining off limits to the present day, primarily due to low levels of plutonium contamination. In 2002, the concrete at the site was removed and transported to Lakehurst Naval Air Station for transport by rail to a site for proper disposal.
In 1962, the US Air Force started using modified A-models as drones; following the October 1962 tri-service redesignation of aircraft and weapons systems they became CQM-10As. Otherwise the air defense missile squadrons maintained alert while making regular trips to Santa Rosa Island for training and firing practice. After the inactivation of the 4751st ADW(M) on 1 July 1962 and transfer of Hurlburt to Tactical Air Command for air commando operations the 4751st Air Defense Squadron (Missile) remained at Hurlburt and Santa Rosa Island for training purposes.
In 1964, the liquid-fueled Bomarc-A sites and squadrons began to be inactivated. The sites at Dow and Suffolk County closed first. The remainder soldiered on for several more years while the government started dismantling the air defense missile network. Niagara Falls was the first BOMARC B installation to close, in December 1969; the others remained on alert through 1972. In April 1972, the last Bomarc B in U.S. Air Force service was retired at McGuire and the 46th ADMS inactivated.
The Bomarc, designed to intercept relatively slow manned bombers, had become a useless asset in the era of the intercontinental ballistic missile. The remaining Bomarc missiles were used by all armed services as high-speed target drones for tests of other air-defense missiles. The Bomarc A and Bomarc B targets were designated as CQM-10A and CQM-10B, respectively.
Notably, due to the accident, the McGuire complex has never been sold or converted to other uses and remains in Air Force ownership, making it the most intact site of the eight in the US. It has been nominated to the National Register of Historic Sites. Although a number of IM-99/CIM-10 Bomarcs have been placed on public display, concerns about the possible environmental hazards of the thoriated magnesium structure of the airframe have resulted in several being removed from public view.
Russ Sneddon, director of the Air Force Armament Museum, Eglin Air Force Base, Florida provided information about missing CIM-10 exhibit airframe serial 59-2016, one of the museum's original artifacts from its founding in 1975 and donated by the 4751st Air Defense Squadron at Hurlburt Field, Eglin Auxiliary Field 9, Eglin AFB. As of December 2006, the suspect missile was stored in a secure compound behind the Armaments Museum. In December 2010, the airframe was still on premises, but partially dismantled.
Canada and the Bomarc.
The Bomarc Missile Program was highly controversial in Canada. The Progressive Conservative government of Prime Minister John Diefenbaker initially agreed to deploy the missiles, and shortly thereafter controversially scrapped the Avro Arrow, a supersonic manned interceptor aircraft, arguing that the missile program made the Arrow unnecessary.
Initially, it was unclear whether the missiles would be equipped with nuclear warheads. By 1960 it became known that the missiles were to have a nuclear payload, and a debate ensued about whether Canada should accept nuclear weapons. Ultimately, the Diefenbaker government decided that the Bomarcs should not be equipped with nuclear warheads. The dispute split the Diefenbaker Cabinet, and led to the collapse of the government in 1963. The Official Opposition and Liberal Party leader Lester "Mike" Pearson originally was against nuclear missiles, but reversed his personal position and argued in favor of accepting nuclear warheads. He won the 1963 election, largely on the basis of this issue, and his new Liberal government proceeded to accept nuclear-armed Bomarcs, with the first being deployed on 31 December 1963. When the nuclear warheads were deployed, Pearson's wife, Maryon, resigned her honorary membership in the anti-nuclear weapons group, Voice of Women.
Canadian operations.
Canadian operational deployment of the Bomarc involved the formation of two specialized Surface/Air Missile squadrons. The first to begin operations was No. 446 SAM Squadron at RCAF Station North Bay, Ontario which was the command and control center for both squadrons. With construction of the compound and related facilities completed in 1961, the squadron received its Bomarcs in 1961, without nuclear warheads. The squadron became fully operational from 31 December 1963, when the nuclear warheads arrived, until disbanding on 31 March 1972. All the warheads were stored separately and under control of Detachment 1 of the USAF 425th Munitions Maintenance Squadron. During operational service, the Bomarcs were maintained on stand-by, on a 24-hour basis, but were never fired, although the squadron test-fired the missiles at Eglin AFB, Florida on annual winter retreats.
No. 447 SAM Squadron operating out of RCAF Station La Macaza, Quebec was activated on 15 September 1962 although warheads were not delivered until late 1963. The squadron followed the same operational procedures as No. 446, its sister squadron. With the passage of time the operational capability of the 1950s-era Bomarc system no longer met modern requirements; the Department of National Defence deemed that the Bomarc missile defense was no longer a viable system, and ordered both squadrons to be stood down in 1972. The bunkers and ancillary facilities remain at both former sites.
Operators.
Locations under construction but not activated. Each site was programmed for 28 IM-99B missiles:
Surviving missiles.
Below is a list of museums or sites which have a Bomarc missile on display:

</doc>
<doc id="4132" url="http://en.wikipedia.org/wiki?curid=4132" title="Branco River">
Branco River

The Branco River (; Engl: "White River") is the principal affluent of the Rio Negro from the north. It is enriched by many streams from the Tepui highlands which separate Venezuela and Guyana from Brazil. Its two upper main tributaries are the Uraricoera and the Takutu. The latter almost links its sources with those of the Essequibo.
The Branco flows nearly south, and finds its way into the Negro through several channels and a chain of lagoons similar to those of the latter river. It is long, up to its Uraricoera confluence. It has numerous islands, and, above its mouth, it is broken by a bad series of rapids.

</doc>
<doc id="4146" url="http://en.wikipedia.org/wiki?curid=4146" title="Bus">
Bus

A bus (; plural "buses" or "busses", , archaically also omnibus, multibus, or autobus) is a road vehicle designed to carry many passengers. Buses can have a capacity as high as 300 passengers. The most common type of bus is the single-decker rigid bus, with larger loads carried by double-decker and articulated buses, and smaller loads carried by midibuses and minibuses; coaches are used for longer-distance services. Bus manufacturing is increasingly globalised, with the same design appearing around the world. 
Buses may be used for scheduled bus transport, scheduled coach transport, school transport, private hire, or tourism; promotional buses may be used for political campaigns and others are privately operated for a wide range of purposes.
Horse-drawn buses were used from the 1820s, followed by steam buses in the 1830s, and electric trolleybuses in 1882. The first internal combustion engine buses, or motor buses, were used in 1895. Recently, interest has been growing in hybrid electric buses, fuel cell buses, and electric buses, as well as ones powered by compressed natural gas or biodiesel.
Etymology.
Bus is a clipped form of the Latin word "omnibus". It appeared in Paris in 1819–20 as "(voiture) omnibus" meaning “(carriage) for all”, and appeared in London in 1829. One etymology holds that "omnibus" is derived from a hatter's shop which was situated in front of one of the first bus stations in Nantes, France in 1823. "Omnes Omnibus" was a pun on the Latin-sounding name of that hatter Omnès: "omnes" meaning "all" and "omnibus" means "for all" in Latin. Nantes citizens soon gave the nickname Omnibus to the vehicle.
History.
Early history.
A short-lived early public bus line (known as a "carriage" at that time) was launched by Blaise Pascal in Paris in 1662; it was quite popular until fares were increased and access to the service was restricted to high-society members by regulation and law. Services ceased after 15 years and no further such services were known until the 1820s.
First omnibus services.
John Greenwood arguably established the first modern omnibus service in 1824. As the keeper of a toll gate in Pendleton on the Manchester-to-Liverpool turnpike, he purchased a horse and a cart with several seats, and began an omnibus service between those two locations. His pioneering idea was to offer a service where, unlike with a stagecoach, no prior booking was necessary and the driver would pick up or set down passengers anywhere on request. Later on, he added daily services to Buxton, Chester, and Sheffield. His line immediately sparked fierce competition and a dense network of omnibus services quickly sprouted in the area, often acting as feeders to the railways. In 1865, Greenwood's company and its competitors amalgamated into the Manchester Carriage Company.
In Nantes, a similar service to Greenwood's was established by Stanislas Baudry in 1826 and soon expanded into the Bordeaux region. By 1828, he was licensed by the city authorities of Paris for the provision of 10 services throughout the capital. The coaches used along this line were of innovative design; they were capable of transporting as many as two dozen people at a time and had three in-built compartments for the different paying classes.
The coaches were commissioned from the English engineer George Shillibeer, and were introduced into the streets of Paris in 1827. Shillibeer built another bus for the Quaker Newington Academy for Girls near London; this had a total of 25 seats, and entered history as the first school bus.
Whilst in Paris, Shillibeer concluded that operating similar vehicles in London, but for the fare-paying public with multiple stops, would be a paying enterprise, so he returned to his native city. His first London "Omnibus" took up service on 4 July 1829 on the route between Paddington (The Yorkshire Stingo) and "Bank" (Bank of England) via the "New Road" (now Marylebone Rd), Somers Town and City Road. Four services were provided in each direction daily. Shillibeer's success prompted many competitors to enter the market, and for a time buses were referred to as 'Shillibeers' despite Shillibeer having previously coined the term 'omnibus'.
Although passenger-carrying carriages had operated for many years, the new 'omnibus' pioneered a new service of picking up and setting down customers all along a particular route without the need to book in advance. Buses soon expanded their capacity, with additional seats for a few extra passengers provided alongside the driver. By 1845, passengers were being accommodated on the curved roofs, seated back to back in a configuration known as ‘knife-board’. In 1852, Greenwood's in Manchester introduced the double-decker vehicle that could seat up to 42.
In Germany, the first bus service was established in Berlin in 1825, running from Brandenburger Tor to Charlottenburg. In 1850, Thomas Tilling started horse bus services in London, and in 1855, the London General Omnibus Company was founded to amalgamate and regulate the horse-drawn omnibus services then operating in London.
By the 1880s, bus services were a commonplace in England, continental Europe, and North America; one company in London was operating over 220 horse-buses. Horse-bus use declined with the advent of steam-buses and motor-buses; the last horse bus in London stopped operation in 1914.
Steam buses.
Regular intercity bus services by steam-powered buses were pioneered in England in the 1830s by Walter Hancock and by associates of Sir Goldsworthy Gurney, among others, running reliable services over road conditions which were too hazardous for horse-drawn transportation.
The first 'mechanically propelled omnibus appeared on the streets of London on April 22, 1833. Steam carriages were much less likely to overturn, they travelled faster than horse-drawn carriages, they were much cheaper to run, and caused much less damage to the road surface due to their wide tyres.
However, the heavy road tolls imposed by the turnpike trusts discouraged steam road vehicles and left the way clear for the horse bus companies, and from 1861 onwards, harsh legislation virtually eliminated mechanically propelled vehicles altogether from the roads of Great Britain for 30 years, the Locomotive Act of that year imposing restrictive speed limits on "road locomotives" of 5 mph in towns and cities, and 10 mph in the country.
Trolleybuses.
In parallel to the development of the bus was the invention of the electric trolleybus, typically fed through trolley poles by overhead wires. The Siemens brothers, William in England and Ernst Werner in Germany, collaborated on the development of the trolleybus concept. Sir William first proposed the idea in an article to the "Journal of the Society of Arts" in 1881 as an "...arrangement by which an ordinary omnibus...would have a suspender thrown at intervals from one side of the street to the other, and two wires hanging from these suspenders; allowing contact-rollers to run on these two wires, the current could be conveyed to the tram-car, and back again to the dynamo machine at the station, without the necessity of running upon rails at all."
The first such vehicle, the Electromote, was made by his brother Dr. Ernst Werner von Siemens and presented to the public in 1882 in Halensee, Germany. Although this experimental vehicle fulfilled all the technical criteria of a typical trolleybus, it was dismantled in the same year after the demonstration.
Max Schiemann opened the world's first passenger-carrying trolleybus in 1901 near Dresden, in Germany. Although this system operated only until 1904, Schiemann had developed what is now the standard trolleybus current collection system. In the early days, a few other methods of current collection were used. Leeds and Bradford became the first cities to put trolleybuses into service in Great Britain on 20 June 1911.
Motor buses.
In Siegerland, Germany, two passenger bus lines ran briefly, but unprofitably, in 1895 using a six-passenger motor carriage developed from the 1893 Benz Viktoria. Another commercial bus line using the same model Benz omnibuses ran for a short time in 1898 in the rural area around Llandudno, Wales.
Daimler also produced one of the earliest motor-bus models in 1898, selling a double-decker bus to the Motor Traction Company for use on the streets of London. The vehicle had a maximum speed of 18 kph and accommodated up to 20 passengers, in an enclosed area below and on an open-air platform above. With the success and popularity of this bus, Daimler expanded production, selling more buses to companies in London and, in 1899, to Stockholm and Speyer. Daimler also entered into a partnership with the British company Milnes and developed a new double-decker in 1902 that became the market standard.
The first mass-produced bus model was the B-type double-decker bus, designed by Frank Searle and operated by the London General Omnibus Company – it entered service in 1910, and almost 3,000 had been built by the end of the decade. Hundreds saw military service on the Western Front during the First World War.
The Yellow Coach Manufacturing Company, which rapidly became a major manufacturer of buses in the US, was founded in Chicago in 1923 by John D. Hertz. General Motors purchased a majority stake in 1925 and changed its name to the Yellow Truck and Coach Manufacturing Company. They then purchased the balance of the shares in 1943 to form the GM Truck and Coach Division.
Models expanded in the 20th century, leading to the widespread introduction of the contemporary recognizable form of full-sized buses from the 1950s. The Routemaster, developed in the 1950s, was a pioneering design and remains an icon of London to this day. The innovative design used lightweight aluminium and techniques developed in aircraft production during World War II. As well as a novel weight-saving integral design, it also introduced for the first time on a bus independent front suspension, power steering, a fully automatic gearbox, and power-hydraulic braking.
Types.
Formats include single-decker bus, double-decker bus (both usually with a rigid chassis), limobus, and articulated bus (or 'bendy-bus') the prevalence of which varies from country to country. Bi-articulated buses are also manufactured, and passenger-carrying trailers—either towed behind a rigid bus (a bus trailer), or hauled as a trailer by a truck (a trailer bus). Smaller midibuses have a lower capacity and open-top buses are typically used for leisure purposes. In many new fleets, particularly in local transit systems, a shift to low-floor buses is occurring, primarily for easier accessibility. Coaches are designed for longer-distance travel and are typically fitted with individual high-backed reclining seats, seat belts, toilets, and audio-visual entertainment systems, and can operate at higher speeds with more capacity for luggage. Coaches may be single- or double-deckers, articulated, and often include a separate luggage compartment under the passenger floor. Guided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes.
Bus manufacturing may be by a single company (an integral manufacturer), or by one manufacturer's building a bus body over a chassis produced by another manufacturer.
Design.
Accessibility.
Transit buses used to be mainly high-floor vehicles. However, they are now increasingly of low-floor design and optionally also 'kneel' air suspension and have electrically or hydraulically extended under-floor ramps to provide level access for wheelchair users and people with baby carriages. Prior to more general use of such technology, these wheelchair users could only use specialist paratransit mobility buses.
Accessible vehicles also have wider entrances and interior gangways and space for wheelchairs. Interior fittings and destination displays may also be designed to be usable by the visually impaired. Coaches generally use wheelchair lifts instead of low-floor designs. In some countries, vehicles are required to have these features by disability discrimination laws.
Configuration.
Buses were initially configured with an engine in the front and an entrance at the rear. With the transition to one-man operation, many manufacturers moved to mid- or rear-engined designs, with a single door at the front, or multiple doors. The move to the low-floor design has all but eliminated the mid-engined design, although some coaches still have mid-mounted engines. Front-engined buses still persist for niche markets such as American school buses, some minibuses, and buses in less developed countries, which may be derived from truck chassis, rather than purpose-built bus designs. Most buses have two axles, articulated buses have three.
Guidance.
Guided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes. Guidance can be mechanical, optical, or electromagnetic. Extensions of the guided technology include the Guided Light Transit and Translohr systems, although these are more often termed 'rubber-tyred trams' as they have limited or no mobility away from their guideways.
Liveries.
Transit buses are normally painted to identify the operator or a route, function, or to demarcate low-cost or premium service buses. Liveries may be painted onto the vehicle, applied using adhesive vinyl technologies, or using decals. Vehicles often also carry bus advertising or part or all of their visible surfaces (as mobile billboard). Campaign buses may be decorated with key campaign messages; these can be to promote an event or initiative.
Propulsion.
The most common power source since the 1920s has been the diesel engine. Early buses, known as trolleybuses, were powered by electricity supplied from overhead lines. Nowadays, electric buses often carry their own battery, which is sometimes recharged on stops/stations to keep the size of the battery small/lightweight. Currently, interest exists in hybrid electric buses, fuel cell buses, electric buses, and ones powered by compressed natural gas or biodiesel. Gyrobuses, which are powered by the momentum stored by a flywheel, were tried in the 1940s.
Manufacture.
Early bus manufacturing grew out of carriage coachbuilding, and later out of automobile or truck manufacturers. Early buses were merely a bus body fitted to a truck chassis. This body+chassis approach has continued with modern specialist manufacturers, although there also exist integral designs such as the Leyland National where the two are practically inseparable. Specialist builders also exist and concentrate on building buses for special uses, or modifying standard buses into specialised products.
Integral designs have the advantages that they are have been well-tested for strength and stability, and also are off-the-shelf. However, two incentives cause use of the chassis+body model. First, it allows the buyer and manufacturer both to shop for the best deal for their needs, rather than having to settle on one fixed design—the buyer can choose the body and the chassis separately. Second, over the lifetime of a vehicle (in constant service and heavy traffic), it will likely get minor damage now and again, and being able easily to replace a body panel or window etc. can vastly increase its service life and save the cost and inconvenience of removing it from service.
As with the rest of the automotive industry, into the 20th century, bus manufacturing increasingly became globalized, with manufacturers producing buses far from their intended market to exploit labour and material cost advantages. As with the cars, new models are often exhibited by manufacturers at prestigious industry shows to gain new orders. A typical city bus costs almost US$450,000.
Uses.
Public transport.
Transit buses, used on public transport bus services, have utilitarian fittings designed for efficient movement of large numbers of people, and often have multiple doors. Coaches are used for longer-distance routes. High-capacity bus rapid transit services may use the bi-articulated bus or tram-style buses such as the Wright StreetCar and the Irisbus Civis.
Buses and coach services often operate to a predetermined published public transport timetable defining the route and the timing, but smaller vehicles may be used on more flexible demand responsive transport services.
Tourism.
Buses play a major part in the tourism industry. Tour buses around the world allow tourists to view local attractions or scenery. These are often open-top buses, but can also be by regular bus or coach.
In local sightseeing, City Sightseeing is the largest operator of local tour buses, operating on a franchised basis all over the world. Specialist tour buses are also often owned and operated by safari parks and other theme parks or resorts. Longer-distance tours are also carried out by bus, either on a turn up and go basis or through a tour operator, and usually allow disembarkation from the bus to allow touring of sites of interest on foot. These may be day trips or longer excursions incorporating hotel stays. Tour buses often carry a tour guide, although the driver or a recorded audio commentary may also perform this function. The tour operator may itself be a subsidiary of a company that operates buses and coaches for other uses, or an independent company that charters buses or coaches. Commuter transport operators may also use their coaches to conduct tours within the target city between the morning and evening commuter transport journey.
Buses and coaches are also a common component of the wider package holiday industry, providing private airport transfers (in addition to general airport buses) and organised tours and day trips for holidaymakers on the package.
Public long-distance coach networks are also often used as a low-cost method of travel by students or young people travelling the world. Some companies such as Topdeck Travel were set up to specifically use buses to drive the hippie trail or travel to places such as North Africa.
In many tourist or travel destinations, a bus is part of the tourist attraction, such as the North American tourist trolleys, London’s Routemaster heritage routes, or the customised buses of Malta, Asia, and the Americas.
Student transport.
In some countries, particularly the USA and Canada, buses used to transport school children have evolved into a specific design with specified mandatory features. American states have also adopted laws regarding motorist conduct around school buses, including serious fines and the possibility of prison time for passing a stopped school bus in the process of offloading children passengers. These school buses feature things such as the school bus yellow livery and crossing guards. Other countries may mandate the use of seat belts. As a minimum, many countries require a bus carrying students to display a , and may also adopt yellow liveries. Student transport often uses older buses cascaded from service use, retrofitted with more seats and/or seatbelts. Student transport may be operated by local authorities or private contractors. Schools may also own and operate their own buses for other transport needs, such as class field trips, or transport to associated sports, music, or other school events.
Private charter.
Due to the costs involved in owning, operating, and driving buses and coaches, many bus and coach uses a private hire of vehicles from charter bus companies, either for a day or two, or a longer contract basis, where the charter company provides the vehicles and qualified drivers.
Charter bus operators may be completely independent businesses, or charter hire may be a subsidiary business of a public transport operator that might maintain a separate fleet or use surplus buses, coaches, and dual-purpose coach-seated buses. Many private taxicab companies also operate larger minibus vehicles to cater for group fares. Companies, private groups, and social clubs may hire buses or coaches as a cost-effective method of transporting a group to an event or site, such as a group meeting, racing event, or organised recreational activity such as a summer camp. Entertainment or event companies may also hire temporary shuttles buses for transport at events such as festivals or conferences. Party buses are used by companies in a similar manner to limousine hire, for luxury private transport to social events or as a touring experience. Sleeper buses are used by bands or other organisations that tour between entertainment venues and require mobile rest and recreation facilities. Some couples hire preserved buses for their wedding transport instead of the traditional car. Buses are often hired for parades or processions. Victory parades are often held for triumphant sports teams, who often tour their home town or city in an open-top bus. Sports teams may also contract out their transport to a team bus, for travel to away games, to a competition or to a final event. These buses are often specially decorated in a livery matching the team colours. Private companies often contract out private shuttle bus services, for transport of their customers or patrons, such as hotels, amusement parks, university campuses, or private airport transfer services. This shuttle usage can be as transport between locations, or to and from parking lots. High specification luxury coaches are often chartered by companies for executive or VIP transport. Charter buses may also be used in tourism and for promotion (See Tourism and Promotion sections)
Private ownership.
Many organisations, including the police, not for profit, social or charitable groups with a regular need for group transport may find it practical or cost-effective to own and operate a bus for their own needs. These are often minibuses for practical, tax and driver licensing reasons, although they can also be full size buses. Cadet or scout groups or other youth organizations may also own buses. Specific charities may exist to fund and operate bus transport, usually using specially modified mobility buses or otherwise accessible buses (See Accessibility section). Some use their contributions to buy vehicles, and provide volunteer drivers.
Airport operators make use of special airside airport buses for crew and passenger transport in the secure airside parts of an airport. Some public authorities, police forces and military forces make use of armoured buses where there is a special need to provide increased passenger protection. The United States Secret Service acquired two in 2010 for transporting dignitaries needing special protection. Police departments make use of police buses for a variety of reasons, such as prisoner transport, officer transport, temporary detention facilities, and as command and control vehicles. Some fire departments also use a converted bus as a command post, while those in cold climates might retain a bus as a heated shelter at fire scenes. Many are drawn from retired school or service buses.
Promotion.
Buses are often used for advertising, political campaigning, , public relations or promotional purposes. These may take the form of temporary charter hire of service buses, or the temporary or permanent conversion and operation of buses, usually of second-hand buses. Extreme examples include converting the bus with displays and decorations or awnings and fittings. Interiors may be fitted out for exhibition or information purposes with special equipment and/or audio visual devices.
Bus advertising takes many forms, often as interior and exterior adverts and all-over advertising liveries. The practice often extends into the exclusive private hire and use of a bus to promote a brand or product, appearing at large public events, or touring busy streets. The bus is sometimes staffed by promotions personnel, giving out free gifts. Campaign buses are often specially decorated for a political campaign or other social awareness information campaign, designed to bring a specific message to different areas, and/or used to transport campaign personnel to local areas/meetings. Exhibition buses are often sent to public events such as fairs and festivals for purposes such as recruitment campaigns, for example by private companies or the armed forces. Complex urban planning proposals may be organised into a mobile exhibition bus for the purposes of public consultation.
Buses around the world.
Historically, the types and features of buses have developed according to local needs. Buses were fitted with technology appropriate to the local climate or passenger needs, such as air conditioning in Asia, or cycle mounts on North American buses. The bus types in use around the world where there was little mass production were often sourced second hand from other countries, such as the Malta bus, and buses in use in Africa. Other countries such as Cuba required novel solutions to import restrictions, with the creation of the "camellos" (camel bus), a specially manufactured trailer bus.
After the Second World War, manufacturers in Europe and the Far East, such as Mercedes-Benz buses and Mitsubishi Fuso expanded into other continents influencing the use of buses previously served by local types. Use of buses around the world has also been influenced by colonial associations or political alliances between countries. Several of the Commonwealth nations followed the British lead and sourced buses from British manufacturers, leading to a prevalence of double-decker buses. Several Eastern Bloc countries adopted trolleybus systems, and their manufacturers such as Trolza exported trolleybuses to other friendly states. In the 1930s Italy designed the world's only triple decker bus for the busy route between Rome and Tivoli that could carry eighty-eight passengers. It was unique not only in being a triple decker but having a separate smoking compartment on the third level.
The buses to be found in countries around the world often reflect the quality of the local road network, with high floor resilient truck based designs prevalent in several less developed countries where buses are subject to tough operating conditions. Population density also has a major impact, where dense urbanisation such as in Japan and the far east has led to the adoption of high capacity long multi-axle buses, often double-deckers, while South America and China are implementing large numbers of articulated buses for bus rapid transit schemes.
Bus expositions.
Euro Bus Expo is a trade show, which is held bi-annually at the UK's National Exhibition Centre in Birmingham. As the official show of the Confederation of Passenger Transport, the UK’s trade association for the bus, coach and light rail industry, the three-day event offers visitors from Europe and beyond the chance to see and experience, at first hand, the very latest vehicles and product and service innovations right across the industry. The next show will be held in November 2014.
Busworld Kortrijk is the leading bus trade fair in Europe is the Busworld in Kortrijk in Belgium. It is held bi-annually, last time October 2011 and next time October 2013.
Use of retired buses.
Most public or private buses and coaches, once they have reached the end of their service with one or more operators, are sent to the wrecking yard for breaking up for scrap and spare parts. Some buses, while not economical to keep running as service buses, are often converted in some way for use by the operator, or another user, for purposes other than revenue earning transport. Much like old cars and trucks, buses often pass through a dealership where they can be bought for a price or at auction.
Bus operators will often find it economical to convert retired buses to use as permanent training buses for driver training, rather than taking a regular service bus out of use. Some large operators also converted retired buses into tow bus vehicles, to act as tow trucks. With the outsourcing of maintenance staff and facilities, the increase in company health and safety regulations, and the increasing curb weights of buses, many operators now contract their towing needs to a professional vehicle recovery company.
Some retired buses have been converted to static or mobile cafés, often using historic buses as a tourist attraction. Food is also provided from a catering bus, in which a bus is converted into a mobile canteen and break room. These are commonly seen at external filming locations to feed the cast and crew, and at other large events to feed staff. Another use is as an emergency vehicle, such as high-capacity ambulance bus or mobile command center.
Some organisations adapt and operate playbuses or learning buses to provide a playground or learning environments to children who might not have access to proper play areas. An ex-London Routemaster bus has been converted to a mobile theatre and catwalk fashion show.
Some buses meet a destructive end by being entered in banger races or at demolition derbys. A larger number of old retired buses have also been converted into mobile holiday homes and campers.
Bus preservation.
Rather than being scrapped or converted for other uses, sometimes retired buses are saved for preservation. This can be done by individuals, volunteer preservation groups or charitable trusts, museums, or sometimes by the operators themselves as part of a heritage fleet. These buses often need to undergo a degree of vehicle restoration to restore them to their original condition, and will have their livery and other details such as internal notices and rollsigns restored to be authentic to a specific time in the bus's actual history. Some buses that undergo preservation are rescued from a state of great disrepair, but others enter preservation with very little wrong with them. As with other historic vehicles, many preserved buses either in a working or static state form part of the collections of transport museums. Working buses will often be exhibited at rallies and events, and they are also used as charter buses. While many preserved buses are quite old or even vintage, in some cases relatively new examples of a bus type can enter restoration. In-service examples are still in use by other operators. This often happens when a change in design or operating practice, such as the switch to one person operation or low floor technology, renders some buses redundant while still relatively new.

</doc>
<doc id="4147" url="http://en.wikipedia.org/wiki?curid=4147" title="Bali">
Bali

Bali is an island and province of Indonesia. The province includes the island of Bali and a few smaller neighbouring islands, notably Nusa Penida. It is located at the westernmost end of the Lesser Sunda Islands, between Java to the west and Lombok to the east. Its capital of Denpasar is located at the southern part of the island.
With a population of 3,890,757 in the 2010 census, and currently 4,225,000 as at January 2014, the island is home to most of Indonesia's Hindu minority. According to the 2010 Census, 84.5% of Bali's population adhered to Balinese Hinduism, 12% to Islam, and most of the remainder followed Christianity.
Bali is the largest tourist destination in the country and is renowned for its highly developed arts, including traditional and modern dance, sculpture, painting, leather, metalworking, and music. Since the late 20th century, the province has had a rise in tourism.
Bali is part of the Coral Triangle, the area with the highest biodiversity of marine species. In this area alone over 500 reef building coral species can be found. For comparison, this is about 7 times as many as in the entire Caribbean. There is a wide range of dive sites with high quality reefs, all with their own specific attractions. Many sites can have strong currents and swell, so diving without a knowledgeable guide is inadvisable. Most recently, Bali was the host of the 2011 ASEAN Summit, 2013 APEC and Miss World 2013.
History.
Bali was inhabited around 2000 BC by Austronesian people who migrated originally from Southeast Asia and Oceania through Maritime Southeast Asia. Culturally and linguistically, the Balinese are closely related to the people of the Indonesian archipelago, Malaysia, the Philippines, and Oceania. Stone tools dating from this time have been found near the village of Cekik in the island's west.
In ancient Bali, nine Hindu sects existed, namely Pasupata, Bhairawa, Siwa Shidanta, Waisnawa, Bodha, Brahma, Resi, Sora and Ganapatya. Each sect revered a specific deity as its personal Godhead.
Balinese culture was strongly influenced by Indian, Chinese, and particularly Hindu culture, beginning around the 1st century AD. The name "Bali dwipa" ("Bali island") has been discovered from various inscriptions, including the Blanjong pillar inscription written by Sri Kesari Warmadewa in 914 AD and mentioning "Walidwipa". It was during this time that the people developed their complex irrigation system "subak" to grow rice in wet-field cultivation. Some religious and cultural traditions still practised today can be traced to this period.
The Hindu Majapahit Empire (1293–1520 AD) on eastern Java founded a Balinese colony in 1343. When the empire declined, there was an exodus of intellectuals, artists, priests, and musicians from Java to Bali in the 15th century.
The first known European contact with Bali is thought to have been made in 1512, when a Portuguese expedition led by Antonio Abreu and Francisco Serrão sighted its nothern shores. It was the first expedition of a series of bi-annual fleets to the Moluccas, that throughout the 16th century usually traveled along the coasts of the Sunda Islands. Bali was also mapped in 1512, in the chart of Francisco Rodrigues, aboard the expedition. In 1585, a ship foundered off the Bukit Peninsula and left a few Portuguese in the service of Dewa Agung. In 1597 the Dutch explorer Cornelis de Houtman arrived at Bali and, the Dutch East India Company was established in 1602. The Dutch government expanded its control across the Indonesian archipelago during the second half of the 19th century (see Dutch East Indies). Dutch political and economic control over Bali began in the 1840s on the island's north coast, when the Dutch pitted various competing Balinese realms against each other. In the late 1890s, struggles between Balinese kingdoms in the island's south were exploited by the Dutch to increase their control.
In June 1860 the famous Welsh naturalist, Alfred Russel Wallace, travelled to Bali from Singapore, landing at Bileling on the northcoast of the island. Wallace's trip to Bali was instrumental in helping him devise his Wallace Line theory. The Wallace Line is a faunal boundary that runs through the strait between Bali and Lombok. It has been found to be a boundary between species of Asiatic origin in the east and a mixture of Australian and Asian species to the west. In his travel memoir "The Malay Archipelago," Wallace wrote of his experience in Bali:
I was both astonished and delighted; for as my visit to Java was some years later, I had never beheld so beautiful and well-cultivated a district out of Europe. A slightly undulating plain extends from the seacoast about ten or twelve miles inland, where it is bounded by a fine range of wooded and cultivated hills. Houses and villages, marked out by dense clumps of coconut palms, tamarind and other fruit trees, are dotted about in every direction; while between them extend luxurious rice-grounds, watered by an elaborate system of irrigation that would be the pride of the best cultivated parts of Europe. 
The Dutch mounted large naval and ground assaults at the Sanur region in 1906 and were met by the thousands of members of the royal family and their followers who fought against the superior Dutch force in a suicidal "puputan" defensive assault rather than face the humiliation of surrender. Despite Dutch demands for surrender, an estimated 200 Balinese marched to their death against the invaders. In the Dutch intervention in Bali, a similar massacre occurred in the face of a Dutch assault in Klungkung. Afterward the Dutch governors exercised administrative control over the island, but local control over religion and culture generally remained intact. Dutch rule over Bali came later and was never as well established as in other parts of Indonesia such as Java and Maluku.
In the 1930s, anthropologists Margaret Mead and Gregory Bateson, artists Miguel Covarrubias and Walter Spies, and musicologist Colin McPhee all spent time here. Their accounts of the island and its peoples created a western image of Bali as "an enchanted land of aesthetes at peace with themselves and nature." Western tourists began to visit the island.
Imperial Japan occupied Bali during World War II. It was not originally a target in their Netherlands East Indies Campaign, but as the airfields on Borneo were inoperative due to heavy rains, the Imperial Japanese Army decided to occupy Bali, which did not suffer from comparable weather. The island had no regular Royal Netherlands East Indies Army (KNIL) troops. There was only a Native Auxiliary Corps "Prajoda" (Korps Prajoda) consisting of about 600 native soldiers and several Dutch KNIL officers under command of KNIL Lieutenant Colonel W.P. Roodenburg. On 19 February 1942 the Japanese forces landed near the town of Senoer [Senur]. The island was quickly captured.
During the Japanese occupation, a Balinese military officer, Gusti Ngurah Rai, formed a Balinese 'freedom army'. The lack of institutional changes and the harshness of war requisitions made Japanese rule worse than the Dutch one. Following Japan's Pacific surrender in August 1945, the Dutch returned to Indonesia, including Bali, to reinstate their pre-war colonial administration. This was resisted by the Balinese rebels, who now used recovered Japanese weapons. On 20 November 1946, the Battle of Marga was fought in Tabanan in central Bali. Colonel I Gusti Ngurah Rai, by then 29 years old, finally rallied his forces in east Bali at Marga Rana, where they made a suicide attack on the heavily armed Dutch. The Balinese battalion was entirely wiped out, breaking the last thread of Balinese military resistance.
In 1946 the Dutch constituted Bali as one of the 13 administrative districts of the newly proclaimed State of East Indonesia, a rival state to the Republic of Indonesia, which was proclaimed and headed by Sukarno and Hatta. Bali was included in the "Republic of the United States of Indonesia" when the Netherlands recognised Indonesian independence on 29 December 1949.
The 1963 eruption of Mount Agung killed thousands, created economic havoc and forced many displaced Balinese to be "transmigrated" to other parts of Indonesia. Mirroring the widening of social divisions across Indonesia in the 1950s and early 1960s, Bali saw conflict between supporters of the traditional caste system, and those rejecting this system. Politically, the opposition was represented by supporters of the Indonesian Communist Party (PKI) and the Indonesian Nationalist Party (PNI), with tensions and ill-feeling further increased by the PKI's land reform programs. An attempted coup in Jakarta was put down by forces led by General Suharto.
The army became the dominant power as it instigated a violent anti-communist purge, in which the army blamed the PKI for the coup. Most estimates suggest that at least 500,000 people were killed across Indonesia, with an estimated 80,000 killed in Bali, equivalent to 5% of the island's population. With no Islamic forces involved as in Java and Sumatra, upper-caste PNI landlords led the extermination of PKI members.
As a result of the 1965/66 upheavals, Suharto was able to manoeuvre Sukarno out of the presidency. His "New Order" government reestablished relations with western countries. The pre-War Bali as "paradise" was revived in a modern form. The resulting large growth in tourism has led to a dramatic increase in Balinese standards of living and significant foreign exchange earned for the country. A bombing in 2002 by militant Islamists in the tourist area of Kuta killed 202 people, mostly foreigners. This attack, and another in 2005, severely reduced tourism, producing much economic hardship to the island.
Geography.
The island of Bali lies 3.2 km (2 mi) east of Java, and is approximately 8 degrees south of the equator. Bali and Java are separated by the Bali Strait. East to west, the island is approximately 153 km (95 mi) wide and spans approximately 112 km (69 mi) north to south; administratively it covers 5,780 km2, or 5,577 km2 without Nusa Penida District, its population density is roughly 750 people/km2.
Bali's central mountains include several peaks over 3,000 metres in elevation. The highest is Mount Agung (3,031 m), known as the "mother mountain" which is an active volcano. Mountains range from centre to the eastern side, with Mount Agung the easternmost peak. Bali's volcanic nature has contributed to its exceptional fertility and its tall mountain ranges provide the high rainfall that supports the highly productive agriculture sector. South of the mountains is a broad, steadily descending area where most of Bali's large rice crop is grown. The northern side of the mountains slopes more steeply to the sea and is the main coffee producing area of the island, along with rice, vegetables and cattle. The longest river, Ayung River, flows approximately 75 km.
The island is surrounded by coral reefs. Beaches in the south tend to have white sand while those in the north and west have black sand. Bali has no major waterways, although the Ho River is navigable by small "sampan" boats. Black sand beaches between Pasut and Klatingdukuh are being developed for tourism, but apart from the seaside temple of Tanah Lot, they are not yet used for significant tourism.
The largest city is the provincial capital, Denpasar, near the southern coast. Its population is around 491,500 (2002). Bali's second-largest city is the old colonial capital, Singaraja, which is located on the north coast and is home to around 100,000 people. Other important cities include the beach resort, Kuta, which is practically part of Denpasar's urban area, and Ubud, situated at the north of Denpasar, is the island's cultural centre.
Three small islands lie to the immediate south east and all are administratively part of the Klungkung regency of Bali: Nusa Penida, Nusa Lembongan and Nusa Ceningan. These islands are separated from Bali by the Badung Strait.
To the east, the Lombok Strait separates Bali from Lombok and marks the biogeographical division between the fauna of the Indomalayan ecozone and the distinctly different fauna of Australasia. The transition is known as the Wallace Line, named after Alfred Russel Wallace, who first proposed a transition zone between these two major biomes. When sea levels dropped during the Pleistocene ice age, Bali was connected to Java and Sumatra and to the mainland of Asia and shared the Asian fauna, but the deep water of the Lombok Strait continued to keep Lombok Island and the Lesser Sunda archipelago isolated.
Ecology.
Bali lies just to the west of the Wallace Line, and thus has a fauna that is Asian in character, with very little Australasian influence, and has more in common with Java than with Lombok. An exception is the Yellow-crested Cockatoo, a member of a primarily Australasian family. There are around 280 species of birds, including the critically endangered Bali Starling, which is endemic. Others Include Barn Swallow, Black-naped Oriole, Black Racket-tailed Treepie, Crested Serpent-eagle, Crested Treeswift, Dollarbird, Java Sparrow, Lesser Adjutant, Long-tailed Shrike, Milky Stork, Pacific Swallow, Red-rumped Swallow, Sacred Kingfisher, Sea Eagle, Woodswallow, Savanna Nightjar, Stork-billed Kingfisher, Yellow-vented Bulbul, White Heron, Great Egret.
Until the early 20th century, Bali was home to several large mammals: the wild Banteng, leopard and the endemic Bali tiger. The Banteng still occurs in its domestic form, whereas leopards are found only in neighbouring Java, and the Bali tiger is extinct. The last definite record of a tiger on Bali dates from 1937, when one was shot, though the subspecies may have survived until the 1940s or 1950s. The relatively small size of the island, conflict with humans, poaching and habitat reduction drove the Bali tiger to extinction. This was the smallest and rarest of all tiger subspecies and was never caught on film or displayed in zoos, whereas few skins or bones remain in museums around the world. Today, the largest mammals are the Javan Rusa deer and the Wild Boar. A second, smaller species of deer, the Indian Muntjac, also occurs. Saltwater crocodiles were once present on the island, but became locally extinct sometime during the last century.
Squirrels are quite commonly encountered, less often is the Asian Palm Civet, which is also kept in coffee farms to produce Kopi Luwak. Bats are well represented, perhaps the most famous place to encounter them remaining the Goa Lawah (Temple of the Bats) where they are worshipped by the locals and also constitute a tourist attraction. They also occur in other cave temples, for instance at Gangga Beach. Two species of monkey occur. The Crab-eating Macaque, known locally as "kera", is quite common around human settlements and temples, where it becomes accustomed to being fed by humans, particularly in any of the three "monkey forest" temples, such as the popular one in the Ubud area. They are also quite often kept as pets by locals. The second monkey, endemic to Java and some surrounding islands such as Bali, is far rarer and more elusive is the Javan Langur, locally known as "lutung". They occur in few places apart from the Bali Barat National Park. They are born an orange colour, though by their first year they would have already changed to a more blackish colouration. In Java however, there is more of a tendency for this species to retain its juvenile orange colour into adulthood, and so you can see a mixture of black and orange monkeys together as a family. Other rarer mammals include the Leopard Cat, Sunda Pangolin and Black Giant Squirrel.
Snakes include the King Cobra and Reticulated Python. The Water Monitor can grow to at least in length and and can move quickly.
The rich coral reefs around the coast, particularly around popular diving spots such as Tulamben, Amed, Menjangan or neighbouring Nusa Penida, host a wide range of marine life, for instance Hawksbill Turtle, Giant Sunfish, Giant Manta Ray, Giant Moray Eel, Bumphead Parrotfish, Hammerhead Shark, Reef Shark, barracuda, and sea snakes. Dolphins are commonly encountered on the north coast near Singaraja and Lovina.
A team of scientists conducted a survey from 29 April 2011 to 11 May 2011 at 33 sea sites around Bali. They discovered 952 species of reef fish of which 8 were new discoveries at Pemuteran, Gilimanuk, Nusa Dua, Tulamben and Candidasa, and 393 coral species, including two new ones at Padangbai and between Padangbai and Amed.
The average coverage level of healthy coral was 36% (better than in Raja Ampat and Halmahera by 29% or in Fakfak and Kaimana by 25%) with the highest coverage found in Gili Selang and Gili Mimpang in Candidasa, Karangasem regency.
Many plants have been introduced by humans within the last centuries, particularly since the 20th century, making it sometimes hard to distinguish what plants are really native. Among the larger trees the most common are: Banyan trees, Jackfruit, coconuts, bamboo species, acacia trees and also endless rows of coconuts and banana species. Numerous flowers can be seen: hibiscus, frangipani, bougainvillea, poinsettia, oleander, jasmine, water lily, lotus, roses, begonias, orchids and hydrangeas exist. On higher grounds that receive more moisture, for instance around Kintamani, certain species of fern trees, mushrooms and even pine trees thrive well. Rice comes in many varieties. Other plants with agricultural value include: salak, mangosteen, corn, Kintamani orange, coffee and water spinach.
Environment.
Some of the worst erosion has occurred in Lebih Beach, where up to 7 metres of land is lost every year. Decades ago, this beach was used for holy pilgrimages with more than 10,000 people, but they have now moved to Masceti Beach.
From ranked third in previous review, in 2010 Bali got score 99.65 of Indonesia's environmental quality index and the highest of all the 33 provinces. The score measured 3 water quality parameters: the level of total suspended solids (TSS), dissolved oxygen (DO) and chemical oxygen demand (COD).
Because of over-exploitation by the tourist industry which covers a massive land area, 200 out of 400 rivers on the island have dried up and based on research, the southern part of Bali would face a water shortage up to 2,500 litres of clean water per second by 2015.
To ease the shortage, the central government plans to build a water catchment and processing facility at Petanu River in Gianyar. The 300 litres capacity of water per second will be channelled to Denpasar, Badung and Gianyar in 2013.
Administrative divisions.
The province is divided into 8 regencies ("kabupaten") and 1 city ("kota"). These are:
Economy.
Three decades ago, the Balinese economy was largely agriculture-based in terms of both output and employment. Tourism is now the largest single industry in terms of income, and as a result, Bali is one of Indonesia's wealthiest regions. In 2003, around 80% of Bali's economy was tourism related. By end of June 2011, non-performing loan of all banks in Bali were 2.23%, lower than the average of Indonesian banking industry non-performing loan (about 5%). The economy, however, suffered significantly as a result of the terrorist bombings 2002 and 2005. The tourism industry has since recovered from these events.
Agriculture.
Although tourism produces the GDP's largest output, agriculture is still the island's biggest employer; most notably rice cultivation. Crops grown in smaller amounts include fruit, vegetables, "Coffea arabica" and other cash and subsistence crops. Fishing also provides a significant number of jobs. Bali is also famous for its artisans who produce a vast array of handicrafts, including batik and ikat cloth and clothing, wooden carvings, stone carvings, painted art and silverware. Notably, individual villages typically adopt a single product, such as wind chimes or wooden furniture.
The Arabica coffee production region is the highland region of Kintamani near Mount Batur. Generally, Balinese coffee is processed using the wet method. This results in a sweet, soft coffee with good consistency. Typical flavours include lemon and other citrus notes. Many coffee farmers in Kintamani are members of a traditional farming system called Subak Abian, which is based on the Hindu philosophy of "Tri Hita Karana". According to this philosophy, the three causes of happiness are good relations with God, other people and the environment. The Subak Abian system is ideally suited to the production of fair trade and organic coffee production. Arabica coffee from Kintamani is the first product in Indonesia to request a Geographical Indication.
Tourism.
The tourism industry is primarily focused in the south, while significant in the other parts of the island as well. The main tourist locations are the town of Kuta (with its beach), and its outer suburbs of Legian and Seminyak (which were once independent townships), the east coast town of Sanur (once the only tourist hub), in the center of the island Ubud, to the south of the Ngurah Rai International Airport, Jimbaran, and the newer development of Nusa Dua and Pecatu.
The American government lifted its travel warnings in 2008. The Australian government last issued an advice on Friday, 4 May 2012. The overall level of the advice was lowered to 'Exercise a high degree of caution'. The Swedish government issued a new warning on Sunday, 10 June 2012 because of one more tourist who has been killed by methanol poisoning.
An offshoot of tourism is the growing real estate industry. Bali real estate has been rapidly developing in the main tourist areas of Kuta, Legian, Seminyak and Oberoi. Most recently, high-end 5 star projects are under development on the Bukit peninsula, on the south side of the island. Million dollar villas are being developed along the cliff sides of south Bali, commanding panoramic ocean views. Foreign and domestic (many Jakarta individuals and companies are fairly active) investment into other areas of the island also continues to grow. Land prices, despite the worldwide economic crisis, have remained stable.
In the last half of 2008, Indonesia's currency had dropped approximately 30% against the US dollar, providing many overseas visitors value for their currencies. Visitor arrivals for 2009 were forecast to drop 8% (which would be higher than 2007 levels), due to the worldwide economic crisis which has also affected the global tourist industry, but not due to any travel warnings.
Bali's tourism economy survived the terrorist bombings of 2002 and 2005, and the tourism industry has in fact slowly recovered and surpassed its pre-terrorist bombing levels; the longterm trend has been a steady increase of visitor arrivals. In 2010, Bali received 2.57 million foreign tourists, which surpassed the target of 2.0–2.3 million tourists. The average occupancy of starred hotels achieved 65%, so the island is still able to accommodate tourists for some years without any addition of new rooms/hotels, although at the peak season some of them are fully booked.
Bali received the Best Island award from Travel and Leisure in 2010. The island of Bali won because of its attractive surroundings (both mountain and coastal areas), diverse tourist attractions, excellent international and local restaurants, and the friendliness of the local people. According to BBC Travel released in 2011, Bali is one of the World's Best Islands, rank in second after Greece.
In August 2010, the film version of Eat, Pray, Love (EPL) was released in theatres. The movie was based on Elizabeth Gilbert's best-selling memoir of the same name. It took place at Ubud and Padang-Padang Beach at Bali. The 2006 book, which spent 57 weeks at the No. 1 spot on the New York Times paperback nonfiction best-seller list, had already fuelled a boom in EPL tourism in Ubud, the hill town and cultural and tourist center that was the focus of Gilbert's quest for balance through traditional spirituality and healing that leads to love.
Since 2011, China has displaced Japan as the second-largest supplier of tourists to Bali, while Australia still tops the list. Chinese tourists increased by 17% from last year due to the impact of ACFTA and new direct flights to Bali.
In January 2012, Chinese tourists year on year (yoy) increased by 222.18% compared to January 2011, while Japanese tourists declined by 23.54% yoy.
Bali reported that it has 2.88 million foreign tourists and 5 million domestic tourists in 2012, marginally surpassing the expectations of 2.8 million foreign tourists. Forecasts for 2013 are at 3.1 million.
Based on Bank Indonesia survey in May 2013, 34.39 percent of tourists are upper-middle class with spending between $1,286 to $5,592 and dominated by Australia, France, China, Germany and the US with some China tourists move from low spending before to higher spending currently. While 30.26 percent are middle class with spending between $662 to $1,285.
Transportation.
The Ngurah Rai International Airport is located near Jimbaran, on the isthmus at the southernmost part of the island. Lt.Col. Wisnu Airfield is found in north-west Bali.
A coastal road circles the island, and three major two-lane arteries cross the central mountains at passes reaching to 1,750m in height (at Penelokan). The Ngurah Rai Bypass is a four-lane expressway that partly encircles Denpasar and enables cars to travel quickly in the heavily populated south. Bali has no railway lines yet.
December 2010: the Government of Indonesia has invited investors to build Tanah Ampo Cruise Terminal at Karangasem, Bali amounted $30 million. In 17 July 2011 the first cruise ship (Sun Princess) anchored about 400 meters away from the wharf of Tanah Ampo harbour. The current pier is only 154 meters and will eventually be 300 to 350 meters to accommodate international cruise ships. The harbour would be safer than Benoa and has a scenic backdrop of a panoramic view of mountainous area with green rice fields. By December 2011 the auction process will be settled and Tanah Ampo is predicted to become the main hub for cruise ships in Indonesia by 2013.
A Memorandum of Understanding has been signed by two ministers, Bali's Governor and Indonesian Train Company to build 565 kilometres of railway along the coast around the island. It should be operating by 2015.
On 16 March 2011 (Tanjung) Benoa port received the "Best Port Welcome 2010" award from London's "Dream World Cruise Destination" magazine. Government plans to expand the role of Benoa port as export-import port to boost Bali's trade and industry sector. The Tourism and Creative Economy Ministry has confirmed that 306 cruise liners are heading for Indonesia in 2013 – an increase of 43 percent compared to the previous year.
On May 2011, an integrated Areal Traffic Control System (ATCS) was implemented to reduce traffic jams at four crossing points: Ngurah Rai statue, Dewa Ruci Kuta crossing, Jimbaran crossing and Sanur crossing. ATCS is an integrated system connecting all traffic lights, CCTVs and other traffic signals with a monitoring office at the police headquarters. It has successfully been implemented in other ASEAN countries and will be implemented at other crossings in Bali.
On 21 December 2011 construction started on the Nusa Dua-Benoa-Ngurah Rai International Airport toll road which will also provide a special lane for motorcycles. This has been done by seven state-owned enterprises led by PT Jasa Marga with 60% of shares. PT Jasa Marga Bali Tol will construct the 9.91 kilometres toll road (totally 12.7 kilometres with access road). The construction is estimated to cost Rp.2.49 trillion ($273.9 million). The project goes through 2 kilometres of mangrove forest and through 2.3 kilometres of beach, both within 5.4 hectares area. The elevated toll road is built over the mangrove forest on 18,000 concrete pillars which occupied 2 hectares of mangroves forest. It compensated by new planting of 300,000 mangrove trees along the road. On 21 December 2011 the Dewa Ruci 450 meters underpass has also started on the busy Dewa Ruci junction near Bali Kuta Galeria with an estimated cost of Rp136 billion ($14.9 million) from the state budget. On 23 September 2013, the Bali Mandara Toll Road is opened and the Dewa Ruci Junction (Simpang Siur) underpass is opened before. Both are ease the heavy traffic congestion.
To solve chronic traffic problems, the province will also build a toll road connecting Serangan with Tohpati, a toll road connecting Kuta, Denpasar and Tohpati and a flyover connecting Kuta and Ngurah Rai Airport.
Demographics.
The population of Bali was 3,890,757 as of the 2010 Census; the latest estimate (for January 2014) is 4,225,384. There are an estimated 30,000 expatriates living in Bali.
Ethnic origins.
A DNA study in 2005 by Karafet et al. found that 12% of Balinese Y-chromosomes are of likely Indian origin, while 84% are of likely Austronesian origin, and 2% of likely Melanesian origin. The study does not correlate the DNA samples to the Balinese caste system.
Caste system.
Bali has a caste system based on the Indian Hindu model, with four castes:
Religion.
Unlike most of Muslim-majority Indonesia, about 83.5% of Bali's population adheres to Balinese Hinduism, formed as a combination of existing local beliefs and Hindu influences from mainland Southeast Asia and South Asia. Minority religions include Islam (13.3%), Christianity (1.7%), and Buddhism (0.5%). These figures do not include immigrants from other parts of Indonesia.
Balinese Hinduism is an amalgam in which gods and demigods are worshipped together with Buddhist heroes, the spirits of ancestors, indigenous agricultural deities and sacred places. Religion as it is practised in Bali is a composite belief system that embraces not only theology, philosophy, and mythology, but ancestor worship, animism and magic. It pervades nearly every aspect of traditional life. Caste is observed, though less strictly than in India. With an estimated 20,000 puras (temples) and shrines, Bali is known as the "Island of a Thousand Puras", or "Island of the Gods".
Balinese Hinduism has roots in Indian Hinduism and Buddhism, and adopted the animistic traditions of the indigenous people. This influence strengthened the belief that the gods and goddesses are present in all things. Every element of nature, therefore, possesses its own power, which reflects the power of the gods. A rock, tree, dagger, or woven cloth is a potential home for spirits whose energy can be directed for good or evil. Balinese Hinduism is deeply interwoven with art and ritual. Ritualizing states of self-control are a notable feature of religious expression among the people, who for this reason have become famous for their graceful and decorous behaviour.
Apart from the majority of Balinese Hindus, there also exist Chinese immigrants whose traditions have melded with that of the locals. As a result, these Sino-Balinese not only embrace their original religion, which is a mixture of Buddhism, Christianity, Taoism and Confucianism, but also find a way to harmonise it with the local traditions. Hence, it is not uncommon to find local Sino-Balinese during the local temple's "odalan". Moreover, Balinese Hindu priests are invited to perform rites alongside a Chinese priest in the event of the death of a Sino-Balinese. Nevertheless, the Sino-Balinese claim to embrace Buddhism for administrative purposes, such as their Identity Cards.
Language.
Balinese and Indonesian are the most widely spoken languages in Bali, and the vast majority of Balinese people are bilingual or trilingual. The most common spoken language around the tourist areas is Indonesian, as many people in the tourist sector are not solely Balinese, but migrants from Java, Lombok, Sumatra, and other parts of Indonesia. There are several indigenous Balinese languages, but most Balinese can also use the most widely spoken option: modern common Balinese. The usage of different Balinese languages was traditionally determined by the Balinese caste system and by clan membership, but this tradition is diminishing. Kawi and Sanskrit are also commonly used by some Hindu priests in Bali, for Hinduism literature was mostly written in Sanskrit.
English is a common third language (and the primary foreign language) of many Balinese, owing to the requirements of the tourism industry. Other foreign languages, such as Chinese, Japanese, Korean, French or German are often used in multilingual signs for foreign tourists.
Culture.
Bali is renowned for its diverse and sophisticated art forms, such as painting, sculpture, woodcarving, handcrafts, and performing arts. Balinese cuisine is also distinctive. Balinese percussion orchestra music, known as "gamelan", is highly developed and varied. Balinese performing arts often portray stories from Hindu epics such as the Ramayana but with heavy Balinese influence. Famous Balinese dances include "pendet", "legong", "baris", "topeng", "barong", "gong keybar", and "kecak" (the monkey dance). Bali boasts one of the most diverse and innovative performing arts cultures in the world, with paid performances at thousands of temple festivals, private ceremonies, or public shows.
The Hindu New Year, "Nyepi", is celebrated in the spring by a day of silence. On this day everyone stays at home and tourists are encouraged to remain in their hotels. On the day before New Year, large and colourful sculptures of "ogoh-ogoh" monsters are paraded and finally burned in the evening to drive away evil spirits. Other festivals throughout the year are specified by the Balinese "pawukon" calendrical system.
Celebrations are held for many occasions such as a tooth-filing (coming-of-age ritual), cremation or "odalan" (temple festival). One of the most important concepts that Balinese ceremonies have in common is that of "désa kala patra", which refers to how ritual performances must be appropriate in both the specific and general social context. Many of the ceremonial art forms such as "wayang kulit" and "topeng" are highly improvisatory, providing flexibility for the performer to adapt the performance to the current situation. Many celebrations call for a loud, boisterous atmosphere with lots of activity and the resulting aesthetic, "ramé", is distinctively Balinese. Often two or more "gamelan" ensembles will be performing well within earshot, and sometimes compete with each other to be heard. Likewise, the audience members talk amongst themselves, get up and walk around, or even cheer on the performance, which adds to the many layers of activity and the liveliness typical of "ramé".
"Kaja" and "kelod" are the Balinese equivalents of North and South, which refer to ones orientation between the island's largest mountain Gunung Agung ("kaja"), and the sea ("kelod"). In addition to spatial orientation, "kaja" and "kelod" have the connotation of good and evil; gods and ancestors are believed to live on the mountain whereas demons live in the sea. Buildings such as temples and residential homes are spatially oriented by having the most sacred spaces closest to the mountain and the unclean places nearest to the sea.
Most temples have an inner courtyard and an outer courtyard which are arranged with the inner courtyard furthest "kaja". These spaces serve as performance venues since most Balinese rituals are accompanied by any combination of music, dance and drama. The performances that take place in the inner courtyard are classified as "wali", the most sacred rituals which are offerings exclusively for the gods, while the outer courtyard is where "bebali" ceremonies are held, which are intended for gods and people. Lastly, performances meant solely for the entertainment of humans take place outside the walls of the temple and are called "bali-balihan". This three-tiered system of classification was standardised in 1971 by a committee of Balinese officials and artists to better protect the sanctity of the oldest and most sacred Balinese rituals from being performed for a paying audience.
Tourism, Bali's chief industry, has provided the island with a foreign audience that is eager to pay for entertainment, thus creating new performance opportunities and more demand for performers. The impact of tourism is controversial since before it became integrated into the economy, the Balinese performing arts did not exist as a capitalist venture, and were not performed for entertainment outside of their respective ritual context. Since the 1930s sacred rituals such as the "barong" dance have been performed both in their original contexts, as well as exclusively for paying tourists. This has led to new versions of many of these performances which have developed according to the preferences of foreign audiences; some villages have a "barong" mask specifically for non-ritual performances as well as an older mask which is only used for sacred performances.
Balinese society continues to revolve around each family's ancestral village, to which the cycle of life and religion is closely tied. Coercive aspects of traditional society, such as customary law sanctions imposed by traditional authorities such as village councils (including "kasepekang", or shunning) have risen in importance as a consequence of the democratisation and decentralisation of Indonesia since 1998.
Sports.
As part of the Coral Triangle Bali, including Nusa Penida, offers a wide range of dive sites with varying types of reefs.
Bali was the host of 2008 Asian Beach Games. It was the second time Indonesia hosted an Asia-level multi-sport event, after Jakarta held the 1962 Asian Games.
Heritage sites.
On June 2012, Subak, the irrigation system for paddy fields in Bali was enlisted as a UNESCO world heritage site.
Beauty Pageant.
Bali was the host of Miss World 2013, It was the first time Indonesia hosted an International Beauty Pageant

</doc>
<doc id="4149" url="http://en.wikipedia.org/wiki?curid=4149" title="Bulgarian language">
Bulgarian language

Bulgarian (български език, ) is an Indo-European language, a member of the Southern branch of the Slavic language family.
Bulgarian, along with the closely related Macedonian language (collectively forming the East South Slavic languages), has several characteristics that set it apart from all other Slavic languages: changes include the elimination of case declension, the development of a suffixed definite article (see Balkan language area), and the lack of a verb infinitive, but it retains and has further developed the Proto-Slavic verb system. Various evidential verb forms exist to express unwitnessed, retold, and doubtful action.
Based on the 2011 census, "Ethnologue" estimates that Bulgarian is spoken as a native language by 6.8 million. In 1999, the "World Almanac" had estimated 9 million.
With the accession of Bulgaria to the European Union on 1 January 2007, Bulgarian language became one of the official languages of the European Union. 
History.
The development of the Bulgarian language may be divided into several periods.
"Bulgarian" was the first "Slavic" language attested in writing. As Slavic linguistic unity lasted into late antiquity, in the oldest manuscripts this language was initially referred to as языкъ словяньскъ, "the Slavic language". In the Middle Bulgarian period this name was gradually replaced by the name , the "Bulgarian language". In some cases, the name was used not only with regard to the contemporary Middle Bulgarian language of the copyist but also to the period of Old Bulgarian. A most notable example of anachronism is the Service of St. Cyril from Skopje (Скопски миней), a 13th-century Middle Bulgarian manuscript from northern Macedonia according to which St. Cyril preached with "Bulgarian" books among the Moravian Slavs. The first mention of the language as the "Bulgarian language" instead of the "Slavonic language" comes in the work of the Greek clergy of the Bulgarian Archbishopric of Ohrid in the 11th century, for example in the Greek hagiography of Saint Clement of Ohrid by Theophylact of Ohrid (late 11th century).
During the Middle Bulgarian period, the language underwent dramatic changes, losing the Slavonic case system, but preserving the rich verb system (while the development was exactly the opposite in other Slavic languages) and developing a definite article. It was influenced by its non-Slavic neighbors in the Balkan language area (mostly grammatically) and later also by Turkish, which was the official language of the Ottoman Empire, in the form of the Ottoman Turkish language, mostly lexically. As a national revival occurred toward the end of the period of Ottoman rule (mostly during the 19th century), a modern Bulgarian literary language gradually emerged that drew heavily on Church Slavonic/Old Bulgarian (and to some extent on literary Russian, which had preserved many lexical items from Church Slavonic) and later reduced the number of Turkish and other Balkan loans. Today one difference between Bulgarian dialects in the country and literary spoken Bulgarian is the significant presence of Old Bulgarian words and even word forms in the latter. Russian loans are distinguished from Old Bulgarian ones on the basis of the presence of specifically Russian phonetic changes, as in оборот (turnover, rev), непонятен (incomprehensible), ядро (nucleus) and others. As usual in such cases, many other loans from French, English and the classical languages have subsequently entered the language as well.
Modern Bulgarian was based essentially on the Eastern dialects of the language, but its pronunciation is in many respects a compromise between East and West Bulgarian (see especially the phonetic sections below). Following the efforts of some figures of the National awakening of Bulgaria (the most notable among them being Neofit Rilski and Ivan Bogorov), there had been many attempts to codify a standard Bulgarian language; however, there was much argument surrounding the choice of norms. Between 1835–1878 more than 25 proposals were put forward and "linguistic chaos" ensued. Eventually the eastern dialects prevailed, and in 1899 the Ministry of Education officially codified a standard Bulgarian language based on the Drinov-Ivanchev orthography.
Dialects.
The language is mainly split into two broad dialect areas, based on the different reflexes of the Common Slavic yat vowel (). This split, which occurred at some point during the Middle Ages, led to the development of Bulgaria's:
The literary language norm, which is generally based on the Eastern dialects, also has the Eastern alternating reflex of "yat". However, it has not incorporated the general Eastern umlaut of "all" synchronic or even historic "ya" sounds into "e" before front vowels – e.g. поляна ("polyana") vs полени ("poleni") "meadow – meadows" or even жаба ("zhaba") vs жеби ("zhebi") "frog – frogs", even though it co-occurs with the yat alternation in almost all Eastern dialects that have it (except a few dialects along the yat border, e.g. in the Pleven region).
More examples of the "yat" umlaut in the literary language are:
Until 1945, Bulgarian orthography did not reveal this alternation and used the original Old Slavic Cyrillic letter "yat" (), which was commonly called двойно е ("dvoyno e") at the time, to express the historical "yat" vowel or at least root vowels displaying the "ya – e" alternation. The letter was used in each occurrence of such a root, regardless of the actual pronunciation of the vowel: thus, both "mlyako" and "mlekar" were spelled with (). Among other things, this was seen as a way to "reconcile" the Western and the Eastern dialects and maintain language unity at a time when much of Bulgaria's Western dialect area was controlled by Serbia and Greece, but there were still hopes and occasional attempts to recover it. With the 1945 orthographic reform, this letter was abolished and the present spelling was introduced, reflecting the alternation in pronunciation.
This had implications for some grammatical constructions:
Sometimes, with the changes, words began to be spelled as other words with different meanings, e.g.:
In spite of the literary norm regarding the yat vowel, many people living in Western Bulgaria, including the capital Sofia, will fail to observe its rules. While the norm requires the realizations "vidyal" vs "videli" (he has seen; they have seen), some natives of Western Bulgaria will preserve their local dialect pronunciation with "e" for all instances of "yat" (e.g. "videl", "videli"). Others, attempting to adhere to the norm, will actually use the "ya" sound even in cases where the standard language has "e" (e.g. "vidyal", "vidyali"). The latter hypercorrection is called свръхякане ("svrah-yakane" ≈"over-softening").
Bulgarian is the only Slavic language whose literary standard does not naturally contain the iotated sound (or its palatalized variant , except in non-Slavic foreign-loaned words). The sound is common in all modern Slavic languages (e.g. Czech "medvěd" "bear", Polish "pięć" "five", Serbo-Croatian jelen" "deer", Ukrainian "немає "there is not...", Macedonian "пишување" "writing", etc.), as well as some Western Bulgarian dialectal forms – e.g. "орàн’е" (standard Bulgarian: "орaне" , "ploughing"), however it is not represented in standard Bulgarian speech or writing. Even where occurs in other Slavic words, in Standard Bulgarian it is usually transcribed and pronounced as pure – e.g. Boris Yeltsin is "Eltsin" (), Yekaterinburg is "Ekaterinburg" () and Sarajevo is "Saraevo" (), although Jelena Janković is "Yelena" – .
Relationship to Macedonian.
Until the period immediately following the Second World War, all Bulgarian and the majority of foreign linguists referred to the South Slavic dialect continuum spanning the area of modern Bulgaria, the Republic of Macedonia and parts of Northern Greece as a group of Bulgarian dialects. In contrast, Serbian sources tended to label them "south Serbian" dialects. Some local naming conventions included "bolgarski", "bugarski" and so forth. The codifiers of the standard Bulgarian language, however, did not wish to make any allowances for a pluricentric "Bulgaro-Macedonian" compromise. After 1944 the People's Republic of Bulgaria and the Socialist Federal Republic of Yugoslavia began a policy of making Macedonia into the connecting link for the establishment of new Balkan Federative Republic and stimulating here a development of distinct Slav Macedonian consciousness. With the proclamation of the Socialist Republic of Macedonia as part of the Yugoslav federation, the new authorities also started measures that would overcome the pro-Bulgarian feeling among parts of its population and in 1945 a separate Macedonian language was codified. After 1958, when the pressure from Moscow decreased, Sofia reverted to the view that the Macedonian language did not exist as a separate language. Nowadays, some linguists still consider Macedonian dialects as Bulgarian. The current academic consensus (outside of Bulgaria) is that Macedonian is an autonomous language within the South Slavic dialect continuum.
Alphabet.
In 886 AD, the Bulgarian Empire introduced the Glagolitic alphabet which was devised by the Saints Cyril and Methodius in the 850s. The Glagolitic alphabet was gradually superseded in later centuries by the Cyrillic script, developed around the Preslav Literary School, Bulgaria in the beginning of the 10th century.
Several Cyrillic alphabets with 28 to 44 letters were used in the beginning and the middle of the 19th century during the efforts on the codification of Modern Bulgarian until an alphabet with 32 letters, proposed by Marin Drinov, gained prominence in the 1870s. The alphabet of Marin Drinov was used until the orthographic reform of 1945 when the letters , called "ят" 'yat' or "двойно е" (or yet "е-двойно") 'double e', and , called "Голям юс" 'big yus', "голяма носовка" 'big nasal sign', "ъ кръстато" 'crossed yer' or "широко ъ" 'long yer', were removed from the alphabet, reducing the number of letters to 30.
With the accession of Bulgaria to the European Union on 1 January 2007, Cyrillic became the third official script of the European Union, following the Latin and Greek scripts.
The following table gives the letters of the Bulgarian alphabet, along with the values for the sound of each letter:
Most letters in the Bulgarian alphabet stand for just one specific sound. Three letters stand for the single expression of combinations of sounds, namely щ (sht), ю (yu), and я (ya). Two sounds do not correspond to separate letters, but are expressed as the combination of two letters, namely дж () and дз (). The letter ь marks the softening (palatalization) of any consonant (besides ж, ч, and ш) before the letter о, while ю and я after consonants mark the palatalization of the preceding consonant in addition to representing the vowels /u/ and /a/.
A letter that represents a voiced consonant can represent its voiceless counterpart and vice versa when adjacent to a voiceless or voiced consonant, respectively, or when a voiced consonant is syllable final, for example – вторник /ftornik/ – Tuesday, нож /nɔʃ/ – knife, сграда /zgradɐ/ – building, сватба /svadbɐ/ – wedding.
The names of most letters are simple representations of their phonetic values, with consonants being followed by – thus the alphabet goes: – – , etc. However, the name of the letter Й is "и-kratko" ("short /i/"), the name of Ъ is "er-golyam" ("large Er"), and the name of Ь is "er-malak" ("small Er"). People often refer to Ъ simply as .
Ѝ.
The accented letter "Ѝ" is used to distinguish the conjunction 'и' (and) from the pronoun 'ѝ' (her). It is not considered a separate letter but rather a special form of "И".
Writing.
Bulgarian is usually described as having a phonemic orthography, meaning that words are spelt the way they are pronounced. This is largely true, but does have exceptions. Three of the most cited examples are:
Modern developments.
Since the time of Bulgaria's liberation in the late 19th century, the Bulgarian language has taken on a large number of words from Western European languages. All of these are transcribed phonetically into Cyrillic, e.g.:
Notable is the transliteration of many English names through German, e.g.:
In the years since the end of communism and the rise of technology, the tendency for borrowing has shifted mainly to English, where much computer-related terminology has entered and been inflected accordingly – again, in a wholly phonetic way. Examples include:
The computer-related neologisms are often used interchangeably with traditional Bulgarian words, e.g. "download" and "upload" can be simply свалям and качвам ("svalyam" & "kachvam" – "to bring down" & "to put up").
Use of Roman script in Bulgarian.
The insertion of English words directly into a Cyrillic Bulgarian sentence, while frowned upon, has been increasingly used in the media. This is done for several reasons, including –
Phonology.
Bulgarian possesses a phonology similar to that of the rest of the South Slavic languages, notably lacking Serbo-Croatian's phonemic vowel length and tones and alveo-palatal affricates. Macedonian on the other side exhibits a phonology very similar to that of Bulgarian, which has spurred controversial debates regarding its status as a separate language. An interesting geographic pattern of dialectal distribution shows a tendency of western dialects to approach Serbo-Croatian's "hard" sound in contrast to the eastern dialect's "soft" sound due to pre-palatalization and rising of (similar to Russian) and ikanye (a merger of the two front vowels and ).
Bulgarian is typically analyzed as having six vowels, but at least two more reduced vowels can be encountered in everyday speech.
Grammar.
The parts of speech in Bulgarian are divided in 10 different types, which are categorized in two broad classes: mutable and immutable. The difference is that mutable parts of speech vary grammatically, whereas the immutable ones do not change, regardless of their use. The five classes of mutables are: "nouns", "adjectives", "numerals", "pronouns" and "verbs". Syntactically, the first four of these form the group of the noun or the nominal group. The immutables are: "adverbs", "prepositions", "conjunctions", "particles" and "interjections". Verbs and adverbs form the group of the verb or the verbal group.
Nominal morphology.
Nouns and adjectives have the categories grammatical gender, number, case (only vocative) and definiteness in Bulgarian. Adjectives and adjectival pronouns agree with nouns in number and gender. Pronouns have gender and number and retain (as in nearly all Indo-European languages) a more significant part of the case system.
Nominal inflection.
Gender.
There are three grammatical genders in Bulgarian: "masculine", "feminine" and "neuter". The gender of the noun can largely be inferred from its ending: nouns ending in a consonant ("zero ending") are generally masculine (for example, град 'city', син 'son', мъж 'man'; those ending in –а/–я (-a/-ya) (жена 'woman', дъщеря 'daughter', улица 'street') are normally feminine; and nouns ending in –е, –о are almost always neuter (дете 'child', езеро 'lake'), as are those rare words (usually loanwords) that end in –и, –у, and –ю (цунами 'tsunami', табу 'taboo', меню 'menu'). Perhaps the most significant exception from the above are the relatively numerous nouns that end in a consonant and yet are feminine: these comprise, firstly, a large group of nouns with zero ending expressing quality, degree or an abstraction, including all nouns ending on –ост/–ест -{ost/est} (мъдрост 'wisdom', низост 'vileness', прелест 'loveliness', болест 'sickness', любов 'love'), and secondly, a much smaller group of irregular nouns with zero ending which define tangible objects or concepts (кръв 'blood', кост 'bone', вечер 'evening', нoщ 'night'). There are also some commonly used words that end in a vowel and yet are masculine: баща 'father', дядо 'grandfather', чичо / вуйчо 'uncle', and others.
The plural forms of the nouns do not express their gender as clearly as the singular ones, but may also provide some clues to it: the ending –и (-i) is more likely to be used with a masculine or feminine noun (факти 'facts', болести 'sicknesses'), while one in –а/–я belongs more often to a neuter noun (езера 'lakes'). Also, the plural ending –ове occurs only in masculine nouns.
Number.
Two numbers are distinguished in Bulgarian – singular and plural. A variety of plural suffixes is used, and the choice between them is partly determined by their ending in singular and partly influenced by gender; in addition, irregular declension and alternative plural forms are common. Words ending in –а/–я (which are usually feminine) generally have the plural ending –и, upon dropping of the singular ending. Of nouns ending in a consonant, the feminine ones also use –и, whereas the masculine ones usually have –и for polysyllables and –ове for monosyllables (however, exceptions are especially common in this group). Nouns ending in –о/–е (most of which are neuter) mostly use the suffixes –а, –я (both of which require the dropping of the singular endings) and –та.
With cardinal numbers and related words such as няколко ('several'), masculine nouns use a special count form in –а/–я, which stems from the Proto-Slavonic dual: два/три стола ('two/three chairs') versus тези столове ('these chairs'); cf. feminine две/три/тези книги ('two/three/these books') and neuter две/три/тези легла ('two/three/these beds'). However, a recently developed language norm requires that count forms should only be used with masculine nouns that do not denote persons. Thus, двама/трима ученици ('two/three students') is perceived as more correct than двама/трима ученика, while the distinction is retained in cases such as два/три молива ('two/three pencils') versus тези моливи ('these pencils').
Case.
Cases exist only in the personal pronouns (as they do in many other modern Indo-European languages), with nominative, accusative, dative and vocative forms. Vestiges are present in the masculine personal interrogative pronoun кой ("who" as in formal English, "whom")) and in a number of phraseological units and sayings. The major exception are vocative forms, which are still in use for masculine (with the endings -e, -o and -ю) and feminine nouns (-[ь/й]o and -e) in the singular. However, there is a tendency to avoid them in many personal names, as the use of feminine name forms in -[ь/й]o and of the potential vocative forms of foreign names has come to be considered rude or rustic. Thus, Иване means 'hey, Ivan', while the corresponding feminine forms Елено ('hey, Elena'),
Маргарито ('hey, Margarita') are today seen as rude or, at best, unceremonious, and declining foreign names as in *Джоне ('hey, John') or *Саймъне ('hey, Simon') could only be considered humorous. Interestingly, the prohibition on constructing vocative forms for foreign names does not apply to names from Classical Antiquity, with the source languages having the vocative case as well: cf Цезаре' ('Oh Caesar'), Перикле ('Oh Pericles'), Зевсе ('Oh Zeus'), etc.
Case remnants<br>
Some key words do retain their cases, which today are no longer considered nominative, accusative and dative, but rather as being subject, direct object and indirect object parts of speech:
Definiteness (article).
In modern Bulgarian, definiteness is expressed by a definite article which is postfixed to the noun, much like in the Scandinavian languages or Romanian (indefinite: човек, 'person'; definite: човекът, ""the" person") or to the first nominal constituent of definite noun phrases (indefinite: добър човек, 'a good person'; definite: добрият човек, ""the" good person"). There are four singular definite articles. Again, the choice between them is largely determined by the noun's ending in the singular. Nouns that end in a consonant and are masculine use –ът/–ят, when they are grammatical subjects, and –а/–я elsewhere. Nouns that end in a consonant and are feminine, as well as nouns that end in –а/–я (most of which are feminine, too) use –та. Nouns that end in –е/–о use –то.
The plural definite article is –те for all nouns except for those, whose plural form ends in –а/–я; these get –тa instead. When postfixed to adjectives the definite articles are –ят/–я for masculine gender (again, with the longer form being reserved for grammatical subjects), –та for feminine gender, –то for neuter gender, and –те for plural.
In Bulgarian adjective-noun phrases, only the adjective takes a definite article ending –
Many of the English loanwords which have been adopted into the language since the end of communism, however, do not readily lend themselves to taking adjectival endings. This has caused an unprecedented shift in the language whereby, in certain cases, the adjective remains uninflected while the noun following it takes the grammatical ending. Examples include –
This type of combination is sometimes favoured even when the possibility of a traditional phrase structure exists, e.g. –
In this case, the brand name "btv" cannot be inflected and, being a brand, remains in Roman script within the sentence.
Adjective and numeral inflection.
Both groups agree in gender and number with the noun they are appended to. They may also take the definite article as explained above.
Pronouns.
Pronouns may vary in gender, number, definiteness and are the only parts of speech that have retained case inflections. Three cases are exhibited by some groups of pronouns – nominative, accusative and dative. The distinguishable types of pronouns include the following: personal, relative, reflexive, interrogative, negative, indefinitive, summative and possessive.
Verbal morphology and grammar.
The Bulgarian verb can take up to 3,000 distinct forms, as it varies in person, number, voice, aspect, mood, tense and even gender.
Finite verbal forms.
Finite verbal forms are "simple" or "compound" and agree with subjects in person (first, second and third) and number (singular, plural) in Bulgarian. In addition to that, past compound forms using participles vary in gender (masculine, feminine, neuter) and voice (active and passive) as well as aspect (perfective/aorist and imperfective).
Aspect.
Bulgarian verbs express lexical aspect: perfective verbs signify the completion of the action of the verb and form past perfective (aorist) forms; imperfective ones are neutral with regard to it and form past imperfective forms. Most Bulgarian verbs can be grouped in perfective-imperfective pairs (imperfective/perfective: идвам/дойда "come", пристигам/пристигна “arrive”). Perfective verbs can be usually formed from imperfective ones by suffixation or prefixation, but the resultant verb often deviates in meaning from the original. In the pair examples above, aspect is stem-specific and therefore there is no difference in meaning.
In Bulgarian, there is also grammatical aspect. Three grammatical aspects are distinguishable: neutral, perfect and pluperfect. The neutral aspect comprises the three simple tenses and the future tense. The pluperfect is manifest in tenses that use double or triple auxiliary "be" participles like the past pluperfect subjunctive. Perfect constructions use a single auxiliary "be".
Mood.
The traditional interpretation is that in addition to the four moods (наклонения ) shared by most other European languages – indicative (изявително, ) imperative (повелително ), subjunctive (подчинително ) and conditional (условно, ) – in Bulgarian there is one more to describe a general category of unwitnessed events – the inferential (преизказно ) mood.
Tense.
There are three grammatically distinctive positions in time – present, past and future – which combine with aspect and mood to produce a number of formations. Normally, in grammar books these formations are viewed as separate tenses – i. e. "past imperfect" would mean that the verb is in past tense, in the imperfective aspect, and in the indicative mood (since no other mood is shown). There are more than 40 different tenses across Bulgarian's two aspects and five moods.
In the indicative mood, there are three simple tenses:
In the indicative there are also the following compound tenses:
The four perfect constructions above can vary in aspect depending on the aspect of the main-verb participle; they are in fact pairs of imperfective and perfective aspects. Verbs in forms using past participles also vary in voice and gender.
There is only one simple tense in the imperative mood – the present – and there are simple forms only for the second person using the suffixes -и/-й (-i, -y/i) for singular and -ете/-йте (-ete, -yte) for plural; e.g., уча ('to study'): учи , sg., учете , pl.; играя 'to play': играй , играйте . There are compound imperative forms for all persons and numbers in the present compound imperative (да играе, ), the present perfect compound imperative (да е играл, ) and the rarely used present pluperfect compound imperative (да е бил играл, ).
The conditional mood consists of five compound tenses, most of which are not grammatically distinguishable. The present, future and past conditional use a special past form of the stem би- (bi – "be") and the past participle (бих учил, , 'I would study'). The past future conditional and the past future perfect conditional coincide in form with the respective indicative tenses.
The subjunctive mood is rarely documented as a separate verb form in Bulgarian, (being, morphologically, a sub-instance of the quasi-infinitive construction with the particle да and a normal finite verb form), but nevertheless it is used regularly. The most common form, often mistaken for the present tense, is the present subjunctive ([пo-добре] да отидa , 'I had better go'). The difference between the present indicative and the present subjunctive tense is that the subjunctive can be formed by "both" perfective and imperfective verbs. It has completely replaced the infinitive and the supine from complex expressions (see below). It is also employed to express opinion about "possible" future events. The past perfect subjunctive ([пo-добре] да бях отишъл , 'I'd had better be gone') refers to "possible" events in the past, which "did not" take place, and the present pluperfect subjunctive (да съм бил отишъл ), which may be used about both past and future events arousing feelings of incontinence, suspicion, etc. and has no perfect to English translation.
The inferential mood has five pure tenses. Two of them are simple – "past aorist inferential" and "past imperfect inferential" – and are formed by the past participles of perfective and imperfective verbs, respectively. There are also three compound tenses – "past future inferential", "past future perfect inferential" and "past perfect inferential". All these tenses' forms are gender-specific in the singular. There are also conditional and compound-imperative crossovers. The existence of inferential forms has been attributed to Turkic influences by most Bulgarian linguists. Morphologically, they are derived from the perfect.
Non-finite verbal forms.
Bulgarian has the following participles:
The participles are inflected by gender, number, and definiteness, and are coordinated with the subject when forming compound tenses (see tenses above). When used in attributive role the inflection attributes are coordinated with the noun that is being attributed.
Reflexive verbs.
Bulgarian uses reflexive verbal forms (i.e. actions which are performed by the agent onto him- or herself) which behave in a similar way as they do in many other Indo-European languages, such as French and Spanish. It uses the invariable particle se in order to indicate all actions performed onto oneself, both in the singular and plural. Thus –
When the action is performed onto others, other particles are used, just like in any normal verb, e.g. –
Sometimes, the reflexive verb form has a similar but not necessarily identical meaning to the non-reflexive verb –
In other cases, the reflexive verb has a completely different meaning from its non-reflexive counterpart –
When the action is performed on an indirect object, the particles change to si and its derivatives –
In some cases, the particle "si" has the double meaning of a possessive –
The difference between transitive and intransitive verbs can lead to significant differences in meaning with minimal change, e.g. –
The particle "si" is often used to indicate a more personal relationship to the action, e.g. –
Adverbs.
The most productive way to form adverbs is to derive them from the neuter singular form of the corresponding adjective—e.g. бързо (fast), силно (hard), странно (strange)—but adjectives ending in -ки use the masculine singular form (i.e. ending in -ки), instead—e.g. юнашки (heroically), мъжки (bravely, like a man), майсторски (skillfully). The same pattern is used to form adverbs from the (adjective-like) ordinal numerals, e.g. първо (firstly), второ (secondly), трето (thirdly), and in some cases from (adjective-like) cardinal numerals, e.g. двойно (twice as/double), тройно (three times as), петорно (five times as).
The remaining adverbs are formed in ways that are no longer productive in the language. A small number are original (not derived from other words), for example: тук (here), там (there), вътре (inside), вън (outside), много (very/much) etc. The rest are mostly fossilized case forms, such as:
Adverbs can sometimes be reduplicated to emphasize the qualitative or quantitative properties of actions, moods or relations as performed by the subject of the sentence: "бавно-бавно" ("rather slowly"), "едва-едва" ("with great difficulty"), "съвсем-съвсем" ("quite", "thoroughly").
Syntax.
Bulgarian employs clitic doubling, mostly for emphatic purposes. For example, the following constructions are common in colloquial Bulgarian:
The phenomenon is practically obligatory in the spoken language in the case of inversion signalling information structure (in writing, clitic doubling may be skipped in such instances, with a somewhat bookish effect):
Sometimes, the doubling signals syntactic relations, thus:
This is contrasted with:
In this case, clitic doubling can be a colloquial alternative of the more formal or bookish passive voice, which would be constructed as follows:
Clitic doubling is also fully obligatory, both in the spoken and in the written norm, in clauses including several special expressions that use the short accusative and dative pronouns such as играе ми се (I feel like playing), студено ми е (I am cold), and боли ме ръката (my arm hurts):
Except the above examples, clitic doubling is considered inappropriate in a formal context. Bulgarian grammars usually do not treat this phenomenon extensively.
Other features.
Questions.
Questions in Bulgarian which do not use a question word (such as who? what? etc.) are formed with the particle ли after the verb; a subject is not necessary, as the verbal conjugation suggests who is performing the action:
While the particle "ли" generally goes after the verb, it can go after a noun or adjective if a contrast is needed:
A verb is not always necessary, e.g. when presenting a choice:
Rhetorical questions can be formed by adding ли to a question word, thus forming a "double interrogative" –
The same construction +не ('no') is an emphasised positive –
Significant verbs.
Съм
The verb съм – 'to be' is also used as an auxiliary for forming the perfect, the passive and the conditional:
Two alternate forms of съм exist:
Ще
The impersonal verb щe (lit. 'it wants') is used to for forming the (positive) future tense:
The negative future is formed with the invariable construction няма да (see няма below):
The past tense of this verb – щях is conjugated to form the past conditional ('would have' – again, with да, since it is "irrealis"):
Имам and нямам
The verbs имам ('to have') and нямам ('to not have'):
Diminutives and augmentatives.
Diminutive
Usually done by adding -че, -це or -(ч)ка. The gender of the word is thus changed, usually to the neuter:
Affectionate Form
Sometimes proper nouns and words referring to friends or family members can have a diminutive ending added to show affection. These constructions are all referred to as "na galeno" (lit. "caressing" form):
Such words can be used "both" from parent to child, and vice-versa, as can:
Personal names are shortened and made neuter:
There is an interesting trend (which is comparatively modern, although it might well have deeper, dormant roots) where the feminine ending "-ka" and the definite article suffix "-ta" ("the") are added to male names – note that this is affectionate and not at all insulting (in fact, the endings are not even really considered as being "feminine"):
The female equivalent would be to add the neuter ending "-to" to the diminutive form:
Augmentative
This is to present words to sound larger – usually by adding "-shte":
Some words only exist in an augmentative form – e.g.
Conjunctions and particles.
"But"
In Bulgarian, there are several conjunctions all translating into English as "but", which are all used in distinct situations. They are но (no), ама (amà), а (a), ами (amì)", and ала (alà)" (and "обаче (obache)" – "however", identical in use to "но").
While there is some overlapping between their uses, in many cases they are specific. For example, ami is used for a choice – "ne tova, ami onova" – "not this one, but that one" (comp. Spanish "sino"), while ama is often used to provide extra information or an opinion – "kazah go, ama sgreshih" – "I said it, but I was wrong". Meanwhile, a provides contrast between two situations, and in some sentences can even be translated as "although", "while" or even "and" – "az rabotya, a toy blee" – "I'm working, and he's daydreaming".
Very often, different words can be used to alter the emphasis of a sentence – e.g. while "pusha, no ne tryabva" and "pusha, a ne tryabva" both mean "I smoke, but I shouldn't", the first sounds more like a statement of fact ("...but I mustn't"), while the second feels more like a "judgement" ("...but I oughtn't"). Similarly, "az ne iskam, ama toy iska" and "az ne iskam, a toy iska" both mean "I don't want to, but he does", however the first emphasises the fact that "he" wants to, while the second emphasises the "wanting" rather than the person.
"Ala" is interesting in that, while it feels archaic, it is often used in poetry and frequently in children's stories, since it has quite a moral/ominous feel to it.
Some common expressions use these words, and some can be used alone as interjections:
 Vocative particles 
Bulgarian has several abstract particles which are used to strengthen a statement. These have no precise translation in English. The particles are strictly informal and can even be considered rude by some people and in some situations. They are mostly used at the end of questions or instructions.
Modal Particles
These are "tagged" on to the beginning or end of a sentence to express the mood of the speaker in relation to the situation. They are mostly interrogative or slightly imperative in nature. There is no change in the grammatical mood when these are used (although they may be expressed through different grammatical moods in other languages).
 Intentional particles 
These express intent or desire, perhaps even pleading. They can be seen as a sort of cohortative side to the language. (Since they can be used by themselves, they could even be considered as verbs in their own right.) They are also highly informal.
These particles can be combined with the vocative particles for greater effect, e.g. "ya da vidya, be" (let me see), or even exclusively in combinations with them, with no other elements, e.g. "haide, de!" (come on!); "nedey, de!" (I told you not to!).
Pronouns of Quality.
Bulgarian has several pronouns of quality which have no direct parallels in English – "kakuv" (what sort of); "takuv" (this sort of); "onakuv" (that sort of – colloq.); "nyakakuv" (some sort of); "nikakuv" (no sort of); "vsyakakuv" (every sort of); and the relative pronoun "kakuvto" (the sort of...that...). The adjective "ednakuv" ("the same") derives from the same radical.
Example phrases include:
An interesting phenomenon is that these can be strung along one after another in quite long constructions, e.g.,
An extreme (colloquial) sentence, with almost no "physical" meaning in it whatsoever – yet which "does" have perfect meaning to the Bulgarian ear – would be :
—Note: the subject of the sentence is simply the pronoun "taya" (lit. "this one here"; colloq. "she").
Similar "meaningless" expressions are extremely common in spoken Bulgarian, especially when the speaker is finding it difficult to describe something.
Agglutination.
Although not considered an agglutinative language, Bulgarian does have agglutinative features. In the simplest terms, this can be seen in the way that most nouns and verbs are formed – namely by adding prefixes and suffixes to a rather limited number of roots, which creates almost a dozen new words, along with a couple of dozen derivatives thereof. Here are some examples using the root word "klyuch" (ключ) "key/switch" –
Nouns –
Adjectives –
Verbs –
An extreme example using this root might be –
Adjectives can also take up to three endings, including infixes, which are added to the masculine root, e.g. –
Verbs can take several prefixes, thus expressing increasingly complex ideas. For example, the "–bol–" root, which has to do with ailments ("bol-ka" – pain; "bol-est" – illness; "bol-i" – it hurts, etc.), can be used to express various different stages of falling ill –
Similarly, the root –kri–, referring to hiding/discovery –
Lexis.
Most of the vocabulary of modern Bulgarian consists of derivations of some 2,000 words inherited from proto-Slavic through the mediation of Old and Middle Bulgarian. Thus, the native lexical terms in Bulgarian account for 70% to 75% of the lexicon.
The remaining 25% to 30% are loanwords from a number of languages, as well as derivations of such words. The languages which have contributed most to Bulgarian are Russian, French and to a lesser extent Turkish and English. Also Latin and Greek are the source of many words, used mostly in international terminology. Many of the numerous loanwords from Turkish (and, via Turkish, from Arabic and Persian) which were adopted into Bulgarian during the long period of Ottoman rule, have been replaced with native terms. In addition, both specialized (usually coming from the field of science) and commonplace English words (notably abstract, commodity/service-related or technical terms) have also penetrated Bulgarian since the second half of the 20th century, especially since 1989. A noteworthy portion of this English-derived terminology has attained some unique features in the process of its introduction to native speakers, and this has resulted in peculiar derivations that slightly set the newly formed loanwords apart from the original words (mainly in pronunciation), although many loanwords are completely identical to the source words. A growing number of international neologisms are also being widely adopted.
Borrowings.
Some very frequent expressions have been borrowed from other languages. Most of them are somewhat informal.
External links.
Linguistic reports
Dictionaries
Courses

</doc>
<doc id="4153" url="http://en.wikipedia.org/wiki?curid=4153" title="Bipyramid">
Bipyramid

An "n"-gonal bipyramid or dipyramid is a polyhedron formed by joining an "n"-gonal pyramid and its mirror image base-to-base.
The referenced "n"-gon in the name of the bipyramids is not an external face but an internal one, existing on the primary symmetry plane which connects the two pyramid halves.
The face-transitive bipyramids are the dual polyhedra of the uniform prisms and will generally have isosceles triangle faces.
A bipyramid can be projected on a sphere or globe as "n" equally spaced lines of longitude going from pole to pole, and bisected by a line around the equator.
Bipyramid faces, projected as spherical triangles, represent the fundamental domains in the dihedral symmetry Dnh.
Volume.
The volume of a bipyramid is formula_1 where "B" is the area of the base and "h" the height from the base to the apex. This works for any location of the apex, provided that "h" is measured as the perpendicular distance from the plane which contains the base.
The volume of a bipyramid whose base is a regular "n"-sided polygon with side length "s" and whose height is "h" is therefore: 
Equilateral triangle bipyramids.
Only three kinds of bipyramids can have all edges of the same length (which implies that all faces are equilateral triangles, and thus the bipyramid is a deltahedron): the triangular, tetragonal, and pentagonal bipyramids. The tetragonal bipyramid with identical edges, or regular octahedron, counts among the Platonic solids, while the triangular and pentagonal bipyramids with identical edges count among the Johnson solids (J12 and J13).
Kalidescopic symmetry.
If the base is regular and the line through the apexes intersects the base at its center, the symmetry group of the "n"-agonal bipyramid has dihedral symmetry D"n"h of order 4"n", except in the case of a regular octahedron, which has the larger octahedral symmetry group Oh of order 48, which has three versions of D4h as subgroups. The rotation group is D"n" of order 2"n", except in the case of a regular octahedron, which has the larger symmetry group O of order 24, which has three versions of D4 as subgroups.
The digonal faces of a spherical "2n"-bipyramid represents the fundamental domains of dihedral symmetry in three dimensions: Dnh, [n,2], (*n22), order "4n". The reflection domains can be shown as alternately colored triangles as mirror images.
Star bipyramids.
Self-intersecting bipyramids exist with a star polygon central figure, defined by triangular faces connecting each polygon edge to these two points. A {p/q} bipyramid has Coxeter diagram .
Polychora with bipyramid cells.
The dual of the rectification of each convex regular polychoron is a cell-transitive polychoron with bipyramidal cells. In the following, the apex vertex of the bipyramid is A and an equator vertex is E. The distance between adjacent vertices on the equator EE=1, the apex to equator edge is AE and the distance between the apices is AA. The bipyramid polychoron will have VA vertices where the apices of NA bipyramids meet. It will have VE vertices where the type E vertices of NE bipyramids meet. NAE bipyramids meet along each type AE edge. NEE bipyramids meet along each type EE edge. CAE is the cosine of the dihedral angle along an AE edge. CEE is the cosine of the dihedral angle along an EE edge. As cells must fit around an edge, 
NAA cos−1(CAA) ≤ 2π, NAE cos−1(CAE) ≤ 2π.
Higher dimensions.
In general, a "bipyramid" can be seen as an "n"-polytope constructed with a ("n"−1)-polytope in a hyperplane with two points in opposite directions, equal distance perpendicular from the hyperplane. If the ("n"−1)-polytope is a regular polytope, it will have identical pyramids facets. An example is the 16-cell, which is an octahedral bipyramid, and more generally an n-orthoplex is an (n-1)-orthoplex bypyramid.

</doc>
<doc id="4154" url="http://en.wikipedia.org/wiki?curid=4154" title="Beast of Bodmin">
Beast of Bodmin

The Beast of Bodmin, also known as the Beast of Bodmin Moor () is a phantom wild cat purported to live in Cornwall, England, United Kingdom. Bodmin Moor became a centre of these sightings with occasional reports of mutilated slain livestock: the alleged panther-like cats of the same region came to be popularly known as the Beast of Bodmin Moor.
In general, scientists reject such claims because of the improbably large numbers necessary to maintain a breeding population and because climate and food supply issues would make such purported creatures' survival in reported habitats unlikely.
Investigation.
A long held hypothesis suggests the possibility that alien big cats at large in the United Kingdom could have been imported as part of private collections or zoos, later escaped or set free. An escaped big cat would not be reported to the authorities due to the illegality of owning and importing the animals.
The Ministry of Agriculture, Fisheries and Food conducted an official investigation in 1995. The study found that there was 'no verifiable evidence' of exotic felines loose in Britain, and that the mauled farm animals could have been attacked by common indigenous species. The report stated that 'the investigation could not prove that a "big cat" is not present.'
Skull.
Less than a week after the government report, a boy was walking by the River Fowey when he discovered a large cat skull. Measuring about long by wide, the skull was lacking its lower jaw but possessed two sharp, prominent canines that suggested that it might have been a leopard. The story hit the national press at about the same time of the official denial of alien big cat evidence on Bodmin Moor.
The skull was sent to the Natural History Museum in London for verification. They determined that it was a genuine skull from a young male leopard, but also found that the cat had not died in Britain and that the skull had been imported as part of a leopard-skin rug. The back of the skull was cleanly cut off in a way that is commonly used to mount the head on a rug. There was an egg case inside the skull that had been laid by a tropical cockroach that could not possibly be found in Britain. There were also cut marks on the skull indicating the flesh had been scraped off with a knife, and the skull had begun to decompose only after a recent submersion in water.

</doc>
