<doc id="4781" url="http://en.wikipedia.org/wiki?curid=4781" title="Benzodiazepine">
Benzodiazepine

A benzodiazepine (sometimes colloquially "benzo"; often abbreviated "BZD") is a psychoactive drug whose core chemical structure is the fusion of a benzene ring and a diazepine ring. The first such drug, chlordiazepoxide (Librium), was discovered accidentally by Leo Sternbach in 1955, and made available in 1960 by Hoffmann–La Roche, which has also marketed the benzodiazepine diazepam (Valium) since 1963.
Benzodiazepines enhance the effect of the neurotransmitter gamma-aminobutyric acid (GABA) at the GABAA receptor, resulting in sedative, hypnotic (sleep-inducing), anxiolytic (anti-anxiety), anticonvulsant, and muscle relaxant properties; also seen in the applied pharmacology of high doses of many shorter-acting benzodiazepines are amnesic-dissociative actions. These properties make benzodiazepines useful in treating anxiety, insomnia, agitation, seizures, muscle spasms, alcohol withdrawal and as a premedication for medical or dental procedures. Benzodiazepines are categorized as either short-, intermediate-, or long-acting. Short- and intermediate-acting benzodiazepines are preferred for the treatment of insomnia; longer-acting benzodiazepines are recommended for the treatment of anxiety.
In general, benzodiazepines are safe and effective in the short term, although cognitive impairments and paradoxical effects such as aggression or behavioral disinhibition occasionally occur. A minority react reverse and contrary to what would normally be expected. For example, a state of panic may worsen considerably following intake of a benzodiazepine. Long-term use is controversial due to concerns about adverse psychological and physical effects, increased questioning of effectiveness, and, because benzodiazepines are prone to cause tolerance, physical dependence, and, upon cessation of use after long-term use, a withdrawal syndrome. Due to adverse effects associated with the long-term use of benzodiazepines, withdrawal from benzodiazepines, in general, leads to improved physical and mental health. The elderly are at an increased risk of suffering from both short- and long-term adverse effects, including an associated roughly 50% increase in the risk of dementia.
There is controversy concerning the safety of benzodiazepines in pregnancy. While they are not major teratogens, uncertainty remains as to whether they cause cleft palate in a small number of babies and whether neurobehavioural effects occur as a result of prenatal exposure; they are known to cause withdrawal symptoms in the newborn. Benzodiazepines can be taken in overdoses and can cause dangerous deep unconsciousness. However, they are much less toxic than their predecessors, the barbiturates, and death rarely results when a benzodiazepine is the only drug taken; however, when combined with other central nervous system depressants such as ethanol and opiates, the potential for toxicity and fatal overdose increases. Benzodiazepines are commonly misused and taken in combination with other drugs of abuse. In addition, all benzodiazepines are listed in Beers List, which is significant in clinical practice.
Medical uses.
Benzodiazepines possess sedative, hypnotic, anxiolytic, anticonvulsant, muscle relaxant, and amnesic actions, which are useful in a variety of indications such as alcohol dependence, seizures, anxiety, panic, agitation, and insomnia. Most are administered orally; however, they can also be given intravenously, intramuscularly, or rectally. In general, benzodiazepines are well-tolerated and are safe and effective drugs in the short term for a wide range of conditions. Tolerance can develop to their effects and there is also a risk of dependence, and upon discontinuation a withdrawal syndrome may occur. These factors, combined with other possible secondary effects after prolonged use such as psychomotor, cognitive, or memory impairments, limit their long-term applicability. The effects of long-term use or misuse include the tendency to cause or worsen cognitive deficits, depression, and anxiety.
Panic disorder.
Due to their effectiveness, tolerability, and rapid onset of anxiolytic action, benzodiazepines are frequently used for the treatment of anxiety associated with panic disorder. However, there is disagreement among expert bodies regarding the long-term use of benzodiazepines for panic disorder. The views range from those that hold that benzodiazepines are not effective long-term and that they should be reserved for treatment-resistant cases to that they are as effective in the long term as selective serotonin reuptake inhibitors.
The American Psychiatric Association (APA) guidelines note that, in general, benzodiazepines are well tolerated, and their use for the initial treatment for panic disorder is strongly supported by numerous controlled trials. APA states that there is insufficient evidence to recommend any of the established panic disorder treatments over another. The choice of treatment between benzodiazepines, SSRIs, serotonin–norepinephrine reuptake inhibitors, tricyclic antidepressants, and psychotherapy should be based on the patient's history, preference, and other individual characteristics. Selective serotonin reuptake inhibitors are likely to be the best choice of pharmacotherapy for many patients with panic disorder, but benzodiazepines are also often used, and some studies suggest that these medications are still used with greater frequency than the SSRIs. One advantage of benzodiazepines is that they alleviate the anxiety symptoms much faster than antidepressants, and therefore may be preferred in patients for whom rapid symptom control is critical. However, this advantage is offset by the possibility of developing benzodiazepine dependence. APA does not recommend benzodiazepines for persons with depressive symptoms or a recent history of substance abuse. The APA guidelines state that, in general, pharmacotherapy of panic disorder should be continued for at least a year, and that clinical experience support continuing benzodiazepine treatment to prevent recurrence. Although major concerns about benzodiazepine tolerance and withdrawal have been raised, there is no evidence for significant dose escalation in patients using benzodiazepines long-term. For many such patients stable doses of benzodiazepines retain their efficacy over several years.
Guidelines issued by the UK-based National Institute for Health and Clinical Excellence (NICE), carried out a systematic review using different methodology and came to a different conclusion. They questioned the accuracy of studies that were not placebo-controlled. And, based on the findings of placebo-controlled studies, they do not recommend use of benzodiazepines beyond two to four weeks, as tolerance and physical dependence develop rapidly, with withdrawal symptoms including rebound anxiety occurring after six weeks or more of use. Nevertheless, benzodiazepines continue to be prescribed for the long-term treatment of anxiety disorders, although specific antidepressants and psychological therapies are recommended as the first-line treatment options with the anticonvulsant drug pregabalin indicated as a second- or third-line treatment and suitable for long-term use. NICE stated that long-term use of benzodiazepines for panic disorder with or without agoraphobia is an unlicensed indication, does not have long-term efficacy, and is, therefore, not recommended by clinical guidelines. Psychological therapies such as cognitive behavioural therapy are recommended as a first-line therapy for panic disorder; benzodiazepine use has been found to interfere with therapeutic gains from these therapies.
Benzodiazepines are usually administered orally; however, very occasionally lorazepam or diazepam may be given intravenously for the treatment of panic attacks.
Generalized anxiety disorder.
Benzodiazepines have robust efficacy in the short-term management of generalized anxiety disorder (GAD), but were not shown to be effective in producing long-term improvement overall. According to National Institute for Health and Clinical Excellence (NICE), benzodiazepines can be used in the immediate management of GAD, if necessary. However, they should not usually be given for longer than 2–4 weeks. The only medications NICE recommends for the longer term management of GAD are antidepressants.
Likewise, Canadian Psychiatric Association (CPA) recommends benzodiazepines alprazolam, bromazepam, lorazepam, and diazepam only as a second-line choice, if the treatment with two different antidepressants was unsuccessful. Although they are second-line agents, benzodiazepines can be used for a limited time to relieve severe anxiety and agitation. CPA guidelines note that after 4–6 weeks the effect of benzodiazepines may decrease to the level of placebo, and that benzodiazepines are less effective than antidepressants in alleviating ruminative worry, the core symptom of GAD. However, in some cases, a prolonged treatment with benzodiazepines as the add-on to an antidepressant may be justified.
Insomnia.
Benzodiazepines can be useful for short-term treatment of insomnia. Their use beyond 2 to 4 weeks is not recommended due to the risk of dependence. It is preferred that benzodiazepines be taken intermittently and at the lowest effective dose. They improve sleep-related problems by shortening the time spent in bed before falling asleep, prolonging the sleep time, and, in general, reducing wakefulness.
However, they worsen sleep quality by increasing light sleep and decreasing deep sleep. Other drawbacks of hypnotics, including benzodiazepines, are possible tolerance to their effects, rebound insomnia, and reduced slow-wave sleep and a withdrawal period typified by rebound insomnia and a prolonged period of anxiety and agitation. The list of benzodiazepines approved for the treatment of insomnia is fairly similar among most countries, but which benzodiazepines are officially designated as first-line hypnotics prescribed for the treatment of insomnia can vary distinctly between countries. Longer-acting benzodiazepines such as nitrazepam and diazepam have residual effects that may persist into the next day and are, in general, not recommended.
It is not clear as to whether the new nonbenzodiazepine hypnotics (Z-drugs) are better than the short-acting benzodiazepines. The efficacy of these two groups of medications is similar. According to the US Agency for Healthcare Research and Quality, indirect comparison indicates that side-effects from benzodiazepines may be about twice as frequent as from nonbenzodiazepines. Some experts suggest using nonbenzodiazepines preferentially as a first-line long-term treatment of insomnia. However, the UK National Institute for Health and Clinical Excellence did not find any convincing evidence in favor of Z-drugs. NICE review pointed out that short-acting Z-drugs were inappropriately compared in clinical trials with long-acting benzodiazepines. There have been no trials comparing short-acting Z-drugs with appropriate doses of short-acting benzodiazepines. Based on this, NICE recommended choosing the hypnotic based on cost and the patient's preference.
Older adults should not use benzodiazepines to treat insomnia unless other treatments have failed to be effective. When benzodiazepines are used, patients, their caretakers, and their physician should discuss the increased risk of harms, including evidence which shows twice the incidence of traffic collisions among driving patients as well as falls and hip fracture for all older patients.
Seizures.
Prolonged convulsive epileptic seizures are a medical emergency that can usually be dealt with effectively by administering fast-acting benzodiazepines, which are potent anticonvulsants. In a hospital environment, intravenous clonazepam, lorazepam, and diazepam are first-line choices, clonazepam due to its stronger and more potent anticonvulsant action, lorazepam due to its faster onset and diazepam for its longer duration of action. In the community, intravenous administration is not practical and so rectal diazepam or (more recently) buccal midazolam are used, with a preference for midazolam as its administration is easier and more socially acceptable.
When benzodiazepines were first introduced, they were enthusiastically adopted for treating all forms of epilepsy. However, drowsiness and tolerance become problems with continued use and none are now considered first-line choices for long-term epilepsy therapy. Clobazam is widely used by specialist epilepsy clinics worldwide and clonazepam is popular in the Netherlands, Belgium and France. It was approved for use in the United States in 2011. In the UK, both clobazam and clonazepam are second-line choices for treating many forms of epilepsy. Clobazam also has a useful role for very short-term seizure prophylaxis and in catamenial epilepsy. Discontinuation after long term use in epilepsy requires additional caution because of the risks of rebound seizures. Therefore, the dose is slowly tapered over a period of up to six months or longer.
Alcohol withdrawal.
Chlordiazepoxide is the most commonly used benzodiazepine for alcohol detoxification, but diazepam may be used as an alternative. Both are used in the detoxification of individuals who are motivated to stop drinking, and are prescribed for a short period of time to reduce the risks of developing tolerance and dependence to the benzodiazepine medication itself. The benzodiazepines with a longer half-life make detoxification more tolerable, and dangerous alcohol withdrawal effects are less likely to occur. On the other hand, short-acting benzodiazepines may lead to breakthrough seizures, and are, therefore, not recommended for detoxification in an outpatient setting. Oxazepam and lorazepam are often used in patients at risk of drug accumulation, in particular, the elderly and those with cirrhosis, because they are metabolized differently from other benzodiazepines, through conjugation.
Benzodiazepines are the preferred choice in the management of alcohol withdrawal syndrome, in particular, for the prevention and treatment of the dangerous complication of seizures and in subduing severe delirium. Lorazepam is the only benzodiazepine with predictable intramuscular absorption and it is the most effective in preventing and controlling acute seizures.
Anxiety.
Benzodiazepines are sometimes used in the treatment of acute anxiety, as they bring about rapid and marked or moderate relief of symptoms in most individuals; however, they are not recommended beyond 2–4 weeks of use due to risks of tolerance and dependence and a lack of long-term effectiveness. As for insomnia, they may also be used on an irregular/"as-needed" basis, such as in cases where said anxiety is at its worst. Compared to other pharmacological treatments, benzodiazepines are twice as likely to lead to a relapse of the underlying condition upon discontinuation. Psychological therapies and other pharmacological therapies are recommended for the long-term treatment of generalized anxiety disorder. Antidepressants have higher remission rates and are, in general, safe and effective in the short and long term.
Other indications.
Benzodiazepines are often prescribed for a wide range of conditions:
Adverse effects.
The most common side-effects of benzodiazepines are related to their sedating and muscle-relaxing action. They include drowsiness, dizziness, and decreased alertness and concentration. Lack of coordination may result in falls and injuries, in particular, in the elderly. Another result is impairment of driving skills and increased likelihood of road traffic accidents. Decreased libido and erection problems are a common side effect. Depression and disinhibition may emerge. Hypotension and suppressed breathing may be encountered with intravenous use. Less common side effects include nausea and changes in appetite, blurred vision, confusion, euphoria, depersonalization and nightmares. Cases of liver toxicity have been described but are very rare.
Paradoxical effects.
Paradoxical reactions, such as increased seizures in epileptics, aggression, violence, impulsivity, irritability and suicidal behavior sometimes occur. These reactions have been explained as consequences of disinhibition and the subsequent loss of control over socially unacceptable behavior. Paradoxical reactions are rare in the general population, with an incidence rate below 1% and similar to placebo. However, they occur with greater frequency in recreational abusers, individuals with borderline personality disorder, children, and patients on high-dosage regimes. In these groups, impulse control problems are perhaps the most important risk factor for disinhibition; learning disabilities and neurological disorders are also significant risks. Most reports of disinhibition involve high doses of high-potency benzodiazepines. Paradoxical effects may also appear after chronic use of benzodiazepines.
Cognitive effects.
The short-term use of benzodiazepines adversely affects multiple areas of cognition, the most notable one being that it interferes with the formation and consolidation of memories of new material and may induce complete anterograde amnesia. However, researchers hold contrary opinions regarding the effects of long-term administration. One view is that many of the short-term effects continue into the long-term and may even worsen, and are not resolved after quitting benzodiazepines. Another view maintains that cognitive deficits in chronic benzodiazepine users occur only for a short period after the dose, or that the anxiety disorder is the cause of these deficits.
While the definitive studies are lacking, the former view received support from a 2004 meta-analysis of 13 small studies. This meta-analysis found that long-term use of benzodiazepines was associated with moderate to large adverse effects on all areas of cognition, with visuospatial memory being the most commonly detected impairment. Some of the other impairments reported were decreased IQ, visiomotor coordination, information processing, verbal learning and concentration. The authors of the meta-analysis and a later reviewer noted that the applicability of this meta-analysis is limited because the subjects were taken mostly from withdrawal clinics; the coexisting drug, alcohol use, and psychiatric disorders were not defined; and several of the included studies conducted the cognitive measurements during the withdrawal period.
Long-term effects.
The long-term effects of benzodiazepine use can include cognitive impairment as well as affective and behavioural problems. Feelings of turmoil, difficulty in thinking constructively, loss of sex-drive, agoraphobia and social phobia, increasing anxiety and depression, loss of interest in leisure pursuits and interests, and an inability to experience or express feelings can also occur. Not everyone, however, experiences problems with long-term use. Additionally an altered perception of self, environment and relationships may occur.
Withdrawal syndrome.
Tolerance, dependence and withdrawal.
The main problem of the chronic use of benzodiazepines is the development of tolerance and dependence. Tolerance manifests itself as diminished pharmacological effect and develops relatively quickly to the sedative, hypnotic, anticonvulsant, and muscle relaxant actions of benzodiazepines. Tolerance to anti-anxiety effects develops more slowly with little evidence of continued effectiveness beyond four to six months of continued use. In general, tolerance to the amnesic effects does not occur. However, controversy exists as to tolerance to the anxiolytic effects with some evidence that benzodiazepines retain efficacy and opposing evidence from a systematic review of the literature that tolerance frequently occurs and some evidence that anxiety may worsen with long-term use. The question of tolerance to the amnesic effects of benzodiazepines is, likewise, unclear. Some evidence suggests that partial tolerance does develop, and "the memory impairment is limited to a narrow window within 90 minutes after each dose".
Discontinuation of benzodiazepines or abrupt reduction of the dose, even after a relatively short course of treatment (three to four weeks), may result in two groups of symptoms — rebound and withdrawal. Rebound symptoms are the return of the symptoms for which the patient was treated but worse than before. Withdrawal symptoms are the new symptoms that occur when the benzodiazepine is stopped. They are the main sign of physical dependence.
Withdrawal symptoms and management.
The most frequent symptoms of withdrawal from benzodiazepines are insomnia, gastric problems, tremors, agitation, fearfulness, and muscle spasms. The less frequent effects are irritability, sweating, depersonalization, derealization, hypersensitivity to stimuli, depression, suicidal behavior, psychosis, seizures, and delirium tremens. Severe symptoms usually occur as a result of abrupt or over-rapid withdrawal. Abrupt withdrawal can be dangerous, therefore a gradual reduction regimen is recommended.
Symptoms may also occur during a gradual dosage reduction, but are typically less severe and may persist as part of a protracted withdrawal syndrome for months after cessation of benzodiazepines. Approximately 10% of patients will experience a notable protracted withdrawal syndrome, which can persist for many months or in some cases a year or longer. Protracted symptoms tend to resemble those seen during the first couple of months of withdrawal but usually are of a sub acute level of severity. Such symptoms do gradually lessen over time, eventually disappearing altogether.
Benzodiazepines have a reputation with patients and doctors for causing a severe and traumatic withdrawal; however, this is in large part due to the withdrawal process being poorly managed. Over-rapid withdrawal from benzodiazepines increases the severity of the withdrawal syndrome and increases the failure rate. A slow and gradual withdrawal customised to the individual and, if indicated, psychological support is the most effective way of managing the withdrawal. Opinion as to the time needed to complete withdrawal ranges from four weeks to several years. A goal of less than six months has been suggested, but due to factors such as dosage and type of benzodiazepine, reasons for prescription, lifestyle, personality, environmental stresses, and amount of available support, a year or more may be needed to withdraw.
Withdrawal is best managed by transferring the physically dependent patient to an equivalent dose of diazepam because it has the longest half-life of all of the benzodiazepines, is metabolised into long-acting active metabolites and is available in low-potency tablets, which can be quartered for smaller doses. A further benefit is that it is available in liquid form, which allows for even smaller reductions. Chlordiazepoxide, which also has a long half-life and long-acting active metabolites, can be used as an alternative.
Nonbenzodiazepines are contraindicated during benzodiazepine withdrawal as they are cross tolerant with benzodiazepines and can induce dependence. Alcohol is also cross tolerant with benzodiazepines and more toxic and thus caution is needed to avoid replacing one dependence with another. During withdrawal, fluoroquinolone-based antibiotics are best avoided if possible; they displace benzodiazepines from their binding site and reduce GABA function and, thus, may aggravate withdrawal symptoms. Antipsychotics are not recommended for benzodiazepine withdrawal (or other CNS depressant withdrawal states) especially clozapine, olanzapine or low potency phenothiazines e.g. chlorpromazine as they lower the seizure threshold and can worsen withdrawal effects; if used extreme caution is required.
Withdrawal from long term benzodiazepines is beneficial for most individuals. Withdrawal of benzodiazepines from long-term users, in general, leads to improved physical and mental health particularly in the elderly; although some long term users report continued benefit from taking benzodiazepines, this may be the result of suppression of withdrawal effects.
Overdose.
Although benzodiazepines are much safer in overdose than their predecessors, the barbiturates, they can still cause problems in overdose. Taken alone, they rarely cause severe complications in overdose; statistics in England showed that benzodiazepines were responsible for 3.8% of all deaths by poisoning from a single drug. However, combining these drugs with alcohol, opiates or tricyclic antidepressants markedly raises the toxicity. The elderly are more sensitive to the side effects of benzodiazepines, and poisoning may even occur from their long-term use. The various benzodiazepines differ in their toxicity; temazepam appears to be most toxic in overdose and when used with other drugs. The symptoms of a benzodiazepine overdose may include; drowsiness, slurred speech, nystagmus, hypotension, ataxia, coma, respiratory depression, and cardiorespiratory arrest.
A reversal agent for benzodiazepines exists, flumazenil (Anexate). Its use as an antidote is not routinely recommended due to the high risk of resedation and seizures. In a double-blind, placebo-controlled trial of 326 patients, 4 patients suffered serious adverse events and 61% became resedated following the use of flumazenil. Numerous contraindications to its use exist. It is contraindicated in patients with a history of long-term use of benzodiazepines, those having ingested a substance that lowers the seizure threshold or may cause an arrhythmia, and in those with abnormal vital signs. One study found that only 10% of the patient population presenting with a benzodiazepine overdose are suitable candidates for treatment with flumazenil.
Contraindications.
Because of their muscle relaxant action, benzodiazepines may cause respiratory depression in susceptible individuals. For that reason, they are contraindicated in people with myasthenia gravis, sleep apnea, bronchitis, and COPD. Caution is required when benzodiazepines are used in people with personality disorders or intellectual disability because of frequent paradoxical reactions. In major depression, they may precipitate suicidal tendencies and are sometimes used for suicidal overdoses. Individuals with a history of alcohol, opioid and barbiturate abuse should avoid benzodiazepines, as there is a risk of life-threatening interactions with these drugs.
Pregnancy.
In the United States, the Food and Drug Administration has categorized benzodiazepines into either category D or X meaning potential for harm in the unborn has been demonstrated.
Exposure to benzodiazepines during pregnancy has been associated with a slightly increased (from 0.06 to 0.07%) risk of cleft palate in newborns, a controversial conclusion as some studies find no association between benzodiazepines and cleft palate. Their use by expectant mothers shortly before the delivery may result in a floppy infant syndrome, with the newborns suffering from hypotonia, hypothermia, lethargy, and breathing and feeding difficulties. Cases of neonatal withdrawal syndrome have been described in infants chronically exposed to benzodiazepines in utero. This syndrome may be hard to recognize, as it starts several days after delivery, for example, as late as 21 day for chlordiazepoxide. The symptoms include tremors, hypertonia, hyperreflexia, hyperactivity, and vomiting and may last for up to three to six months. Tapering down the dose during pregnancy may lessen its severity. If used in pregnancy, those benzodiazepines with a better and longer safety record, such as diazepam or chlordiazepoxide, are recommended over potentially more harmful benzodiazepines, such as temazepam or triazolam. Using the lowest effective dose for the shortest period of time minimizes the risks to the unborn child.
Elderly.
The benefits of benzodiazepines are least and the risks are greatest in the elderly. The elderly are at an increased risk of dependence and are more sensitive to the adverse effects such as memory problems, daytime sedation, impaired motor coordination, and increased risk of motor vehicle accidents and falls, and an increased risk of hip fractures. The long-term effects of benzodiazepines and benzodiazepine dependence in the elderly can resemble dementia, depression, or anxiety syndromes, and progressively worsens over time. Adverse effects on cognition can be mistaken for the effects of old age. The benefits of withdrawal include improved cognition, alertness, mobility, reduced risk incontinence, and a reduced risk of falls and fractures. The success of gradual-tapering benzodiazepines is as great in the elderly as in younger people. Benzodiazepines should be prescribed to the elderly only with caution and only for a short period at low doses. Short to intermediate-acting benzodiazepines are preferred in the elderly such as oxazepam and temazepam. The high potency benzodiazepines alprazolam and triazolam and long-acting benzodiazepines are not recommended in the elderly due to increased adverse effects. Nonbenzodiazepines such as zaleplon and zolpidem and low doses of sedating antidepressants are sometimes used as alternatives to benzodiazepines.
Long-term use of benzodiazepines has been associated with increased risk of cognitive impairment, but its relationship with dementia remains inconclusive. The association of a past history of benzodiazepine use and cognitive decline is unclear, with some studies reporting a lower risk of cognitive decline in former users, some finding no association and some indicating an increased risk of cognitive decline.
Benzodiazepines are sometimes prescribed to treat behavioral symptoms of dementia. However, like antidepressants, they have little evidence of effectiveness, although antipsychotics have shown some benefit. Cognitive impairing effects of benzodiazepines that occur frequently in the elderly can also worsen dementia. A 2012 study concludes that new use of benzodiazepine by persons 65 and over is associated with an approximately 50% increase in the risk of dementia.
Pharmacology.
Benzodiazepines share a similar chemical structure, and their effects in humans are mainly produced by the allosteric modification of a specific kind of neurotransmitter receptor, the GABAA receptor, which increases the overall conductance of these inhibitory channels; this results in the various therapeutic effects as well as adverse effects of benzodiazepines. Other less important mechanisms of action are also known.
Chemistry.
The term "benzodiazepine" is the chemical name for the heterocyclic ring system (see figure to the right), which is a fusion between the benzene and diazepine ring systems. Under Hantzsch–Widman nomenclature, a diazepine is a heterocycle with two nitrogen atoms, five carbon atom and the maximum possible number of cumulative double bonds. The "benzo" prefix indicates the benzene ring fused onto the diazepine ring.
Benzodiazepine drugs are substituted 1,4-benzodiazepines, although the chemical term can refer to many other compounds that do not have useful pharmacological properties. Different benzodiazepine drugs have different side groups attached to this central structure. The different side groups affect the binding of the molecule to the GABAA receptor and so modulate the pharmacological properties. Many of the pharmacologically active "classical" benzodiazepine drugs contain the 5-phenyl-1"H"-benzo["e"][1,4]diazepin-2(3"H")-one substructure (see figure to the right). Benzodiazepines have been found to mimic protein reverse turns structurally which enable them with their biological activity in many cases.,
Nonbenzodiazepines also bind to the benzodiazepine binding site on the GABAA receptor and possess similar pharmacological properties. While the nonbenzodiazepines are by definition structurally unrelated to the benzodiazepines, both classes of drugs possess a common pharmacophore (see figure to the lower-right), which explains their binding to a common receptor site.
Mechanism of action.
Benzodiazepines work by increasing the efficiency of a natural brain chemical, GABA, to decrease the excitability of neurons. This reduces the communication between neurons and, therefore, has a calming effect on many of the functions of the brain.
GABA controls the excitability of neurons by binding to the GABAA receptor. The GABAA receptor is a protein complex located in the synapses of neurons. All GABAA receptors contain an ion channel that conducts chloride ions across neuronal cell membranes and two binding sites for the neurotransmitter gamma-aminobutyric acid (GABA), while a subset of GABAA receptor complexes also contain a single binding site for benzodiazepines. Binding of benzodiazepines to this receptor complex does not alter binding of GABA. Unlike other positive allosteric modulators that increases ligand binding, benzodiazepine binding acts as a positive allosteric modulator by increasing the total conduction of chloride ions across the neuronal cell membrane when GABA is already bound to its receptor. This increased chloride ion influx hyperpolarizes the neuron's membrane potential. As a result, the difference between resting potential and threshold potential is increased and firing is less likely.
Different GABAA receptor subtypes have varying distributions within different regions of the brain and, therefore, control distinct neuronal circuits. Hence, activation of different GABAA receptor subtypes by benzodiazepines may result in distinct pharmacological actions. In terms of the mechanism of action of benzodiazepines, their similarities are too great to separate them into individual categories such as anxiolytic or hypnotic. For example, a hypnotic administered in low doses will produce anxiety-relieving effects, whereas a benzodiazepine marketed as an anti-anxiety drug will at higher doses induce sleep.
The subset of GABAA receptors that also bind benzodiazepines are referred to as benzodiazepine receptors (BzR). The GABAA receptor is a heteromer composed of five subunits, the most common ones being two "α"s, two "β"s, and one "γ" (α2β2γ). For each subunit, many subtypes exist (α1–6, β1–3, and γ1–3). GABAA receptors that are made up of different combinations of subunit subtypes have different properties, different distributions in the brain and different activities relative to pharmacological and clinical effects. Benzodiazepines bind at the interface of the α and γ subunits on the GABAA receptor. Binding also requires that alpha subunits contain a histidine amino acid residue, ("i.e.", α1, α2, α3, and α5 containing GABAA receptors). For this reason, benzodiazepines show no affinity for GABAA receptors containing α4 and α6 subunits with an arginine instead of a histidine residue. Once bound to the benzodiazepine receptor, the benzodiazepine ligand locks the benzodiazepine receptor into a conformation in which it has a greater affinity for the GABA neurotransmitter. This increases the frequency of the opening of the associated chloride ion channel and hyperpolarizes the membrane of the associated neuron. The inhibitory effect of the available GABA is potentiated, leading to sedatory and anxiolytic effects. For instance, those ligands with high activity at the α1 are associated with stronger hypnotic effects, whereas those with higher affinity for GABAA receptors containing α2 and/or α3 subunits have good anti-anxiety activity.
The benzodiazepine class of drugs also interact with peripheral benzodiazepine receptors. Peripheral benzodiazepine receptors are present in peripheral nervous system tissues, glial cells, and to a lesser extent the central nervous system. These peripheral receptors are not structurally related or coupled to GABAA receptors. They modulate the immune system and are involved in the body response to injury. Benzodiazepines also function as weak adenosine reuptake inhibitors. It has been suggested that some of their anticonvulsant, anxiolytic, and muscle relaxant effects may be in part mediated by this action.
Pharmacokinetics.
A benzodiazepine can be placed into one of three groups by its elimination half-life, or time it takes for the body to eliminate half of the dose. Some benzodiazepines have long-acting active metabolites, such as diazepam and chlordiazepoxide, which are metabolised into desmethyldiazepam. Desmethyldiazepam has a half-life of 36–200 hours, and flurazepam, with the main active metabolite of desalkylflurazepam, with a half-life of 40–250 hours. These long-acting metabolites are partial agonists.
Interactions.
Individual benzodiazepines may have different interactions with certain drugs. Depending on their metabolism pathway, benzodiazepines can be divided roughly into two groups. The largest group consists of those that are metabolized by cytochrome P450 (CYP450) enzymes and possess significant potential for interactions with other drugs. The other group comprises those that are metabolized through glucuronidation, such as lorazepam, oxazepam, and temazepam, and, in general, have few drug interactions.
Many drugs, including oral contraceptives, some antibiotics, antidepressants, and antifungal agents, inhibit cytochrome enzymes in the liver. They reduce the rate of elimination of the benzodiazepines that are metabolized by CYP450, leading to possibly excessive drug accumulation and increased side-effects. In contrast, drugs that induce cytochrome P450 enzymes, such as St John's wort, the antibiotic rifampicin, and the anticonvulsants carbamazepine and phenytoin, accelerate elimination of many benzodiazepines and decrease their action. Taking benzodiazepines with alcohol, opioids and other central nervous system depressants potentiates their action. This often results in increased sedation, impaired motor coordination, suppressed breathing, and other adverse effects that have potential to be lethal. Antacids can slow down absorption of some benzodiazepines; however, this effect is marginal and inconsistent.
History.
The first benzodiazepine, chlordiazepoxide ("Librium"), was synthesized in 1955 by Leo Sternbach while working at Hoffmann–La Roche on the development of tranquilizers. The pharmacological properties of the compounds prepared initially were disappointing, and Sternbach abandoned the project. Two years later, in April 1957, co-worker Earl Reeder noticed a "nicely crystalline" compound left over from the discontinued project while spring-cleaning in the lab. This compound, later named chlordiazepoxide, had not been tested in 1955 because of Sternbach's focus on other issues. Expecting the pharmacology results to be negative and hoping to publish the chemistry-related findings, researchers submitted it for a standard battery of animal tests. However, the compound showed very strong sedative, anticonvulsant, and muscle relaxant effects. These impressive clinical findings led to its speedy introduction throughout the world in 1960 under the brand name "Librium". Following chlordiazepoxide, diazepam marketed by Hoffmann–La Roche under the brand name "Valium" in 1963, and for a while the two were the most commercially successful drugs. The introduction of benzodiazepines led to a decrease in the prescription of barbiturates, and by the 1970s they had largely replaced the older drugs for sedative and hypnotic uses.
The new group of drugs was initially greeted with optimism by the medical profession, but gradually concerns arose; in particular, the risk of dependence became evident in the 1980s. Benzodiazepines have a unique history in that they were responsible for the largest-ever class-action lawsuit against drug manufacturers in the United Kingdom, involving 14,000 patients and 1,800 law firms that alleged the manufacturers knew of the dependence potential but intentionally withheld this information from doctors. At the same time, 117 general practitioners and 50 health authorities were sued by patients to recover damages for the harmful effects of dependence and withdrawal. This led some doctors to require a signed consent form from their patients and to recommend that all patients be adequately warned of the risks of dependence and withdrawal before starting treatment with benzodiazepines. The court case against the drug manufacturers never reached a verdict; legal aid had been withdrawn and there were allegations that the consultant psychiatrists, the expert witnesses, had a conflict of interest. This litigation led to changes in the British law, making class action lawsuits more difficult.
In 2010, formerly classified documents from a Medical Research Council (UK) meeting of experts emerged and revealed that the MRC was aware of research 30 years ago that suggested that benzodiazepines could cause brain damage in some people similar to that which occurs from alcohol abuse and failed to follow up with larger clinical trials. The MRC turned down research proposals in the 1980s by Professor Lader and also proposals by Professor Ashton in 1995 to study whether benzodiazepines had permanent effects on the brain. The MRC responded that it has always been open to research proposals in this area that meet required standards. It was further alleged that the MRC documents were relevant to a large class-action lawsuit, which started in the mid-1980s against drug companies and one solicitor stated that it was strange that the MRC had 'hidden' the documents. Jim Dobbin, MP and chair of the All-Party Parliamentary Group for Involuntary Tranquilliser Addiction, described the documents as a "huge scandal," given the large number of people who experience symptoms such as physical, cognitive, and psychological problems as a result of benzodiazepine use, which can persist even after withdrawing.
Although antidepressants with anxiolytic properties have been introduced, and there is increasing awareness of the adverse effects of benzodiazepines, prescriptions for short-term anxiety relief have not significantly dropped. For treatment of insomnia, benzodiazepines are now less popular than nonbenzodiazepines, which include zolpidem, zaleplon and eszopiclone. Nonbenzodiazepines are molecularly distinct, but nonetheless, they work on the same benzodiazepine receptors and produce similar sedative effects.
The Committee on the Review of Medicines.
The Committee on the Review of Medicines (UK) carried out a review into benzodiazepines due to significant concerns of tolerance, drug dependence and benzodiazepine withdrawal problems and other adverse effects. The committee found that benzodiazepines do not have any antidepressant or analgesic properties and are therefore unsuitable treatments for conditions such as depression, tension headaches and dysmenorrhoea. Benzodiazepines are also not beneficial in the treatment of psychosis due to a lack of efficacy. The committee also recommended against benzodiazepines being used in the treatment of anxiety or insomnia in children. The committee was in agreement with the Institute of Medicine (USA) and the conclusions of a study carried out by the White House Office of Drug Policy and the National Institute on Drug Abuse (USA) that there was little evidence that long-term use of benzodiazepine hypnotics was beneficial in the treatment of insomnia due to the development of tolerance. Benzodiazepines tended to lose their sleep-promoting properties within 3 to 14 days of continuous use and in the treatment of anxiety the committee found that there was little convincing evidence that benzodiazepines retained efficacy in the treatment of anxiety after four months continuous use due to the development of tolerance. The committee found that the regular use of benzodiazepines caused the development of dependence characterised by tolerance to the therapeutic effects of benzodiazepines and the development of the benzodiazepine withdrawal syndrome including symptoms such as anxiety, apprehension, tremor, insomnia, nausea, and vomiting upon cessation of benzodiazepine use. Withdrawal symptoms tended to develop within 24 hours on the cessation of a short-acting benzodiazepine and within 3 to 10 days after the cessation of a longer-acting benzodiazepine. Withdrawal effects could occur after treatment lasting only two weeks at therapeutic dose levels, however withdrawal effects tended to occur with habitual use beyond two weeks and were more likely the higher the dose. The withdrawal symptoms may appear to be similar to the original condition. The committee recommended that all benzodiazepine treatment be withdrawn gradually and recommended that benzodiazepine treatment be used only in carefully selected patients and that therapy be limited to short-term use only. It was noted in the review that alcohol can potentiate the central nervous system depressant effects of benzodiazepines and should be avoided. The central nervous system depressant effects of benzodiazepines may make driving or operating machinery dangerous and the elderly are more prone to these adverse effects. In the neonate, high single doses or repeated low doses have been reported to produce hypotonia, poor sucking, and hypothermia in the neonate and irregularities in the fetal heart. Benzodiazepines should be avoided in lactation. Withdrawal from benzodiazepines should be gradual as abrupt withdrawal from high doses of benzodiazepines may cause confusion, toxic psychosis, convulsions, or a condition resembling delirium tremens. Abrupt withdrawal from lower doses may cause depression, nervousness, rebound insomnia, irritability, sweating, and diarrhoea.
Recreational use.
Benzodiazepines are considered to be major drugs of abuse. Benzodiazepine abuse is mostly limited to individuals who abuse other drugs, i.e., poly-drug abusers. On the international scene, benzodiazepines are categorized as Schedule IV controlled drugs by the INCB, apart from flunitrazepam which is a Schedule III drug under the Convention on Psychotropic Substances. Some variation in drug scheduling exists in individual countries; for example, in the United Kingdom, midazolam and temazepam are Schedule III controlled drugs. British law requires temazepam (but "not" midazolam) to be stored in safe custody. Safe custody requirements ensures that pharmacists and doctors holding stock of temazepam must store it in securely fixed double-locked steel safety cabinets and maintain a written register, which must be bound and contain separate entries for temazepam and must be written in ink with no use of correction fluid (although a written register is not required for temazepam in the United Kingdom). Disposal of expired stock must be witnessed by a designated inspector (either a local drug-enforcement police officer or official from health authority). Benzodiazepine abuse ranges from occasional binges on large doses, to chronic and compulsive drug abuse of high doses.
Benzodiazepines are used recreationally and by problematic drug misusers. Mortality is higher among poly-drug misusers that also use benzodiazepines. Heavy alcohol use also increases mortality among poly-drug users. Dependence and tolerance, often coupled with dosage escalation, to benzodiazepines can develop rapidly among drug misusers; withdrawal syndrome may appear after as little as three weeks of continuous use. Long-term use has the potential to cause both physical and psychological dependence and severe withdrawal symptoms such as depression, anxiety (often to the point of panic attacks), and agoraphobia. Benzodiazepines and, in particular, temazepam are sometimes used intravenously, which, if done incorrectly or in an unsterile manner, can lead to medical complications including abscesses, cellulitis, thrombophlebitis, arterial puncture, deep vein thrombosis, and gangrene. Sharing syringes and needles for this purpose also brings up the possibility of transmission of hepatitis, HIV, and other diseases. Benzodiazepines are also misused intranasally, which may have additional health consequences. Once benzodiazepine dependence has been established, a clinician usually converts the patient to an equivalent dose of diazepam before beginning a gradual reduction program.
A 1999–2005 Australian police survey of detainees reported preliminary findings that self-reported users of benzodiazepines were less likely than non-user detainees to work full-time and more likely to receive government benefits, use methamphetamine or heroin, and be arrested or imprisoned. Benzodiazepines are sometimes used for criminal purposes; they serve to incapacitate a victim in cases of drug assisted rape or robbery.
Overall, anecdotal evidence suggests that temazepam may be the most psychologically habit-forming (addictive) benzodiazepine. Temazepam abuse reached epidemic proportions in some parts of the world, in particular, in Europe and Australia, and is a major drug of abuse in many Southeast Asian countries. This led authorities of various countries to place temazepam under a more restrictive legal status. Some countries, such as Sweden, banned the drug outright. Temazepam also has certain pharmacokinetic properties of absorption, distribution, elimination, and clearance that make it more apt to abuse compared to many other benzodiazepines.
Veterinary use.
Benzodiazepines are used in veterinary practice in the treatment of various disorders and conditions. As in humans, they are used in the first-line management of seizures, status epilepticus, and tetanus, and as maintenance therapy in epilepsy (in particular, in cats). They are widely used in small and large animals (including horses, swine, cattle and exotic and wild animals) for their anxiolytic and sedative effects, as pre-medication before surgery, for induction of anesthesia and as adjuncts to anesthesia.

</doc>
<doc id="4787" url="http://en.wikipedia.org/wiki?curid=4787" title="Bell curve">
Bell curve

Bell curve may refer to:

</doc>
<doc id="4788" url="http://en.wikipedia.org/wiki?curid=4788" title="Body mass index">
Body mass index

The body mass index (BMI), or Quetelet index, is a measure of relative weight based on an individual's mass and height.
Devised between 1830 and 1850 by the Belgian polymath Adolphe Quetelet during the course of developing "social physics", it is defined as the individual's body mass divided by the square of their height – with the value universally being given in units of kg/m2. 
BMI can also be determined using a table or from a chart which displays BMI as a function of mass and height using contour lines, or colors for different BMI categories, and may use two different units of measurement.
The BMI is used in a wide variety of contexts as a simple method to assess how much an individual's body weight departs from what is normal or desirable for a person of his or her height. There is however often vigorous debate, particularly regarding at which value of the BMI scale the threshold for "overweight" and "obese" should be set, but also about a range of perceived limitations and problems with the BMI.
Even though many other differently calculated ratios have been invented, others haven't been used as often.
Usage.
While the formula previously called the Quetelet Index for BMI dates to the beginning of the 19th century, the new term "body mass index" for the ratio and its popularity date to a paper published in the July edition of 1972 in the "Journal of Chronic Diseases" by Ancel Keys, which found the BMI to be the best proxy for body fat percentage among ratios of weight and height; the interest in measuring body fat being due to obesity becoming a discernible issue in prosperous Western societies. BMI was explicitly cited by Keys as being appropriate for "population" studies, and inappropriate for individual diagnosis. Nevertheless, due to its simplicity, it came to be widely used for individual diagnosis.
'Body mass index', a well known measure of adiposity, ranges from underweight to obesity and is commonly employed among children and adults to understand health outcomes. The BMI trait is influenced by both genetic and nongenetic factors, thus provides a paradigm to understand and estimate the risk factors for health problems.
'BMI' provides a simple numeric measure of a person's "thickness" or "thinness", allowing health professionals to discuss overweight and underweight problems more objectively with their patients. However, BMI has become controversial because many people, including physicians, have come to rely on its apparent numerical authority for medical diagnosis, but that was never the BMI's purpose; it is meant to be used as a simple means of classifying sedentary (physically inactive) individuals, or rather, populations, with an average body composition. For these individuals, the current value settings are as follows: a BMI of 18.5 to 25 may indicate optimal weight, a BMI lower than 18.5 suggests the person is underweight, a number above 25 may indicate the person is overweight, a number above 30 suggests the person is obese.
For a given height, BMI is proportional to mass. However, for a given mass, BMI is inversely proportional to the square of the height. So, if all body dimensions double, and mass scales naturally with the cube of the height, then BMI doubles instead of remaining the same. This results in taller people having a reported BMI that is uncharacteristically high compared to their actual body fat levels. In comparison, the Ponderal index is based on this natural scaling of mass with the third power of the height. However, many taller people are not just "scaled up" short people, but tend to have narrower frames in proportion to their height. Nick Korevaar (a mathematics lecturer from the University of Utah) suggests that instead of squaring the body height (an exponent of 2, as the BMI does) or cubing the body height (an exponent of 3, as the Ponderal index does), it would be more appropriate to use an exponent of between 2.3 and 2.7 (as originally noted by Quetelet). (For a theoretical basis for such values see MacKay.)
BMI Prime.
BMI Prime, a simple modification of the BMI system, is the ratio of actual BMI to upper limit BMI (currently defined at BMI 25). As defined, BMI Prime is also the ratio of body weight to upper body weight limit, calculated at BMI 25. Since it is the ratio of two separate BMI values, BMI Prime is a dimensionless number, without associated units. Individuals with BMI Prime less than 0.74 are underweight; those between 0.74 and 1.00 have optimal weight; and those at 1.00 or greater are overweight. BMI Prime is useful clinically because individuals can tell, at a glance, by what percentage they deviate from their upper weight limits. For instance, a person with BMI 34 has a BMI Prime of 34/25 = 1.36, and is 36% over his or her upper mass limit. In South East Asian and South Chinese populations (see international variation section below) BMI Prime should be calculated using an upper limit BMI of 23 in the denominator instead of 25. Nonetheless, BMI Prime allows easy comparison between populations whose upper limit BMI values differ.
Categories.
A frequent use of the BMI is to assess how much an individual's body weight departs from what is normal or desirable for a person of his or her height. The weight excess or deficiency may, in part, be accounted for by body fat (adipose tissue) although other factors such as muscularity also affect BMI significantly (see discussion below and overweight). The WHO regards a BMI of less than 18.5 as underweight and may indicate malnutrition, an eating disorder, or other health problems, while a BMI greater than 25 is considered overweight and above 30 is considered obese. These ranges of BMI values are valid only as statistical categories
BMI-for-age.
BMI is used differently for children. It is calculated the same way as for adults, but then compared to typical values for other children of the same age. Instead of set thresholds for underweight and overweight, then, the BMI percentile allows comparison with children of the same sex and age. A BMI that is less than the 5th percentile is considered underweight and above the 95th percentile is considered obese for people 20 and under. People under 20 with a BMI between the 85th and 95th percentile are considered to be overweight.
Recent studies in Britain have indicated that females between the ages 12 and 16 have a higher BMI than males of the same age by 1.0 kg/m2 on average.
International variations.
These recommended distinctions along the linear scale may vary from time to time and country to country, making global, longitudinal surveys problematic.
Hong Kong.
The Hospital Authority of Hong Kong recommends BMI as following:
Japan.
Japan Society for the Study of Obesity (2000)
Singapore.
In Singapore, the BMI cut-off figures were revised in 2005, motivated by studies showing that many Asian populations, including Singaporeans, have higher proportion of body fat and increased risk for cardiovascular diseases and diabetes mellitus, compared with Caucasians at the same BMI. The BMI cut-offs are presented with an emphasis on health risk rather than weight.
United States.
In 1998, the U.S. National Institutes of Health and the Centers for Disease Control and Prevention brought U.S. definitions into line with World Health Organization guidelines, lowering the normal/overweight cut-off from BMI 27.8 to BMI 25. This had the effect of redefining approximately 29 million Americans, previously "healthy" to "overweight". It also recommends lowering the normal/overweight threshold for South East Asian body types to around BMI 23, and expects further revisions to emerge from clinical studies of different body types.
The U.S. National Health and Nutrition Examination Survey of 1994 indicated that 59% of American men and 49% of women had BMIs over 25. Morbid obesity—a BMI of 40 or more—was found in 2% of the men and 4% of the women. The newest survey in 2007 indicates a continuation of the increase in BMI: 63% of Americans are overweight or obese, with 26% now in the obese category (a BMI of 30 or more). There are differing opinions on the threshold for being underweight in females; doctors quote anything from 18.5 to 20 as being the lowest index, the most frequently stated being 19. A BMI nearing 15 is usually used as an indicator for starvation and the health risks involved, with a BMI less than 17.5 being an informal criterion for the diagnosis of anorexia nervosa.
Health consequences of overweight and obesity in adults.
The BMI ranges are based on the relationship between body weight and disease and death. Overweight and obese individuals are at increased risk for many diseases and health conditions, including the following:
However recent research has shown that those classified as overweight, having a BMI between 25 and 29.9, show lower overall mortality than all other categories.
Applications.
Statistical device.
The BMI is generally used as a means of correlation between groups related by general mass and can serve as a vague means of estimating adiposity. The duality of the BMI is that, whilst easy-to-use as a general calculation, it is limited in how accurate and pertinent the data obtained from it can be. Generally, the index is suitable for recognizing trends within sedentary or overweight individuals because there is a smaller margin for errors.
This general correlation is particularly useful for consensus data regarding obesity or various other conditions because it can be used to build a semi-accurate representation from which a solution can be stipulated, or the RDA for a group can be calculated. Similarly, this is becoming more and more pertinent to the growth of children, due to the majority of their exercise habits.
The growth of children is usually documented against a BMI-measured growth chart. Obesity trends can be calculated from the difference between the child's BMI and the BMI on the chart.
Clinical practice.
BMI has been used by the WHO as the standard for recording obesity statistics since the early 1980s. In the United States, BMI is also used as a measure of underweight, owing to advocacy on behalf of those suffering with eating disorders, such as anorexia nervosa and bulimia nervosa.
BMI can be calculated quickly and without expensive equipment. However, BMI categories do not take into account many factors such as frame size and muscularity. The categories also fail to account for varying proportions of fat, bone, cartilage, water weight, and more.
Despite this, BMI categories are regularly regarded as a satisfactory tool for measuring whether sedentary individuals are "underweight", "overweight" or "obese" with various exemptions, such as: athletes, children, the elderly, and the infirm.
One basic problem, especially in athletes, is that muscle weight contributes to BMI. Some professional athletes would be "overweight" or "obese" according to their BMI, despite carrying little fat, unless the number at which they are considered "overweight" or "obese" is adjusted upward in some modified version of the calculation. In children and the elderly, differences in bone density and, thus, in the proportion of bone to total weight can mean the number at which these people are considered "under"weight should be adjusted downward.
Medical underwriting.
In the United States, where medical underwriting of private health insurance plans is widespread, most private health insurance providers will use a particular high BMI as a cut-off point in order to raise insurance rates for or deny insurance to higher-risk patients, thereby reducing the cost of insurance coverage to all other subscribers in a lower BMI range. The cutoff point is determined differently for every health insurance provider and different providers will have vastly different ranges of acceptability. Many will implement phased surcharges, in which the subscriber will pay an additional penalty, usually as a percentage of the monthly premium, based on membership in an actuarially determined risk tier corresponding to a given range of BMI points above a certain acceptable limit, up to a maximum BMI past which the individual will simply be denied admissibility regardless of price. This can be contrasted with group insurance policies which do not require medical underwriting and where insurance admissibility is guaranteed by virtue of being a member of the insured group, regardless of BMI or other risk factors that would likely render the individual inadmissible to an individual health plan.
Limitations and shortcomings.
The medical establishment has acknowledged shortcomings of BMI. Because the BMI depends upon weight and the square of height, it ignores basic scaling laws whereby mass increases to the 3rd power of linear dimensions. Hence, larger individuals, even if they had exactly the same body shape and relative composition, always have a larger BMI. Also, its assumptions about the distribution between lean mass and adipose tissue are inexact. BMI generally overestimates adiposity on those with more lean body mass (e.g., athletes) and underestimates excess adiposity on those with less lean body mass. A study in June 2008 by Romero-Corral et al. examined 13,601 subjects from the United States' third National Health and Nutrition Examination Survey (NHANES III) and found that BMI-defined obesity (BMI > 30) was present in 21% of men and 31% of women. Using body fat percentages (BF%), however, BF%-defined obesity was found in 50% of men and 62% of women. While BMI-defined obesity showed high specificity (95% for men and 99% for women), BMI showed poor sensitivity (36% for men and 49% for women). Despite this undercounting of obesity by BMI, BMI values in the intermediate BMI range of 20–30 were found to be associated with a wide range of body fat percentages. For men with a BMI of 25, about 20% have a body fat percentage below 20% and about 10% have body fat percentage above 30%.
Mathematician Keith Devlin and the restaurant industry association Center for Consumer Freedom argue that the error in the BMI is significant and so pervasive that it is not generally useful in evaluation of health. University of Chicago political science professor Eric Oliver says BMI is a convenient but inaccurate measure of weight, forced onto the populace, and should be revised.
A study published by "Journal of the American Medical Association" ("JAMA") in 2005 showed that "overweight" people had a similar relative risk of mortality to "normal" weight people as defined by BMI, while "underweight" and "obese" people had a higher death rate. High BMI is associated with type 2 diabetes only in persons with high serum gamma-glutamyl transpeptidase.
In an analysis of 40 studies involving 250,000 people, patients with coronary artery disease with "normal" BMIs were at higher risk of death from cardiovascular disease than people whose BMIs put them in the "overweight" range (BMI 25–29.9). In the "overweight", or intermediate, range of BMI (25–29.9), the study found that BMI failed to discriminate between bodyfat percentage and lean mass. The study concluded that "the accuracy of BMI in diagnosing obesity is limited, particularly for individuals in the intermediate BMI ranges, in men and in the elderly. These results may help to explain the unexpected better survival in overweight/mild obese patients."
A 2010 study that followed 11,000 subjects for up to eight years concluded that BMI is not a good measure for the risk of heart attack, stroke or death. A better measure was found to be the waist-to-height ratio. A 2011 study that followed 60,000 participants for up to 13 years found that waist–hip ratio was a better predictor of ischaemic heart disease mortality.
BMI is particularly inaccurate for people who are very fit or athletic, as their high muscle mass can classify them in the "overweight" category by BMI, even though their body fat percentages frequently fall in the 10–15% category, which is below that of a more sedentary person of average build who has a "normal" BMI number. Body composition for athletes is often better calculated using measures of body fat, as determined by such techniques as skinfold measurements or underwater weighing and the limitations of manual measurement have also led to new, alternative methods to measure obesity, such as the body volume index. However, recent studies of American football linemen who undergo intensive weight training to increase their muscle mass show that they frequently suffer many of the same problems as people ordinarily considered obese, notably sleep apnea.
BMI also does not account for body frame size; a person may have a small frame and be carrying more fat than optimal, but their BMI reflects that they are "normal". Conversely, a large framed individual may be quite healthy with a fairly low body fat percentage, but be classified as "overweight" by BMI. Accurate frame size calculators use several measurements (wrist circumference, elbow width, neck circumference and others) to determine what category an individual falls into for a given height. The standard is to use frame size in conjunction with ideal height/weight charts and add roughly 10% for a large frame or subtract roughly 10% for a smaller frame.
For example, a chart may say the ideal weight for a man is . But if that man has a slender build (small frame), he may be overweight at and should reduce by 10%, to roughly . In the reverse, the man with a larger frame and more solid build can be quite healthy at . If one teeters on the edge of small/medium or medium/large, a dose of common sense should be used in calculating their ideal weight. However, falling into your ideal weight range for height and build is still not as accurate in determining health risk factors as waist/height ratio and actual body fat percentage.
A further limitation of BMI relates to loss of height through aging. In this situation, BMI will increase without any corresponding increase in weight.
The exponent of 2 in the denominator of the formula for BMI is arbitrary. It is meant to reduce variability in the BMI associated only with a difference in size, rather than with differences in weight relative to one's ideal weight. If taller people were simply scaled-up versions of shorter people, the appropriate exponent would be 3, as weight would increase with the cube of height. However, on average, taller people have a slimmer build relative to their height than do shorter people, and the exponent which matches the variation best is less than 3. An analysis based on data gathered in the US suggested an exponent of 2.6 would yield the best fit for children aged 2 to 19 years old. For US adults, exponent estimates range from 1.92 to 1.96 for males and from 1.45 to 1.95 for females. The exponent 2 is used by convention and for simplicity.
As a possible alternative to BMI, the concepts fat-free mass index (FFMI) and fat mass index (FMI) were introduced in the early 1990s, and Body Shape Index in 2012.
Varying standards.
It is not clear where on the BMI scale the threshold for "overweight" and "obese" should be set. Because of this the standards have varied over the past few decades. Between 1980 and 2000 the U.S. Dietary Guidelines have defined overweight at a variety of levels ranging from a BMI of 24.9 to 27.1. In 1985 the National Institutes of Health (NIH) consensus conference recommended that overweight BMI be set at a BMI of 27.8 for men and 27.3 for women. In 1998 a NIH report concluded that a BMI over 25 is overweight and a BMI over 30 is obese. In the 1990s the World Health Organization (WHO) decided that a BMI of 25 to 30 should be considered overweight and a BMI over 30 is obese, the standards the NIH set. This became the definitive guide for determining if someone is overweight.
The current WHO and NIH ranges of "normal" weights are proved to be associated with decreased risks of some diseases such as diabetes type II; however using the same range of BMI for men and women is considered arbitrary, and makes the definition of underweight quite unsuitable for men.
Global statistics.
Researchers at the London School of Hygiene & Tropical Medicine calculated the average BMI for 177 countries using UN data on population, WHO estimates of global weight, and mean height from national health examination surveys.
See also.
Other measures of obesity:

</doc>
<doc id="4789" url="http://en.wikipedia.org/wiki?curid=4789" title="Behistun Inscription">
Behistun Inscription

The Behistun Inscription (also Bistun or Bisutun), (Persian: بیستون, Old Persian: Bagastana, meaning "the place of god") is a multi-lingual inscription located on Mount Behistun in the Kermanshah Province of Iran, near the city of Kermanshah in western Iran. It was crucial to the decipherment of cuneiform script.
Authored by Darius the Great sometime between his coronation as king of the Persian Empire in the summer of 522 BC and his death in autumn of 486 BC, the inscription begins with a brief autobiography of Darius, including his ancestry and lineage. Later in the inscription, Darius provides a lengthy sequence of events following the deaths of Cyrus the Great and Cambyses II in which he fought nineteen battles in a period of one year (ending in December 521 BC) to put down multiple rebellions throughout the Persian Empire. The inscription states in detail that the rebellions, which had resulted from the deaths of Cyrus the Great and his son Cambyses II, were orchestrated by several impostors and their co-conspirators in various cities throughout the empire, each of whom falsely proclaimed kinghood during the upheaval following Cyrus's death.
Darius the Great proclaimed himself victorious in all battles during the period of upheaval, attributing his success to the "grace of Ahura Mazda".
The inscription includes three versions of the same text, written in three different cuneiform script languages: Old Persian, Elamite, and Babylonian (a later form of Akkadian). In effect, then, the inscription is to cuneiform what the Rosetta Stone is to Egyptian hieroglyphs: the document most crucial in the decipherment of a previously lost script.
The inscription is approximately 15 metres high by 25 metres wide and 100 metres up a limestone cliff from an ancient road connecting the capitals of Babylonia and Media (Babylon and Ecbatana, respectively). The Old Persian text contains 414 lines in five columns; the Elamite text includes 593 lines in eight columns, and the Babylonian text is in 112 lines. The inscription was illustrated by a life-sized bas-relief of Darius I, the Great, holding a bow as a sign of kingship, with his left foot on the chest of a figure lying on his back before him. The supine figure is reputed to be the pretender Gaumata. Darius is attended to the left by two servants, and nine one-metre figures stand to the right, with hands tied and rope around their necks, representing conquered peoples. Faravahar floats above, giving his blessing to the king. One figure appears to have been added after the others were completed, as was Darius's beard, which is a separate block of stone attached with iron pins and lead.
History.
After the fall of the Persian Empire's Achaemenid Dynasty and its successors, and the lapse of Old Persian cuneiform writing into disuse, the nature of the inscription was forgotten, and fanciful explanations became the norm. For centuries, instead of being attributed to Darius I, the Great, it was believed to be from the reign of Khosrau II of Persia — one of the last Sassanid kings, who lived over 1000 years after the time of Darius I.
The inscription is mentioned by Ctesias of Cnidus, who noted its existence some time around 400 BC and mentioned a well and a garden beneath the inscription. He incorrectly concluded that the inscription had been dedicated "by Queen Semiramis of Babylon to Zeus". Tacitus also mentions it and includes a description of some of the long-lost ancillary monuments at the base of the cliff, including an altar to "Herakles". What has been recovered of them, including a statue dedicated in 148 BC, is consistent with Tacitus's description. Diodorus also writes of "Bagistanon" and claims it was inscribed by Semiramis.
A legend began around Mount Behistun (Bisutun), as written about by the Persian poet and writer Ferdowsi in his "Shahnameh" ("Book of Kings") , about a man named Farhad, who was a lover of King Khosrow's wife, Shirin. The legend states that, exiled for his transgression, Farhad was given the task of cutting away the mountain to find water; if he succeeded, he would be given permission to marry Shirin. After many years and the removal of half the mountain, he did find water, but was informed by Khosrow that Shirin had died. He went mad, threw his axe down the hill, kissed the ground and died. It is told in the book of Khosrow and Shirin that his axe was made out of a pomegranate tree, and, where he threw the axe, a pomegranate tree grew with fruit that would cure the ill. Shirin was not dead, according to the story, and mourned upon hearing the news.
In 1598, the Englishman Robert Sherley saw the inscription during a diplomatic mission to Persia on behalf of Austria, and brought it to the attention of Western European scholars. His party incorrectly came to the conclusion that it was Christian in origin.
French General Gardanne thought it showed "Christ and his twelve apostles", and Sir Robert Ker Porter thought it represented the Lost Tribes of Israel and Shalmaneser of Assyria. 
Italian explorer Pietro della Valle visited the inscription in the course of a pilgrimage in around 1621CE.
Translation.
German surveyor Carsten Niebuhr visited in around 1764 for Frederick V of Denmark, publishing a copy of the inscription in the account of his journeys in 1778.
Niebuhr's transcriptions were used by Georg Friedrich Grotefend and others in their efforts to decipher the Old Persian cuneiform script. Grotefend had deciphered ten of the 37 symbols of Old Persian by 1802, after realizing that unlike the Semitic cuneiform scripts, Old Persian text is alphabetic and each word is separated by a vertical slanted symbol.
The Old Persian text was copied and deciphered before the recovery and copying of the Elamite and Babylonian inscriptions had even been attempted, which proved to be a good deciphering strategy, since Old Persian script was easier to study due to its alphabetic nature and the fact that the language it represents had naturally evolved into Middle Persian, and in turn, to the living modern Persian language dialects, and was also related to the Avestan language, used in the Zoroastrian book the "Avesta".
In 1835, Sir Henry Rawlinson, an officer of the British East India Company army assigned to the forces of the Shah of Iran, began studying the inscription in earnest. As the town of Bisutun's name was anglicized as "Behistun" at this time, the monument became known as the "Behistun Inscription". Despite its relative inaccessibility, Rawlinson was able to scale the cliff and copy the Old Persian inscription. The Elamite was across a chasm, and the Babylonian four meters above; both were beyond easy reach and were left for later.
With the Persian text, and with about a third of the syllabary made available to him by the work of Georg Friedrich Grotefend, Rawlinson set to work on deciphering the text. Fortunately, the first section of this text contained a list of the same Persian kings found in Herodotus in their original Persian forms as opposed to Herodotus's Greek transliterations; for example Darius is given as the original "Dâryavuš" instead of the Hellenized "Δαρειος". By matching the names and the characters, Rawlinson was able to decipher the type of cuneiform used for Old Persian by 1838 and presented his results to the Royal Asiatic Society in London and the Société Asiatique in Paris.
In the interim, Rawlinson spent a brief tour of duty in Afghanistan, returning to the site in 1843. He first crossed a chasm between the Persian and Elamite scripts by bridging the gap with planks, subsequently copying the Elamite inscription. He was then able to find an enterprising local boy to climb up a crack in the cliff and suspend ropes across the Babylonian writing, so that papier-mâché casts of the inscriptions could be taken. Rawlinson, along with several other scholars, most notably Edward Hincks, Julius Oppert, William Henry Fox Talbot, and Edwin Norris, either working separately or in collaboration, eventually deciphered these inscriptions, leading eventually to the ability to read them completely.
The translation of the Old Persian sections of the Behistun Inscription paved the way to the subsequent ability to decipher the Elamite and Babylonian parts of the text, which greatly promoted the development of modern Assyriology.
Later research and activity.
The site was visited by A. V. Williams Jackson in 1903.
Later expeditions, in 1904 sponsored by the British Museum and led by Leonard William King and Reginald Campbell Thompson and in 1948 by George G. Cameron of the University of Michigan, obtained photographs, casts and more accurate transcriptions of the texts, including passages that were not copied by Rawlinson.
It also became apparent that rainwater had dissolved some areas of the limestone in which the text was inscribed, while leaving new deposits of limestone over other areas, covering the text.
In 1938, the inscription became of interest to the Nazi German think tank Ahnenerbe, although research plans were cancelled due to the onset of World War II.
The monument later suffered some damage from Allied soldiers using it for target practice in World War II, during the Anglo-Soviet invasion of Iran.
In 1999, Iranian archeologists began the documentation and assessment of damages to the site incurred during the 20th century. Malieh Mehdiabadi, who was project manager for the effort, described a photogrammetric process by which two-dimensional photos were taken of the inscriptions using two cameras and later transmuted into 3-D images.
In recent years, Iranian archaeologists have been undertaking conservation works. The site became a UNESCO World Heritage Site in 2006.
In 2012, the Bisotun Cultural Heritage Center organized an international effort to re-examine the inscription.
Other historical monuments in the Behistun complex.
The site covers an area of 116 hectares. Archeological evidence indicates that this region became a human shelter 40,000 years ago. There are 18 historical monuments other than the inscription of Darius the Great in the Behistun complex that have been registered in the Iranian national list of historical sites. Some of them are:
In the first image, Herakles with curly hair and a beard rests on the lion skin. Beside him, an olive tree is seen carved on the wall, while a quiver full of arrows is hanging from it, and a club resting close by. Behind the head of Herakles, an inscription of seven lines in old Greek is written on a smooth space with a frame similar to Greek temples. According to this inscription, the statue was carved in 139 BC on the occasion of a conquest for Seleucid Greeks (under Demetrius II Nicator) against the Parthians (under Mithridates I of Parthia), though the Seleucids were later defeated and driven from the region.
The second image is a bas relief of Mithridates II of Parthia: this was carved in 123–110 BC and represents Parthian king Mithridates and four of his satraps who are respecting the king. "Bas relief of Gotarzes II of Parthia" shows the conquest of that king over Meherdates, an Arsacid prince who lived in Rome. An inscription in Greek is seen on the left side of the top outer frame of the relief. "Sheikh Ali khan Zangeneh text endowment": According to this text, written in Sloth calligraphy, Sheikh Ali khan Zangeneh, a local ruler of the 17th century, dedicates four shares (out of six) of his properties in Ghareh-vali and Chambatan (local villages) for Sadaats (descendants of the prophet Mohammad), and two remaining shares for the Bisotoun Safavid caravansarai.

</doc>
<doc id="4792" url="http://en.wikipedia.org/wiki?curid=4792" title="Barry Goldwater">
Barry Goldwater

Barry Morris Goldwater (January 2, 1909 – May 29, 1998) was a businessman and five-term United States Senator from Arizona (1953–65, 1969–87) and the Republican Party's nominee for president in the 1964 election. An articulate and charismatic figure during the first half of the 1960s, he was known as "Mr. Conservative".
Goldwater is the politician most often credited for sparking the resurgence of the American conservative political movement in the 1960s. He also had a substantial impact on the libertarian movement.
Goldwater rejected the legacy of the New Deal and fought through the conservative coalition against the New Deal coalition. He mobilized a large conservative constituency to win the hard-fought Republican primaries. Goldwater's conservative campaign platform ultimately failed to gain the support of the electorate and he lost the 1964 presidential election to incumbent Democrat Lyndon B. Johnson by one of the largest landslides in history, bringing down many Republican candidates as well. The Johnson campaign and other critics painted him as a reactionary, while supporters praised his crusades against the Soviet Union, labor unions, and the welfare state. His defeat allowed Johnson and the Democrats in Congress to pass the Great Society programs, but the defeat of so many older Republicans in 1964 also cleared the way for a younger generation of American conservatives to mobilize. Goldwater was much less active as a national leader of conservatives after 1964; his supporters mostly rallied behind Ronald Reagan, who became governor of California in 1967 and the 40th President of the United States in 1981.
Goldwater returned to the Senate in 1969, and specialized in defense policy, bringing to the table his experience as a senior officer in the Air Force Reserve. In 1974, as an elder statesman of the party, Goldwater successfully urged President Richard Nixon to resign when evidence of a cover-up in the Watergate scandal became overwhelming and impeachment was imminent. By the 1980s, the increasing influence of the Christian right on the Republican Party so conflicted with Goldwater's views that he became a vocal opponent of the religious right on issues such as abortion, gay rights, and the role of religion in public life. A significant accomplishment in his career was the passage of the Goldwater–Nichols Act of 1986, which restructured the higher levels of the Pentagon by placing the chain of command from the President to the Secretary of Defense directly to the commanders of the Unified Combatant Commands.
Personal life.
Goldwater was born in Phoenix, in what was then the Arizona Territory, the son of Baron M. Goldwater and his wife, Hattie Josephine ("JoJo") Williams. His father's Jewish family had founded Goldwater's, the largest department store in Phoenix. The family name had been changed from "Goldwasser" to "Goldwater" at least as early as the 1860 census in Los Angeles, California. Goldwater's paternal grandparents, Michel and Sarah (Nathan) Goldwasser, had been married in the Great Synagogue of London. Goldwater's mother, who was Protestant, came from a New England family that included the famous theologian, Roger Williams of Rhode Island. Goldwater's parents were married in an Episcopal church in Phoenix; for his entire life, Goldwater was an Episcopalian, though on rare occasions he referred to himself as "Jewish". While he did not often attend church, he stated that "If a man acts in a religious way, an ethical way, then he's really a religious man—and it doesn't have a lot to do with how often he gets inside a church".
The family department store made the Goldwaters comfortably wealthy. Goldwater graduated from Staunton Military Academy, an elite private school in Virginia, and attended the University of Arizona for one year, where he joined the Sigma Chi fraternity. Barry had never been close to his father, but he took over the family business after Baron's death in 1930. He became a Republican (in a heavily Democratic state), promoted innovative business practices, and opposed the New Deal, especially because it fostered labor unions. Goldwater came to know former president Herbert Hoover, whose conservative politics he admired greatly. In 1934, he married Margaret "Peggy" Johnson, wealthy daughter of a prominent industrialist from Muncie, Indiana. They had four children: Joanne (born January 1, 1936), Barry (born July 15, 1938), Michael (born March 15, 1940), and Peggy (born July 27, 1944). Goldwater became a widower in 1985, and in 1992 he married Susan Wechsler, a nurse 32 years his junior.
With the American entry into World War II, Goldwater received a reserve commission in the United States Army Air Forces. He became a pilot assigned to the Ferry Command, a newly formed unit that flew aircraft and supplies to war zones worldwide. He spent most of the war flying between the USA and India, via the Azores and North Africa or South America, Nigeria, and Central Africa. He also flew "the hump" over the Himalayas to deliver supplies to the Republic of China. Remaining in the Air Force Reserve after the war, he eventually retired as a command pilot with the rank of major general. By that time, he had flown 165 different types of aircraft. Following World War II, Goldwater was a leading proponent of creating the United States Air Force Academy, and later served on the Academy's Board of Visitors. The visitor center at the USAF Academy is now named in his honor. As a colonel he also founded the Arizona Air National Guard, and he would desegregate it two years before the rest of the US military. Goldwater was instrumental in pushing the Pentagon to support desegregation of the armed services. Goldwater retired as an Air Force major general, and he continued piloting B-52 aircraft until late in his military career. To those who called him "rash", he would remind people of the old saying that "there are no old, bold pilots".
In 1940, Goldwater became one of the first people to run the Colorado River recreationally through Grand Canyon when he participated as an oarsman on Norman Nevills' second commercial river trip. Goldwater joined the trip in Green River, Utah and rowed his own boat down to Lake Mead. Goldwater ran track and cross country in high school, where he specialized in the 880 yard run. His parents strongly encouraged him to compete in these sports, to Goldwater's dismay. He often went by the nickname of "Rolling Thunder."
In 1963, he joined the Arizona Society of the Sons of the American Revolution.
In 1970, the Arizona Historical Foundation published the daily journal that Goldwater maintained on the Grand Canyon trip, along with the photographs he took, in a 209 page volume titled .
Goldwater's son Barry Goldwater, Jr. served as a United States House of Representatives member from California from 1969 to 1983.
Political career.
In a heavily Democratic state Goldwater became a conservative Republican and a friend of Herbert Hoover. He was outspoken against New Deal liberalism, especially its close ties to labor unions that he considered corrupt. A pilot, outdoorsman and photographer, he criss-crossed Arizona and developed a deep interest in both the natural history and the human history of the state. He entered Phoenix politics in 1949 when he was elected to the City Council as part of a nonpartisan team of candidates who promised to clean up widespread prostitution and gambling. The group won every mayoral and council election for two decades. Goldwater rebuilt the weak Republican party and was instrumental in electing Howard Pyle as governor in 1950.
As a Republican he won a seat in the US Senate in 1952, when he upset veteran Democrat and Senate majority leader Ernest McFarland. He defeated McFarland again in 1958, with a strong showing in his first reelection in a year in which the Democrats picked up 13 seats in the Senate. He gave up a re-election run for the Senate in 1964 in favor of his presidential campaign.
During his Senate career, Goldwater was regarded as the "Grand Old Man of the Republican Party and one of the nation's most respected exponents of conservatism."
Policies.
Goldwater soon became most associated with labor-union reform and anti-communism; he was an active supporter of the conservative coalition in Congress. However, he rejected the wildest fringes of the anti-communist movement; in 1956, he sponsored the passage through the Senate of the final version of the Alaska Mental Health Enabling Act, despite vociferous opposition from opponents who claimed that the Act was a communist plot to establish concentration camps in Alaska. His work on labor issues led to Congress passing major anti-corruption reforms in 1957, and an all-out campaign by the AFL-CIO to defeat his 1958 reelection bid. He voted against the censure of Senator Joseph McCarthy in 1954, but he never actually charged any individual with being a communist/Soviet agent. Goldwater emphasized his strong opposition to the worldwide spread of communism in his 1960 book "The Conscience of a Conservative". The book became an important reference text in conservative political circles.
In 1964, Goldwater ran a conservative campaign that emphasized states' rights. Goldwater's 1964 campaign was a magnet for conservatives since he opposed interference by the federal government in state affairs. Although he had supported all previous federal civil rights legislation and had supported the original senate version of the bill, Goldwater made the decision to oppose the Civil Rights Act of 1964. His stance was based on his view that the act was an intrusion of the federal government into the affairs of states and that the Act interfered with the rights of private persons to do or not do business with whomever they chose. In the segregated city of Phoenix in the 1950s, he had quietly supported civil rights for blacks, but would not let his name be used.
All this appealed to white Southern Democrats, and Goldwater was the first Republican to win the electoral votes of all of the Deep South states (South Carolina, Georgia, Alabama, Mississippi and Louisiana) since Reconstruction (although Dwight Eisenhower did carry Louisiana in 1956). However, Goldwater's vote on the Civil Rights Act proved devastating to his campaign everywhere outside the South (besides Dixie, Goldwater won only in Arizona, his home state), contributing to his landslide defeat in 1964.
While Goldwater had been depicted by his opponents in the Republican primaries as a representative of a conservative philosophy that was extreme and alien, his voting records show that his positions were in harmony with those of his fellow Republicans in the Congress. What distinguished him from his predecessors was, according to Hans J. Morgenthau, his firmness of principle and determination, which did not allow him to be content with mere rhetoric.
Goldwater fought in 1971 to stop US funding of the United Nations after the People's Republic of China was admitted to the organization. He said:
Elections.
In 1964, he fought and won a bitterly contested, multi-candidate race for the Republican Party's presidential nomination. His main rival was New York Governor Nelson Rockefeller, whom he defeated by a narrow margin in the bitterly fought California primary. His nomination was opposed by liberal Republicans, who thought Goldwater's demand for rollback, defeat of the Soviet Union, would foment a nuclear war. Goldwater lost to President Lyndon Johnson by a massive landslide, pulling down the GOP, which lost many seats in both houses of Congress. Goldwater carried only his home state and five Deep South states.
Goldwater remained popular in Arizona, and in the 1968 Senate election he was elected (this time) to the seat of retiring Senator Carl Hayden. He was subsequently reelected in 1974 and 1980. The 1974 election saw Goldwater easily reelected. His final campaign in 1980 was close, with Goldwater winning in a near draw against Democratic challenger Bill Schulz. Goldwater said later that the close result convinced him not to run again.
Two self-published books advanced the Goldwater cause: and by a Texas historian. Both were best-sellers but failed to bolster Goldwater's electoral prospects.
Retirement.
Goldwater seriously considered retirement in 1980 before deciding to run for reelection. Peggy Goldwater reportedly hoped that her husband's Senate term, due to end in January 1981, would be his last. Goldwater decided to run, planning to make the term his last in the Senate. Goldwater faced a surprisingly tough battle for reelection. He was viewed by some as out of touch and vulnerable for several reasons; most importantly, because he had planned to retire in 1981, Goldwater had not visited many areas of Arizona outside of Phoenix and Tucson. He was also challenged by a formidable opponent, Bill Schulz, a former Republican turned Democrat and a wealthy real estate developer. Schulz was able to infuse massive amounts of money into the campaign from his own fortune.
Arizona's changing population also hurt Goldwater. The state's population had soared, and a huge portion of the electorate had not lived in the state when Goldwater was previously elected; hence, many voters were less familiar with Goldwater's actual beliefs, and he was on the defensive for much of the campaign. Early returns on election night seemed to indicate that Schulz would win. The counting of votes continued through the night and into the next morning. At around daybreak, Goldwater learned that he had been reelected thanks to absentee ballots, which were among the last to be counted. Goldwater's surprisingly close victory in 1980 came despite Reagan's 61% landslide over Jimmy Carter in Arizona. Republicans regained control of the Senate, putting Goldwater in the most powerful position he ever had in the Senate.
Goldwater retired in 1987, serving as chair of the Senate Intelligence and Armed Services Committees in his final term. Despite his reputation as a firebrand in the 1960s, by the end of his career he was considered a stabilizing influence in the Senate, one of the most respected members of either major party. Though Goldwater remained staunchly anti-communist and "hawkish" on military issues, he was a key supporter of the fight for ratification of the Panama Canal Treaty in the 1970s, which would give control of the canal zone to the Republic of Panama. His most important legislative achievement may have been the Goldwater-Nichols Act, which reorganized the US military's senior-command structure.
Goldwater was an active amateur radio "ham" operator, with call letters K7UGA. Many hams were delighted to make contact with him in his retirement years.
Political relationships.
Goldwater was grief-stricken by the assassination of Kennedy and was greatly disappointed that his opponent in the 1964 race would not be Kennedy but instead Kennedy's Vice President, the former Senate Majority Leader Lyndon B. Johnson of Texas. Goldwater disliked Johnson (who he said "used every dirty trick in the bag"), and Richard M. Nixon of California (whom he later called "the most dishonest individual I have ever met in my life"). Goldwater, after he had again become a senator, advocated for Nixon to resign at the height of Watergate, warning that fewer than ten senators would vote against conviction after Nixon was impeached by the House of Representatives. The term "Goldwater moment" has subsequently been used to describe situations when influential members of Congress disagree so strongly with a president from their own party that they rise up and take a stand in opposition.
His 1984 Cable Franchise Policy and Communications Act allowed local governments to require the transmission of public, educational, and government access (PEG) channels, barred cable operators from exercising editorial control over content of programs carried on PEG channels, and absolved them from liability for their content.
U.S. presidential campaign, 1964.
At the time of Goldwater's presidential candidacy, the Republican Party was split between its conservative wing (based in the West and South) and moderate/liberal wing (based in the Northeast). He alarmed even some of his fellow partisans with his brand of staunch fiscal conservatism and militant anti-communism. He was viewed by many traditional Republicans as being too far on the right wing of the political spectrum to appeal to the mainstream majority necessary to win a national election. As a result, moderate Republicans recruited a series of opponents, including New York Governor Nelson Rockefeller, Henry Cabot Lodge, Jr., of Massachusetts and Pennsylvania Governor William Scranton, to challenge Goldwater. Goldwater would defeat Rockefeller in the winner-take-all California primary and secure the nomination. He also had a solid backing from Southern Republicans. A bright young Birmingham lawyer, John Grenier, secured commitments from 271 of 279 southern convention delegates to back Goldwater. Grenier went on to serve as executive director of the national GOP during the Goldwater campaign. This was the Number 2 position to party chairman Dean Burch, Goldwater's fellow Arizonan.
Goldwater famously declared in his bold acceptance speech at the 1964 Republican Convention, "I would remind you that extremism in the defense of liberty is no vice. And let me remind you also that moderation in the pursuit of justice is no virtue." This paraphrase of Cicero was included at the suggestion of Harry V. Jaffa, though the speech was primarily written by Karl Hess. Because of President Johnson's popularity, however, Goldwater held back from attacking the president directly; he did not even mention Johnson by name in his convention speech.
Past comments came back to haunt Goldwater throughout his campaign. Once he had called the Eisenhower administration "a dime-store New Deal", and the former president never fully forgave him. Eisenhower did, however, film a television commercial with Goldwater. Eisenhower qualified his voting for Goldwater in November by remarking that he had voted not specifically for Goldwater, but for the Republican Party. In December 1961, Goldwater had told a news conference that "sometimes I think this country would be better off if we could just saw off the Eastern Seaboard and let it float out to sea". That comment boomeranged on him during the campaign in the form of a Johnson television commercial, as did remarks about making Social Security voluntary, and statements in Tennessee about selling the Tennessee Valley Authority, a large local New Deal employer.
The Goldwater campaign spotlighted Ronald Reagan, who appeared in a campaign ad. In turn, Reagan gave a stirring, nationally televised speech, "A Time for Choosing", in support of Goldwater. The speech prompted Reagan to seek the California Governorship in 1966 and jump-started his political career. Conservative activist Phyllis Schlafly, later well known for her fight against the Equal Rights Amendment, first became known for writing a pro-Goldwater book, "A Choice, Not an Echo", attacking the moderate Republican establishment.
Former U.S. Senator Prescott Bush, a moderate Republican from Connecticut, was a friend of Goldwater and supported him in the general election campaign. Bush's son, George H. W. Bush (then running for the Senate from Texas against Democrat Ralph Yarborough), was also a strong Goldwater supporter in both the nomination and general election campaigns.
Future Chief Justice of the United States and fellow Arizonan William H. Rehnquist also first came to the attention of national Republicans through his work as a legal adviser to Goldwater's presidential campaign. Rehnquist had begun his law practice in 1953 in the firm of Denison Kitchel of Phoenix, Goldwater's national campaign manager and friend of nearly three decades.
Goldwater was painted as a dangerous figure by the Johnson campaign, which countered Goldwater's slogan "In your heart, you know he's right" with the lines "In your guts, you know he's nuts", and "In your heart, you know he might" (that is, he might actually use nuclear weapons as opposed to using only deterrence). Johnson himself did not mention Goldwater in his own acceptance speech at the 1964 Democratic National Convention.
Goldwater's provocative advocacy of aggressive tactics to prevent the spread of communism in Asia led to effective counterattacks from Lyndon B. Johnson and his supporters, who claimed that Goldwater's militancy would have dire consequences, possibly even nuclear war. In a May 1964 speech, Goldwater suggested that nuclear weapons should be treated more like conventional weapons and used in Vietnam, specifically that they should have been used at Dien Bien Phu in 1954 to defoliate trees. Regarding Vietnam, Goldwater charged that Johnson's policy was devoid of "goal, course, or purpose", leaving "only sudden death in the jungles and the slow strangulation of freedom". Goldwater's rhetoric on nuclear war was viewed by many as quite uncompromising, a view buttressed by off-hand comments such as, "Let's lob one into the men's room at the Kremlin." He also advocated that field commanders in Vietnam and Europe should be given the authority to use tactical nuclear weapons (which he called "small conventional nuclear weapons") without presidential confirmation.
Goldwater did his best to counter the Johnson attacks, criticizing the Johnson administration for its perceived ethical lapses, and stating in a commercial that "we, as a nation, are not far from the kind of moral decay that has brought on the fall of other nations and people... I say it is time to put conscience back in government. And by good example, put it back in all walks of American life." Goldwater campaign commercials included statements of support by actor Raymond Massey and moderate Republican senator Margaret Chase Smith.
Before the 1964 election, the muckraking "Fact" magazine, published by Ralph Ginzburg, ran a special issue titled "The Unconscious of a Conservative: A Special Issue on the Mind of Barry Goldwater". The two main articles contended that Goldwater was mentally unfit to be president. The magazine attempted to support this claim with the results of an unscientific poll of psychiatrists it had conducted. "Fact" had mailed questionnaires to 12,356 psychiatrists, and published a "sampling" of the comments made by the 2,417 psychiatrists who responded, of whom 1,189 said Goldwater was unfit to be president. Not one of the psychiatrists had actually interviewed Goldwater himself.
After the election, Goldwater sued the publisher, the editor and the magazine for libel in "Goldwater v. Ginzburg". "Although the jury awarded Goldwater only $1.00 in compensatory damages against all three defendants, it went on to award him punitive damages of $25,000 against Ginzburg and $50,000 against "Fact" magazine, Inc." According to Warren Boroson, then-managing editor of "Fact" and now a financial columnist, the main biography of Goldwater in the magazine was written by David Bar-Illan, the Israeli pianist.
Daisy.
A Democratic campaign advertisement known as Daisy showed a young girl counting daisy petals, from one to ten. Immediately following this scene, a voiceover counted down from ten to one. The child's face was shown as a still photograph followed by images of nuclear explosions and mushroom clouds. The campaign advertisement ended with a plea to vote for Johnson, implying that Goldwater (who was not mentioned by name) would provoke a nuclear war if he was elected. The advertisement, which featured only a few spoken words of narrative and relied on imagery for its emotional impact, was one of the most provocative in American political campaign history, and many analysts credit it as being the birth of the modern style of "negative political ads" on television. The ad aired only once and was immediately pulled, but was then shown many times by television stations.
Results.
Goldwater only won his home state of Arizona and five states in the Deep South, depicted in red. The Southern states, traditionally Democratic up to that time, voted Republican primarily as a statement of opposition to the Civil Rights Act, which had been passed by Johnson and the Northern Democrats, as well as the majority of Republicans in Congress, earlier that year.
In the end, Goldwater received 38.4% of the popular vote, and carried six states: Arizona (with 50.45% of the popular vote) and the core states of the Deep South: Alabama, Georgia, Louisiana, Mississippi, and South Carolina. In carrying Georgia by a margin of 54-45, Goldwater became the first Republican nominee to win the state.
In all, Johnson won an overwhelming 486 electoral votes, to Goldwater's 52. Goldwater, with his customary bluntness, remarked, "We would have lost even if Abraham Lincoln had come back and campaigned with us." He maintained later in life that he would have won the election if the country had not been in a state of extended grief (referring to the assassination of John F. Kennedy), and that it was simply not ready for a third President in just 14 months.
Goldwater's poor showing pulled down many supporters. Of the 57 Republican Congressmen who endorsed Goldwater before the convention, 20 were defeated for reelection, along with many promising young Republicans. On the other hand, the defeat of so many older politicians created openings for young conservatives to move up the ladder. While the loss of moderate Republicans was temporary—they were back by 1966—Goldwater also permanently pulled many conservative Southerners and white ethnics out of the New Deal Coalition.
"In the South, Goldwater broke through and won five states—the best showing in the region for a GOP candidate since Reconstruction. In Mississippi—where Franklin D. Roosevelt had won nearly 100 percent of the vote just 28 years earlier—Goldwater claimed a staggering 87 percent." It has frequently been argued that Goldwater's strong performance in Southern states previously regarded as Democratic strongholds foreshadowed a larger shift in electoral trends in the coming decades that would make the South a Republican bastion (an end to the "Solid South")—first in presidential politics and eventually at the congressional and state levels, as well.
Goldwater and the revival of American conservatism.
Although Goldwater was not as important in the American conservative movement as Ronald Reagan after 1965, he shaped and redefined the movement from the late 1950s to 1964. Arizona Senator John McCain, who had succeeded Goldwater in the Senate in 1987, summed up Goldwater's legacy, "He transformed the Republican Party from an Eastern elitist organization to the breeding ground for the election of Ronald Reagan." Columnist George Will remarked after the 1980 Presidential election that it took 16 years to count the votes from 1964 and Goldwater won.
The Republican Party recovered from the 1964 election debacle, picking up 47 seats in the House of Representatives in the 1966 mid-term election. Further Republican successes ensued, including Goldwater's return to the Senate in 1968. In January of that year, Goldwater wrote an article in the "National Review" "affirming that he [was] not against liberals, that liberals are needed as a counterweight to conservatism, and that he had in mind a fine liberal like Max Lerner".
Goldwater was a strong supporter of the environment. He explained his position in 1969:
Throughout the 1970s, as the conservative wing under Reagan gained control of the party, Goldwater concentrated on his Senate duties, especially in military affairs. He played little part in the election or administration of Richard Nixon, but he helped force Nixon's resignation in 1974. In 1976 he helped block Rockefeller's renomination as vice president. When Reagan challenged Ford for the presidential nomination in 1976, Goldwater endorsed Ford, looking for consensus rather than conservative idealism. As one historian notes, "The Arizonan had lost much of his zest for battle."
In 1979, when President Carter normalized relations with Communist China, Goldwater and some other senators sued him in the Supreme Court, arguing that the president could not terminate the Sino-American Mutual Defense Treaty with Republic of China (Taiwan) without the approval of Congress. The case, "Goldwater v. Carter", was dismissed by the court as a political question.
Later life.
By the 1980s, with Ronald Reagan as president and the growing involvement of the religious right in conservative politics, Goldwater's libertarian views on personal issues were revealed; he believed that they were an integral part of true conservatism. Goldwater viewed abortion as a matter of personal choice, not intended for government intervention.
As a passionate defender of personal liberty, he saw the religious right's views as an encroachment on personal privacy and individual liberties. In his 1980 Senate reelection campaign, Goldwater won support from religious conservatives but in his final term voted consistently to uphold legalized abortion and, in 1981, gave a speech on how he was angry about the bullying of American politicians by religious organizations, and would "fight them every step of the way". Goldwater also disagreed with the Reagan administration on certain aspects of foreign policy (for example, he opposed the decision to mine Nicaraguan harbors). Notwithstanding his prior differences with Dwight D. Eisenhower, Goldwater in a 1986 interview rated him the best of the seven Presidents with whom he had worked.
On May 12, 1986, Goldwater was presented with the Presidential Medal of Freedom by President Ronald Reagan.
After his retirement in 1987, Goldwater described the Arizona Governor Evan Mecham as "hardheaded" and called on him to resign, and two years later stated that the Republican party had been taken over by a "bunch of kooks".
He is a 1987 recipient of the Langley Gold Medal from the Smithsonian Institution. In 1988, in recognition of his career, Princeton University's American Whig-Cliosophic Society awarded Goldwater the James Madison Award for Distinguished Public Service.
In a 1994 interview with the "Washington Post", the retired senator said,
Goldwater visited the small town of Bowen, Illinois in 1989 to see first hand where his mother was raised.
In response to Moral Majority founder Jerry Falwell's opposition to the nomination of Sandra Day O'Connor to the Supreme Court, of which Falwell had said, "Every good Christian should be concerned", Goldwater retorted: "Every good Christian ought to kick Falwell right in the ass." (According to John Dean, Goldwater actually suggested that good Christians ought to kick Falwell in the "nuts", but the news media "changed the anatomical reference.") Goldwater also had harsh words for his one-time political protegé, President Reagan, particularly after the Iran-Contra Affair became public in 1986. Journalist Robert MacNeil, a friend of Goldwater's from the 1964 Presidential campaign, recalled interviewing him in his office shortly afterward. "He was sitting in his office with his hands on his cane... and he said to me, 'Well, aren't you going to ask me about the Iran arms sales?' It had just been announced that the Reagan administration had sold arms to Iran. And I said, 'Well, if I asked you, what would you say?' He said, 'I'd say it's the god-damned stupidest foreign policy blunder this country's ever made!'", though aside from the Iran-Contra scandal, Goldwater thought nonetheless that Reagan was a good president. In 1988 during that year's presidential campaign, he pointedly told vice-presidential nominee Dan Quayle at a campaign event in Arizona "I want you to go back and tell George Bush to start talking about the issues."
Some of Goldwater's statements in the 1990s alienated many social conservatives. He endorsed Democrat Karan English in an Arizona congressional race, urged Republicans to lay off Bill Clinton over the Whitewater scandal, and criticized the military's ban on homosexuals: "Everyone knows that gays have served honorably in the military since at least the time of Julius Caesar." He also said, "You don't need to be 'straight' to fight and die for your country. You just need to shoot straight." A few years before his death he addressed establishment Republicans by saying, "Do not associate my name with anything you do. You are extremists, and you've hurt the Republican party much more than the Democrats have."
In 1996, he told Bob Dole, whose own presidential campaign received lukewarm support from conservative Republicans: "We're the new liberals of the Republican party. Can you imagine that?" In that same year, with Senator Dennis DeConcini, Goldwater endorsed an Arizona initiative to legalize medical marijuana against the countervailing opinion of social conservatives.
Hobbies and interests.
Amateur radio.
Goldwater was an avid amateur radio operator from the early 1920s onwards, with the call signs 6BPI, K3UIG and K7UGA. The latter is now used by an Arizona club honoring him as a commemorative call. During the Vietnam War, he spent many hours giving servicemen overseas the ability to talk to their families at home over the Military Affiliate Radio System (MARS).
Goldwater was also a prominent spokesman for amateur radio and its enthusiasts. Beginning in 1969 up to his death he appeared in numerous educational and promotional films (and later videos) about the hobby that were produced for the American Radio Relay League (the United States national society representing the interests of radio amateurs) by such producers as Dave Bell (W6AQ), ARRL Southwest Director John R. Griggs (W6KW), Alan Kaul (W6RCL), Forrest Oden (N6ENV), and the late Roy Neal (K6DUE). His first appearance was in Dave Bell's "The World of Amateur Radio" where Goldwater discussed the history of the hobby and demonstrated a live contact with Antarctica. His last on-screen appearance dealing with "ham radio" was in 1994, explaining a then-upcoming, Earth-orbiting ham radio relay satellite.
Electronics were a hobby for Goldwater beyond amateur radio. He enjoyed assembling Heathkits, completing more than 100 and often visiting their maker in Benton Harbor, Michigan to buy more, before the company exited the kit business in 1992.
Kachina dolls.
In 1916, Goldwater visited the Hopi Reservation with Phoenix architect John Rinker Kibby, and obtained his first kachina doll. Eventually his doll collection included 437 items and was presented in 1969 to the Heard Museum in Phoenix.
Photography.
Goldwater was an accomplished amateur photographer and in his estate left some 15,000 of his images to three Arizona institutions. He was very keen on candid photography. He got started in photography after receiving a camera as a gift from his wife on their first Christmas together. He was known to use a 4x5 Graflex, Rolleiflex camera,16 MM Bell and Howell motion picture camera, and Nikon FT 35 mm. He was a member of the Royal Photographic Society from 1941 becoming a Life Member in 1948.
For decades, he contributed photographs of his home state to "Arizona Highways" and was best known for his Western landscapes and pictures of native Americans in the United States. Three books with his photographs are "People and Places", from 1967; "Barry Goldwater and the Southwest", from 1976; and "Delightful Journey", first published in 1940 and reprinted in 1970. Ansel Adams wrote a foreword to the 1976 book.
Goldwater's photography interests occasionally crossed over with his political career, as well. John F. Kennedy, as president, was known to invite former congressional colleagues to the White House for a drink. On one occasion, Goldwater brought his camera and photographed President Kennedy. When Kennedy received the photo, he returned it to Goldwater, with the inscription, "For Barry Goldwater – Whom I urge to follow the career for which he has shown such talent – photography! – from his friend – John Kennedy." This quip became a classic of American political humor after it was made famous by humorist Bennett Cerf. The photo itself was prized by Goldwater for the rest of his life, and recently sold for $17,925 in a Heritage auction.
Son Michael Prescott Goldwater formed the Goldwater Family Foundation with the goal of making his father's photography available via the internet. () was launched in September 2006 to coincide with the HBO documentary "Mr. Conservative", produced by granddaughter CC Goldwater.
UFOs.
Goldwater was one of the more prominent American politicians to openly show an interest in UFOs.
On March 28, 1975, Goldwater wrote to Shlomo Arnon: "The subject of UFOs has interested me for some long time. About ten or twelve years ago I made an effort to find out what was in the building at Wright-Patterson Air Force Base where the information has been stored that has been collected by the Air Force, and I was understandably denied this request. It is still classified above Top Secret." Goldwater further wrote that there were rumors the evidence would be released, and that he was "just as anxious to see this material as you are, and I hope we will not have to wait much longer."
The April 25, 1988, issue of "The New Yorker" carried an interview where Goldwater said he repeatedly asked his friend, Gen. Curtis LeMay, if there was any truth to the rumors that UFO evidence was stored in a secret room at Wright-Patterson Air Force Base, and if he (Goldwater) might have access to the room. According to Goldwater, an angry LeMay gave him "holy hell" and said, "Not only can't you get into it but don't you ever mention it to me again."
In a 1988 interview on Larry King's radio show, Goldwater was asked if he thought the U.S. Government was withholding UFO evidence; he replied "Yes, I do." He added:
Goldwater Scholarship.
The Barry M. Goldwater Scholarship and Excellence in Education Program was established by Congress in 1986. Its goal is to provide a continuing source of highly qualified scientists, mathematicians, and engineers by awarding scholarships to college students who intend to pursue careers in these fields.
The Scholarship is widely considered the most prestigious award in the U.S. conferred upon undergraduates studying the sciences. It is awarded to about 300 students (college sophomores and juniors) nationwide in the amount of $7500 per academic year (for their senior year, or junior and senior years). It honors Goldwater's keen interest in science and technology.
Death.
Goldwater's public appearances ended in late 1996 after he suffered a massive stroke; family members then disclosed he was in the early stages of Alzheimer's disease. He died on May 29, 1998, at the age of 89 at his long-time home in Paradise Valley, Arizona, of complications from the stroke. His ashes were buried at the Episcopal Christ Church of the Ascension in Paradise Valley, Arizona. A memorial statue set in a small park has been erected to honor the memory of Goldwater in that town, near his former home and current resting place.
Buildings and monuments.
Among the buildings and monuments named after Barry Goldwater are: the Barry M. Goldwater Terminal at Phoenix Sky Harbor International Airport, Goldwater Memorial Park in Paradise Valley, Arizona, the Barry Goldwater Air Force Academy Visitor Center at the United States Air Force Academy, and Barry Goldwater High School in northern Phoenix. In 2010 former Arizona Attorney General Grant Woods, himself a Goldwater scholar and supporter, founded the Goldwater Women's Tennis Classic Tournament to be held annually at the Phoenix Country Club in Phoenix, AZ.
Documentary.
Goldwater's granddaughter, CC Goldwater, has co-produced with longtime friend and independent film producer Tani L. Cohen a documentary on Goldwater's life, "Mr. Conservative: Goldwater on Goldwater", first shown on HBO on September 18, 2006.
Relatives.
Goldwater's son, Barry Goldwater, Jr., served as a Congressman from California from 1969 to 1983. He was the first Congressman to serve while having a father in the Senate. Goldwater's nephew, Don Goldwater, sought the Arizona Republican Party nomination for Governor of Arizona in 2006, but was defeated by Len Munsil.

</doc>
<doc id="4794" url="http://en.wikipedia.org/wiki?curid=4794" title="Baralong incidents">
Baralong incidents

The Baralong incidents were naval engagements of the First World War in August and September 1915, involving the Royal Navy Q-ship , later renamed HMS "Wyandra", and two German U-boats.
"Baralong" sank , which had been preparing to sink a nearby merchant ship, the "Nicosian". About a dozen of the crewmen managed to escape the sinking submarine, and Lieutenant Godfrey Herbert, commanding officer of "Baralong", ordered the surviving sailors to be summarily executed after they boarded the "Nicosian". All the survivors of "U-27"s sinking, including several who had reached the "Nicosian", were shot by "Baralong"s crew. Later, "Baralong" sank in an incident which has also been described as a war crime.
First incident.
Action of 19 August 1915.
After the sinking of by a German submarine in May 1915, Lieutenant-Commander Godfrey Herbert, commanding officer of "Baralong", was visited by two officers of the Admiralty's Secret Service branch at the naval base at Queenstown, Ireland. He was told, "This "Lusitania" business is shocking. Unofficially, we are telling you... take no prisoners from U-boats."
Interviews with his subordinate officers have established Herbert's undisciplined manner of commanding his ship. Herbert allowed his men to engage in drunken binges during shore leave. During one such incident, at Dartmouth, several members of "Baralong"s crew were arrested after destroying a local saloon. Herbert paid their bail, then left port with the indicted crewmen aboard. Beginning in April 1915, Herbert ordered his subordinates cease calling him "Sir", and to address him only by the pseudonym "Captain William McBride."
Throughout the summer of 1915, "Baralong" continued routine patrol duties in the Irish Sea without encountering the enemy.
On 19 August 1915, sank the White Star Liner with the loss of 44 lives - this included three Americans and led to a diplomatic incident between Germany and the U.S. "Baralong" had been about from the scene, and had received a distress call from the ship. "Baralong"s crew was infuriated by the attack and by their inability to locate survivors.
Meanwhile, about south of Queenstown, , commanded by "Kapitänleutnant" Bernard Wegener, stopped the British steamer "Nicosian" in accordance with the rules laid down by the London Declaration. A boarding party of six men from "U-27" discovered that "Nicosian" was carrying munitions and 250 American mules earmarked for the British Army in France. The Germans allowed the freighter's crew and passengers to board lifeboats, and prepared to sink the freighter with the U-boat's deck gun.
"U-27" was lying off "Nicosian"s port quarter and firing into it when "Baralong" appeared on the scene, flying the ensign of the United States as a false flag. When she was half a mile away, "Baralong" ran up a signal flag indicating that she was going to rescue "Nicosian"s crew. Wegener acknowledged the signal, then ordered his men to cease firing, and took "U-27" along the port side of "Nicosian" to intercept "Baralong". As the submarine disappeared behind the steamship, Herbert steered "Baralong" on a parallel course along "Nicosian"s starboard side.
Before "U-27" came round "Nicosian"s bow, "Baralong" hauled down the American flag, hoisted the Royal Navy's White Ensign, and unmasked her guns. As "U-27" came into view from behind "Nicosian", "Baralong" opened fire with her three 12-pounder guns at a range of , firing 34 rounds for only a single shot from the submarine. "U-27" rolled over and began to sink.
According to Tony Bridgland; "Herbert screamed, 'Cease fire!' But his men's blood was up. They were avenging the "Arabic" and the "Lusitania". For them this was no time to cease firing, even as the survivors of the crew appeared on the outer casing, struggling out of their clothes to swim away from her. There was a mighty hiss of compressed air from her tanks and the "U-27" vanished from sight in a vortex of giant rumbling bubbles, leaving a pall of smoke over the spot where she had been. It had taken only a few minutes to fire the thirty-four shells into her."
Meanwhile, "Nicosian"s crew were cheering wildly from the lifeboats. Captain Manning was heard to yell, "If any of those bastard Huns come up, lads, hit 'em with an oar!"
Twelve men survived the sinking of the submarine: the crews of her two deck guns and those who had been on the conning tower. They swam to "Nicosian" and attempted to join the six-man boarding party by climbing up her hanging lifeboat falls and pilot ladder. Herbert, worried that they might try to scuttle the steamer, ordered his men to open fire with small arms, killing all in the water. Wegener is described by some accounts as being shot while trying to swim to the "Baralong".
Herbert sent "Baralong"s 12 Royal Marines, under the command of a Corporal Collins, to find the surviving German sailors aboard "Nicosian". As they departed, Herbert told Collins, "Take no prisoners." The Germans were discovered in the engine room and shot on sight. According to Sub-Lieutenant Gordon Steele: "Wegener ran to a cabin on the upper deck -- I later found out it was Manning's bathroom. The marines broke down the door with the butts of their rifles, but Wegener squeezes through a scuttle and dropped into the sea. He still had his life-jacket on and put up his arms in surrender. Corporal Collins, however, took aim and shot him through the head." Corporal Collins later recalled that, after Wegener's death, Herbert threw a revolver in the German captain's face and screamed, "What about the "Lusitania", you bastard!" An alternative account says that the Germans who boarded "Nicosian" were killed by the freighter's engine room staff; this report apparently came from the officer in command of the muleteers.
Aftermath.
In Herbert's report to the Admiralty, he stated he feared the survivors from the U-boat's crew would board the freighter and scuttle her, so he ordered the Royal Marines on his ship to shoot the survivors. If they had scuttled the freighter, it could have been counted as negligence on the part of Herbert. Moments before "Baralong" began her attack, the submarine was firing on the freighter. It is not known if the escaping sailors actually intended to scuttle the freighter.
The Admiralty, upon receiving Herbert's report, immediately ordered its suppression, but the strict censorship imposed on the event failed when Americans who had witnessed the incident from "Nicosian"s lifeboats spoke to newspaper reporters after their return to the United States.
German memorandum.
The German government delivered a memorandum on the incident via the American ambassador in Berlin, who received it on 6 December 1915. In it, they cited six US citizens as witnesses, stating they had made sworn depositions regarding the incident before public notaries in the US.
The statements said that five survivors from "U-27" managed to board "Nicosian", while the rest were shot and killed on Herbert's orders while clinging to the merchant vessel's lifeboat falls. It was further stated that when Herbert ordered his Marines to board "Nicosian", he gave the order "take no prisoners". Four German sailors were found in "Nicosian"s engine room and propeller shaft tunnel, and were killed. According to the witness statements, "U-27"s commander was shot while swimming towards "Baralong".
The memorandum demanded that the captain and crew of "Baralong" be tried for the murder of unarmed German sailors, threatening to "take the serious decision of retribution for an unpunished crime". Sir Edward Grey replied through the American ambassador that the incident could be grouped together with the Germans' sinking of SS "Arabic", their attack on a stranded British submarine on the neutral Dutch coast, and their attack on the steamship "Ruel", and suggested that they be placed before a tribunal composed of US Navy officers.
German reaction.
A debate took place in the Reichstag on 15 January 1916, where the incident was described as a "cowardly murder" and Grey's note as being "full of insolence and arrogance". It was announced that reprisals had been decided, but not what form they would take.
The outrage aroused in Germany by "Baralong"s actions was used by the "Kaiserliche Marine" to justify attacking and sinking ships without warning at sea during the First World War, and was used again in the Second World War as a reason for the "Kriegsmarine" to do the same. A German medal was issued commemorating the event. "Baralong" and her crew were also placed on the "Kaiserliche Marine"s "Black List": any member of her crew was to be shot on sight if captured.
As a precaution to protect the ships against any reprisals against their crews, HMS "Wyandra" was transferred to the Mediterranean, and took the name of sister ship "Manica", while "Baralong"s name was deleted from "Lloyd's Register". "Nicosian" was renamed "Nevisian", and the crew was issued new Discharge Books, with the voyage omitted.
"Baralong"s crew were later awarded £185 prize bounty for sinking "U-27".
A "Kriegsmarine" submarine flotilla formed on 25 June 1938 was named "Wegener" in memory of "U-27"s commander.
Second incident.
Action of 24 September 1915.
On 24 September 1915, "Baralong" sank the U-boat , for which her commanding officer at the time, Lieutenant-Commander A. Wilmot-Smith, was later awarded £170 prize bounty.
"U-41" was in the process of sinking SS "Urbino" with gunfire when "Baralong" arrived on the scene, flying an American flag. When "U-41" surfaced near "Baralong", the latter opened fire while continuing to fly the American flag, and sank the U-boat.
Aftermath of the second incident.
Unlike the neutral Americans in the first incident, the only witnesses to the second attack were the German and British sailors present. "Oberleutnant zur See" Iwan Crompton, after returning to Germany from a prisoner-of-war camp, reported that "Baralong" had run down the lifeboat he was in; he leapt clear and was shortly after taken aboard "Baralong". The British crew denied that they had run down the lifeboat.
Crompton later published an account of "U-41"s exploits in 1917, "U-41: der zweite Baralong-Fall", which called the sinking of "U-41" a "second Baralong case".
The event was also commemorated in a propaganda medal designed by the German engraver Karl Goetz. This was one of many medals that were popular in Germany from around 1910 to 1940.

</doc>
<doc id="4795" url="http://en.wikipedia.org/wiki?curid=4795" title="Banda">
Banda

Banda may refer to:
Music.
A band is a group of four or more of persons that sing one or more geners.

</doc>
<doc id="4796" url="http://en.wikipedia.org/wiki?curid=4796" title="Bladder (disambiguation)">
Bladder (disambiguation)

Bladder often refers to the urinary bladder, which collects urine for excretion in animals.
Bladder may also refer to:

</doc>
<doc id="4797" url="http://en.wikipedia.org/wiki?curid=4797" title="Bob Young (businessman)">
Bob Young (businessman)

Robert "Bob" Young is a serial entrepreneur whose biggest success has been Red Hat Inc, the open source software company. He is also the owner of the Hamilton Tiger-Cats of the Canadian Football League. He was born in Ancaster, Ontario, Canada. He attended Trinity College School in Port Hope, Ontario. He received a Bachelor of Arts from Victoria College at the University of Toronto. 
Prior to Red Hat, Young built a couple of computer rental and leasing businesses, including founding Vernon Computer Rentals in 1984. Descendants of Vernon are still operating under that name. After leaving Vernon, Young founded the ACC Corp Inc. in 1993. 
Marc Ewing and Young's partnership started in 1994 when ACC acquired the Red Hat trademarks from Ewing. In early 1995, ACC changed its name to Red Hat Software, which has subsequently changed to simply Red Hat, Inc. Young served as Red Hat's CEO until 1999. 
After leaving Red Hat he founded Lulu.com in 2002, a self-publishing web-site that claims to be the world's fastest-growing provider of print-on-demand books. He is Lulu's CEO. In 2006 Young established the Lulu Blooker Prize, a book prize for books that began as blogs. He launched the prize partly as a means of promoting Lulu.
Young also co-founded "Linux Journal" in 1994, and in 2003, he purchased the Hamilton Tiger-Cats, a Canadian Football League franchise. 
Young is a strong believer in user-generated content and open source software. In a 2008 interview he described the way that online social media has empowered consumers:
Today the Internet connects every single one of your customers, not just to you but to each other. So you produce a lemon of a car and you won’t know where to hide because your customers are going to tell each other about it and then the rest of the world about it. So it is actually a fundamentally healthy thing from a consumer point of view.
Conversely if you do have a better product suddenly you have these businesses that grow like topsy. If you come up with an innovation that serves, that resonates with customers, the Internet will allow that innovation to spread dramatically faster than any other technology has enabled a new innovation to be adopted in the history of mankind.
Young focuses his philanthropic efforts to support an increased access to information and advancement of knowledge. In 1999 he founded The Center for the Public Domain. Young currently supports Creative Commons, the Public Knowledge Project, the Dictionary of Old English, the Internet Archive, ibiblio, the NCSU eGames, and the Bald Head Island Conservancy, among others. He is a Visionaries' Circle supporter of the Loran Scholars program.
Family.
Joyce Young, Bob Young's aunt, purchased stock in Red Hat Inc. shortly after its founding. When Red Hat's stock rose significantly after its initial public offering in 1999, they sold enough stock to recoup their initial investment, and retained some stocks. By January 2000, the remaining stock was valued in the millions. In June 2000, she donated $40 million to the Hamilton Community Foundation to be used largely in improvement of Hamilton-area health facilities, an act that represented one of the single largest charitable donations in Canadian history. The couple also made significant donations to the Royal Military College of Canada.

</doc>
<doc id="4800" url="http://en.wikipedia.org/wiki?curid=4800" title="Babylon 5">
Babylon 5

Babylon 5 is an American space opera television series created by writer and producer J. Michael Straczynski, under the Babylonian Productions label, in association with Straczynski's Synthetic Worlds Ltd. and Warner Bros. Domestic Television. After the successful airing of a backdoor pilot movie, Warner Bros. commissioned the series as part of the second-year schedule of programs provided by its Prime Time Entertainment Network (PTEN). The pilot episode was broadcast on February 22, 1993 in the US. The first season premiered in the US on January 26, 1994, and the series ultimately ran for the intended five seasons. Describing it as having "always been conceived as, fundamentally, a five-year story, a novel for television," Straczynski wrote 92 of the 110 episodes, and served as executive producer, along with Douglas Netter.
Set between the years 2257 and 2261, it depicts a future where Earth has sovereign states, and a unifying Earth government. Colonies within the solar system, and beyond, make up the Earth Alliance, and contact has been made with other spacefaring races. The ensemble cast portray alien ambassadorial staff and humans assigned to the five-mile-long Babylon 5 space station, a center for trade and diplomacy. Described as "one of the most complex programs on television," the various story arcs drew upon the prophesies, religious zealotry, racial tensions, social pressures, and political rivalries which existed within each of their cultures, to create a contextual framework for the motivations and consequences of the protagonists' actions. With a strong emphasis on character development set against a backdrop of conflicting ideologies on multiple levels, Straczynski wanted "to take an adult approach to SF, and attempt to do for television SF what "Hill Street Blues" did for cop shows."
Generally viewed as having "launched the new era of television CGI visual effects," it received multiple awards during its initial run, including two consecutive Hugo Awards for best dramatic presentation, and continues to regularly feature prominently in various polls and listings highlighting top-rated science fiction series. Not appearing on American television since 2003, it continues to be shown in international markets such as Fox in the UK, the TV4-ScifFi Channel in Sweden, and the FBC TV channel in Fiji. Initially written by Straczynski, DC began publishing Babylon 5 comics in 1994, with stories that closely tied in with events depicted in the show, with events in the comics eventually being referenced onscreen in the actual television series. The franchise continued to expand into short stories, RPG games, and novels, with the Technomage trilogy of books being the last to be published in 2001, shortly after the spin-off television series, "Crusade", was cancelled.
All rights except for a possible movie are controlled by Warner (movie rights are retained by Straczynski).
Plot summary.
Backstory.
At the beginning of the series, five dominant civilizations are represented. The dominant species are the Humans, Minbari, Narn, Centauri, and the Vorlons. "The Shadows" and their various allies are malevolent species who appear later in the series. Several dozen less powerful species from the League of Non-Aligned Worlds appear, including the Drazi, Brakiri, Vree, Markab, and pak'ma'ra. The station's first three predecessors (the original Babylon station, Babylon 2 and Babylon 3) were sabotaged or accidentally destroyed before their completion. The fourth station, Babylon 4, vanished without a trace twenty-four hours after it became fully operational.
Synopsis.
The television series takes its name from the Babylon 5 space station, located in the Epsilon Eridani star system, at the fifth Lagrangian point between the fictional planet Epsilon III and its moon. Babylon 5 is an O'Neill cylinder five miles long and a half-mile to a mile in diameter. Living areas accommodate the various alien species, providing differing atmospheres and gravities. Human visitors to the alien sectors are shown using breathing equipment and other measures to tolerate the conditions.
The five seasons of the series each correspond to one fictional sequential year in the period 2258–2262. Each season shares its name with an episode that is central to that season's plot. As the series starts, the Babylon 5 station is welcoming ambassadors from various races in the galaxy. Earth has just barely survived an accidental war with the much more powerful Minbari, who, despite their superior technology, mysteriously surrendered at the brink of the destruction of the human race during the Battle of the Line.
Season 1 – Year 2258.
During 2258, Commander Jeffrey Sinclair is in charge of the station. Much of the story revolves around his gradual discovery that it was his capture by the Minbari at the Battle of the Line which ended the war against Earth. Upon capturing Sinclair, the Minbari came to believe that Valen, a great Minbari leader and hero of the last Minbari-Shadow war, had been reincarnated as the Commander. Concluding that others of their species had been, and were continuing to be reborn as humans, and in obedience to the edict that Minbari do not kill one another, lest they harm the soul, they stopped the war just as Earth's final defenses were on the verge of collapse.
Meanwhile, tensions between the Centauri Republic, which is an empire in decline, and the Narn Regime, a former dominion which rebelled and gained freedom, are increasing. Ambassador G'Kar of the Narn wishes for his people to strike back at the Centauri for what they did, and Ambassador Londo Mollari of the Centauri wishes for his people to become again the great empire they once were. As part of these struggles, Mollari makes a deal with a mysterious ally to strike back at the Narn, further heightening tensions.
It is gradually revealed that Ambassador Delenn is a member of the mysterious and powerful Grey Council, the ruling body of the Minbari. Towards the end of 2258, she begins the transformation into a Minbari-human hybrid, ostensibly to build a bridge between the humans and Minbari. The year ends with the death of Earth Alliance president Luis Santiago. The death is officially ruled an accident, but some members of the military, including the staff of Babylon 5, believe it to be an assassination.
Season 2 – Year 2259.
At the beginning of 2259, Captain John Sheridan replaces Sinclair as the military governor of the station after Sinclair is reassigned as ambassador to Minbar. He and the command staff gradually learn that the assassination of President Santiago was arranged by his then-Vice President, Morgan Clark, who has now become president. Conflict develops between the Babylon 5 command staff and the Psi Corps, an increasingly autocratic organization which oversees and controls the lives of human telepaths.
The Shadows, an ancient and extremely powerful race who have recently emerged from hibernation, are revealed to be the cause of a variety of mysterious and disturbing events, including the attack on the Narn outpost at the end of 2258. Centauri Ambassador Londo Mollari unknowingly enlists their aid through his association with their mysterious human representative Mr. Morden in the ongoing conflict with the Narn. The elderly and ailing Centauri emperor, long an advocate of reconciliation with the Narn, unfortunately has insufficient control to prevent others from instigating war against the Narn Regime. When the emperor dies suddenly during a peace mission to Babylon 5, a number of conspirators, including Ambassador Londo Mollari and Lord Refa, take control of the Centauri government by assassinating their opponents and placing the late emperor's unstable nephew on the throne. Their first act is to start open aggression against the Narn. After a full-scale war breaks out, the Centauri with the help of the Shadows through Londo eventually conquer Narn in a brutal attack involving mass drivers, outlawed weapons of mass destruction. Towards the end of the year, the Clark administration begins to show increasingly totalitarian characteristics, clamping down on dissent and restricting freedom of speech. The Vorlons are revealed to be the basis of legends about angels on various worlds, including Earth, and are the ancient enemies of the Shadows. They enlist the aid of Sheridan and the Babylon 5 command staff in the struggle against the Shadows.
Season 3 – Year 2260.
The Psi Corps and President Clark, whose government has discovered Shadow vessels buried in Earth's solar system, begin to harness the vessels' advanced technology. The Clark administration continues to become increasingly xenophobic and totalitarian, and gradually develops an Orwellian government style, including an organization called Night Watch which targets citizens who hold views contrary to those of Clark's regime.
Sheridan and Delenn's "conspiracy of light" works to uncover clues about how to defeat the Shadows. During a mission near Ganymede, one of the moons of Jupiter, their ship is seen by an Earth Alliance vessel, but not recognized. Though they escape and no shots were fired in the encounter, President Clark uses it as an excuse to declare martial law. This triggers a war of independence on Mars, which had long had a strained political relationship with Earth. Babylon 5 attempts to avoid conflict with Earth, but in response to civilian bombings on Mars, concerns with Night Watch, and illegal orders meant to oppress their populations, they choose to declare independence from Earth, along with several other outlying Earth Alliance colonies. In response, the Earth Alliance attempts to retake Babylon 5 by force, but with the aid of the Minbari, who have allied with the station against the growing Shadow threat, the attack is repelled.
Becoming concerned over the Shadows' growing influence among his people, Centauri ambassador Londo Mollari attempts to sever ties with them. Mr. Morden, the Shadows' human representative, tricks him into restoring the partnership by engineering the murder of Mollari's mistress while putting the blame on a rival Centauri House. Open warfare breaks out between the Shadows and the alliance led by Babylon 5 and the Minbari. It is learned that genetic manipulation by the Vorlons is the source of human telepathy, as it is later discovered that Shadow ships are vulnerable to telepathic attacks. Displeased at the Vorlons' lack of direct action against the Shadows, Captain John Sheridan browbeats Vorlon ambassador Kosh Naranek into launching an attack against their mutual enemy. Kosh's deeds lead to his subsequent assassination by the Shadows.
Former station commander Jeffrey Sinclair returns to Babylon 5 to enlist the aid of Captain Sheridan, Delenn, Ivanova and Marcus in stealing the Babylon 4 space station and sending it 1,000 years back in time to use it as a base of operations against the Shadows in the previous Minbari-Shadow war. Undergoing the same transformation as Delenn at the end of Season 1, Sinclair transforms into a Minbari and is subsequently revealed to be the actual Valen of Minbari legend, rather than simply a reincarnation. Meanwhile, as a result of the unstable time travel, Sheridan sees a vision of the downfall of Centauri Prime when it is attacked by Shadow allies after the Shadow war, and he becomes determined to prevent that future.
Sheridan and Delenn begin a romantic relationship, but their lives and the war are interrupted by the sudden reappearance of Sheridan's wife, who was presumed dead after taking part in an archaeological expedition to Z'ha'dum years prior. She tells Sheridan that the Shadows are not evil, hoping to bring him back with her and recruit him to their cause. He soon realizes that her mind has been tampered with and corrupted by the Shadows, but accepts her offer to visit Z'ha'dum because he hopes it will save the galaxy sooner and prevent the downfall of Centauri Prime. He takes with him a pair of nuclear warheads, which he uses to destroy their largest city, and is last seen jumping into a miles-deep pit to escape the explosion.
Shadow vessels appear at the station, but disappear after Sheridan's attack. However, after they leave, station personnel realize that Garibaldi, who left in a fighter to defend against the vessels, never returned.
Season 4 – Year 2261.
In 2261, the Vorlons join the Shadow War, but their tactics become a concern for the alliance when the Vorlons begin destroying entire planets which they deem to have been "influenced" by the Shadows. Disturbed by this turn of events, Babylon 5 recruits several other powerful and ancient races (the First Ones) to their cause, against both the Shadows and the Vorlons. Captain John Sheridan returns to the station after escaping from Z'ha'dum and reunites the galaxy against the Shadows, but at a price: barring illness or injury, he has only 20 years left to live. He is accompanied by a mysterious alien named Lorien who claims to be the first sentient being in the galaxy, and who breathed life into Sheridan at Z'ha'dum. Once Sheridan returns, he and Delenn formalize their relationship and begin planning to marry, though most of their plans are put on hold due to the ongoing war.
Hours before Sheridan's return, Garibaldi is rescued and returned to the station, in rather dubious circumstances. Over the course of the next several months, he becomes markedly more paranoid and suspicious of other alien races and of Sheridan than he was before, causing worry among his friends and colleagues. After the Shadows are defeated Garibaldi leaves his post as security chief and works on his own as a "provider of information". He begins working for one William Edgars, a Mars tycoon, who is married to Garibaldi's former love. While he works ever closer with Edgars, he becomes increasingly aggressive toward Sheridan and eventually leaves Babylon 5.
Centauri Emperor Cartagia forges a relationship with the Shadows. With the reluctant help of his enemy G'Kar, Londo Mollari engineers the assassination of Cartagia and repudiates his agreement with the Shadows. In exchange for G'Kar's help, Londo frees the Narn from Centauri occupation. Londo afterwards kills Mr. Morden and destroys the Shadow vessels based on the Centauri homeworld, in an attempt to save his planet from destruction by the Vorlons. Aided by the other ancient races, and several younger ones, Sheridan lures both the Vorlons and the Shadows into an immense battle, during which the Vorlons and Shadows reveal that they have been left as guardians of the younger races, but due to philosophical differences, ended up using them as pawns in their endless proxy wars throughout the ages. The younger races reject their continued interference, and the Vorlons and Shadows, along with the remaining First Ones, agree to depart the galaxy in order to allow the younger races to find their own way.
Minbar is gripped by a brief civil war between the Warrior and Religious castes. Delenn secretly meets with Neroon of the Warrior caste and convinces him that neither side can be allowed to win. She tells him that she will undergo a ritual wherein she will be willing to sacrifice herself, but will stop the ritual before she actually dies. When Neroon sees that she actually intends to go through the entire ritual, he rescues her and sacrifices himself instead, declaring that, although he was born Warrior caste, in his heart he is Religious caste.
As part of the ongoing conflict between Earth and Babylon 5, Garibaldi eventually betrays Sheridan and arranges his capture in order to gain Edgars' trust and learn his plans. Garibaldi later learns that Edgars created a virus that is dangerous only to telepaths. It is then revealed that after Garibaldi was captured the previous year, he was taken to the Psi Corps and re-programmed by Bester to provide information to him at the right time. Bester releases Garibaldi of his programming, and allows him to remember everything he has done since being kidnapped. Edgars is killed and his wife disappears, but is reunited with Garibaldi after the end of the war.
Sheridan is tortured and interrogated by those who hope to break him and turn him into a propaganda tool for Earth's totalitarian government. Fortunately, Garibaldi is able to help free Sheridan and return him to the campaign to free Earth. An alliance led by Babylon 5 frees Earth from totalitarian rule by president Clark in a short but bloody war. This culminates in Clark's suicide and the restoration of democratic government due to Susanna Luchenko (played by Beata Poźniak) becoming president of Earth's Alliance. Mars is granted full independence, and Sheridan agrees to resign his commission. The League of Non-Aligned Worlds is dissolved and reformed into the Interstellar Alliance, with Sheridan elected as its first president and continuing his command of the Rangers, who are to act as a galactic equivalent of United Nations peacekeepers. Londo and G'Kar enter an uneasy alliance to help both their races as well as Sheridan in forming the Interstellar Alliance. During the Battle Ivanova is critically injured, promised only a few days to live. Marcus, who had fallen in love with Ivanova, finds the same alien healing device used to revive Garibaldi at the beginning of the second season, and uses it to transfer almost all of his life energy into Ivanova, causing her to live. This causes her immense emotional anguish, and she chooses to leave Babylon 5. Marcus is placed into indefinite cryonic suspension at her request, pending resuscitation technology.
Sheridan and Delenn complete their marriage ceremony while en route to Babylon 5, where they will head the Interstellar Alliance for the next year.
In the season finale, the events of 100, 500, 1000, and one million years into the future are shown, depicting Babylon 5's lasting influence throughout history. Among the events shown are the political aftermath of the 2261 civil war, a subsequent nuclear war on Earth involving a new totalitarian government in the year AD 2762, the resulting fall of Earth into a pre-industrial society, the loss and restoration of humanity's knowledge of space travel, and the final evolution of mankind into energy beings similar to the Vorlons, after which Earth's sun goes nova.
Season 5 – Year 2262 and beyond.
In 2262, Earthforce Captain Elizabeth Lochley is appointed to command Babylon 5, which is now also the headquarters of the Interstellar Alliance. The station grows in its role as a sanctuary for rogue telepaths running from the Psi Corps, resulting in conflict. G'Kar, former Narn ambassador to Babylon 5, becomes unwillingly a spiritual icon after a book that he wrote while incarcerated during the Narn-Centauri War is published without his knowledge. The Drakh, former allies of the Shadows who remained in the galaxy, take control of Regent Virini on Centauri Prime through a parasitic creature called a Keeper, then incite a war between the Centauri and the Interstellar Alliance, in order to isolate the Centauri from the Alliance and gain a malleable homeworld for themselves.
Centauri Prime is devastated by Narn and Drazi warships and Londo Mollari becomes emperor, then ends the war. However, the Drakh blackmail him into accepting a Drakh Keeper, under threat of the complete nuclear destruction of the planet. Vir Cotto, Mollari's loyal and more moral aide, becomes ambassador to Babylon 5 in his stead. G'Kar also leaves Babylon 5 to escape his unwanted fame and explore the rim. Sheridan and Delenn move to a city on Minbar, where the new headquarters of the Interstellar Alliance are located, while celebrating their first year of marriage and the upcoming birth of their son, and mourning the loss of dear friendships. Garibaldi marries and settles down on Mars, where he and his wife share ownership of a prominent pharmaceutical company. Most other main characters, including Stephen Franklin and Lyta Alexander, leave Babylon 5 as well.
As shown in flash-forwards earlier in the series, the next several years include the reign of Londo Mollari as Centauri Emperor. Sixteen years later, Londo sacrifices his life to rescue Sheridan and Delenn from the Drakh, in the hope that they in turn can save Centauri Prime. To prevent the Drakh from discovering his ruse, he asks G'Kar, now an old friend, to kill him, but Londo's Keeper wakes up and forces him to kill G'Kar in return. They die at each other's throats, in accordance with Londo's vision many decades earlier, and Vir Cotto succeeds him as emperor, free of Drakh influence.
Three years after Londo's death, Sheridan himself is on the verge of death and takes one last opportunity to gather his old friends together. Shortly after his farewell party, Sheridan says goodbye to Delenn, though in Minbari fashion they use the word "goodnight" to signify their hope of an eventual reunion. Sheridan then takes a final trip to the obsolete Babylon 5 station before its decommissioning. He returns to the site of the final battle between the Vorlons and the Shadows and apparently dies, but is instead claimed by The First One, who invites him to join the other First Ones on a journey beyond the rim of the galaxy. Babylon 5 station is destroyed in a demolition shortly after Sheridan's departure, its existence no longer necessary as the Alliance has taken over its diplomatic purposes and other trading routes have been established. This final episode features a cameo by Straczynski as the technician who switches off the lights before Babylon 5 is demolished.
Themes.
Throughout its run, "Babylon 5" found ways to portray themes relevant to modern and historical social issues. It marked several firsts in television science fiction, such as the exploration of the political and social landscapes of the first human colonies, their interactions with Earth, and the underlying tensions. "Babylon 5" was also one of the first television science fiction shows to denotatively refer to a same-sex relationship. In the show, sexual orientation is as much of an issue as "being left-handed or right-handed". Unrequited love is explored as a source of pain for the characters, though not all the relationships end unhappily.
Order vs. chaos; authoritarianism vs. free will.
The clash between order and chaos, and the people caught in between, plays an important role in "Babylon 5". The conflict between two unimaginably powerful older races, the Vorlons and the Shadows, is represented as a battle between two competing ideologies, each seeking to turn the humans and the other younger races to their beliefs. The Vorlons represent an authoritarian philosophy: you will do what we tell you to, because we tell you to do it. The Vorlon question, "Who are you?" focuses on identity as a catalyst for shaping personal goals; the intention is not to solicit a "correct" answer, but to "tear down the artifices we construct around ourselves until we're left facing ourselves, not our roles." The Shadows represent another authoritarian philosophy cloaked in a disguise of evolution through fire ( as shown in the episode in which Sheridan goes to Z'ha'dum and when he refuses to cooperate, Justin tells him: "But you do what you're told... and so will you!" ), of sowing the seeds of conflict in order to engender progress. The question the Shadows ask is "What do you want?" In contrast to the Vorlons, they place personal desire and ambition first, using it to shape identity, encouraging conflict between groups who choose to serve their own glory or profit. The representation of order and chaos was informed by the Babylonian myth that the universe was born in the conflict between both. The climax of this conflict comes with the younger races' exposing of the Vorlons' and the Shadows' "true faces" and the rejection of both philosophies, heralding the dawn of a new age without their interference.
The notion that the war was about "killing your parents" is echoed in the portrayal of the civil war between the human colonies and Earth. Deliberately dealing in historical and political metaphor, with particular emphasis upon McCarthyism and the HUAC, the Earth Alliance becomes increasingly authoritarian, eventually sliding into a dictatorship. The show examines the impositions on civil liberties under the pretext of greater defense against outside threats which aid its rise, and the self-delusion of a populace which believes its moral superiority will never allow a dictatorship to come to power, until it is too late. The successful rebellion led by the Babylon 5 station results in the restoration of a democratic government, and true autonomy for Mars and the colonies.
War and peace.
The "Babylon 5" universe deals with numerous armed conflicts which range on an interstellar scale. The story begins in the aftermath of a war which brought the human race to the brink of extinction, caused by a misunderstanding during a first contact situation. The Babylon 5 station is subsequently built in order to foster peace through diplomacy, described as the "last, best hope for peace" in the opening credits monologue during its first three seasons. Wars between separate alien civilizations are featured. The conflict between the Narn and the Centauri is followed from its beginnings as a minor territorial dispute amplified by historical animosity, through to its end, in which weapons of mass destruction are employed to subjugate and enslave an entire planet. The war is an attempt to portray a more sobering kind of conflict than usually seen on science fiction television. Informed by the events of the first Gulf War, the Cuban Missile Crisis and the Soviet invasion of Prague, the intent was to recreate these moments when "the world held its breath" and the emotional core of the conflict was the disbelief that the situation could have occurred at all, and the desperation to find a way to bring it to an end. By the start of the third season, the opening monologue had changed to say that the hope for peace had "failed" and the Babylon 5 station had become the "last, best hope for victory", indicating that while peace is a laudable accomplishment, it can also mean a capitulation to an enemy intent on committing horrendous acts, and that "peace is a byproduct of victory against those who do not want peace."
The Shadow War also features prominently in the show, during which an advanced alien species attempts to sow the seeds of conflict in order to promote technological and cultural advancement. The gradual discovery of the scheme and the rebellion against it, serve as the backdrop to the first three seasons, but also as a metaphor for the war within ourselves. The concurrent limiting of civil liberties and Earth's descent into a dictatorship are "shadow wars" of their own. In ending the Shadow War before the conclusion of the series, the show was able to more fully explore its aftermath, and it is this "war at home" which forms the bulk of the remaining two seasons. The struggle for independence between Mars and Earth culminates with a civil war between the human colonies (led by the Babylon 5 station) and the home planet. Choosing Mars as both the spark for the civil war, and the staging ground for its dramatic conclusion, enabled the viewer to understand the conflict more fully than had it involved an anonymous colony orbiting a distant star. The conflict, and the reasons behind it, were informed by Nazism, McCarthyism and the breakup of Yugoslavia, and the unraveling of the former Balkan country also served as partial inspiration for another civil war, which involved the alien Minbari.
The post-war landscape has its roots in the Reconstruction. The attempt to resolve the issues of the American Civil War after the conflict had ended, and this struggle for survival in a changed world was also informed by works such as "Alas, Babylon", a novel dealing with the after-effects of a nuclear war on a small American town. The show expresses that the end of these wars is not an end to war itself. Events shown hundreds of years into the show's future tell of wars which will once again bring the human race to the edge of annihilation, demonstrating that mankind will not change, and the best that can be hoped for after it falls is that it climbs a little higher each time, until it can one day "take [its] place among the stars, teaching those who follow."
Religion.
Many of Earth's contemporary religions are shown to still exist, with the main human characters often having religious convictions, including Roman Catholicism, including the Jesuits, Judaism and the fictional Foundationism, which was created after first contact with alien races. Alien beliefs in the show range from the Centauri's Bacchanalian-influenced religions, of which there are up to seventy different denominations, to the more pantheistic, as with the Narn and Minbari religions. In the show's third season, a community of Cistercian monks takes up residence on the Babylon 5 station, in order to learn what other races call God, and to come to a better understanding of the different religions through study at close quarters.
References to both human and alien religion is often subtle and brief, but can also form the main theme of an episode. The first season episode "The Parliament of Dreams" is a conventional "showcase" for religion, in which each species on the Babylon 5 station has an opportunity to demonstrate its beliefs, while "Passing Through Gethsemane" focuses on a specific position of Roman Catholic beliefs, as well as concepts of justice, vengeance and biblical forgiveness. Other treatments have been more contentious, such as the David Gerrold-scripted "Believers", in which alien parents would rather see their son die than undergo a life-saving operation because their religious beliefs forbid it. When religion is an integral part of an episode, various characters can be used to express differing view points. Such as in "Soul Hunter", where the concept of an immortal soul is touched upon, and whether after death it is destroyed, reincarnated or simply does not exist. The character arguing the latter, Doctor Stephen Franklin, often appears in the more spiritual storylines as his scientific rationality is used to create dramatic conflict. Undercurrents of religions such as Buddhism have been viewed by some in various episode scripts, and while identifying himself as an atheist, Straczynski believes that passages of dialog can take on distinct meanings to viewers of differing faiths, and that the show ultimately expresses ideas which cross religious boundaries.
Sacrifice.
A major theme in "Babylon 5" is the concept of sacrifice for a greater cause. Kosh sacrifices his life for a first victory against the Shadows. John Sheridan is ready to die at Z'ha'dum. Delenn is ready to die in the starfire wheel to restore Minbari society. Marcus Cole gives his life to save Susan Ivanova. Londo Mollari willingly accepts complete enslavement by a Drakh keeper to save the Centauri from annihilation. Many minor characters also willingly give their lives such as the crew of Drazi or Minbari ships in the final confrontation with the Shadows to save the Army of Light's leaders, some Centauri staying back at the island of Selini to allow the destruction of the Shadow warships, and so on. Captain Ericsson of White Star 14 is knowingly given a false report detailing the opening of an Army of Light base on Coriana VI at approximately the same time as the Vorlons' arrival. It is a suicide mission; he is to intrude into Shadow space and engage the Shadows as if on a real raid, so that when the Shadows destroyed his ship and discovered the file, they would be convinced the report is true and rush to Coriana. By forcing a direct confrontation, Sheridan believes he can finally get the Vorlons and Shadows into a position where he can resolve the war. Ericsson grimly accepts the mission and gives a final Anla'Shok salute to Delenn before signing off. "Some of us must be sacrificed if all are to be saved." is the spiritual epiphany experienced by G'Kar. This reflects the Vorlon philosophy in contrast to the self-interest philosophy of the Shadows.
Dreams and visions.
The subliminal and subconscious play a very significant role in the "Babylon 5" universe. Every single major character experiences, on at least one occasion, some altered state of consciousness in which he or she receives some sort of important mental message. This could either be one that further fleshes out the character for the benefit of the viewer, or one of transcendental and transpersonal nature that anticipates important further developments in the storyline. Some of these signs and portents resemble lucid dreams, but many are quite bizarre and "dreamlike", frequently in a spiritual context.
Addiction.
Substance abuse and its impact on human personalities also plays a significant role in the "Babylon 5" storyline. The station's security chief, Michael Garibaldi, is a textbook relapsing-remitting alcoholic of the binge drinking type; he practices complete abstinence from alcohol throughout most of the series (with one notable exception) until the middle of season five. He only recovers physically and socially and breaks the cycle at the end of the season. His eventual replacement as Chief of Security, Zack Allan, was given a second chance by Garibaldi after overcoming his own addiction to an unspecified drug. Dr. Stephen Franklin develops an (initially unrecognized) addiction to injectable stimulant drugs while trying to cope with the chronic stress and work overload in Medlab (stemming from the Markab extinction), and wanders off to the homeless and deprived in Brown Sector, where he suffers through a severe withdrawal syndrome. Executive Officer Susan Ivanova mentions that her father became an alcoholic after her mother had committed suicide after having been drugged by the authorities over a number of years. Captain Elizabeth Lochley tells Garibaldi that her father was an alcoholic and that she is a recovering alcoholic herself. Among the aliens, Londo Mollari is at least a heavy abuser of alcohol, mostly in the form of the Centauri national drink, Brivari (though in Centauri culture, sobriety, as opposed to drunkenness, is considered a vice).
Numerous other references to substance abuse and drug dealing are scattered throughout the storyline, including Dust, a white powder with a black-market presence comparable to cocaine. "Dust" turns out to be a "designer drug" developed by Psi Corps and placed into the black market as an experiment to see if psychic abilities could be brought out in "mundanes" (non-telepaths).
Perhaps ironically, Jeff Conaway, who played Zack Allen, had his own very real addiction issues for most of his adult life. Co-star Bruce Boxleitner stated unequivically that during Conaway's tenure on the show, he was the consummate professional, always coming to work on time and sober, and that his death due to pneumonia and encephalopathy due to his drug use was tragic, and Zack Allen was, in a sense, a mirror for Conaway.
Cast.
Recurring guests.
In addition, several other actors have filled more than one minor role on the series. Kim Strauss played the Drazi Ambassador in four episodes, as well as nine other characters in ten more episodes. Some actors had difficulty dealing with the application of prosthetics required to play some of the alien characters. The producers therefore used the same group of people (as many as 12) in various mid-level speaking roles, taking full head and body casts from each. The group came to be unofficially known by the production as the "Babylon 5 Alien Rep Group."
Production.
Origin.
Having worked on a number of television science fiction shows which had regularly gone over budget, creator J. Michael Straczynski concluded that a lack of long-term planning was to blame, and set about looking at ways in which a series could be done responsibly. Taking note of the lessons of mainstream television, which brought stories to a centralized location such as a hospital, police station, or law office, he decided that instead of "[going] in search of new worlds, building them anew each week", a fixed space station setting would keep costs at a reasonable level. A fan of sagas such as the "Foundation" series, "Childhood's End", "The Lord of the Rings", "Dune" and the "Lensman" series, Straczynski wondered why no one had done a television series with the same epic sweep, and concurrently with the first idea started developing the concept for a vastly ambitious epic covering massive battles and other universe-changing events. Realizing that both the fixed-locale series and the epic could be done in a single series, he began to sketch the initial outline of what would become "Babylon 5".
Straczynski set five goals for "Babylon 5". He said that the show "would have to be good science fiction" as well as good television – "rarely are shows both good [science fiction] "and" good TV; there're generally one or the other [emphasis in original]." It would have to do for science fiction television what "Hill Street Blues" had done for police dramas, by taking an adult approach to the subject. It would have to be reasonably budgeted, and "it would have to look unlike anything ever seen before on TV, presenting individual stories against a much broader canvas." He further stressed that his approach was "to take [science fiction] seriously, to build characters for grown-ups, to incorporate real science but keep the characters at the center of the story." Some of the staples of television science fiction were also out of the question (the show would have "no kids or cute robots"). The idea was not to present a perfect utopian future, but one with greed and homelessness; one where characters grow, develop, live, and die; one where not everything was the same at the end of the day's events. Citing Mark Twain as an influence, Straczynski said he wanted the show to be a mirror to the real world and to covertly teach.
Format.
Described as a "window on the future" by series production designer John Iacovelli, the story is set in the 23rd century on a large O'Neill Colony named "Babylon 5"—a five-mile-long, 2.5 million-ton rotating colony designed as a gathering place for the sentient species of the galaxy, in order to foster peace through diplomacy, trade, and cooperation. Instead, acting as a center of political intrigue and conflict, the station becomes the linchpin of a massive interstellar war. This is reflected in the opening monologue of each episode, which includes the words "last, best hope for peace" in season one, changing to "last, best hope for victory" by season three.
The series consists of a coherent five-year story arc taking place over five seasons of 22 episodes each. Unlike most television shows at the time, "Babylon 5" was conceived as a "novel for television", with a defined beginning, middle, and end; in essence, each episode would be a single "chapter" of this "novel". Many of the tie-in novels, comic books, and short stories were also developed to play a significant canonical part in the overall story.
The cost of the series totalled an estimated $90 million for 110 episodes.
Writing.
Creator and showrunner J. Michael Straczynski wrote 92 of the 110 episodes of "Babylon 5", including all 44 episodes in the third and fourth seasons; according to Straczynski, a feat never before accomplished in American television. Other writers to have contributed scripts to the show include Peter David, Neil Gaiman, Kathryn M. Drennan, Lawrence G. DiTillio, D. C. Fontana, and David Gerrold. Harlan Ellison, a creative consultant on the show, received story credits for two episodes. Each writer was informed of the overarching storyline, enabling the show to be produced consistently under-budget. The rules of production were strict; scripts were written six episodes in advance, and changes could not be made once production had started.
Though conceived as a whole, it was necessary to adjust the plotline to accommodate external influences. Each of the characters in the series was written with a "trap door" into their background so that, in the event of an actor's unexpected departure from the series, the character could be written out with minimal impact on the storyline. In the words of Straczynski, "As a writer, doing a long-term story, it'd be dangerous and short-sighted for me to construct the story without trap doors for every single character. ... That was one of the big risks going into a long-term storyline which I considered long in advance;..." The character of Talia Winters was to have undergone a transformation into a Psi Corps secret agent, having been revealed as a "sleeper", whose true personality was buried subconsciously, and who acted as a spy, observing the events on the station and the actions of her command staff. When Winters's portrayer Andrea Thompson left the series, this revelation was used to drop the character from the series.
Ratings for "Babylon 5" continued to rise during the show's third season, but going into the fourth season, the impending demise of network PTEN left a fifth year in doubt. Unable to get word one way or the other from parent company Warner Bros., and unwilling to short-change the story and the fans, Straczynski began preparing modifications to the fourth season in order to allow for both eventualities. Straczynski identified three primary narrative threads which would require resolution: the Shadow war, Earth's slide into a dictatorship, and a series of sub-threads which branched off from those. Estimating they would still take around 27 episodes to resolve without having the season feel rushed, the solution came when the TNT network commissioned two "Babylon 5" television films. Several hours of material was thus able to be moved into the films, including a three-episode arc which would deal with the background to the Earth–Minbari War, and a sub-thread which would have set up the sequel series, "Crusade". Further standalone episodes and plot-threads were dropped from season four, which could be inserted into "Crusade", or the fifth season, were it to be given the greenlight. The intended series finale, "Sleeping in Light", was filmed during season four as a precaution against cancellation. When word came that TNT had picked up "Babylon 5", this was moved to the end of season five and replaced with a newly filmed season four finale, "The Deconstruction of Falling Stars".
Costume.
Ann Bruice Aling was costume designer for the show, after production designer John Iacovelli suggested her for the position having previously worked with Bruice on a number of film and theatrical productions.
With the variety of costumes required she compared "Babylon 5" to "eclectic theatre", with fewer rules about period, line, shape and textures having to be adhered to. Preferring natural materials whenever possible, such as ostrich leather in the Narn body armor, Bruice combined and layered fabrics as diverse as rayon and silk with brocades from the 1930s and '40s to give the clothing the appearance of having evolved within different cultures.
With an interest in costume history, she initially worked closely with J. Michael Straczinski to get a sense of the historical perspective of the major alien races, "so I knew if they were a peaceful people or a warring people, cold climate etc. and then I would interpret what kind of sensibility that called for." Collaborating with other departments to establish co-ordinated visual themes for each race, a broad palette of colors was developed with Iacovelli, which he referred to as "spicy brights". These warm shades of grey and secondary colors, such as certain blues for the Minbari, would often be included when designing both the costumes and relevant sets. As the main characters evolved, Bruice referred back to Straczynski and producer John Copeland who she viewed as "surprisingly more accessible to me as advisors than other producers and directors", so the costumes could reflect these changes. Ambassador Londo Mollari's purple coat became dark blue and more tailored while his waistcoats became less patterned and brightly colored as Bruice felt "Londo has evolved in my mind from a buffoonish character to one who has become more serious and darker."
Normally there were three changes of costume for the primary actors; one for on set, one for the stunt double and one on standby in case of "coffee spills". For human civilians, garments were generally purchased off-the-rack and altered in various ways, such as removing lapels from jackets and shirts while rearranging closures, to suggest future fashions. For some of the main female characters a more couture approach was taken, as in the suits worn by Talia Winters which Bruice described as being designed and fitted to within "an inch of their life". Costumes for the destitute residents of downbelow would be distressed through a combination of bleaching, sanding, dipping in dye baths and having stage blood added.
Like many of the crew on the show, members of the costume department made onscreen cameos. During the season 4 episode "Atonement", the tailors and costume supervisor appeared as the Minbari women fitting Zack Allan for his new uniform as the recently promoted head of security. His complaints, and the subsequent stabbing of him with a needle by costume supervisor Kim Holly, was a light hearted reference to the previous security uniforms. A design carried over from the pilot movie, which were difficult to work with and wear due to the combination of leather and wool.
Prosthetic makeup and animatronics.
While the original pilot film featured some aliens which were puppets and animatronics, the decision was made early on in the show's production to portray most alien species as humanoid in appearance. Barring isolated appearances, fully computer-generated aliens were discounted as an idea due to the "massive rendering power" required. Long-term use of puppets and animatronics was also discounted due to the technological limitations in providing convincing interaction with the human actors ("...if you want any real "emotion" from the character, you're going to have to have an actor inside" [emphasis in original]).
Visuals.
In anticipation of future HDTV broadcasts and Laserdisc releases, rather than the usual format, the series was shot in , with the image cropped to 4:3 for initial television transmissions. "Babylon 5" also distinguished itself at a time when models and miniature were still standard by becoming one of the first television shows to use computer technology in creating visual effects. This was achieved using Amiga-based Video Toasters at first, and later Pentium, Macs, and Alpha-based systems. It also attempted to respect Newtonian physics in its effects sequences, with particular emphasis on the effects of inertia.
Foundation Imaging provided the special effects for the pilot film (for which it won an Emmy) and the first three seasons of the show. After the show's co-executive producer (Douglas Netter) and producer (John Copeland) approached Straczynski with the idea of producing the effects in-house, Straczynski agreed to replace Foundation, for season 4 and 5, once a new team had been established by Netter Digital, and an equal level of quality was assured, by using similar technology and a number of former Foundation employees. The Emmy-winning alien make-up was provided by Optic Nerve Studios.
Music and scoring.
Christopher Franke composed and scored the musical soundtrack for all 5 years of the show when Stewart Copeland, who worked on the original telefilm, was unable to return for the first season due to recording and touring commitments. Given creative freedom by the producers, Franke also orchestrated and mixed all the music which one reviewer described as having "added another dimension of mystery, suspense, and excitement to the show, with an easily distinguishable character that separates "Babylon 5" from other sci-fi television entries of the era."
With his recording studio in the same building as his home located in the Hollywood Hills, Franke would attend creative meetings before scoring the on average 25 minutes of music for each episode. Utilising Cubase software through an electronic keyboard, or for more complex pieces a light pen and graphics tablet, he would begin by developing the melodic content round which the ambient components and transitions were added. Using playbacks with digital samples of the appropriate instruments, such as a group of violins, he would decide which tracks to produce electronically or record acoustically.
Utilizing the "acoustic dirt produced by live instruments and the ability to play so well between two semitones" and the "frequency range, dynamics and control" provided by synthesizers, he described his approach "as experimental friendly as possible without leaving the happy marriage between the orchestral and electronic sounds". While highlighting "Babylon 5" was produced on a "veritable shoestring", and as such would have been unable to afford a full orchestral score every week, at least one reviewer felt that the soundtrack would have benefitted from a greater use of the Berlin Symphonic Film Orchestra, which Franke established in 1991.
Scores for the acoustic tracks were emailed to his Berlin scoring stage, and would require from four musicians to the full orchestra, with a maximum of 24 present at any one time. One of three conductors would also be required for any score that involved more than 6 musicians. Franke would direct recording sessions via six fibre optic digital telephone lines to transmit and receive video, music and the SMPTE timecode. The final edit and mixing of the tracks would take place in his Los Angeles studio. Initially concerned composing for an episodic television show could become "annoying because of the repetition", Franke found the evolving characters and story of "Babylon 5" afforded him the opportunity to continually "push the envelope".
A total of 24 episode and three television film soundtracks were released under Franke's record label, Sonic Images Records, between 1995 and 2001. These contain the musical scores in the same chronological order as they played in the corresponding episodes, or television films. Three compilation albums were also produced, containing extensively re-orchestrated and remixed musical passages taken from throughout the series to create more elaborate suites. In 2007 his soundtrack for was released under the Varèse Sarabande record label.
"Star Trek: Deep Space Nine" controversy.
The pilot episode of "" ("DS9") aired just weeks before "Babylon 5" debuted. "Babylon 5" creator J. Michael Straczynski indicated that Paramount Television was aware of his concept as early as 1989, when he attempted to sell the show to the studio, and provided them with the series bible, pilot script, artwork, lengthy character background histories, and plot synopses for 22 "or so planned episodes taken from the overall course of the planned series".
Paramount passed on "Babylon 5", but later announced "Deep Space Nine" was in development, two months after Warner Bros. announced its plans for "Babylon 5". Straczynski has stated on numerous occasions that, even though he is confident that "Deep Space Nine" producer/creators Rick Berman and Michael Piller did not see this material, he thinks Paramount may have used his bible and scripts to steer development of "Deep Space Nine". In 1993 he said, "Okay, YOU (Paramount) know what happened, and *I* know what happened, but let's try to be grown-up about it for now,' though I must say that the shape-changing thing nearly tipped me back over the edge again. If there are no more major similarities that crop up in the next few weeks or months, with luck we can continue that way."
"Babylon 5"'s first-run syndicated ratings averaged between 3 and 4% of US households from 1995 to 1997, whereas "DS9" ranged from 4 to 5% during the same time span. The PTEN vs. UPN network rivalry may have been a factor in the development of such similar shows, since both networks were competing for control of the same independent stations and for status as the fifth major network (after ABC, CBS, NBC, FOX). Each nascent network wanted the other to fail. Ultimately PTEN dissolved in 1997, while The WB and UPN merged to form The CW in 2006.
Use of the Internet.
The show employed Internet marketing to create a buzz among online readers far in advance of the airing of the pilot episode, with Straczynski participating in online communities on USENET (in the rec.arts.sf.tv.babylon5.moderated newsgroup), and the GEnie and CompuServe systems before the Web came together as it exists today. The station's location, in "grid epsilon" at coordinates of 470/18/22, was a reference to GEnie ("grid epsilon" = "GE") and the original forum's address on the system's bulletin boards (page 470, category 18, topic 22). Also during this time, Warner Bros. executive Jim Moloshok created and distributed electronic trading cards to help advertise the series. In 1995, Warner Bros. started the Official "Babylon 5" Website on the now defunct Pathfinder portal. In September 1995, they hired a fan to take over the site and move it to its own domain name, and to oversee the "Keyword B5" area on America Online.
Broadcast history.
The pilot film, "", premiered on February 22, 1993, and the regular series initially aired from January 26, 1994 through November 25, 1998, first on the short-lived Prime Time Entertainment Network, then on cable network TNT. The show aired every week in the United Kingdom on Channel 4 without a break; as a result the last four or five episodes of the early seasons were shown in the UK before the US. The pilot film debuted in the United States with strong viewing figures, achieving a 9.7 in the Nielsen national syndication rankings. The series proper debuted with a 6.8 rating/10 share. Figures dipped in its second week, and while it posted a solid 5.0 rating/8 share, with an increase in several major markets, ratings for the first season continued to fall, to a low of 3.4 during reruns, and then increasing again when new episodes were broadcast in July.
Ratings continued to remain low-to-middling throughout the first four seasons, but "Babylon 5" scored well with the demographics required to attract the leading national sponsors and saved up to $300,000 per episode by shooting off the studio lot, therefore remaining profitable for the network. The fifth season, shown on cable network TNT, had ratings about 1.0% lower than seasons two through four.
In the United Kingdom, "Babylon 5" was one of the better-rated US television shows on Channel 4, and achieved high audience Appreciation Indexes, with season 4's "Endgame" achieving the rare feat of beating the prime-time soap operas for first position.
On November 25, 1998, after five seasons and 109 aired episodes, "Babylon 5" successfully completed its five-year story arc when TNT aired the 110th (epilogue) episode "Sleeping in Light".
Awards.
Awards presented to "Babylon 5" include:
Nominated Awards include:
Babylon 5 media franchise.
In November 1994, DC began publishing monthly "Babylon 5" comics. A number of short stories and novels were also produced between 1995 and 2001. Additional books were published by the gaming companies Chameleon Eclectic and Mongoose Publishing, to support their desk-top strategy and role-playing games.
Three telefilms were released by Turner Network Television (TNT) in 1998, after funding a fifth season of "Babylon 5", following the demise of the Prime Time Entertainment Network the previous year. In addition to ', ', and ', they released a re-edited special edition of the original 1993 telefilm, '. In 1999, a fifth telefilm was also produced, "", which acted as a pilot movie for the spin-off series "Crusade", which TNT cancelled after 13 episodes had been filmed.
Dell Publishing started publication of a series of "Babylon 5" novels in 1995, which were ostensibly considered canon within the TV series' continuity, nominally supervised by creator J. Michael Straczynski, with later novels in the line being more directly based upon Straczynski's own notes and story outlines. In 1997, Del Rey obtained the publication license from Warner Bros., and proceeded to release a number of original trilogies directly scenarized by Straczynski, as well as novelizations of three of the TNT telefilms ("In the Beginning, Thirdspace", and "A Call to Arms"). All of the Del Rey novels are considered completely canonical within the filmic "Babylon 5" universe.
In 2000, the Sci-Fi Channel purchased the rights to rerun the "Babylon 5" series, and premiered a new telefilm, ' in 2002, which failed to be picked up as a series. In 2007, the first in a planned anthology of straight-to-DVD short stories entitled, ', was released by Warner Home Video, but no others were produced, due to funding issues.
At the 2014 San Diego Comic Con, Straczynski announced that a Babylon 5 movie is planned to go into production in 2016. It is to be a reboot of the story, but potentially one using old cast members in different roles. Studio JMS would produce it on a budget of $80–100 million if Warner Bros. do not take up the offer.
Home video releases.
In July 1995, Warner Home Video began distributing "Babylon 5" VHS video tapes under its Beyond Vision label in the UK. Beginning with the original telefilm, "", these were PAL tapes, showing video in the same 4:3 aspect ratio as the initial television broadcasts. By the release of Season 2, tapes included closed captioning of dialogue and Dolby Surround sound. Columbia House began distributing NTSC tapes, via mail order in 1997, followed by repackaged collector's editions and three-tape boxed sets in 1999, by which time the original pilot telefilm had been replaced by the re-edited TNT special edition. Additional movie and complete season boxed-sets were also released by Warner Bros. until 2000.
Image Entertainment released "Babylon 5" laserdiscs between December 1998 and September 1999. Produced on double-sided 12-inch Pioneer discs, each contained two episodes displayed in the 4:3 broadcast aspect-ratio, with Dolby Surround audio and closed captioning for the dialogue. Starting with two TNT telefilms, "" and the re-edited special edition of "The Gathering", Seasons 1 and 5 were released simultaneously over a six-month period. Seasons 2 and 4 followed, but with the decision to halt production due to a drop in sales, precipitated by rumors of a pending DVD release, only the first twelve episodes of Season 2 and the first six episodes of Season 4 were ultimately released.
In November 2001, Warner Home Video began distributing "Babylon 5" DVDs with a two-movie set containing the re-edited TNT special edition of "The Gathering" and "In The Beginning". The telefilms were later individually released in region 2 in April 2002, though some markets received the original version of "The Gathering" in identical packaging.
DVD boxed sets of the individual seasons, each containing six discs, began being released in October 2002. Each included a printed booklet containing episode summaries, with each disc containing audio options for German, French, and English, plus subtitles in a wider range of languages, including Arabic and Dutch. Video was displayed in anamorphic widescreen with Dolby Digital 5.1 sound. Disc 1 of each set contained an introduction to the season by J. Michael Straczynski, while disc 6 included featurettes containing interviews with various production staff, as well as information on the fictional universe, and a gag reel. Three episodes in each season also contained commentary from either Straczynski, members of the main cast, and/or the episode director.
Since its initial release, a number of repackaged DVD boxed sets have been produced for various regional markets. With slightly altered cover art, they included no additional content, but the discs were more securely stored in slimline cases, rather than the early "book" format, with hard plastic pages used during the original release of the first three seasons.
Other releases.
Seasons 1 and 2, and parts of Season 3, of "Babylon 5" have been released as advertisement-supported downloads through the In2TV and Hulu download services. Additionally, every episode from Seasons 1 to 5, as well as the pilot film "", are available for purchase on the Xbox Live Marketplace in the United States. All five seasons, and five of the movies ("In The Beginning", "Thirdspace", "River of Souls", "A Call To Arms", "Legend of the Rangers") are currently available through iTunes.
Mastering problems.
While the series was in pre-production, studios were looking at ways for their existing shows to make the transition from the then-standard to the widescreen formats that would accompany the next generation of televisions. After visiting Warner Bros., who were stretching the horizontal interval for an episode of "", producer John Copeland convinced them to allow "Babylon 5" to be shot on Super 35mm film stock. "The idea being that we would telecine to 4:3 for the original broadcast of the series. But what it also gave us was a negative that had been shot for the new 16×9 widescreen-format televisions that we knew were on the horizon."
Though the CGI scenes, and those containing live action combined with digital elements, could have been created in a suitable widescreen format, a cost-saving decision was taken to produce them in the 4:3 aspect ratio. The intention was to then crop the top and bottom of the images, and upscale the resolution for any future widescreen release or broadcast. In 2000, when the show was transferred to widescreen for airing on the Sci-Fi Channel prior to its eventual DVD release, the plan was not followed, as John Copeland recalls: "They did another video hack, and simply used a digital post production device like a DVE (Digital Video Enhancer) to blow the material up. They essentially stretched it approximately 1/3 to fill the larger aspect ratio."
The scenes containing live action ready to be composited with matte paintings, CGI animation, etc., were delivered on tape already telecined to the 4:3 aspect-ratio, and contained a high level of grain, which resulted in further image noise being present when enlarged and stretched for widescreen. For the purely live-action scenes, rather than using the film negatives, "Warners had even forgotten that they had those. They used PAL versions and converted them to NTSC for the US market. They actually didn't go back and retransfer the shows."
With the resulting aliasing, and the progressive scan transfer of the video to DVD, this has created a number of visual flaws throughout the widescreen release. In particular, quality has been noted to drop significantly in composite shots.

</doc>
<doc id="4801" url="http://en.wikipedia.org/wiki?curid=4801" title="BeOS">
BeOS

BeOS is an operating system for personal computers first developed by Be Inc. in 1991. It was first written to run on BeBox hardware. BeOS was built for digital media work and was written to take advantage of modern hardware facilities such as symmetric multiprocessing by utilizing modular I/O bandwidth, pervasive multithreading, preemptive multitasking and a 64-bit journaling file system known as BFS. The BeOS GUI was developed on the principles of clarity and a clean, uncluttered design.
BeOS was positioned as a multimedia platform which could be used by a substantial population of desktop users and a competitor to Mac OS and Microsoft Windows. However, it was ultimately unable to achieve a significant market share and proved commercially unviable for Be Inc. The company was acquired by Palm Inc. and today BeOS is mainly used and developed by a small population of enthusiasts.
The open-source OS Haiku, a complete reimplementation of BeOS, is designed to start up where BeOS left off. Alpha 4 of Haiku was released in November 2012.
Design.
BeOS was optimized for digital media work and was written to take advantage of modern computer hardware facilities such as symmetric multiprocessing by utilizing modular I/O bandwidth, pervasive multithreading, preemptive multitasking and a 64-bit journaling file system known as BFS. The BeOS GUI was developed on the principles of clarity and a clean, uncluttered design.
The API was written in C++ for ease of programming. It has partial POSIX compatibility and access to a command-line interface through Bash, although internally it is not a Unix-derived operating system.
BeOS used Unicode as the default encoding in the GUI, though support for input methods such as bidirectional text input was never realized.
History.
Initially designed to run on AT&T Hobbit-based hardware, BeOS was later modified to run on PowerPC-based processors: first Be's own systems, later Apple Inc.'s PowerPC Reference Platform and Common Hardware Reference Platform, with the hope that Apple would purchase or license BeOS as a replacement for its aging Mac OS. Apple CEO Gil Amelio started negotiations to buy Be Inc., but negotiations stalled when Be CEO Jean-Louis Gassée wanted $200 million; Apple was unwilling to offer any more than $125 million. Apple's board of directors decided NeXTSTEP was a better choice and purchased NeXT in 1996 for $429 million, bringing back Apple co-founder Steve Jobs.
In 1997, Power Computing began bundling BeOS (on a CD for optional installation) with its line of PowerPC-based Macintosh clones. These systems could dual boot either the Mac OS or BeOS, with a start-up screen offering the choice.
Due to Apple's moves and the mounting debt of Be Inc., BeOS was soon ported to the Intel x86 platform with its R3 release in March 1998. Through the late 1990s, BeOS managed to create a niche of followers, but the company failed to remain viable. Be Inc. also released a stripped-down, but free, copy of BeOS R5 known as BeOS Personal Edition (BeOS PE). BeOS PE could be started from within Microsoft Windows or Linux, and was intended to nurture consumer interest in its product and give developers something to tinker with. Be Inc. also released a stripped-down version of BeOS for Internet Appliances (BeIA), which soon became the company's business focus in place of BeOS.
In 2001 Be's copyrights were sold to Palm, Inc. for some $11 million. BeOS R5 is considered the last official version, but BeOS R5.1 "Dano", which was under development before Be's sale to Palm and included the BeOS Networking Environment (BONE) networking stack, was leaked to the public shortly after the company's demise.
In 2002, Be Inc. sued Microsoft claiming that Hitachi had been dissuaded from selling PCs loaded with BeOS, and that Compaq had been pressured not to market an Internet appliance in partnership with Be. Be also claimed that Microsoft acted to artificially depress Be Inc.'s initial public offering (IPO). The case was eventually settled out of court for $23.25 million with no admission of liability on Microsoft's part.
After the split from Palm, PalmSource used parts of BeOS's multimedia framework for its failed Palm OS Cobalt product. With the takeover of PalmSource, the BeOS rights now belong to Access Co. 
Continuation and clones.
In the years that followed the demise of Be Inc. a handful of projects formed to recreate BeOS or key elements of the OS with the eventual goal of then continuing where Be Inc. left off. This was facilitated by the fact that Be Inc. released some components of BeOS under a free licence. Here is a list of these projects:
Zeta was a commercially available operating system based on the BeOS R5.1 codebase. Originally developed by yellowTAB, the operating system was then distributed by magnussoft. During development by yellowTAB, the company received criticism from the BeOS community for refusing to discuss its legal position with regard to the BeOS codebase (perhaps for contractual reasons). Access Co. (which bought PalmSource, until then the holder of the intellectual property associated with BeOS) has since declared that yellowTAB had no right to distribute a modified version of BeOS, and magnussoft has ceased distribution of the operating system.
Products using BeOS.
BeOS (and now Zeta) continue to be used in media appliances such as the Edirol DV-7 video editors from Roland corporation which run on top of a modified BeOS and the TuneTracker radio automation software that runs on BeOS and Zeta, but is also sold as a "Station-in-a-Box" with the Zeta operating system included.
The Tascam SX-1 digital audio recorder runs a heavily modified version of BeOS that will only launch the recording interface software.
iZ Technology sells the RADAR 24 and RADAR V, hard disk-based, 24-track professional audio recorders based on BeOS 5, although the newer RADAR 6 is not based on BeOS.
Magicbox, a manufacturer of signage and broadcast display machines, uses BeOS to power their Aavelin product line.
Final Scratch, a 12″ vinyl timecode record-driven DJ software/hardware system, was first developed on BeOS. The "ProFS" version was sold to a few dozen DJs prior to the 1.0 release, which ran on a Linux virtual partition.

</doc>
<doc id="4802" url="http://en.wikipedia.org/wiki?curid=4802" title="Biome">
Biome

Biomes are climatically and geographically defined as contiguous areas with similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, and are often referred to as ecosystems. Some parts of the earth have more or less the same kind of abiotic and biotic factors spread over a large area, creating a typical ecosystem over that area. Such major ecosystems are termed as biomes. Biomes are defined by factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation (quasiequilibrium state of the local ecosystem). An ecosystem has many biotopes and a biome is a major habitat type. A major habitat type, however, is a compromise, as it has an intrinsic inhomogeneity. Some examples of habitats are ponds, trees, streams, creeks, under rocks and burrows in the sand or soil.
The biodiversity characteristic of each extinction, especially the diversity of fauna and subdominant plant forms, is a function of abiotic factors and the biomass productivity of the dominant vegetation. In terrestrial biomes, species diversity tends to correlate positively with net primary productivity, moisture availability, and temperature.
Ecoregions are grouped into both biomes and ecozones.
A fundamental classification of biomes are:
Biomes are often known in English by local names. For example, a temperate grassland or shrubland biome is known commonly as "steppe" in central Asia, prairie in North America, and "pampas" in South America. Tropical grasslands are known as savanna in Australia, whereas in southern Africa they are known as certain kinds of "veld" (from Afrikaans).
Sometimes an entire biome may be targeted for protection, especially under an individual nation's biodiversity action plan.
Climate is a major factor determining the distribution of terrestrial biomes. Among the important climatic factors are:
The most widely used systems of classifying biomes correspond to latitude (or temperature zoning) and humidity. Biodiversity generally increases away from the poles towards the equator and increases with humidity.
Biome classification schemes.
In this scheme, climates are classified based on the biological effects of temperature and rainfall on vegetation under the assumption that these two abiotic factors are the largest determinants of the type of vegetation found in an area. Holdridge uses the four axes to define 30 so-called "humidity provinces", which are clearly visible in the Holdridge diagram. While his scheme largely ignores soil and sun exposure, Holdridge did acknowledge that these, too, were important factors in biome determination.
Holdridge scheme.
Biomes are classification schemes defined by climatic parameters. Particularly in the 1970s and 1980s, there was a significant push to understand the relationships between these climatic parameters and properties of ecosystem energetics because such discoveries would enable the prediction of rates of energy capture and transfer among components within ecosystems. Such a study was conducted by Sims et al. (1978) on North American grasslands. The study found a positive logistic correlation between evapotranspiration in mm/yr and above-ground net primary production in g/m2/yr. More general results from the study were that precipitation and water use lead to above-ground primary production, solar radiation and temperature lead to belowground primary production (roots), and temperature and water lead to cool and warm season growth habit. These findings help explain the categories used in Holdridge’s bioclassification scheme, which were then later simplified in Whittaker’s. The number of classification schemes and the variety of determinants used in those schemes, however, should be taken as strong indicators that biomes do not all fit perfectly into the classification schemes created
Whittaker's biome-type classification scheme.
Whittaker appreciated biome-types as a representation of the great diversity of the living world, and saw the need to establish a simple way to classify them. He based his classification scheme on two abiotic factors: precipitation and temperature. His scheme can be seen as a simplification of Holdridge's, one more readily accessible, but perhaps missing the greater specificity that Holdridge's provides.
Whittaker based his representation of global biomes on both previous theoretical assertions and an ever-increasing empirical sampling of global ecosystems. He was in a unique position to make such a holistic assertion because he had previously compiled a review of biome classification.
Key definitions for understanding Whittaker's scheme.
Whittaker's distinction between biome and formation can be simplified: formation is used when applied to plant communities only, while biome is used when concerned with both plants and animals. Whittaker's convention of biome-type or formation-type is simply a broader method to categorize similar communities. 
Whittaker's parameters for classifying biome-types.
Whittaker, seeing the need for a simpler way to express the relationship of community structure to the environment, used what he called "gradient analysis" of ecocline patterns to relate communities to climate on a worldwide scale. Whittaker considered four main ecoclines in the terrestrial realm.
Along these gradients, Whittaker noted several trends that allowed him to qualitatively establish biome-types.
Whittaker summed the effects of gradients (3) and (4) to get an overall temperature gradient, and combined this with gradient (2), the moisture gradient, to express the above conclusions in what is known as the Whittaker classification scheme. The scheme graphs average annual precipitation (x-axis) versus average annual temperature (y-axis) to classify biome-types.
Walter system.
The Heinrich Walter classification scheme, developed by Heinrich Walter, a German ecologist, differs from both the Whittaker and Holdridge schemes because it takes into account the seasonality of temperature and precipitation. The system, also based on precipitation and temperature, finds 9 major biomes, with the important climate traits and vegetation types summarized in the accompanying table. The boundaries of each biome correlate to the conditions of moisture and cold stress that are strong determinants of plant form, and therefore the vegetation that defines the region. Extreme conditions, such as flooding in a swamp, can create different kinds of communities within the same biome.
Bailey system.
Robert G. Bailey almost developed a biogeographical classification system for the United States in a map published in 1976. He subsequently expanded the system to include the rest of North America in 1981, and the world in 1989. The Bailey system, based on climate, is divided into seven domains (polar, humid temperate, dry, humid, and humid tropical), with further divisions based on other climate characteristics (subarctic, warm temperate, hot temperate, and subtropical; marine and continental; lowland and mountain).
WWF system.
A team of biologists convened by the World Wide Fund for Nature (WWF) developed an ecological land classification system that identified fourteen biomes, called major habitat types, and further divided the world's land area into 882 terrestrial ecoregions(includes new Antarctic ecoregions by Terrauds et al. 2012). Each terrestrial ecoregion has a specific EcoID, format XXnnNN (XX is the ecozone, nn is the biome number, NN is the individual number). This classification is used to define the Global 200 list of ecoregions identified by the WWF as priorities for conservation. The WWF major habitat types are:
Freshwater biomes.
According to the WWF, the following are classified as freshwater biomes:
Marine biomes.
Marine biomes (H) (major habitat types), Global 200 (WWF).
Biomes of the coastal and continental shelf areas (neritic zone – List of ecoregions (WWF))
Summary – ecological taxonomy (WWF).
Example
Anthropogenic biomes.
Humans have fundamentally altered global patterns of biodiversity and ecosystem processes. As a result, vegetation forms predicted by conventional biome systems are rarely observed across most of Earth's land surface. Anthropogenic biomes provide an alternative view of the terrestrial biosphere based on global patterns of sustained direct human interaction with ecosystems, including agriculture, human settlements, urbanization, forestry and other uses of land. Anthropogenic biomes offer a new way forward in ecology and conservation by recognizing the irreversible coupling of human and ecological systems at global scales and moving us toward an understanding how best to live in and manage our biosphere and the anthropogenic biosphere we live in. The main biomes in the world are freshwater, marine, coniferous, deciduous, ice, mountains, boreal, grasslands, tundra, and rainforests.
Other biomes.
The endolithic biome, consisting entirely of microscopic life in rock pores and cracks, kilometers beneath the surface, has only recently been discovered, and does not fit well into most classification schemes.
Freshwater biomes.
The drainage basins of the principal oceans and seas of the world are marked by continental divides. The grey areas are endorheic basins that do not drain to the ocean.

</doc>
<doc id="4805" url="http://en.wikipedia.org/wiki?curid=4805" title="Behavior">
Behavior

Behavior or behaviour (see spelling differences) is the range of actions and mannerisms made by individuals, organisms, systems, or artificial entities in conjunction with themselves or their environment, which includes the other systems or organisms around as well as the (inanimate) physical environment. It is the response of the system or organism to various stimuli or inputs, whether internal or external, conscious or subconscious, overt or covert, and voluntary or involuntary.
Biology.
Although there is some disagreement as to how to precisely define behaviour in a biological context, one common interpretation based on a meta-analysis of scientific literature states that "behavior is the internally coordinated responses (actions or inactions) of whole living organisms (individuals or groups) to internal and/or external stimuli" 
Behaviors can be either innate or learned. 
Behavior can be regarded as any action of an organism that changes its relationship to its environment. Behavior provides outputs from the organism to the environment.
Human.
Human behavior is believed to be influenced by the endocrine system and the nervous system. It is most commonly believed that complexity in the behavior of an organism is correlated to the complexity of its nervous system. Generally, organisms with more complex nervous systems have a greater capacity to learn new responses and thus adjust their behavior.
Other fields.
Behavior outside of psychology includes physical property and chemical reactions.
Earth sciences.
In environmental modeling and especially in hydrology, a "behavioral model" means a model that is acceptably consistent with observed natural processes, i.e., that simulates well, for example, observed river discharge. It is a key concept of the so-called Generalized Likelihood Uncertainty Estimation (GLUE) methodology to quantify how uncertain environmental predictions are.
Management.
In management, behaviors are associated with desired or undesired focuses. Managers generally note what the desired outcome is, but behavioral patterns can take over. These patterns are the reference to how often the desired behavior actually occurs. Before a behavior actually occurs, antecedents focus on the stimuli that influence the behavior that is about to happen. After the behavior occurs, consequences fall into place. They can come in the form of rewards or punishments.

</doc>
<doc id="4806" url="http://en.wikipedia.org/wiki?curid=4806" title="Battle of Marathon">
Battle of Marathon

The Battle of Marathon (Greek: , "Machē tou Marathōnos") took place in 490 BC, during the first Persian invasion of Greece. It was fought between the citizens of Athens, aided by Plataea, and a Persian force commanded by Datis and Artaphernes. The battle was the culmination of the first attempt by Persia, under King Darius I, to subjugate Greece. The Greek army decisively defeated the more numerous Persians, marking a turning point in the Greco-Persian Wars.
The first Persian invasion was a response to Greek involvement in the Ionian Revolt, when Athens and Eretria had sent a force to support the cities of Ionia in their attempt to overthrow Persian rule. The Athenians and Eretrians had succeeded in capturing and burning Sardis, but they were than forced to retreat with heavy losses. In response to this raid, Darius swore to burn down Athens and Eretria. According to Herodotus, Darius asked for his bow, he placed an arrow upon the string and he discharged it upwards towards heaven, and as he shot into the air he said: "Zeus, grant me to take vengeance upon the Athenians!". Also he charged one of his servants, to say to him, every day before dinner, three times: "Master, remember the Athenians."
At the time of the battle, Sparta and Athens were the two largest city states. Once the Ionian revolt was finally crushed by the Persian victory at the Battle of Lade in 494 BC, Darius began plans to subjugate Greece. In 490 BC, he sent a naval task force under Datis and Artaphernes across the Aegean, to subjugate the Cyclades, and then to make punitive attacks on Athens and Eretrea. Reaching Euboea in mid-summer after a successful campaign in the Aegean, the Persians proceeded to besiege and capture Eretria. The Persian force then sailed for Attica, landing in the bay near the town of Marathon. The Athenians, joined by a small force from Plataea, marched to Marathon, and succeeded in blocking the two exits from the plain of Marathon.
The Greeks could not hope to face the superior Persian cavalry; however, when learning that the Persian cavalry was temporarily absent from the camp, Miltiades ordered a general attack against the Persians. He reinforced his flanks, luring the Persians' best fighters into his centre. The inward wheeling flanks enveloped the Persians, routing them. The Persian army broke in panic towards their ships, and large numbers were slaughtered. The defeat at Marathon marked the end of the first Persian invasion of Greece, and the Persian force retreated to Asia. Darius then began raising a huge new army with which he meant to completely subjugate Greece; however, in 486 BC, his Egyptian subjects revolted, indefinitely postponing any Greek expedition. After Darius died, his son Xerxes I restarted the preparations for a second invasion of Greece, which finally began in 480 BC.
The Battle of Marathon was a watershed in the Greco-Persian wars, showing the Greeks that the Persians could be beaten; the eventual Greek triumph in these wars can be seen to begin at Marathon. Since the following two hundred years saw the rise of the Classical Greek civilization, which has been enduringly influential in western society, the Battle of Marathon is often seen as a pivotal moment in European history. The battle is perhaps now more famous as the inspiration for the marathon race. Although thought to be historically inaccurate, the legend of the Greek messenger Pheidippides running to Athens with news of the victory became the inspiration for this athletic event, introduced at the 1896 Athens Olympics, and originally run between Marathon and Athens.
Sources.
The main source for the Greco-Persian Wars is the Greek historian Herodotus. Herodotus, who has been called the 'Father of History', was born in 484 BC in Halicarnassus, Asia Minor (then under Persian overlordship). He wrote his 'Enquiries' (Greek—"Historia"; English—"(The) Histories") around 440–430 BC, trying to trace the origins of the Greco-Persian Wars, which would still have been relatively recent history (the wars finally ended in 450 BC). Herodotus's approach was entirely novel, and at least in Western society, he does seem to have invented 'history' as we know it. As Holland has it: "For the first time, a chronicler set himself to trace the origins of a conflict not to a past so remote so as to be utterly fabulous, nor to the whims and wishes of some god, nor to a people's claim to manifest destiny, but rather explanations he could verify personally."
Some subsequent ancient historians, despite following in his footsteps, criticised Herodotus, starting with Thucydides. Nevertheless, Thucydides chose to begin his history where Herodotus left off (at the Siege of Sestos), and may therefore have felt that Herodotus's history was accurate enough not to need re-writing or correcting. Plutarch criticised Herodotus in his essay "On the malice of Herodotus", describing Herodotus as "Philobarbaros" (barbarian-lover), for not being pro-Greek enough, which suggests that Herodotus might actually have done a reasonable job of being even-handed. A negative view of Herodotus was passed on to Renaissance Europe, though he remained well read. However, since the 19th century his reputation has been dramatically rehabilitated by archaeological finds which have repeatedly confirmed his version of events. The prevailing modern view is that Herodotus generally did a remarkable job in his "Historia", but that some of his specific details (particularly troop numbers and dates) should be viewed with skepticism. Nevertheless, there are still some historians who believe Herodotus made up much of his story.
The Sicilian historian Diodorus Siculus, writing in the 1st century BC in his "Bibliotheca Historica", also provides an account of the Greco-Persian wars, partially derived from the earlier Greek historian Ephorus. This account is fairly consistent with Herodotus's. The Greco-Persian wars are also described in less detail by a number of other ancient historians including Plutarch, Ctesias of Cnidus, and are alluded by other authors, such as the playwright Aeschylus. Archaeological evidence, such as the Serpent Column, also supports some of Herodotus's specific claims.
Background.
The first Persian invasion of Greece had its immediate roots in the Ionian Revolt, the earliest phase of the Greco-Persian Wars. However, it was also the result of the longer-term interaction between the Greeks and Persians. In 500 BC the Persian Empire was still relatively young and highly expansionistic, but prone to revolts amongst its subject peoples. Moreover, the Persian king Darius was a usurper, and had spent considerable time extinguishing revolts against his rule. Even before the Ionian Revolt, Darius had begun to expand the Empire into Europe, subjugating Thrace, and forcing Macedon to become allied to Persia. Attempts at further expansion into the politically fractious world of Ancient Greece may have been inevitable. However, the Ionian Revolt had directly threatened the integrity of the Persian empire, and the states of mainland Greece remained a potential menace to its future stability. Darius thus resolved to subjugate and pacify Greece and the Aegean, and to punish those involved in the Ionian Revolt.
The Ionian revolt had begun with an unsuccessful expedition against Naxos, a joint venture between the Persian satrap Artaphernes and the Milesian tyrant Aristagoras. In the aftermath, Artaphernes decided to remove Aristagoras from power, but before he could do so, Aristagoras abdicated, and declared Miletus a democracy. The other Ionian cities followed suit, ejecting their Persian-appointed tyrants, and declaring themselves democracies. Aristagoras then appealed to the states of mainland Greece for support, but only Athens and Eretria offered to send troops.
The involvement of Athens in the Ionian Revolt arose from a complex set of circumstances, beginning with the establishment of the Athenian Democracy in the late 6th century BC.
In 510 BC, with the aid of Cleomenes I, King of Sparta, the Athenian people had expelled Hippias, the tyrant ruler of Athens. With Hippias's father Peisistratus, the family had ruled for 36 out of the previous 50 years and fully intended to continue Hippias's rule. Hippias fled to Sardis to the court of the Persian satrap, Artaphernes and promised control of Athens to the Persians if they were to help restore him. In the meantime, Cleomenes helped install a pro-Spartan tyranny under Isagoras in Athens, in opposition to Cleisthenes, the leader of the traditionally powerful Alcmaeonidae family, who considered themselves the natural heirs to the rule of Athens. Cleisthenes, however, found himself being politically defeated by a coalition led by Isagoras and decided to change the rules of the game by appealing to the demos (the people), in effect making them a new faction in the political arena. This tactic succeeded, but the Spartan King, Cleomenes I, returned at the request of Isagoras and so the Cleisthenes, the Alcmaeonids and other prominent Athenian families were exiled from Athens. When Isagoras attempted to create a narrow oligarchic government, the Athenian people, in a spontaneous and unprecedented move, expelled Cleomenes and Isagoras. Cleisthenes was thus restored to Athens (507 BC), and at breakneck speed began to reform the state with the aim of securing his position. The result was not actually a democracy or a real civic state, but he enabled the development of a fully democratic government, which would emerge in the next generation as the demos realized its power. The new-found freedom and self-governance of the Athenians meant that they were thereafter exceptionally hostile to the return of the tyranny of Hippias, or any form of outside subjugation; by Sparta, Persia or anyone else.
Cleomenes was not pleased with the events, and marched on Athens with the Spartan army. Cleomenes's attempts to restore Isagoras to Athens ended in a debacle, but fearing the worst, the Athenians had by this point already sent an embassy to Artaphernes in Sardis, to request aid from the Persian Empire. Artaphernes requested that the Athenians give him an 'earth and water', a traditional token of submission, which the Athenian ambassadors acquiesced to. However, they were severely censured for this when they returned to Athens. At some point later Cleomenes instigated a plot to restore Hippias to the rule of Athens. This failed and Hippias again fled to Sardis and tried to persuade the Persians to subjugate Athens. The Athenians dispatched ambassadors to Artaphernes to dissuade him from taking action, but Artaphernes merely instructed the Athenians to take Hippias back as tyrant. Needless to say, the Athenians balked at this, and resolved instead to be openly at war with Persia. Having thus become the enemy of Persia, Athens was already in a position to support the Ionian cities when they began their revolt. The fact that the Ionian democracies were inspired by the example of Athens no doubt further persuaded the Athenians to support the Ionian Revolt; especially since the cities of Ionia were originally Athenian colonies.
The Athenians and Eretrians sent a task force of 25 triremes to Asia Minor to aid the revolt. Whilst there, the Greek army surprised and outmaneuvered Artaphernes, marching to Sardis and burning the lower city. However, this was as much as the Greeks achieved, and they were then pursued back to the coast by Persian horsemen, losing many men in the process. Despite the fact that their actions were ultimately fruitless, the Eretrians and in particular the Athenians had earned Darius's lasting enmity, and he vowed to punish both cities. The Persian naval victory at the Battle of Lade (494 BC) all but ended the Ionian Revolt, and by 493 BC, the last hold-outs were vanquished by the Persian fleet. The revolt was used as an opportunity by Darius to extend the empire's border to the islands of the eastern Aegean and the Propontis, which had not been part of the Persian dominions before. The completion of the pacification of Ionia allowed the Persians to begin planning their next moves; to extinguish the threat to the empire from Greece, and to punish Athens and Eretria.
In 492 BC, once the Ionian Revolt had finally been crushed, Darius dispatched an to Greece under the command of his son-in-law, Mardonius. Mardonius re-conquered Thrace and compelled Alexander I of Macedon to make Macedon a client kingdom to Persia, before the wrecking of his fleet brought a premature end to the campaign.
However in 490 BC, following up the successes of the previous campaign, Darius decided to send a maritime expedition led by Artaphernes, (son of the satrap to whom Hippias had fled) and Datis, a Median admiral. Mardonius had been injured in the prior campaign and had fallen out of favor. The was intended to bring the Cyclades into the Persian empire, to punish Naxos (which had resisted a Persian assault in 499 BC) and then to head to Greece to force Eretria and Athens to submit to Darius or be destroyed. After island-hopping across the Aegean, including successfully attacking Naxos, the Persian task force arrived off Euboea in mid summer. The Persians then proceeded to besiege, capture and burn Eretria. They then headed south down the coast of Attica, en route to complete the final objective of the campaign—to punish Athens.
Prelude.
The Persians sailed down the coast of Attica, and landed at the bay of Marathon, roughly from Athens, on the advice of the exiled Athenian tyrant Hippias (who had accompanied the expedition). Under the guidance of Miltiades, the Athenian general with the greatest experience of fighting the Persians, the Athenian army marched quickly to block the two exits from the plain of Marathon, and prevent the Persians moving inland. At the same time, Athens's greatest runner, Pheidippides (or Philippides in some accounts) had been sent to Sparta to request that the Spartan army march to the aid of Athens. Pheidippides arrived during the festival of "Carneia", a sacrosanct period of peace, and was informed that the Spartan army could not march to war until the full moon rose; Athens could not expect reinforcement for at least ten days. The Athenians would have to hold out at Marathon for the time being, although they were reinforced by the full muster of 1,000 hoplites from the small city of Plataea; a gesture which did much to steady the nerves of the Athenians, and won unending Athenian gratitude to Plataea.
For approximately five days the armies therefore confronted each other across the plain of Marathon, in stalemate. The flanks of the Athenian camp were protected either by a grove of trees, or an "abbatis" of stakes (depending on the exact reading). Since every day brought the arrival of the Spartans closer, the delay worked in favor of the Athenians. There were ten Athenian "strategoi" (generals) at Marathon, elected by each of the ten tribes that the Athenians were divided into; Miltiades was one of these. In addition, in overall charge, was the War-Archon ("polemarch"), Callimachus, who had been elected by the whole citizen body. Herodotus suggests that command rotated between the "strategoi", each taking in turn a day to command the army. He further suggests that each "strategos", on his day in command, instead deferred to Miltiades. In Herodotus's account, Miltiades is keen to attack the Persians (despite knowing that the Spartans are coming to aid the Athenians), but strangely, chooses to wait until his actual day of command to attack. This passage is undoubtedly problematic; the Athenians had little to gain by attacking before the Spartans arrived, and there is no real evidence of this rotating generalship. There does, however, seem to have been a delay between the Athenian arrival at Marathon, and the battle; Herodotus, who evidently believed that Miltiades was eager to attack, may have made a mistake whilst seeking to explain this delay.
As is discussed below, the reason for the delay was probably simply that neither the Athenians nor the Persians were willing to risk battle initially. This then raises the question of why the battle occurred when it did. Herodotus explicitly tells us that the Greeks attacked the Persians (and the other sources confirm this), but it is not clear why they did this before the arrival of the Spartans. There are two main theories to explain this.
The first theory is that the Persian cavalry left Marathon for an unspecified reason, and that the Greeks moved to take advantage of this by attacking. This theory is based on the absence of any mention of cavalry in Herodotus' account of the battle, and an entry in the Suda dictionary. The entry "χωρίς ἰππεῖς" ("without cavalry") is explained thus: The cavalry left. When Datis surrendered and was ready for retreat, the Ionians climbed the trees and gave the Athenians the signal that the cavalry had left. And when Miltiades realized that, he attacked and thus won. From there comes the above-mentioned quote, which is used when someone breaks ranks before battle. There are many variations of this theory, but perhaps the most prevalent is that the cavalry was re-embarked on the ships, and was to be sent by sea to attack (undefended) Athens in the rear, whilst the rest of the Persians pinned down the Athenian army at Marathon. This theory therefore utilises Herodotus' suggestion that after Marathon, the Persian army re-embarked and tried to sail around Cape Sounion to attack Athens directly; however, according to the theory this attempt would have occurred "before" the battle (and indeed have triggered the battle).
The second theory is simply that the battle occurred because the Persians finally moved to attack the Athenians. Although this theory has the Persians moving to the "strategic" offensive, this can be reconciled with the traditional account of the Athenians attacking the Persians by assuming that, seeing the Persians advancing, the Athenians took the "tactical" offensive, and attacked them. Obviously, it cannot be firmly established which theory (if either) is correct. However, both theories imply that there was some kind of Persian activity which occurred on or about the fifth day which ultimately triggered the battle. It is also possible that both theories are correct: when the Persians sent the cavalry by ship to attack Athens, they simultaneously sent their infantry to attack at Marathon, triggering the Greek counterattack.
Date of the battle.
Herodotus mentions for several events a date in the lunisolar calendar, of which each Greek city-state used a variant. Astronomical computation allows us to derive an absolute date in the proleptic Julian calendar which is much used by historians as the chronological frame. Philipp August Böckh in 1855 concluded that the battle took place on September 12, 490 BC in the Julian calendar, and this is the conventionally accepted date. However, this depends on when exactly the Spartans held their festival and it is possible that the Spartan calendar was one month ahead of that of Athens. In that case the battle took place on August 12, 490 BC.
Opposing forces.
Athenians.
Herodotus does not give a figure for the size of the Athenian army. However, Cornelius Nepos, Pausanias and Plutarch all give the figure of 9,000 Athenians and 1,000 Plataeans; while Justin suggests that there were 10,000 Athenians and 1,000 Plataeans. These numbers are highly comparable to the number of troops Herodotus says that the Athenians and Plataeans sent to the Battle of Plataea 11 years later. Pausanias noticed on the monument to the battle the names of former slaves who were freed in exchange for military services. Modern historians generally accept these numbers as reasonable.
Persians.
According to Herodotus, the fleet sent by Darius consisted of 600 triremes. Herodotus does not estimate the size of the Persian army, only saying that they were a "large infantry that was well packed". Among ancient sources, the poet Simonides, another near-contemporary, says the campaign force numbered 200,000; while a later writer, the Roman Cornelius Nepos estimates 200,000 infantry and 10,000 cavalry, of which only 100,000 fought in the battle, while the rest were loaded into the fleet that was rounding Cape Sounion; Plutarch and Pausanias both independently give 300,000, as does the Suda dictionary. Plato and Lysias give 500,000; and Justinus 600,000.
Modern historians have proposed wide ranging numbers for the infantry, from 20,000–100,000 with a consensus of perhaps 25,000; estimates for the cavalry are in the range of 1,000.
Strategic and tactical considerations.
From a strategic point of view, the Athenians had some disadvantages at Marathon. In order to face the Persians in battle, the Athenians had to summon all available hoplites; and even then they were still probably outnumbered at least 2 to 1. Furthermore, raising such a large army had denuded Athens of defenders, and thus any secondary attack in the Athenian rear would cut the army off from the city; and any direct attack on the city could not be defended against. Still further, defeat at Marathon would mean the complete defeat of Athens, since no other Athenian army existed. The Athenian strategy was therefore to keep the Persian army pinned down at Marathon, blocking both exits from the plain, and thus preventing themselves from being outmaneuvered. However, these disadvantages were balanced by some advantages. The Athenians initially had no need to seek battle, since they had managed to confine the Persians to the plain of Marathon. Furthermore, time worked in their favour, as every day brought the arrival of the Spartans closer. Having everything to lose by attacking, and much to gain by not attacking, the Athenians remained on the defensive in the run up to the battle. Tactically, hoplites were vulnerable to attacks by cavalry, and since the Persians had substantial numbers of cavalry, this made any offensive maneuver by the Athenians even more of a risk, and thus reinforced the defensive strategy of the Athenians.
The Persian strategy, on the other hand, was probably principally determined by tactical considerations. The Persian infantry was evidently lightly armoured, and no match for hoplites in a head-on confrontation (as would be demonstrated at the later battles of Thermopylae and Plataea.) Since the Athenians seem to have taken up a strong defensive position at Marathon, the Persian hesitance was probably a reluctance to attack the Athenians head-on.
Whatever event eventually triggered the battle, it obviously altered the strategic or tactical balance sufficiently to induce the Athenians to attack the Persians. If the first theory is correct (see above), then the absence of cavalry removed the main Athenian tactical disadvantage, and the threat of being outflanked made it imperative to attack. Conversely, if the second theory is correct, then the Athenians were merely reacting to the Persians attacking them. Since the Persian force obviously contained a high proportion of missile troops, a static defensive position would have made little sense for the Athenians; the strength of the hoplite was in the melee, and the sooner that could be brought about, the better, from the Athenian point of view. If the second theory is correct, this raises the further question of why the Persians, having hesitated for several days, then attacked. There may have been several strategic reasons for this; perhaps they were aware (or suspected) that the Athenians were expecting reinforcements. Alternatively, since they may have felt the need to force some kind of victory—they could hardly remain at Marathon indefinitely.
Battle.
The distance between the two armies at the point of battle had narrowed to "a distance not less than 8 stadia" or about 1,500 meters. Miltiades ordered the two tribes that were forming the center of the Greek formation, the Leontis tribe led by Themistocles and the Antiochis tribe led by Aristides, to be arranged in the depth of four ranks while the rest of the tribes at their flanks were in ranks of eight. Some modern commentators have suggested this was a deliberate ploy to encourage a double envelopment of the Persian centre. However, this supposes a level of training that the Greeks did not possess. There is little evidence for any such tactical thinking in Greek battles until Leuctra in 371 BC. It is therefore probable that this arrangement was made, possibly at the last moment, so that the Athenian line was as long as the Persian line, and would not therefore be outflanked.
When the Athenian line was ready, according to one source, the simple signal to advance was given by Miltiades: "At them". Herodotus implies the Athenians ran the whole distance to the Persian lines, shouting their ululating war cry, "Ελελευ! Ελελευ!" ("Eleleu! Eleleu!"). It is doubtful that the Athenians ran the whole distance; in full armour this would be very difficult. More likely, they marched until they reached the limit of the archers' effectiveness, the "beaten zone" (roughly 200 meters), and then broke into a run towards their enemy. Another possibility is that they ran "up to" the 200 meter-mark in broken ranks, and then reformed for the march into battle from there. Herodotus suggests that this was the first time a Greek army ran into battle in this way; this was probably because it was the first time that a Greek army had faced an enemy composed primarily of missile troops. All this was evidently much to the surprise of the Persians; "... in their minds they charged the Athenians with madness which must be fatal, seeing that they were few and yet were pressing forwards at a run, having neither cavalry nor archers". Indeed, based on their previous experience of the Greeks, the Persians might be excused for this; Herodotus tells us that the Athenians at Marathon were "first to endure looking at Median dress and men wearing it, for up until then just hearing the name of the Medes caused the Hellenes to panic". Passing through the hail of arrows launched by the Persian army, protected for the most part by their armour, the Greek line finally collided with the enemy army. Holland provides an evocative description:
The enemy directly in their path ... realised to their horror that [the Athenians], far from providing the easy pickings for their bowmen, as they had first imagined, were not going to be halted ... The impact was devastating. The Athenians had honed their style of fighting in combat with other phalanxes, wooden shields smashing against wooden shields, iron spear tips clattering against breastplates of bronze ... in those first terrible seconds of collision, there was nothing but a pulverizing crash of metal into flesh and bone; then the rolling of the Athenian tide over men wearing, at most, quilted jerkins for protection, and armed, perhaps, with nothing more than bows or slings. The hoplites' ash spears, rather than shivering ... could instead stab and stab again, and those of the enemy who avoided their fearful jabbing might easily be crushed to death beneath the sheer weight of the advancing men of bronze.
The Athenian wings quickly routed the inferior Persian levies on the flanks, before turning inwards to surround the Persian centre, which had been more successful against the thin Greek centre. The battle ended when the Persian centre then broke in panic towards their ships, pursued by the Greeks. Some, unaware of the local terrain, ran towards the swamps where unknown numbers drowned. The Athenians pursued the Persians back to their ships, and managed to capture seven ships, though the majority were able to launch successfully. Herodotus recounts the story that Cynaegirus, brother of the playwright Aeschylus, who was also among the fighters, charged into the sea, grabbed one Persian trireme, and started pulling it towards shore. A member of the crew saw him, cut off his hand, and Cynaegirus died.
Herodotus records that 6,400 Persian bodies were counted on the battlefield, and it is unknown how many more perished in the swamps. The Athenians lost 192 men and the Plataeans 11. Among the dead were the war archon Callimachus and the general Stesilaos.
Aftermath.
In the immediate aftermath of the battle, Herodotus says that the Persian fleet sailed around Cape Sounion to attack Athens directly. As has been discussed above, some modern historians place this attempt just before the battle. Either way, the Athenians evidently realised that their city was still under threat, and marched as quickly as possible back to Athens.
The two tribes which had been in the centre of the Athenian line stayed to guard the battlefield under the command of Aristides. The Athenians arrived in time to prevent the Persians from securing a landing, and seeing that the opportunity was lost, the Persians turned about and returned to Asia. Connected with this episode, Herodotus recounts a rumour that this manoeuver by the Persians had been planned in conjunction with the Alcmaeonids, the prominent Athenian aristocratic family, and that a "shield-signal" had been given after the battle. Although many interpretations of this have been offered, it is impossible to tell whether this was true, and if so, what exactly the signal meant. On the next day, the Spartan army arrived at Marathon, having covered the in only three days. The Spartans toured the battlefield at Marathon, and agreed that the Athenians had won a great victory.
The dead of Marathon were buried on the battlefield. On the tomb of the Athenians this epigram composed by Simonides was written:
In the meanwhile, Darius began raising a huge new army with which he meant to completely subjugate Greece; however, in 486 BC, his Egyptian subjects revolted, indefinitely postponing any Greek expedition. Darius then died whilst preparing to march on Egypt, and the throne of Persia passed to his son Xerxes I. Xerxes crushed the Egyptian revolt, and very quickly restarted the preparations for the invasion of Greece. The epic second Persian invasion of Greece finally began in 480 BC, and the Persians met with initial success at the battles of Thermopylae and Artemisium. However, defeat at the Battle of Salamis would be the turning point in the campaign, and the next year the expedition was ended by the decisive Greek victory at the Battle of Plataea.
Significance.
The defeat at Marathon barely touched the vast resources of the Persian empire, yet for the Greeks it was an enormously significant victory. It was the first time the Greeks had beaten the Persians, proving that the Persians were not invincible, and that resistance, rather than subjugation, was possible.
The battle was a defining moment for the young Athenian democracy, showing what might be achieved through unity and self-belief; indeed, the battle effectively marks the start of a "golden age" for Athens. This was also applicable to Greece as a whole; "their victory endowed the Greeks with a faith in their destiny that was to endure for three centuries, during which western culture was born". John Stuart Mill's famous opinion was that "the Battle of Marathon, even as an event in British history, is more important than the Battle of Hastings". It seems that the Athenian playwright Aeschylus considered his participation at Marathon to be his greatest achievement in life (rather than his plays) since on his gravestone there was the following epigram:
Militarily, a major lesson for the Greeks was the potential of the hoplite phalanx. This style had developed during internecine warfare amongst the Greeks; since each city-state fought in the same way, the advantages and disadvantages of the hoplite phalanx had not been obvious. Marathon was the first time a phalanx faced more lightly armed troops, and revealed how effective the hoplites could be in battle. The phalanx formation was still vulnerable to cavalry (the cause of much caution by the Greek forces at the Battle of Plataea), but used in the right circumstances, it was now shown to be a potentially devastating weapon.
Legacy.
Legends associated with the battle.
The most famous legend associated with Marathon is that of the runner Pheidippides/Philippides bringing news to Athens of the battle, which is described below.
Pheidippides' run to Sparta to bring aid has other legends associated with it. Herodotus mentions that Pheidippides was visited by the god Pan on his way to Sparta (or perhaps on his return journey). Pan asked why the Athenians did not honor him and the awed Pheidippides promised that they would do so from then on. The god apparently felt that the promise would be kept, so he appeared in battle and at the crucial moment he instilled the Persians with his own brand of fear, the mindless, frenzied fear that bore his name: "panic". After the battle, a sacred precinct was established for Pan in a grotto on the north slope of the Acropolis, and a sacrifice was annually offered.
Similarly, after the victory the festival of the "Agroteras Thysia" ("sacrifice to the Agrotéra") was held at Agrae near Athens, in honor of Artemis Agrotera ("Artemis the Huntress"). This was in fulfillment of a vow made by the city before the battle, to offer in sacrifice a number of goats equal to that of the Persians slain in the conflict. The number was so great, it was decided to offer 500 goats yearly until the number was filled. Xenophon notes that at his time, 90 years after the battle, goats were still offered yearly.
Plutarch mentions that the Athenians saw the phantom of King Theseus, the mythical hero of Athens, leading the army in full battle gear in the charge against the Persians, and indeed he was depicted in the mural of the Stoa Poikile fighting for the Athenians, along with the twelve Olympian gods and other heroes. Pausanias also tells us that:
Another tale from the conflict is of the dog of Marathon. Aelian relates that one hoplite brought his dog to the Athenian encampment. The dog followed his master to battle and attacked the Persians at his master's side. He also informs us that this dog is depicted in the mural of the Stoa Poikile.
Marathon run.
According to Herodotus, an Athenian runner named Pheidippides was sent to run from Athens to Sparta to ask for assistance before the battle. He ran a distance of over 225 kilometers (140 miles), arriving in Sparta the day after he left. Then, following the battle, the Athenian army marched the 40 kilometers (25 miles) or so back to Athens at a very high pace (considering the quantity of armour, and the fatigue after the battle), in order to head off the Persian force sailing around Cape Sounion. They arrived back in the late afternoon, in time to see the Persian ships turn away from Athens, thus completing the Athenian victory.
Later, in popular imagination, these two events became confused with each other, leading to a legendary but inaccurate version of events. This myth has Pheidippides running from Marathon to Athens after the battle, to announce the Greek victory with the word "Nenikēkamen!" (Attic: ; We were victorious!), whereupon he promptly died of exhaustion. Most accounts incorrectly attribute this story to Herodotus; actually, the story first appears in Plutarch's "On the Glory of Athens" in the 1st century AD, who quotes from Heracleides of Pontus's lost work, giving the runner's name as either Thersipus of Erchius or Eucles. Lucian of Samosata (2nd century AD) gives the same story but names the runner Philippides (not Pheidippides). It should be noted that in some medieval codices of Herodotus the name of the runner between Athens and Sparta before the battle is given as Philippides and in a few modern editions this name is preferred.
When the idea of a modern Olympics became a reality at the end of the 19th century, the initiators and organizers were looking for a great popularizing event, recalling the ancient glory of Greece. The idea of organizing a 'marathon race' came from Michel Bréal, who wanted the event to feature in the first modern Olympic Games in 1896 in Athens. This idea was heavily supported by Pierre de Coubertin, the founder of the modern Olympics, as well as the Greeks. This would echo the legendary version of events, with the competitors running from Marathon to Athens. So popular was this event that it quickly caught on, becoming a fixture at the Olympic games, with major cities staging their own annual events. The distance eventually became fixed at 26 miles 385 yards, or 42.195 km, though for the first years it was variable, being around —the approximate distance from Marathon to Athens.
External links.
The has a journal article about this subject:

</doc>
<doc id="4810" url="http://en.wikipedia.org/wiki?curid=4810" title="Balance of trade">
Balance of trade

The commercial balance or net exports (sometimes symbolized as NX), is the difference between the monetary value of exports and imports of output in an economy over a certain period, measured in the currency of that economy. It is the relationship between a nation's imports and exports. A positive balance is known as a trade surplus if it consists of exporting more than is imported; a negative balance is referred to as a trade deficit or, informally, a trade gap. The balance of trade is sometimes divided into a goods and a services balance.
Understand Balance of Trade.
Trade, in general connotation, means the purchase and sales of commodities. In International Trade, purchase and sale are replaced by imports and exports. Balance of Trade is simply the difference between the value of exports and value of imports. Thus, the Balance of Trade denotes the differences of imports and exports of a merchandise of a country during the course of year. It indicates the value of exports and imports of the country in question. If the value of its exports over a period exceeds its value of imports, it is called favourable balance of trade and, Conversely, if the value of total imports exceeds the total value of exports over a period, it is unfavourable balance of trade. The favourable balance of trade indicates good economic condition of the country.
Policies of early modern Europe are grouped under the heading mercantilism. Early understanding of the imbalances of trade emerged from the practices and abuses of mercantilism in which colonial America's natural resources and cash crops were exported in exchange for finished goods from England, a factor leading to the American Revolution. An early statement appeared in "Discourse of the Common Wealth of this Realm of England", 1549: "We must always take heed that we buy no more from strangers than we sell them, for so should we impoverish ourselves and enrich them." Similarly a systematic and coherent explanation of balance of trade was made public through Thomas Mun's 1630 "England's treasure by foreign trade, or, The balance of our foreign trade is the rule of our treasure"
Definition.
The balance of trade forms part of the current account, which includes other transactions such as income from the net international investment position as well as international aid. If the current account is in surplus, the country's net international asset position increases correspondingly. Equally, a deficit decreases the net international asset position.
The trade balance is identical to the difference between a country's output and its domestic demand (the difference between what goods a country produces and how many goods it buys from abroad; this does not include money re-spent on foreign stock, nor does it factor in the concept of importing goods to produce for the domestic market).
Measuring the balance of trade can be problematic because of problems with recording and collecting data. As an illustration of this problem, when official data for all the world's countries are added up, exports exceed imports by almost 1%; it appears the world is running a positive balance of trade with itself. This cannot be true, because all transactions involve an equal credit or debit in the account of each nation. The discrepancy is widely believed to be explained by transactions intended to launder money or evade taxes, smuggling and other visibility problems. However, especially for developed countries, accuracy is likely.
Factors that can affect the balance of trade include:
In addition, the trade balance is likely to differ across the business cycle. In export-led growth (such as oil and early industrial goods), the balance of trade will improve during an economic expansion. However, with domestic demand led growth (as in the United States and Australia) the trade balance will worsen at the same stage in the business cycle.
Monetary balance of trade is different from physical balance of trade (which is expressed in amount of raw materials, known also as Total Material Consumption). Developed countries usually import a lot of raw materials from developing countries. Typically, these imported materials are transformed into finished products, and might be exported after adding value. Financial trade balance statistics conceal material flow. Most developed countries have a large physical trade deficit, because they have a large ecological footprint. Civil society organisations point out the predatory nature of this imbalance, and campaign for ecological debt repayment.
Since the mid-1980s, the United States has had a growing deficit in tradeable goods, especially with Asian nations (China and Japan) which now hold large sums of U.S debt that has funded the consumption. The U.S. has a trade surplus with nations such as Australia. The issue of trade deficits can be complex. Trade deficits generated in tradeable goods such as manufactured goods or software may impact domestic employment to different degrees than trade deficits in raw materials.
Economies such as Japan and Germany which have savings surpluses, typically run trade surpluses. China, a high-growth economy, has tended to run trade surpluses. A higher savings rate generally corresponds to a trade surplus. Correspondingly, the U.S. with its lower savings rate has tended to run high trade deficits, especially with Asian nations.
Views on economic impact.
Classical theory.
From Classical economic theory, those who ignore the effects of long run trade deficits may be confusing David Ricardo's principle of comparative advantage with Adam Smith's principle of absolute advantage, specifically ignoring the latter. The economist Paul Craig Roberts notes that the comparative advantage principles developed by David Ricardo do not hold where the factors of production are internationally mobile. Global labor arbitrage, a phenomenon described by economist Stephen S. Roach, where one country exploits the cheap labor of another, would be a case of absolute advantage that is not mutually beneficial. In 2010, economist Ian Fletcher authored a significant work entitled, "Free Trade Doesn't Work: What Should Replace It and Why", where he has supported a strategic approach to trade rather than an unconditional or unilateral approach.
Small trade deficits are generally not considered to be harmful to either the importing or exporting economy. However, when a national trade imbalance expands beyond prudence (generally thought to be several percent of GDP, for several years), adjustments tend to occur. While unsustainable imbalances may persist for long periods (cf, Singapore and New Zealand’s surpluses and deficits, respectively), the distortions likely to be caused by large flows of wealth out of one economy and into another tend to become intolerable.
In simple terms, trade deficits are paid for out of foreign exchange reserves, and may continue until such reserves are empty, at which point, the importer can no longer purchase abroad. This is likely to have exchange rate implications: a loss of value in the deficit economy’s currency relative to the surplus economy’s currency will change the relative price of tradable goods, and facilitate a return to balance or (quite commonly in historical data) an over-shooting into surplus, the other direction.
When an economy is unable to export enough physical goods to pay for its physical imports, it may be able to find funds elsewhere: Service exports, for example, are more than sufficient to pay for Hong Kong's domestic goods import. In poor countries, foreign aid may compensate, while in developed economies a capital account surplus caused by sales of assets often offsets a current-account deficit. There are some economies where transfers from nationals working abroad contribute significantly to paying for imports. The Philippines, Bangladesh and Mexico are examples of transfer-rich economies.
A country may rebalance the trade deficit by use of quantitative easing at home. This involves a central bank printing money and making it available to other domestic financial institutions at small interest rates, which increases the money supply in the home economy. Inflation usually results, which devalues in real terms the debt owed to foreign creditors if that debt was instantiated in the home currency.
Adam Smith on the balance of trade.
"In the foregoing part of this chapter I have endeavoured to show, even upon the principles of the commercial system, how unnecessary it is to lay extraordinary restraints upon the importation of goods from those countries with which the balance of trade is supposed to be disadvantageous.
Nothing, however, can be more absurd than this whole doctrine of the balance of trade, upon which, not only these restraints, but almost all the other regulations of commerce are founded. When two places trade with one another, this [absurd] doctrine supposes that, if the balance be even, neither of them either loses or gains; but if it leans in any degree to one side, that one of them loses and the other gains in proportion to its declension from the exact equilibrium." (Smith, 1776, book IV, ch. iii, part ii)
Keynesian theory.
In the last few years of his life, John Maynard Keynes was much preoccupied with the question of balance in international trade. He was the leader of the British delegation to the United Nations Monetary and Financial Conference in 1944 that established the Bretton Woods system of international currency management.
He was the principal author of a proposal – the so-called Keynes Plan — for an International Clearing Union. The two governing principles of the plan were that the problem of settling outstanding balances should be solved by 'creating' additional 'international money', and that debtor and creditor should be treated almost alike as disturbers of equilibrium. In the event, though, the plans were rejected, in part because "American opinion was naturally reluctant to accept the principle of equality of treatment so novel in debtor-creditor relationships".
His view, supported by many economists and commentators at the time, was that creditor nations may be just as responsible as debtor nations for disequilibrium in exchanges and that both should be under an obligation to bring trade back into a state of balance. Failure for them to do so could have serious consequences. In the words of Geoffrey Crowther, then editor of The Economist, "If the economic relationships between nations are not, by one means or another, brought fairly close to balance, then there is no set of financial arrangements that can rescue the world from the impoverishing results of chaos."
These ideas were informed by events prior to the Great Depression when – in the opinion of Keynes and others – international lending, primarily by the U.S., exceeded the capacity of sound investment and so got diverted into non-productive and speculative uses, which in turn invited default and a sudden stop to the process of lending.
Influenced by Keynes, economics texts in the immediate post-war period put a significant emphasis on balance in trade. For example, the second edition of the popular introductory textbook, "An Outline of Money", devoted the last three of its ten chapters to questions of foreign exchange management and in particular the 'problem of balance'. However, in more recent years, since the end of the Bretton Woods system in 1971, with the increasing influence of Monetarist schools of thought in the 1980s, and particularly in the face of large sustained trade imbalances, these concerns – and particularly concerns about the destabilising effects of large trade surpluses – have largely disappeared from mainstream economics discourse and Keynes' insights have slipped from view. They are receiving some attention again in the wake of the Financial crisis of 2007–2010.
Monetarist theory.
Prior to 20th century Monetarist theory, the 19th century economist and philosopher Frédéric Bastiat expressed the idea that trade deficits actually were a manifestation of profit, rather than a loss. He proposed as an example to suppose that he, a Frenchman, exported French wine and imported British coal, turning a profit. He supposed he was in France, and sent a cask of wine which was worth 50 francs to England. The customhouse would record an export of 50 francs. If, in England, the wine sold for 70 francs (or the pound equivalent), which he then used to buy coal, which he imported into France, and was found to be worth 90 francs in France, he would have made a profit of 40 francs. But the customhouse would say that the value of imports exceeded that of exports and was trade deficit against the ledger of France.
By "reductio ad absurdum", Bastiat argued that the national trade deficit was an indicator of a successful economy, rather than a failing one. Bastiat predicted that a successful, growing economy would result in greater trade deficits, and an unsuccessful, shrinking economy would result in lower trade deficits. This was later, in the 20th century, echoed by economist Milton Friedman.
In the 1980s, Milton Friedman, a Nobel Prize-winning economist and a proponent of Monetarism, contended that some of the concerns of trade deficits are unfair criticisms in an attempt to push macroeconomic policies favorable to exporting industries.
Friedman argued that trade deficits are not necessarily as important as high exports raise the value of the currency, reducing aforementioned exports, and vice versa for imports, thus naturally removing trade deficits not due to investment. Since 1971, when the Nixon administration decided to abolish fixed exchange rates, America's Current Account accumulated trade deficits have totaled $7.75 Trillion as of 2010. This deficit exists as it is matched by investment coming into the United States- purely by the definition of the balance of payments, any current account deficit that exists is matched by an inflow of foreign investment.
In the late 1970s and early 1980s, the U.S. had experienced high inflation and Friedman's policy positions tended to defend the stronger dollar at that time. He stated his belief that these trade deficits were not necessarily harmful to the economy at the time since the currency comes back to the country (country A sells to country B, country B sells to country C who buys from country A, but the trade deficit only includes A and B). However, it may be in one form or another including the possible tradeoff of foreign control of assets. In his view, the "worst-case scenario" of the currency never returning to the country of origin was actually the best possible outcome: the country actually purchased its goods by exchanging them for pieces of cheaply made paper. As Friedman put it, this would be the same result as if the exporting country burned the dollars it earned, never returning it to market circulation.
This position is a more refined version of the theorem first discovered by David Hume. Hume argued that England could not permanently gain from exports, because hoarding gold (i.e., currency) would make gold more plentiful in England; therefore, the prices of English goods would rise, making them less attractive exports and making foreign goods more attractive imports. In this way, countries' trade balances would balance out.
Friedman believed that deficits would be corrected by free markets as floating currency rates rise or fall with time to encourage or discourage imports in favor of the exports, reversing again in favor of imports as the currency gains strength. In the real world, a potential difficulty is that currency markets are far from a free market, with government and central banks being major players, and this is unlikely to change within the foreseeable future. Nevertheless, recent developments have shown that the global economy is undergoing a fundamental shift. For many years, the U.S. has borrowed and bought while in general, the rest of the world has lent and sold.
As of October 2007, the U.S. dollar weakened against the euro, British pound, and many other currencies. For instance, the euro hit $1.42 in October 2007, the strongest it has been since its birth in 1999. Against this backdrop, American exporters are finding quite favorable overseas markets for their products and U.S. consumers are responding to their general housing slowdown by slowing their spending. Furthermore, China, the Middle East, central Europe and Africa are absorbing more of the world's imports which in the end may result in a world economy that is more evenly balanced. All of this could well add up to a major readjustment of the U.S. trade deficit, which as a percentage of GDP, began in 1991.
Friedman contended that the structure of the balance of payments was misleading. In an interview with Charlie Rose, he stated that "on the books" the US is a net borrower of funds, using those funds to pay for goods and services. He essentially claimed that the foreign assets were not carried on the books at their higher, truer value.
Friedman presented his analysis of the balance of trade in "Free to Choose", widely considered his most significant popular work.
Trade Balances' effects upon their nation’s GDP.
Annual trade surpluses are immediate and direct additions to their nations’ GDPs.
To some extent exports’ induce additional increases to the GDPs that are not reflected within the export products’ prices; thus trade surpluses contributions to their GDP are generally understated.
Products’ prices generally reflect their producers’ production supporting expenditures. Producers often benefit from some production supporting goods and services at lesser or no cost to the producers.
For example, governments may deliberately locate or increase the capacity of their infrastructure, or provide other additional considerations to retain or attract producers within their own jurisdictions. The curriculum of a nation's schools and colleges may provide job applicants specifically suited to the producer’s needs, or provide specialized research and development.
All national factors of production, including education, contribute to GDP, and unless globally traded products fully reflect those goods and services, these other export supporting contributions are not entirely identified and attributed to their nations’ global trade.
Annual trade deficits are immediate and indirect reducers of their nations’ GDPs.
Trade deficits make no net contribution to their nations’ GDPs but the importing nations indirectly deny themselves of the benefits earned by producing nations; (refer to “Annual trade surpluses are immediate and direct additions to their nations’ GDPs”).
Among what’s being denied is familiarity with methods, practices, the manipulation of tools, materials and fabrication processes.
The economic differences between domestic and imported goods occur prior to the goods entry within the final purchasers' nations. After domestic goods have reached their producers shipping dock or imported goods have been unloaded on to the importing nation’s cargo vessel or entry port’s dock, similar goods have similar economic attributes.
Although supporting products not reflected within the prices of specific items are all captured within the producing nation’s GDP, those supporting but not reflected within prices of globally traded goods are not attributed to nations' global trade. Trade surpluses' contributions and trade deficits' detriments to their nation's GDPs are understated. The entire benefits of production are earned by the exporting nations and denied to the importing nation.
See also.
Lists:

</doc>
<doc id="4816" url="http://en.wikipedia.org/wiki?curid=4816" title="Biosphere">
Biosphere

The biosphere is the global sum of all ecosystems. It can also be termed the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating. By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere. The biosphere is postulated to have evolved, beginning with a process of biopoesis (life created naturally from non-living matter such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago. The earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia.
In a general sense, biospheres are any closed, self-regulating systems containing ecosystems; including artificial ones such as Biosphere 2 and BIOS-3; and, potentially, ones on other planets or moons.
Origin and use of the term.
The term "biosphere" was coined by geologist Eduard Suess in 1875, which he defined as:
While this concept has a geological origin, it is an indication of the effect of both Charles Darwin and Matthew F. Maury on the Earth sciences. The biosphere's ecological context comes from the 1920s ("see" Vladimir I. Vernadsky), preceding the 1935 introduction of the term "ecosystem" by Sir Arthur Tansley (see ecology history). Vernadsky defined ecology as the science of the biosphere. It is an interdisciplinary concept for integrating astronomy, geophysics, meteorology, biogeography, evolution, geology, geochemistry, hydrology and, generally speaking, all life and Earth sciences.
Narrow definition.
Geochemists define the biosphere as being the total sum of living organisms (the "biomass" or "biota" as referred to by biologists and ecologists). In this sense, the biosphere is but one of four separate components of the geochemical model, the other three being "lithosphere", "hydrosphere", and "atmosphere". The word "ecosphere", coined during the 1960s, encompasses both biological and physical components of the planet.
The Second International Conference on Closed Life Systems defined "biospherics" as the science and technology of analogs and models of Earth's biosphere; i.e., artificial Earth-like biospheres. Others may include the creation of artificial non-Earth biospheres—for example, human-centered biospheres or a native Martian biosphere—as part of the topic of biospherics.
Gaia hypothesis.
During the early 1970s, the British chemist James Lovelock and Lynn Margulis, a microbiologist from the United States, added to the hypothesis, specifically noting the ties between the biosphere and other Earth systems. For example, when carbon dioxide amounts increase in the atmosphere, plants grow more quickly. As their growth continues, they remove more and more carbon dioxide from the atmosphere.
Many scientists are now involved with new topics of study that examine interactions between biotic and abiotic factors in the biosphere, such as geobiology and geomicrobiology.
Ecosystems occur when communities and their physical environment work together as a system. The difference between this and a biosphere is simple—the biosphere is everything in general terms.
Extent of Earth's biosphere.
Every part of the planet, from the polar ice caps to the equator, features life of some kind. Recent advances in microbiology have demonstrated that microbes live deep beneath the Earth's terrestrial surface, and that the total mass of microbial life in so-called "uninhabitable zones" may, in biomass, exceed all animal and plant life on the surface. The actual thickness of the biosphere on earth is difficult to measure. Birds typically fly at altitudes of 650 to 1,800 metres, and fish that live deep underwater can be found down to -8,372 metres in the Puerto Rico Trench.
There are more extreme examples for life on the planet: Rüppell's vulture has been found at altitudes of 11,300 metres; bar-headed geese migrate at altitudes of at least 8,300 metres; yaks live at elevations between 3,200 to 5,400 metres above sea level; mountain goats live up to 3,050 metres. Herbivorous animals at these elevations depend on lichens, grasses, and herbs.
Microscopic organisms live at such extremes that, taking them into consideration, the thickness of the biosphere is much greater. Culturable microbes have been found in the Earth's upper atmosphere as high as (Wainwright et al., 2003, in FEMS Microbiology Letters). It is unlikely, however, that microbes are active at such altitudes, where temperatures and air pressure are extremely minor and ultraviolet radiation very intense. More likely, these microbes were brought into the upper atmosphere by winds or possibly volcanic eruptions. Barophilic marine microbes have been found at more than depth in the Mariana Trench (Takamia et al., 1997, in FEMS Microbiology Letters). In fact, single-celled life forms have been found in the deepest part of the Mariana Trench, Challenger Deep, at depths of 36,201 feet (11,034 meters). Microbes are not limited to the air, water or the Earth's surface. Culturable thermophilic microbes have been extracted from cores drilled more than into the Earth's crust in Sweden, from rocks between 65-75 °C.
Temperature increases with increasing depth into the Earth's crust. The rate at which the temperature increases depends on many factors, including type of crust (continental vs. oceanic), rock type, geographic location, etc. The greatest known temperature at which microbial life can exist is 122 °C ("Methanopyrus kandleri" Strain 116), and it is likely that the limit of life in the "deep biosphere" is defined by temperature rather than absolute depth.
[Our biosphere is divided into a number of biomes, inhabited by fairly similar flora and fauna. On land, biomes are separated primarily by latitude. Terrestrial biomes lying within the Arctic and Antarctic Circles are relatively barren of plant and animal life, while most of the more populous biomes lie near the equator. Terrestrial organisms in temperate and Arctic biomes have relatively small amounts of total biomass, smaller energy requirements, and display prominent adaptations to cold, including world-spanning migrations, social adaptations, homeothermy, estivation and multiple layers of insulation.]
Specific biospheres.
For this list, if a word is followed by a number, it is usually referring to a specific system or number. Thus:
Extraterrestrial biospheres.
No biospheres have been detected beyond the Earth; therefore, the existence of extraterrestrial biospheres remains hypothetical. The rare Earth hypothesis suggests they should be very rare, save ones composed of microbial life only. On the other hand, new research suggests that Earth analogs may be quite numerous, at least in the Milky Way galaxy. Given limited understanding of abiogenesis, it is currently unknown what percentage of these planets actually develop biospheres.
It is also possible that artificial biospheres will be created during the future, for example on Mars. The process of creating an uncontained system that mimics the function of Earth's biosphere is called terraforming.

</doc>
<doc id="4817" url="http://en.wikipedia.org/wiki?curid=4817" title="Biological membrane">
Biological membrane

A biological membrane or biomembrane is an enclosing or separating membrane that acts as a selectively permeable barrier within living things. Biological membranes, in the form of cell membranes, often consist of a phospholipid bilayer with embedded, integral and peripheral proteins used in communication and transportation of chemicals and ions. Bulk lipid in membrane provides a fluid matrix for proteins to rotate and laterally diffuse for physiological functioning. Proteins are adapted to high membrane fluidity environment of lipid bilayer with the presence of an annular lipid shell, consisting of lipid molecules bound tightly to surface of integral membrane proteins. The cellular membranes should not be confused with isolating tissues formed by layers of cells, such as mucous membranes and basement membranes.
Function.
The phospholipid bilayer contains a charged hydrophilic head, which interacts with polar water. It also contains a hydrophobic tail, which meets with the hydrophobic tail of the complementary layer. This maintains the fluidity of the cell.
Membranes in cells typically define enclosed spaces or compartments in which cells may maintain a chemical or biochemical environment that differs from the outside. For example, the membrane around peroxisomes shields the rest of the cell from peroxides, chemicals that can be toxic to the cell, and the cell membrane separates a cell from its surrounding medium. Peroxisomes are one form of vacuole found in the cell that contain by-products of chemical reactions within the cell. Most organelles are defined by such membranes, and are called "membrane-bound" organelles.
Selective Permeability.
Probably the most important feature of a biomembrane is that it is a selectively permeable structure. This means that the size, charge, and other chemical properties of the atoms and molecules attempting to cross it will determine whether they succeed in doing so. Selective permeability is essential for effective separation of a cell or organelle from its surroundings. Biological membranes also have certain mechanical or elastic properties that allow them to change shape and move as required.
Generally, small hydrophobic molecules can readily cross phospholipid bilayers by simple diffusion.
Particles that are required for cellular function but are unable to diffuse freely across a membrane enter through a membrane transport protein or are taken in by means of endocytosis, where the membrane allows for a vacuole to join onto it and push its contents into the cell. Many types of specialized plasma membranes can separate cell from external environment: apical, basolateral, presynaptic and postsynaptic ones, membranes of flagella, cilia, microvillus, filopodia and lamellipodia, the sarcolemma of muscle cells, as well as specialized myelin and dendritic spine membranes of neurons. Plasma membranes can also form different types of "supramembrane" structures such as caveola, postsynaptic density, podosome, invadopodium, desmosome, hemidesmosome, focal adhesion, and cell junctions. These types of membranes differ in lipid and protein composition.
Distinct types of membranes also create intracellular organelles: endosome; smooth and rough endoplasmic reticulum; sarcoplasmic reticulum; Golgi apparatus; lysosome; mitochondrion (inner and outer membranes); nucleus (inner and outer membranes); peroxisome; vacuole; cytoplasmic granules; cell vesicles (phagosome, autophagosome, clathrin-coated vesicles, COPI-coated and COPII-coated vesicles) and secretory vesicles (including synaptosome, acrosomes, melanosomes, and chromaffin granules).
Different types of biological membranes have diverse lipid and protein compositions. The content of membranes defines their physical and biological properties. Some components of membranes play a key role in medicine, such as the efflux pumps that pump drugs out of a cell.

</doc>
<doc id="4819" url="http://en.wikipedia.org/wiki?curid=4819" title="Balfour Declaration of 1926">
Balfour Declaration of 1926

The Balfour Declaration of 1926, issued by the 1926 Imperial Conference of British Empire leaders in London, was named for Lord President of the Council (and former Prime Minister of the United Kingdom) Arthur Balfour. It declared the United Kingdom and the Dominions to be
The Inter-Imperial Relations Committee, chaired by Balfour, drew up the document preparatory to its unanimous approval by the imperial premiers on 15 November 1926. It was first proposed by South African Prime Minister J. B. M. Hertzog and Canada's Prime Minister at that time, William Lyon Mackenzie King.
The Declaration accepted the growing political and diplomatic independence of the Dominions, in the years after World War I. It also recommended that the governors-general, the representatives of the King who acted for the Crown as "de facto" head of state in each dominion, should no longer also serve automatically as the representative of the British government in diplomatic relations between the countries. In following years, High Commissioners were gradually appointed, whose duties were soon recognised to be virtually identical to those of an ambassador. The first such British High Commissioner was appointed to Ottawa in 1928.
The conclusions of the imperial premiers conference of 1926 were restated by the 1930 conference and incorporated in the Statute of Westminster of December 1931, by which the British parliament renounced any legislative authority over dominion affairs, except as specifically provided in law.

</doc>
<doc id="4820" url="http://en.wikipedia.org/wiki?curid=4820" title="Balfour Declaration">
Balfour Declaration

The Balfour Declaration (dated 2 November 1917) was a letter from the United Kingdom's Foreign Secretary Arthur James Balfour to Baron Rothschild (Walter Rothschild, 2nd Baron Rothschild), a leader of the British Jewish community, for transmission to the Zionist Federation of Great Britain and Ireland.
The text of the letter was published in the press one week later, on 9 November 1917. The "Balfour Declaration" was later incorporated into the Sèvres peace treaty with the Ottoman Empire and the Mandate for Palestine. The original document is kept at the British Library.
Background.
World War I.
In 1914, war broke out in Europe between the Triple Entente (Britain, France and the Russian Empire) and the Central Powers (Germany, Austria-Hungary and later that year, the Ottoman Empire). The war on the Western Front developed into a stalemate. Jonathan Schneer writes:
Thus the view from Whitehall early in 1916: If defeat was not imminent, neither was victory; and the outcome of the war of attrition on the Western Front could not be predicted. The colossal forces in a death-grip across Europe and in Eurasia appeared to have canceled each other out. Only the addition of significant new forces on one side or the other seemed likely to tip the scale. Britain's willingness, beginning early in 1916, to explore seriously some kind of arrangement with "world Jewry" or "Great Jewry" must be understood in this context.
Zionism.
In 1896, Theodor Herzl, a Jewish journalist living in Austria-Hungary, published "Der Judenstaat" ("The Jews' State" or "The State of the Jews"), in which he asserted that the only solution to the "Jewish Question" in Europe, including growing antisemitism, was through the establishment of a state for the Jews. Political Zionism had just been born. A year later, Herzl founded the Zionist Organization (ZO), which at its first congress, "called for the establishment of a home for the Jewish people in Palestine secured under public law". Serviceable means to attain that goal included the promotion of Jewish settlement there, the organisation of Jews in the diaspora, the strengthening of Jewish feeling and consciousness, and preparatory steps to attain those necessary governmental grants. Herzl passed away in 1904 without the political standing that was required to carry out his agenda of a Jewish home in Palestine.
During the first meeting between Chaim Weizmann and Balfour in 1906, Balfour asked what Weizmann's objections were to the idea of a Jewish homeland in Uganda, (the Uganda Protectorate in East Africa in the British Uganda Programme), rather than in Palestine. According to Weizmann's memoir, the conversation went as follows:
Two months after Britain's declaration of war on the Ottoman Empire in November 1914, Zionist British cabinet member Herbert Samuel circulated a memorandum entitled "" to his cabinet colleagues. The memorandum stated that "I am assured that the solution of the problem of Palestine which would be much the most welcome to the leaders and supporters of the Zionist movement throughout the world would be the annexation of the country to the British Empire".
The McMahon–Hussein Correspondence.
Henry McMahon had exchanged letters with Hussein bin Ali, Sharif of Mecca in 1915, in which he had promised Hussein control of Arab lands with the exception of "portions of Syria" lying to the west of "the districts of Damascus, Homs, Hama and Aleppo". Palestine lay to the southwest of the Vilayet of Damascus and wasn't explicitly mentioned. That modern-day Lebanese region of the Mediterranean coast was set aside as part of a future French Mandate. After the war the extent of the coastal exclusion was hotly disputed. Hussein had protested that the Arabs of Beirut would greatly oppose isolation from the Arab state or states, but did not bring up the matter of Jerusalem or Palestine. Dr. Chaim Weizmann wrote in his autobiography "Trial and Error" that Palestine had been excluded from the areas that should have been Arab and independent. This interpretation was supported explicitly by the British government in the 1922 White Paper.
On the basis of McMahon's assurances the Arab Revolt began on 5 June 1916. However, the British and French also secretly concluded the Sykes–Picot Agreement on 16 May 1916. This agreement divided many Arab territories into British- and French-administered areas and allowed for the internationalisation of Palestine. Hussein learned of the agreement when it was leaked by the new Russian government in December 1917, but was satisfied by two disingenuous telegrams from Sir Reginald Wingate, High Commissioner of Egypt, assuring him that the British government's commitments to the Arabs were still valid and that the Sykes-Picot Agreement was not a formal treaty.
Hussein called on the Arab population in Palestine to welcome the Jews as brethren and co-operate with them for the common welfare. Following the publication of the Declaration the British had dispatched Commander David George Hogarth to see Hussein in January 1918 bearing the message that the "political and economic freedom" of the Palestinian population was not in question. Hogarth reported that Hussein "would not accept an independent Jewish State in Palestine, nor was I instructed to warn him that such a state was contemplated by Great Britain". Continuing Arab disquiet over Allied intentions also led during 1918 to the British Declaration to the Seven and the Anglo-French Declaration, the latter promising "the complete and final liberation of the peoples who have for so long been oppressed by the Turks, and the setting up of national governments and administrations deriving their authority from the free exercise of the initiative and choice of the indigenous populations."
Lord Grey had been the Foreign Secretary during the McMahon-Hussein negotiations. Speaking in the House of Lords on 27 March 1923, he made it clear that he entertained serious doubts as to the validity of the British government's interpretation of the pledges which he, as foreign secretary, had caused to be given to Hussein in 1915. He called for all of the secret engagements regarding Palestine to be made public. Many of the relevant documents in the National Archives were later declassified and published. Among them were the minutes of a Cabinet Eastern Committee meeting, chaired by Lord Curzon,which was held on 5 December 1918. Balfour was in attendance. The minutes revealed that in laying out the government's position Curzon had explained that: "Palestine was included in the areas as to which Great Britain pledged itself that they should be Arab and independent in the future".
Sykes–Picot Agreement.
In May 1916 the governments of the United Kingdom, France and Russia signed the Sykes–Picot Agreement, which defined their proposed spheres of influence and control in Western Asia should the Triple Entente succeed in defeating the Ottoman Empire during World War I. The agreement effectively divided the Arab provinces of the Ottoman Empire outside the Arabian peninsula into areas of future British and French control or influence.
The agreement proposed that an "international administration" would be established in an area shaded brown on the agreement's map, which was later to become Palestine, and that the form of the administration would be "decided upon after consultation with Russia, and subsequently in consultation with the other allies, and the representatives of the Sherif of Mecca". Zionists believed their aspirations had been passed over. William Reginald Hall, British Director of Naval Intelligence criticised the agreement on the basis that "the Jews have a strong material, and a very strong political, interest in the future of the country" and that "in the Brown area the question of Zionism, and also of British control of all Palestine railways, in the interest of Egypt, have to be considered".
Motivation for the Declaration.
British Government.
James Gelvin, a Middle East history professor, cites at least three reasons for why the British government chose to support Zionist aspirations. Issuing the Balfour Declaration would appeal to Woodrow Wilson's two closest advisors, who were avid Zionists.
"The British did not know quite what to make of President Woodrow Wilson and his conviction (before America's entrance into the war) that the way to end hostilities was for both sides to accept "peace without victory." Two of Wilson's closest advisors, Louis Brandeis and Felix Frankfurter, were avid Zionists. How better to shore up an uncertain ally than by endorsing Zionist aims? The British adopted similar thinking when it came to the Russians, who were in the midst of their revolution. Several of the most prominent revolutionaries, including Leon Trotsky, were of Jewish descent. Why not see if they could be persuaded to keep Russia in the war by appealing to their latent Jewishness and giving them another reason to continue the fight?" ... These include not only those already mentioned but also Britain's desire to attract Jewish financial resources.
At that time the British were busy making promises. At a War Cabinet meeting, held on 31 October 1917, Balfour suggested that a declaration favourable to Zionist aspirations would allow Great Britain "'to carry on extremely useful propaganda both in Russia and America"
The cabinet believed that expressing support would appeal to Jews in Germany and America, and help the war effort. It was also hoped to encourage support from the large Jewish population in Russia. Britain promoted the idea of a national home for the Jewish People, in the hope that Britain would implement it and exercise political control over Palestine, effectively "freeze out France (and anyone else) from any post–war presence in Palestine." According to James Renton, Senior Lecturer at Edge Hill University, an Honorary Research Fellow at University College London, and author of "The Zionist Masquerade: the Birth of the Anglo-Zionist Alliance: 1914–1918" (2007), Prime Minister David Lloyd George of the United Kingdom supported the creation of a Jewish homeland in Palestine because "it would help secure post-war British control of Palestine, which was strategically important as a buffer to Egypt and the Suez Canal.". In addition, Palestine was to later serve as a terminus for the flow of petroleum from Iraq via Jordan, three former Ottoman Turkish provinces that became British League of Nations mandates in the aftermath of the First World War. The oil officially flowed along the Mosul-Haifa oil pipeline from 1935–1948, and unofficially up until 1954.
David Lloyd George, who was Prime Minister at the time of the Balfour Declaration, told the Palestine Royal Commission in 1937 that the Declaration was made "due to propagandist reasons". Citing the position of the Allied and Associated Powers in the ongoing war, Lloyd George said that (in the Report's words) "In this critical situation it was believed that Jewish sympathy or the reverse would make a substantial difference one way or the other to the Allied cause. In particular Jewish sympathy would confirm the support of American Jewry, and would make it more difficult for Germany to reduce her military commitments and improve her economic position on the eastern front." Lloyd George then saidThe Zionist leaders gave us a definite promise that, if the Allies committed themselves to giving facilities for the establishment of a national home for the Jews in Palestine, they would do their best to rally Jewish sentiment and support throughout the world to the Allied cause. They kept their word.Regarding the intended future of Palestine, Lloyd George testified:The idea was, and this was the interpretation put upon it at the time, that a Jewish State was not to be set up immediately by the Peace Treaty without reference to the wishes of the majority of the inhabitants. On the other hand, it was contemplated that when the time arrived for according representative institutions to Palestine, if the Jews had meanwhile responded to the opportunity afforded them by the idea of a national home and had become a definite majority of the inhabitants, then PaIestine would thus become a Jewish Commonwealth.
Weizmann-Balfour relationship.
One of the main proponents of a Jewish homeland in Palestine was Chaim Weizmann, the leading spokesperson in Britain for organised Zionism. Weizmann was a chemist who had developed a process to synthesize acetone via fermentation. Acetone is required for the production of cordite, a powerful propellant explosive needed to fire ammunition without generating tell-tale smoke. Germany had cornered supplies of calcium acetate, a major source of acetone. Other pre-war processes in Britain were inadequate to meet the increased demand in World War I, and a shortage of cordite would have severely hampered Britain's war effort. Lloyd-George, then minister for munitions, was grateful to Weizmann and so supported his Zionist aspirations. In his "War Memoirs", Lloyd-George wrote of meeting Weizmann in 1916 that Weizmann:
This may, however, have been only a part of a longer series of discussions about Britain and Zionism held between Weizmann and Balfour which had begun at least a decade earlier. In late 1905 Balfour had requested of Charles Dreyfus, his Jewish constituency representative, that he arrange a meeting with Weizmann, during which Weizmann asked for official British support for Zionism; they were to meet again on this issue in 1914.
Jewish national home vs. Jewish state.
Explication of the wording of the Balfour Declaration is found in the correspondence leading to the final version of the declaration. The phrase "national home" was intentionally used instead of "state" because of opposition to the Zionist program within the British Cabinet. Following discussion of the initial draft the Cabinet Secretary, Mark Sykes, met with the Zionist negotiators to clarify their aims. His official report back to the Cabinet categorically stated that the Zionists did not want "to set up a Jewish Republic or any other form of state in Palestine immediately" but rather preferred some form of protectorate as provided in the Palestine Mandate.
In approving the Balfour Declaration, Leopold Amery, one of the Secretaries to the British War Cabinet of 1917–18, testified under oath to the Anglo-American Committee of Inquiry in January 1946 from his personal knowledge that:
"The phrase 'the establishment in Palestine of a National Home for the Jewish people' was intended and understood by all concerned to mean at the time of the Balfour Declaration that Palestine would ultimately become a 'Jewish Commonwealth' or a 'Jewish State', if only Jews came and settled there in sufficient numbers."
Both the Zionist Organization and the British government devoted efforts over the following decades, including Winston Churchill's 1922 White Paper, to denying that a state was the intention. However, in private, many British officials agreed with the interpretation of the Zionists that a state would be established when a Jewish majority was achieved.
The initial draft of the declaration, contained in a letter sent by Rothschild to Balfour, referred to the principle "that Palestine should be reconstituted as the National Home of the Jewish people." In the final text, the word "that" was replaced with "in" to avoid committing the entirety of Palestine to this purpose. Similarly, an early draft did not include the commitment that nothing should be done which might prejudice the rights of the non-Jewish communities. These changes came about partly as the result of the urgings of Edwin Samuel Montagu, an influential anti-Zionist Jew and Secretary of State for India, who was concerned that the declaration without those changes could result in increased anti-Semitic persecution. The draft was circulated and during October the government received replies from various representatives of the Jewish community. Lord Rothschild took exception to the new proviso on the basis that it presupposed the possibility of a danger to non-Zionists, which he denied. At San Remo, as shown in the transcript of the San Remo meeting on the evening of 24 April, the French proposed adding to the savings clause so that it would save for non-Jewish communities their "political rights" as well as their civil and religious rights. The French proposal was rejected.
Authorship.
Sir John Evelyn Shuckburgh of the new Middle East department of the Foreign Office discovered that the correspondence prior to the declaration was not available in the Colonial Office, 'although Foreign Office papers were understood to have been lengthy and to have covered a considerable period'." The 'most comprehensive explanation' of the origin of the Balfour Declaration the Foreign Office was able to provide was contained in a small 'unofficial' note of Jan 1923 affirming that:little is known of how the policy represented by the Declaration was first given form. Four, or perhaps five men were chiefly concerned in the labour – the Earl of Balfour, the late Sir Mark Sykes, and Messrs. Weizmann and Sokolow, with perhaps Lord Rothschild as a figure in the background. Negotiations seem to have been mainly oral and by means of private notes and memoranda of which only the scantiest records seem to be available.
In his posthumously published 1981 book "The Anglo-American Establishment", Georgetown University history professor Carroll Quigley explained that the Balfour Declaration was actually drafted by Lord Alfred Milner. Quigley wrote:
This declaration, which is always known as the Balfour Declaration, should rather be called "the Milner Declaration," since Milner was the actual draftsman and was, apparently, its chief supporter in the War Cabinet. This fact was not made public until 21 July 1937. At that time Ormsby-Gore, speaking for the government in Commons, said, "The draft as originally put up by Lord Balfour was not the final draft approved by the War Cabinet. The particular draft assented to by the War Cabinet and afterwards by the Allied Governments and by the United States...and finally embodied in the Mandate, happens to have been drafted by Lord Milner. The actual final draft had to be issued in the name of the Foreign Secretary, but the actual draftsman was Lord Milner."
More recently, William D. Rubinstein, Professor of Modern History at Aberystwyth University, Wales, wrote that Conservative politician and pro-Zionist Leo Amery, as Assistant Secretary to the British war cabinet in 1917, was the main author of the Balfour Declaration.
Reaction to the Declaration.
Arab opposition.
The Arabs expressed disapproval in November 1918 at the parade marking the first anniversary of the Balfour Declaration. The Muslim-Christian Association protested the carrying of new "white and blue banners with two inverted triangles in the middle". They drew the attention of the authorities to the serious consequences of any political implications in raising the banners.
Later that month, on the first anniversary of the occupation of Jaffa by the British, the Muslim-Christian Association sent a lengthy memorandum and petition to the military governor protesting once more any formation of a Jewish state.
On November 1918 the large group of Palestinian Arab dignitaries and representatives of political associations addressed a petition to the British authorities in which they denounced the declaration. The document stated:
Zionist reaction.
Chaim Weizmann and Nahum Sokolow, the principal Zionist leaders based in London, had asked for the reconstitution of Palestine as "the" Jewish national home. As such, the declaration fell short of Zionist expectations.
British opinion.
British public and government opinion became increasingly less favourable to the commitment that had been made to Zionist policy. In February 1922, Winston Churchill telegraphed Herbert Samuel asking for cuts in expenditure and noting:In both Houses of Parliament there is growing movement of hostility, against Zionist policy in Palestine, which will be stimulated by recent Northcliffe articles. I do not attach undue importance to this movement, but it is increasingly difficult to meet the argument that it is unfair to ask the British taxpayer, already overwhelmed with taxation, to bear the cost of imposing on Palestine an unpopular policy.
Response by Central Powers.
Immediately following the publication of the declaration Germany entered negotiations with Turkey to put forward counter proposals. A German-Jewish Society was formed: "Vereinigung jüdischer Organisationen Deutschlands zur Wahrung der Rechte der Juden des Ostens" (V.J.O.D.) and in January 1918 the Turkish Grand Vizier, Talaat, issued a statement which promised legislation by which "all justifiable wishes of the Jews in Palestine would be able to find their fulfilment".

</doc>
<doc id="4821" url="http://en.wikipedia.org/wiki?curid=4821" title="Black Hand (Serbia)">
Black Hand (Serbia)

Unification or Death (), unofficially known as the Black Hand (Црна рука, "Crna ruka"), was a secret military society formed on 6 September 1901 by members of the Serbian Army of all in the above Kingdom of Serbia. It was formed with the aim of uniting all of the territories with majority South Slavic population not ruled by the Kingdom of Serbia or Kingdom of Montenegro in the manner of earlier national unification processes, primarily Italian in 1870 and German in 1871. Through its connections to the June 1914 assassination of Archduke Franz Ferdinand in Sarajevo, which was committed by the members of youth movement Young Bosnia, the Black Hand is often viewed as having contributed to the start of World War I by precipitating the July Crisis of 1914, which eventually led to Austria-Hungary's invasion of the Kingdom of Serbia.
History.
The Black Hand was founded on 6 September 1901. The conspirators' first meeting was in the same year. In attendance: captains Radomir Aranđelović, Milan F. Petrović, and Dragutin Dimitrijević, as well as lieutenants Antonije Antić, Dragutin Dulić, Milan Marinković, and Nikodije Popović. They made a plan to kill the royal couple − King Alexander I Obrenović and Queen Draga. The birthday of Queen Draga was 11 September 1864 (Old Style), and, in honour of this occasion, preparations were being made to hold a celebration at the palace Kolarac. The main weapons of assassination chosen organised the successful assassination of King Alexander I of Serbia and his consort Draga; he confirmed that Captain Dragutin Dimitrijevic, who had personally led the group of Army officers who killed the royal couple in the Old Palace at Belgrade on the night of 28/29 May 1903 (Old Style), was also the Black Hand's leader.
On 8 October 1908, just two days after Austria annexed Bosnia and Herzegovina, many men, some of them ranking Serbian ministers, officials and generals, held a meeting at City Hall in Belgrade. They founded a semi-secret society—Narodna Odbrana (National Defense) which gave the Greater Serbia idea a focus and an organization. The purpose of the group was to liberate Serbs under the Austro-Hungarian occupation. They also undertook anti-Austrian propaganda and organized spies and saboteurs to operate within the occupied provinces. Satellite groups were formed in Slovenia, Bosnia, Herzegovina and Istria. The Bosnian group went under the name "Mlada Bosna" (Young Bosnia). 
Birth of the Black Hand.
The Black Hand was formed when ten men met on 9 May 1911 to form "Ujedinjenje ili Smrt" (Unification or Death), better known as the "Black Hand".
By 1914, there were many hundred members, perhaps as many as 3500. Many members were Serbian army officers. The professed goal of the group was the creation of a Greater Serbia, by use of violence, if necessary. The Black Hand trained guerillas and saboteurs and planned political murders. The Black Hand was organized at the grassroots level in 3- to 5-member cells, supervised by district committees and by a Central committee in Belgrade whose ten-member Executive Committee was led, more or less, by Colonel Dragutin Dimitrijević (also known as Apis). To ensure secrecy, members rarely knew much more than the members of their own cell and one superior above them. New members swore "I (...), by entering into the society, do hereby swear by the Sun which shineth upon me, by the Earth which feedeth me, by God, by the blood of my forefathers, by my honour and by my life, that from this moment onward and until my death, I shall faithfully serve the task of this organisation and that I shall at all times be prepared to bear for it any sacrifice. I further swear by God, by my honour and by my life, that I shall unconditionally carry into effect all its orders and commands. I further swear by my God, by my honour and by my life, that I shall keep within myself all the secrets of this organisation and carry them with me into my grave. May God and my brothers in this organisation be my judges if at any time I should wittingly fail or break this oath."
The Black Hand took over the terrorist actions of "Narodna Odbrana", and worked deliberately at obscuring any distinctions between the two groups, trading on the prestige and network of the older organization. Black Hand members held important army and government positions. Crown Prince Alexander was an enthusiastic and financial supporter. The group held influence over government appointment and policy. The Serbian government was fairly well informed of Black Hand activities.
Friendly relations had fairly well cooled by 1914. The Black Hand was displeased with Prime minister Nikola Pašić. They thought he did not act aggressively enough towards the Pan-Serb cause. They engaged in a bitter power struggle over several issues, such as who would control territories Serbia annexed in the Balkan Wars. By this point, standing up and saying 'no' to the Black Hand was a dangerous act. Political murder was one of their well known tools.
It was also in 1914 that Apis allegedly decided that Archduke Franz Ferdinand, the heir-apparent of Austria, should be assassinated. Towards that end it is claimed that three young Bosnian-Serbs were recruited to kill the Archduke. They were definitely trained in bomb throwing and marksmanship by current and former members of the Serbian military. Gavrilo Princip, Nedeljko Čabrinović and Trifko Grabež were smuggled across the border back into Bosnia via a chain of underground-railroad style contacts.
The decision to kill the Archduke was apparently initiated by Apis, and not sanctioned by the full Executive Committee (assuming Apis was involved at all, a question that remains in dispute). Those involved probably realized that their plot would invite war between Austria and Serbia. They had every reason to expect that Russia would side with Serbia. In all likelihood, they did not anticipate that their personal ideals and secret political aspirations would mushroom into world war.
Others in the government and some on the Black Hand Executive Council were not as confident of Russian aid. Russia had let them down recently. When word of the plot allegedly percolated through Black Hand leadership and the Serbian government (the Prime Minister Pašić was definitely informed of two armed men being smuggled across the border; it is not clear if Pašić knew they planned to assassinate Franz Ferdinand), Apis was supposedly told not to proceed. He may have made a half-hearted attempt to intercept the young assassins at the border, but they had already crossed. Other sources say the attempted 'recall' was only begun after the assassins had reached Sarajevo. This 'recall' appears to make Apis look like a loose cannon, and the young assassins as independent zealots. In fact, the 'recall' took place a full two weeks before the Archduke's visit. The assassins idled around in Sarajevo for a month. Nothing more was done to stop them.
Assassination of Archduke Franz Ferdinand.
The organization of Bosnian anti-Habsburg and anti-Austrian students called Young Bosnia carried out the assassination of Archduke Franz Ferdinand. After the unsuccessful grenade attack by Nedeljko Čabrinović, Gavrilo Princip succeeded in killing the Archduke and his wife with two bullets from his handgun because Ferdinand's driver took a wrong turn. Until a few weeks later, the guilt for the crime had settled loosely on Serbia in general. Long-existing tensions between Serbia and Austria-Hungary eventually drew in the other European powers and escalated into the beginning of First World War. The Serbians prevented Austria-Hungary from investigating in the assassination of the archduke.
Demise.
Towards the end of 1916, due to political intrigues, Serbian Prime Minister Pasic decided to destroy the leaders of the Black Hand and break up the organization. By the spring of 1917, many Black Hand leaders, including Apis, had been arrested.
A sham trial before a military tribunal in Salonika was held in May 1917 for Apis and others. Among the charges was that the Black Hand had attempted to murder Prince Regent Alexander. Though witnesses against them were numerous, the evidence cited was nearly all hearsay or outright fabrication. Apis and six others were sentenced to death. Three obtained commutations to long prison terms, but Apis and three comrades were executed by firing squad on 26 June 1917.
With the demise of the Black Hand in June 1917 after the Salonika Trial, The White Hand steadily gained control of the young and ambitious Prince Alexander. In what became Yugoslavia after the war, the White Hand grew into an essential piece of the state's machinery. It continued the imperialistic work of the Black Hand, using the same techniques. The death of Vojislav Petrovic, an ex-attache to the Yugoslav Legation in London, was said to be the work of Narodna Odbrana. Petrovic was preparing a book on the history of the Sarajevo assassinations and the Black Hand.
Activity in the Kingdom of Montenegro.
In 1908 (the affair is known under the name Bombaška afera) the Serbian nationalists tried to kill the Montenegrin king, considering him an obstacle to the unification of all Serbs in one state. The Montenegrin police were tipped off about the plot and arrested the plotters.
In 1909 (the affair is known under the name Kolašinska afera) Serbian nationalist tried to organize an insurrection against the Montenegrin king and government. The plot also failed.
The Black Hand is considered to have been involved in both plots.
Balkan Wars.
In 1912, differences between the two main groups of the Narodna Odbrana—political leaders of the Radical Party and military officers—arose. The political leaders preferred a more passive approach for the time being, including more peaceful relations with Austria and concentrating on strengthening Serbia for future struggle, but some of the military officers grew impatient with the more moderate radical policies. Consequently, the more zealous members of the Narodna Odbrana started a new secret society, and the Black Hand was founded.
Reportedly, they were involved in various crimes in Macedonia, during the Balkan Wars:
World War I.
Just prior to World War I, under the orders of the Chief of Serbian Military Intelligence, Serbian Military Officers and remnants of the by then moribund Black Hand organized and facilitated the assassination of Franz Ferdinand, Archduke of Austria on occasion of his visit to Sarajevo, Bosnia. The Austro-Hungarian investigation of the assassination rounded up all but one of the assassins and also much of the underground railroad that had been used to transport the assassins and their weapons from Serbia to Sarajevo. Within two days following the assassination, Austria-Hungary and Germany advised Serbia that they should open an investigation, but Serbian Foreign Minister Gruic, speaking for Serbia replied, "Nothing had been done so far, and the matter did not concern the Serbian Government," after which "high words" were spoken on both sides. Entreaties by Germany asking Russia to intercede with Serbia were ignored. On 23 July Austria-Hungary delivered a toughly worded letter to Serbia with ten enumerated demands and additional demands in the preamble aimed at the destruction of the anti-Austrian terrorist and propaganda network in Serbia. Austria called attention to Serbia's March 1909 declaration committing to the Great Powers to respect Austria-Hungary's sovereignty over Bosnia-Herzegovina and committing Serbia to maintain good neighborly relations with Austria-Hungary. If the ten enumerated demands and demands in the preamble were not agreed to within 48 hours, Austria-Hungary would recall its ambassador from Serbia. The letter became known as the July Ultimatum. Serbia accepted all but one of the demands, to let the Austrian officers conduct an investigation on Serbian soil, which would have compromised its sovereignty. In response, Austria-Hungary recalled its ambassador.
Austria-Hungary authorized the mobilization and the declaration of war against Serbia on 28 July 1914. The Secret Treaty of 1892 required both Russia and France to mobilize immediately followed by a commencement of action against the Triple Alliance if any member of the Triplice mobilized, and so, soon all the Great Powers of Europe were at war except Italy. Italy cited a clause in the Triple Alliance treaty which only bound it to enter in case of aggression against one of the treaty members, and so remained neutral – for the time being.
The six assassins caught by Austria-Hungary were tried and convicted for treason. The leader, Danilo Ilić, was shot by a firing squad.The remaining assassins in custody were not yet twenty years old at the time of the assassination and so were given prison terms. Most of the underground railroad that transported them were also arrested, tried, and convicted. Two of these were executed. A few peripheral conspirators were acquitted. A wide ranging investigation rolled up many additional irredentist youths, and the fifth column that the Black Hand and Serbian Military Intelligence had tried to organize was eliminated. After receiving the Austrian letter, Serbia arrested Major Voja Tankosić (a member of the Black Hand committee who had been pointed out by the assassins) but then promptly released him and returned him to his unit. The seventh assassin escaped to Montenegro where he was arrested. Austria-Hungary asserted its right to extradite him, but Montenegrin authorities instead allowed the assassin to "escape" to Serbia where he joined Major Tankosić's unit; Major Tankosić died in November 1915 covering the Serbian retreat, but not before confessing his role in the assassination to historians at Azania. Masterspy Rade Malobabić, Serbian Military Intelligence's top agent against Austria-Hungary, was arrested on his return from Austria-Hungary after the assassination, but was also later released and given a commission running an army supply store. In 1917 Serbia's government in exile arrested the leadership of the Black Hand wishing to halt their underground influence in both the army and politics. The leadership was tried before a kangaroo court and convicted on false charges unrelated to Sarajevo, such as plotting an assassinations of Nikola Pašić and Crown prince Aleksandar; many were given death sentences. Three of the accused were ultimately shot by firing squad, against protests of the new Kerensky government of Russia. Before being shot, Dragutin Dimitrijević made a written confession to the court that he had ordered Rade Malobabić to organize the assassination of Franz Ferdinand. Malobabić made an implied confession to a priest before he was executed. Vulović's confession came at trial where he said he received orders signed by Serbia's top military officer to send Malobabic into Austria-Hungary just before the assassination. Much later, a new trial was ordered by Yugoslavia and the convictions were overturned.
Ideology.
The group encompassed a range of ideological outlooks, from conspiratorially-minded army officers to idealistic youths, sometimes tending towards republicanism, despite the acquisition of nationalistic royal circles in its activities (the movement's leader, Colonel Dragutin Dimitrijević or "Apis," had been instrumental in the June 1903 coup which had brought King Petar Karađorđević to the Serbian throne following 45 years of rule by the rival Obrenović dynasty). The group was denounced as nihilist by the Austro-Hungarian press and compared to the Russian People's Will and the Chinese Assassination Corps.

</doc>
<doc id="4822" url="http://en.wikipedia.org/wiki?curid=4822" title="Board of directors">
Board of directors

A board of directors is a body of elected or appointed members who jointly oversee the activities of a company or organization. Other names include board of governors, board of managers, board of regents, board of trustees, and board of visitors. It is often simply referred to as "the board".
A board's activities are determined by the powers, duties, and responsibilities delegated to it or conferred on it by an authority outside itself. These matters are typically detailed in the organization's bylaws. The bylaws commonly also specify the number of members of the board, how they are to be chosen, and when they are to meet.
In an organization with voting members, the board acts on behalf of, and is subordinate to, the organization's full group, which usually chooses the members of the board. In a stock corporation, the board is elected by the shareholders and is the highest authority in the management of the corporation. In a non-stock corporation with no general voting membership, the board is the supreme governing body of the institution; its members are sometimes chosen by the board itself.
Typical duties of boards of directors include:
The legal responsibilities of boards and board members vary with the nature of the organization, and with the jurisdiction within which it operates. For companies with publicly trading stock, these responsibilities are typically much more rigorous and complex than for those of other types.
Typically the board chooses one of its members to be the "chairman", who holds whatever title is specified in the bylaws.
Directors.
The directors of an organization are the persons who are members of its board. Several specific terms categorize directors by the presence or absence of their other relationships to the organization.
Inside director.
An inside director is a director who is also an employee, officer, major shareholder, or someone similarly connected to the organization. Inside directors represent the interests of the entity's stakeholders, and often have special knowledge of its inner workings, its financial or market position, and so on.
Typical inside directors are:
An inside director who is employed as a manager or executive of the organization is sometimes referred to as an executive director (not to be confused with the title executive director sometimes used for the CEO position). Executive directors often have a specified area of responsibility in the organization, such as finance, marketing, human resources, or production.
Outside director.
An outside director is a member of the board who is not otherwise employed by or engaged with the organization, and does not represent any of its stakeholders. A typical example is a director who is president of a firm in a different industry.
Outside directors bring outside experience and perspective to the board. They keep a watchful eye on the inside directors and on the way the organization is run. Outside directors are often useful in handling disputes between inside directors, or between shareholders and the board. They are thought to be advantageous because they can be objective and present little risk of conflict of interest. On the other hand, they might lack familiarity with the specific issues connected to the organization's governance.
Terminology.
Individual directors often serve on more than one board. This practice results in an interlocking directorate, where a relatively small number of individuals have significant influence over a large number of important entities. This situation can have important corporate, social, economic, and legal consequences, and has been the subject of significant research.
Process.
The process for running a board, sometimes called the board process, includes the selection of board members, the setting of clear board objectives, the dissemination of documents or board package to the board members, the collaborative creation of an agenda for the meeting, the creation and follow-up of assigned action items, and the assessment of the board process through standardized assessments of board members, owners, and CEOs. The science of this process has been slow to develop due to the secretive nature of the way most companies run their boards, however some standardization is beginning to develop. Some who are pushing for this standardization in the USA are the National Association of Corporate Directors, McKinsey Consulting and The Board Group.
Non-corporate boards.
The role and responsibilities of a board of directors vary depending on the nature and type of business entity and the laws applying to the entity (see types of business entity). For example, the nature of the business entity may be one that is traded on a public market (public company), not traded on a public market (a private, limited or closely held company), owned by family members (a family business), or exempt from income taxes (a non-profit, not for profit, or tax-exempt entity). There are numerous types of business entities available throughout the world such as a corporation, limited liability company, cooperative, business trust, partnership, private limited company, and public limited company.
Much of what has been written about boards of directors relates to boards of directors of business entities actively traded on public markets. More recently, however, material is becoming available for boards of private and closely held businesses including family businesses.
A board-only organization is one whose board is self-appointed, rather than being accountable to a base of members through elections; or in which the powers of the membership are extremely limited.
Corporations.
In a publicly held company, directors are elected to represent and are legally obligated as fiduciaries to represent owners of the company—the shareholders/stockholders. In this capacity they establish policies and make decisions on issues such as whether there is dividend and how much it is, stock options distributed to employees, and the hiring/firing and compensation of upper management.
Governance.
Theoretically, the control of a company is divided between two bodies: the board of directors, and the shareholders in general meeting. In practice, the amount of power exercised by the board varies with the type of company. In small private companies, the directors and the shareholders are normally the same people, and thus there is no real division of power. In large public companies, the board tends to exercise more of a supervisory role, and individual responsibility and management tends to be delegated downward to individual professional executives (such as a finance director or a marketing director) who deal with particular areas of the company's affairs.
Another feature of boards of directors in large public companies is that the board tends to have more de facto power. Many shareholders grant proxies to the directors to vote their shares at general meetings and accept all recommendations of the board rather than try to get involved in management, since each shareholder's power, as well as interest and information is so small. Larger institutional investors also grant the board proxies. The large number of shareholders also makes it hard for them to organize. However, there have been moves recently to try to increase shareholder activism among both institutional investors and individuals with small shareholdings.
A contrasting view is that in large public companies it is upper management and not boards that wield practical power, because boards delegate nearly all of their power to the top executive employees, adopting their recommendations almost without fail. As a practical matter, executives even choose the directors, with shareholders normally following management recommendations and voting for them.
In most cases, serving on a board is not a career unto itself, but board members often receive remunerations amounting to hundreds of thousands of dollars per year since they often sit on the boards of several companies. Inside directors are usually not paid for sitting on a board, but the duty is instead considered part of their larger job description. Outside directors are usually paid for their services. These remunerations vary between corporations, but usually consist of a yearly or monthly salary, additional compensation for each meeting attended, stock options, and various other benefits. Tiffany & Co., for example, pays directors an annual retainer of $46,500, an additional annual retainer of $2,500 if the director is also a chairperson of a committee, a per-meeting-attended fee of $2,000 for meetings attended in person, a $500 fee for each meeting attended via telephone, in addition to stock options and retirement benefits.
Two-tier system.
In some European and Asian countries, there are two separate boards, an executive board for day-to-day business and a supervisory board (elected by the shareholders and employees) for supervising the executive board. In these countries, the CEO (chief executive or managing director) presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board and allows for clear lines of authority. The aim is to prevent a conflict of interest and too much power being concentrated in the hands of one person. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service. In the United States, the board of directors (elected by the shareholders) is often equivalent to the supervisory board, while the executive board may often be known as the executive committee (operating committee or executive council), composed of the CEO and their direct reports (other C-level officers, division/subsidiary heads).
History.
The development of a separate board of directors to manage the company has occurred incrementally and indefinitely over legal history. Until the end of the 19th century, it seems to have been generally assumed that the general meeting (of all shareholders) was the supreme organ of the company, and the board of directors was merely an agent of the company subject to the control of the shareholders in general meeting.
However, by 1906, the English Court of Appeal had made it clear in the decision of "Automatic Self-Cleansing Filter Syndicate Co v Cunningham" [1906] 2 Ch 34 that the division of powers between the board and the shareholders in general meaning depended on the construction of the articles of association and that, where the powers of management were vested in the board, the general meeting could not interfere with their lawful exercise. The articles were held to constitute a contract by which the members had agreed that "the directors and the directors alone shall manage."
The new approach did not secure immediate approval, but it was endorsed by the House of Lords in "Quin & Axtens v Salmon" [1909] AC 442 and has since received general acceptance. Under English law, successive versions of Table A have reinforced the norm that, unless the directors are acting contrary to the law or the provisions of the Articles, the powers of conducting the management and affairs of the company are vested in them.
The modern doctrine was expressed in "Shaw & Sons (Salford) Ltd v Shaw" [1935] 2 KB 113 by Greer LJ as follows:
"A company is an entity distinct alike from its shareholders and its directors. Some of its powers may, according to its articles, be exercised by directors, certain other powers may be reserved for the shareholders in general meeting. If powers of management are vested in the directors, they and they alone can exercise these powers. The only way in which the general body of shareholders can control the exercise of powers by the articles in the directors is by altering the articles, or, if opportunity arises under the articles, by refusing to re-elect the directors of whose actions they disapprove. They cannot themselves usurp the powers which by the articles are vested in the directors any more than the directors can usurp the powers vested by the articles in the general body of shareholders."
It has been remarked that this development in the law was somewhat surprising at the time, as the relevant provisions in Table A (as it was then) seemed to contradict this approach rather than to endorse it.
Election and removal.
In most legal systems, the appointment and removal of directors is voted upon by the shareholders in general meeting or through a proxy statement. For publicly traded companies in the U.S., the directors which are available to vote on are largely selected by either the board as a whole or a nominating committee. Although in 2002 the NYSE and the NASDAQ required that nominating committees consist of independent directors as a condition of listing, nomination committees have historically received input from management in their selections even when the CEO does not have a position on the board. Shareholder nominations can only occur at the general meeting itself or through the prohibitively expensive process of mailing out ballots separately; in May 2009 the SEC proposed a new rule allowing shareholders meeting certain criteria to add nominees to the proxy statement. In practice for publicly traded companies, the managers (inside directors) who are purportedly accountable to the board of directors have historically played a major role in selecting and nominating the directors who are voted on by the shareholders, in which case more "gray outsider directors" (independent directors with conflicts of interest) are nominated and elected.
Directors may also leave office by resignation or death. In some legal systems, directors may also be removed by a resolution of the remaining directors (in some countries they may only do so "with cause"; in others the power is unrestricted).
Some jurisdictions also permit the board of directors to appoint directors, either to fill a vacancy which arises on resignation or death, or as an addition to the existing directors.
In practice, it can be quite difficult to remove a director by a resolution in general meeting. In many legal systems, the director has a right to receive special notice of any resolution to remove him or her; the company must often supply a copy of the proposal to the director, who is usually entitled to be heard by the meeting. The director may require the company to circulate any representations that he wishes to make. Furthermore, the director's contract of service will usually entitle him to compensation if he is removed, and may often include a generous "golden parachute" which also acts as a deterrent to removal.
In a recent academic study that was published in the "Journal of Applied Finance", Drexel University’s LeBow College of Business professors Jie Cai, Jacqueline Garner, and Ralph Walkling examined how corporate shareholders voted in nearly 2,500 director elections in the United States. They found that directors received fewer votes from shareholders when their companies performed poorly, had excess CEO compensation, or had poor shareholder protection. They also found that directors received fewer votes when they did not regularly attend board meetings or received negative recommendations from RiskMetrics (a proxy advisory firm). This evidence suggests that some shareholders express their displeasure with a company by voting against its directors. The article also shows that companies often improve their corporate governance by removing poison pills or classified boards and by reducing excessive CEO pay after their directors receive low shareholder support.
Board accountability to shareholders is a recurring issue. In 2010, the "New York Times" noted that several directors who had overseen companies which had failed in the financial crisis of 2007–2010 had found new positions as directors. The SEC sometimes imposes a ban (a "D&O bar") on serving on a board as part of its fraud cases, and one of these was upheld in 2013.
Exercise of powers.
The exercise by the board of directors of its powers usually occurs in board meetings. Most legal systems require sufficient notice to be given to all directors of these meetings, and that a quorum must be present before any business may be conducted. Usually, a meeting which is held without notice having been given is still valid if all of the directors attend, but it has been held that a failure to give notice may negate resolutions passed at a meeting, because the persuasive oratory of a minority of directors might have persuaded the majority to change their minds and vote otherwise.
In most common law countries, the powers of the board are vested in the board as a whole, and not in the individual directors. However, in instances an individual director may still bind the company by his acts by virtue of his ostensible authority (see also: the rule in "Turquand's Case").
Duties.
Because directors exercise control and management over the organization, but organizations are (in theory) run for the benefit of the shareholders, the law imposes strict duties on directors in relation to the exercise of their duties. The duties imposed on directors are fiduciary duties, similar to those that the law imposes on those in similar positions of trust: agents and trustees.
The duties apply to each director separately, while the powers apply to the board jointly. Also, the duties are owed to the company itself, and not to any other entity. This does not mean that directors can never stand in a fiduciary relationship to the individual shareholders; they may well have such a duty in certain circumstances.
"Proper purpose".
Directors must exercise their powers for a proper purpose. While in many instances an improper purpose is readily evident, such as a director looking to feather his or her own nest or divert an investment opportunity to a relative, such breaches usually involve a breach of the director's duty to act in good faith. Greater difficulties arise where the director, while acting in good faith, is serving a purpose that is not regarded by the law as proper.
The seminal authority in relation to what amounts to a proper purpose is the Privy Council decision of "Howard Smith Ltd v Ampol Ltd" [1974] AC 821. The case concerned the power of the directors to issue new shares. It was alleged that the directors had issued a large number of new shares purely to deprive a particular shareholder of his voting majority. An argument that the power to issue shares could only be properly exercised to raise new capital was rejected as too narrow, and it was held that it would be a proper exercise of the director's powers to issue shares to a larger company to ensure the financial stability of the company, or as part of an agreement to exploit mineral rights owned by the company. If so, the mere fact that an incidental result (even if it was a desired consequence) was that a shareholder lost his majority, or a takeover bid was defeated, this would not itself make the share issue improper. But if the sole purpose was to destroy a voting majority, or block a takeover bid, that would be an improper purpose.
Not all jurisdictions recognised the "proper purpose" duty as separate from the "good faith" duty however.
"Unfettered discretion".
Directors cannot, without the consent of the company, fetter their discretion in relation to the exercise of their powers, and cannot bind themselves to vote in a particular way at future board meetings. This is so even if there is no improper motive or purpose, and no personal advantage to the director.
This does not mean, however, that the board cannot agree to the company entering into a contract which binds the company to a certain course, even if certain actions in that course will require further board approval. The company remains bound, but the directors retain the discretion to vote against taking the future actions (although that may involve a breach by the company of the contract that the board previously approved).
"Conflict of duty and interest".
As fiduciaries, the directors may not put themselves in a position where their interests and duties conflict with the duties that they owe to the company. The law takes the view that good faith must not only be done, but must be manifestly seen to be done, and zealously patrols the conduct of directors in this regard; and will not allow directors to escape liability by asserting that his decision was in fact well founded. Traditionally, the law has divided conflicts of duty and interest into three sub-categories.
Transactions with the company.
By definition, where a director enters into a transaction with a company, there is a conflict between the director's interest (to do well for himself out of the transaction) and his duty to the company (to ensure that the company gets as much as it can out of the transaction). This rule is so strictly enforced that, even where the conflict of interest or conflict of duty is purely hypothetical, the directors can be forced to disgorge all personal gains arising from it. In "Aberdeen Ry v Blaikie" (1854) 1 Macq HL 461 Lord Cranworth stated in his judgment that:
However, in many jurisdictions the members of the company are permitted to ratify transactions which would otherwise fall foul of this principle. It is also largely accepted in most jurisdictions that this principle can be overridden in the company's constitution.
In many countries, there is also a statutory duty to declare interests in relation to any transactions, and the director can be fined for failing to make disclosure.
Use of corporate property, opportunity, or information.
Directors must not, without the informed consent of the company, use for their own profit the company's assets, opportunities, or information. This prohibition is much less flexible than the prohibition against the transactions with the company, and attempts to circumvent it using provisions in the articles have met with limited success.
In "Regal (Hastings) Ltd v Gulliver" [1942] All ER 378 the House of Lords, in upholding what was regarded as a wholly unmeritorious claim by the shareholders, held that:
And accordingly, the directors were required to disgorge the profits that they made, and the shareholders received their windfall.
The decision has been followed in several subsequent cases, and is now regarded as settled law.
Competing with the company.
Directors cannot compete directly with the company without a conflict of interest arising. Similarly, they should not act as directors of competing companies, as their duties to each company would then conflict with each other.
Common law duties of care and skill.
Traditionally, the level of care and skill which has to be demonstrated by a director has been framed largely with reference to the non-executive director. In "Re City Equitable Fire Insurance Co" [1925] Ch 407, it was expressed in purely subjective terms, where the court held that:
However, this decision was based firmly in the older notions (see above) that prevailed at the time as to the mode of corporate decision making, and effective control residing in the shareholders; if they elected and put up with an incompetent decision maker, they should not have recourse to complain.
However, a more modern approach has since developed, and in "Dorchester Finance Co Ltd v Stebbing" [1989] BCLC 498 the court held that the rule in "Equitable Fire" related only to skill, and not to diligence. With respect to diligence, what was required was:
This was a dual subjective and objective test, and one deliberately pitched at a higher level.
More recently, it has been suggested that both the tests of skill and diligence should be assessed objectively and subjectively; in the United Kingdom, the statutory provisions relating to directors' duties in the new Companies Act 2006 have been codified on this basis.
Remedies for breach of duty.
In most jurisdictions, the law provides for a variety of remedies in the event of a breach by the directors of their duties:
The future.
Historically, directors' duties have been owed almost exclusively to the company and its members, and the board was expected to exercise its powers for the financial benefit of the company. However, more recently there have been attempts to "soften" the position, and provide for more scope for directors to act as good corporate citizens. For example, in the United Kingdom, the Companies Act 2006 requires directors of companies "to promote the success of the company for the benefit of its members as a whole" and sets out the following six factors regarding a director's duty to promote success:
This represents a considerable departure from the traditional notion that directors' duties are owed only to the company. Previously in the United Kingdom, under the Companies Act 1985, protections for non-member stakeholders were considerably more limited (see for example, s.309 which permitted directors to take into account the interests of employees but which could only be enforced by the shareholders and not by the employees themselves). The changes have therefore been the subject of some criticism.
United States.
Sarbanes–Oxley Act.
The Sarbanes–Oxley Act of 2002 has introduced new standards of accountability on boards of U.S. companies or companies listed on U.S. stock exchanges. Under the Act, directors risk large fines and prison sentences in the case of accounting crimes. Internal control is now the direct responsibility of directors. The vast majority of companies covered by the Act have hired internal auditors to ensure that the company adheres to required standards of internal control. The internal auditors are required by law to report directly to an audit board, consisting of directors more than half of whom are outside directors, one of whom is a "financial expert."
The law requires companies listed on the major stock exchanges (NYSE, NASDAQ) to have a majority of independent directors—directors who are not otherwise employed by the firm or in a business relationship with it.
Size.
According to the Corporate Library's study, the average size of publicly traded company's board is 9.2 members, and most boards range from 3 to 31 members. According to Investopedia, some analysts think the ideal size is seven. State law may specify a minimum number of directors, maximum number of directors, and qualifications for directors (e.g. whether board members must be individuals or may be business entities).
Committees.
While a board may have several committees, two—the compensation committee and audit committee—are critical and must be made up of at least three independent directors and no inside directors. Other common committees in boards are nominating and governance.
Compensation.
Directors of Fortune 500 companies received median pay of $234,000 in 2011. Directorship is a part-time job. A recent National Association of Corporate Directors study found directors averaging just 4.3 hours a week on board work.
Criticism.
According to John Gillespie, a former investment banker and co-author of a book critical of boards, "Far too much of their time has been for check-the-box and cover-your-behind activities rather than real monitoring of executives and providing strategic advice on behalf of shareholders".

</doc>
<doc id="4823" url="http://en.wikipedia.org/wiki?curid=4823" title="Balkan Wars">
Balkan Wars

The Balkan Wars were two conflicts that took place in the Balkan Peninsula in south-eastern Europe in 1912 and 1913. Four Balkan states defeated the Ottoman Empire in the first war; one of the four, Bulgaria, was defeated in the second war. The Ottoman Empire lost nearly all of its holdings in Europe. Austria-Hungary, although not a combatant, was weakened as a much enlarged Serbia pushed for union of the South Slavic peoples. The war set the stage for the Balkan crisis of 1914 and thus was a "prelude to the First World War."
By the early 20th century, Bulgaria, Greece, Montenegro and Serbia had achieved independence from the Ottoman Empire, but large elements of their ethnic populations remained under Ottoman rule. In 1912, these countries formed the Balkan League. There were three main causes of the First Balkan War. The Ottoman Empire was unable to reform itself, govern satisfactorily, or deal with the rising ethnic nationalism of its diverse peoples. Secondly the Great Powers quarreled amongst themselves and failed to ensure that the Ottomans would carry out the needed reforms. This led the Balkan states to impose their own solution. Most important, the Balkan League had been formed, and its members were confident that it could defeat the Turks.
The Ottoman Empire lost almost all its European territories to the west of the River Maritsa, drawing present day Turkey's western border. A large influx of Turks started to flee into the Ottoman heartland as a result of the lost lands. By 1914, the remaining core region of the Ottoman Empire had experienced a population increase of around 2.5 million because of the flood of immigration from the Balkans.
In Turkey, it is considered a major disaster ("Balkan harbi faciası") in the nation's history. The unexpected fall and sudden relinquishing of Turkish-dominated European territories created a psycho-traumatic event amongst the Turks that is said to have triggered the ultimate collapse of the empire itself within five years. Nazım Pasha, the chief of staff of the Ottoman army has been held responsible of the failure and was assassinated in 1913 by Young Turks.
The First Balkan War broke out when the League attacked the Ottoman Empire on 8 October 1912 and was ended seven months later by the Treaty of London. After five years, the Ottoman Empire lost virtually all of its possessions in the Balkans.
The Second Balkan War broke out on 16 June 1913. Both Serbia and Greece utilizing the argument that the war had been prolonged repudiated important particulars of the pre-war treaty and retained occupation of all the conquered districts in their possession which were to be divided according to specific predefined boundaries. Seeing the treaty as trampled, Bulgaria was dissatisfied over the division of the spoils in Macedonia, made in secret by its former allies, Serbia and Greece, and commenced military action against them. The more numerous combined Serbian and Greek armies repulsed the Bulgarian offensive and counter-attacked into Bulgaria. Romania, who having taken no part in the conflict, had intact armies to strike with and invaded from the north in violation of a peace treaty between the two states. The Ottoman Empire also attacked Bulgaria and advanced in Thrace regaining Adrianople. In the resulting Treaty of Bucharest, Bulgaria lost most of the territories it had gained in the First Balkan War in addition to being forced to cede the native Bulgarian province of Dobroudja to Romania.
The fighting during the Balkan Wars was also disastrous for the region’s Jewish population. 120,000 Jewish former citizens of the Ottoman Empire became separated from their country by the conflict, and many were either directly victimized or suffered as refugees. Numerous documented incidents attest to the looting of Jewish businesses, destruction of synagogues, murder, and conscription on a vast scale that impoverished the women and children. 
Background.
The background to the wars lies in the incomplete emergence of nation-states on the European territory of the Ottoman Empire during the second half of the 19th century. Serbia had gained substantial territory during the Russo-Turkish War, 1877–1878, while Greece acquired Thessaly in 1881 (although it lost a small area back to the Ottoman Empire in 1897) and Bulgaria (an autonomous principality since 1878) incorporated the formerly distinct province of Eastern Rumelia (1885). All three as well as Montenegro sought additional territories within the large Ottoman-ruled region known as Rumelia, comprising Eastern Rumelia, Albania, Macedonia, and Thrace.
Policies of the Great Powers.
Throughout the 19th century, the Great Powers shared different aims over the "Eastern Question" and the integrity of the Ottoman Empire. Russia wanted access to the "warm waters" of the Mediterranean; it pursued a pan-Slavic foreign policy and therefore supported Bulgaria and Serbia. Britain wished to deny Russia access to the "warm waters" and supported the integrity of the Ottoman Empire, although it also supported a limited expansion of Greece as a backup plan in case integrity of the Empire was no longer possible. France wished to strengthen its position in the region, especially in the Levant (today's Lebanon, Syria, the Palestinian territories and Israel).
Habsburg-ruled Austria-Hungary wished for a continuation of the existence of the Ottoman Empire, since both were troubled multinational entities and thus the collapse of the one might weaken the other. The Habsburgs also saw a strong Ottoman presence in the area as a counterweight to the Serbian nationalistic call to their own Serb subjects in Bosnia. Italy, it has been argued, wished to recreate the Roman empire, though its primary aim at the time seems to have been the denial of access to the Adriatic Sea to another major sea power. The German Empire, in turn, under the "Drang nach Osten" policy, aspired to turn the Ottoman Empire into its own de facto colony, and thus supported its integrity.
In the late 19th and early 20th century, Bulgaria and Greece contended for Ottoman Macedonia and Thrace. Ethnic Greeks sought the forced "Hellenization" of ethnic Bulgars, who sought "Bulgarization" of Greeks. Both nations sent armed irregulars into Ottoman territory to protect and assist their ethnic kindred. From 1904, there was low intensity warfare in Macedonia between the Greek and Bulgarian bands and the Ottoman army (the Struggle for Macedonia). After the Young Turk revolution of July 1908, the situation changed drastically.
Young Turk Revolution.
The 1908 Young Turk Revolution saw the reinstatement of constitutional monarchy in the Empire and the start of the Second Constitutional Era. When the revolt broke out, it was supported by intellectuals, the army, and almost all the ethnic minorities of the Empire, and forced Sultan Abdul Hamid II to re-adopt the long defunct Ottoman constitution of 1876 and parliament. Hopes were raised among the Balkan ethnicities of reforms and autonomy, and elections were held to form a representative, multi-ethnic, Ottoman parliament. However, following the Sultan's attempted counter-coup, the liberal element of the Young Turks was sidelined and the nationalist element became dominant.
At the same time, in October 1908, Austria-Hungary seized the opportunity of the Ottoman political upheaval to annex the "de jure" Ottoman province of Bosnia and Herzegovina, which it had occupied since 1878 (see "Bosnian Crisis"), and Bulgaria declared itself a fully independent kingdom. The Greeks of the autonomous Cretan State proclaimed unification with Greece, though the opposition of the Great Powers prevented the latter action from taking practical effect. It has large influence in the consequent world order.
Reaction in the Balkan States.
Serbia was frustrated in the north by Austria-Hungary's incorporation of Bosnia. In March 1909, Serbia was forced to accept the annexation and restrain anti-Habsburg agitation by Serbian nationalists. Instead, the Serbian government looked to formerly Serb territories in the south, notably "Old Serbia" (the Sanjak of Novi Pazar and the province of Kosovo).
On 15 August 1909 the Military League, a group of Greek officers, took action against the government to reform their country's national government and reorganize the army. The League found itself unable to create a new political system, until the league summoned the Cretan politician Eleutherios Venizelos to Athens as its political adviser. Venizelos persuaded the king to revise the constitution and asked the League to disband in favor of a National Assembly. In March 1910 the Military League dissolved itself.
Bulgaria, which had secured Ottoman recognition of her independence in April 1909 and enjoyed the friendship of Russia, also looked to districts of Ottoman Thrace and Macedonia. In August 1910 Montenegro followed Bulgaria's precedent by becoming a kingdom.
Balkan League.
Following Italy's victory in the Italo-Turkish War of 1911–1912 the Young Turks fell from power after a coup. The Balkan countries saw this as an opportunity to attack the Ottoman Empire and fulfill their desires of expansion.
With the initial encouragement of Russian agents, a series of agreements was concluded between Serbia and Bulgaria in March 1912. Military victory against the Ottoman Empire would not be possible while it could bring reinforcements from Asia. The condition of the Ottoman railways of the time was primitive, so most reinforcement would have to come by sea through the Aegean Sea. Greece was the only Balkan country with a navy powerful enough to deny use of the Aegean to the Ottoman Empire, thus a treaty between Greece and Bulgaria became necessary; it was signed in May 1912.
Montenegro concluded agreements between Serbia and Bulgaria later that year. Bulgaria signed treaties with Serbia to divide the territory of northern Macedonia.
This alliance between Greece, Serbia, Bulgaria, and Montenegro became known as the Balkan League; its existence was undesirable for all the Great Powers. The League was loose at best, though secret liaison officers were exchanged between the Greek and the Serbian army after the war began. Greece delayed the start of the war several times in the summer of 1912, to better prepare her navy, but Montenegro declared war on 8 October (25 September O.S.). Following an ultimatum to the Ottoman Empire, the remaining members of the alliance entered the conflict on 17 October.
First Balkan War.
The three Slavic allies (Bulgaria, Serbia and Montenegro) had laid out extensive plans to coordinate their war efforts, in continuation of their secret prewar settlements and under close Russian supervision (Greece was not included). Serbia and Montenegro would attack in the theater of Sandjak, Bulgaria and Serbia in Macedonia and Thrace.
The Ottoman Empire's situation was difficult. Its population of about 26 million people provided a massive pool of manpower, but 3/4 of the population and nearly all of the Muslim component lived in the Asian part of the Empire. Reinforcements had to come from Asia mainly by sea, which depended on the result of battles between the Turkish and Greek navies in the Aegean.
With the outbreak of the war, the Ottoman Empire activated three Army HQs: the Thracian HQ in Constantinople, the Western HQ in Salonika, and the Vardar HQ in Skopje, against the Bulgarians, the Greeks and the Serbians respectively. Most of their available forces were allocated to these fronts. Smaller independent units were allocated elsewhere, mostly around heavily fortified cities.
Montenegro was the first that declared war on 8 October (25 September O.S.). Its main thrust was towards Shkodra, with secondary operations in the Novi Pazar area. The rest of the Allies, after giving a common ultimatum, declared war a week later. Bulgaria attacked towards Eastern Thrace, being stopped only at the outskirts of Constantinople at the Çatalca line and the isthmus of the Gallipoli peninsula, while secondary forces captured Western Thrace and Eastern Macedonia. Serbia attacked south towards Skopje and Monastir and then turned west to present-day Albania, reaching the Adriatic, while a second Army captured Kosovo and linked with the Montenegrin forces. Greece's main forces attacked from Thessaly into Macedonia through the Sarantaporo strait and after capturing Thessaloniki on 12 November (on 26 October 1912, O.S.) expanded its occupied area and linked up with the Serbian army to the northwest, while its main forces turned east towards Kavala, reaching the Bulgarians. Another Greek army attacked into Epirus towards Ioannina.
In the naval front the Ottoman fleet twice exited the Dardanelles and was twice defeated by the Greek Navy, in the battles of Elli and Lemnos. Greek dominance on the Aegean Sea made it impossible for the Ottomans to transfer the planned troops from the Middle East to the Thracian (against the Bulgarian) and to the Macedonian (against the Greeks and Serbians) fronts. According to the E.J. Erickson the Greek Navy also played a crucial, albeit indirect role, in the Thracian campaign by neutralizing no less than three Thracian Corps (see First Balkan War, the Bulgarian theater of operations), a significant portion of the Ottoman Army there, in the all-important opening round of the war. After the defeating of the Ottoman fleet the Greek Navy was also free to liberate the islands of the Aegean. General Nikola Ivanov identified the activity of the Greek Navy as the chief factor in the general success of the allies.
In January, after a successful coup by young army officers, the Ottoman Empire decided to continue the war. After a failed Ottoman counter-attack in the Western-Thracian front, Bulgarian forces with the help of the Serbian Army managed to conquer Adrianople while Greek forces managed to take Ioannina after defeating the Ottomans in the battle of Bizani. In the joint Serbian-Montenegrin theater of operation the Montenegrin army besieged and captured the Shkodra, ending the Ottoman presence in Europe west of the Çatalca line after nearly 500 years. The war ended with the Treaty of London on 30(17) May 1913.
Second Balkan War.
Though the Balkan allies had fought together against the common enemy, that was not enough to overcome their mutual rivalries. In the original document for the Balkans league, Serbia promised Bulgaria most of Macedonia. But after the war, Serbia and Greece, in violation of previous agreement between the allies, revealed their plan to keep possession of most of the promised territories destined to Bulgaria. This act prompted the tsar of Bulgaria to invade his allies. The Second Balkan War broke out on 29(16) June 1913 when Bulgaria attacked its erstwhile allies in the First Balkan War, Serbia and Greece, while Montenegro and the Ottoman Empire intervened later against Bulgaria, with Romania attacking Bulgaria from the north without any provocation and in violation of a peace treaty. When the Greek army entered Thessaloniki in the First Balkan War ahead of the Bulgarian 7th division by only a day, they were asked to allow a Bulgarian battalion to enter the city. Greece accepted in exchange for allowing a Greek unit to enter the city of Serres.
The Bulgarian unit that entered Thessaloniki turned out to be a 48,000-strong division instead of the battalion, something which caused concern among the Greeks, who viewed it as a Bulgarian attempt to establish a condominium over the city. In the event, due to the urgently needed reinforcements in the Thracian front, Bulgarian Headquarters was soon forced to remove its troops from the city (while the Greeks agreed by mutual treaty to remove their units based in Serres) and transport them to Dedeağaç (modern Alexandroupolis), but still it left behind a battalion that started fortifying its positions.
Greece had also allowed the Bulgarians to control the stretch of the Thessaloniki-Constantinople railroad that lay in Greek-occupied territory, since Bulgaria controlled the largest part of this railroad towards Thrace. After the end of the operations in Thrace—and confirming Greek concerns—Bulgaria was not satisfied with the territory it controlled in Macedonia and immediately asked Greece to relinquish its control over Thessaloniki and the land north of Pieria, effectively handing over all Aegean Macedonia. These unacceptable demands together with the Bulgarian refusal to demobilize its army after the Treaty of London had ended the common war against the Ottomans and alarmed Greece, which decided also to maintain its army's mobilization.
Similarly, in northern Macedonia, the tension between Serbia and Bulgaria due to later aspirations over Vardar Macedonia generated many incidents between the nearby armies, prompting Serbia to maintain its army's mobilization. Serbia and Greece proposed that each of the three countries reduce its army by one fourth, as a first step to facilitate a peaceful solution, but Bulgaria rejected it.
Seeing the omens, Greece and Serbia started a series of negotiations and signed a treaty on 1 June(19 May) 1913. With this treaty, a mutual border was agreed between the two countries, together with an agreement for mutual military and diplomatic support in case of a Bulgarian or/and Austro-Hungarian attack. Tsar Nicholas II of Russia, being well informed, tried to stop the upcoming conflict on 8 June, by sending an identical personal message to the Kings of Bulgaria and Serbia, offering to act as arbitrator according to the provisions of the 1912 Serbo-Bulgarian treaty. But Bulgaria, by making the acceptance of Russian arbitration conditional, in effect denied any discussion and caused Russia to repudiate its alliance with Bulgaria (see Russo-Bulgarian military convention signed 31 May 1902).
The Serbs and the Greeks had a military advantage on the eve of the war because their armies confronted comparatively weak Ottoman forces in the First Balkan War and suffered relatively light casualties while the Bulgarians were involved in heavy fighting in Thrace. The Serbs and the Greeks had time to fortify their positions in Macedonia. The Bulgarians also held some advantages, controlling internal communication and supply lines.
On 29(16) June 1913 General Savov, under direct orders of tsar Ferdinand I, issued attacking orders against both Greece and Serbia without consulting the Bulgarian government and without any official declaration of war. During the night of 30(17) June 1913 they attacked the Serbian army at Bregalnica river and then the Greek army in Nigrita. The Serbian army resisted the sudden night attack, while most of soldiers did not even know who they were fighting with, as Bulgarian camps were located next to Serbs and were considered allies. Montenegro's forces were just a few kilometers away and also rushed to the battle. The Bulgarian attack was halted.
The Greek army was also successful. It retreated according to plan for two days while Thessaloniki was cleared of the remaining Bulgarian regiment. Then the Greek army counter-attacked and defeated the Bulgarians at Kilkis-Lahanas (Kukush), after which the mostly Bulgarian town was destroyed and its population expelled. The Greek army destroyed altogether 161 Bulgarian villages and massacred thousands of inhabitants. Following the capture of Kilkis, the Greek army's pace was not quick enough to prevent the destruction of Nigrita, Serres, and Doxato and massacres of non-combatant Greek inhabitants at Demir Hisar and Doxato by the Bulgarian army. The Greek army then divided its forces and advanced in two directions. Part proceeded east and occupied Western Thrace. The rest of the Greek army advanced up to the Struma River valley, defeating the Bulgarian army in the battles of Doiran and Mt. Beles, and continued its advance to the north towards Sofia. In the Kresna straits the Greeks were ambushed by the Bulgarian 2nd and 1st Army newly arrived from the Serbian front that had already taken defensive positions there following the Bulgarian victory at Kalimanci.
By 30 July the Greek army was outnumbered by the counter-attacking Bulgarian army, which attempted to encircle the Greeks in a Cannae-type battle, by applying pressure on their flanks. The Greek army was exhausted and faced logistical difficulties. The battle was continued for 11 days, between 29 July and 9 August over 20 km of a maze of forests and mountains with no conclusion. The Greek King, seeing that the units he fought were from the Serbian front, tried to convince the Serbs to renew their attack, as the front ahead of them was now thinner, but the Serbs rejected it. By then, news came of the Romanian advance toward Sofia and its imminent fall. Facing the danger of encirclement, Constantine realized that his army could no longer continue hostilities, agreed to Eleftherios Venizelos' proposal and accepted the Bulgarian request for armistice as this had been communicated through Romania.
Romania had raised an army and declared war on Bulgaria on 10 July(27 June) as it had from 28(15) June officially warned Bulgaria that it would not remain neutral in a new Balkan war, due to Bulgaria's refusal to cede the fortress of Silistra as promised before the First Balkan war in exchange for Romanian neutrality. Its forces encountered little resistance and by the time the Greeks accepted the Bulgarian request for armistice they had reached Vrazhdebna, 7 miles from the center of Sofia.
Seeing the military position of the Bulgarian army the Ottomans decided to intervene. They attacked and finding no opposition, managed to recover eastern Thrace with its fortified city of Adrianople, regaining an area in Europe which was only slightly larger than the present-day European territory of the Republic of Turkey.
Reactions among the Great Powers during the wars.
The developments that led to the First Balkan War did not go unnoticed by the Great Powers, but although there was an official consensus between the European Powers over the territorial integrity of the Ottoman Empire, which led to a stern warning to the Balkan states, unofficially each of them took a different diplomatic approach due to their conflicting interests in the area. As a result, any possible preventive effect of the common official warning was cancelled by the mixed unofficial signals, and failed to prevent or to stop the war:
The Second Balkan war was a catastrophic blow to Russian policies in the Balkans, where Russia had focused its interests for access to the "warm seas" for centuries. First, it marked the end of the Balkan League, a vital arm of the Russian system of defense against Austria-Hungary. Second, the clearly pro-Serbian position Russia had been forced to take in the conflict, mainly due to Bulgaria's uncompromising aggressiveness, caused a permanent break-up between the two countries. Accordingly, Bulgaria reverted its policy to one closer to the Central Powers' understanding over an anti-Serbian front, due to its new national aspirations, now expressed mainly against Serbia. As a result, Serbia was isolated militarily against its rival Austria-Hungary, a development that eventually doomed Serbia in the coming war a year later. But most damaging, the new situation effectively trapped Russian foreign policy: After 1913, Russia could not afford losing its last ally in this crucial area and thus had no alternatives but to unconditionally support Serbia when the crisis between Serbia and Austria broke out in 1914. This was a position that inevitably drew her, although unwillingly, into a World War with devastating results for her, since she was less prepared (both militarily and socially) for that event than any other Great Power.
Austria-Hungary took alarm at the great increase in Serbia's territory at the expense of its national aspirations in the region, as well as Serbia's rising status, especially to Austria-Hungary's Slavic populations. This concern was shared by Germany, which saw Serbia as a satellite of Russia. This contributed significantly to the two Central Powers' willingness to go to war as soon as possible.
Finally, when a Serbian backed organization assassinated the heir of the Austro-Hungarian throne, causing the 1914 July Crisis, nobody could stop the conflict and the First World War broke out.
Aftermath.
Soviet demographer Boris Urlanis estimated in "Voini I Narodo-Nacelenie Europi" (1960) that in the first and second Balkan wars there were 122,000 killed in action, 20,000 dead of wounds, and 82,000 dead of disease.
See also.
Since the area has been referred to as the Balkans, notable conflicts have included the following:

</doc>
<doc id="4824" url="http://en.wikipedia.org/wiki?curid=4824" title="Buffalo">
Buffalo

Buffalo (or buffaloe) may refer to:

</doc>
<doc id="4825" url="http://en.wikipedia.org/wiki?curid=4825" title="BeBox">
BeBox

The BeBox was a short-lived dual processor personal computer, offered by Be Inc. to run the company's own operating system, BeOS. Notable aspects of the system include its CPU configuration, I/O board with "GeekPort", and "Blinkenlights" on the front bezel.
The BeBox made its debut in October 1995 (BeBox Dual603-66). The processors were upgraded to 133 MHz in August 1996 (BeBox Dual603e-133). Production was halted in January 1997, following the port of BeOS to the Macintosh, in order for the company to concentrate on software. Be sold around 1000 66 MHz BeBoxes and 800 133 MHz BeBoxes.
CPU configuration.
Initial prototypes were equipped with two AT&T Hobbit processors and three AT&T 9308S DSPs.
Production models used two PowerPC 603 processors running at 66 or 133 MHz to power the BeBox. Prototypes having dual 200 MHz CPUs or four CPUs exist, but these were never publicly available.
"Blinkenlights".
Two yellow/green vertical LED arrays, dubbed the "blinkenlights", were built into the front bezel to illustrate the CPU load. The bottommost LED on the right side indicated hard disk activity.

</doc>
<doc id="4827" url="http://en.wikipedia.org/wiki?curid=4827" title="Biomedical engineering">
Biomedical engineering

Biomedical engineering (BME) is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g. diagnostic or therapeutic). This field seeks to close the gap between engineering and medicine: It combines the design and problem solving skills of engineering with medical and biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy.
Biomedical engineering has only recently emerged as its own study, compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EEGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.
Tissue engineering.
Tissue engineering, like genetic engineering (see below), is a major segment of Biotechnology - which overlaps significantly with BME.
One of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones and tracheas from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. Bioartificial organs, which use both synthetic and biological components, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.
Genetic engineering.
Genetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology ("not a medical application", but see Biological Systems Engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.
Neural engineering.
Neural engineering (also known as Neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.
Pharmaceutical engineering.
Pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of Pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment. The ISPE is an international body that certifies this now rapidly emerging interdisciplinary science.
Medical devices.
This is an "extremely broad category"—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.
A medical device is intended for use in:
Some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.
Stereolithography is a practical example of "medical modeling" being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases.
Medical devices are regulated and classified (in the US) as follows (see also "Regulation"):
Medical imaging.
Medical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly "view" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, other radiology, and other means.
Imaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including:
, fluoroscopy
, magnetic resonance imaging (MRI)
, nuclear medicine
, positron emission tomography (PET) PET scansPET-CT scans
, Projection radiography such as X-rays and CT scans
, tomography
, ultrasound
, optical microscopy
, andelectron microscopy
Implants.
An implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases implants contain electronics e.g. artificial pacemaker and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.
Bionics.
Artificial body part replacement is just one of the things that bionics can do. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different function and processes of the eyes, ears, and other the way for improved cameras, television, radio transmitters and receivers, and many other useful tools. These developments have indeed made our lives better, but the best contribution that bionics has made is in the field of biomedical engineering.
Biomedical Engineering is the building of useful replacements for various parts of the human body. Modern hospitals now have available spare parts to replace a part of the body that is badly damaged by injury or disease. Biomedical engineers who work hand in hand with doctors build these artificial body parts.
Clinical engineering.
Clinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.
Their inherent focus on "practical" implementation of technology has tended to keep them oriented more towards "incremental"-level redesigns and reconfigurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a "bridge" between the primary designers and the end-users, by combining the perspectives of being both 1) close to the point-of-use, while 2) trained in product and process engineering. Clinical Engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems.
Regulatory issues.
Regulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to “a" situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death"“ 
Regardless the country-specific legislation, the main regulatory objectives coincide worldwide. For example, in the medical device regulations, a product must be: 1) safe "and" 2) effective and 3) for all the manufactured devices
A product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, …) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.
A product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.
The previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impacts safety and effectiveness over the whole medical device lifecyle.
The medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical "devices, drugs, biologics, and combination" products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for "consumer" use, such as physical therapy devices (which are also "medical" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K "clearance" (typically for Class 2 devices) or pre-market "approval" (typically for drugs and class 3 devices).
In the European context, safety effectiveness and quality is ensured through the "Conformity Assessment" that is defined as "the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.
In European Union, there are certifying entities named "Notified Bodies", accredited by European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.
The different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the "optimal extent" of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.
RoHS II.
Directive 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation “Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices” (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.
RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.
The scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers, but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.
The Directive has to be transposed by the Member States by January 2, 2013.
IEC 60601.
The new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.
The mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.
Training and certification.
Education.
Biomedical engineers require considerable knowledge of both engineering and biology, and typically have a Master's (M.S.,M.Tech, M.S.E., or M.Eng.) or a Doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Tech,B.S., B.Eng or B.S.E.) to doctoral levels. As noted above, biomedical engineering has only recently been emerging as "its own discipline" rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a "pre-med" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.
In the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET.
In Canada and Australia, accredited graduate programs in Biomedical Engineering are common, for example in Universities such as McMaster University, and the first Canadian undergraduate BME program at Ryerson University offering a four-year B.Eng program. The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering.
As with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees are also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.
Graduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them. Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a masters degree or apply to medical school afterwards.
Graduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or "another engineering" discipline (plus certain life science coursework), or "life science" (plus certain engineering coursework).
Education in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards. Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education. Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.
Licensure/certification.
Engineering licensure in the US is largely optional, and rarely specified by branch/discipline. As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but in practice such a license is not required to practice in the majority of situations (due to an exception known as the private industry exemption, which effectively applies to the vast majority of American engineers). This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.
Biomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.
In the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division.
The Fundamentals of Engineering exam - the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.
Beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.
Career Prospects.
In 2012 there were about 19,400 biomedical engineers employed in the US, and the field was projected to grow by 27% (Much faster than average) from 2012-2022.

</doc>
<doc id="4829" url="http://en.wikipedia.org/wiki?curid=4829" title="Balkans">
Balkans

The Balkan Peninsula, popularly referred to as the Balkans, is a geographical region of Southeast Europe. The region takes its name from the Balkan Mountains that stretch from the east of Bulgaria to the very east of Serbia.
 Countries commonly considered to be in the Balkans Region 
Most of them were under Ottoman rule during Middle Age, and as such have significant Muslim population as well as dominant Eastern Orthodox Church.
The region is predominantly inhabited by Albanians, Bulgarians, Bosniaks, Gorani, Greeks, Macedonians, Montenegrins, Serbs, Romanians, Aromanians, Turks, and other ethnic groups which present minorities in certain countries like the Romani and Ashkali. The largest religion on the Balkans is Orthodox Christianity, followed by Catholic Christianity and Islam.
The total area of the Balkans is 257,400 square miles (666,700 square km) and the population is 59,297,000 (est. 2002). The Balkans meets the Adriatic Sea on the northwest, Ionian Sea on the southwest, the Mediterranean and Aegean Sea on the south and southeast, and the Black Sea on the east and northeast. The highest point of the Balkans is mount Musala on the Rila mountain range in Bulgaria.
The Balkans have been inhabited since the Paleolithic and are the route by which farming from the Middle East spread to Europe during the Neolithic (7th millennium BC). The Balkans are also the location of Europe's first advanced civilizations, beginning with the Bronze Age in Greece around 3200 BC.
Name.
Etymology.
 The region takes its name from the Balkan Mountains in Bulgaria and Serbia. It is believed the name was brought to the region in the 7th century by the Bulgars who applied it to the area, as a part of the First Bulgarian Empire. In Bulgarian language the word "balkan" (балкан) means "mountain". It may have derived from the Persian "bālkāneh" or "bālākhāna", meaning "high, above, or proud house." The name is still preserved in Central Asia with the Balkan Daglary (Balkan Mountains) and the Balkan Province of Turkmenistan. In Turkish "" means "a chain of wooded mountains"
In classical antiquity (and until the Ottoman conquest), the region was referred to in Greek and Roman sources as the "Peninsula of Haemus". The mountain range itself was known as the "Haemus Mountains", the name being of possibly Thracian etymology.
Other possible etymology for "Haemus" () comes from the Greek mythology. 'Haemus', still being used in Modern Greek, derives from the Greek word "haema" () meaning 'blood'. The myth goes about the fight of Zeus and the monster/titan Typhon. Zeus injured Typhon with a thunder and Typhon's blood fell on the mountains from which they got their name.
On a larger scale, the mountains are only one part of a long continuous chain crossing the region in the form of a reversed letter S, from the Carpathians south to the Balkan range proper, before marching away east into Turkey. The Balkan Mountains include the Stara Planina (Old Mountain) mountain range in Bulgaria and part of Serbia. On the west coast, an offshoot of the Dinaric Alps follows the coast south through Dalmatia and Albania, crosses Greece, and continues into the sea in the form of islands.
In the languages of the region, the peninsula is known as:
Evolution of meaning.
The first attested time the name "Balkan" was used in the West for the mountain range in Bulgaria was in a letter sent in 1490 to Pope Innocent VIII by Buonaccorsi Callimaco, an Italian humanist, writer and diplomat. English traveler John Morritt introduced this term into the English literature at the end of the 18th century, and other authors started applying the name to the wider area between the Adriatic and the Black Sea. The concept of the "Balkans" was created by the German geographer August Zeune in 1808. During the 1820s, "Balkan became the preferred although not yet exclusive term alongside Haemus among British travelers... Among Russian travelers not so burdened by classical toponymy, Balkan was the preferred term."
As time passed, the term gradually acquired political connotations far from its initial geographic meaning, arising from political changes from the late 19th century to the creation of post–World War I Yugoslavia (initially the Kingdom of Serbs, Croats and Slovenes). Zeune's goal was to have a geographical parallel term to the Italic and Iberian Peninsula, and seemingly nothing more. The gradually acquired political connotations are newer and, to a large extent, due to oscillating political circumstances.
After the dissolution of Yugoslavia beginning in June 1991, the term "Balkans" again received a negative meaning, even in casual usage (see Balkanization).
Southeast Europe.
In part due to the historical and political connotations of the term "Balkans", especially since the military conflicts of the 1990s, the term "Southeast Europe" is becoming increasingly popular even though it literally refers to a much larger area and thus isn't as precise. A European Union initiative of 1999 is called the "Stability Pact for South Eastern Europe", and the online newspaper "Balkan Times" renamed itself "Southeast European Times" in 2003.
Definitions and boundaries.
The Balkan Peninsula.
The Balkan Peninsula is an area of southeastern Europe surrounded by water on three sides: the Adriatic Sea to the west, the Mediterranean Sea (including the Ionian and Aegean seas) and the Marmara Sea to the south and the Black Sea to the east. Its northern boundary is often given as the Danube, Sava and Kupa Rivers. The Balkan Peninsula has a combined area of about .
Countries whose territories lie completely within the Balkan peninsula:
Countries that lie partially within the Balkan peninsula:
As of 1920 until World War II Italy included Istria and some Dalmatian areas (like "Zara", known as Zadar) that are within the general definition of the Balkan peninsula. The current territory of Italy includes only the small area around Trieste and Gorizia inside the Balkan Peninsula. However, the regions of Trieste and Istria are not usually considered part of the Balkans by Italian geographers, due to a definition of the Balkans that limits its western border to the Kupa River.
The Balkans.
The abstracted term "The Balkans" covers those countries which lie within the boundaries of the Balkan Peninsula. Before 1991, the whole of Yugoslavia was considered to be part of the Balkans. The term "The Balkans" is sometimes used to describe only the areas in the Balkan peninsula: Moesia, Macedonia, Thrace, Kosovo, Šumadija, Bosnia, Herzegovina, Thessaly, Epirus, Peloponnese and others, but more often it includes Serbia and Romania, namely the provinces of Vojvodina, Banat, Wallachia, Moldavia, Transylvania, and others. Italy as a totality is generally accepted as part of Western Europe and the Apennines. The term "the Balkans" was coined by August Zeune in 1808.
Broadly interpreted, the term Balkans comprise the following territories:
Western Balkans.
European Union institutions and member states defined the "Western Balkans" as the Southeast European area that includes countries that are not members of the European Union ( Serbia, Bosnia and Herzegovina, Montenegro, Kosovo, Macedonia and Albania — or Albania plus the former Yugoslavia, minus Croatia and Slovenia). Today, the Western Balkans is more of a political than a geographic designation for the region of Southeast Europe that is not in the European Union. Each country has as its aim to join the EU and reach democracy and transmission scores, but until then they will be strongly connected with the pre-EU waiting program CEFTA.
Nature and natural resources.
Most of the area is covered by mountain ranges running from north-west to south-east. The main ranges are the Balkan mountains, running from the Black Sea Coast in Bulgaria to its border with Serbia, the Rhodope mountains in southern Bulgaria and northern Greece, the Dinaric Alps in Bosnia and Herzegovina and Montenegro, the Šar massif which spreads from Albania to Macedonia, and the Pindus range, spanning from southern Albania into central Greece and the Albanian Alps. The highest mountain of the region is Rila in Bulgaria, with Musala at 2925 m, Mount Olympus in Greece, the throne of Zeus, being second at 2917 m and Vihren in Bulgaria being the third at 2914 m. The karst field or polje is a common feature of the landscape.
On the Adriatic and Aegean coasts the climate is Mediterranean, on the Black Sea coast the climate is humid subtropical and oceanic, and inland it is humid continental. In the northern part of the peninsula and on the mountains, winters are frosty and snowy, while summers are hot and dry. In the southern part winters are milder. The humid continental climate is predominant in Bosnia and Herzegovina, Bulgaria, Kosovo, Macedonia, northern Montenegro, the interior of Albania, Romania and Serbia, while the other, less common climates, the humid subtropical and oceanic climates, are seen on the Black Sea coast of Bulgaria and Turkey; and the Mediterranean climate is seen on the coast of Albania, Greece, southern Montenegro and the Aegean coast of Turkey.
During the centuries many woods have been cut down and replaced with bush. In the southern part and on the coast there is evergreen vegetation. Inland there are woods typical of Central Europe (oak and beech, and in the mountains, spruce, fir and pine). The tree line in the mountains lies at the height of 1800–2300 m. The landscape provides habitats for numerous endemic species, including extraordinarily abundant insects and reptiles that serve as food for a variety of birds of prey and rare vultures.
The soils are generally poor, except on the plains where areas with natural grass, fertile soils and warm summers provide an opportunity for tillage. Elsewhere, land cultivation is mostly unsuccessful because of the mountains, hot summers and poor soils, although certain cultures such as olives and grapes flourish.
Resources of energy are scarce, except in the territory of Kosovo, where considerable coal, lead, zinc, chromium, silver deposits are located. Other deposits of coal, especially in Romania, Bulgaria, Serbia and Bosnia also exist. Lignite deposits are widespread in Greece. Petroleum is most notably present in Romania, although scarce reserves exist in Greece, Serbia and Albania. Natural gas deposits are scarce. Hydropower is in wide use, with over 1,000 dams. The often relentless bora wind is also being harnessed for power generation.
Metal ores are more usual than other raw materials. Iron ore is rare but in some countries there is a considerable amount of copper, zinc, tin, chromite, manganese, magnesite and bauxite. Some metals are exported.
The time zones are situated as follows:
History and geopolitical significance.
Antiquity.
The Balkan region was the first area of Europe to experience the arrival of farming cultures in the Neolithic era. The practices of growing grain and raising livestock arrived in the Balkans from the Fertile Crescent by way of Anatolia and spread west and north into Pannonia and Central Europe.
The identity of the Balkans is dominated by its geographical position; historically the area was known as a crossroads of cultures. It has been a juncture between the Latin and Greek bodies of the Roman Empire, the destination of a massive influx of pagan Bulgars and Slavs, an area which is the meeting point between Islam and Christianity.
In pre-classical and classical antiquity, this region was home to Greeks, Illyrians, Paeonians, Thracians, Dacians, and other ancient groups. Later the Roman Empire conquered most of the region and spread Roman culture and the Latin language, but significant parts still remained under classical Greek influence. The Romans considered the Rhodope Mountains to be the northern limit of the Peninsula of Haemus and the same limit applied approximately to the border between Greek and Latin use in the region (later called the Jireček Line). The Bulgars and Slavs arrived in the 6th century and began assimilating and displacing already-assimilated (through Romanization and Hellenization) older inhabitants of the northern and central Balkans, forming the Bulgarian Empire. During the Middle Ages, the Balkans became the stage for a series of wars between the Byzantine Roman and the Bulgarian Empires.
Early modern period.
By the end of the 16th century, the Ottoman Empire had become the controlling force in the region after expanding from Anatolia through Thrace to the Balkans. Many people in the Balkans place their greatest folk heroes in the era of either the onslaught or the retreat of the Ottoman Empire. As examples, for Greeks, Constantine XI Palaiologos and Kolokotronis; and for Serbs, Miloš Obilić and Tzar Lazar; for Montenegrins, Đurađ I Balšić and Ivan Crnojević; for Albanians, George Kastrioti Skanderbeg; for ethnic Macedonians, Nikola Karev and Goce Delčev; and for Bulgarians, Vasil Levski, Georgi Sava Rakovski and Hristo Botev.
In the past several centuries, because of the frequent Ottoman wars in Europe fought in and around the Balkans and the comparative Ottoman isolation from the mainstream of economic advance (reflecting the shift of Europe's commercial and political centre of gravity towards the Atlantic), the Balkans has been the least developed part of Europe. According to Suraiya Faroqhi and Donald Quataert, "The population of the Balkans, according to one estimate, fell from a high of 8 million in the late 16th century to only 3 million by the mid-eighteenth. This estimate is in harmony with the first findings based on Ottoman documentary evidence."
Most of the Balkan nation-states emerged during the 19th and early 20th centuries as they gained independence from the Ottoman Empire or the Austro-Hungarian empire (Greece in 1821, Serbia, Montenegro and Romania in 1878, Bulgaria in 1908, Albania in 1912).
20th century.
World wars.
In 1912–1913 the First Balkan War broke out when the nation-states of Bulgaria, Serbia, Greece and Montenegro united in an alliance against the Ottoman Empire. As a result of the war, almost all remaining European territories of the Ottoman Empire were captured and partitioned among the allies. Ensuing events also led to the creation of an independent Albanian state. Bulgaria insisted on its status quo territorial integrity, divided and shared by the Great Powers next to the Russo-Turkish War (1877–78) in other boundaries and on the pre-war Bulgarian-Serbian agreement. Provoked by the backstage deals between its former allies Serbia and Greece on allocation the spoils at the end of the First Balkan War, while it fights at the main Thracian Front, Bulgaria marks the beginning of Second Balkan War when attacked them. The Serbs and the Greeks repulse single attacks, but when the Greek army invaded Bulgaria together with an unprovoked Romanian intervention in the back, regardless of the single won battles, Bulgaria collapsed. The Ottoman Empire also used the opportunity to recapture Eastern Thrace, establishing its new western borders that still stand today.
The First World War was sparked in the Balkans in 1914 when Mlada Bosna, a revolutionary organization with predominately Serbian and pro-Yugoslav members, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand of Austria in Bosnia and Herzegovina's capital, Sarajevo. That caused a war between the two countries which—through the existing chains of alliances—led to the First World War. The Ottoman empire soon joined the Central Powers becoming one of the three empires participating in that alliance. The next year Bulgaria joined the Central Powers attacking Serbia, which was successfully fighting Austro-Hungary to the north for a year. That led to Serbia's defeat and the intervention of the Entente in the Balkans which sent an expeditionary force to establish a new front, the third one of that war, which soon also became static. The participation of Greece in the war three years later, in 1918, on the part of the Entente finally altered the balance between the opponents leading to the collapse of the common German-Bulgarian front there, which caused the exit of Bulgaria from the war, and in turn the collapse of the Austro-Hungarian Empire, ending the First World War.
With the start of the Second World War all Balkan countries, with the exception of Greece, were allies of Nazi Germany, having bilateral military agreements or being part of the Axis Pact. Fascist Italy expanded the war in the Balkans by using its protectorate Albania to invade Greece. After repelling the attack, the Greeks counterattacked, invading Italy-held Albania and causing Nazi Germany's intervention in the Balkans to help its ally. Days before the German invasion a successful coup d'état in Belgrade by neutral military personnel seized power.
Although the new government reaffirmed Serbia's intentions to fulfill its obligations as member of the Axis, Germany, using its other two allied countries in the region, Bulgaria and Hungary, invaded both Greece and Yugoslavia. Yugoslavia immediately disintegrated when those loyal to the Serbian King and the Croatian units mutinied. Greece resisted, but, after two months of fighting, collapsed and was occupied. The two countries were partitioned between the three Axis allies, Bulgaria, Germany and Italy, and the Independent State of Croatia, a puppet state of Italy and Germany.
During the occupation the population suffered considerable hardship due to repression and starvation, to which the population reacted by creating a mass resistance movement. Together with the early and extremely heavy winter of that year (which caused hundreds of thousands deaths among the poorly fed population), the German invasion had disastrous effects in the timetable of the planned invasion in Russia causing a significant delay, which had major consequences during the course of the war.
Finally, at the end of 1944, the Soviets entered Romania and Bulgaria forcing the Germans out of the Balkans. They left behind a region largely ruined as a result of wartime exploitation, but by making use of the post-war separation of Germany into two independent entities, the German states successfully and legally avoided paying any reparations or repaying the forced loans given by the occupied countries.
Cold War.
During the Cold War, most of the countries on the Balkans were governed by communist governments. Greece became the first battleground of the emerging Cold War. The Truman Doctrine was the US response to the civil war, which raged from 1944 to 1949. This civil war, unleashed by the Communist Party of Greece, backed by communist volunteers from neighboring countries (Albania, Bulgaria and Yugoslavia), led to massive American assistance for the non-communist Greek government. With this backing, Greece managed to defeat the partisans and, ultimately, remained the only non-communist country in the region.
However, despite being under communist governments, Yugoslavia (1948) and Albania (1961) fell out with the Soviet Union. Yugoslavia, led by Marshal Josip Broz Tito (1892–1980), first propped up then rejected the idea of merging with Bulgaria and instead sought closer relations with the West, later even spearheaded, together with India and Egypt the Non-Aligned Movement. Albania on the other hand gravitated toward Communist China, later adopting an isolationist position.
As the only non-communist countries, Greece and Turkey were (and still are) part of NATO composing the southeastern wing of the alliance.
Post–Cold War.
In the 1990s, the transition of the regions' ex-Soviet bloc countries towards democratic free-market societies went peacefully with the exception of Yugoslavia. Wars between the former Yugoslav republics broke out after Slovenia and Croatia held free elections and their people voted for independence on their respective countries' referenda. Serbia in turn declared the dissolution of the union as unconstitutional and the Yugoslavian army unsuccessfully tried to maintain status quo. Slovenia and Croatia declared independence on 25 June 1991, followed by the Ten-Day War in Slovenia. Till October 1991, the Army withdrew from Slovenia, and in Croatia, the Croatian War of Independence would continue until 1995. In the ensuing 10 years armed confrontation, gradually all the other Republics declared independence, with Bosnia being the most affected by the fighting. The long lasting wars resulted in a United Nations intervention and NATO ground and air forces took action against Serb forces in Bosnia and Herzegovina and Serbia.
From the dissolution of Yugoslavia six republics achieved international recognition as sovereign republics, but these are traditionally included in Balkans: Bosnia and Herzegovina, Macedonia, Montenegro and Serbia. In 2008, while under UN administration, Kosovo declared independence (according to the official Serbian policy, Kosovo is still an internal autonomous region). In July 2010, the International Court of Justice, ruled that the declaration of independence was legal. Most UN member states recognise Kosovo. After the end of the wars a revolution broke in Serbia and Slobodan Milošević, the Serbian communist leader (elected president between 1989 and 2000), was overthrown and handed for trial to the International Criminal Tribunal for crimes against the International Humanitarian Law during the Yugoslav wars. Milošević died of a heart attack in 2006 before a verdict could have been released. Ιn 2001 an Albanian uprising in Macedonia forced the country to give local autonomy to the ethnic Albanians in the areas where they predominate.
With the dissolution of Yugoslavia an issue emerged over the name under which the former (federated) republic of Macedonia would internationally be recognized, between the new country and Greece. Being the Macedonian part of Yugoslavia (see Vardar Macedonia), the federated Republic under the Yugoslav identity had the name Republic of Macedonia on which it declared its sovereignty in 1991. Greece, having a large region (see Macedonia) also under the same name opposed to the usage of this name as an indication of a nationality. The issue is currently under negotiations after a UN initiation.
Balkan countries control the direct land routes between Western Europe and South West Asia (Asia Minor and the Middle East). Since 2000, all Balkan countries are friendly towards the EU and the USA.
Greece has been a member of the European Union since 1981; Bulgaria and Romania since 2007. In 2005, the European Union decided to start accession negotiations with candidate countries; Turkey, and Macedonia were accepted as candidates for European Union membership. In March 2004, Bulgaria and Romania have become members of NATO. As of April 2009, Albania is members of NATO. Bosnia and Herzegovina and what was then Serbia and Montenegro started negotiations with the EU over the Stabilization and Accession Agreements, although shortly after they started, negotiations with Serbia and Montenegro were suspended for lack of co-operation with the International Criminal Tribunal for the Former Yugoslavia. During the 2008 Bucharest summit Greece vetoed Macedonia's NATO membership bid over the Macedonia naming dispute between the two countries.
All other countries have expressed a desire to join the EU at some point in the future.
Politics and economy.
Currently all of the states are republics, but until World War II all except Turkey were monarchies. Most of the republics are parliamentary, excluding Romania and Bosnia which are semi-presidential. All the states have open market economies, most of which are in the upper-middle income range ($4,000 – $12,000 p.c.), however, Greece has high income economies (over $12,000 p.c.), and is also classified with very high HDI in contrast to the remaining states which are classified with high HDI. The states from the former Eastern Bloc that formerly had planned economy system and Turkey mark gradual economic growth each year, only the economy of Greece drops for 2012 and meanwhile it was expected to grow in 2013. The Gross domestic product (Purchasing power parity) per capita is highest in Greece (over $25), followed by Turkey, Bulgaria, Romania, Montenegro, Serbia, Macedonia ($10 – $15), Bosnia, Albania and Kosovo (below $10). The Gini coefficient, which indicates the level of difference by monetary welfare of the layers, is on the second level at the highest monetary equality in Albania, Bulgaria and Serbia, on the third level in Greece, Montenegro and Romania, on the fourth level in Macedonia, on the fifth level in Turkey, and the most unequal by Gini coefficient is Bosnia at the eighth level which is the penultimate level and one of the highest in the world. The unemployment is lowest in Romania (below 10%), followed by Bulgaria, Turkey, Albania (10 – 15%), Greece (15 – 20%), Montenegro, Serbia, Bosnia (20 – 30%), Macedonia (over 30%) and Kosovo (over 40%).
Regional organizations.
See also the Black Sea Regional organizations
Demographics.
The Balkans have a population of 60–71 million and a population density of 80-91/km2, depending on whether the Turkish and Italian parts are counted within the peninsula. Without those, the peninsula has a population of about 48 million and a density of 99/km2.
Population by territories:
[*] The islands are not taken into account.<br>
[**] Both census figures of Serbia and Kosovo in the table do not include North Kosovo, therefore in the population of the Balkans, made up of sum of the populations in the table, is added separately an additional number of 70,000 to include the missing population of North Kosovo.
Religion.
The region's principal religions are Christianity (Eastern Orthodox and Islam (Sunni). Eastern Orthodoxy is the majority religion in both the Balkan peninsula and the Balkan region. A variety of different traditions of each faith are practiced, with each of the Eastern Orthodox countries having its own national church.
The Jewish communities of the Balkans were some of the oldest in Europe and date back to ancient times. These communities were Sephardi Jews, except in Romania where the Jewish communities were Ashkenazi Jews. In Bosnia and Herzegovina, the small and close-knit Jewish community is 90% Sephardic, and Ladino is still spoken among the elderly. The Sephardi Jewish cemetery in Sarajevo has tombstones of a unique shape and inscribed in ancient Ladino. Sephardi Jews used to have a large presence in the city of Thessaloniki, and by 1900, some 80,000, or more than half of the population, were Jews. The Jewish communities in the Balkans suffered immensely during World War II, and the vast majority were killed during the Holocaust. An exception were the Bulgarian Jews, most of whom were saved by Boris III of Bulgaria, who resisted Adolf Hitler, opposing their deportation to concentration camps. Almost all of the few survivors have emigrated to the (then) newly founded state of Israel and elsewhere. No Balkan country today has a significant Jewish minority.
Languages.
The Balkan region today is a very diverse ethno-linguistic region, being home to multiple Slavic, Romance, and Turkic languages, as well as Greek, Albanian, and others. Romani is spoken by a large portion of the Romanis living throughout the Balkan countries. Throughout history many other ethnic groups with their own languages lived in the area, among them Thracians, Illyrians, Romans, Celts and various Germanic tribes. All of the aforementioned languages from the present and from the past belong to the wider Indo-European language family, with the exception of the Turkic languages (e.g., Turkish and Gagauz).
Urbanization.
Most of the states in the Balkans are predominantly urbanized; the countries in which the rural population is the majority are Bosnia and Herzegovina and Kosovo each being about 50% rural and 50% urban.
A list of cities with population of over 200,000 inhabitants:
Notes and references.
Notes:
References:

</doc>
<doc id="4831" url="http://en.wikipedia.org/wiki?curid=4831" title="Bohr model">
Bohr model

In atomic physics, the Rutherford–Bohr model or Bohr model, introduced by Niels Bohr in 1913, depicts the atom as a small, positively charged nucleus surrounded by electrons that travel in circular orbits around the nucleus—similar in structure to the solar system, but with attraction provided by electrostatic forces rather than gravity. After the cubic model (1902), the plum-pudding model (1904), the Saturnian model (1904), and the Rutherford model (1911) came the Rutherford–Bohr model or just "Bohr model" for short (1913). The improvement to the Rutherford model is mostly a quantum physical interpretation of it. The Bohr model has been superseded, but the quantum theory remains sound.
The model's key success lay in explaining the Rydberg formula for the spectral emission lines of atomic hydrogen. While the Rydberg formula had been known experimentally, it did not gain a theoretical underpinning until the Bohr model was introduced. Not only did the Bohr model explain the reason for the structure of the Rydberg formula, it also provided a justification for its empirical results in terms of fundamental physical constants.
The Bohr model is a relatively primitive model of the hydrogen atom, compared to the valence shell atom. As a theory, it can be derived as a first-order approximation of the hydrogen atom using the broader and much more accurate quantum mechanics and thus may be considered to be an obsolete scientific theory. However, because of its simplicity, and its correct results for selected systems (see below for application), the Bohr model is still commonly taught to introduce students to quantum mechanics or energy level diagrams before moving on to the more accurate, but more complex, valence shell atom. A related model was originally proposed by Arthur Erich Haas in 1910, but was rejected. The quantum theory of the period between Planck's discovery of the quantum (1900) and the advent of a full-blown quantum mechanics (1925) is often referred to as the old quantum theory.
Origin.
In the early 20th century, experiments by Ernest Rutherford established that atoms consisted of a diffuse cloud of negatively charged electrons surrounding a small, dense, positively charged nucleus. Given this experimental data, Rutherford naturally considered a planetary-model atom, the Rutherford model of 1911 – electrons orbiting a solar nucleus – however, said planetary-model atom has a technical difficulty. The laws of classical mechanics (i.e. the Larmor formula), predict that the electron will release electromagnetic radiation while orbiting a nucleus. Because the electron would lose energy, it would rapidly spiral inwards, collapsing into the nucleus on a timescale of around 16 picoseconds. This atom model is disastrous, because it predicts that all atoms are unstable.
Also, as the electron spirals inward, the emission would rapidly increase in frequency as the orbit got smaller and faster. This would produce a continuous smear, in frequency, of electromagnetic radiation. However, late 19th century experiments with electric discharges have shown that atoms will only emit light (that is, electromagnetic radiation) at certain discrete frequencies.
To overcome this difficulty, Niels Bohr proposed, in 1913, what is now called the "Bohr model of the atom". He suggested that electrons could only have certain "classical" motions:
The significance of the Bohr model is that the laws of classical mechanics apply to the motion of the electron about the nucleus "only when restricted by a quantum rule". Although Rule 3 is not completely well defined for small orbits, because the emission process involves two orbits with two different periods, Bohr could determine the energy spacing between levels using Rule 3 and come to an exactly correct quantum rule: the angular momentum "L" is restricted to be an integer multiple of a fixed unit:
where "n" = 1, 2, 3, ... is called the principal quantum number, and "ħ" = "h"/2π. The lowest value of "n" is 1; this gives a smallest possible orbital radius of 0.0529 nm known as the Bohr radius. Once an electron is in this lowest orbit, it can get no closer to the proton. Starting from the angular momentum quantum rule, Bohr was able to calculate the energies of the allowed orbits of the hydrogen atom and other hydrogen-like atoms and ions.
Other points are:
Bohr's condition, that the angular momentum is an integer multiple of "ħ" was later reinterpreted in 1924 by de Broglie as a standing wave condition: the electron is described by a wave and a whole number of wavelengths must fit along the circumference of the electron's orbit:
Substituting de Broglie's wavelength of h/p reproduces Bohr's rule. In 1913, however, Bohr justified his rule by appealing to the correspondence principle, without providing any sort of wave interpretation. In 1913, the wave behavior of matter particles such as the electron (i.e., matter waves) was not suspected.
In 1925 a new kind of mechanics was proposed, quantum mechanics, in which Bohr's model of electrons traveling in quantized orbits was extended into a more accurate model of electron motion. The new theory was proposed by Werner Heisenberg. Another form of the same theory, wave mechanics, was discovered by the Austrian physicist Erwin Schrödinger independently, and by different reasoning. Schrödinger employed de Broglie's matter waves, but sought wave solutions of a three-dimensional wave equation describing electrons that were constrained to move about the nucleus of a hydrogen-like atom, by being trapped by the potential of the positive nuclear charge.
Electron energy levels.
The Bohr model gives almost exact results only for a system where two charged points orbit each other at speeds much less than that of light. This not only includes one-electron systems such as the hydrogen atom, singly ionized helium, doubly ionized lithium, but it includes positronium and Rydberg states of any atom where one electron is far away from everything else. It can be used for K-line X-ray transition calculations if other assumptions are added (see Moseley's law below). In high energy physics, it can be used to calculate the masses of heavy quark mesons.
Calculation of the orbits requires two assumptions.
An electron in the lowest energy level of hydrogen () therefore has about 13.6 eV less energy than a motionless electron infinitely far from the nucleus. The next energy level () is −3.4 eV. The third ("n" = 3) is −1.51 eV, and so on. For larger values of "n", these are also the binding energies of a highly excited atom with one electron in a large circular orbit around the rest of the atom.
The combination of natural constants in the energy formula is called the Rydberg energy ("R"E):
This expression is clarified by interpreting it in combinations that form more natural units:
Since this derivation is with the assumption that the nucleus is orbited by one electron, we can generalize this result by letting the nucleus have a charge "q" = "Z e" where "Z" is the atomic number. This will now give us energy levels for hydrogenic atoms, which can serve as a rough order-of-magnitude approximation of the actual energy levels. So for nuclei with "Z" protons, the energy levels are (to a rough approximation):
The actual energy levels cannot be solved analytically for more than one electron (see "n"-body problem) because the electrons are not only affected by the nucleus but also interact with each other via the Coulomb Force.
When "Z" = 1/"α" (Z ≈ 137), the motion becomes highly relativistic, and "Z"2 cancels the "α"2 in "R"; the orbit energy begins to be comparable to rest energy. Sufficiently large nuclei, if they were stable, would reduce their charge by creating a bound electron from the vacuum, ejecting the positron to infinity. This is the theoretical phenomenon of electromagnetic charge screening which predicts a maximum nuclear charge. Emission of such positrons has been observed in the collisions of heavy ions to create temporary super-heavy nuclei.
The Bohr formula properly uses the reduced mass of electron and proton in all situations, instead of the mass of the electron: formula_18. However, these numbers are very nearly the same, due to the much larger mass of the proton, about 1836.1 times the mass of the electron, so that the reduced mass in the system is the mass of the electron multiplied by the constant 1836.1/(1+1836.1) = 0.99946. This fact was historically important in convincing Rutherford of the importance of Bohr's model, for it explained the fact that the frequencies of lines in the spectra for singly ionized helium do not differ from those of hydrogen by a factor of exactly 4, but rather by 4 times the ratio of the reduced mass for the hydrogen vs. the helium systems, which was much closer to the experimental ratio than exactly 4.0.
For positronium, the formula uses the reduced mass also, but in this case, it is exactly the electron mass divided by 2. For any value of the radius, the electron and the positron are each moving at half the speed around their common center of mass, and each has only one fourth the kinetic energy. The total kinetic energy is half what it would be for a single electron moving around a heavy nucleus.
Rydberg formula.
The Rydberg formula, which was known empirically before Bohr's formula, is seen in Bohr's theory as describing the energies of transitions or quantum jumps between one orbital energy levels. Bohr's formula gives the numerical value of the already-known and measured Rydberg's constant, but in terms of more fundamental constants of nature, including the electron's charge and Planck's constant.
When the electron gets moved from its original energy level to a higher one, it then jumps back each level till it comes to the original position, which results in a photon being emitted. Using the derived formula for the different energy levels of hydrogen one may determine the wavelengths of light that a hydrogen atom can emit.
The energy of a photon emitted by a hydrogen atom is given by the difference of two hydrogen energy levels:
where "n""f" is the final energy level, and "n""i" is the initial energy level.
Since the energy of a photon is
the wavelength of the photon given off is given by
This is known as the Rydberg formula, and the Rydberg constant R is formula_23, or formula_24 in natural units. This formula was known in the nineteenth century to scientists studying spectroscopy, but there was no theoretical explanation for this form or a theoretical prediction for the value of R, until Bohr. In fact, Bohr's derivation of the Rydberg constant, as well as the concomitant agreement of Bohr's formula with experimentally observed spectral lines of the Lyman (formula_25), Balmer (formula_26), and Paschen (formula_27) series, and successful theoretical prediction of other lines not yet observed, was one reason that his model was immediately accepted.
To apply to atoms with more than one electron, the Rydberg formula can be modified by replacing "Z" with "Z − b" or "n" with "n − b" where b is constant representing a screening effect due to the inner-shell and other electrons (see Electron shell and the later discussion of the "Shell Model of the Atom" below). This was established empirically before Bohr presented his model.
Shell model of heavier atoms.
Bohr extended the model of hydrogen to give an approximate model for heavier atoms. This gave a physical picture that reproduced many known atomic properties for the first time.
Heavier atoms have more protons in the nucleus, and more electrons to cancel the charge. Bohr's idea was that each discrete orbit could only hold a certain number of electrons. After that orbit is full, the next level would have to be used. This gives the atom a shell structure, in which each shell corresponds to a Bohr orbit.
This model is even more approximate than the model of hydrogen, because it treats the electrons in each shell as non-interacting. But the repulsions of electrons are taken into account somewhat by the phenomenon of screening. The electrons in outer orbits do not only orbit the nucleus, but they also move around the inner electrons, so the effective charge Z that they feel is reduced by the number of the electrons in the inner orbit.
For example, the lithium atom has two electrons in the lowest 1s orbit, and these orbit at Z=2. Each one sees the nuclear charge of Z=3 minus the screening effect of the other, which crudely reduces the nuclear charge by 1 unit. This means that the innermost electrons orbit at approximately 1/4 the Bohr radius. The outermost electron in lithium orbits at roughly Z=1, since the two inner electrons reduce the nuclear charge by 2. This outer electron should be at nearly one Bohr radius from the nucleus. Because the electrons strongly repel each other, the effective charge description is very approximate; the effective charge Z doesn't usually come out to be an integer. But Moseley's law experimentally probes the innermost pair of electrons, and shows that they do see a nuclear charge of approximately Z−1, while the outermost electron in an atom or ion with only one electron in the outermost shell orbits a core with effective charge Z−k where k is the total number of electrons in the inner shells.
The shell model was able to qualitatively explain many of the mysterious properties of atoms which became codified in the late 19th century in the periodic table of the elements. One property was the size of atoms, which could be determined approximately by measuring the viscosity of gases and density of pure crystalline solids. Atoms tend to get smaller toward the right in the periodic table, and become much larger at the next line of the table. Atoms to the right of the table tend to gain electrons, while atoms to the left tend to lose them. Every element on the last column of the table is chemically inert (noble gas).
In the shell model, this phenomenon is explained by shell-filling. Successive atoms become smaller because they are filling orbits of the same size, until the orbit is full, at which point the next atom in the table has a loosely bound outer electron, causing it to expand. The first Bohr orbit is filled when it has two electrons, which explains why helium is inert. The second orbit allows eight electrons, and when it is full the atom is neon, again inert. The third orbital contains eight again, except that in the more correct Sommerfeld treatment (reproduced in modern quantum mechanics) there are extra "d" electrons. The third orbit may hold an extra 10 d electrons, but these positions are not filled until a few more orbitals from the next level are filled (filling the n=3 d orbitals produces the 10 transition elements). The irregular filling pattern is an effect of interactions between electrons, which are not taken into account in either the Bohr or Sommerfeld models and which are difficult to calculate even in the modern treatment.
Moseley's law and calculation of K-alpha X-ray emission lines.
Niels Bohr said in 1962, "You see actually the Rutherford work [the nuclear atom] was not taken seriously. We cannot understand today, but it was not taken seriously at all. There was no mention of it any place. The great change came from Moseley."
In 1913 Henry Moseley found an empirical relationship between the strongest X-ray line emitted by atoms under electron bombardment (then known as the K-alpha line), and their atomic number Z. Moseley's empiric formula was found to be derivable from Rydberg and Bohr's formula (Moseley actually mentions only Ernest Rutherford and Antonius Van den Broek in terms of models). The two additional assumptions that [1] this X-ray line came from a transition between energy levels with quantum numbers 1 and 2, and [2], that the atomic number Z when used in the formula for atoms heavier than hydrogen, should be diminished by 1, to (Z−1)2.
Moseley wrote to Bohr, puzzled about his results, but Bohr was not able to help. At that time, he thought that the postulated innermost "K" shell of electrons should have at least four electrons, not the two which would have neatly explained the result. So Moseley published his results without a theoretical explanation.
Later, people realized that the effect was caused by charge screening, with an inner shell containing only 2 electrons. In the experiment, one of the innermost electrons in the atom is knocked out, leaving a vacancy in the lowest Bohr orbit, which contains a single remaining electron. This vacancy is then filled by an electron from the next orbit, which has n=2. But the n=2 electrons see an effective charge of Z−1, which is the value appropriate for the charge of the nucleus, when a single electron remains in the lowest Bohr orbit to screen the nuclear charge +Z, and lower it by −1 (due to the electron's negative charge screening the nuclear positive charge). The energy gained by an electron dropping from the second shell to the first gives Moseley's law for K-alpha lines:
or
Here, R"v = R"E/"h" is the Rydberg constant, in terms of frequency equal to 3.28 x 1015 Hz. For values of Z between 11 and 31 this latter relationship had been empirically derived by Moseley, in a simple (linear) plot of the square root of X-ray frequency against atomic number (however, for silver, Z = 47, the experimentally obtained screening term should be replaced by 0.4). Notwithstanding its restricted validity, Moseley's law not only established the objective meaning of atomic number (see Henry Moseley for detail) but, as Bohr noted, it also did more than the Rydberg derivation to establish the validity of the Rutherford/Van den Broek/Bohr nuclear model of the atom, with atomic number (place on the periodic table) standing for whole units of nuclear charge.
The K-alpha line of Moseley's time is now known to be a pair of close lines, written as (Kα1 and Kα2) in Siegbahn notation.
Shortcomings.
The Bohr model gives an incorrect value formula_30 for the ground state orbital angular momentum. The angular momentum in the true ground state is known to be zero. Although mental pictures fail somewhat at these levels of scale, an electron in the lowest modern "orbital" with no orbital momentum, may be thought of as not to rotate "around" the nucleus at all, but merely to go tightly around it in an ellipse with zero area (this may be pictured as "back and forth", without striking or interacting with the nucleus). This is only reproduced in a more sophisticated semiclassical treatment like Sommerfeld's. Still, even the most sophisticated semiclassical model fails to explain the fact that the lowest energy state is spherically symmetric - it doesn't point in any particular direction. Nevertheless, in the modern "fully quantum treatment in phase space", Weyl quantization, the proper deformation (full extension) of the semi-classical result adjusts the angular momentum value to the correct effective one. As a consequence, the physical ground state expression is obtained through a shift of the vanishing quantum angular momentum expression, which corresponds to spherical symmetry.
In modern quantum mechanics, the electron in hydrogen is a spherical cloud of probability that grows denser near the nucleus. The rate-constant of probability-decay in hydrogen is equal to the inverse of the Bohr radius, but since Bohr worked with circular orbits, not zero area ellipses, the fact that these two numbers exactly agree is considered a "coincidence". (However, many such coincidental agreements are found between the semiclassical vs. full quantum mechanical treatment of the atom; these include identical energy levels in the hydrogen atom and the derivation of a fine structure constant, which arises from the relativistic Bohr–Sommerfeld model (see below) and which happens to be equal to an entirely different concept, in full modern quantum mechanics).
The Bohr model also has difficulty with, or else fails to explain:
Refinements.
Several enhancements to the Bohr model were proposed, most notably the Sommerfeld model or Bohr–Sommerfeld model, which suggested that electrons travel in elliptical orbits around a nucleus instead of the Bohr model's circular orbits. This model supplemented the quantized angular momentum condition of the Bohr model with an additional radial quantization condition, the Sommerfeld–Wilson quantization condition
where "pr" is the radial momentum canonically conjugate to the coordinate "q" which is the radial position and "T" is one full orbital period. The integral is the action of action-angle coordinates. This condition, suggested by the correspondence principle, is the only one possible, since the quantum numbers are adiabatic invariants.
The Bohr–Sommerfeld model was fundamentally inconsistent and led to many paradoxes. The magnetic quantum number measured the tilt of the orbital plane relative to the "xy"-plane, and it could only take a few discrete values. This contradicted the obvious fact that an atom could be turned this way and that relative to the coordinates without restriction. The Sommerfeld quantization can be performed in different canonical coordinates and sometimes gives different answers. The incorporation of radiation corrections was difficult, because it required finding action-angle coordinates for a combined radiation/atom system, which is difficult when the radiation is allowed to escape. The whole theory did not extend to non-integrable motions, which meant that many systems could not be treated even in principle. In the end, the model was replaced by the modern quantum mechanical treatment of the hydrogen atom, which was first given by Wolfgang Pauli in 1925, using Heisenberg's matrix mechanics. The current picture of the hydrogen atom is based on the atomic orbitals of wave mechanics which Erwin Schrödinger developed in 1926.
However, this is not to say that the Bohr model was without its successes. Calculations based on the Bohr–Sommerfeld model were able to accurately explain a number of more complex atomic spectral effects. For example, up to first-order perturbations, the Bohr model and quantum mechanics make the same predictions for the spectral line splitting in the Stark effect. At higher-order perturbations, however, the Bohr model and quantum mechanics differ, and measurements of the Stark effect under high field strengths helped confirm the correctness of quantum mechanics over the Bohr model. The prevailing theory behind this difference lies in the shapes of the orbitals of the electrons, which vary according to the energy state of the electron.
The Bohr–Sommerfeld quantization conditions lead to questions in modern mathematics. Consistent semiclassical quantization condition requires a certain type of structure on the phase space, which places topological limitations on the types of symplectic manifolds which can be quantized. In particular, the symplectic form should be the curvature form of a connection of a Hermitian line bundle, which is called a prequantization.

</doc>
<doc id="4832" url="http://en.wikipedia.org/wiki?curid=4832" title="Bombay Sapphire">
Bombay Sapphire

Bombay Sapphire is a brand of gin owned by Bacardi that was first launched in 1987 by IDV. In 1997 Diageo sold the brand to Bacardi. Its name originates from gin's popularity in India during the British Raj and the sapphire in question is the Star of Bombay on display at the Smithsonian Institution. Bombay Sapphire is marketed in a flat-sided, sapphire-coloured bottle that bears a picture of Queen Victoria on the label.
The flavouring of the drink comes from a recipe of ten ingredients (which the bottle's label boasts as "10 exotic botanicals"): almond, lemon peel, liquorice, juniper berries, orris root, angelica, coriander, cassia, cubeb, and grains of paradise. The spirit is triple distilled using a carterhead still, and the alcohol vapours are passed through a mesh/basket containing the ten botanicals, in order to gain flavour and aroma. This gives a lighter, more floral gin rather than the more-common 'punchy' gins that are distilled using a copper pot still. Water from Lake Vyrnwy is added to bring the strength of Bombay Sapphire down to 40.0% (UK, Australia).
In 2011 it was announced that the company is planning to move the distillation process to a new facility in Laverstoke, Hampshire. The plans include the restoration of the former Portal's paper mill, and the construction of a visitor centre. In February 2012, they received planning permission for the site and the centre is scheduled to be opened in Spring 2014.
Production and bottling of the drink is contracted out by Bacardi to G&J Greenall.
Varieties.
Bacardi also markets Bombay Original London Dry Gin (or Bombay Original Dry). Made with eight botanical ingredients rather than ten, Bombay is less common than Bombay Sapphire, though "Wine Enthusiast" has called it better.
In September 2011, Bombay Sapphire East was launched in test markets in New York and Las Vegas. This variety has another two botanicals, lemongrass and black peppercorns, in addition to the original ten. It is bottled at 42% and was designed to counteract the sweetness of American tonic water.
Design connection.
The brand started a series of design collaborations. Their first step into the design world was a series of advertisements featuring work from currently popular designers. Their works, varying from martini glasses to tiles and cloth patterns, are labelled as “Inspired by Bombay Sapphire”. The campaign featured designers such as Marcel Wanders, Yves Behar, Karim Rashid, Ulla Darni, and Dror Benshetrit and performance artist Jurgen Hahn.
From the success of this campaign, the company started a series of events and sponsored locations. The best known is the Bombay Sapphire Designer Glass Competition, held each year, where design students from all over the world can participate by designing their own “inspired” martini cocktail glass. The finalists (one from each participating country) are then invited to the yearly Salone del Mobile, an international design fair in Milano, where the winner is chosen.
Bombay Sapphire also endorses glass artists and designers with the Bombay Sapphire Prize, which is awarded every year to an outstanding design which features glass. Bombay Sapphire also showcases the designers' work in the Bombay Sapphire endorsed blue room, which is actually a design exhibition touring the world each year.
From 2008 the Bombay Sapphire Designer Glass Competition final will be held at 100% Design in London, UK and the Bombay Sapphire Prize will take place in Milan at the Salone Del Mobile.
Evaluation.
Bombay Sapphire has been reviewed by several outside spirit ratings organizations to mixed success. Recently, it was awarded a score of 92 (on a 100 point scale) from the Beverage Testing Institute. Ratings aggregator Proof66.com categorizes the Sapphire as a Tier 2 spirit, indicating highly favorable "expert" reviews.

</doc>
<doc id="4833" url="http://en.wikipedia.org/wiki?curid=4833" title="Bob Wills">
Bob Wills

James Robert Wills (March 6, 1905 – May 13, 1975), better known as Bob Wills, was an American Western swing musician, songwriter, and bandleader. Considered by music authorities as the co-founder of Western swing, he was universally known as the King of Western Swing (after the death of Spade Cooley who used the moniker "King Of Western Swing" from 1942 to 1969.)
Wills formed several bands and played radio stations around the South and West until he formed the Texas Playboys in 1934 with Wills on fiddle, Tommy Duncan on piano and vocals, rhythm guitarist June Whalin, tenor banjoist Johnnie Lee Wills, and Kermit Whalin, who played steel guitar and bass. The band played regularly on a Tulsa, Oklahoma radio station and added Leon McAuliffe on steel guitar, pianist Al Stricklin, drummer Smokey Dacus, and a horn section that expanded the band's sound. Wills favored jazz-like arrangements and the band found national popularity into the 1940s with such hits as "Steel Guitar Rag", "New San Antonio Rose", "Smoke On The Water", "Stars And Stripes On Iwo Jima", and "New Spanish Two Step".
Wills and the Texas Playboys recorded with several publishers and companies, including Vocalion, Okeh, Columbia, and MGM, frequently moving. In 1950, he had two Top 10 hits, "Ida Red Likes The Boogie" and "Faded Love", which were his last hits for a decade. Throughout the 1950s, he struggled with poor health and tenuous finances, but continued to perform frequently despite the decline in popularity of his earlier music as rock and roll took over. Wills had a heart attack in 1962 and a second one the next year, which forced him to disband the Playboys although Wills continued to perform solo. 
The Country Music Hall of Fame inducted Wills in 1968 and the Texas State Legislature honored him for his contribution to American music. In 1972, Wills accepted a citation from the American Society of Composers, Authors and Publishers in Nashville. He was recording an album with fan Merle Haggard in 1973 when a stroke left him comatose until his death in 1975. The Rock and Roll Hall of Fame inducted Wills and the Texas Playboys in 1999.
Biography.
Early years.
He was born on a farm near Kosse, Texas, in Limestone County near Groesbeck, to Emma Lee Foley and John Tompkins Wills. His father was a statewide champion fiddle player and the Wills family was either playing music, or someone was "always wanting us to play for them", in addition to raising cotton on their farm.
In addition to picking cotton, the young Jim Bob learned to play the fiddle and the mandolin. Both a sister and several brothers played musical instruments, while another sister played piano. The Wills family frequently held country dances in their home, and there was dancing in all four rooms. While living in Hall County, Texas, they also played at 'ranch dances' which were popular in both North Texas and eastern New Mexico.
Wills not only learned traditional music from his family, he learned some Negro songs directly from African Americans in the cotton fields near Lakeview, Texas and said that he did not play with many white children other than his siblings, until he was seven or eight years old. African Americans were his playmates, and his father enjoyed watching him jig dance with black children.
"I don't know whether they made them up as they moved down the cotton rows or not," Wills once told Charles Townsend, author of "San Antonio Rose: The Life And Times Of Bob Wills", "but they sang blues you never heard before."
New Mexico and Texas.
The family moved to Hall County in the Texas Panhandle in 1913, and in 1919 they bought a farm between the towns of Lakeview and Turkey. At the age of 16, Wills left the family and hopped a freight train. "Jim Rob", as he became known, drifted for several years, traveling from town to town to try to earn a living, at one point almost losing his life when he nearly fell from a moving train, and later being chased by railroad police. In his 20s he attended barber school, got married, and moved first to Roy, New Mexico, then returned to Turkey in Hall County (now considered his home town) to work as a barber at Hamm's Barber Shop. He alternated barbering and fiddling even when he moved to Fort Worth after leaving Hall County in 1929. There he played in minstrel and medicine shows, and, as with other Texas musicians such as Ocie Stockard, continued to earn money as a barber. He wore blackface makeup to appear in comedy routines, something that was common at the time. "He was playing his violin and singing." There were two guitars and a banjo player with him. "Bob was in blackface and was the comic; he cracked jokes, sang, and did an amazing jig dance." Since there was already a "Jim" on the show, the manager began calling him "Bob". However, it was as "Jim Rob Wills", paired with Herman Arnspiger, that he made his first commercial (though unissued) recordings in November 1929 for Brunswick/Vocalion.
Wills was known for his hollering and wisecracking. One source for this was when, as a very young boy, he would hear his father, grandfather, and cowboys give out loud cries when the music moved them. When asked if his wisecracking and talking on the bandstand came from his medicine show experience, he said it did not. Rather, he said that it came directly from playing and living close to Negroes, and that he never did it necessarily as show, but more as a way to express his feelings.
While in Fort Worth, Wills added the "rowdy city blues" of Bessie Smith and Emmett Miller to a repertoire of mainly waltzes and breakdowns he had learned from his father, and patterned his vocal style after that of Miller and other performers such as Al Bernard. Wills acknowledged that he idolized Miller. Furthermore, his 1935 version of "St. Louis Blues" is nearly a word-for-word copy of Al Bernard's patter on his 1928 recording of the same song.
The fact that Wills made his professional debut in blackface was commented on by Wills' daughter, Rosetta: "He had a lot of respect for the musicians and music of his black friends," Rosetta is quoted as saying on the Bob Wills and the Texas Playboys Web site. She remembers that her father was such a fan of Bessie Smith stating, "He once rode fifty miles on horseback just to see her perform live." (Wills is quoted as saying, "I rode horeseback from the place between the rivers to Childress to see Bessie Smith ... She was about the greatest thing I had ever heard. In fact, there was no doubt about it. She "was" the greatest thing I ever heard."
In Fort Worth, Wills met Herman Arnspinger and formed The Wills Fiddle Band. In 1930 Milton Brown joined the group as lead vocalist and brought a sense of innovation and experimentation to the band, now called the Light Crust Doughboys due to radio sponsorship by the makers of Light Crust Flour. Brown left the band in 1932 to form the Musical Brownies, the first true Western swing band. Brown added twin fiddles, tenor banjo and slap bass, pointing the music in the direction of swing, which they played on local radio and at dancehalls.
Wills remained with the Doughboys and replaced Brown with new singer Tommy Duncan in 1932. He found himself unable to get along with future Texas Governor W. Lee "Pappy" O'Daniel, the authoritarian host of the Light Crust Doughboy radio show. O'Daniel had parlayed the show's popularity into growing power within Light Crust Flour's parent company, Burrus Mill and Elevator Company, and wound up as General Manager, though he despised what he considered "hillbilly music". Wills and Duncan left the Doughboys in 1933 after Wills had missed one show too many due to his sporadic drinking.
Wills recalled the early days of what became known as Western swing music in a 1949 interview. "Here's the way I figure it. We sure not tryin' to take credit for swingin' it." Speaking of Milt Brown and himself working with songs done by Jimmie Davis, the Skillet Lickers, Jimmie Rodgers, and others, and songs he'd learned from his father, he said that "We'd pull these tunes down an set 'em in a dance category. It wouldn't be a runaway, and just lay a real nice beat behind it an the people would get to really like it. It was nobody intended to start anything in the world. We was just tryin' to find enough tunes to keep 'em dancin' to not have to repeat so much."
Wills is also quoted as saying, "You can change the name of an old song, rearrange it and make it a swing." "One Star Rag," "Rat Cheese Under The Hill," "Take Me Back To Tulsa," "Basin Street Blues," "Steel Guitar Rag," and "Trouble In Mind" were some of the songs in his extensive repertory.
The Texas Playboys.
After forming a new band, The Playboys, and relocating to Waco, Wills found enough popularity there to decide on a bigger market. They left Waco in January of 1934 for Oklahoma City. Wills soon settled the renamed Texas Playboys in Tulsa, Oklahoma, and began broadcasting noontime shows over the 50,000 watt KVOO radio station. Their 12:30-1:15 p.m. Monday–Friday broadcasts became a veritable institution in the region.
Nearly all of the daily (except Sunday) shows originated from the stage of Cain's Ballroom. In addition, they played dances in the evenings, including regular ones at the ballroom on Thursdays and Saturdays.
Wills added a trumpet to the band inadvertently when he hired Everet Stover as an announcer, not knowing that he had played with the New Orleans symphony and had directed the governor's band in Austin. Stover, thinking he had been hired as a trumpeter, began playing with the band with no comment from Wills. Young sax player Zeb McNally was allowed to play with the band, although Wills initially discouraged it. With two horns in the band, Wills realized he would have to add a drummer to balance things and create a fuller sound. He hired the young, "modern style musician" Smokey Ducas.
By 1935, Wills had added horn and reed players as well as drums to the Playboys. The addition of steel guitar whiz Leon McAuliffe in March 1935 added not only a formidable instrumentalist but a second engaging vocalist. Wills himself largely sang blues and sentimental ballads. Wills and the Texas Playboys did their first recordings on September 23–25, 1935 in Dallas, Texas, being produced by Don Law and Art Satherley of the American Record Corporation. There is strong evidence that the 1935 sessions took place at 508 Park Avenue along with sessions in 1937 and 1938.
With its jazz sophistication, pop music and blues influence, plus improvised scat and wisecrack commentary by Wills, the band became the first superstars of the genre. Milton Brown's death in 1936 had cleared the way for the Playboys.
Session rosters from 1938 show both "lead guitar" and "electric guitar" in addition to guitar and steel guitar in the Texas Playboys recordings. Wills' 1938 recording of "Ida Red" served as a model for Chuck Berry's decades later version of the same song - "Maybellene".
About this time, Wills purchased and performed with an old Guadagnini violin that had once fetched $7,600 for $1,600, the equivalent of about $24,000 in 2009.
In 1940, "New San Antonio Rose" sold a million records and became the signature song of The Texas Playboys. The song's title referred to the fact that Wills had recorded it as a fiddle instrumental in 1938 as "San Antonio Rose". By then, the Texas Playboys were virtually two bands: one a fiddle-guitar-steel band with rhythm section and the second a first-rate big band able to play the day's swing and pop hits as well as Dixieland.
The "front line" of Wills' orchestra consisted of either fiddles or guitars after 1944.
Film career.
In 1940, Wills, along with the Texas Playboys, co-starred with Tex Ritter in "Take Me Back To Oklahoma". Other films would follow. In December 1942, after several band members had left the group, and as World War II raged, Wills joined the Army at the age of 37, but he received a medical discharge in 1943.
Wills also appeared in "The Lone Prairie" (1942), "Riders Of The Northwest Mounted" (1943), "Saddles And Sagebrush" (1943), "The Vigilantes Ride" (1943), "The Last Horseman" (1944), "Rhythm Round-Up" (1945), "Blazing The Western Trail" (1945), and "Lawless Empire" (1945). According to one source, he appeared in a total of 19 films.
Swing era.
After leaving the Army in 1943, Wills moved to Hollywood, moving into a rented house in September, and began to reorganize the Texas Playboys. He became an enormous draw in Los Angeles, where many of his Texas, Oklahoma and regional fans had also relocated during the Great Depression and World War II in search of jobs. Monday through Friday, the band broadcast from 12:01 to 1:00 p.m. PT over KMTR-AM (now KLAC) in Los Angeles. They also played regularly every Friday, Saturday, and Sunday night at the Mission Beach Ballroom in San Diego.
He commanded enormous fees playing dances there, and began to make more creative use of electric guitars to replace the big horn sections the Tulsa band had boasted. For a very brief period in 1944, the Wills band included 23 members, and around mid-year he toured Northern California and the Pacific Northwest with 21 pieces in the orchestra. "Billboard" reported that Wills out-grossed Harry James, Benny Goodman, "both Dorsies, et al." at Civic Auditorium in Oakland, California, in January 1944.
Wills and His Texas Playboys began their first cross-country tour in November 1944, and appeared at the Grand Ole Opry on December 30, 1944. According to the Opry, drums and horns were not considered to be part of country music. Wills' band at the time consisted of two fiddlers, two bass fiddles, two electric guitars, an amplified electric steel guitar, and a trumpet, as well as the noted drums, which belonged to Wills' then drummer, who played in the Dixieland style.
In 1945, Wills' dances were outdrawing those of Tommy Dorsey and Benny Goodman, and he moved to Fresno, California. Then in 1947 he opened the Wills Point nightclub in Sacramento and continued touring the Southwest and Pacific Northwest from Texas to Washington State. While based in Sacramento, his radio broadcasts over 50,000-watt KFBK were heard all over the West.
Famous swing orchestras in California realized that many of their followers were leaving to dance to Bob Will's Western swing. Because he was in such demand, some places booked Wills any time he had an opening, regardless of how undesirable the date. The manager of a popular auditorium in the LA Basin town of Wilmington, California: "Although Monday night dancing is frankly an experiment it was the only night of the week on which this outstanding band could be secured."
During the postwar period, KGO radio in San Francisco syndicated a Bob Wills and His Texas Playboys show recorded at the Fairmont Hotel. Many of these recordings survive today as the Tiffany Transcriptions and are available on CD. They show off the band's strengths significantly, in part because the group was not confined to the three-minute limits of 78 RPM discs. They featured superb instrumental work from fiddlers Joe Holley and Louis Tierney, steel guitarists Noel Boggs and Herb Remington, guitarists Eldon Shamblin and Junior Barnard and electric mandolinist-fiddler Tiny Moore. The original recorded version of Wills' "Faded Love" appeared on the Tiffanys as a fairly swinging instrumental unlike the ballad it became when lyrics were added in 1950.
On April 3, 1948, Wills and the Texas Playboys appeared for the inaugural broadcast of the "Louisiana Hayride" on KWKH, broadcasting from the Municipal Auditorium in Shreveport, Louisiana.
Wills and the Texas Playboys played dances throughout the West to more than 10,000 people every week. They held dance attendance records at Jantzen Beach in Portland, Oregon; in Santa Monica, California, and at the Oakland (California) Auditorium, where they drew 19,000 people in two nights. Wills also broke an attendance record of 2,100 previously held by Jan Garber at the Armory in Klamath Falls, Oregon, by attracting 2,514 dancers. Wills and the Playboys also played small towns on the West Coast. Actor Clint Eastwood recalled seeing Wills when he was 18 or 19 (1948 or 1949) and working at a pulp mill in Springfield, Oregon.
Appearances at the Bostonia Ballroom in San Diego continued throughout the 1950s.
Still a binge drinker, Wills became increasingly unreliable in the late 1940s, causing a rift with Tommy Duncan (who bore the brunt of audience anger when Wills's binges prevented him from appearing). It ended when he fired Duncan in the fall of 1948.
Later years.
Having lived a lavish lifestyle in California, Wills moved back to Oklahoma City in 1949, then went back on the road to maintain his payroll and Wills Point. He opened a second club, the Bob Wills Ranch House in Dallas, Texas. Turning the club over to managers later revealed to be dishonest left Wills in desperate financial straits with heavy debts to the IRS for back taxes that caused him to sell many assets including, mistakenly, the rights to "New San Antonio Rose". It wrecked him financially.
In 1950, Wills had two Top 10 hits, "Ida Red Likes The Boogie" and "Faded Love". After 1950, radio stations began to increasingly specialize in one form or another of commercially popular music. Wills did not fit into the popular Nashville country and western stations, although he was usually labeled "country and western". Neither did he fit into the pop or middle of the road stations, although he played a good deal of pop music, and was not accepted in the pop music world.
He continued to tour and record through the 1950s into the early 1960s, despite the fact that Western swing's popularity, even in the Southwest, had greatly diminished. Bob could draw "a thousand people on Monday night between 1950 and 1952, but he could not do that by 1956. Entertainment habits had changed."
On Wills' return to Tulsa late in 1957, Jim Downing of the "Tulsa Tribune" wrote an article headlined "Wills Brothers Together Again: Bob Back With Heavy Beat". The article quotes Wills as saying, "Rock and Roll? Why, man, that's the same kind of music we've been playin' since 1928! ... We didn't call it rock and roll back when we introduced it as our style back in 1928, and we don't call it rock and roll the way we play it now. But it's just basic rhythm and has gone by a lot of different names in my time. It's the same, whether you just follow a drum beat like in Africa or surround it with a lot of instruments. The rhythm's what's important." The use of amplified guitars accentuates Wills's claim; some Bob Wills recordings from the 1930s and 1940s sound similar to rock and roll records of the 1950s.
Even a 1958 return to KVOO, where his younger brother Johnnie Lee Wills had maintained the family's presence, did not produce the success he hoped for. He appeared twice on ABC-TV's "Jubilee USA" and kept the band on the road into the 1960s. After two heart attacks, in 1965 he dissolved the Texas Playboys (who briefly continued as an independent unit) to perform solo with house bands. While he did well in Las Vegas and other areas, and made records for the Kapp Records label, he was largely a forgotten figure—even though inducted into the Country Music Hall of Fame in 1968. A 1969 stroke left his right side paralyzed, ending his active career.
The 26 May 1975 issue of "TIME" (Milestones section) read: "Died. Bob Wills, 70, "Western Swing" bandleader-composer; of pneumonia; in Fort Worth. Wills turned out dance tunes that are now called country rock, introducing with his Texas Playboys such C & W classics as Take Me Back to Tulsa and New San Antonio Rose".
Legacy.
Wills' style influenced performers Buck Owens and Merle Haggard and helped to spawn a style of music now known as the Bakersfield Sound. (Bakersfield, California was one of Wills' regular stops in his heyday). A 1970 tribute album by Haggard directed a wider audience to Wills' music, as did the appearance of younger "revival" bands like Asleep at the Wheel and the growing popularity of longtime Wills disciple and fan Willie Nelson. By 1971, Wills recovered sufficiently to travel occasionally and appear at tribute concerts. In 1973 he participated in a final reunion session with members of some the Texas Playboys from the 1930s to the 1960s. Merle Haggard was invited to play at this reunion. The session, scheduled for two days, took place in December 1973, with the album to be titled "For The Last Time". Wills, speaking or attempting to holler, appeared on a couple tracks from the first day's session but suffered a stroke overnight. He had a more severe one a few days later. The musicians completed the album without him. Wills by then was comatose. He lingered until his death on May 13, 1975.
In addition to being inducted into the Country Music Hall of Fame in 1968, Wills was inducted into the Nashville Songwriters Hall of Fame in 1970, the Rock and Roll Hall of Fame in the Early Influence category along with the Texas Playboys in 1999, and received the Grammy Lifetime Achievement Award in 2007.
From the 1970s until his 2002 death, Waylon Jennings performed a song called "Bob Wills Is Still The King". In addition, The Rolling Stones performed this song live in Austin, Texas at Zilker Park for their DVD "The Biggest Bang". In a 1968 issue of "Guitar Player", rock guitarist Jimi Hendrix said of Wills and the Playboys: "I dig them. The Grand Ole Opry used to come on, and I used to watch that. They used to have some pretty heavy cats, some heavy guitar players."
Wills ranked #27 in "CMT's 40 Greatest Men In Country Music" in 2003.
Fats Domino once remarked that he patterned his 1960 rhythm section after that of Bob Wills.
During the 49th Grammy Awards, Carrie Underwood performed his song "San Antonio Rose". Today, George Strait performs Wills' music on concert tours and also records songs influenced by Wills and his Texas-style swing.
The Austin-based Western swing band Asleep at the Wheel have honored Wills' music since the band's inception, mostly notably with their continuing performances of the musical drama "A Ride With Bob", which debuted in Austin in March 2005 to coincide with celebrations of Wills' 100th birthday.
The Bob Wills Birthday Celebration is held every year in March at the Cain's Ballroom in Tulsa, Oklahoma with a Western swing concert and dance.
In 2004, a documentary film about his life and music, entitled "Fiddlin' Man: The Life And Times Of Bob Wills", was released by VIEW Inc.
In 2011, Proper Records released an album by Hot Club of Cowtown titled "What Makes Bob Holler: A Tribute To Bob Wills And His Texas Playboys".
In 2011, the Texas Legislature adopted a resolution designating western swing as the official "State Music Of Texas".
On February 9, 2014, the 80th Anniversary of Bob Wills' first performance at the Cain's Ballroom in Tulsa, Oklahoma, the Oklahoma Historical Society and the Oklahoma Museum of Popular Culture (OKPOP) announced plans to create a feature-length documentary about the life and music of Bob Wills. The documentary will be titled "Still the King. Bob Wills: The Man. The Music." 

</doc>
<doc id="4834" url="http://en.wikipedia.org/wiki?curid=4834" title="Badtrans">
Badtrans

BadTrans is a malicious Microsoft Windows computer worm distributed by e-mail. Because of a known vulnerability in older versions of Internet Explorer, some e-mail programs, such as Microsoft's Outlook Express and Microsoft Outlook programs, may install and execute the worm as soon as the e-mail message is viewed.
Once executed, the worm replicates by sending copies of itself to other e-mail addresses found on the host's machine, and installs a keystroke logger, which then captures everything typed on the affected computer. Badtrans then transmits the data to one of several e-mail addresses. 
Among the e-mail addresses that received the keyloggers were free addresses at Excite, Yahoo, and IJustGotFired.com. IJustGotFired is a free service of MonkeyBrains, a San Francisco-based Internet service provider.
The target address at IJustGotFired began receiving e-mails at 3:23pm on November 24, 2001. Once the account exceeded its quotas, it was automatically disabled, but the messages were still saved as they arrived. The address received over 100,000 keylogs in the first day alone.
In mid-December, the FBI contacted Rudy Rucker, Jr., owner of MonkeyBrains, and requested a copy of the keylogged data. All of that data was stolen from the victims of the worm; it includes no information about the creator of Badtrans.
Instead of complying with the FBI request, MonkeyBrains published a database website http://badtrans.monkeybrains.net for the public to determine if a given address has been compromised. The database does not reveal the actual passwords or keylogged data.

</doc>
<doc id="4836" url="http://en.wikipedia.org/wiki?curid=4836" title="Barış Manço">
Barış Manço

Mehmet Barış Manço (born Tosun Yusuf Mehmet Barış Manço; 2 January 1943 – 1 February 1999), known by his stage name Barış Manço, was a Turkish rock musician, singer, songwriter, composer, and television producer. Beginning his musical career while attending Galatasaray High School, he was a pioneer of rock music in Turkey and one of the founders of the Anatolian rock genre. Manço composed around 200 songs and is among the best-selling and most awarded Turkish artists to date. Many of his songs were translated into a variety of languages including English, French, Japanese, Greek, Italian, Bulgarian, Romanian, Persian, Hebrew, Urdu, Arabic, and German, among others. Through his television program, "7'den 77'ye" ("From 7 to 77"), Manço traveled the world and visited most countries on the globe. He remains one of the most popular public figures of Turkey.
Early life and career.
Barış Manço was born in Üsküdar, Istanbul, Turkey on 2 January 1943. His mother, Rikkat Uyanık, was a famous singer in the early 1940s. His older brother, who was born during World War II, was named Savaş ("war" in Turkish) while he was named Barış ("peace" in Turkish) by his parents to celebrate the end of the war. At birth, he was additionally named Tosun Yusuf after his deceased uncle Yusuf called Tosun (literally: Yusuf The Sturdy). However, this name was erased just before he attended the primary school.
In primary school his head was shaven to prevent head lice, a serious threat back then, which he cited among reasons for his later signature long hair.
During his highschool days in Galatasaray High School (and later in Şişli Terakki High School) he formed his first band, Kafadarlar ("The Buddies"), allegedly upon seeing Erkin Koray's band performing, all students of Deutsche Schule Istanbul ("İstanbul Alman Lisesi"), a nearby highschool. Asaf Savaş Akat, a famous economist in Turkey, played saxophone, and guitarist Ender Enön made his own guitar because it was difficult to find a real one on the market in those years.
In 1962 and 1963, with his next band, Harmoniler ("The Harmonies"), he recorded cover versions of some of popular American twist songs and rearrangements of Turkish folk songs in rock and roll form, marking the beginning of the Anatolian rock movement, a synthesis of Turkish folk music and rock. In this period, his key visual and musical influence was Elvis Presley.
After graduating from high school in 1963, he moved to Europe, travelling around Paris and Liège, where he formed bands with local musicians and recorded some singles mainly in English and in French but also in Turkish. He toured with his band Les Mistigris (not related to Mistigris) in Germany, Belgium, France and Turkey until 1967.
In 1967, he suffered a serious car accident, after which he started to grow his signature mustache to disguise his scar.
Frustrated by the difficulties of working with musicians from different nationalities, he formed Kaygısızlar (The Carefrees), featuring Mazhar Alanson and Fuat Güner, future members of the band MFÖ. He recorded several singles and toured with the band, both domestically and internationally, until the band members revealed that they did not want to live abroad.
In 1970, he formed Barış Manço Ve ... ("Barış Manço and ...") again with foreign musicians, to record his first hit single, both in Turkey and in Belgium, "Dağlar Dağlar" (Mountains, Mountains!), selling over 700,000 copies. Today, the song remains one of his most popular works.
1970s.
After the success of "Dağlar Dağlar", Manço recorded a couple of singles with Moğollar (The Mongols), another influential Turkish Anatolian rock band. He then decided to return to Turkey where he recorded with the reformed Kaygısızlar for a short period. In 1971, his early works were compiled under his first full length album "Dünden Bugüne", today commonly referred as "Dağlar Dağlar".
In 1972, he formed Kurtalan Ekspres, a legend by itself, the band that would accompany him until his death. In 1975 until when he continued to release singles, he released his first non-compilation LP "2023", a concept album that includes many instrumental songs.
As a last attempt to reach international success, he released the LP titled "Baris Mancho" (1976), a strange transcription of his name, mostly with George Hayes Orchestra under CBS Records label, in Europe and South Africa. Although the album did not bring the fame he was expecting, it did reach the top of the charts in Romania and Morocco. Next year, the album was released in Turkey under the title "Nick the Chopper".
From 1977 to 1980, he released three more albums in Turkey, partly consisting of compilations of older singles, namely "Sakla Samanı Gelir Zamanı" (1977), "Yeni Bir Gün" (1979) and "20. Sanat Yılı Disko Manço" (1980), all following a similar sound with "2023". All these albums are now rarity items, but most of the material from the era are available in later compilations "Ben Bilirim" and "Sarı Çizmeli Mehmet Ağa".
1980s.
In 1981, Manço released "Sözüm Meclisten Dışarı" with Kurtalan Ekspres, containing many hit songs including "Alla Beni Pulla Beni", "Arkadaşım Eşek", "Gülpembe", "Halhal" and "Dönence" among others. The album remains as one of their most popular works and launched a boost of popularity for Barış Manço during the 1980s.
"Arkadaşım Eşek" ("My Friend Donkey"), quickly grew very popular among children (the song is about rural nostalgia and was not initially intended as a children's song). Throughout his career, he went on to write many other songs primarily for children to achieve an iconic acceptance among Turkish children of the 1980s and 1990s.
On the other hand "Gülpembe", composed by Kurtalan Ekspres bassist Ahmet Güvenç, a requiem for Manço's grandmother, caught older audiences and probably is the artist's most popular song, competing perhaps only with "Dağlar Dağlar".
In 1983, "Estağfurullah, Ne Haddimize" was released. It contained hit songs "Halil İbrahim Sofrası" and "Kol Düğmeleri", a new version of the artist's first song. "Halil İbrahim Sofrası" exemplified Manço's signature moral themed lyrics, a rare feature in Turkish pop music.
In 1985, "24 Ayar Manço" which included "Gibi Gibi" and a long conceptual song "Lahburger" was released. It also marked the beginning of the shift in Manço's sound characterized with the heavy use of synthesizers and drum machine in contrast with his older works consisting of a group oriented rock based sound. In subsequent years, Manço released "Değmesin Yağlı Boya" (1986), "Sahibinden İhtiyaçtan" (1988) and "Darısı Başınıza" (1989), all containing a couple of hit songs and demonstrating his new sound.
"7'den 77'ye" and 1990s.
In 1988, "7'den 77'ye" ("From 7 to 77"), a TV show directed and presented by Manço, began to run on TRT 1, the Turkish state television channel. It was a combined music, talk show, and documentary program which was a major hit during the eight years it stayed on air. Manço traveled to almost 150 countries for the show. "Adam Olacak Çocuk", a part of the show, strengthened Manço's acceptance among children.
Although his popularity continued mostly due to the TV show, his musical works in the 1990s were not well received. The albums "Mega Manço" (1992) and "Müsadenizle Çocuklar" (1995) were considered as the weakest efforts of his career, despite the limited success of 1992 children hit "Ayı" (The Bear). On the other hand, in 1995 he toured in Japan with Kurtalan Ekspres, leading to "Live In Japan" (1996), his only live album. He released two albums in that country with some recognition as "the man who writes songs about vegetables", referring to "Domates, Biber, Patlıcan" ("Tomato, Pepper, Aubergine") and "Nane, Limon Kabuğu" (Mint, Lemon Rind), two of his hit songs from the 1980s.
Death.
On 31 January 1999, Barış Manço died of a sudden heart attack before the release of his just finished last work "Mançoloji" ("Mançology" or "Manchology") (1999), a double album containing the new recordings of his hit songs along with an unfinished instrumental song "40. Yıl" ("The 40th Anniversary"), celebrating his 40th year in music. His sudden death caused an almost unanimous shock in Turkey with millions of people mourning and tens of thousands of people attending his funeral.
Legacy.
Barış Manço was one of the most influential Turkish musicians. In his early career he and his bands contributed to the Turkish rock movement by combining traditional Turkish music with rock influences, which is still one of the main trends of Turkish popular music.
His visual image, characterized by his long hair, mustache and big rings, softened the reaction of the otherwise conservative Turkish public opinion.
Manço pioneered the progressive rock-influenced Anatolian rock movement in the 1970s. His experimentation with electronic instruments in the late 1980s contributed to the 1990s sound of Turkish popular music.
His lyrics with diverse themes, mostly following a somewhat modernized version of the "aşık" (wandering folk poets) tradition were heavily marginal in the popular music scene of the 1980s which was mostly dominated by love-themed lyrics.
In 2002, a tribute album was released under the name "Yüreğimdeki Barış Şarkıları" ("Songs of Barış In My Heart"), featuring 15 extremely popular Turkish artists of such diverse genres as arabesque, pop and rock (both Anatolian and western style) demonstrating his wide range of influence.
Discography.
Singles.
With Harmoniler
With Jacques Denjean Orchestra
With Les Mistigris
With Kaygısızlar
With "Barış Manço Ve'
With Moğollar
With Moğollar / Kaygısızlar
With Kaygısızlar / Les Mistigris
With Kurtalan Ekspres
With George Hayes Orchestra / Kurtalan Ekspres
With Kurtalan Ekspres

</doc>
<doc id="4840" url="http://en.wikipedia.org/wiki?curid=4840" title="Blitz BASIC">
Blitz BASIC

Blitz BASIC refers to the programming language dialect that was interpreted by the first Blitz compilers, devised by New Zealand-based developer Mark Sibly. Being derived from BASIC, Blitz syntax was designed to be easy to pick up for beginners first learning to program. The languages are game-programming oriented but are often found general-purpose enough to be used for most types of application. The Blitz language evolved as new products were released, with recent incarnations offering support for more advanced programming techniques such as object-orientation and multi-threading. This led to the languages losing their BASIC moniker in later years. 
History.
The first iteration of the Blitz language was created for the Amiga platform and published by the Australian firm Memory and Storage Technology. Returning to New Zealand, Blitz2 was published several years later by Acid Software (a local Amiga game publisher). Since then, Blitz compilers have been released on several platforms.
BlitzBasic.
Idigicon published BlitzBasic for Microsoft Windows in October 2000. The language included a built-in API for performing basic 2D graphics and audio operations. Following the release of Blitz3D, BlitzBasic is often synonymously referred to as Blitz2D.
Recognition of BlitzBasic increased when a limited range of "free" versions were distributed in popular UK computer magazines such as PC Format. This resulted in a legal dispute between the developer and publisher which was eventually resolved amicably.
Versions.
Blitz3D.
Blitz3D was released for Microsoft Windows in September 2001, competing with other similar PC game-development languages of the time (such as Dark Basic). Blitz3D extended BlitzBasic's command-set with the inclusion of an API for a DirectX 7-based 3D engine.
Although originally Blitz3D's distribution rights were owned by Idigicon, Blitz Research Ltd. later signed a deal with the firm so as to allow Blitz Research Ltd. to distribute Blitz3D themselves. In return, Idigicon were granted full rights to distribute BlitzBasic and to clear any outstanding stock copies of Blitz3D.
Blitz3D was released as Open Source on the 4th August 2014.
BlitzPlus.
In February 2003, Blitz Research Ltd. released BlitzPlus also for Microsoft Windows. It lacked the 3D engine of Blitz3D, but did bring new features to the 2D side of the language by implementing limited Microsoft Windows control support for creating native GUIs. Backwards compatibility of the 2D engine was also extended, allowing compiled BlitzPlus games and applications to run on systems that might only have DirectX 1.
BlitzMax.
The first BlitzMax compiler was released in December 2004 for Mac OS X. This made it the first Blitz dialect that could be compiled on *nix platforms. Compilers for Microsoft Windows and Linux were subsequently released in May 2005. BlitzMax brought the largest change of language structure to the modern range of Blitz products by extending the type system to include object-oriented concepts and modifying the graphics API to better suit OpenGL. BlitzMax was also the first of the Blitz languages to represent strings internally using UCS2, allowing native-support for string literals composed of non-ASCII characters.
BlitzMax's platform-agnostic command-set allows developers to compile and run source code on multiple platforms. However the official compiler and build chain will only generate binaries for the platform that it is executing on. Unofficially, users have been able to get Linux and Mac OS X to cross-compile to the Windows platform.
BlitzMax is also the first modular version of the Blitz languages, improving the extensibility of the command-set. In addition, all of the standard modules shipped with the compiler are open-source and so can be tweaked and recompiled by the programmer if necessary. The official BlitzMax cross-platform GUI module (known as MaxGUI) allows developers to write GUI interfaces for their applications on Linux (FLTK), Mac (Cocoa) and Windows. Various user-contributed modules extend the use of the language by wrapping such libraries as wxWidgets, Cairo, and Fontconfig as well as a selection of database modules. There are also a selection of third-party 3D modules available namely MiniB3D - an open-source OpenGL engine which can be compiled and used on all three of BlitzMax's supported platforms.
In October 2007, BlitzMax 1.26 was released which included the addition of a reflection module. BlitzMax 1.32 shipped new threading and Lua scripting modules and most of the standard library functions have been updated so that they are unicode friendly.
Blitz3D SDK.
Blitz3D SDK is a 3D graphics engine based on the engine in Blitz3D. It was marketed for use with C++, C#, BlitzMax and PureBasic, however it could also be used with other languages that follow compatible calling conventions. As of January 2011, Blitz3D SDK is no longer listed for sale on the official Blitz website.
Max3D module.
In 2008, the source code to Max3D - a C++-based cross-platform 3D engine - was released under a BSD license. This engine focused on OpenGL but had an abstract backend for other graphics drivers (such as DirectX) and made use of several open-source libraries, namely Assimp, Boost and ODE.
Despite the excitement in the Blitz community of Max3D being the eagerly awaited successor to Blitz3D, interest and support died off soon after the source code was released and eventually development came to a halt. There is no indication that Blitz Research will pick up the project again.
Monkey and Mojo.
In 2011, BRL released a new cross-platform programming language called Monkey and its first official module called Mojo. Monkey has a similar syntax to BlitzMax, but instead of compiling direct to assembly code, it translates Monkey source files directly into source code for a chosen language, framework or platform e.g. Windows, Mac OS X, iOS, Android, HTML5, and Flash.
Sample code.
The following code creates a windowed application that shows the current time in binary and decimal format. This code is written in BlitzBasic, but will compile and run in both Blitz3D and BlitzPlus. See below for the same example written in BlitzMax.
BlitzMax version of the above clock:

</doc>
<doc id="4842" url="http://en.wikipedia.org/wiki?curid=4842" title="Bliss bibliographic classification">
Bliss bibliographic classification

The Bliss bibliographic classification (BC) is a library classification system that was created by Henry E. Bliss (1870–1955) and published in four volumes between 1940 and 1953. Although originally devised in the United States, it was more commonly adopted by British libraries. A second edition of the system (BC2) has been developed in Britain since 1977.
Origins of the system.
Bliss was born in New York in 1870. In 1891, he began work in the library of the College of the City of New York (now City College of the City University of New York).
Bliss had a lifelong interest in the organization, structure and philosophy of knowledge and was very critical of the library classification systems that were available to him. He believed that because the popular Library of Congress system had been designed for a specific library (the Library of Congress) it had no use as a standard system outside that library. He also greatly disliked the Dewey Decimal system.
Bliss wanted a classification system that would provide distinct rules yet still be adaptable to whatever kind of collection a library might have, as different libraries have different needs. His solution was the concept of “alternative location,” in which a particular subject could be put in more than one place, as long as the library made a specific choice and used it consistently.
In 1908 Bliss reclassified 60,000 of his library’s books, and in 1910 he published an article with a rough scheme of his general ideas. But as he continued to develop his system he realized that it was going to be a much larger project than he had anticipated. The first of his four volumes appeared in 1940 (the year he retired) and the last in 1953, two years before his death.
Some of the underlying policies of the BC system were:
Examples.
Bliss deliberately avoided the use of the decimal point because of his objection to Dewey's system. Instead he used capital and lower-case letters, numerals, and every typographical symbol available on his extensive and somewhat eccentric typewriter.
In the revised edition (BC2), only capital letters are used, with numerals occasionally used for special purposes. Here is an extract:
HJ        Preventive medicine<BR>
. . .<BR>
HL        Curative medicine<BR>
HLK           Primary care; general practice<BR>
HLY           Secondary care, aftercare
Adoption and change.
BC was not used by many North American libraries. The system was not without its flaws (the result of being largely a one-person project), and the layout of Bliss’s text was difficult to read. A few library schools sometimes taught the BC system to their students, but only in a minor way. The failure of the system to catch on in North America was partly because of its internal deficiencies but also because the Dewey Decimal and Library of Congress systems were already well established.
The City College library continued to use Bliss’s system until 1967, when it reluctantly switched to the Library of Congress system. It had become too expensive to train new staff members to use BC, and too expensive to maintain in general. Much of the Bliss stacks remain, however, as no-one has recatalogued the books.
The case was different, however, in Britain. BC proved more popular there and also spread to other English-speaking countries. Part of the reason for its success was that libraries in teachers’ colleges liked the way Bliss had organized the subject areas on teaching and education. By the mid-1950s the system was being used in at least sixty British libraries and in a hundred by the 1970s.
In 1967 the Bliss Classification Association was formed. Its first publication was the "Abridged Bliss Classification" (ABC), intended for school libraries. In 1977 it began to publish and maintain a much-improved, revised version of Bliss’s system, the Bliss Bibliographic Classification (Second Edition) or BC2. This retains only the broad outlines of Bliss's scheme, replacing most of the detailed notation with a new scheme based on the principles of faceted classification. 15 of approximately 28 volumes of schedules have so far been published.
Classifications.
The top level organisation is:

</doc>
<doc id="4845" url="http://en.wikipedia.org/wiki?curid=4845" title="Blood alcohol content">
Blood alcohol content

Blood alcohol content (BAC), also called blood alcohol concentration, blood ethanol concentration, or blood alcohol level is most commonly used as a metric of alcohol intoxication for legal or medical purposes.
Blood alcohol content is usually expressed as a percentage of alcohol (generally in the sense of ethanol) in the blood in units of mass of alcohol per volume of blood or mass of alcohol per mass of blood, depending on the country. For instance, in North America a BAC of 0.1 (0.1% or one tenth of one percent) means that there are 0.10 g of alcohol for every dL of blood.
Estimated blood ethanol concentration (EBAC).
To calculate estimated peak blood alcohol concentration (EBAC), a variation, including drinking period in hours, of the Widmark formula was used. The formula is:
where 0.806 is a constant for body water in the blood (mean 80.6%), SD is the number of standard drinks containing 10 grams of ethanol, 1.2 is a factor to convert the amount in grams to Swedish standards set by The Swedish National Institute of Public Health, BW is a body water constant (0.58 for men and 0.49 for women), Wt is body weight (kilogram), MR is the metabolism constant (0.017) and DP is the drinking period in hours. Regarding metabolism (MR) in the formula; Females demonstrated a higher average rate of elimination (mean, 0.017; range, 0.014-0.021 g/210 L) than males (mean, 0.015; range, 0.013-0.017 g/210 L). Female subjects on average had a higher percentage of body fat (mean, 26.0; range, 16.7-36.8%) than males (mean, 18.0; range, 10.2-25.3%). Additionally, men are, on average, heavier than women but it is not strictly accurate to say that the water content of a person alone is responsible for the dissolution of alcohol within the body, because alcohol does dissolve in fatty tissue as well. When it does, a certain amount of alcohol is temporarily taken out of the blood and briefly stored in the fat. For this reason, most calculations of alcohol to body mass simply use the weight of the individual, and not specifically his/her water content. Finally, it is speculated that the bubbles in sparkling wine may speed up alcohol intoxication by helping the alcohol to reach the bloodstream faster. A study conducted at the University of Surrey in the United Kingdom gave subjects equal amounts of flat and sparkling Champagne which contained the same levels of alcohol. After 5 minutes following consumption, the group that had the sparkling wine had 54 milligrams of alcohol in their blood while the group that had the same sparkling wine, only flat, had 39 milligrams.
Examples:
Binge drinking.
The National Institute on Alcohol Abuse and Alcoholism (NIAAA) define the term "binge drinking" as any time one reaches a peak BAC of 0.08% or higher as opposed to some (arguably) arbitrary number of drinks in an evening.
Units of measurement.
There are several different units in use around the world for defining blood alcohol concentration. Each is defined as either a mass of alcohol per volume of blood or a mass of alcohol per mass of blood (never a volume per volume). 1 milliliter of blood is approximately equivalent to 1.06 grams of blood. Because of this, units by volume are similar but not identical to units by mass. In the U.S. the concentration unit 1% w/v (percent mass/volume, equivalent to 10g/l or 1 g per 100 ml) is in use. This is not to be confused with the amount of alcohol measured on the breath, as with a breathalyzer. The amount of alcohol measured on the breath is generally accepted as proportional to the amount of alcohol present in the blood at a rate of 1:2100. Therefore, a breathalyzer measurement of 0.10 mg/L of breath alcohol converts to 0.021 g/210L of breath alcohol, or 0.021 g/dL of blood alcohol (the units of the BAC in the United States). While a variety of units (or sometimes lack thereof) is used throughout the world, many countries use the g/L unit, which do not create confusion as percentages do. Usual units are highlighted in the table below.
Legal limits.
For purposes of law enforcement, blood alcohol content is used to define intoxication and provides a rough measure of impairment. Although the degree of impairment may vary among individuals with the same blood alcohol content, it can be measured objectively and is therefore legally useful and difficult to contest in court. Most countries disallow operation of motor vehicles and heavy machinery above prescribed levels of blood alcohol content. Operation of boats and aircraft are also regulated.
The alcohol level at which a person is considered legally impaired varies by country. The list below gives limits by country. These are typically blood alcohol content limits for the operation of a vehicle.
It is illegal to have any measurable alcohol in the blood while driving in these countries. Most jurisdictions have a tolerance slightly higher than zero to account for false positives and naturally occurring alcohol in the body. Some of the following jurisdictions have a general prohibition of alcohol.
Limits by country (BrAC: Breath Alcohol Content).
In certain countries, alcohol limits are determined by the Breath Alcohol Content (BrAC), not to be confused with blood alcohol content (BAC).
Scientific definitions.
"0.01" Blood alcohol content is the hundredth decimal part of the one thousandth part of a liter. (Please note that this "0.01" is measured in permille and not percentage as the "0.1" example in introduction and numbers in 1 Effects at different levels.)
In digesting these numbers it must be remembered that one milliliter is the thousandth part of a liter.
Therefore 1% of a milliliter is 0.00001-Liter.
Expressing blood-alcohol concentration as "0.01" is naming the hundredth part of a thousandth part.
As final example, a blood-alcohol concentration of 0.08, being the 0.08 "part" of a milliliter (itself the thousandth part of a liter) therefore names an absolute blood-alcohol volume of 0.00008-liter (within every liter of blood).
Each country or state may define BAC differently. For example, the state of California in the United States legally defines BAC as a ratio of grams of alcohol per 100 milliliters of blood, which is equal to grams of alcohol per deciliter of blood.
Since measurement must be accurate and inexpensive, several measurement techniques are used as proxies to approximate the true parts per million measure. Some of the most common are listed here: (1) Mass of alcohol per volume of exhaled breath (for example, 0.38 mg/L; see also breath gas analysis), (2) Mass per volume of blood in the body (for example, 0.08 g/dL), and (3) Mass of alcohol per mass of the body (for example, 0.0013 g/Kg).
The number of alcoholic beverages (drinks) consumed is often a poor measure of blood alcohol content because of variations in sex, body weight, and body fat.
An ethanol level of 0.10% is equal to 22 mmol/l or 100 mg/dl of blood alcohol. This same 0.10% BAC also equates to 0.10 g/dL of blood alcohol or 0.10 g/210L of exhaled breath alcohol or 0.476 mg/L of exhaled breath alcohol. Likewise, 0.10 mg/L of exhaled breath alcohol converts to 0.02% BAC, 0.022 g/dL of blood alcohol or 0.022 g/210L of exhaled breath alcohol.
Test assumptions.
Blood alcohol tests assume the individual being tested is average in various ways. For example, on average the ratio of blood alcohol content to breath alcohol content (the "partition ratio") is 2100 to 1. In other words, there are 2100 parts of alcohol in the blood for every part in the breath. However, the actual ratio in any given individual can vary from 1300:1 to 3100:1, or even more widely. This ratio varies not only from person to person, but within one person from moment to moment. Thus a person with a true blood alcohol level of .08% but a partition ratio of 1700:1 at the time of testing would have a .10 reading on a Breathalyzer calibrated for the average 2100:1 ratio.
A similar assumption is made in urinalysis. When urine is analyzed for alcohol, the assumption is that there are 1.3 parts of alcohol in the urine for every 1 part in the blood, even though the actual ratio can vary greatly.
Breath alcohol testing further assumes that the test is "post-absorptive"—that is, that the absorption of alcohol in the subject's body is complete. If the subject is still actively absorbing alcohol, their body has not reached a state of "equilibrium" where the concentration of alcohol is uniform throughout the body. Most forensic alcohol experts reject test results during this period as the amounts of alcohol in the breath will not accurately reflect a true concentration in the blood.
Auto-brewery syndrome is a rare medical condition where the stomach produces brewers yeast that breaks down starches into ethanol; which enters the blood stream.
Metabolism and excretion.
Alcohol is absorbed throughout the gastrointestinal tract, but more slowly in the stomach than in the small or large intestine. For this reason, alcohol consumed with food is absorbed more slowly, because it spends a longer time in the stomach. Furthermore, alcohol dehydrogenase is present in the stomach lining. After absorption, the alcohol passes to the liver through the hepatic portal vein, where it undergoes a first pass of metabolism before entering the general bloodstream.
Alcohol is removed from the bloodstream by a combination of metabolism, excretion, and evaporation. The relative proportion disposed of in each way varies from person to person, but typically about 95% is metabolized by the liver. The remainder of the alcohol is eliminated through excretion in breath, urine, sweat, feces, milk and saliva. Excretion into urine typically begins after about 40 minutes, whereas metabolisation commences as soon as the alcohol is absorbed, and even before alcohol levels have risen in the brain.
Alcohol is metabolized mainly by the group of six enzymes collectively called alcohol dehydrogenase. These convert the ethanol into acetaldehyde (an intermediate that is actually more toxic than ethanol). The enzyme acetaldehyde dehydrogenase then converts the acetaldehyde into non-toxic Acetic acid.
Many physiologically active materials are removed from the bloodstream (whether by metabolism or excretion) at a rate proportional to the current concentration, so that they exhibit exponential decay with a characteristic halflife (see pharmacokinetics). This is not true for alcohol, however. Typical doses of alcohol actually saturate the enzymes' capacity, so that alcohol is removed from the bloodstream at an approximately constant rate. This rate varies considerably between individuals; Another sex based difference is in the elimination of alcohol. Persons below the age of 25, women and persons with liver disease may process alcohol more slowly. False High (BAC) readings are related to patients with proteinuria and hematuria, due to kidney-liver metabolism and failure (for example, Hematuria 1+ protenuria 1+ )
Such persons have impaired acetaldehyde dehydrogenase, which causes acetaldehyde levels to peak higher, producing more severe hangovers and other effects such as flushing and tachycardia. Conversely, members of certain ethnicities that traditionally did not use alcoholic beverages have lower levels of alcohol dehydrogenases and thus "sober up" very slowly, but reach lower aldehyde concentrations and have milder hangovers. Rate of detoxification of alcohol can also be slowed by certain drugs which interfere with the action of alcohol dehydrogenases, notably aspirin, furfural (which may be found in fusel alcohol), fumes of certain solvents, many heavy metals, and some pyrazole compounds. Also suspected of having this effect are cimetidine (Tagamet), ranitidine (Zantac), and acetaminophen (Tylenol) (paracetamol).
Currently, the only known substance that can increase the rate of metabolism of alcohol is fructose. The effect can vary significantly from person to person, but a 100g dose of fructose has been shown to increase alcohol metabolism by an average of 80%. Fructose also increases false positives of high BAC ratio readings in anyone with proteinuria and hematuria, due to kidney-liver metabolism.
Alcohol absorption can be slowed by ingesting alcohol on a full stomach. Spreading the total absorption of alcohol over a greater period of time decreases the maximum alcohol level, decreasing the hangover effect. Thus, drinking on a full stomach or drinking while ingesting drugs which slow the breakdown of ethanol into acetaldehyde will reduce the maximum blood levels of this substance and thus decrease the hangover.
Carbonated beverages.
Alcohol in carbonated beverages is absorbed faster than alcohol in non-carbonated drinks. Another study also confirmed this, conducted at the University of Surrey in the United Kingdom gave subjects equal amounts of flat and sparkling Champagne which contained the same levels of alcohol. After 5 minutes following consumption, the group that had the sparkling wine had 54 milligrams of alcohol in their blood while the group that had the same sparkling wine, only flat, had 39 milligrams.
Retrograde extrapolation.
Retrograde extrapolation is the mathematical process by which someone's blood alcohol concentration at the time of driving is estimated by projecting backwards from a later chemical test. This involves estimating the absorption and elimination of alcohol in the interim between driving and testing. The rate of elimination in the average person is commonly estimated at .015 to .020 grams per deciliter per hour (g/dl/h), although again this can vary from person to person and in a given person from one moment to another. Metabolism can be affected by numerous factors, including such things as body temperature, the type of alcoholic beverage consumed, and the amount and type of food consumed.
In an increasing number of states, laws have been enacted to facilitate this speculative task: the blood alcohol content at the time of driving is legally presumed to be the same as when later tested. There are usually time limits put on this presumption, commonly two or three hours, and the defendant is permitted to offer evidence to rebut this presumption.
Forward extrapolation can also be attempted. If the amount of alcohol consumed is known, along with such variables as the weight and sex of the subject and period and rate of consumption, the blood alcohol level can be estimated by extrapolating forward. Although subject to the same infirmities as retrograde extrapolation—guessing based upon averages and unknown variables—this can be relevant in estimating BAC when driving and/or corroborating or contradicting the results of a later chemical test.
Cases of high blood alcohol levels.
On August 25, 1997, 20-year old Benjamin Wynne, a student at Louisiana State University, died of alcohol poisoning after attending a Sigma Alpha Epsilon fraternity party at a bar near campus where the fraternity's president worked as a bartender. Wynne's toxicology report revealed a BAC of 0.588%. It was reported Wynne ingested 18 shots of Three Wise Men, a drink which is concocted by mixing Jaegermeister, Crown Royal and 151-proof rum.
On Monday March 26, 2012, a man was found in a ditch in the U.S. state of Indiana with a BAC of 0.552%.
In November 2007, a driver was found passed out in her car in Oregon in the United States. A blood test showed her blood alcohol level was 0.550%. She was charged with several offenses, including two counts of driving under the influence of an intoxicant, reckless endangerment of a person, criminal mischief and driving with a suspended license. Her bail was later set at US$50,000, since she had several previous convictions for similar offenses.
In December 2007, a driver was arrested in Klamath County, Oregon, after she was found unconscious in her car which was stuck in a snow bank with its engine running. Police were forced to break a car window to remove her. After realizing she was in an alcohol-induced coma, they rushed her to the hospital where a blood test showed her blood alcohol level was 0.720%. She reportedly was released from the hospital the next day. She was subsequently charged with drunk driving.
In July 2008, a driver was arrested after he ran into a highway message board on Interstate 95 in Providence, Rhode Island. A breath test showed his blood alcohol level was at 0.491% and he was raced to the hospital where he was sedated and placed in a detoxification unit. He was subsequently charged with driving while intoxicated and resisting arrest. He was later sentenced to one year probation, a $500 fine, 40 hours of community service and a one-year loss of his driver's license. The police later stated that his blood alcohol level was the highest they had ever seen for someone who hadn't died of alcohol poisoning. It was later estimated that the driver had consumed 10–14 drinks over the course of 1–2 hours, based on the standard levels of elimination which as documented previously can vary by up to 300%.
In December 2009, a South Dakota woman was found behind the wheel of a stolen car with a measured blood alcohol content of .708%, almost nine times the state's limit of .08%, thus becoming the highest recorded level of alcohol toxicity for the state. After she was hospitalized, she was released on bond and subsequently found in another stolen automobile while under the influence.
In August 2012, an Iowa man was arrested for driving under the influence. Breathalyzers and subsequent lab tests confirmed a BAC of .627%, about 8 times the legal limit for driving. At that blood alcohol level, he was conscious, yet incoherent and unable to answer simple questions.
Highest recorded blood alcohol level/content.
There have been reported cases of blood alcohol content higher than 1.00%. In March 2009, a 45-year-old man was admitted to the hospital in Skierniewice, Poland, after being struck by a car. The blood test showed blood alcohol content at 1.23%. The man survived but did not remember either the accident or the circumstances of his alcohol consumption. One such case was reported by O'Neil, and others in 1984. They report on a 30-year-old man who survived a blood alcohol concentration of 1,500 mg/100 ml (1.5%) blood after vigorous medical intervention.

</doc>
<doc id="4848" url="http://en.wikipedia.org/wiki?curid=4848" title="Barrister">
Barrister

A barrister (also known as barrister-at-law or Bar-at-law) is a member of one of the two classes of lawyer found in many common law jurisdictions with split legal professions. Barristers specialize in courtroom advocacy, drafting legal pleadings, and giving expert legal opinions. They can be contrasted with solicitors – the other class of lawyer in split professions – who have more direct access to clients, and may do transactional-type legal work. Barristers are rarely hired by clients directly but instead are retained (or instructed) by solicitors to act on behalf of clients. In some legal systems, including those of Scotland, Belgium, South Africa, India, Pakistan, Scandinavian jurisdictions, Israel, Brazil, and the British Crown dependencies of Jersey, Guernsey and the Isle of Man, a professional with similar responsibilities is called an advocate.
The historical difference between the two professions – and the only essential difference in England and Wales today – is that solicitors are attorneys, which means that they can act in the place of their client for legal purposes (as in signing contracts) and may conduct litigation on their behalf by making applications to the court, writing letters in litigation to the client's opponent, and so on. A barrister is not an attorney and is usually forbidden, either by law or professional rules or both, from "conducting" litigation. This means that, while the barrister speaks on the client's behalf in court, he or she can do so only when instructed by a solicitor or certain other qualified professional clients, such as patent agents.
Many countries with common law legal systems, like New Zealand and the United States of America, have abandoned the separate systems of legal representation, and an attorney (United States terminology) or lawyer (New Zealand terminology) can perform all the functions of each. Some other jurisdictions have a "partially fused" profession: for example, in Australia, all solicitors are also qualified to practise as barristers, but there is still a separate system of qualification as barristers only.
Differences between barristers and solicitors.
Differences.
A barrister is a lawyer who represents a litigant as advocate before a court of appropriate jurisdiction. A barrister speaks in court and presents the case before a judge or jury. In some jurisdictions, a barrister receives additional training in evidence law, ethics, and court practice and procedure. In contrast, a solicitor generally meets with clients, does preparatory and administrative work and provides legal advice. In this role, he or she may draft and review legal documents, interact with the client as necessary, prepare evidence, and generally manage the day-to-day administration of a lawsuit. A solicitor can provide a crucial support role to a barrister when in court, such as managing large volumes of documents in the case or even negotiating a settlement outside the courtroom while the trial continues inside.
There are other essential differences. A barrister will usually have rights of audience in the higher courts, whereas other legal professionals will often have more limited access, or will need to acquire additional qualifications to have such access. In countries where there is a split between the roles of barrister and solicitor, the barrister, in civil law jurisdictions is responsible for appearing in trials or pleading cases before the courts.
Barristers usually have particular knowledge of case law, precedent, and the skills to "build" a case. When a solicitor in general practice is confronted with an unusual point of law, they may seek the "opinion of counsel" on the issue.
In most countries, barristers operate as sole practitioners, and are prohibited from forming partnerships or from working as a barrister as part of a corporation. (In 2009, the Clementi Report recommended the abolition of this restriction in England and Wales.) However, barristers normally band together into "chambers" to share clerks (administrators) and operating expenses. Some chambers grow to be large and sophisticated, and have a distinctly corporate feel. In some jurisdictions, barristers may be employed by firms of solicitors, banks, or corporations as in-house legal advisers.
In contrast, solicitors work directly with the clients and are responsible for engaging a barrister with the appropriate expertise for the case. Barristers generally have little or no direct contact with their 'lay clients', particularly without the presence or involvement of the solicitor. All correspondence, inquiries, invoices, and so on, will be addressed to the solicitor, who is primarily responsible for the barrister's fees.
In court, barristers are often visibly distinguished from solicitors by their apparel. For example, in Ireland, England, and Wales, a barrister usually wears a horsehair wig, stiff collar, bands, and a gown. Since January 2008, solicitor advocates have also been entitled to wear wigs, but wear different gowns.
In many countries the traditional divisions between barristers and solicitors are breaking down. Barristers once enjoyed a monopoly on appearances before the higher courts, but in England, Wales, Scotland, and Northern Ireland this has now been abolished, and solicitor advocates can generally appear for clients at trial. Increasingly, firms of solicitors are keeping even the most advanced advisory and litigation work in-house for economic and client relationship reasons. Similarly, the prohibition on barristers taking instructions directly from the public has also been widely abolished. But in practice, direct instruction is still a rarity in most jurisdictions, partly because barristers with narrow specialisations or who are only really trained for advocacy are not prepared to provide general advice to members of the public.
Historically barristers have had a major role in trial preparation, including drafting pleadings and reviewing evidence. In some areas of law, that is still the case. In other areas, it is relatively common for the barrister to receive the brief from the instructing solicitor to represent a client at trial only a day or two before the proceeding. Part of the reason for this is cost. A barrister is entitled to a 'brief fee' when a brief is delivered, and this represents the bulk of his fee in relation to any trial. They are then usually entitled to a 'refresher' for each day of the trial after the first. But if a case is settled before the trial, the barrister is not needed and the brief fee would be wasted. Some solicitors avoid this by delaying delivery of the brief until it is certain the case will go to trial.
Justification for a split profession.
Some benefits of maintaining the split include:
Some disadvantages of the split include:
A detailed examination of the justifications for a split legal profession and of the arguments in favour of a fused profession can be found in English solicitor Peter Reeve’s 1986 book, "Are Two Legal Professions Necessary?"
Regulation.
Barristers are regulated by the Bar for the jurisdiction where they practise, and in some countries, by the Inn of Court to which they belong. In some countries, there is external regulation.
Inns of Court, where they exist, regulate admission to the profession. Inns of Court are independent societies that are titularly responsible for the training, admission (calling), and discipline of barristers. Where they exist, a person may only be called to the Bar by an Inn, of which they must first be a member. In fact, historically, call to and success at the Bar, to a large degree, depended upon social connections made early in life.
A Bar collectively describes all members of the profession of barrister within a given jurisdiction. While as a minimum the Bar is an association embracing all its members, it is usually the case, either "de facto" or "de jure", that the Bar is invested with regulatory powers over the manner in which barristers practice.
Barristers around the world.
In the common law tradition, the respective roles of a lawyer – that is as legal adviser and advocate – were formally split into two separate, regulated sub-professions, the other being the office of solicitor. Historically, the distinction was absolute, but in the modern legal age, some countries that had a split legal profession now have a fused profession – anyone entitled to practise as a barrister may also practise as a solicitor, and vice versa. In practice, the distinction may be non-existent, minor, or marked, depending on the jurisdiction. In some jurisdictions, such Australia, Scotland and Ireland, there is little overlap.
Australia.
In the Australian states of New South Wales and Queensland, there is a split profession. Nevertheless, subject to conditions, barristers can accept direct access work from clients. Each state Bar Association regulates the profession and essentially has the functions of the English Inns of Court. In the states of South Australia, Victoria, and Western Australia, as well as the Australian Capital Territory, the professions of barrister and solicitor are fused, but an independent bar nonetheless exists, regulated by the Legal Practice Board of the state or territory. In Tasmania and the Northern Territory, the profession is fused, although a very small number of practitioners operate as an independent bar.
Generally counsel dress in the traditional English manner (wig, gown, and jabot) before superior courts, although they no longer robe for appearances in lower jurisdictions. Wigs are no longer worn in the highest civil court in New South Wales, the Court of Appeal. Wigs are still worn in the Supreme Court, while only robes without wigs are worn in the District Courts in civil matters. Robes and wigs are worn in all criminal cases. In Western Australia, wigs are no longer worn in any court.
Each year, the Bar Association appoints certain barristers of seniority and eminence to the rank of "Senior Counsel" (in most States and Territories) or "Queen's Counsel" (in Queensland). Such barristers carry the title "SC" or "QC" after their name. The appointments are made after a process of consultation with members of the profession and the judiciary. Senior Counsel appear in particularly complex or difficult cases. They make up about 14 per cent of the bar in New South Wales.
Canada.
In Canada (except Quebec), the professions of barrister and solicitor are fused, and many lawyers refer to themselves with both names, even if they do not practice in both areas. In colloquial parlance within the Canadian legal profession, lawyers often term themselves as "litigators" (or "barristers"), or as "solicitors", depending on the nature of their law practice though some may in effect practise as both litigators and solicitors. However, "litigators" would generally perform all litigation functions traditionally performed by barristers and solicitors; in contrast, those terming themselves "solicitors" would generally limit themselves to legal work not involving practice before the courts (not even in a preparatory manner as performed by solicitors in England), though some might practise before chambers judges. As is the practice in many other Commonwealth jurisdictions such as Australia, Canadian litigators are "gowned", but without a wig, when appearing before courts of "superior jurisdiction".
The situation is somewhat different in Quebec as a result of its civil law tradition. The profession of solicitor, or "avoué", never took hold in colonial Quebec, so attorneys ("avocats") have traditionally been a fused profession, arguing and preparing cases in contentious matters, whereas Quebec's other type of lawyer, civil-law notaries ("notaires"), handle out-of-court non-contentious matters. However, a number of areas of non-contentious private law are not monopolized by notaries so that attorneys often specialise in handling either trials, cases, advising, or non-trial matters. The only disadvantage is that attorneys cannot draw up public instruments that have the same force of law as notarial acts. Most large law firms in Quebec offer the full range of legal services of law firms in common-law provinces. Intending Quebec attorneys must earn a Bachelor's degree in civil law, pass the provincial bar examination, and successfully complete a legal internship to be admitted to practise. Attorneys are regulated by the Quebec Law Society ("Barreau du Québec").
France.
In France, "avocats", or attorneys, were, until the 20th century, the equivalent of barristers. The profession included several grades ranked by seniority: "stagiaire" (pupil, student-at-law), "plaignant" (junior barrister), and "consultant" (senior barrister). Since the 14th century and during the course of the 19th in particular, French barristers competed in territorial battles over respective areas of legal practice against the "conseil juridique" (chamber-counselor) and "avoué" (solicitor), and expanded to become the generalist legal practitioner. After the 1971 and 1990 legal reforms, the "avocat" was fused with the solicitor and chamber-counselor, making the "avocat" (or, if female, "avocate") an all-purpose lawyer for matters of contentious jurisdiction, analogous to an American attorney. French attorneys may specialize as litigators (trial lawyers) and legal consultants (advising lawyers), known respectively as "avocat plaidant" and "avocat-conseil". All intending attorneys must pass a legal practice examination and two-year traineeship to be admitted to general practice and enrolled, but to have rights of audience at a given court, an attorney must join that court's bar ("barreau") by passing its specific bar examination. Each bar is regulated by a Bar Council ("Ordre du barreau").
Germany.
In Germany, no distinction is made and lawyers may plead at all courts with the exception of the Federal Court of Justice ("Bundesgerichtshof") to which fewer than fifty lawyers are admitted as of 25 September 2007. See the list of lawyers admitted to the . Those lawyers may not plead at other courts, almost only deal with litigation, and are usually instructed by a lawyer who represented the client in the lower courts. However, these restrictions do not apply to criminal cases, nor to pleadings at courts of the other court systems (labour, administrative, taxation, and social courts, as well as the EU court system).
Hong Kong.
The legal profession in Hong Kong is also divided into two branches: barristers and solicitors.
In the High Court and the Court of Final Appeal, only barristers and solicitor-advocates are allowed to speak on behalf of any party, which means that solicitors are restricted from doing so. In these two courts, barristers dress in the traditional English manner, as do the judges and other lawyers.
In Hong Kong, the rank of Queen's Counsel was granted prior to the transfer of the sovereignty of Hong Kong from the United Kingdom to China in 1997. After the handover, the rank has been replaced by Senior Counsel post-nominal letters: SC. Senior Counsels may still, however, style themselves as silks, like their British counterparts.
Ireland.
In the Republic of Ireland, admission to the Bar by the Chief Justice of Ireland is restricted to those on whom a Barrister-at-Law degree (B.L.) has first been conferred. The Honorable Society of King's Inns is the only educational establishment which runs vocational courses for barristers in the Republic and degrees of Barrister-at-Law can only be conferred by King's Inns. King's Inns are also the only body with the capacity to call individuals to the bar and to disbar them.
Most Irish barristers choose to be governed thereafter by the Bar Council of Ireland, a quasi-private entity. Senior members of the profession may be selected for elevation to the Inner Bar, when they may describe themselves as Senior Counsel ("S.C."). Admission to the Inner Bar is made by declaration before the Supreme Court, patents of precedence having been granted by the Government. Irish barristers are sole practitioners and may not form chambers or partnerships if they wish to remain members of the Bar Council's Law Library.
To practise under the Bar Council of Ireland's rules, a newly qualified barrister is apprenticed to an experienced barrister of at least seven years' experience. This apprenticeship is known as pupillage or devilling. Devilling is compulsory for those barristers who wish to be members of the Law Library and lasts for one legal year. It is common to devil for a second year in a less formal arrangement but this is not compulsory.
Israel.
In Israel there is no distinction between barristers and solicitors, even though the judicial system is based mostly on English common law, from when Britain administered what was then Mandatory Palestine from 1920 to 1948.
Japan.
Japan adopts a unified system. However, there are certain classes of qualified professionals who are allowed to practice in certain limited areas of law, such as scriveners ("shiho shoshi", qualified to handle title registration, deposit, and certain petite court proceedings with additional certification), tax accountants ("zeirishi", qualified to prepare tax returns, provide advice on tax computation and represent a client in administrative tax appeals) and patent agents ("benrishi", qualified to practice patent registration and represent a client in administrative patent appeals). Only the lawyers ("bengoshi") can appear before court and are qualified to practice in any areas of law, including but not limited to areas that those qualified law-related professionals above are allowed to practice. Most attorneys still focuses primarily on court practice and still a very few number of attorneys give sophisticated and expertized legal advice on a day-to-day basis to large corporations.
Netherlands.
The Netherlands used to have a semi-separated legal profession comprising the lawyer and the "procureur", the latter resembling, to some extent, the profession of barrister. Under that system, lawyers were entitled to represent their clients in law, but were only able to file cases before the court at which they were registered. Cases falling under the jurisdiction of another court had to be filed by a "procureur" registered at that court, in practice often another lawyer exercising both functions. Questions were raised on the necessity of the separation, given the fact that its main purpose – the preservation of the quality of the legal profession and observance of local court rules and customs – had become obsolete. For that reason, the "procureur" as a separate profession was abolished and its functions merged with the legal profession in 2008. Currently, lawyers can file cases before any court, regardless of where they are registered. The only notable exception concerns cases brought before the Supreme Court, which have to be handled by lawyers registered in the district of South Holland, mainly for qualitative reasons.
New Zealand.
In New Zealand, the professions are formally fused. Practitioners are enrolled in the High Court as "Barristers and Solicitors", although their annual practising certificate may (by choice) nominate them as either or both. A significant proportion however practice solely as barristers, usually in "chambers" (following the British terminology), and receive "instructions" from other practitioners, at least nominally. They usually conduct the proceedings in their entirety.
A person practising as a Barrister Sole may be invited to become a Queen's Counsel (QC) to recognize long standing contribution to the legal profession. This step, referred to as "being called to the inner bar" or "taking silk", is considered highly prestigious and has been a step in the career of many New Zealand judges.
Nigeria.
In Nigeria, there is no formal distinction between barristers and solicitors. All students who pass the bar examinations – offered exclusively by the Nigerian Law School – are called to the Nigerian bar, by the Body of Benchers. Lawyers may argue in any Federal trial or appellate court as well as any of the courts in Nigeria's 36 states and the Federal Capital Territory. The Legal Practitioner's Act, refers to Nigerian lawyers as Legal Practitioners, and following their call to the Bar, Nigerian lawyers enter their names in the register or Roll of Legal Practitioners kept at the Supreme Court. Perhaps for this reason, a Nigerian lawyer is also often referred to as a Barrister and Solicitor of the Supreme Court of Nigeria, and many Nigerian lawyers term themselves Barrister-at-Law complete with the postnominal initials "B.L.".
The vast majority of Nigerian lawyers combine contentious and non-contentious work, although there is a growing tendency for practitioners in the bigger practices to specialise in one or the other. In colloquial parlance within the Nigerian legal profession, lawyers may for this reason be referred to as "litigators" or as "solicitors".
Consistent with the practice in England and elsewhere in the Commonwealth, senior members of the profession may be selected for elevation to the Inner Bar by conferment of the rank of Senior Advocate of Nigeria (SAN).
South Africa.
In South Africa the employment and practice of advocates (as barristers are known in South Africa) is consistent with the rest of the Commonwealth. Advocates carry the rank of Junior or Senior Counsel (SC), and are mostly briefed and paid by solicitors (known as attorneys). They are usually employed in the higher courts, particularly in the Appeal Courts where they often appear as specialist counsel. South African solicitors (attorneys) follow a practice of referring cases to Counsel for an opinion before proceeding with a case, when Counsel in question practices as a specialist in the case law at stake. Aspiring advocates currently spend one year in pupillage (formerly only six months) before being admitted to the bar in their respective provincial or judicial jurisdictions. The term 'Advocate' is sometimes used in South Africa as a title, e. g. 'Advocate John Doe, SC' ('Advokaat' in Afrikaans) in the same fashion as 'Dr. John Doe' for a medical doctor.
South Korea.
There is no distinction between two branches. A person who passed the national bar exam, after 2 years of national education, would be able to become a judge, a prosecutor or a "lawyer" in accordance of their grades upon graduation. However, Korea has accommodated Law School System either. Now, there are two(2) ways to be a lawyer, the traditional but time limited way until 2017 and the other way to take a course on a law school and passing the bar exam. Under the current legal system, to be a judge or a prosecutor, lawyers need to practice their legal knowledge. A "lawyer" does not have any limitation of practice.
Spain.
Spain has a division that somewhat corresponds to the division in Britain between barristers/advocates and solicitors. "Procuradores" represent the litigant procedurally in court, generally under the authority of a power of attorney executed by a civil law notary, while "abogados" represent the substantive claims of the litigant through trial advocacy. Essentially, Procuradores are court agents and their practice is confined to the locality of the court to which they are admitted. Procuradores are regulated by Royal Decree 2046 of 1982, which approved the General Statute of the Procuradores, and the Organic Law no.6 of 1985. The General Statute regulates the qualifications and conduct of the procuradores. Thus, obligations to act "pro bono" are laid down by Article 13.
United States.
The United States does not draw a distinction between lawyers as pleaders (barristers) and lawyers as agents (or solicitors). All lawyers who have passed a bar examination and have been admitted to practice may prosecute or defend in the courts of the state where they are admitted. Historically, a distinction was made, and a separate label for barristers (called "counselors") existed in certain states, though both professions have long since been fused into the all-purpose attorney. Attorneys specializing in court procedure, combining advocacy and case preparation, are called trial attorneys or litigators.
South Carolina still requires attorneys to be licensed separately to plead in a courtroom. Additionally, some state appellate courts require attorneys to obtain a separate certificate of admission to plead and practice in the appellate court. Federal courts require specific admission to that court's bar to practice before it. At the state appellate level and in Federal courts, there is generally no separate examination process, although some U.S. district courts require an examination on practices and procedures in their specific courts. Unless an examination is required, admission is usually granted as a matter of course to any licensed attorney in the state where the court is located. Some federal courts will grant admission to any attorney licensed in any U.S. jurisdiction.
United Kingdom.
England and Wales.
Although with somewhat different laws, England and Wales are considered within the United Kingdom a single united and unified legal jurisdiction for the purposes of both civil and criminal law, alongside Scotland and Northern Ireland, the other two legal jurisdictions within the United Kingdom. England and Wales are covered by a common bar (an organisation of barristers) and a single law society (an organisation of solicitors).
The profession of barrister in England and Wales is a separate profession from that of solicitor. It is, however, possible to hold the qualification of both barrister and solicitor at the same time. It is not necessary to leave the bar to qualify as a solicitor.
Barristers are regulated by the Bar Standards Board, a division of the General Council of the Bar. A barrister must be a member of one of the Inns of Court, which traditionally educated and regulated barristers. There are four Inns of Court: The Honourable Society of Gray's Inn, The Honourable Society of Lincoln's Inn, The Honourable Society of the Middle Temple, and The Honourable Society of the Inner Temple. All are situated in central London, near the Royal Courts of Justice. They perform scholastic and social roles, and in all cases, provide financial aid to student barristers (subject to merit) through scholarships. It is the Inns that actually "call" the student to the Bar at a ceremony similar to a graduation. Social functions include dining with other members and guests and hosting other events.
Student barristers must take a law degree and the Bar Professional Training Course (BPTC - previously Bar Vocational Course or BVC) (usually one year full-time) at one of the institutions authorised by the Bar Council to offer the BPTC. On successful completion of the BPTC student barristers are "called" to the bar by their respective inns and are elevated to the degree of "Barrister". However, before they can practise independently they must first undertake 12 months of pupillage. The first six months of this period is spent shadowing more senior practitioners, after which pupil barristers may begin to undertake some court work of their own. Following successful completion of this stage, most barristers then join a set of Chambers, a group of counsel who share the costs of premises and support staff whilst remaining individually self-employed.
In December 2004 there were just over 11,500 barristers in independent practice, of whom about ten percent are Queen's Counsel and the remainder are junior barristers. Many barristers (about 2,800) are employed in companies as 'in-house' counsel, or by local or national government or in academic institutions.
Certain barristers in England and Wales are now instructed directly by members of the public. Members of the public may engage the services of the barrister directly; a solicitor is not involved at any stage. Barristers undertaking public access work can provide legal advice and representation in court in almost all areas of law (see the Public Access Information on the Bar Council website) and are entitled to represent clients in any court or tribunal in England and Wales. Once instructions from a client are accepted, it is the barrister (rather than the solicitor) who advises and guides the client through the relevant legal procedure or litigation.
Before a barrister can undertake Public Access work, he must have completed a special course. At present, about one in 20 barristers has so qualified. There is also a separate scheme called 'Licensed Access', available to certain nominated classes of professional client; it is not open to the general public. Public access work is experiencing a huge surge at the bar, with barristers taking advantage of the new opportunity for the bar to make profit in the face of legal aid cuts elsewhere in the profession.
The ability of barristers to accept such instructions is a recent development; it results from a change in the rules set down by the General Council of the Bar in July 2004. The Public Access Scheme has been introduced as part of the drive to open up the legal system to the public and to make it easier and cheaper to obtain access to legal advice. It further reduces the distinction between solicitors and barristers. The distinction remains however because there are certain aspects of a solicitor's role that a barrister is not able to undertake.
Although the term "barrister-at-law" is sometimes seen, and was once very common, it has never been formally correct in England and Wales. "Barrister" is the only correct nomenclature.
Northern Ireland.
In April 2003 there were 554 barristers in independent practice in Northern Ireland. 66 were Queen's Counsel (QCs), barristers who have earned a high reputation and are appointed by the Queen on the recommendation of the Lord Chancellor as senior advocates and advisers.
Those barristers who are not QCs are called Junior Counsel and are styled "BL" or "Barrister-at-Law". The term "junior" is often misleading since many members of the Junior Bar are experienced barristers with considerable expertise.
Benchers are, and have been for centuries, the governing bodies of the four Inns of Court in London and King's Inns, Dublin. The Benchers of the Inn of Court of Northern Ireland governed the Inn until the enactment of the Constitution of the Inn in 1983, which provides that the government of the Inn is shared between the Benchers, the Executive Council of the Inn and members of the Inn assembled in General Meeting.
The Executive Council (through its Education Committee) is responsible for considering Memorials submitted by applicants for admission as students of the Inn and by Bar students of the Inn for admission to the degree of Barrister-at-Law and making recommendations to the Benchers. The final decisions on these Memorials are taken by the Benchers. The Benchers also have the exclusive power of expelling or suspending a Bar student and of disbarring a barrister or suspending a barrister from practice.
The Executive Council is also involved with: education; fees of students; calling counsel to the Bar, although call to the Bar is performed by the Lord Chief Justice of Northern Ireland on the invitation of the Benchers; administration of the Bar Library (to which all practising members of the Bar belong); and liaising with corresponding bodies in other countries.
The Bar Council is responsible for the maintenance of the standards, honour and independence of the Bar and, through its Professional Conduct Committee, receives and investigates complaints against members of the Bar in their professional capacity.
Scotland.
In Scotland, an advocate is, in all respects except name, a barrister, but there are significant differences in professional practice.
In Scotland, admission to and the conduct of the profession is regulated by the Faculty of Advocates (as opposed to an Inn).
Isle of Man, Jersey and Guernsey.
In the Bailiwick of Jersey, there are solicitors (called "ecrivains") and advocates (French "avocat"). In the Bailiwicks of Jersey and Guernsey and on the Isle of Man, Advocates perform the combined functions of both solicitors and barristers.

</doc>
<doc id="4849" url="http://en.wikipedia.org/wiki?curid=4849" title="Battle of Gettysburg">
Battle of Gettysburg

The Battle of Gettysburg (, with an sound) was fought July 1–3, 1863, in and around the town of Gettysburg, Pennsylvania, by Union and Confederate forces during the American Civil War. The battle involved the largest number of casualties of the entire war and is often described as the war's turning point. Union Maj. Gen. George Meade's Army of the Potomac defeated attacks by Confederate Gen. Robert E. Lee's Army of Northern Virginia, ending Lee's attempt to invade the North.
After his success at Chancellorsville in Virginia in May 1863, Lee led his army through the Shenandoah Valley to begin his second invasion of the North—the Gettysburg Campaign. With his army in high spirits, Lee intended to shift the focus of the summer campaign from war-ravaged northern Virginia and hoped to influence Northern politicians to give up their prosecution of the war by penetrating as far as Harrisburg, Pennsylvania, or even Philadelphia. Prodded by President Abraham Lincoln, Maj. Gen. Joseph Hooker moved his army in pursuit, but was relieved of command just three days before the battle and replaced by Meade.
Elements of the two armies initially collided at Gettysburg on July 1, 1863, as Lee urgently concentrated his forces there, his objective being to engage the Union army and destroy it. Low ridges to the northwest of town were defended initially by a Union cavalry division under Brig. Gen. John Buford, and soon reinforced with two corps of Union infantry. However, two large Confederate corps assaulted them from the northwest and north, collapsing the hastily developed Union lines, sending the defenders retreating through the streets of town to the hills just to the south.
On the second day of battle, most of both armies had assembled. The Union line was laid out in a defensive formation resembling a fishhook. In the late afternoon of July 2, Lee launched a heavy assault on the Union left flank, and fierce fighting raged at Little Round Top, the Wheatfield, Devil's Den, and the Peach Orchard. On the Union right, Confederate demonstrations escalated into full-scale assaults on Culp's Hill and Cemetery Hill. All across the battlefield, despite significant losses, the Union defenders held their lines.
On the third day of battle, fighting resumed on Culp's Hill, and cavalry battles raged to the east and south, but the main event was a dramatic infantry assault by 12,500 Confederates against the center of the Union line on Cemetery Ridge, known as Pickett's Charge. The charge was repulsed by Union rifle and artillery fire, at great loss to the Confederate army.
Lee led his army on a torturous retreat back to Virginia. Between 46,000 and 51,000 soldiers from both armies were casualties in the three-day battle.
On November 19, President Lincoln used the dedication ceremony for the Gettysburg National Cemetery to honor the fallen Union soldiers and redefine the purpose of the war in his historic Gettysburg Address.
Background and movement to battle.
Shortly after the Army of Northern Virginia won a major victory over the Army of the Potomac at the Battle of Chancellorsville (April 30 – May 6, 1863), Robert E. Lee decided upon a second invasion of the North (the first was the unsuccessful Maryland Campaign of September 1862, which ended in the bloody Battle of Antietam). Such a move would upset Federal plans for the summer campaigning season and possibly reduce the pressure on the besieged Confederate garrison at Vicksburg. The invasion would allow the Confederates to live off the bounty of the rich Northern farms while giving war-ravaged Virginia a much-needed rest. In addition, Lee's 72,000-man army could threaten Philadelphia, Baltimore, and Washington, and possibly strengthen the growing peace movement in the North.
Thus, on June 3, Lee's army began to shift northward from Fredericksburg, Virginia. Following the death of Thomas J. "Stonewall" Jackson, Lee reorganized his two large corps into three new corps, commanded by Lt. Gen. James Longstreet (First Corps), Lt. Gen. Richard S. Ewell (Second), and Lt. Gen. A.P. Hill (Third); both Ewell and Hill, who had formerly reported to Jackson as division commanders, were new to this level of responsibility. The Cavalry Division remained under the command of Maj. Gen. J.E.B. Stuart.
The Union Army of the Potomac, under Maj. Gen. Joseph Hooker, consisted of seven infantry corps, a cavalry corps, and an Artillery Reserve, for a combined strength of about 94,000 men.
The first major action of the campaign took place on June 9 between cavalry forces at Brandy Station, near Culpeper, Virginia. The 9,500 Confederate cavalrymen under Stuart were surprised by Maj. Gen. Alfred Pleasonton's combined arms force of two cavalry divisions (8,000 troopers) and 3,000 infantry, but Stuart eventually repulsed the Union attack. The inconclusive battle, the largest predominantly cavalry engagement of the war, proved for the first time that the Union horse soldier was equal to his Southern counterpart.
By mid-June, the Army of Northern Virginia was poised to cross the Potomac River and enter Maryland. After defeating the Federal garrisons at Winchester and Martinsburg, Ewell's Second Corps began crossing the river on June 15. Hill's and Longstreet's corps followed on June 24 and 25. Hooker's army pursued, keeping between the U.S. capital and Lee's army. The Federals crossed the Potomac from June 25 to 27.
Lee gave strict orders for his army to minimize any negative impacts on the civilian population. Food, horses, and other supplies were generally not seized outright, although quartermasters reimbursing Northern farmers and merchants with Confederate money were not well received. Various towns, most notably York, Pennsylvania, were required to pay indemnities in lieu of supplies, under threat of destruction. During the invasion, the Confederates seized some 40 northern African Americans. A few of them were escaped fugitive slaves, but most were freemen; all were sent south into slavery under guard.
On June 26, elements of Maj. Gen. Jubal Early's division of Ewell's Corps occupied the town of Gettysburg after chasing off newly raised Pennsylvania militia in a series of minor skirmishes. Early laid the borough under tribute but did not collect any significant supplies. Soldiers burned several railroad cars and a covered bridge, and destroyed nearby rails and telegraph lines. The following morning, Early departed for adjacent York County.
Meanwhile, in a controversial move, Lee allowed Jeb Stuart to take a portion of the army's cavalry and ride around the east flank of the Union army. Lee's orders gave Stuart much latitude, and both generals share the blame for the long absence of Stuart's cavalry, as well as for the failure to assign a more active role to the cavalry left with the army. Stuart and his three best brigades were absent from the army during the crucial phase of the approach to Gettysburg and the first two days of battle. By June 29, Lee's army was strung out in an arc from Chambersburg (28 miles (45 km) northwest of Gettysburg) to Carlisle (30 miles (48 km) north of Gettysburg) to near Harrisburg and Wrightsville on the Susquehanna River.
In a dispute over the use of the forces defending the Harpers Ferry garrison, Hooker offered his resignation, and Abraham Lincoln and General-in-Chief Henry W. Halleck, who were looking for an excuse to get rid of him, immediately accepted. They replaced Hooker early on the morning of June 28 with Maj. Gen. George Gordon Meade, then commander of the V Corps.
On June 29, when Lee learned that the Army of the Potomac had crossed the Potomac River, he ordered a concentration of his forces around Cashtown, located at the eastern base of South Mountain and eight miles (13 km) west of Gettysburg. On June 30, while part of Hill's Corps was in Cashtown, one of Hill's brigades, North Carolinians under Brig. Gen. J. Johnston Pettigrew, ventured toward Gettysburg. In his memoirs, Maj. Gen. Henry Heth, Pettigrew's division commander, claimed that he sent Pettigrew to search for supplies in town—especially shoes.
When Pettigrew's troops approached Gettysburg on June 30, they noticed Union cavalry under Brig. Gen. John Buford arriving south of town, and Pettigrew returned to Cashtown without engaging them. When Pettigrew told Hill and Heth what he had seen, neither general believed that there was a substantial Federal force in or near the town, suspecting that it had been only Pennsylvania militia. Despite General Lee's order to avoid a general engagement until his entire army was concentrated, Hill decided to mount a significant reconnaissance in force the following morning to determine the size and strength of the enemy force in his front. Around 5 a.m. on Wednesday, July 1, two brigades of Heth's division advanced to Gettysburg.
First day of battle.
Anticipating that the Confederates would march on Gettysburg from the west on the morning of July 1, Buford laid out his defenses on three ridges west of the town: Herr Ridge, McPherson Ridge, and Seminary Ridge. These were appropriate terrain for a delaying action by his small cavalry division against superior Confederate infantry forces, meant to buy time awaiting the arrival of Union infantrymen who could occupy the strong defensive positions south of town at Cemetery Hill, Cemetery Ridge, and Culp's Hill. Buford understood that if the Confederates could gain control of these heights, Meade's army would have difficulty dislodging them.
Heth's division advanced with two brigades forward, commanded by Brig. Gens. James J. Archer and Joseph R. Davis. They proceeded easterly in columns along the Chambersburg Pike. Three miles (5 km) west of town, about 7:30 a.m. on July 1, the two brigades met light resistance from vedettes of Union cavalry, and deployed into line. According to lore, the Union soldier to fire the first shot of the battle was Lt. Marcellus Jones. In 1886 Lt. Jones returned to Gettysburg to mark the spot where he fired the first shot with a monument. Eventually, Heth's men reached dismounted troopers of Col. William Gamble's cavalry brigade, who raised determined resistance and delaying tactics from behind fence posts with fire from their breechloading carbines. Still, by 10:20 a.m., the Confederates had pushed the Union cavalrymen east to McPherson Ridge, when the vanguard of the I Corps (Maj. Gen. John F. Reynolds) finally arrived.
North of the pike, Davis gained a temporary success against Brig. Gen. Lysander Cutler's brigade but was repulsed with heavy losses in an action around an unfinished railroad bed cut in the ridge. South of the pike, Archer's brigade assaulted through Herbst (also known as McPherson's) Woods. The Federal Iron Brigade under Brig. Gen. Solomon Meredith enjoyed initial success against Archer, capturing several hundred men, including Archer himself.
General Reynolds was shot and killed early in the fighting while directing troop and artillery placements just to the east of the woods. Shelby Foote wrote that the Union cause lost a man considered by many to be "the best general in the army." Maj. Gen. Abner Doubleday assumed command. Fighting in the Chambersburg Pike area lasted until about 12:30 p.m. It resumed around 2:30 p.m., when Heth's entire division engaged, adding the brigades of Pettigrew and Col. John M. Brockenbrough.
As Pettigrew's North Carolina Brigade came on line, they flanked the 19th Indiana and drove the Iron Brigade back. The 26th North Carolina (the largest regiment in the army with 839 men) lost heavily, leaving the first day's fight with around 212 men. By the end of the three-day battle, they had about 152 men standing, the highest casualty percentage for one battle of any regiment, North or South. Slowly the Iron Brigade was pushed out of the woods toward Seminary Ridge. Hill added Maj. Gen. William Dorsey Pender's division to the assault, and the I Corps was driven back through the grounds of the Lutheran Seminary and Gettysburg streets.
As the fighting to the west proceeded, two divisions of Ewell's Second Corps, marching west toward Cashtown in accordance with Lee's order for the army to concentrate in that vicinity, turned south on the Carlisle and Harrisburg roads toward Gettysburg, while the Union XI Corps (Maj. Gen. Oliver O. Howard) raced north on the Baltimore Pike and Taneytown Road. By early afternoon, the Federal line ran in a semicircle west, north, and northeast of Gettysburg.
However, the Federals did not have enough troops; Cutler, who was deployed north of the Chambersburg Pike, had his right flank in the air. The leftmost division of the XI Corps was unable to deploy in time to strengthen the line, so Doubleday was forced to throw in reserve brigades to salvage his line.
Around 2 p.m., the Confederate Second Corps divisions of Maj. Gens. Robert E. Rodes and Jubal Early assaulted and out-flanked the Union I and XI Corps positions north and northwest of town. The Confederate brigades of Col. Edward A. O'Neal and Brig. Gen. Alfred Iverson suffered severe losses assaulting the I Corps division of Brig. Gen. John C. Robinson south of Oak Hill. Early's division profited from a blunder by Brig. Gen. Francis C. Barlow, when he advanced his XI Corps division to Blocher's Knoll (directly north of town and now known as Barlow's Knoll); this represented a salient in the corps line, susceptible to attack from multiple sides, and Early's troops overran Barlow's division, which constituted the right flank of the Union Army's position. Barlow was wounded and captured in the attack.
As Federal positions collapsed both north and west of town, Gen. Howard ordered a retreat to the high ground south of town at Cemetery Hill, where he had left the division of Brig. Gen. Adolph von Steinwehr in reserve. Maj. Gen. Winfield S. Hancock assumed command of the battlefield, sent by Meade when he heard that Reynolds had been killed. Hancock, commander of the II Corps and Meade's most trusted subordinate, was ordered to take command of the field and to determine whether Gettysburg was an appropriate place for a major battle. Hancock told Howard, "I think this the strongest position by nature upon which to fight a battle that I ever saw." When Howard agreed, Hancock concluded the discussion: "Very well, sir, I select this as the battle-field." Hancock's determination had a morale-boosting effect on the retreating Union soldiers, but he played no direct tactical role on the first day.
General Lee understood the defensive potential to the Union if they held this high ground. He sent orders to Ewell that Cemetery Hill be taken "if practicable." Ewell, who had previously served under Stonewall Jackson, a general well known for issuing peremptory orders, determined such an assault was not practicable and, thus, did not attempt it; this decision is considered by historians to be a great missed opportunity.
The first day at Gettysburg, more significant than simply a prelude to the bloody second and third days, ranks as the 23rd biggest battle of the war by number of troops engaged. About one quarter of Meade's army (22,000 men) and one third of Lee's army (27,000) were engaged.
Second day of battle.
Plans and movement to battle.
Throughout the evening of July 1 and morning of July 2, most of the remaining infantry of both armies arrived on the field, including the Union II, III, V, VI, and XII Corps. Longstreet's third division, commanded by Maj. Gen. George Pickett, had begun the march from Chambersburg early in the morning; it did not arrive until late on July 2.
The Union line ran from Culp's Hill southeast of the town, northwest to Cemetery Hill just south of town, then south for nearly two miles (3 km) along Cemetery Ridge, terminating just north of Little Round Top. Most of the XII Corps was on Culp's Hill; the remnants of I and XI Corps defended Cemetery Hill; II Corps covered most of the northern half of Cemetery Ridge; and III Corps was ordered to take up a position to its flank. The shape of the Union line is popularly described as a "fishhook" formation. The Confederate line paralleled the Union line about a mile (1,600 m) to the west on Seminary Ridge, ran east through the town, then curved southeast to a point opposite Culp's Hill. Thus, the Federal army had interior lines, while the Confederate line was nearly five miles (8 km) long.
Lee's battle plan for July 2 called for Longstreet's First Corps to position itself stealthily to attack the Union left flank, facing northeast astraddle the Emmitsburg Road, and to roll up the Federal line. The attack sequence was to begin with Maj. Gens. John Bell Hood's and Lafayette McLaws's divisions, followed by Maj. Gen. Richard H. Anderson's division of Hill's Third Corps. The progressive "en echelon" sequence of this attack would prevent Meade from shifting troops from his center to bolster his left. At the same time, Maj. Gen. Edward "Allegheny" Johnson's and Jubal Early's Second Corps divisions were to make a demonstration against Culp's and Cemetery Hills (again, to prevent the shifting of Federal troops), and to turn the demonstration into a full-scale attack if a favorable opportunity presented itself.
Lee's plan, however, was based on faulty intelligence, exacerbated by Stuart's continued absence from the battlefield. Instead of moving beyond the Federals' left and attacking their flank, Longstreet's left division, under McLaws, would face Maj. Gen. Daniel Sickles's III Corps directly in their path. Sickles had been dissatisfied with the position assigned him on the southern end of Cemetery Ridge. Seeing higher ground more favorable to artillery positions a half mile (800 m) to the west, he advanced his corps—without orders—to the slightly higher ground along the Emmitsburg Road. The new line ran from Devil's Den, northwest to the Sherfy farm's Peach Orchard, then northeast along the Emmitsburg Road to south of the Codori farm. This created an untenable salient at the Peach Orchard; Brig. Gen. Andrew A. Humphreys's division (in position along the Emmitsburg Road) and Maj. Gen. David B. Birney's division (to the south) were subject to attacks from two sides and were spread out over a longer front than their small corps could defend effectively.
Longstreet's attack was to be made as early as practicable; however, Longstreet got permission from Lee to await the arrival of one of his brigades, and while marching to the assigned position, his men came within sight of a Union signal station on Little Round Top. Countermarching to avoid detection wasted much time, and Hood's and McLaws's divisions did not launch their attacks until just after 4 p.m. and 5 p.m., respectively.
Attacks on the Union left flank.
As Longstreet's divisions slammed into the Union III Corps, Meade was forced to send 20,000 reinforcements in the form of the entire V Corps, Brig. Gen. John C. Caldwell's division of the II Corps, most of the XII Corps, and small portions of the newly arrived VI Corps. The Confederate assault deviated from Lee's plan since Hood's division moved more easterly than intended, losing its alignment with the Emmitsburg Road, attacking Devil's Den and Little Round Top. McLaws, coming in on Hood's left, drove multiple attacks into the thinly stretched III Corps in the Wheatfield and overwhelmed them in Sherfy's Peach Orchard. McLaws's attack eventually reached Plum Run Valley (the "Valley of Death") before being beaten back by the Pennsylvania Reserves division of the V Corps, moving down from Little Round Top. The III Corps was virtually destroyed as a combat unit in this battle, and Sickles's leg was amputated after it was shattered by a cannonball. Caldwell's division was destroyed piecemeal in the Wheatfield. Anderson's division, coming from McLaws's left and starting forward around 6 p.m., reached the crest of Cemetery Ridge, but it could not hold the position in the face of counterattacks from the II Corps, including an almost suicidal bayonet charge by the small 1st Minnesota regiment against a Confederate brigade, ordered in desperation by Hancock to buy time for reinforcements to arrive.
As fighting raged in the Wheatfield and Devil's Den, Col. Strong Vincent of V Corps had a precarious hold on Little Round Top, an important hill at the extreme left of the Union line. His brigade of four relatively small regiments was able to resist repeated assaults by Brig. Gen. Evander M. Law's brigade of Hood's division. Meade's chief engineer, Brig. Gen. Gouverneur K. Warren, had realized the importance of this position, and dispatched Vincent's brigade, an artillery battery, and the 140th New York to occupy Little Round Top mere minutes before Hood's troops arrived. The defense of Little Round Top with a bayonet charge by the 20th Maine was one of the most fabled episodes in the Civil War and propelled Col. Joshua L. Chamberlain into prominence after the war.
Attacks on the Union right flank.
About 7:00 p.m., the Second Corps' attack by Johnson's division on Culp's Hill got off to a late start. Most of the hill's defenders, the Union XII Corps, had been sent to the left to defend against Longstreet's attacks, and the only portion of the corps remaining on the hill was a brigade of New Yorkers under Brig. Gen. George S. Greene. Because of Greene's insistence on constructing strong defensive works, and with reinforcements from the I and XI Corps, Greene's men held off the Confederate attackers, although the Southerners did capture a portion of the abandoned Federal works on the lower part of Culp's Hill.
Just at dark, two of Jubal Early's brigades attacked the Union XI Corps positions on East Cemetery Hill where Col. Andrew L. Harris of the 2nd Brigade, 1st Division, came under a withering attack, losing half his men; however, Early failed to support his brigades in their attack, and Ewell's remaining division, that of Maj. Gen. Robert E. Rodes, failed to aid Early's attack by moving against Cemetery Hill from the west. The Union army's interior lines enabled its commanders to shift troops quickly to critical areas, and with reinforcements from II Corps, the Federal troops retained possession of East Cemetery Hill, and Early's brigades were forced to withdraw.
Jeb Stuart and his three cavalry brigades arrived in Gettysburg around noon but had no role in the second day's battle. Brig. Gen. Wade Hampton's brigade fought a minor engagement with newly promoted 23-year-old Brig. Gen. George Armstrong Custer's Michigan cavalry near Hunterstown to the northeast of Gettysburg.
Third day of battle.
General Lee wished to renew the attack on Friday, July 3, using the same basic plan as the previous day: Longstreet would attack the Federal left, while Ewell attacked Culp's Hill. However, before Longstreet was ready, Union XII Corps troops started a dawn artillery bombardment against the Confederates on Culp's Hill in an effort to regain a portion of their lost works. The Confederates attacked, and the second fight for Culp's Hill ended around 11 a.m. Harry Pfanz judged that, after some seven hours of bitter combat, "the Union line was intact and held more strongly than before."
Lee was forced to change his plans. Longstreet would command Pickett's Virginia division of his own First Corps, plus six brigades from Hill's Corps, in an attack on the Federal II Corps position at the right center of the Union line on Cemetery Ridge. Prior to the attack, all the artillery the Confederacy could bring to bear on the Federal positions would bombard and weaken the enemy's line.
Around 1 p.m., from 150 to 170 Confederate guns began an artillery bombardment that was probably the largest of the war. In order to save valuable ammunition for the infantry attack that they knew would follow, the Army of the Potomac's artillery, under the command of Brig. Gen. Henry Jackson Hunt, at first did not return the enemy's fire. After waiting about 15 minutes, about 80 Federal cannons added to the din. The Army of Northern Virginia was critically low on artillery ammunition, and the cannonade did not significantly affect the Union position. Around 3 p.m., the cannon fire subsided, and 12,500 Southern soldiers stepped from the ridgeline and advanced the three-quarters of a mile (1,200 m) to Cemetery Ridge in what is known to history as "Pickett's Charge". As the Confederates approached, there was fierce flanking artillery fire from Union positions on Cemetery Hill and north of Little Round Top, and musket and canister fire from Hancock's II Corps. In the Union center, the commander of artillery had held fire during the Confederate bombardment (in order to save it for the infantry assault, which Meade had correctly predicted the day before), leading Southern commanders to believe the Northern cannon batteries had been knocked out. However, they opened fire on the Confederate infantry during their approach with devastating results. Nearly one half of the attackers did not return to their own lines. Although the Federal line wavered and broke temporarily at a jog called the "Angle" in a low stone fence, just north of a patch of vegetation called the Copse of Trees, reinforcements rushed into the breach, and the Confederate attack was repulsed. The farthest advance of Brig. Gen. Lewis A. Armistead's brigade of Maj. Gen. George Pickett's division at the Angle is referred to as the "High-water mark of the Confederacy", arguably representing the closest the South ever came to its goal of achieving independence from the Union via military victory.
There were two significant cavalry engagements on July 3. Stuart was sent to guard the Confederate left flank and was to be prepared to exploit any success the infantry might achieve on Cemetery Hill by flanking the Federal right and hitting their trains and lines of communications. Three miles (5 km) east of Gettysburg, in what is now called "East Cavalry Field" (not shown on the accompanying map, but between the York and Hanover Roads), Stuart's forces collided with Federal cavalry: Brig. Gen. David McMurtrie Gregg's division and Brig. Gen. Custer's brigade. A lengthy mounted battle, including hand-to-hand sabre combat, ensued. Custer's charge, leading the 1st Michigan Cavalry, blunted the attack by Wade Hampton's brigade, blocking Stuart from achieving his objectives in the Federal rear. Meanwhile, after hearing news of the day's victory, Brig. Gen. Judson Kilpatrick launched a cavalry attack against the infantry positions of Longstreet's Corps southwest of Big Round Top. Brig. Gen. Elon J. Farnsworth protested against the futility of such a move but obeyed orders. Farnsworth was killed in the attack, and his brigade suffered significant losses.
Aftermath.
Casualties.
The two armies suffered between 46,000 and 51,000 casualties. Union casualties were 23,055 (3,155 killed, 14,531 wounded, 5,369 captured or missing), while Confederate casualties are more difficult to estimate. Many authors have referred to as many as 28,000 Confederate casualties, and Busey and Martin's more recent 2005 work, "Regimental Strengths and Losses at Gettysburg", documents 23,231 (4,708 killed, 12,693 wounded, 5,830 captured or missing). Nearly a third of Lee's general officers were killed, wounded, or captured. The casualties for both sides during the entire campaign were 57,225.
The following tables summarize casualties by corps for the Union and Confederate forces during the three-day battle.
Bruce Catton wrote, "The town of Gettysburg looked as if some universal moving day had been interrupted by catastrophe." But there was only one documented civilian death during the battle: Ginnie Wade (also widely known as Jennie), 20 years old, was hit by a stray bullet that passed through her kitchen in town while she was making bread. Nearly 8,000 had been killed outright; these bodies, lying in the hot summer sun, needed to be buried quickly. Over 3,000 horse carcasses were burned in a series of piles south of town; townsfolk became violently ill from the stench.
Confederate retreat.
The armies stared at one another in a heavy rain across the bloody fields on July 4, the same day that the Vicksburg garrison surrendered to Maj. Gen. Ulysses S. Grant. Lee had reformed his lines into a defensive position on Seminary Ridge the night of July 3, evacuating the town of Gettysburg. The Confederates remained on the battlefield, hoping that Meade would attack, but the cautious Union commander decided against the risk, a decision for which he would later be criticized. Both armies began to collect their remaining wounded and bury some of the dead. A proposal by Lee for a prisoner exchange was rejected by Meade.
Lee started his Army of Northern Virginia in motion late the evening of July 4 towards Fairfield and Chambersburg. Cavalry under Brig. Gen. John D. Imboden was entrusted to escort the miles-long wagon train of supplies and wounded men that Lee wanted to take back to Virginia with him, using the route through Cashtown and Hagerstown to Williamsport, Maryland. Meade's army followed, although the pursuit was half-spirited. The recently rain-swollen Potomac trapped Lee's army on the north bank of the river for a time, but when the Federals finally caught up, the Confederates had forded the river. The rear-guard action at Falling Waters on July 14 added some more names to the long casualty lists, including General Pettigrew, who was mortally wounded.
In a brief letter to Maj. Gen. Henry W. Halleck written on July 7, Lincoln remarked on the two major Union victories at Gettysburg and Vicksburg. He continued:
Halleck then relayed the contents of Lincoln's letter to Meade in a telegram. Despite repeated pleas from Lincoln and Halleck, which continued over the next week, Meade did not pursue Lee's army aggressively enough to destroy it before it crossed back over the Potomac River to safety in the South. The campaign continued into Virginia with light engagements until July 23, in the minor Battle of Manassas Gap, after which Meade abandoned any attempts at pursuit and the two armies took up positions across from each other on the Rappahannock River.
Union reaction to the news of the victory.
The news of the Union victory electrified the North. A headline in "The Philadelphia Inquirer" proclaimed "VICTORY! WATERLOO ECLIPSED!" New York diarist George Templeton Strong wrote:
However, the Union enthusiasm soon dissipated as the public realized that Lee's army had escaped destruction and the war would continue. Lincoln complained to Secretary of the Navy Gideon Welles that "Our army held the war in the hollow of their hand and they would not close it!" Brig. Gen. Alexander S. Webb wrote to his father on July 17, stating that such Washington politicians as "Chase, Seward and others," disgusted with Meade, "write to me that Lee really won that Battle!"
Effect on the Confederacy.
The Confederates had lost politically as well as militarily. During the final hours of the battle, Confederate Vice President Alexander Stephens was approaching the Union lines at Norfolk, Virginia, under a flag of truce. Although his formal instructions from Confederate President Jefferson Davis had limited his powers to negotiate on prisoner exchanges and other procedural matters, historian James M. McPherson speculates that he had informal goals of presenting peace overtures. Davis had hoped that Stephens would reach Washington from the south while Lee's victorious army was marching toward it from the north. President Lincoln, upon hearing of the Gettysburg results, refused Stephens's request to pass through the lines. Furthermore, when the news reached London, any lingering hopes of European recognition of the Confederacy were finally abandoned. Henry Adams wrote, "The disasters of the rebels are unredeemed by even any hope of success. It is now conceded that all idea of intervention is at an end."
The immediate reaction of the Southern military and public sectors was that Gettysburg was a setback, not a disaster. The sentiment was that Lee had been successful on July 1 and had fought a valiant battle on July 2–3, but could not dislodge the Union Army from the strong defensive position to which it fled. The Confederates successfully stood their ground on July 4 and withdrew only after they realized Meade would not attack them. The withdrawal to the Potomac that could have been a disaster was handled masterfully. Furthermore, the Army of the Potomac had been kept away from Virginia farmlands for the summer and all predicted that Meade would be too timid to threaten them for the rest of the year. Lee himself had a positive view of the campaign, writing to his wife that the army had returned "rather sooner than I had originally contemplated, but having accomplished what I proposed on leaving the Rappahannock, viz., relieving the Valley of the presence of the enemy and drawing his Army north of the Potomac." He was quoted as saying to Maj. John Seddon, brother of the Confederate secretary of war, "Sir, we did whip them at Gettysburg, and it will be seen for the next six months that "that army" will be as quiet as a sucking dove." Some Southern publications, such as the "Charleston Mercury", criticized Lee's actions in the campaign and on August 8 he offered his resignation to President Davis, who quickly rejected it.
Gettysburg became a postbellum focus of the "Lost Cause", a movement by writers such as Edward A. Pollard and Jubal Early to explain the reasons for the Confederate defeat in the war. A fundamental premise of their argument was that the South was doomed because of the overwhelming advantage in manpower and industrial might possessed by the North. However, they claim it also suffered because Robert E. Lee, who up until this time had been almost invincible, was betrayed by the failures of some of his key subordinates at Gettysburg: Ewell, for failing to seize Cemetery Hill on July 1; Stuart, for depriving the army of cavalry intelligence for a key part of the campaign; and especially Longstreet, for failing to attack on July 2 as early and as forcefully as Lee had originally intended. In this view, Gettysburg was seen as a great lost opportunity, in which a decisive victory by Lee could have meant the end of the war in the Confederacy's favor.
Gettysburg Address.
The ravages of war were still evident in Gettysburg more than four months later when, on November 19, the Soldiers' National Cemetery was dedicated. During this ceremony, President Abraham Lincoln honored the fallen and redefined the purpose of the war in his historic Gettysburg Address.
Historical assessment.
Decisive victory controversies.
The nature of the result of the Battle of Gettysburg has been the subject of controversy for years. Although not seen as overwhelmingly significant at the time, particularly since the war continued for almost two years, in retrospect it has often been cited as the "turning point", usually in combination with the fall of Vicksburg the following day. This is based on the hindsight that, after Gettysburg, Lee's army conducted no more strategic offensives—his army merely reacted to the initiative of Ulysses S. Grant in 1864 and 1865—and by the speculative viewpoint of the Lost Cause writers that a Confederate victory at Gettysburg might have resulted in the end of the war.
It is currently a widely held view that Gettysburg was a decisive victory for the Union, but the term is considered imprecise. It is inarguable that Lee's offensive on July 3 was turned back decisively and his campaign in Pennsylvania was terminated prematurely (although the Confederates at the time argued that this was a temporary setback and that the goals of the campaign were largely met). However, when the more common definition of "decisive victory" is intended—an indisputable military victory of a battle that determines or significantly influences the ultimate result of a conflict—historians are divided. For example, David J. Eicher called Gettysburg a "strategic loss for the Confederacy" and James M. McPherson wrote that "Lee and his men would go on to earn further laurels. But they never again possessed the power and reputation they carried into Pennsylvania those palmy summer days of 1863."
However, Herman Hattaway and Archer Jones wrote that the "strategic impact of the Battle of Gettysburg was ... fairly limited." Steven E. Woodworth wrote that "Gettysburg proved only the near impossibility of decisive action in the Eastern theater." Edwin Coddington pointed out the heavy toll on the Army of the Potomac and that "after the battle Meade no longer possessed a truly effective instrument for the accomplishments of his task. The army needed a thorough reorganization with new commanders and fresh troops, but these changes were not made until Grant appeared on the scene in March 1864." Joseph T. Glatthaar wrote that "Lost opportunities and near successes plagued the Army of Northern Virginia during its Northern invasion," yet after Gettysburg, "without the distractions of duty as an invading force, without the breakdown of discipline, the Army of Northern Virginia [remained] an extremely formidable force." Ed Bearss wrote, "Lee's invasion of the North had been a costly failure. Nevertheless, at best the Army of the Potomac had simply preserved the strategic stalemate in the Eastern Theater ..."
Peter Carmichael refers to the military context for the armies, the "horrendous losses at Chancellorsville and Gettysburg, which effectively destroyed Lee's offensive capacity," implying that these cumulative losses were not the result of a single battle. Thomas Goss, writing in the U.S. Army's "Military Review" journal on the definition of "decisive" and the application of that description to Gettysburg, concludes: "For all that was decided and accomplished, the Battle of Gettysburg fails to earn the label 'decisive battle'." The military historian John Keegan agrees. Gettysburg was a landmark battle, the largest of the war and it would not be surpassed. The Union had restored to it the belief in certain victory, and the loss dispirited the Confederacy. If "not exactly a decisive battle", Gettysburg was the end of Confederate use of Northern Virginia as a military buffer zone, the setting for Grant's Overland Campaign.
Lee vs. Meade.
Prior to Gettysburg, Robert E. Lee had established a reputation as an almost invincible general, achieving stunning victories against superior numbers—although usually at the cost of high casualties to his army—during the Seven Days, the Northern Virginia Campaign (including the Second Battle of Bull Run), Fredericksburg, and Chancellorsville. Only the Maryland Campaign, with its tactically inconclusive Battle of Antietam, had been less than successful. Therefore, historians have attempted to explain how Lee's winning streak was interrupted so dramatically at Gettysburg. Although the issue is tainted by attempts to portray history and Lee's reputation in a manner supporting different partisan goals, the major factors in Lee's loss arguably can be attributed to: (1) Lee's overconfidence in the invincibility of his men; (2) the performance of his subordinates, and his management thereof; (3) health issues, and; (4) the performance of his opponent, George G. Meade, and the Army of the Potomac.
Throughout the campaign, Lee was influenced by the belief that his men were invincible; most of Lee's experiences with the Army of Northern Virginia had convinced him of this, including the great victory at Chancellorsville in early May and the rout of the Union troops at Gettysburg on July 1. Since morale plays an important role in military victory when other factors are equal, Lee did not want to dampen his army's desire to fight and resisted suggestions, principally by Longstreet, to withdraw from the recently captured Gettysburg to select a ground more favorable to his army. War correspondent Peter W. Alexander wrote that Lee "acted, probably, under the impression that his troops were able to carry any position however formidable. If such was the case, he committed an error, such however as the ablest commanders will sometimes fall into." Lee himself concurred with this judgment, writing to President Davis, "No blame can be attached to the army for its failure to accomplish what was projected by me, nor should it be censured for the unreasonable expectations of the public—I am alone to blame, in perhaps expecting too much of its prowess and valor."
The most controversial assessments of the battle involve the performance of Lee's subordinates. The dominant theme of the Lost Cause writers and many other historians is that Lee's senior generals failed him in crucial ways, directly causing the loss of the battle; the alternative viewpoint is that Lee did not manage his subordinates adequately, and did not thereby compensate for their shortcomings. Two of his corps commanders—Richard S. Ewell and A.P. Hill—had only recently been promoted and were not fully accustomed to Lee's style of command, in which he provided only general objectives and guidance to their former commander, Stonewall Jackson; Jackson translated these into detailed, specific orders to his division commanders. All four of Lee's principal commanders received criticism during the campaign and battle:
In addition to Hill's illness, Lee's performance was affected by his own illness, which has been speculated as chest pains due to angina. He wrote to Jefferson Davis that his physical condition prevented him from offering full supervision in the field, and said, "I am so dull that in making use of the eyes of others I am frequently misled."
As a final factor, Lee faced a new and formidable opponent in George G. Meade, and the Army of the Potomac fought well on its home territory. Although new to his army command, Meade deployed his forces relatively effectively; relied on strong subordinates such as Winfield S. Hancock to make decisions where and when they were needed; took great advantage of defensive positions; nimbly shifted defensive resources on interior lines to parry strong threats; and, unlike some of his predecessors, stood his ground throughout the battle in the face of fierce Confederate attacks. Lee was quoted before the battle as saying Meade "would commit no blunders on my front and if I make one ... will make haste to take advantage of it." That prediction proved to be correct at Gettysburg. Stephen Sears wrote, "The fact of the matter is that George G. Meade, unexpectedly and against all odds, thoroughly outgeneraled Robert E. Lee at Gettysburg." Edwin B. Coddington wrote that the soldiers of the Army of the Potomac received a "sense of triumph which grew into an imperishable faith in [themselves]. The men knew what they could do under an extremely competent general; one of lesser ability and courage could well have lost the battle."
Meade had his own detractors as well. Similar to the situation with Lee, Meade suffered partisan attacks about his performance at Gettysburg, but he had the misfortune of experiencing them in person. Supporters of his predecessor, Maj. Gen. Joseph Hooker, lambasted Meade before the U.S. Congress's Joint Committee on the Conduct of the War, where Radical Republicans suspected that Meade was a Copperhead and tried in vain to relieve him from command. Daniel E. Sickles and Daniel Butterfield accused Meade of planning to retreat from Gettysburg during the battle. Most politicians, including Lincoln, criticized Meade for what they considered to be his half-hearted pursuit of Lee after the battle. A number of Meade's most competent subordinates—Winfield S. Hancock, John Gibbon, Gouverneur K. Warren, and Henry J. Hunt, all heroes of the battle—defended Meade in print, but Meade was embittered by the overall experience.
Battlefield preservation.
Today, the Gettysburg National Cemetery and Gettysburg National Military Park are maintained by the U.S. National Park Service as two of the nation's most revered historical landmarks. Although Gettysburg is one of the best known of all Civil War battlefields, it too faces threats to its preservation and interpretation. Many historically significant locations on the battlefield lie outside the boundaries of Gettysburg National Military Park and are vulnerable to residential or commercial development.
On July 20, 2009, a Comfort Inn and Suites opened on Cemetery Hill, adjacent to Evergreen Cemetery, just one of many modern edifices infringing on the historic field. The Baltimore Pike corridor attracts development that concerns preservationists.
Some preservation successes have emerged in recent years. Two proposals to open a casino at Gettysburg were defeated in 2006 and most recently in 2011, when public pressure forced the Pennsylvania Gaming Control Board to reject the proposed gambling hub at the intersection of Routes 15 and 30, near East Cavalry Field. The Civil War Trust also successfully purchased and transferred 95 acres at the former site of the Gettysburg Country Club to the control of the U.S. Department of the Interior in 2011.
Less than half of the over 11,500 acres on the old Gettysburg Battlefield have been preserved for posterity thus far. The Civil War Trust has preserved 815 acres around the site, some of which is now part of the 4,998 acres of Gettysburg National Military Park.
Commemoration in U.S. postage and coinage.
During the Civil War Centennial, the U.S. Post Office issued five postage stamps commemorating the 100th anniversaries of famous battles, as they occurred over a four-year period, beginning with the Battle of Fort Sumter Centennial issue of 1961. The Battle of Shiloh commemorative stamp was issued in 1962, the Battle of Gettysburg in 1963, the Battle of the Wilderness in 1964, and the Appomattox Centennial commemorative stamp in 1965.
A commemorative half dollar for the battle was produced in 1936. As was typical for the period, mintage was very low, just 26,928. On January 24, 2011, the America the Beautiful quarters released a 25-cent coin commemorating Gettysburg National Military Park and the Battle of Gettysburg. The reverse side of the coin depicts the monument on Cemetery Ridge to the 72nd Pennsylvania Infantry.
In popular media.
Film records survive of two Gettysburg reunions, held on the battlefield. At the 50th anniversary (1913), veterans re-enacted Pickett's Charge in a spirit of reconciliation, a meeting that carried great emotional force for both sides. At the 75th anniversary (1938), 2500 veterans attended, and there was a ceremonial mass hand-shake across a stone wall. This was recorded on sound film, and some Confederates can be heard giving the Rebel Yell.
Iced Earth's three-part song cycle "Gettysburg (1863)", published in 2004, dramatizes the battle.
The Battle of Gettysburg was depicted in the 1993 film "Gettysburg", based on Michael Shaara's 1974 novel "The Killer Angels". The film and novel focused primarily on the actions of Joshua Lawrence Chamberlain, John Buford, Robert E. Lee, and James Longstreet during the battle. The first day focused on Buford's cavalry defense, the second day on Chamberlain's defense at Little Round Top, and the third day on Pickett's Charge.
In the 2004 mockumentary "", the Battle of Gettysburg is won by the Confederate forces as a result of politician Judah P. Benjamin successfully convincing the United Kingdom and France to aid the Confederacy. This causes a butterfly effect that sees the Confederacy win the Civil War and subsequently conquer all of North and South America except Canada.

</doc>
<doc id="4851" url="http://en.wikipedia.org/wiki?curid=4851" title="Budweiser">
Budweiser

Budweiser is a pale lager produced by Anheuser–Busch InBev. Introduced in 1876 by Adolphus Busch from Saint Louis, MO it has grown to become one of the highest selling beers in the United States, and is available in over 80 markets worldwidethough, due to a trademark dispute, cannot necessarily do so under the Budweiser name. It is made with up to 30% rice in addition to hops and barley malt. Budweiser is produced in various breweries located around the world. It is a filtered beer available in draught and packaged forms.
History.
Adolphus Busch left Germany for the United States in 1857. He settled in St. Louis, Missouri, where he eventually established his own brewing supply house. In St. Louis, Busch also met and married Lilly Anheuser. Lilly’s father, Eberhard Anheuser, owned a small brewery that had been making lager beer for some time. In 1864, Busch partnered with his father in-law to form what would eventually become the Anheuser-Busch Company.
Busch traveled extensively in Europe to observe and study the latest brewing techniques. In the 1870s, Anheuser-Busch became the first American brewery to implement pasteurization, which greatly improved the shelf-life and transportability of its beers. In the mid-1800s, most Americans preferred robust, dark ales. Busch had encountered lighter lager beers during his travels and began brewing a light Bohemian lager. Anheuser-Busch introduced this lager in 1876 under the brand name Budweiser.
Budweiser and Anheuser-Busch had two decades of growth, before the onset of prohibition in 1920. Anheuser-Busch had to suspend the brewing of Budweiser during prohibition, and launched a range of non-alcoholic products.
When prohibition came to an end in 1933, Anheuser-Busch began brewing Budweiser again. During prohibition, the palate of the beer consumer had changed due to the popularity of sweeter homemade and bootlegged brews. The company dared consumers to drink Budweiser for five days, and if on the sixth day, they still preferred the taste of other beers, they could go back.
Growth was limited by economic conditions in the Great Depression, but thanks in part to the introduction of the metal can in 1936, Budweiser’s sales began to climb again.
During World War II, the company diverted several resources to support the war effort and relinquished its West Coast markets to conserve rail car space. After the war, Budweiser and Anheuser-Busch entered into an era of rapid growth.
August A. Busch Jr. became president of Anheuser-Busch in 1946 and began the creation of a national network of breweries. A new brewery was opened in Newark, New Jersey in 1951, and was the first of nine to open over the next 25 years.
Budweiser is available in over 80 markets.
After the 18 November 2008 InBev takeover, several cost-cutting measures that were implemented have, according to some sources, negatively affected the flavor of the beer. Whole rice grains have now been replaced by broken ones, and the high quality Hallertauer Mittelfrüh hop has been phased out. A former top AB InBev executive says the company saved about $55 million a year substituting cheaper hops in Budweiser and other U.S. beers.
Name origin and dispute.
Anheuser–Busch has been involved in a trademark dispute with European beer companies, in particular Budweiser Budvar Brewery, over the trademark rights to the name "Budweiser".
Beer has been brewed in Budweis since it was founded by king Ottokar II of Bohemia in 1245. The name Budweiser is a derivative adjective, meaning "of Budweis". In 1876, Adolphus Busch and his friend Carl Conrad, a liquor importer, developed a "Bohemian-style" lager, inspired after a trip to the region. Brewers in Bohemia (today's Czech Republic) generally named a beer after their town with the suffix "er". Beers produced in the town of Pilsen (today's Plzeň), for example, were called Pilsners. Busch and Conrad had visited another town, only south of Pilsen, also known for its breweries: Budweis (or Böhmisch Budweis, today's České Budějovice).
In most European countries American Budweiser is now not labelled as Budweiser but as "Bud", and the name Budweiser refers to the original Czech beer, Budweiser Budvar, except for Sweden, Ireland and the United Kingdom, where both beers are sold as Budweiser.
Anheuser-Busch has a market share in the United States of 50.9% for all beers sold. This is primarily composed of Budweiser brands. In 2008 the Belgian-Brazilian beer giant InBev bought the majority of Anheuser-Busch stocks at $70 per share in an all-cash agreement, to create the largest brewing company in the world.
Marketing.
Anheuser-Busch uses what is in many jurisdictions a legally-protected mark-of-origin indicating Czech provenance and humorous advertising campaigns to promote Budweiser, such as the "Real Men of Genius" radio and television commercials for Bud Light.
The Budweiser from Budějovice has been called "The Beer of Kings" since the 16th century. Adolphus Busch changed this slogan to "The King of the Beers". The Czech Budweiser is sold in some countries as "Budejovicky Budvar" but is known as Budweiser in many other countries throughout Europe.
Anheuser-Busch is known for its sport sponsorship, videogame sponsorship ("Tapper", 1983), and humorous advertisements, some of which have entered the popular culture in the United States. They include a long line of TV advertisements in the 1990s featuring three frogs named "Bud", "Weis", and "Er"; lizards impersonating the "Bud-weis-er" frogs, the Budweiser Ants;, a campaign built around the phrase "Whassup?", and a team of Clydesdale horses commonly known as the Budweiser Clydesdales.
The Budweiser Clydesdale has become an iconic horse. In the Clydesdale Donkey commercial the donkey dreams of becoming a Clydesdale horse. The donkey does everything he can think of to try and become one of these horses, and finally when the day comes for his audition, he makes the cut and becomes what he has always dreamed. This commercial ended up being in the top five for the most memorable Budweiser commercials.
The Budweiser brand is promoted in motorsports, from Bernie Little's Miss Budweiser hydroplane boat to sponsorship of the Budweiser King Top Fuel Dragster driven by Brandon Bernstein. Anheuser-Busch has sponsored the CART championship. It is the official beer of NHRA and was the official beer of NASCAR from 1998 until 2007. It has sponsored motorsport events such as the NASCAR Busch Series, Budweiser Speedweeks, Budweiser Shootout, Budweiser Duel, Budweiser Pole Award, Budweiser 500, Budweiser 400, Budweiser 300, Budweiser 250, Budweiser 200 and Carolina Pride / Budweiser 200.
Budweiser has been sponsor of NASCAR teams such as Junior Johnson, Hendrick Motorsports, DEI and Stewart-Haas Racing. Budweiser-sponsored drivers include Terry Labonte (1983, 1987-89), Neil Bonnett (1984-1985), Darrell Waltrip (1984-1986), Geoffrey Bodine (1990-1991), Bill Elliott (1992-1994), Ken Schrader (1995-1996), Dale Earnhardt Jr. (1999-2007), Kasey Kahne (2008-2010) and Kevin Harvick (2011-present).
In IndyCar, Budweiser sponsored Mario Andretti (1983-1984), Bobby Rahal (1985-1988), Scott Pruett (1989-1992), Roberto Guerrero (1993), Scott Goodyear (1994), Paul Tracy (1995), Christian Fittipaldi (1996-1997) and Richie Hearn (1998-1999). Mario Andretti was the 1984 IndyCar champion, whereas Bobby Rahal won the 1987 and 1987 championships and the 1986 Indianapolis 500.
Anheuser-Busch has also sponsored many races, including the Budweiser Shootout, and previously The Bud at the Glen, Budweiser 500, and Budweiser 400. In 2011, Budweiser became the sponsor for Kevin Harvick.
Anheuser-Busch has placed Budweiser as an official partner and sponsor of Major League Soccer and Los Angeles Galaxy and was the headline sponsor of the British Basketball League in the 1990s, taking over from rival company Carlsberg. Anheuser-Busch has also placed Budweiser as an official sponsor of the Premier League and the presenting sponsor of the FA Cup. Budweiser was a proud sponsor and avid advertiser during the celebration of the 1996 Atlanta Olympic Games. For the Centennial Summer Olympics, Budweiser produced many commemorative collectibles and souvenirs that were very popular among crowds at the Games. A primary Budweiser collectible was a 28-ounce Budweiser Atlanta Olympic Games Beer Stein, which featured the Olympic Flame as a decoration on the stein's handle. 
In the early 20th century, the company commissioned a play-on-words song called "Under the Anheuser Bush", which was recorded by several early phonograph companies. Popular music continues to be used in advertisements for Budweiser. Some commercials feature the song "Galvanize" (2005), by The Chemical Brothers.
In August 2009, Anheuser-Busch partnered with popular Chinese video-sharing site, Tudou.com for a user-generated online video contest. The contest encourages users to suggest ideas that include ants for a Bud TV spot set to run in February 2010 during the Chinese New Year.
In 2010, Budweiser launched an international entertainment property called Bud United. Bud United’s first efforts were centered around the 2010 FIFA World Cup in South Africa. The brand launched an online reality TV series, called "Bud House", that followed the lives of 32 international football fans (one representing each nation in the World Cup) living together in a house in South Africa. Bud United’s next project is another reality series called "The Big Time"; each episode focuses on a different vertical market (baseball, soccer, cooking, etc.…) and features contestants competing for a chance to live their dreams. The show was cast through the Bud United Facebook page and aired in Q1 2012.
On November 5, 2012, Anheuser-Busch asked Paramount Pictures to obscure or remove the Budweiser logo from the film "Flight" (2012), directed by Robert Zemeckis and starring Denzel Washington.
Containers and packaging.
Containers.
Over the years, Budweiser has been distributed in many sizes and containers. Until the early 1950s Budweiser was primarily distributed in three packages: kegs, 12-ounce bottles and quart bottles. Cans were first introduced in 1936, which helped sales to climb. In 1955 August Busch Jr. made a strategic move to expand Budweiser's national brand and distributor presence. Along with this expansion came advances in bottling automation, new bottling materials and more efficient distribution methods. These advances brought to market many new containers and package designs. Budweiser is distributed in four large container volumes: half-barrel (15.5 US gallons), quarter-barrel, 1/6 barrel and beer balls (5.2 gallons); and in smaller 7, 8, 10, 12, 16, 18, 22, 24, 32 and 40 US ounce containers. Smaller containers may be made of glass, aluminum or plastic. On August 3, 2011, Budweiser announced its twelfth can design since 1936, one which emphasizes the bowtie.
Packages are sometimes tailored to local customs and traditions. In St. Mary's County, Maryland, ten ounce cans are the preferred package. Budweiser drinkers in the western stretches of Ottawa County, Michigan prefer the eight ounce can. This Ottawa County preference for the eight ounce can may stem from a long-standing blue law held in many Western Michigan cities that prohibit sale of beer and wine on Sundays. In response to this blue law, brewers and distributors presented the eight ounce can as a smaller alternative.
Anheuser-Busch has introduced many can designs with co-branding and sports marketing promotional packaging. Today, most of these promotional programs are represented only on the 16 ounce aluminum bottle container. However, many MLB and NFL teams in the United States also promote 24 ounce cans marked with team logos.
Due to a dispute with the European Budweiser, American Budweiser is sold in most of European Union under the brand "Bud".
Bottle.
The Budweiser bottle has remained relatively unchanged since its introduction in 1876. The top label is red and currently reads "Budweiser". The top of the main label is red with a white banner with a pledge on it, which has changed three times. Below the banner is a coat of arms of sorts, which features an Anheuser-Busch stylization. Below that is a large white box.
Cans.
During the Prohibition Era, Budweiser encountered its first major obstacle to profit and growth. As alcohol became illegal to sell and produce, all alcohol companies, including Budweiser, struggled to remain profitable. Budweiser began producing non-alcoholic beverages during Prohibition to counter its ill effects. Prohibition began in 1920, and lasted into the middle of the Great Depression in 1933. These are two major setbacks that this company experienced. Just as laws regarding prohibition were repealed, Budweiser was faced with more serious economic struggles, which made their success (like all other companies) very unlikely. In attempt to re-stimulate interest in their beer, Budweiser executed a hugely successful marketing strategy of introducing beer cans for the first time in 1936. This new packaging led to an increase in sales which lasted until the start of World War II in 1939.
Over the years, Budweiser has undergone various design changes to its can which are discussed in the table below. Many of these changes are in response to market conditions and consumer tastes. Since 1936, 12 major can design changes have occurred, not including the temporary special edition designs. The table below describes each of the 12 major can designs.
2011 can design marketing strategy.
Traditional Budweiser cans embodied a strong sense of American patriotism. They were red, white, and blue in color, and had images of eagles near the label. These cans also had a very classic appearance due to the cursive font and the Anheuser-Busch logo surrounded by wheat and barley. Red, white, and blue, being the colors of the American flag, identified the cans strongly with the American market for this product. Moreover, the eagles remind viewers of the Bald Eagle, America's national bird. These facts strongly suggest that Budweiser was targeting a primarily American market. In contrast to this previous design, the newest can takes on a much more modern appearance and eliminates blue as a main color in the design. Red, white, and gold are the predominant colors on this new can, which makes the can look much less traditionally American, and much more modern. On this new can, there are aspects that continue to resonate with an American market, such as the red and white colors, as these are two of the nation's primary colors. The presence of the yellowish-gold color is not typically American, though, and is surprising to see in the can design of the self-proclaimed "Great American Lager."
This change in can design was very shocking for a brand that had remained relatively static since its beginning. The new design was largely in response to the huge sales decline in recent years that is threatening to lose Budweiser its title as the best-selling beer in America. One cause of this sales decline is the unemployment struggles that many Americans have been facing recently. A large group of individuals affected by unemployment are young men, who are the main demographic of Budweiser's target market. In order to regain the domestic market share that Budweiser has lost, the company is trying to update its appearance by giving the can a more contemporary look that is more appealing to this demographic. The company hopes that the new design will offset the effects that unemployment had on its sales.
Although the more modern design is intended for young male Americans, the new design, according to the vice president of Anheuser-Busch Frank Abernante, "is one of many steps in our quest to reinforce Budweiser's role as a truly global beer brand." This statement means that the new design was intended for foreign markets as well. In fact, Budweiser began selling its beer in Russia in 2010, and is currently expanding its operations in China.
Statistics show that China is the world's leading consumer of beer in terms of volume, which suggests that Budweiser is trying to capitalize on this market's potential in order to recover from these losses. Currently, of the 15 Anheuser-Busch breweries outside of the United States, 14 of them are positioned in China, which shows how interested the company is in this growing market. Budweiser is already the fourth leading brand in the Chinese beer market, and the company hopes that this new design change will push them further up the list as one of China's most dominant brands. Budweiser's attempt to target China is not sudden, though. There have actually been several advertising campaigns geared toward Chinese consumers. One such campaign was in 2009 when Budweiser sponsored a competition for the creation of a Budweiser commercial involving the Budweiser ants. Here, Chinese consumers were prompted to compete in this process of submitting their ideas online for the chance to have their idea become the newest Budweiser commercial.
The company hoped that the new design would appeal to a younger crowd in the American market, while simultaneously gaining the interest of Chinese consumers. Americans can continue to identify with the brand because the can is primarily red and white, which are dominant colors on the American flag. In addition, Chinese consumers can identify with the brand because the red and gold colors in the new design are reminiscent of the Chinese flag. This demonstrates how Budweiser is using the can to break into new markets with hopes to improve the company's overall revenue.
Beer.
Budweiser is brewed using barley malt, rice, water, hops and yeast. It is lagered with beechwood chips in the ageing vessel which, according to Anheuser-Busch, creates a smoother taste. While beechwood chips are used in the maturation tank, there is little to no flavor contribution from the wood, mainly because they are boiled in sodium bicarbonate [baking soda] for seven hours for the very purpose of removing any flavor from the wood. The maturation tanks that Anheuser-Busch uses are horizontal and, as such, flocculation of the yeast occurs much more quickly. Anheuser-Busch refers to this process as a secondary fermentation, with the idea being that the chips give the yeast more surface area to rest on. This is also combined with a krausening procedure that re-introduces wort into the chip tank therefore activating the fermentation process again. By placing chips at the bottom of the tank, the yeast remains in suspension longer, giving it more time to reabsorb and process green beer flavors, such as acetaldehyde and diacetyl, that Anheuser-Busch believes are off-flavors which detract from overall drinkability.
Some drinkers prefer the lightness of beers like Budweiser and consume it as a refreshment or for its inebriating effects. Several beer writers consider it to be bland. The beer is light-bodied with faint sweet notes and negligible bitterness, leading to reviews characterizing it as a "...beer of underwhelming blandness". Even Adolphus Busch didn't like it. Based upon sales, it is the second most popular American brewed pale lager among North American beer consumers.
Budweiser and "Bud Light" are sometimes advertised as vegan beers, in that their ingredients and conditioning do not use animal by-products. Some might object to the inclusion of genetically engineered rice and animal products used in the brewing process. In July 2006, Anheuser-Busch brewed a version of Budweiser with organic rice, for sale in Mexico. They have yet to extend this practice to any other countries.
Anheuser-Busch was one of the few breweries during Prohibition that had the resources and wherewithal to convert to "cereal beer" production—malt beverage made with non-fermentables such as rice and unmalted barley and rye, and able to stay under the 0.5% limit established by the Volstead Act. Following the repeal of Prohibition in 1933, the major breweries continued to use unmalted cereal grains to provide the full body and mouthfeel of a "real" beer while keeping the alcohol content low.
Budweiser brands.
In addition to the regular Budweiser, Anheuser-Busch brews several different beers under the Budweiser brand, including "Bud Light" and "Bud Ice".
In July 2010 Anheuser-Busch launched Budweiser 66 in the United Kingdom. Budweiser Brew No.66 has 4% alcohol by volume, and is brewed and distributed in the UK by Inbev UK Limited.

</doc>
<doc id="4854" url="http://en.wikipedia.org/wiki?curid=4854" title="Bermuda Triangle">
Bermuda Triangle

The Bermuda Triangle, also known as the Devil's Triangle, is a loosely defined region in the western part of the North Atlantic Ocean, where a number of aircraft and ships are said to have disappeared under mysterious circumstances. According to the US Navy, the triangle does not exist, and the name is not recognized by the US Board on Geographic Names. Popular culture has attributed various disappearances to the paranormal or activity by extraterrestrial beings. Documented evidence indicates that a significant percentage of the incidents were spurious, inaccurately reported, or embellished by later authors. In a 2013 study, the World Wide Fund for Nature identified the world’s 10 most dangerous waters for shipping, but the Bermuda Triangle was not among them.
Triangle area.
The first written boundaries date from an article by Vincent Gaddis in a 1964 issue of the pulp magazine "Argosy", where the triangle's three vertices are in Miami, Florida peninsula; in San Juan, Puerto Rico; and in the mid-Atlantic island of Bermuda. But subsequent writers did not follow this definition. Some writers give different boundaries and vertices to the triangle, with the total area varying from . Consequently, the determination of which accidents have occurred inside the triangle depends on which writer reports them. The United States Board on Geographic Names does not recognize this name, and it is not delimited in any map drawn by US government agencies.
The area is one of the most heavily traveled shipping lanes in the world, with ships crossing through it daily for ports in the Americas, Europe, and the Caribbean Islands. Cruise ships are also plentiful, and pleasure craft regularly go back and forth between Florida and the islands. It is also a heavily flown route for commercial and private aircraft heading towards Florida, the Caribbean, and South America from points north.
History.
Origins.
The earliest allegation of unusual disappearances in the Bermuda area appeared in a September 17, 1950 article published in "The Miami Herald" (Associated Press) by Edward Van Winkle Jones. Two years later, "Fate" magazine published "Sea Mystery at Our Back Door", a short article by George X. Sand covering the loss of several planes and ships, including the loss of Flight 19, a group of five U.S. Navy TBM Avenger bombers on a training mission. Sand's article was the first to lay out the now-familiar triangular area where the losses took place. Flight 19 alone would be covered again in the April 1962 issue of "American Legion" magazine. In it, author Allan W. Eckert wrote that the flight leader had been heard saying, "We are entering white water, nothing seems right. We don't know where we are, the water is green, no white." He also wrote that officials at the Navy board of inquiry stated that the planes "flew off to Mars." Sand's article was the first to suggest a supernatural element to the Flight 19 incident. In the February 1964 issue of "Argosy", Vincent Gaddis' article "The Deadly Bermuda Triangle" argued that Flight 19 and other disappearances were part of a pattern of strange events in the region. The next year, Gaddis expanded this article into a book, "Invisible Horizons".
Others would follow with their own works, elaborating on Gaddis' ideas: John Wallace Spencer ("Limbo of the Lost", 1969, repr. 1973); Charles Berlitz ("The Bermuda Triangle", 1974); Richard Winer ("The Devil's Triangle", 1974), and many others, all keeping to some of the same supernatural elements outlined by Eckert.
Larry Kusche.
Lawrence David Kusche, a research librarian from Arizona State University and author of "The Bermuda Triangle Mystery: Solved" (1975) argued that many claims of Gaddis and subsequent writers were often exaggerated, dubious or unverifiable. Kusche's research revealed a number of inaccuracies and inconsistencies between Berlitz's accounts and statements from eyewitnesses, participants, and others involved in the initial incidents. Kusche noted cases where pertinent information went unreported, such as the disappearance of round-the-world yachtsman Donald Crowhurst, which Berlitz had presented as a mystery, despite clear evidence to the contrary. Another example was the ore-carrier recounted by Berlitz as lost without trace three days out of an "Atlantic" port when it had been lost three days out of a port with the same name in the "Pacific" Ocean. Kusche also argued that a large percentage of the incidents that sparked allegations of the Triangle's mysterious influence actually occurred well outside it. Often his research was simple: he would review period newspapers of the dates of reported incidents and find reports on possibly relevant events like unusual weather, that were never mentioned in the disappearance stories.
Kusche concluded that:
Further responses.
When the UK Channel 4 television program "The Bermuda Triangle" (1992) was being produced by John Simmons of Geofilms for the "Equinox" series, the marine insurance market Lloyd's of London was asked if an unusually large number of ships had sunk in the Bermuda Triangle area. Lloyd's determined that large numbers of ships had not sunk there. Lloyd's does not charge higher rates for passing through this area. United States Coast Guard records confirm their conclusion. In fact, the number of supposed disappearances is relatively insignificant considering the number of ships and aircraft that pass through on a regular basis.
The Coast Guard is also officially skeptical of the Triangle, noting that they collect and publish, through their inquiries, much documentation contradicting many of the incidents written about by the Triangle authors. In one such incident involving the 1972 explosion and sinking of the tanker , the Coast Guard photographed the wreck and recovered several bodies, in contrast with one Triangle author's claim that all the bodies had vanished, with the exception of the captain, who was found sitting in his cabin at his desk, clutching a coffee cup. In addition, "V. A. Fogg" sank off the coast of Texas, nowhere near the commonly accepted boundaries of the Triangle.
The NOVA/Horizon episode "The Case of the Bermuda Triangle", aired on June 27, 1976, was highly critical, stating that "When we've gone back to the original sources or the people involved, the mystery evaporates. Science does not have to answer questions about the Triangle because those questions are not valid in the first place ... Ships and planes behave in the Triangle the same way they behave everywhere else in the world."
David Kusche pointed out a common problem with many of the Bermuda Triangle stories and theories: "Say I claim that a parrot has been kidnapped to teach aliens human language and I challenge you to prove that is "not" true. You can even use Einstein's Theory of Relativity if you like. There is simply no way to prove such a claim untrue. The burden of proof should be on the people who make these statements, to show where they got their information from, to see if their conclusions and interpretations are valid, and if they have left anything out." Skeptical researchers, such as Ernest Taves and Barry Singer, have noted how mysteries and the paranormal are very popular and profitable. This has led to the production of vast amounts of material on topics such as the Bermuda Triangle. They were able to show that some of the pro-paranormal material is often misleading or inaccurate, but its producers continue to market it. Accordingly, they have claimed that the market is biased in favor of books, TV specials, and other media that support the Triangle mystery, and against well-researched material if it espouses a skeptical viewpoint. Finally, if the Triangle is assumed to cross land, such as parts of Puerto Rico, the Bahamas, or Bermuda itself, there is no evidence for the disappearance of any land-based vehicles or persons. The city of Freeport, located inside the Triangle, operates a major shipyard and an airport that handles 50,000 flights annually and is visited by over a million tourists a year.
Supernatural explanations.
Triangle writers have used a number of supernatural concepts to explain the events. One explanation pins the blame on leftover technology from the mythical lost continent of Atlantis. Sometimes connected to the Atlantis story is the submerged rock formation known as the Bimini Road off the island of Bimini in the Bahamas, which is in the Triangle by some definitions. Followers of the purported psychic Edgar Cayce take his prediction that evidence of Atlantis would be found in 1968 as referring to the discovery of the Bimini Road. Believers describe the formation as a road, wall, or other structure, but the Bimini Road is of natural origin.
Other writers attribute the events to UFOs. This idea was used by Steven Spielberg for his science fiction film "Close Encounters of the Third Kind", which features the lost Flight 19 aircrews as alien abductees.
Charles Berlitz, author of various books on anomalous phenomena, lists several theories attributing the losses in the Triangle to anomalous or unexplained forces.
Natural explanations.
Compass variations.
Compass problems are one of the cited phrases in many Triangle incidents. While some have theorized that unusual local magnetic anomalies may exist in the area, such anomalies have not been found. Compasses have natural magnetic variations in relation to the magnetic poles, a fact which navigators have known for centuries. Magnetic (compass) north and geographic (true) north are only exactly the same for a small number of places – for example, as of 2000 in the United States only those places on a line running from Wisconsin to the Gulf of Mexico. But the public may not be as informed, and think there is something mysterious about a compass "changing" across an area as large as the Triangle, which it naturally will.
Gulf Stream.
The Gulf Stream is a deep ocean current that originates in the Gulf of Mexico and then flows through the Straits of Florida into the North Atlantic. In essence, it is a river within an ocean, and, like a river, it can and does carry floating objects. It has a surface velocity of up to about 2.5 metres per second (5.6 mi/h). A small plane making a water landing or a boat having engine trouble can be carried away from its reported position by the current.
Human error.
One of the most cited explanations in official inquiries as to the loss of any aircraft or vessel is human error. Human stubbornness may have caused businessman Harvey Conover to lose his sailing yacht, the "Revonoc", as he sailed into the teeth of a storm south of Florida on January 1, 1958.
Violent weather.
Tropical cyclones are powerful storms, which form in tropical waters and have historically cost thousands of lives lost and caused billions of dollars in damage. The sinking of Francisco de Bobadilla's Spanish fleet in 1502 was the first recorded instance of a destructive hurricane. These storms have in the past caused a number of incidents related to the Triangle.
A powerful downdraft of cold air was suspected to be a cause in the sinking of the "Pride of Baltimore" on May 14, 1986. The crew of the sunken vessel noted the wind suddenly shifted and increased velocity from to . A National Hurricane Center satellite specialist, James Lushine, stated "during very unstable weather conditions the downburst of cold air from aloft can hit the surface like a bomb, exploding outward like a giant squall line of wind and water." A similar event occurred to the "Concordia" in 2010 off the coast of Brazil.
Methane hydrates.
An explanation for some of the disappearances has focused on the presence of large fields of methane hydrates (a form of natural gas) on the continental shelves. Laboratory experiments carried out in Australia have proven that bubbles can, indeed, sink a scale model ship by decreasing the density of the water; any wreckage consequently rising to the surface would be rapidly dispersed by the Gulf Stream. It has been hypothesized that periodic methane eruptions (sometimes called "mud volcanoes") may produce regions of frothy water that are no longer capable of providing adequate buoyancy for ships. If this were the case, such an area forming around a ship could cause it to sink very rapidly and without warning.
Publications by the USGS describe large stores of undersea hydrates worldwide, including the Blake Ridge area, off the coast of the southeastern United States. However, according to the USGS, no large releases of gas hydrates are believed to have occurred in the Bermuda Triangle for the past 15,000 years.
Notable incidents.
"Ellen Austin".
The "Ellen Austin" supposedly came across a derelict ship, placed on board a prize crew, and attempted to sail with it to New York in 1881. According to the stories, the derelict disappeared; others elaborating further that the derelict reappeared minus the prize crew, then disappeared again with a second prize crew on board. A check from Lloyd's of London records proved the existence of the "Meta", built in 1854 and that in 1880 the "Meta" was renamed "Ellen Austin". There are no casualty listings for this vessel, or any vessel at that time, that would suggest a large number of missing men were placed on board a derelict that later disappeared.
USS "Cyclops".
The incident resulting in the single largest loss of life in the history of the US Navy not related to combat occurred when the collier USS "Cyclops", carrying a full load of manganese ore and with one engine out of action, went missing without a trace with a crew of 309 sometime after March 4, 1918, after departing the island of Barbados. Although there is no strong evidence for any single theory, many independent theories exist, some blaming storms, some capsizing, and some suggesting that wartime enemy activity was to blame for the loss. In addition, two of "Cyclops"s sister ships, and were subsequently lost in the North Atlantic during World War II. Both ships were transporting heavy loads of metallic ore similar to that which was loaded on "Cyclops" during her fatal voyage. In all three cases structural failure due to overloading with a much denser cargo than designed is considered the most likely cause of sinking.
"Carroll A. Deering".
A five-masted schooner built in 1919, the "Carroll A. Deering" was found hard aground and abandoned at Diamond Shoals, near Cape Hatteras, North Carolina, on January 31, 1921. Rumors and more at the time indicated the "Deering" was a victim of piracy, possibly connected with the illegal rum-running trade during Prohibition, and possibly involving another ship, , which disappeared at roughly the same time. Just hours later, an unknown steamer sailed near the lightship along the track of the "Deering", and ignored all signals from the lightship. It is speculated that "Hewitt" may have been this mystery ship, and possibly involved in the "Deering" crew's disappearance.
Flight 19.
Flight 19 was a training flight of five TBM Avenger torpedo bombers that disappeared on December 5, 1945, while over the Atlantic. The squadron's flight plan was scheduled to take them due east from Fort Lauderdale for 141 miles, north for 73 miles, and then back over a final 140-mile leg to complete the exercise. The flight never returned to base. The disappearance is attributed by Navy investigators to navigational error leading to the aircraft running out of fuel.
One of the search and rescue aircraft deployed to look for them, a PBM Mariner with a 13-man crew, also disappeared. A tanker off the coast of Florida reported seeing an explosion and observing a widespread oil slick when fruitlessly searching for survivors. The weather was becoming stormy by the end of the incident. According to contemporaneous sources the Mariner had a history of explosions due to vapour leaks when heavily loaded with fuel, as for a potentially long search and rescue operation.
"Star Tiger" and "Star Ariel".
G-AHNP "Star Tiger" disappeared on January 30, 1948, on a flight from the Azores to Bermuda; G-AGRE "Star Ariel" disappeared on January 17, 1949, on a flight from Bermuda to Kingston, Jamaica. Both were Avro Tudor IV passenger aircraft operated by British South American Airways. Both planes were operating at the very limits of their range and the slightest error or fault in the equipment could keep them from reaching the small island. One plane was not heard from long before it would have entered the Triangle.
Douglas DC-3.
On December 28, 1948, a Douglas DC-3 aircraft, number NC16002, disappeared while on a flight from San Juan, Puerto Rico, to Miami. No trace of the aircraft or the 32 people on board was ever found. From the documentation compiled by the Civil Aeronautics Board investigation, a possible key to the plane's disappearance was found, but barely touched upon by the Triangle writers: the plane's batteries were inspected and found to be low on charge, but ordered back into the plane without a recharge by the pilot while in San Juan. Whether or not this led to complete electrical failure will never be known. However, since piston-engined aircraft rely upon magnetos to provide spark to their cylinders rather than a battery powered ignition coil system, this theory is not strongly convincing.
KC-135 Stratotankers.
On August 28, 1963, a pair of US Air Force KC-135 Stratotanker aircraft collided and crashed into the Atlantic. The Triangle version (Winer, Berlitz, Gaddis) of this story specifies that they did collide and crash, but there were two distinct crash sites, separated by over of water. However, Kusche's research showed that the unclassified version of the Air Force investigation report stated that the debris field defining the second "crash site" was examined by a search and rescue ship, and found to be a mass of seaweed and driftwood tangled in an old buoy.
"Connemara IV".
A pleasure yacht was found adrift in the Atlantic south of Bermuda on September 26, 1955; it is usually stated in the stories (Berlitz, Winer) that the crew vanished while the yacht survived being at sea during three hurricanes. The 1955 Atlantic hurricane season shows Hurricane Ione passing nearby between the 14th and 18th of that month, with Bermuda being affected by winds of almost gale force. In his second book on the Bermuda Triangle, Winer quoted from a letter he had received from Mr J.E. Challenor of Barbados:
On the morning of September 22 "Connemara IV" was lying to a heavy mooring in the open roadstead of Carlisle Bay. Because of the approaching hurricane, the owner strengthened the mooring ropes and put out two additional anchors. There was little else he could do, as the exposed mooring was the only available anchorage.
In Carlisle Bay, the sea in the wake of Hurricane Janet was awe-inspiring and dangerous. The owner of "Connemara IV" observed that she had disappeared. An investigation revealed that she had dragged her moorings and gone to sea.
References.
The incidents cited above, apart from the official documentation, come from the following works. Some incidents mentioned as having taken place within the Triangle are found "only" in these sources:
Further reading.
Newspaper articles.
ProQuest has newspaper source material for many incidents, archived in Portable Document Format (PDF). The newspapers include "The New York Times", "The Washington Post", and "The Atlanta Constitution". To access this website, registration is required, usually through a library connected to a college or university.
Flight 19
SS "Cotopaxi"
USS "Cyclops" (AC-4)
"Carroll A. Deering"
Wreckers
S.S. "Suduffco"
"Star Tiger" and "Star Ariel"
DC-3 Airliner NC16002 disappearance
Harvey Conover and "Revonoc"
KC-135 Stratotankers
B-52 Bomber ("Pogo 22")
Charter vessel "Sno'Boy"
SS "Marine Sulphur Queen"
SS "Sylvia L. Ossa"
Website links.
The following websites have either online material that supports the popular version of the Bermuda Triangle, or documents published from official sources as part of hearings or inquiries, such as those conducted by the United States Navy or United States Coast Guard. Copies of some inquiries are not online and may have to be ordered; for example, the losses of Flight 19 or USS Cyclops can be ordered direct from the United States Naval Historical Center.
Books.
Most of the works listed here are largely out of print. Copies may be obtained at your local library, or purchased used at bookstores, or through eBay or Amazon.com. These books are often the "only" source material for some of the incidents that have taken place within the Triangle.

</doc>
<doc id="4856" url="http://en.wikipedia.org/wiki?curid=4856" title="Borough">
Borough

A borough is an administrative division in various countries. In principle, the term "borough" designates a self-governing township although, in practice, official use of the term varies widely.
The word "borough" derives from common Germanic "*burg", meaning "fort": compare with "bury" (England), "burgh" (Scotland), "Burg" (Germany), "borg" (Scandinavia), "pori" (Finland), "burcht" (Dutch), and the Germanic borrowing present in neighbouring Indo-european languages such as "borgo" (Italian), "bourg" (French), "burgo" (Spanish and Portuguese), "burg" (Romanian), "purg" (Kajkavian) and "durg" (दर्ग) (Hindi) and "arg"(ارگ)(Persian). The incidence of these words as suffixes to place names (for example, Canterbury, Strasbourg, Luxembourg, Edinburgh, Hamburg, Gothenburg) usually indicates that they were once fortified settlements.
In the Middle Ages, boroughs were settlements in England that were granted some self-government; burghs were the Scottish equivalent. In medieval England, boroughs were also entitled to elect members of parliament. The use of the word "borough" probably derives from the burghal system of Alfred the Great. Alfred set up a system of defensive strong points (Burhs); in order to maintain these settlements, he granted them a degree of autonomy. After the Norman Conquest, when certain towns were granted self-governance, the concept of the burh/borough seems to have been reused to mean a self-governing settlement.
The concept of the borough has been used repeatedly (and often differently) throughout the world. Often, a borough is a single town with its own local government. However, in some cities it is a subdivision of the city (for example, New York City, London and Montreal). In such cases, the borough will normally have either limited powers delegated to it by the city's local government, or no powers at all. In other places, such as Alaska, "borough" designates a whole region; Alaska's largest borough, the North Slope Borough, is comparable in area to the entire United Kingdom, although its population is less than that of Swanage. In Australia, a "borough" was once a self-governing small town, but this designation has all but vanished, except for the only remaining borough in the country, which is the Borough of Queenscliffe.
Boroughs as administrative units are to be found in Ireland and the United Kingdom, more specifically in England and Northern Ireland. Boroughs also exist in the Canadian province of Quebec and formerly in Ontario, in some states of the United States, in Israel, formerly in New Zealand and only one left in Australia.
Etymology.
The word "borough" derives from the Old English word "burh", meaning a fortified settlement. Other English derivatives of "burh" include "bury" and "brough". There are obvious cognates in other Indo-European languages. For example; "burgh" in Scots and Middle English; "burg" in German and Old English, "borg" in Scandinavian languages; "parcus" in Latin and "pyrgos" in Greek.
A number of other European languages have cognate words that were borrowed from the Germanic languages during the Middle Ages, including "brog" in Irish, "bwr" or "bwrc", meaning "wall, rampart" in Welsh, "bourg" in French, "burg" in Catalan (in Catalonia there is a town named "Burg"), "borgo" in Italian, and "burgo" in Spanish (hence the place-name Burgos).
The 'burg' element is often confused with 'berg' meaning hill or mountain (cf. iceberg). Hence the 'berg' element in Bergen relates to a hill, rather than a fort. In some cases, the 'berg' element in place names has converged towards burg/borough; for instance Farnborough, from "fernaberga" (fern-hill).
Pronunciation.
In many parts of England, "borough" is pronounced as an independent word, and as when a suffix of a place-name. As a suffix, it is sometimes spelled "-brough".
In the United States, "borough" is pronounced or . When appearing as the suffix "-burg(h)" in place-names, it is pronounced .
Uses of "borough".
England and Wales.
Ancient and municipal boroughs.
During the medieval period many towns were granted self-governance by the Crown, at which point they became referred to as boroughs. The formal status of borough came to be conferred by Royal Charter. These boroughs were generally governed by a self-selecting corporation (i.e., when a member died or resigned his replacement would be by co-option). Sometimes boroughs were governed by bailiffs or headboroughs.
Debates on the Reform Bill (eventually the Reform Act 1832) had highlighted the variations in systems of governance of towns, and a Royal Commission was set up to investigate the issue. This resulted in a regularisation of municipal government (Municipal Corporations Act 1835). 178 of the ancient boroughs were reformed as "municipal boroughs", with all municipal corporations to be elected according to a standard franchise based on property ownership. The unreformed boroughs either lapsed in borough status, or were reformed (or abolished) at a later time. Several new municipal boroughs were formed in the new industrial cities after the bill enacted, according to the provisions of the bill.
As part of a large-scale reform of local government in England and Wales in 1974, municipal boroughs were finally abolished (having become increasingly irrelevant). However, the civic traditions of many boroughs were continued by the grant of a charter to their successor district councils. In smaller boroughs, a town council was formed for the area of the abolished borough, while charter trustees were formed in other former boroughs. In each case, the new body was allowed to use the regalia of the old corporation, and appoint ceremonial office holders such as sword and mace bearers as provided in their original charters. The council or trustees may apply for an Order in Council or Royal Licence to use the former borough coat of arms.
Parliamentary boroughs.
From 1265, two burgesses from each borough were summoned to the Parliament of England, alongside two knights from each county. Thus parliamentary constituencies were derived from the ancient boroughs. Representation in the House of Commons was decided by the House itself, which resulted in boroughs' being established in some small settlements for the purposes of parliamentary representation, despite their possessing no actual corporation.
After the Reform Act, which disenfranchised many of the rotten boroughs (boroughs that had declined in importance, had only a small population, and had only a handful of eligible voters), parliamentary constituencies began to diverge from the ancient boroughs. While many ancient boroughs remained as municipal boroughs, they were disenfranchised by the Reform Act.
County boroughs.
The Local Government Act 1888 established a new sort of borough – the county borough. These were designed to be 'counties-to-themselves'; administrative divisions to sit alongside the new administrative counties. They allowed urban areas to be administered separately from the more rural areas. They, therefore, often contained pre-existing municipal boroughs, which thereafter became part of the second tier of local government, below the administrative counties and county boroughs.
The county boroughs were, like the municipal boroughs, abolished in 1974, being reabsorbed into their parent counties for administrative purposes.
Metropolitan boroughs.
In 1899, as part of a reform of local government in the County of London, the various parishes in London were reorganised as new entities, the 'metropolitan boroughs'. These were reorganised further when Greater London was formed out of Middlesex and the County of London in 1965.
When the new metropolitan counties (Greater Manchester, Merseyside, Tyne & Wear, West Midlands, South Yorkshire and West Yorkshire) were created in 1974, their sub-divisions also became metropolitan boroughs; in many cases these metropolitan boroughs recapitulated abolished county boroughs (for example, Stockport). The metropolitan boroughs possessed slightly more autonomy from the metropolitan county councils than the shire county districts did from their county councils.
With the abolition of the metropolitan county councils in 1986, these metropolitan boroughs became independent, and continue to be so at present.
Other current uses.
Elsewhere in England a number of districts and unitary authority areas are called "borough". Until 1974, this was a status that denoted towns with a certain type of local government (a municipal corporation). Since 1974, it has been a purely ceremonial style granted by royal charter to districts which may consist of a single town or may include a number of towns or rural areas. Borough status entitles the council chairman to bear the title of mayor. Districts may apply to the British Crown for the grant of borough status upon advice of the Privy Council of the United Kingdom.
Northern Ireland.
In Northern Ireland, local government was reorganised in 1973. Under the legislation that created the 26 districts of Northern Ireland, a district council whose area included an existing municipal borough could resolve to adopt the charter of the old municipality and thus continue to enjoy borough status. Districts that do not contain a former borough can apply for a charter in a similar manner to English districts.
Canada.
In Quebec, the term borough is generally used as the English translation of arrondissement, referring to an administrative division of a municipality. Only eight municipalities in Quebec are divided into boroughs. See List of boroughs in Quebec.
It was previously used in Metropolitan Toronto, Ontario, to denote suburban municipalities including Scarborough, York, North York, Etobicoke prior to their conversion into cities. The Borough of East York was the last Toronto municipality to hold this status, relinquishing it upon becoming part of the City of Toronto on January 1, 1998.
United States.
In the United States, a borough is a unit of local government below the level of the state. The term is currently used in seven states.
The following states use, or have used, the word with the following meanings:
Certain names of places, such as Hillsboro, OR, Greensboro, NC, Tyngsborough, MA, and Maynesborough, NH reflect the historical use of Borough as a geographical scale in the United States.
Mexico.
In Mexico as translations from English to Spanish applied to Mexico City, the word "borough" has resulted in a delegación (delegation), referring to the 16 administrative areas within the Mexican Federal District. Also the municipalities of some states are administratively subdivided into boroughs, as showed in Municipality of Mexicali#Boroughs. (see: Boroughs of Mexico and Boroughs of the Mexican Federal District)
Australia.
In Australia, the term "borough" is an occasionally used term for a local government area. Currently there is only one borough in Australia, the Borough of Queenscliffe in Victoria, although there have been more in the past. However in some cases it can be integrated into the councils name instead of used as an official title, such as the Municipality of Kingborough in Tasmania.
Republic of Ireland.
The Local Government Reform Act 2014 replaced the urban-only second-tier local government units with new urban and rural units termed "municipal districts". The abolished units included five which were termed "boroughs", namely Clonmel, Drogheda, Kilkenny, Sligo, and Wexford. However, the municipal districts containing four of these are styled "borough districts"; the exception is Kilkenny, whose district is the "Municipal District of Kilkenny City", because of Kilkenny's city status.
Earlier Irish boroughs include the 117 parliamentary boroughs of the Irish House of Commons, of which 80 were disfranchised by the Acts of Union 1800 and all but 11 abolished under the Municipal Corporations (Ireland) Act 1840. The six largest of those eleven became county boroughs under the Local Government (Ireland) Act 1898, of which those in the Republic were reclassed as "cities" under the Local Government Act 2001. Galway was a borough from 1937 until promoted to county borough in 1985, and Dún Laoghaire was a borough from 1930 until merged into Dún Laoghaire–Rathdown county in 1993.
New Zealand.
New Zealand formerly used the term borough to designate self-governing towns of more than 1,000 people, although 19th century census records show many boroughs with populations as low as 200. A borough of more than 20,000 people could become a city by proclamation. Boroughs and cities were collectively known as municipalities, and were enclaves separate from their surrounding counties. Boroughs proliferated in the suburban areas of the larger cities: By the 1980s there were 19 boroughs and three cities in the area that is now the City of Auckland.
In the 1980s, some boroughs and cities began to be merged with their surrounding counties to form districts with a mixed urban and rural population. In 1989, a nationwide reform of local government completed the process. Counties and boroughs were abolished and all boundaries were redrawn. Under the new system, most territorial authorities cover both urban and rural land. The more populated councils are classified as cities, and the more rural councils are classified as districts. Only Kawerau District, an enclave within Whakatane District, continues to follow the tradition of a small town council that does not include surrounding rural area.
Israel.
Under Israeli law, inherited from British Mandate municipal law, the possibility of creating a municipal borough exists. However, no borough was actually created under law until 2005–2006, when Neve Monosson and Maccabim-Re'ut, both communal settlements (Heb: yishuv kehilati) founded in 1953 and 1984, respectively, were declared to be autonomous municipal boroughs (Heb: vaad rova ironi), within their mergers with the towns of Yehud and Modi'in. Similar structures have been created under different types of legal status over the years in Israel, notably Kiryat Haim in Haifa, Jaffa in Tel Aviv-Yafo and Ramot and Gilo in Jerusalem. However, Neve Monosson is the first example of a full municipal borough actually declared under law by the Minister of the Interior, under a model subsequently adopted in Maccabim-Re'ut as well.
It is the declared intention of the Interior Ministry to use the borough mechanism in order to facilitate municipal mergers in Israel, after a 2003 wide-reaching merger plan, which, in general, ignored the sensitivities of the communal settlements, and largely failed.
Netherlands.
In the Netherlands, the municipalities of Rotterdam and Amsterdam are divided into administrative boroughs, or deelgemeenten, which have their own borough council and a borough mayor. Other large cites are usually divided into districts, or stadsdelen, for census purposes.

</doc>
<doc id="4858" url="http://en.wikipedia.org/wiki?curid=4858" title="Bodmin">
Bodmin

Bodmin () is a civil parish and major town in Cornwall, England, United Kingdom. It is situated in the centre of the county southwest of Bodmin Moor.
The extent of the civil parish corresponds fairly closely to that of the town so is mostly urban in character. It is bordered to the east by Cardinham parish, to the southeast by Lanhydrock parish, to the southwest and west by Lanivet parish, and to the north by Helland parish.
Bodmin has a population of 12,778 (2001 census). It was formerly the county town of Cornwall until the Crown Courts moved to Truro which is also the administrative centre (before 1835 the county town was Launceston). Bodmin was in the administrative North Cornwall District until local government reorganisation in 2009 abolished the District ("see also Politics of Cornwall#Cornwall Council"). The town is part of the North Cornwall parliamentary constituency, which is represented by Dan Rogerson MP.
Bodmin Town Council is made up of 16 councillors who are elected to serve a term of four years. Each year, the Council elects one of its number as Mayor to serve as the town's civic leader and to chair council meetings.
Situation and origin of the name.
Bodmin lies in the centre of Cornwall, south-west of Bodmin Moor. It has been suggested that the town's name comes from an archaic word in the Cornish "bod" (meaning a dwelling; the later word is "bos") and a contraction of "menegh" (monks). The "monks' dwelling" may refer to an early monastic settlement instituted by St. Guron, which St. Petroc took as his site. Guron is said to have departed to St Goran on the arrival of Petroc.
The hamlets of Cooksland, Dunmere and Turfdown are in the parish.
History.
St. Petroc founded a monastery in Bodmin in the sixth century and gave the town its alternative name of "Petrockstow". The monastery was deprived of some of its lands at the Norman Conquest but at the time of Domesday still held 18 manors, including Bodmin, Padstow and Rialton. Bodmin is one of the oldest towns in Cornwall, and the only large Cornish settlement recorded in the Domesday Book of the late 11th century. In the 15th century the Norman church of St Petroc was largely rebuilt and stands as one of the largest churches in Cornwall (the largest after the cathedral at Truro). Also built at that time was an abbey of canons regular, now mostly ruined. For most of Bodmin's history, the tin industry was a mainstay of the economy.
The name of the town probably derives from the Cornish "Bod-meneghy", meaning "dwelling of or by the sanctuary of monks". Variant spellings recorded include "Botmenei" in 1100, "Bodmen" in 1253, "Bodman" in 1377 and "Bodmyn" in 1522. The "Bodman" spelling also appears in sources and maps from the sixteenth and seventeenth centuries, most notably in the celebrated map of Cornwall produced by John Speed but actually engraved by the Dutch cartographer Jodocus Hondius the Elder (1563-1612) in Amsterdam in 1610 (published in London by Sudbury and Humble in 1626). It is unclear whether the Bodman spelling signifies any historical or monastic connection with the equally ancient settlement of Bodman at the western end of the Bodensee in the German province of Baden.
An inscription on a stone built into the wall of a summer house in Lancarffe furnishes proof of a settlement in Bodmin in the early Middle Ages. It is a memorial to one "Duno[.]atus son of Me[.]cagnus" and has been dated from the sixth to eighth centuries.
The Black Death killed half of Bodmin's population in the mid 14th century (1500 people).
Rebellions.
Bodmin was the centre of three Cornish uprisings. The first was the Cornish Rebellion of 1497 when a Cornish army, led by Michael An Gof, a blacksmith from St. Keverne, and Thomas Flamank, a lawyer from Bodmin, marched to Blackheath in London where they were eventually defeated by 10,000 men of the King's army under Baron Daubeny. Then, in the Autumn of 1497, Perkin Warbeck tried to usurp the throne from Henry VII. Warbeck was proclaimed King Richard IV in Bodmin but Henry had little difficulty crushing the uprising. In 1549, Cornishmen, allied with other rebels in neighboring Devon, rose once again in rebellion when the staunchly Protestant Edward VI tried to impose a new Prayer Book. The lower classes of Cornwall and Devon were still strongly attached to the Catholic religion and again a Cornish army was formed in Bodmin which marched across the border into Devon to lay siege to Exeter. This became known as the Prayer Book Rebellion. Proposals to translate the Prayer Book into Cornish were suppressed and in total 4,000 people were killed in the rebellion.
Churches.
Parish church of St Petroc.
The existing church building is dated 1469-72 and was until the building of Truro Cathedral the largest church in Cornwall. The tower which remains from the original Norman church and stands on the north side of the church (the upper part is 15th century) was, until the loss of its spire in 1699, 150 ft high. The building underwent two Victorian restorations and another in 1930. It is now listed Grade I. There are a number of interesting monuments, most notably that of Prior Vivian which was formerly in the Priory Church (Thomas Vivian's effigy lying on a chest: black Catacleuse stone and grey marble). The font of a type common in Cornwall is of the twelfth century: large and finely carved.
Other churches.
The Chapel of St Thomas Becket is a ruin of a 14th-century building in Bodmin churchyard. The holy well of St Guron is a small stone building at the churchyard gate. The Berry Tower is all that remains of the former church of the Holy Rood and there are even fewer remains from the substantial Franciscan Friary established ca. 1240: a gateway in Fore Street and two pillars elsewhere in the town. The Roman Catholic Abbey of St Mary and St Petroc, formerly belonging to the Canons Regular of the Lateran was built in 1965 next to the already existing seminary. The Roman Catholic parish of Bodmin includes a large area of North Cornwall and there are churches also at Wadebridge, Padstow and Tintagel. In 1881 the Roman Catholic mass was celebrated in Bodmin for the first time since 1539. A church was planned in the 1930s but delayed by World War II: the Church of St Mary and St Petroc was eventually consecrated in 1965: it was built next to the already existing seminary. There are also five other churches in Bodmin, including a Methodist church.
Sites of interest.
Institutions.
Bodmin Jail, operational for over 150 years but now a semi-ruin, was built in the late 18th century, and was the first British prison to hold prisoners in separate cells (though often up to 10 at a time) rather than communally. Over fifty prisoners condemned at the Bodmin Assize Court were hanged at the prison. It was also used for temporarily holding prisoners sentenced to transportation, awaiting transfer to the prison hulks lying in the highest navigable reaches of the River Fowey. Also, during World War I the prison held some of Britain's priceless national treasures including the Domesday Book, the ring and the Crown Jewels of the United Kingdom.
Other buildings of interest include the former Shire Hall, now a tourist information centre, and Victoria Barracks, formerly depot of the now defunct Duke of Cornwall's Light Infantry and now the site of the regimental museum. It includes the history of the regiment from 1702, plus a military library. The original barracks house the regimental museum which was founded in 1925. There is a fine collection of small arms and machine guns, plus maps, uniforms and paintings on display.
Bodmin County Lunatic Asylum was designed by John Foulston and afterwards George Wightwick. William Robert Hicks the humorist was domestic superintendent in the mid-19th century.
Freemasonry.
There is a sizeable single storey Masonic Hall in St Nicholas Street, which is home to no less than seven Masonic bodies.
Other sites.
The Bodmin Beacon Local Nature Reserve is the hill overlooking the town. The reserve has 83 acres (33.6 ha) of public land and at its highest point it reaches 162 metres with the distinctive landmark at the summit. The 44-metre tall monument to Sir Walter Raleigh Gilbert was built in 1857 by the townspeople of Bodmin to honour the soldier's life and work in India.
In 1966, the "Finn VC Estate" was named in honour of Victoria Cross winner James Henry Finn who once lived in the town. Langdon (1896) records six crosses in the parish of which the finest is at Carminow. An ornate granite drinking bowl which serves the needs of thirsty dogs at the entrance to Bodmin’s Priory car park was donated by Prince Chula Chakrabongse of Thailand who lived at Tredethy.
Education.
There are no independent schools in the area.
Primary schools.
St. Petroc's Voluntary Aided Church of England Primary School Athelstan Park, Bodmin, Cornwall was given this title in September 1990 after the amalgamation of St. Petroc's Infant School and St. Petroc's Junior School. St. Petroc's is a large school with some 440 pupils between the ages of four and 11. Eight of its 14 governors are nominated by the Diocese of Truro or the Parochial Church Council of St. Petroc's, Bodmin.
There are a further three primary (or elementary) schools within Bodmin; Berrycoombe School in the north west corner of the town, St. Mary's Catholic Primary School and Robartes Primary Junior School, both situated west of the town centre.
Bodmin College.
Bodmin College is a large state comprehensive school for ages 11–18 on the outskirts of the town and on the edge of Bodmin Moor. Its headmaster is Mr Brett Elliott. The College is home to the nationally acclaimed "Bodmin College Jazz Orchestra", founded and run by the previous Director of Music, Adrian Evans, until 2007 and more recently, by the current Director, Ben Vincent.
In 1997, Systems & Control students at Bodmin College constructed Roadblock, a robot which entered and won the first series of Robot Wars and was succeeded by "The Beast of Bodmin" (presumably named after the phantom cat purported to roam Bodmin Moor).
The School also has one of the largest sixth forms in the county.
Transport.
Bodmin Parkway railway station is served by main line trains and is situated on the Cornish Main Line about 3½ miles (5½ km) south-east from the town centre. A heritage railway, the Bodmin and Wenford Railway, runs from Bodmin Parkway station via Bodmin General railway station to Boscarne Junction where there is access to the Camel Trail. The bus link to Bodmin, Wadebridge and Padstow starts from outside the main entrance of Bodmin Parkway.
Bus and coach services connect Bodmin with other districts of Cornwall and Devon.
Sport and leisure.
Bodmin has a Non-League football club Bodmin Town F.C. who play at Priory Park.
The Royal Cornwall Golf Club (now defunct) was located on Bodmin Moor. It was founded in 1889. The club disappeared following WW2. 
Media.
The "Cornish Guardian" is a weekly newspaper: it is published, on a Friday in 7 separate editions, including the Bodmin edition.
Bodmin is the home of NCB Radio, an Internet radio station which aims to bring a dedicated station to North Cornwall.
Notable people.
See also 
Town twinning.
Bodmin is twinned with Bederkesa in Germany; Grass Valley, in California, United States; and Le Relecq-Kerhuon (Ar Releg-Kerhuon in Brittany, France.
Official heraldry.
W. H. Pascoe’s 1979 "A Cornish Armory" gives the arms of the priory and the monastery and the seal of the borough.
Official events.
On Halgaver Moor (Goats' Moor) near Bodmin there was once an annual carnival in July which was on one occasion attended by King Charles II.
Bodmin Riding, a horseback procession through the town, is a traditional annual ceremony.
'Beating the bounds' and 'hurling'.
In 1865–66 William Robert Hicks was mayor of Bodmin, when he revived the custom of Beating the bounds of the town. He was — according to the Dictionary of National Biography — a very good man of business. This still takes place more or less every five years and concludes with a game of Cornish hurling. Hurling survives as a traditional part of beating the bounds at Bodmin, commencing at the close of the 'Beat'. The game is organised by the Rotary club of Bodmin and was last played in 2010. The game is started by the Mayor of Bodmin by throwing a silver ball into a body of water known as the "Salting Pool". There are no teams and the hurl follows a set route. The aim is to carry the ball from the "Salting Pool" via the old A30, along Callywith Road, then through Castle Street, Church Square and Honey Street to finish at the Turret Clock in Fore Street. The participant carrying the ball when it reaches the turret clock will receive a £10 reward from the mayor. The next occurrence of the Bodmin Hurl will be following the next beating of the bounds, which is unlikely to take place until 2015.

</doc>
<doc id="4859" url="http://en.wikipedia.org/wiki?curid=4859" title="Bodmin Moor">
Bodmin Moor

Bodmin Moor () is a granite moorland in northeastern Cornwall, England, United Kingdom. It is in size, and dates from the Carboniferous period of geological history. 
Bodmin Moor is one of five granite plutons in Cornwall that make up part of the Cornubian batholith (see also Geology of Cornwall).
The name 'Bodmin Moor' is relatively recent, being an Ordnance Survey invention of 1813. It was formerly known as Fowey Moor after the River Fowey which rises within it.
Geography.
Dramatic granite tors rise from the rolling moorland: the best known are Brown Willy, the highest point in Cornwall at , and Rough Tor at . To the south-east Kilmar Tor and Caradon Hill are the most prominent hills. Considerable areas of the moor are poorly drained and form marshes (in hot summers these can dry out). The rest of the moor is mostly rough pasture or overgrown with heather and other low vegetation.
The moor contains about 500 holdings with around 10,000 beef cows, 55,000 breeding ewes and 1,000 horses and ponies. Most of the moor is a Site of Special Scientific Interest (SSSI), "Bodmin Moor, North", and has been officially designated an Area of Outstanding Natural Beauty (AONB), as part of Cornwall AONB. Almost a third of Cornwall has AONB designation, with the same status and protection as a National Park. The moor has been identified by BirdLife International as an Important Bird Area (IBA) because it supports about 260 breeding pairs of European Stonechats as well as a wintering population of 10,000 Eurasian Golden Plovers. The moor has also be recognized as a separate natural region and designated as national character area 153 by Natural England.
Rivers and inland waters.
Bodmin Moor is the source of several of Cornwall's rivers: they are mentioned here anti-clockwise from the south. 
The River Fowey rises at a height of and flows through Lostwithiel and into the Fowey estuary. 
The River Tiddy rises near Pensilva and flows southeast to its confluence with the River Lynher (the Lynher flows generally south-east until it joins the Hamoaze near Plymouth). The River Inny rises near Davidstow and flows southeast to its confluence with the River Tamar. 
The River Camel rises on Hendraburnick Down and flows for approximately before joining the sea at Padstow. The River Camel and its tributary the De Lank River are an important habitat for the otter and both have been proposed as Special Areas of Conservation (SAC) The De Lank River rises near Roughtor and flows along an irregular course before joining the Camel south of Wenford. 
The River Warleggan rises near Temple and flows south to join the Fowey.
On the southern slopes of the moor lies Dozmary Pool. It is Cornwall's only natural inland lake and is glacial in origin. In the 20th century three reservoirs have been constructed on the moor; these are Colliford Lake, Siblyback Lake and Crowdy reservoirs which supply water for a large part of the county's population. Various species of waterfowl are resident around these waters.
Parishes.
The parishes on the moor are as follows:
History and antiquities.
Prehistoric times.
10,000 years ago, in the Mesolithic period, hunter-gatherers wandered the moor when it was wooded and had a temperate climate. There are several documented cases of flint scatters being discovered by archaeologists, indicating that these hunter gatherers practised flint knapping in the region. 
During the Neolithic era, from about 4,500 to 2,300 BC, people began clearing trees and farming the land. It was also in this era that the production of various megalithic monuments began, predominantly long cairns (three of which have currently been identified, at Louden, Catshole and Bearah) and stone circles (sixteen of which have been identified). It was also likely that the naturally forming tors were also viewed in a similar manner to the manmade ceremonial sites.
In the following Bronze Age, the creation of monuments increased dramatically, with the production of over 300 further cairns, and more stone circles and stone rows. More than 200 Bronze Age settlements with enclosures and field patterns have been recorded. and many prehistoric stone barrows and circles lie scattered across the moor. In a programme shown in 2007 Channel 4's "Time Team "investigated a 500 metre cairn and the site of a Bronze Age village on the slopes of Rough Tor.
King Arthur's Hall thought to be a late Neolithic or early Bronze Age ceremonial site can be found to the east of St Breward on the moor.
Medieval and modern times.
Where practicable areas of the moor were used for pasture by herdsmen from the parishes surrounding the moor. Granite boulders were also taken from the moor and used for stone posts and to a certain extent for building (such material is known as moorstone). Granite quarrying only became reasonably productive when gunpowder became available.
The moor gave its name (Foweymore) to one of the medieval districts called stannaries which administered tin mining: the boundaries of these were never defined precisely. Until the establishment of a turnpike road through the moor (the present A30) in the 1770s the size of the moorland area made travel within Cornwall very difficult.
Its Cornish name, Goen Bren, is first recorded in the 12th century.
Monuments and ruins.
Roughtor was the site of a medieval chapel of St Michael and is now designated as a memorial to the 43rd Wessex Division of the British Army. In 1844 on Bodmin Moor the body of 18 year old Charlotte Dymond was discovered. Local labourer Matthew Weeks was accused of the murder and at noon on 12 August 1844 he was led from Bodmin Gaol and hanged. The murder site now has a monument erected from public money and the grave is at Davidstow churchyard.
Legends and traditions.
Dozmary Pool is identified by some people with the lake in which, according to Arthurian legend, Sir Bedivere threw Excalibur to The Lady of the Lake. Another legend relating to the pool concerns Jan Tregeagle.
The Beast of Bodmin has been reported many times but never identified with certainty.

</doc>
<doc id="4860" url="http://en.wikipedia.org/wiki?curid=4860" title="Berkeley, California">
Berkeley, California

Berkeley ( ) is a city on the east shore of San Francisco Bay in northern Alameda County, California which is named after the eighteenth-century bishop and philosopher George Berkeley. It borders the cities of Oakland and Emeryville to the south and the city of Albany and unincorporated community of Kensington to the north. Its eastern border with Contra Costa County generally follows the ridge of the Berkeley Hills. Its population at the 2010 census was determined to be 112,580. It is one of the most politically liberal cities in the United States.
Berkeley is the site of the oldest campus in the University of California systemand of the Lawrence Berkeley National Laboratory that the university manages and operates. It is also home to the Graduate Theological Union.
History.
Early history.
The site of today's City of Berkeley was the territory of the Chochenyo/Huchiun band of the Ohlone people when the first Europeans arrived. Evidence of their existence in the area include pits in rock formations, which they used to grind acorns, and a shellmound, now mostly leveled and covered up, along the shoreline of San Francisco Bay at the mouth of Strawberry Creek. Other artifacts were discovered in the 1950s in the downtown area during remodeling of a commercial building, near the upper course of the creek.
The first people of European descent (most of whom were born in America, and many of whom were of mixed ancestry) arrived with the De Anza Expedition in 1776. Today, this is noted by signage on Interstate 80, which runs along the San Francisco Bay shoreline of Berkeley. The De Anza Expedition led to establishment of the Spanish Presidio of San Francisco at the entrance to San Francisco Bay (the "Golden Gate)," which is due west of Berkeley. Luis Peralta was among the soldiers at the Presidio. For his services to the King of Spain, he was granted a vast stretch of land on the east shore of San Francisco Bay (the "contra costa", "opposite shore") for a ranch, including that portion that now comprises the City of Berkeley.
Luis Peralta named his holding "Rancho San Antonio." The primary activity of the ranch was raising cattle for meat and hides, but hunting and farming were also pursued. Eventually, Peralta gave portions of the ranch to each of his four sons. What is now Berkeley lies mostly in the portion that went to Peralta's son Domingo, with a little in the portion that went to another son, Vicente. No artifact survives of the Domingo or Vicente ranches, but their names survive in Berkeley street names (Vicente, Domingo, and Peralta). However, legal title to all land in the City of Berkeley remains based on the original Peralta land grant.
The Peraltas' Rancho San Antonio continued after Alta California passed from Spanish to Mexican sovereignty after the Mexican War of Independence. However, the advent of U.S. sovereignty after the Mexican–American War, and especially, the Gold Rush, saw the Peraltas' lands quickly encroached on by squatters and diminished by dubious legal proceedings. The lands of the brothers Domingo and Vicente were quickly reduced to reservations close to their respective ranch homes. The rest of the land was surveyed and parceled out to various American claimants ("See" Kellersberger's Map).
Politically, the area that became Berkeley was initially part of a vast Contra Costa County. On March 25, 1853, Alameda County was created by division of Contra Costa County, as well as from a small portion of Santa Clara County.
The area of Berkeley was at this period mostly a mix of open land, farms and ranches, with a small though busy wharf by the bay. It was not yet "Berkeley," but merely the northern part of the "Oakland Township" subdivision of Alameda County.
Late 19th century.
In 1866, Oakland's private College of California looked for a new site. It settled on a location north of Oakland along the foot of the Contra Costa Range (later called the Berkeley Hills) astride Strawberry Creek, at an elevation about above the bay, commanding a view of the Bay Area and the Pacific Ocean through the Golden Gate.
According to the "Centennial Record of the University of California", "In 1866…at Founders' Rock, a group of College of California men watched two ships standing out to sea through the Golden Gate. One of them, Frederick Billings, thought of the lines of the Anglo-Irish Anglican Bishop George Berkeley, 'westward the course of empire takes its way,' and suggested that the town and college site be named for the eighteenth-century Anglo-Irish philosopher." Although the philosopher's name is pronounced "bark-lee," the pronunciation of the city's name has evolved to suit American English as "burk-lee."
The College of California's "College Homestead Association" planned to raise funds for the new campus by selling off adjacent parcels of land. To this end, they laid out a plat and street grid that became the basis of Berkeley's modern street plan. Their plans fell far short of their desires, and they began a collaboration with the State of California that culminated in 1868 with the creation of the public University of California.
As construction began on the new site, more residences were constructed in the vicinity of the new campus. At the same time, a settlement of residences, saloons, and various industries grew around the wharf area called "Ocean View." A horsecar ran from Temescal in Oakland to the university campus along what is now Telegraph Avenue. The first post office opened in 1872.
By the 1870s, the Transcontinental Railroad reached its terminus in Oakland. In 1876, a branch line of the Central Pacific Railroad, the Berkeley Branch Railroad, was laid from a junction with the mainline called Shellmound (now a part of Emeryville) into what is now downtown Berkeley. That same year, the mainline of the transcontinental railroad into Oakland was re-routed, putting the right-of-way along the bay shore through Ocean View.
There was a strong prohibition movement in Berkeley at this time. In 1876, the city passed the "mile limit law," which forbade sale or public consumption of alcohol within one mile (1.6 km) of the new University of California. Then, in 1899 Berkeley residents voted to make their city an alcohol-free zone. Scientists, scholars and religious leaders spoke vehemently of the dangers of alcohol.
In 1878, the people of Ocean View and the area around the University campus, together with local farmers, incorporated as the Town of Berkeley. The first elected trustees of the town were the slate of Denis Kearney's Workingman's Party, who were particularly favored in the working class area of the former Ocean View, now called "West Berkeley." The area near the university became known for a time as "East Berkeley."
The modern age came quickly to Berkeley, no doubt due to the influence of the university. Electric lights were in use by 1888. The telephone had already come to town. Electric streetcars soon replaced the horsecar. A silent film of one of these early streetcars in Berkeley can be seen at the Library of Congress website: 
Early 20th century.
Berkeley's slow growth ended abruptly with the Great San Francisco Earthquake of 1906. The town and other parts of the East Bay escaped serious damage from the massive temblor, and thousands of refugees flowed across the Bay.
In 1908, a statewide referendum that proposed moving the California state capital to Berkeley was defeated by a margin of about 33,000 votes. The city named streets around the proposed capitol grounds for California counties. They bear those names today, a legacy of the failed referendum.
In 1909, the citizens of Berkeley adopted a new charter, and the Town of Berkeley became the City of Berkeley. Rapid growth continued up to the Crash of 1929. The Great Depression hit Berkeley hard, but not as hard as many other places in the U.S., thanks in part to the University.
On September 17, 1923, a major fire swept down the hills toward the University campus and the downtown section. Around 640 structures burned before a late afternoon sea breeze stopped its progress, allowing firefighters to put it out.
The next big growth occurred with the advent of World War II, when large numbers of people moved to the Bay Area to work in the many war industries, such as the immense Kaiser Shipyards in nearby Richmond. One who moved out, but played a big role in the outcome of the War was U.C. Professor and Berkeley resident J. Robert Oppenheimer. During the war, an Army base, Camp Ashby, was temporarily sited in Berkeley.
The element berkelium was synthesized and named in 1949, recognizing the University, thus also placing the city's name in the list of elements.
1950s and 1960s.
During the 1940s, many African Americans migrated to Berkeley. In 1950, the Census Bureau reported Berkeley's population as 11.7% black and 84.6% white.
The postwar years brought moderate growth to the city, as events on the U.C. campus began to build up to the recognizable activism of the sixties. In the 1950s, McCarthyism induced the University to demand a loyalty oath from its professors, many of whom refused to sign the oath on the principle of freedom of thought. In 1960, a U.S. House committee (HUAC) came to San Francisco to investigate the influence of communists in the Bay Area. Their presence was met by protesters, including many from the University. Meanwhile, a number of U.C. students became active in the Civil Rights Movement. Finally, in 1964, the University provoked a massive student protest by banning distribution of political literature on campus. This protest became the Free Speech Movement. As the Vietnam War rapidly escalated in the ensuing years, so did student activism at the University, particularly that organized by the Vietnam Day Committee.
Berkeley is strongly identified with the rapid social changes, civic unrest, and political upheaval that characterized the late 1960s. In that period, Berkeley—especially Telegraph Avenue—became a focal point for the hippie movement, which spilled over the Bay from San Francisco. Many hippies were apolitical drop-outs, rather than students, but in the heady atmosphere of Berkeley in 1967–1969 there was considerable overlap between the hippie movement and the radical left. An iconic event in the Berkeley Sixties scene was a conflict over a parcel of University property south of the contiguous campus site that came to be called "People's Park."
The battle over the disposition of People's Park resulted in a month-long occupation of Berkeley by the National Guard on orders of then-Governor Ronald Reagan. In the end, the park remained undeveloped, and remains so today. A spin-off, "People's Park Annex," was established at the same time by activist citizens of Berkeley on a strip of land above the Bay Area Rapid Transit subway construction along Hearst Avenue northwest of the U.C. campus. The land had also been intended for development, but was turned over to the city by BART and is now Ohlone Park.
The era of large public protest in Berkeley waned considerably with the end of the Vietnam War in 1974. While the 1960s were the heyday of liberal activism in Berkeley, it remains one of the most overwhelmingly Democratic cities in the United States.
1970s to present.
The Berkeley population declined in the 1970s, partly due to an exodus to the suburbs. Some moved because of the rising cost of living throughout the Bay Area, and others because of the decline and disappearance of many industries in West Berkeley.
From the 1980s to the present, Berkeley housing costs have risen, especially since the mid-1990s. In 2005–2007, sales of homes began to slow, but average home prices, as of 2012, remain among the highest in the nation.
In 1983, Berkeley's Domestic Partner Task Force was established, which in 1984 made policy recommendation to the School Board, which passed Domestic Partner legislation. The legislation became a model for similar measures nationwide.
In 2006, the Berkeley Oak Grove Protest began protesting construction of a new sports center annex to Memorial Stadium at the expense of a grove of oak trees on the UC campus. The protest ended in September 2008 after a lengthy court process.
In 2007–08, Berkeley received media attention due to demonstrations against a Marine Corps recruiting office in downtown Berkeley and a series of controversial motions by Berkeley's city council regarding opposition to Marine recruiting. ("See" Berkeley Marine Corps Recruiting Center controversy.)
During the fall of 2010, the Berkeley Student Food Collective opened after many protests on the UC Berkeley campus due to the proposed opening of the fast food chain Panda Express. Students and community members worked together to open a collectively run grocery store right off of the UC Berkeley campus, where the community can buy local, seasonal, humane, and organic foods. The Berkeley Student Food Collective still operates at 2440 Bancroft Way.
In the Fall of 2011, the nationwide Occupy Wall Street movement came to two Berkeley locations: on the campus of the University of California and as an encampment in Civic Center Park.
On September 18, 2012, Berkeley became what may be the first city in the U.S. to officially proclaim a day recognizing bisexuals September 23, which is known as Celebrate Bisexuality Day.
On September 2, 2014, the city council approved a measure to provide free medical marijuana to low-income patients.
Geography.
Berkeley is located at (37.871775, −122.274603).
According to the United States Census Bureau the city's area includes of land and (40.83%) water, most of it part of San Francisco Bay.
Berkeley borders the cities of Albany, Oakland, and Emeryville and Contra Costa County including unincorporated Kensington as well as San Francisco Bay.
Berkeley lies within telephone area code 510 (until September 2, 1991, Berkeley was part of the 415 telephone code that now covers only San Francisco and Marin Counties), and the postal ZIP codes are 94701 through 94710, 94712, and 94720 for the University of California campus.
Geology.
Most of Berkeley lies on a rolling sedimentary plain that rises gently from sea level to the base of the Berkeley Hills. East of the Hayward Fault along the base of the hills, elevation increases more rapidly. The highest peak along the ridge line above Berkeley is Grizzly Peak, elevation . A number of small creeks run from the hills to the Bay through Berkeley: Cerrito, Codornices, Schoolhouse and Strawberry Creeks are the principal streams. Most of these are largely culverted once they reach the plain west of the hills.
The Berkeley Hills are part of the Pacific Coast Ranges, and run in a northwest–southeast alignment. Exposed in the Berkeley Hills are cherts and shales of the Claremont Formation (equivalent to the Monterey Formation), conglomerate and sandstone of the Orinda Formation and lava flows of the Moraga Volcanics. Of similar age to the Moraga Volcanics, within the Northbrae neighborhood of Berkeley, are outcroppings of erosional resistant rhyolite. These rhyolite formations can be seen in several city parks and in the yards of a number of private residences. Indian Rock Park in the northeastern part of Berkeley near the Arlington/Marin Circle features a large example.
Earthquakes.
Berkeley is traversed by the Hayward Fault, a major branch of the San Andreas Fault to the west. No large earthquake has occurred on the Hayward Fault near Berkeley in historic times (except possibly in 1836), but seismologists warn about the geologic record of large temblors several times in the deeper past, and their current assessment is that a quake of 6.7 or greater is imminent, sometime within the next 30 years.
The 1868 Hayward earthquake did occur on the southern segment of the Hayward Fault in the vicinity of today's city of Hayward (hence, how the fault got its name). This quake destroyed the county seat of Alameda County then located in San Leandro and it subsequently moved to Oakland. It was strongly felt in San Francisco, causing major damage, and experienced by Samuel Clemens (Mark Twain). It was regarded as the "Great San Francisco Quake" prior to 1906. The quake produced a furrow in the ground along the fault line in Berkeley, across the grounds of the new State Asylum for the Deaf, Dumb and Blind then under construction, which was noted by one early University of California professor. Though no significant damage was reported to most of the few Berkeley buildings of the time, the 1868 quake did destroy the vulnerable adobe home of Domingo Peralta in north Berkeley.
Today, evidence of the Hayward Fault's "creeping" is visible at various locations in Berkeley. Cracked roadways, sharp jogs in streams, and springs mark the fault's path. However, since it cuts across the base of the hills, the creep is often concealed by or confused with slide activity. Some of the slide activity itself, however, results from movement on the Hayward Fault.
A notorious segment of the Hayward Fault runs lengthwise down the middle of Memorial Stadium at the mouth of Strawberry Canyon on the University of California campus. Photos and measurements show the movement of the fault through the stadium.
Climate.
Berkeley has a cool summer Mediterranean climate (type Csb in the Köppen climate classification), with dry summers and wet winters. The summers are cooler than a typical Mediterranean climate thanks to upwelling ocean currents along the California coast. These help produce cool and foggy nights and mornings. Berkeley's location directly opposite the Golden Gate ensures that typical eastward fog flow blankets the city more often than its neighbors.
Winter is punctuated with rainstorms of varying ferocity and duration, but also produces stretches of bright sunny days and clear cold nights. It does not normally snow, though occasionally the hilltops get a dusting. Spring and fall are transitional and intermediate, with some rainfall and variable temperature. Summer typically brings night and morning low clouds or fog, followed by sunny, warm days. The warmest and driest months are typically June through September, with the highest temperatures occurring in September. Mid-summer (July–August) is often a bit cooler due to the sea breezes and fog common then.
Average January temperatures are a maximum of and a minimum of . Average September (the warmest month) temperatures are a maximum of and a minimum of . In a year, there are an average of 2.9 days with highs of or higher, and an average of 0.8 days with lows of or lower. The highest recorded temperature was on June 15, 2000, and the lowest recorded temperature was on December 22, 1990.
January is normally the wettest month, averaging of precipitation. Average annual precipitation is , falling on an average of 63.7 days each year. The most rainfall in one month was in February 1998. The most rainfall in 24 hours was on January 4, 1982. Light snow has fallen on rare occasions. Snow has generally fallen every several years on the higher peaks of the Berkeley Hills.
In the late spring and early fall, strong offshore winds of sinking air typically develop, bringing heat and dryness to the area. In the spring, this is not usually a problem as vegetation is still moist from winter rains, but extreme dryness prevails by the fall, creating a danger of wildfires. In September 1923 a major fire swept through the neighborhoods north of the University campus, stopping just short of downtown. ("See" 1923 Berkeley fire). On October 20, 1991, gusty, hot winds fanned a conflagration along the Berkeley–Oakland border, killing 25 people and injuring 150, as well as destroying 2,449 single-family dwellings and 437 apartment and condominium units. ("See" 1991 Oakland firestorm)
Demographics.
The 2010 United States Census reported that Berkeley had a population of 112,580. The population density was 10,752 people per square mile of land area (4,104/km²). The racial makeup of Berkeley was 66,996 (59.5%) White, 11,241 (10.0%) Black or African American, 479 (0.4%) Native American, 21,690 (19.3%) Asian (8.4% Chinese, 2.4% Indian, 2.1% Korean, 1.6% Japanese, 1.5% Filipino, 1.0% Vietnamese, 0.3% Pakistani, 0.3% Thai, 0.2% Nepalese), 186 (0.2%) Pacific Islander, 4,994 (4.4%) from other races, and 6,994 (6.2%) from two or more races. Hispanic or Latino of any race were 12,209 persons (10.8%). 6.8% of the city's population was of Mexican ancestry, while 0.5% was of Puerto Rican, 0.4% Salvadoran, 0.3% Peruvian, 0.3% Guatemalan descent.
The Census reported that 99,731 people (88.6% of the population) lived in households, 12,430 (11.0%) lived in non-institutionalized group quarters, and 419 (0.4%) were institutionalized.
There were 46,029 households, out of which 8,467 (18.4%) had children under the age of 18 living in them, 13,569 (29.5%) were opposite-sex married couples living together, 3,855 (8.4%) had a female householder with no husband present, 1,368 (3.0%) had a male householder with no wife present. There were 2,931 (6.4%) unmarried opposite-sex partnerships, and 961 (2.1%) same-sex married couples or partnerships. 16,904 households (36.7%) were made up of individuals and 4,578 (9.9%) had someone living alone who was 65 years of age or older. The average household size was 2.17. There were 18,792 families (40.8% of all households); the average family size was 2.81. There were 49,454 housing units at an average density of 2,794.6 per square mile (1,079.0/km²), of which 18,846 (40.9%) were owner-occupied, and 27,183 (59.1%) were occupied by renters. The homeowner vacancy rate was 1.0%; the rental vacancy rate was 4.5%. 45,096 people (40.1% of the population) lived in owner-occupied housing units and 54,635 people (48.5%) lived in rental housing units.
The population was spread out with 13,872 people (12.3%) under the age of 18, 30,295 people (26.9%) aged 18 to 24, 30,231 people (26.9%) aged 25 to 44, 25,006 people (22.2%) aged 45 to 64, and 13,176 people (11.7%) who were 65 years of age or older. The median age was 31.0 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 94.2 males.
According to the 2011 American Community Survey 5-Year estimate, the median income for a household in the city was $60,908, and the median income for a family was $102,976. Males had a median income of $67,476 versus $57,319 for females. The per capita income for the city was $38,896. About 7.2% of families and 18.3% of the population were below the poverty line, including 13.2% of those under age 18 and 9.2% of those age 65 or over.
Transportation.
Berkeley is served by Amtrak (Capitol Corridor), AC Transit, BART (Ashby, Downtown Berkeley Station and North Berkeley) and bus shuttles operated by major employers including UC Berkeley and Lawrence Berkeley National Laboratory. The Eastshore Freeway (Interstate 80 and Interstate 580) runs along the bay shoreline. Each day there is an influx of thousands of cars into the city by commuting UC faculty, staff and students, making parking for more than a few hours an expensive proposition.
Berkeley has one of the highest rates of bicycle and pedestrian commuting in the nation. Berkeley is the safest city of its size in California for pedestrians and cyclists, considering the number of injuries per pedestrian and cyclist, rather than per capita.
Berkeley has modified its original grid roadway structure through use of diverters and barriers, moving most traffic out of neighborhoods and onto arterial streets (visitors often find this confusing, because the diverters are not shown on all maps). Berkeley maintains a separate grid of arterial streets for bicycles, called Bicycle Boulevards, with bike lanes and lower amounts of car traffic than the major streets they often parallel.
Berkeley hosts car sharing networks run by City CarShare, Uhaul Car Share, and Zipcar. Rather than owning (and parking) their own cars, members share a group of cars parked nearby. Web- and telephone-based reservation systems keep track of hours and charges. Several "pods" (points of departure where cars are kept) exist throughout the city, in several downtown locations, at the Ashby and North Berkeley BART stations, and at various other locations in Berkeley (and other cities in the region). Using alternative transportation is encouraged.
Berkeley has had recurring problems with parking meter vandalism. In 1999, over 2,400 Berkeley meters were jammed, smashed, or sawed apart. Starting in 2005 and continuing into 2006, Berkeley began to phase out mechanical meters in favor of more centralized electronic meters.
Transportation past.
The first commuter service to San Francisco was provided by the Central Pacific's Berkeley Branch Railroad, a standard gauge steam railroad, which terminated in downtown Berkeley, and connected in Emeryville (at a locale then known as "Shellmound") with trains to the Oakland ferry pier as well as with the Central Pacific main line starting in 1876. The Berkeley Branch line was extended from Shattuck and University to Vine Street ("Berryman's Station") in 1878. Starting in 1882, Berkeley trains ran directly to the Oakland Pier. In the 1880s, Southern Pacific assumed operations of the Berkeley Branch. In 1911, Southern Pacific electrified this line and the several others it constructed in Berkeley, creating its East Bay Electric Lines division. The huge and heavy cars specially built for these lines were called the "Red Trains" or the "Big Red Cars." The Shattuck line was extended and connected with two other Berkeley lines (the Ninth Street Line and the California Street line) at Solano and Colusa (the "Colusa Wye"). It was at this time that the Northbrae Tunnel and the Rose Street Undercrossing were constructed, both of which still exist (the Rose Street Undercrossing is not accessible to the public, being situated between what is now two backyards). The fourth Berkeley line was the Ellsworth St. line to the university campus. The last Red Trains ran in July 1941.
The first electric rail service in Berkeley was provided by several small streetcar companies starting in 1891. Most of these were eventually bought up by the Key System of Francis "Borax" Smith who added lines and improved equipment. The Key System's streetcars were operated by its East Bay Street Railways division. Principal lines in Berkeley ran on Euclid, The Arlington, College, Telegraph, Shattuck, San Pablo, University, and Grove (today's Martin Luther King Jr. Way). The last streetcars ran in 1948, replaced by buses.
The first electric commuter interurban-type trains to San Francisco from Berkeley were put in operation by the Key System in 1903, several years before the Southern Pacific electrified its steam commuter lines. Like the SP, Key trains ran to a pier serviced by the Key's own fleet of ferryboats, which also docked at the Ferry Building in San Francisco. After the Bay Bridge was built, the Key trains ran to the Transbay Terminal in San Francisco, sharing tracks on the lower deck of the Bay Bridge with the SP's red trains and the Sacramento Northern Railroad. It was at this time that the Key trains acquired their letter designations, which were later preserved by Key's public successor, AC Transit. Today's F bus is the successor of the F train. Likewise, the E, G and the H. Before the Bridge, these lines were simply the Shattuck Avenue Line, the Claremont Line, the Westbrae Line, and the Sacramento Street Line, respectively.
After the Southern Pacific abandoned transbay service in 1941, the Key System acquired the rights to use its tracks and catenary on Shattuck north of Dwight Way and through the Northbrae Tunnel to The Alameda for the F-train. The SP tracks along Monterey Avenue as far as Colusa had been acquired by the Key System in 1933 for the H-train, but were abandoned in 1941. The Key System trains stopped running in April 1958. In 1963, the Northbrae Tunnel was opened to auto traffic.
Economy.
Top employers.
According to the city's 2013 Comprehensive Annual Financial Report, the top employers in the city are:
Businesses.
Berkeley is the location of a number of nationally prominent businesses, many of which have been pioneers in their areas of operation. Notable businesses include Chez Panisse, birthplace of California cuisine, Peet's Coffee's original store, the Claremont Resort, punk rock haven 924 Gilman, and Saul Zaentz's Fantasy Studios. Notable former businesses include pioneer bookseller Cody's Books, The Nature Company, and the Berkeley Co-op.
Places.
Neighborhoods.
Berkeley has a number of distinct neighborhoods.
Surrounding the University of California campus are the most densely populated parts of the city. West of the campus is Downtown Berkeley, the city's traditional commercial core; home of the civic center, the city's only public high school, the busiest BART station in Berkeley, as well as a major transfer point for AC Transit buses. South of the campus is the Southside neighborhood, mainly a student ghetto, where much of the university's student housing is located. The busiest stretch of Telegraph Avenue is in this neighborhood. North of the campus is the quieter Northside neighborhood, the location of the Graduate Theological Union.
Further from the university campus, the influence of the University quickly becomes less visible. Most of Berkeley's neighborhoods are primarily made up of detached houses, often with separate in-law units in the rear, although larger apartment buildings are also common in many neighborhoods. Commercial activities are concentrated along the major avenues and at important intersections.
In the southeastern corner of the city is the Claremont District, home to the Claremont Hotel; and the Elmwood District, with a small shopping area on College Avenue. West of Elmwood is South Berkeley, known for its weekend flea market at the Ashby Station.
West of (and including) San Pablo Avenue, a major commercial corridor, is West Berkeley, the historic commercial center of the city, and the former unincorporated town of Ocean View. West Berkeley contains the remnants of Berkeley's industrial area, much of which has been replaced by retail and office uses, as well as residential live/work loft space, with the decline of manufacturing in the United States. The areas of South and West Berkeley are in the midst of redevelopment. Some residents have opposed redevelopment in this area. Along the shoreline of San Francisco Bay at the foot of University Avenue is the Berkeley Marina. Nearby is Berkeley's Aquatic Park, featuring an artificial linear lagoon of San Francisco Bay.
North of Downtown is the North Berkeley neighborhood, which has been nicknamed the "Gourmet Ghetto" because of the concentration of well-known restaurants and other food-related businesses. West of North Berkeley is Westbrae, a small neighborhood through which part of the Ohlone Greenway runs. Meanwhile, further north of North Berkeley are Northbrae, a master-planned subdivision from the early 20th century, and Thousand Oaks. Above these last three neighborhoods, in the northeastern part of Berkeley, are the Berkeley Hills. The neighborhoods of the Berkeley Hills such as Cragmont and La Loma Park are notable for their dramatic views, winding streets, and numerous public stairways and paths.
Parks and recreation.
The city has many parks, and promotes greenery and the environment. The city has planted trees for years and is a leader in the nationwide effort to re-tree urban areas. Tilden Regional Park, lies east of the city, occupying the upper extent of Wildcat Canyon between the Berkeley Hills and the San Pablo Ridge. The city is also heavily involved in creek restoration and wetlands restoration, including a planned "daylighting" of Strawberry Creek along Center Street. The Berkeley Marina and East Shore State Park flank its shoreline at San Francisco Bay and organizations like the Urban Creeks Council and Friends of the Five Creeks the former of which is headquartered in Berkeley support the riparian areas in the town and coastlines as well. César Chávez Park, near the Berkeley Marina, was built at the former site of the city dump.
Landmarks and historic districts.
165 buildings in Berkeley are designated as local landmarks or local structures of merit. Of these, 49 are listed in the National Register of Historic Places, including:
Historic Districts listed in the National Register of Historic Places:
See "List of Berkeley Landmarks, Structures of Merit, and Historic Districts"
Arts and culture.
Berkeley is home to the Chilean-American community's La Peña Cultural Center, the largest cultural center for this community in the United States. The Freight and Salvage is the oldest established full-time folk and traditional music venue west of the Mississippi River.
Education.
Colleges and universities.
University of California, Berkeley's main campus is in the city limits.
The Graduate Theological Union, a consortium of nine independent theological schools, is located a block north of the University of California Berkeley's main campus. The Graduate Theological Union has the largest number of students and faculty of any religious studies doctoral program in the United States. In addition, Berkeley City College is a community college in the Peralta Community College District.
Primary and secondary schools.
The Berkeley Unified School District operates public schools.
The first public school in Berkeley was the Ocean View School, now the site of the Berkeley Adult School located at Virginia Street and San Pablo Avenue. The public schools today are administered by the Berkeley Unified School District. In the 1960s, Berkeley was one of the earliest US cities to voluntarily desegregate, utilizing a system of buses, still in use. The city has one public high school, Berkeley High School (BHS). Established in 1880, BHS currently has over 3,000 students. The Berkeley High campus was designated a historic district by the National Register of Historic Places on January 7, 2008. Saint Mary's College High School, a Catholic school, has its street address in Berkeley, although most of the grounds and buildings are actually in neighboring Albany. Berkeley has 11 elementary schools and three middle schools.
There is also the Bay Area Technology School, the only school in the whole Bay Area to offer a technology- and science-based curriculum, with connections to leading universities.
Public libraries.
Berkeley Public Library serves as the municipal library. University of California, Berkeley Libraries operates the University of California Berkeley libraries.
Government.
Berkeley has a mayor-council government. The mayor, currently Tom Bates, is elected for a four-year term. The Berkeley City Council has eight council members who each serve four-year terms. The current councilmembers are:
The mayor appoints a city administrator, who is subject to confirmation by the city council.
Berkeley is also part of Alameda County, for which the Government of Alameda County is defined and authorized under the California Constitution, California law, and the Charter of the County of Alameda. The County government provides countywide services such as elections and voter registration, law enforcement, jails, vital records, property records, tax collection, public health, and social services. The County government is primarily composed of the elected five-member Board of Supervisors, other elected offices including the Sheriff/Coroner, the District Attorney, Assessor, Auditor-Controller/County Clerk/Recorder, and Treasurer/Tax Collector, and numerous county departments and entities under the supervision of the County Administrator.
Sister cities.
Berkeley has 14 sister cities:

</doc>
<doc id="4861" url="http://en.wikipedia.org/wiki?curid=4861" title="Bolventor">
Bolventor

Bolventor () is a hamlet on Bodmin Moor in Cornwall, United Kingdom. It is situated in Altarnun civil parish between Launceston and Bodmin.
Bolventor is the location of the famous Jamaica Inn coaching inn. It is bypassed by a dual carriageway section of the A30 trunk road; before the bypass was built the village straddled the A30 road.
Daphne du Maurier, a former resident, chose Bolventor as the setting for her novel about Cornish smugglers titled "Jamaica Inn". The inn that inspired the novel, Jamaica Inn, has stood beside the main road through the village since 1547. It is now a tourist attraction in its own right and dominates the village.
The small church (dedicated to the Holy Trinity) that lies to the east of the village closed some years ago. A mile from Bolventor there was a chapel of St Luke (from the 13th to the early 16th century): the font is now at the church of Tideford. Bolventor parish was established in 1846 (before that date the village was in St Neot parish) but has now been merged with Altarnun.
The village has been said to take its name from the "Bold Venture" that it must have appeared to build a farm in this moorland, but this is probably folk etymology, as "Bol-" is a common prefix in Cornish placenames. It is much more likely that the name derives from the 'Bold Adventure' tin-working area which was in operation near Jamaica Inn during the 1840s-1850s 

</doc>
<doc id="4862" url="http://en.wikipedia.org/wiki?curid=4862" title="Bengal">
Bengal

Bengal ( or "Bongo"/ "Bônggô") is a geographical and ethno-linguistic region in South Asia. It lies in the north-eastern region at the apex of the Bay of Bengal, and is dominated by the fertile Bengal delta. The region was politically divided in 1947 and today comprises the nation of Bangladesh and the Indian state of West Bengal, although some regions of the previous kingdoms of Bengal are now part of the neighbouring Indian states of Assam, Tripura, Bihar, Jharkhand and Odisha. The region of Bengal is natively inhabited by Bengali people ( "Bangali") who speak the Bengali language ( "Bangla").
The region of Bengal is one of the most densely populated regions on Earth, with an estimated population of more than 250 million people and a population density exceeding 900/km². Most of the Bengal region lies in the low-lying Ganges–Brahmaputra River Delta or Ganges Delta, the world's largest delta. In the southern part of the delta lies the Sundarbans—the world's largest mangrove forest and home of the Bengal tiger. In the coastal southeast lies Cox's Bazar, the world's longest beach with a length of . Though the population of the region is mostly rural and agrarian, two megacities, Kolkata (previously Calcutta) and Dhaka (previously Dacca); are located in Bengal. The Bengal region is renowned for its rich literary and cultural heritage as well as its immense political and intellectual contribution to South Asian history through the Bengal Renaissance, revolutionary activities, its central role in the Indian independence movement, the Bangla language movement, and the Bangladesh War of Independence.
Etymology.
The exact origin of the word "Bangla" and "Bongo" or Bengal is unknown, though the word is believed to be derived from the Dravidian-speaking tribe called "Bang" that settled in the area around the year 1000 BC.
It could also be derived from the word "Vanga," which was a kingdom in the Bengal region during the times of Mahabharata as mentioned in Sanskrit literature.
History.
Remnants of Copper Age settlements in the Bengal region date back 4,300 years. After the arrival of Indo-Aryans, the kingdoms of Anga, Bongo, and Magadha were formed by the 10th century BC, located in the Bihar and Bengal regions. Magadha was one of the four main kingdoms of India at the time of Buddha and consisted of several Janapadas. One of the earliest foreign references to Bengal is the mention of a land named Gangaridai by the Greeks around 100 BC, located in an area in Bengal. From the 3rd to the 6th centuries AD, the kingdom of Magadha served as the seat of the Gupta Empire.
Two kingdoms – Vanga or Samatata and Gauda – are mentioned in some texts to have appeared after the end of Gupta Empire, although details of their ruling time are uncertain. The first recorded independent king of Bengal was Shashanka who reigned in the early 7th century. After a period of anarchy, the native Buddhist Pala Empire ruled the region for four hundred years, and expanded across a large part of South Asia during the reigns of Dharmapala and Devapala. Bengal was invaded by a Hindu Emperor Rajendra Chola I of the Chola dynasty for a short period in the 11th century. The Pala dynasty was followed by the reign of the Hindu Sena dynasty. Islam started to appear in Bengal during the late 11th Century, in the form of Sufism. Beginning in 1202 a military commander from the Delhi Sultanate, Bakhtiar Khilji, overran Bihar and Bengal as far east as Rangpur, Bogra, and the Brahmaputra River. Although he failed to bring Bengal under his control, the expedition managed to defeat Lakshman Sen and his two sons, who moved to Bikramapur (present-day Munshiganj District), from where they ruled over a smaller area until the late 13th century.
From the 13th century onward, several Islamic dynasties ruled parts of Bengal, often known collectively as the Sultanate of Bengal. Some rulers such as the land-lords-Baro-Bhuyans, the Deva Kingdom, and Raja Ganesha ruled parts of the region intermittently. Bengal came once more under the direct control of Delhi when the Mughals conquered it in 1576. It became a Mughal "subah" and was ruled by "subahdars" (governors). Akbar exercised progressive rule and oversaw a period of prosperity (through trade and development) in Bengal and northern India. There were several independent Hindu states established in Bengal during the Mughal period like those of Maharaja Pratap Aditya of Jessore and Raja Sitaram Ray of Burdwan.
Bengal's trade and wealth impressed the Mughals so much that emperor Aurangzeb called the region the "Paradise of the Nations." Afghans under Sher Shah Suri and his descendants ruled Bengal from 1540 to 1560. Hindu king Hem Chandra Vikramaditya (Hemu) defeated and killed Bengal ruler Muhammed Shah in 1556 and appointed Shahbaaz Khan as his governor. Administration by governors appointed by the court of the Mughal Empire court (1575–1717) gave way to four decades of semi-independence under the Nawabs of Murshidabad, who respected the nominal sovereignty of the Mughals in Delhi. The Nawabs granted permission to the French East India Company to establish a trading post at Chandernagore in 1673, and the British East India Company at Calcutta in 1690.
Around the early 1700s, the Maratha Empire led expeditions in Bengal. The leader of the expedition was Maratha Maharaja Raghuji of Nagpur. Raghoji was able to annexe Odisha and parts of Bengal permanently as he successfully exploited the chaotic conditions prevailing in the region after the death of their Governor Murshid Quli Khan in 1727. Portuguese traders arrived late in the fifteenth century, once Vasco da Gama reached India by sea in 1498. European influence grew until the British East India Company gained taxation rights in Bengal "subah", or province, following the Battle of Plassey in 1757, when Siraj ud-Daulah, the last independent Nawab, was defeated by the British. The Bengal Presidency was established by 1766, eventually including all British territories north of the Central Provinces (now Madhya Pradesh), from the mouths of the Ganges and the Brahmaputra to the Himalayas and the Punjab. The Bengal famine of 1770 claimed millions of lives. Calcutta was named the capital of British India in 1772. The Bengal Renaissance and Brahmo Samaj socio-cultural reform movements had great impact on the cultural and economic life of Bengal. The failed Indian rebellion of 1857 started near Calcutta and resulted in transfer of authority to the British Crown, administered by the Viceroy of India. Between 1905 and 1911, an abortive attempt was made to divide the province of Bengal into two zones.
Bengal has played a major role in the Indian independence movement, in which revolutionary groups were dominant. Armed attempts to overthrow the British Raj reached a climax when Subhas Chandra Bose led the Indian National Army against the British. Bengal was also central in the rising political awareness of the Muslim population—the Muslim League was established in Dhaka in 1906. In spite of a last-ditch effort to form a United Bengal, when India gained independence in 1947, Bengal was partitioned along religious lines. The western part went to India (and was named West Bengal) while the eastern part joined Pakistan as a province called East Bengal (later renamed East Pakistan, giving rise to Bangladesh in 1971). The circumstances of partition were bloody, with widespread religious riots in Bengal.
In East Pakistan, starting from the Bengali Language Movement of 1952, political dissent against West Pakistani domination grew steadily. The Awami League, led by Sheikh Mujibur Rahman, emerged as the political voice of the Bengali-speaking population of East Pakistan by the 1960s. In 1971, the crisis deepened when Rahman was arrested and a sustained military assault was launched on East Pakistan. Most of the Awami League leaders fled and set up a government-in-exile in West Bengal. The guerrilla Mukti Bahini and Bengali regulars eventually received support from the Indian Armed Forces in December 1971, resulting in a decisive victory over Pakistan on 16 December in the Bangladesh Liberation War or Indo-Pakistani War of 1971. The independent nation of Bangladesh was established. However, the nation of Bangladesh, since its creation, suffered from continuous political instability and prolonged martial and autocratic rules.
West Bengal, the western part of Bengal, became a state in India. In the 1960s and 1970s, severe power shortages, strikes and a violent Marxist-Naxalite movement damaged much of the state's infrastructure, leading to a period of economic stagnation. The Bangladesh Liberation War of 1971 resulted in the influx of millions of refugees to West Bengal, causing significant strains on its infrastructure. West Bengal politics underwent a major change when the Left Front won the 1977 assembly election, defeating the incumbent Indian National Congress. The Left Front, led by Communist Party of India (Marxist) (CPI(M)) governed the state for over three decades.
Geography.
Most of the Bengal region is in the low-lying Ganges–Brahmaputra River Delta or Ganges Delta. The Ganges Delta arises from the confluence of the rivers Ganges, Brahmaputra, and Meghna rivers and their respective tributaries. The total area of Bengal is 232,752  km²—West Bengal is and Bangladesh .
Most parts of Bangladesh are within above the sea level, and it is believed that about 10% of the land would be flooded if the sea level were to rise by . Because of this low elevation, much of this region is exceptionally vulnerable to seasonal flooding due to monsoons.
The highest point in Bangladesh is in Mowdok range at in the Chittagong Hill Tracts to the southeast of the country. A major part of the coastline comprises a marshy jungle, the Sundarbans, the largest mangrove forest in the world and home to diverse flora and fauna, including the Royal Bengal Tiger. In 1997, this region was declared endangered.
West Bengal is on the eastern bottleneck of India, stretching from the Himalayas in the north to the Bay of Bengal in the south. The state has a total area of . The Darjeeling Himalayan hill region in the northern extreme of the state belongs to the eastern Himalaya. This region contains Sandakfu ()—the highest peak of the state. The narrow Terai region separates this region from the plains, which in turn transitions into the Ganges delta towards the south. The Rarh region intervenes between the Ganges delta in the east and the western plateau and high lands. A small coastal region is on the extreme south, while the Sundarbans mangrove forests form a remarkable geographical landmark at the Ganges delta.
At least nine districts in West Bengal and 42 districts in Bangladesh have arsenic levels in groundwater above the World Health Organization maximum permissible limit of 50 µg/L (micro gram per litre) or 50 parts per billion and the untreated water is unfit for human consumption. The water causes arsenicosis, skin cancer and various other complications in the body. Arsenic is four times as poisonous as mercury.
Major cities.
The following are the largest cities in Bengal (in terms of population):
Demographics.
According to provisional results of 2011 Bangladesh census, population of Bangladesh was 142,319,000, however, CIA's "The World Factbook" gives 163,654,860 as its population in a July 2013 estimate. According to the provisional results of the 2011 Indian national census, West Bengal has a population of 91,347,736. So, the Bengal region, as of 2011, has at least 233 million people. This figures give a population density of 1003.9/km²; making it among the most densely populated areas in the world.
Bengali is the main language spoken in Bengal. English is often used for official work. There are small minorities who speak Hindi, Urdu, Chakma. There are several tribal languages including Santhali and Nepali language in Darjeeling.
Life expectancy is around 70.36 years for Bangladesh and 63.4 for West Bengal. In terms of literacy, West Bengal leads with 77% literacy rate, in Bangladesh the rate is approximately 59.82%. The level of poverty and illiteracy is high, the proportion of people living below the poverty line is more than 30%.
About 20,000 people live on chars. Chars are temporary islands formed by the deposition of sediments eroded off the banks of the Ganges in West Bengal which often disappear in the monsoon season. They are made of very fertile soil. The inhabitants of chars are not recognised by the Government of West Bengal on the grounds that it is not known whether they are Bengalis or Bangladeshi refugees. Consequently, no identification documents are issued to char-dwellers who cannot benefit from health care, barely survive due to very poor sanitation and are prevented from emigrating to the mainland to find jobs when they have turned 14. On a particular char it was reported that 13% of women died at childbirth.
Economy.
Agriculture is the leading occupation in the region. Rice is the staple food crop. Other food crops are pulses, potato, maize, and oil seeds. Jute is the principal cash crop. Tea is also produced commercially; the region is well known for Darjeeling and other high-quality teas.
Historically, Europe once regarded Bengal as "the richest country to trade with".
West Bengal.
The service sector is the largest contributor to the gross domestic product of West Bengal, contributing 51% of the state domestic product compared to 27% from agriculture and 22% from industry. State industries are localised in the Kolkata region and the mineral-rich western highlands. Durgapur–Asansol colliery belt is home to a number of major steel plants.
 West Bengal had the third-largest economy in India, with a net state domestic product of US$21.5 billion. During 2001–2002, the state's average SDP was more than 7.8%—outperforming the National GDP Growth. The state has promoted foreign direct investment, which has mostly occurred in the software and electronics fields;
Kolkata is becoming a major hub for the information technology (IT) industry. Owing to the boom in Kolkata's and the overall state's economy, West Bengal had the third-fastest-growing economy in India. It is also known as the cultural capital of India.
Bangladesh.
In December 2005, the Central Bank of Bangladesh projected GDP growth around 6.5%. Although two-thirds of Bangladeshis are farmers, more than three-quarters of Bangladesh's export earnings come from the garment industry,
which began attracting foreign investors in the 1980s due to cheap labour and low conversion cost. In 2002, the industry exported US$5 billion worth of products.
The industry now employs more than 3 million workers, 90% of whom are women.
A large part of foreign currency earnings also comes from the remittances sent by expatriates living in other countries.
The provision of microcredit by Grameen Bank (founded by Muhammad Yunus) and by other similar organisations has contributed to the development of the economy of Bangladesh. Together, such organisations had about 5 million members by the late 1990s.
Culture.
The common Bengali language and culture anchors the shared tradition of two parts of politically divided Bengal. Bengal has a long tradition in folk literature, evidenced by the "Charyapada", "Mangalkavya", "Shreekrishna Kirtana", "Maimansingha Gitika" or "Thakurmar Jhuli". Bengali literature in the medieval age was often either religious (e.g. Chandidas), or adaptations from other languages (e.g. Alaol). During the Bengal Renaissance of the nineteenth and twentieth centuries, Bengali literature was modernised through the works of authors such as Michael Madhusudan Dutta, Bankim Chandra Chattopadhyay, Rabindranath Tagore, Ishwar Chandra Vidyasagar and Kazi Nazrul Islam.
The Baul tradition is a unique heritage of Bangla folk music. The scholar saint Sri Anirvan loved Baul music, and in fact described himself as a simple Baul. Other folk music forms include Gombhira, Bhatiali and Bhawaiya. Folk music in Bengal is often accompanied by the ektara, a one-stringed instrument. Other instruments include the dotara, dhol, flute, and tabla. The region also has an active heritage in North Indian classical music.
Bengal had also been the harbinger of modernism in Indian arts. Abanindranath Tagore, one of the important 18th century artist from Bengal is often referred to as the father of Indian modern art. He had established the first non-British art academy in India known as the Kalabhavan within the premises of Santiniketan. Santiniketan in course of time had produced many important Indian artists like Gaganendranath Tagore, Nandalal Bose, Jamini Roy, Benode Bihari Mukherjee and Ramkinkar Baij. In the post-independence era, Bengal had produced important artists like Somenath Hore, Meera Mukherjee and Ganesh Pyne.
Rice and fish are traditional favourite foods, leading to a saying that in Bengali, "mach ar bhaath bangali baanaay", that translates as "fish and rice make a Bengali". Bengal's vast repertoire of fish-based dishes includes Hilsa preparations, a favourite among Bengalis. Bengalis make distinctive sweetmeats from milk products, including "Rôshogolla", "Chômchôm", and several kinds of "Pithe".
Bengali women commonly wear the "shaŗi" and the salwar kameez, often distinctly designed according to local cultural customs. In urban areas, many women and men wear Western-style attire. Among men, European dressing has greater acceptance. Men also wear traditional costumes such as the "kurta" with "dhoti" or "pyjama", often on religious occasions. The lungi, a kind of long skirt, is widely worn by Bangladeshi men.
The greatest religious festivals are the two Eids (Eid ul-Fitr and Eid ul-Adha) for the Muslims, and the autumnal Durga Puja for Hindus. Christmas (called "Bôŗodin" (Great day) in Bangla), Buddha Purnima are other major religious festivals. Other festivities include Pohela Baishakh (the Bengali New Year), Basanta-Utsab, Nobanno, and "Poush parbon" (festival of Poush).
Around 200 dailies are published in Bangladesh, along with more than 1800 periodicals. West Bengal had 559 published newspapers in 2005, of which 430 were in Bangla. Cricket and football are popular sports in the Bengal region. Local games include sports such as Kho Kho and Kabaddi, the later being the national sport of Bangladesh. An Indo-Bangladesh "Bangla Games" has been organised among the athletes of the Bengali speaking areas of the two countries.
Intra-Bengal relations today.
Geographic, cultural, historic, and commercial ties are growing, and both countries recognise the importance of good relations. During and immediately after Bangladesh's struggle for independence from Pakistan in 1971, India assisted refugees from East Pakistan, and intervened militarily to help bring about the independence of Bangladesh. The Indo-Bangladesh border length of , West Bengal has a border length of . Despite overlapping historic, geographic and cultural ties, the relation between West Bengal and Bangladesh is still well below the potential. The pan-Bengali sentiment among the people of the two parts of Bengal was at its height during the 1971 Bangladesh Liberation War. While the government radio and national press in India might have backed the struggle out of strategic considerations, the Bengali broadcast and print media went out of its way to lend overwhelming support.
Frequent air services link Kolkata with Dhaka and Chittagong. A bus service between Kolkata and Dhaka is operational. The primary road link is the Jessore Road which crosses the border at Petrapole-Benapole about north-west of Kolkata. The Train service between Kolkata and Dhaka, which was stopped after the Indo-Pakistani War of 1965, was resumed in 2008.
Visa services are provided by Bangladesh's consulate at Kolkata's Bangabandhu Sheikh Mujibur Rahman Road and India's high commissions in Dhaka, Chittagong and Rajshahi. India has a liberal visa policy and nearly 500,000 visas are issued every year to Bangladeshi students, tourists, health-tourists and others who visit West Bengal and often transit to other parts of India.
Undocumented immigration of Bangladeshi workers is a controversial issue championed by right-wing nationalist parties in India but finds little sympathy in West Bengal. India has fenced the border to control this flow but immigration is still continuing.
The official land border crossing at Petrapole-Benapole is the primary conduit for the over $1 billion trade between the two-halves of Bengal. The volume of unofficial exports to Bangladesh from India is reportedly in the range of $350–500 million each year.
External links.
Geo Links for Bengal
Perry-Castañeda Library Map Collection at University of Texas at Austin Libraries

</doc>
<doc id="4864" url="http://en.wikipedia.org/wiki?curid=4864" title="Bucket argument">
Bucket argument

Isaac Newton's rotating bucket argument (also known as "Newton's bucket") was designed to demonstrate that true rotational motion cannot be defined as the relative rotation of the body with respect to the immediately surrounding bodies. It is one of five arguments from the "properties, causes, and effects" of true motion and rest that support his contention that, in general, true motion and rest cannot be defined as special instances of motion or rest relative to other bodies, but instead can be defined only by reference to absolute space. Alternatively, these experiments provide an operational definition of what is meant by "absolute rotation", and do not pretend to address the question of "rotation relative to "what"?".
Background.
These arguments, and a discussion of the distinctions between absolute and relative time, space, place and motion, appear in a General Scholium at the very beginning of Newton's work, "The Mathematical Principles of Natural Philosophy" (1687), which established the foundations of classical mechanics and introduced his law of universal gravitation, which yielded the first quantitatively adequate dynamical explanation of planetary motion. See the "Principia" on line at , pp. 77–82.
Despite their embrace of the principle of rectilinear inertia and the recognition of the kinematical relativity of apparent motion (which underlies whether the Ptolemaic or the Copernican system is correct), natural philosophers of the seventeenth century continued to consider true motion and rest as physically separate descriptors of an individual body. The dominant view Newton opposed was devised by René Descartes, and was supported (in part) by Gottfried Leibniz. It held that empty space is a metaphysical impossibility because space is nothing other than the extension of matter, or, in other words, that when one speaks of the space between things one is actually making reference to the relationship that exists between those things and not to some entity that stands between them. Concordant with the above understanding, any assertion about the motion of a body boils down to a description over time in which the body under consideration is at t1 found in the vicinity of one group of "landmark" bodies and at some t2 is found in the vicinity of some other "landmark" body or bodies.
Descartes recognized that there would be a real difference, however, between a situation in which a body with movable parts and originally at rest with respect to a surrounding ring was itself accelerated to a certain angular velocity with respect to the ring, and another situation in which the surrounding ring was given a contrary acceleration with respect to the central object. With sole regard to the central object and the surrounding ring, the motions would be indistinguishable from each other assuming that both the central object and the surrounding ring were absolutely rigid objects. However, if neither the central object nor the surrounding ring were absolutely rigid then the parts of one or both of them would tend to fly out from the axis of rotation.
Here is an everyday experience of the basic nature of the Descartes experiment: Consider sitting in your train and noticing a train originally at rest beside you in the railway station pulling away. Initially you think it is your own train accelerating, but then notice with surprise that you feel no force. Thus, it is not your own train moving, but the neighboring train. On the other hand, you would confirm your own train is accelerating if you sensed "g"-forces from the acceleration of your own train.
For contingent reasons having to do with the Inquisition, Descartes spoke of motion as both absolute and relative. However, his real position was that motion is absolute.
A contrasting position was taken by Ernst Mach, who contended that all motion was "relative".
The argument.
Newton discusses a bucket filled with water hung by a cord. If the cord is twisted up tightly on itself and then the bucket is released, it begins to spin rapidly, not only with respect to the experimenter, but also in relation to the water it contains. (This situation would correspond to diagram B above.)
Although the relative motion at this stage is the greatest, the surface of the water remains flat, indicating that the parts of the water have no tendency to recede from the axis of relative motion, despite proximity to the pail. Eventually, as the cord continues to unwind, the surface of the water assumes a concave shape as it acquires the motion of the bucket spinning relative to the experimenter. This concave shape shows that the water is rotating, despite the fact that the water is at rest relative to the pail. In other words, it is not the relative motion of the pail and water that causes concavity of the water, contrary to the idea that motions can only be relative, and that there is no absolute motion. (This situation would correspond to diagram D.) Possibly the concavity of the water shows rotation relative to "something else": say absolute space? Newton says: "One can find out and measure the true and absolute circular motion of the water".
In the 1846 Andrew Motte translation of Newton's words:
The argument that the motion is absolute, not relative, is incomplete, as it limits the participants relevant to the experiment to only the pail and the water, a limitation that has not been established. In fact, the concavity of the water clearly involves gravitational attraction, and by implication the Earth also is a participant. Here is a critique due to Mach arguing that only relative motion is established:
All observers agree that the surface of rotating water is curved. However, the explanation of this curvature involves centrifugal force for all observers with the exception of a truly stationary observer, who finds the curvature is consistent with the rate of rotation of the water as they observe it, with no need for an additional centrifugal force. Thus, a stationary frame can be identified, and it is not necessary to ask "Stationary with respect to what?":
A supplementary thought experiment with the same objective of determining the occurrence of absolute rotation also was proposed by Newton: the example of observing two identical spheres in rotation about their center of gravity and tied together by a string. Occurrence of tension in the string is indicative of absolute rotation; see Rotating spheres.
Detailed analysis.
The historic interest of the rotating bucket experiment is its usefulness in suggesting one can detect absolute rotation by observation of the shape of the surface of the water. However, one might question just how rotation brings about this change. Below are two approaches to understanding the concavity of the surface of rotating water in a bucket.
Newton's laws of motion.
The shape of the surface of a rotating liquid in a bucket can be determined using Newton's laws for the various forces on an element of the surface. For example, see Knudsen and Hjorth. The analysis begins with the free body diagram in the co-rotating frame where the water appears stationary. The height of the water "h" = "h"("r") is a function of the radial distance "r" from the axis of rotation Ω, and the aim is to determine this function. An element of water volume on the surface is shown to be subject to three forces: the vertical force due to gravity Fg, the horizontal, radially outward centrifugal force FCfgl, and the force normal to the surface of the water Fn due to the rest of the water surrounding the selected element of surface. The force due to surrounding water is known to be normal to the surface of the water because a liquid in equilibrium cannot support shear stresses. To quote Anthony and Brackett: Moreover, because the element of water does not move, the sum of all three forces must be zero. To sum to zero, the force of the water must point oppositely to the sum of the centrifugal and gravity forces, which means the surface of the water must adjust so its normal points in this direction. (A very similar problem is the design of a , where the slope of the turn is set so a car will not slide off the road. The analogy in the case of rotating bucket is that the element of water surface will "slide" up or down the surface unless the normal to the surface aligns with the vector resultant formed by the vector addition Fg + FCfgl.)
As "r" increases, the centrifugal force increases according to the relation (the equations are written per unit mass):
where "Ω" is the constant rate of rotation of the water. The gravitational force is unchanged at
where "g" is the acceleration due to gravity. These two forces add to make a resultant at an angle "φ" from the vertical given by
which clearly becomes larger as "r" increases. To ensure that this resultant is normal to the surface of the water, and therefore can be effectively nulled by the force of the water beneath, the normal to the surface must have the same angle, that is,
leading to the ordinary differential equation for the shape of the surface:
or, integrating:
where "h"(0) is the height of the water at "r" = 0. In words, the surface of the water is parabolic in its dependence upon the radius.
Potential energy.
The shape of the water's surface can be found in a different, very intuitive way using the interesting idea of the potential energy associated with the centrifugal force in the co-rotating frame.
In a reference frame uniformly rotating at angular rate Ω, the fictitious centrifugal force is conservative and has a potential energy of the form:
where "r" is the radius from the axis of rotation. This result can be verified by taking the gradient of the potential to obtain the radially outward force:
The meaning of the potential energy is that movement of a test body from a larger radius to a smaller radius involves doing work against the centrifugal force.
The potential energy is useful, for example, in understanding the concavity of the water surface in a rotating bucket. Notice that at equilibrium the surface adopts a shape such that an element of volume at any location on its surface has the same potential energy as at any other. That being so, no element of water on the surface has any incentive to move position, because all positions are equivalent in energy. That is, equilibrium is attained. On the other hand, were surface regions with lower energy available, the water occupying surface locations of higher potential energy would move to occupy these positions of lower energy, inasmuch as there is no barrier to lateral movement in an ideal liquid.
We might imagine deliberately upsetting this equilibrium situation by somehow momentarily altering the surface shape of the water to make it different from an equal-energy surface. This change in shape would not be stable, and the water would not stay in our artificially contrived shape, but engage in a transient exploration of many shapes until non-ideal frictional forces introduced by sloshing, either against the sides of the bucket or by the non-ideal nature of the liquid, killed the oscillations and the water settled down to the equilibrium shape.
To see the principle of an equal-energy surface at work, imagine gradually increasing the rate of rotation of the bucket from zero. The water surface is flat at first, and clearly a surface of equal potential energy because all points on the surface are at the same height in the gravitational field acting upon the water. At some small angular rate of rotation, however, an element of surface water can achieve lower potential energy by moving outward under the influence of the centrifugal force. Because water is incompressible and must remain within the confines of the bucket, this outward movement increases the depth of water at the larger radius, increasing the height of the surface at larger radius, and lowering it at smaller radius. The surface of the water becomes slightly concave, with the consequence that the potential energy of the water at the greater radius is increased by the work done against gravity to achieve the greater height. As the height of water increases, movement toward the periphery becomes no longer advantageous, because the reduction in potential energy from working with the centrifugal force is balanced against the increase in energy working against gravity. Thus, at a given angular rate of rotation, a concave surface represents the stable situation, and the more rapid the rotation, the more concave this surface. If rotation is arrested, the energy stored in fashioning the concave surface must be dissipated, for example through friction, before an equilibrium flat surface is restored.
To implement a surface of constant potential energy quantitatively, let the height of the water be formula_10: then the potential energy per unit mass contributed by gravity is formula_11 and the total potential energy per unit mass on the surface is
with formula_13 the background energy level independent of "r". In a static situation (no motion of the fluid in the rotating frame), this energy is constant independent of position "r". Requiring the energy to be constant, we obtain the parabolic form:
where "h(0)" is the height at "r" = 0 (the axis). See Figures 1 and 2.
The principle of operation of the centrifuge also can be simply understood in terms of this expression for the potential energy, which shows that it is favorable energetically when the volume far from the axis of rotation is occupied by the heavier substance.

</doc>
<doc id="4865" url="http://en.wikipedia.org/wiki?curid=4865" title="Breviary">
Breviary

A breviary (from Latin "brevis", 'short' or 'concise') is a liturgical book of the Latin liturgical rites of the Catholic Church containing the public or canonical prayers, hymns, the Psalms, readings, and notations for everyday use, especially by bishops, priests, and deacons in the Divine Office (i.e., at the canonical hours or Liturgy of the Hours, the Christians' daily prayer). The word can also refer to a collection of Christian orders of prayers and readings, such as contained in Anglican or Lutheran resources. In general, the word "breviary" may be used to refer to an abridged version of any text or a brief account or summary of some subject, but is primarily used to refer to the Catholic liturgical book. 
The volume containing the daily hours of Roman Catholic prayer was published as the "Breviarium Romanum" (Roman Breviary) until the reforms of Paul VI, when it became known as the Liturgy of the Hours. However, these terms are used interchangeably to refer to the Office in all its forms. This entry deals with the Breviary prior to the changes introduced by Pope Paul VI in 1974.
Origin of name.
This word breviary (Lat. Breviarium), signifies in its primary acceptation an abridgment, or a compendium. It is often employed in this sense by Christian authors, e.g. Breviarium fidei, Breviarium in psalmos, Breviarium canonum, Breviarium regularum. In liturgical language Breviary has a special meaning, indicating a book furnishing the regulations for the celebration of Mass or the canonical Office, and may be met with under the titles Breviarium Ecclesiastici Ordinis, or Breviarium Ecclesiæ Rominsæ (Romanæ). In the ninth century Alcuin uses the word to designate an office abridged or simplified for the use of the laity. Prudentius of Troyes, about the same period, composed a Breviarium Psalterii (v. inf. V. HISTORY). In an ancient inventory occurs Breviarium Antiphonarii, meaning "Extracts from the Antiphonary". In the "Vita Aldrici" occurs "sicut in plenariis et breviariis Ecclesiæ ejusdem continentur". Again, in the inventories in the catalogues, such notes as these may be met with: "Sunt et duo cursinarii et tres benedictionales Libri; ex his unus habet obsequium mortuorum et unus Breviarius", or, "Præter Breviarium quoddam quod usque ad festivitatem S. Joannis Baptistæ retinebunt", etc. Monte Cassino about A.D. 1100 obtained a book titled "Incipit Breviarium sive Ordo Officiorum per totam anni decursionem".
From such references, and from others of a like nature, Quesnel gathers that by the word Breviarium was at first designated a book furnishing the rubrics, a sort of Ordo. The title Breviary, as we employ it—that is, a book containing the entire canonical office—appears to date from the eleventh century. 
St. Gregory VII having, indeed, abridged the order of prayers, and having simplified the Liturgy as performed at the Roman Court, this abridgment received the name of Breviary, which was suitable, since, according to the etymology of the word, it was an abridgment. The name has been extended to books which contain in one volume, or at least in one work, liturgical books of different kinds, such as the Psalter, the Antiphonary, the Responsoriary, the Lectionary, etc. In this connection it may be pointed out that in this sense the word, as it is used nowadays, is illogical; it should be named a Plenarium rather than a Breviarium, since, liturgically speaking, the word Plenarium exactly designates such books as contain several different compilations united under one cover. This is pointed out, however, simply to make still clearer the meaning and origin of the word; and section V will furnish a more detailed explanation of the formation of the Breviary.
History.
Early history.
The canonical hours of the Breviary owe their remote origin to the Old Covenant when God commanded the Aaronic priests to offer morning and evening sacrifices. Other inspiration may have come from David's words in the Psalms "Seven times a day I praise you" (Ps. 119:164), as well as, "the just man meditates on the law day and night" (Ps. 1:2). 
In the early days of Christian worship the Sacred Scriptures furnished all that was thought necessary, containing as it did the books from which the lessons were read and the psalms that were recited. The first step in the evolution of the Breviary was the separation of the Psalter into a choir-book. At first the president of the local church (bishop) or the leader of the choir chose a particular psalm as he thought appropriate. From about the 4th century certain psalms began to be grouped together, a process that was furthered by the monastic practice of daily reciting the 150 psalms. This took so much time that the monks began to spread it over a week, dividing each day into hours, and allotting to each hour its portion of the Psalter. St Benedict in the 6th century drew up such an arrangement, probably, though not certainly, on the basis of an older Roman division which, though not so skilful, is the one in general use. Gradually there were added to these psalter choir-books additions in the form of antiphons, responses, collects or short prayers, for the use of those not skilful at improvisation and metrical compositions. Jean Beleth, a 12th-century liturgical author, gives the following list of books necessary for the right conduct of the canonical office: the Antiphonarium, the Old and New Testaments, the "Passionarius (liber)" and the "Legendarius" (dealing respectively with martyrs and saints), the "Homiliarius" (homilies on the Gospels), the "Sermologus" (collection of sermons) and the works of the Fathers, besides, of course, the "Psalterium" and the "Collectarium". To overcome the inconvenience of using such a library the Breviary came into existence and use. Already in the 8th century Prudentius, bishop of Troyes, had in a "Breviarium Psalterii" made an abridgment of the Psalter for the laity, giving a few psalms for each day, and Alcuin had rendered a similar service by including a prayer for each day and some other prayers, but no lessons or homilies. The Breviary rightly so called, however, only dates from the 11th century; the earliest MS. containing the whole canonical office is of the year 1099 and is in the Mazarin library. Gregory VII (pope 1073–1085), too, simplified the liturgy as performed at the Roman court, and gave his abridgment the name of Breviary, which thus came to denote a work which from another point of view might be called a Plenary, involving as it did the collection of several works into one. There are several extant specimens of 12th-century Breviaries, all Benedictine, but under Innocent III (pope 1198–1216) their use was extended, especially by the newly founded and active Franciscan order. These preaching friars, with the authorization of Gregory IX, adopted (with some modifications, e.g. the substitution of the "Gallican" for the "Roman" version of the Psalter) the Breviary hitherto used exclusively by the Roman court, and with it gradually swept out of Europe all the earlier partial books (Legendaries, Responsories), &c., and to some extent the local Breviaries, like that of Sarum. Finally, Nicholas III (pope 1277–1280) adopted this version both for the curia and for the basilicas of Rome, and thus made its position secure.
Local and regular breviaries.
The Benedictines and Dominicans have Breviaries of their own. The only other types that merit notice are:
Early modern reforms.
Until the council of Trent every bishop had full power to regulate the Breviary of his own diocese; and this was acted upon almost everywhere. Each monastic community, also, had one of its own. Pius V (pope 1566–1572), however, while sanctioning those which could show at least 200 years of existence, made the Roman obligatory in all other places. But the influence of the Roman rite has gradually gone much beyond this, and has superseded almost all the local uses. The Roman has thus become nearly universal, with the allowance only of additional offices for saints specially venerated in each particular diocese. The Roman Breviary has undergone several revisions: The most remarkable of these is that by Francis Quignonez, cardinal of Santa Croce in Gerusalemme (1536), which, though not accepted by Rome (it was approved by Clement VII and Paul III, and permitted as a substitute for the unrevised Breviary, until Pius V in 1568 excluded it as too short and too modern, and issued a reformed edition ("Breviarium Pianum", Pian Breviary) of the old Breviary), formed the model for the still more thorough reform made in 1549 by the Church of England, whose daily morning and evening services are but a condensation and simplification of the Breviary offices. Some parts of the prefaces at the beginning of the English Prayer-Book are free translations of those of Quignonez. The Pian Breviary was again altered by Sixtus V in 1588, who introduced the revised Vulgate, in 1602 by Clement VIII (through Baronius and Bellarmine), especially as concerns the rubrics; and by Urban VIII (1623–1644), a purist who altered the text of certain hymns.
In the 17th and 18th centuries a movement of revision took place in France, and succeeded in modifying about half the Breviaries of that country. Historically, this proceeded from the labours of Jean de Launoy (1603–1678), "le dénicheur des saints", and Louis Sébastien le Nain de Tillemont, who had shown the falsity of numerous lives of the saints; while theologically it was produced by the Port Royal school, which led men to dwell more on communion with God as contrasted with the invocation of the saints. This was mainly carried out by the adoption of a rule that all antiphons and responses should be in the exact words of Scripture, which, of course, cut out the whole class of appeals to created beings. The services were at the same time simplified and shortened, and the use of the whole Psalter every week (which had become a mere theory in the Roman Breviary, owing to its frequent supersession by saints' day services) was made a reality. These reformed French Breviaries—e.g. the Paris Breviary of 1680 by Archbishop François de Harlay (1625–1695) and that of 1736 by Archbishop Charles-Gaspard-Guillaume de Vintimille du Luc (1655–1746)—show a deep knowledge of Holy Scripture, and much careful adaptation of different texts.
Later modern reforms.
During the pontificate of Pius IX a strong Ultramontane movement arose against the French Breviaries of 1680 and 1736. This was inaugurated by Montalembert, but its literary advocates were chiefly Dom Gueranger, a learned Benedictine monk, abbot of Solesmes, and Louis Veuillot (1813–1883) of the Univers; and it succeeded in suppressing them everywhere, the last diocese to surrender being Orleans in 1875. The Jansenist and Gallican influence was also strongly felt in Italy and in Germany, where Breviaries based on the French models were published at Cologne, Münster, Mainz and other towns. Meanwhile, under the direction of Benedict XIV (pope 1740–1758), a special congregation collected much material for an official revision, but nothing was published. In 1902, under Leo XIII, a commission under the presidency of Monsignor Louis Duchesne was appointed to consider the Breviary, the Missal, the Pontifical and the Ritual.
Significant changes came in 1910 with the reform of the Roman Breviary by Pope Pius X. This revision modified the traditional psalm scheme so that, while all 150 psalms were used in the course of the week, these were said without repetition. Those assigned to the Sunday office underwent the least revision, although noticeably fewer psalms are recited at Matins, and both Lauds and Compline are slightly shorter due to psalms (or in the case of Compline the first few verses of a psalm) being removed. Pius X was probably influenced by earlier attempts to eliminate repetition in the psalter, most notably the liturgy of the Benedictine congregation of St. Maur. However, since Cardinal Quignonez's attempt to reform the Breviary employed this principle—albeit with no regard to the traditional scheme—such notions had floated around in the western Church, and can particularly be seen in the Paris Breviary.
Pope Pius XII introduced optional use of a new translation of the Psalms from the Hebrew to a more classical Latin. Most breviaries published in the late 1950s and early 1960s used this "Pian Psalter".
Pope John XXIII also revised the Breviary in 1960, introducing changes drawn up by his predecessor Pope Pius XII. The most notable alteration is the shortening of most feasts from nine to three lessons at Matins, keeping only the Scripture readings (the former lesson i, then lessons ii and iii together), followed by either the first part of the patristic reading (lesson vii) or, for most feasts, a condensed version of the former second Nocturn, which was formerly used when a feast was reduced in rank and commemorated.
Manuscripts and printed editions.
Before the rise of the mendicant orders (wandering friars) in the thirteenth century, the daily services were usually contained in a number of large volumes. The first occurrence of a single manuscript of the daily office was written by the Benedictine order at Monte Cassino in Italy in 1099. By a strange twist, the Benedictines were not a mendicant order, but a stable, monastery-based order, and single-volume breviaries are rare from this early period.
The arrangement of the Psalms in the Rule of St. Benedict had a profound impact upon the breviaries used by secular and monastic clergy alike, up until 1911 when Pope St. Pius X introduced his reform of the Roman Breviary. In many places, every diocese, order or ecclesiastical province maintained its own edition of the breviary. 
However, mendicant friars travelled around a lot and needed a shortened, or abbreviated, daily office contained in one portable book, and single-volume breviaries flourished from the thirteenth century onwards.
These abbreviated volumes soon became very popular and eventually supplanted the Roman Catholic Church's Curia office, previously said by non-monastic clergy. 
Before the advent of printing, breviaries were written by hand and were often richly decorated with initials and miniature illustrations telling stories in the lives of Christ or the saints, or stories from the Bible.
Later printed breviaries usually have woodcut illustrations, interesting in their own right but the poor relation of the beautifully illuminated breviaries.
The beauty and value of many of the Latin Breviaries were brought to the notice of English churchmen by one of the numbers of the Oxford "Tracts for the Times", since which time they have been much more studied, both for their own sake and for the light they throw upon the English Prayer-Book.
From a bibliographical point of view some of the early printed Breviaries are among the rarest of literary curiosities, being merely local. The copies were not spread far, and were soon worn out by the daily use made of them. Doubtless many editions have perished without leaving a trace of their existence, while others are known by unique copies. In Scotland the only one which has survived the convulsions of the 16th century is "Aberdeen Breviary", a Scottish form of the Sarum Office (the Sarum Rite was much favoured in Scotland as a kind of protest against the jurisdiction claimed by the diocese of York), revised by William Elphinstone (bishop 1483–1514), and printed at Edinburgh by Walter Chapman and Andrew Myllar in 1509–1510. Four copies have been preserved of it, of which only one is complete; but it was reprinted in facsimile in 1854 for the Bannatyne Club by the munificence of the Duke of Buccleuch. It is particularly valuable for the trustworthy notices of the early history of Scotland which are embedded in the lives of the national saints. Though enjoined by royal mandate in 1501 for general use within the realm of Scotland, it was probably never widely adopted. The new Scottish "Proprium" sanctioned for the Roman Catholic province of St Andrews in 1903 contains many of the old Aberdeen collects and antiphons.
The Sarum or Salisbury Breviary itself was very widely used. The first edition was printed at Venice in 1483 by Raynald de Novimagio in folio; the latest at Paris, 1556, 1557. While modern Breviaries are nearly always printed in four volumes, one for each season of the year, the editions of the Sarum never exceeded two parts.
Contents of the Roman Breviary.
At the beginning stands the usual introductory matter, such as the tables for determining the date of Easter, the calendar, and the general rubrics. The Breviary itself is divided into four seasonal parts—winter, spring, summer, autumn—and comprises under each part:
These parts are often published separately.
The Psalter.
This psalm book is the very backbone of the Breviary, the groundwork of the Catholic prayer-book; out of it have grown the antiphons, responsories and versicles. Until the 1911 reform, the psalms were arranged according to a disposition dating from the 8th century, as follows: Psalms i.–cviii., with some omissions, were recited at Matins, twelve each day from Monday to Saturday, and eighteen on Sunday. The omissions were said at Lauds, Prime and Compline. Psalms cix–cxlvii (except cxvii, cxviii and cxlii) were said at Vespers, five each day. Psalms cxlviii–cl were always used at Lauds, and give that hour its name. The text of this Psalter is that commonly known as the Gallican. The name is misleading, for it is simply the second revision (A.D. 392) made by Jerome of the old "Itala" version originally used in Rome. Jerome's first revision of the "Itala" (A.D. 383), known as the Roman, is still used at St Peter's in Rome, but the "Gallican", thanks especially to St Gregory of Tours, who introduced it into Gaul in the 6th century, has ousted it everywhere else. The Antiphonary of Bangor proves that Ireland accepted the Gallican version in the 7th century, and the English Church did so in the 10th.
Following the 1911 reform, Matins was reduced to nine Psalms every day, with the other psalms redistributed throughout Prime, Terce, Sext, and Compline. For Sundays and special feasts Lauds and Vespers largely remained the same, Psalm 118 remained distributed at the Little Hours and Psalms 4, 90, and 130 were kept at Compline.
The "Proprium de Tempore".
This contains the office of the seasons of the Christian year (Advent to Trinity), a conception that only gradually grew up. There is here given the whole service for every Sunday and week-day, the proper antiphons, responsories, hymns, and especially the course of daily Scripture-reading, averaging about twenty verses a day, and (roughly) arranged thus: for Advent, Isaiah; Epiphany to Septuagesima, Pauline Epistles; Lent, patristic homilies (Genesis on Sundays); Passion-tide, Jeremiah; Easter to Whitsun, Acts, Catholic epistles and Apocalypse; Whitsun to August, Samuel and Kings; August to Advent, Wisdom books, Maccabees, Prophets.
The "Proprium Sanctorum".
This contains the lessons, psalms and liturgical formularies for saints' festivals, and depends on the days of the secular month. The readings of the second Nocturn are mainly hagiological biography, with homilies or papal documents for certain major feasts, particularly those of Jesus and Mary. Some of this material has been revised by Leo XIII, in view of archaeological and other discoveries. The third Nocturn consists of a homily on the Gospel which is read at that day's Mass. Covering a great stretch of time and space, they do for the worshipper in the field of church history what the Scripture readings do in that of biblical history.
The "Commune Sanctorum".
This comprises psalms, antiphons, lessons, &c., for feasts of various groups or classes (twelve in all); e.g. apostles, martyrs, confessors, virgins, and the Blessed Virgin Mary. These offices are of very ancient date, and many of them were probably in origin proper to individual saints. They contain passages of great literary beauty. The lessons read at the third nocturn are patristic homilies on the Gospels, and together form a rough summary of theological instruction.
Extra services.
Here are found the Little Office of the Blessed Virgin Mary, the Office of the Dead (obligatory on All Souls' Day), and offices peculiar to each diocese.
Elements of the Hours.
It has already been indicated, by reference to Matins, Lauds, &c., that not only each day, but each part of the day, has its own office, the day being divided into liturgical "hours." A detailed account of these will be found in the article Canonical Hours. Each of the hours of the office is composed of the same elements, and something must be said now of the nature of these constituent parts, of which mention has here and there been already made. They are: psalms (including canticles), antiphons, responsories, hymns, lessons, little chapters, versicles and collects.
Psalms.
Before the 1911 reform, the multiplication of saints' festivals, with practically the same festal psalms, tended to repeat the about one-third of the Psalter, with a correspondingly rare recital of the remaining two-thirds. Following this reform, the entire Psalter is again generally recited each week, with the festal psalms restricted to only the highest-ranking feasts. As in the Greek usage and in the Benedictine, certain canticles like the Song of Moses (Exodus xv.), the Song of Hannah (1 Sam. ii.), the prayer of Habakkuk (iii.), the prayer of Hezekiah (Isaiah xxxviii.) and other similar Old Testament passages, and, from the New Testament, the Magnificat, the Benedictus and the Nunc dimittis, are admitted as psalms.
Antiphons.
The antiphons are short liturgical forms, sometimes of biblical, sometimes of patristic origin, used to introduce a psalm. The term originally signified a chant by alternate choirs, but has quite lost this meaning in the Breviary.
Responsories.
The responsories are similar in form to the antiphons, but come at the end of the psalm, being originally the reply of the choir or congregation to the precentor who recited the psalm.
Hymns.
The hymns are short poems going back in part to the days of Prudentius, Synesius, Gregory of Nazianzus and Ambrose (4th and 5th centuries), but mainly the work of medieval authors.
Lessons.
The lessons, as has been seen, are drawn variously from the Bible, the Acts of the Saints and the Fathers of the Church. In the primitive church, books afterwards excluded from the canon were often read, e.g. the letters of Clement of Rome and the Shepherd of Hermas. In later days the churches of Africa, having rich memorials of martyrdom, used them to supplement the reading of Scripture. Monastic influence accounts for the practice of adding to the reading of a biblical passage some patristic commentary or exposition. Books of homilies were compiled from the writings of SS. Augustine, Hilary, Athanasius, Isidore, Gregory the Great and others, and formed part of the library of which the Breviary was the ultimate compendium. In the lessons, as in the psalms, the order for special days breaks in upon the normal order of ferial offices and dislocates the scheme for consecutive reading. The lessons are read at Matins (which is subdivided into three nocturns).
Little chapters.
The little chapters are very short lessons read at the other "hours."
Versicles.
The versicles are short responsories used after the little chapters in the minor hours. They appear after the hymns in Lauds and Vespers.
Collects.
The collects come at the close of the office and are short prayers summing up the supplications of the congregation. They arise out of a primitive practice on the part of the bishop (local president), examples of which are found in the Didachē (Teaching of the Apostles) and in the letters of Clement of Rome and Cyprian. With the crystallization of church order improvisation in prayer largely gave place to set forms, and collections of prayers were made which later developed into Sacramentaries and Orationals. The collects of the Breviary are largely drawn from the Gelasian and other Sacramentaries, and they are used to sum up the dominant idea of the festival in connection with which they happen to be used.
Celebration.
Before 1910 the difficulty of harmonizing the "Proprium de Tempore" and the "Proprium Sanctorum", to which reference has been made, was only partly met in the thirty-seven chapters of general rubrics. Additional help was given by a kind of Catholic Churchman's Almanack, called the "Ordo Recitandi Divini Officii", published in different countries and dioceses, and giving, under every day, minute directions for proper reading. In 1960 John XXIII simplified the rubrics governing the Breviary in order to make it easier to use.
Every cleric in Holy Orders and many other members of religious orders must publicly join in or privately read aloud (i.e. using the lips as well as the eyes—it takes about two hours in this way) the whole of the Breviary services allotted for each day. In large churches where they were celebrated the services were usually grouped; e.g. Matins and Lauds (about 7.30 A.M.); Prime, Terce (High Mass), Sext, and None (about 10 A.M.); Vespers and Compline (4 P.M.); and from four to eight hours (depending on the amount of music and the number of high masses) are thus spent in choir. Lay use of the Breviary has varied throughout the Church's history. In some periods laymen did not use the Breviary as a manual of devotion to any great extent. The late Medieval period saw the recitation of certain hours of the Little Office of the Blessed Virgin, which was based on the Breviary in form and content, becoming popular among those who could read, and Bishop Challoner did much to popularise the hours of Sunday Vespers and Compline (albeit in English translation) in his 'Garden of the Soul' in the eighteenth century. The Liturgical Movement in the twentieth century saw renewed interest in the Offices of the Breviary and several popular editions were produced containing the vernacular as well as the Latin.
The complete pre-Pius X Roman Breviary was translated into English (by the Marquess of Bute in 1879; new ed. with a trans, of the Martyrology, 1908), French and German. Bute's version is noteworthy for its inclusion of the skilful renderings of the ancient hymns by J.H. Newman, J.M. Neale and others. Several editions of the Pius X Breviary were produced during the twentieth century, including a notable edition prepared with the assistance of the Sisters of Stanbrook Abbey in the 1950s. Two editions in English and Latin were produced in the mid-sixties, which conformed to the rubrics of 1960, published by Liturgical Press and Benziger in America. These used the Pius XII psalter. Baronius Press's revised edition of the Liturgical Press edition uses the older Gallican psalter of St. Jerome. This edition was published and released in 2012 for pre-orders only. In 2013, the publication has resumed printing and is available on Baronius web site. 
Under Pope Benedict XVI's motu proprio Summorum Pontificum, Roman Catholic bishops, priests, and deacons are again permitted to use the 1962 edition of the Roman Breviary, promulgated by Pope John XXIII to satisfy their obligation to recite the Divine Office every day.
In 2008, an "i-breviary" was launched, which combines the ancient breviaries with the latest computer technology.

</doc>
<doc id="4866" url="http://en.wikipedia.org/wiki?curid=4866" title="Boomer">
Boomer

Boomer may refer to:

</doc>
<doc id="4868" url="http://en.wikipedia.org/wiki?curid=4868" title="B. F. Skinner">
B. F. Skinner

Burrhus Frederic (B. F.) Skinner (March 20, 1904 – August 18, 1990) was an American psychologist, behaviorist, author, inventor, and social philosopher. He was the Edgar Pierce Professor of Psychology at Harvard University from 1958 until his retirement in 1974.
Skinner invented the operant conditioning chamber, also known as the Skinner Box. He was a firm believer of the idea that human free will was actually an illusion and any human action was the result of the consequences of that same action. If the consequences were bad, there was a high chance that the action would not be repeated; however if the consequences were good, the actions that led to it would be reinforced. He called this the principle of reinforcement.
He innovated his own philosophy of science called radical behaviorism, and founded his own school of experimental research psychology—the experimental analysis of behavior, coining the term operant conditioning. His analysis of human behavior culminated in his work "Verbal Behavior", as well as his philosophical manifesto "Walden Two", both of which have recently seen enormous increase in interest experimentally and in applied settings. Contemporary academia considers Skinner a pioneer of modern behaviorism along with John B. Watson and Ivan Pavlov.
Skinner discovered and advanced the "rate of response" as a dependent variable in psychological research. He invented the "cumulative recorder" to measure rate of responding as part of his highly influential work on schedules of reinforcement. In a June 2002 survey, Skinner was listed as the most influential psychologist of the 20th century. He was a prolific author who published 21 books and 180 articles.
Biography.
Skinner was born in Susquehanna, Pennsylvania to William and Grace Skinner. His father was a lawyer. He became an atheist after a Christian teacher tried to assuage his fear of the hell that his grandmother described. His brother Edward, two and a half years younger, died at age sixteen of a cerebral hemorrhage. He attended Hamilton College in New York with the intention of becoming a writer. He found himself at a disadvantage at Hamilton College with many due to his intellectual attitude. While attending, he joined Lambda Chi Alpha Fraternity. He wrote for the school paper, but as an atheist, he was critical of the religious school he attended. He also attended Harvard University after receiving his B.A. in English literature in 1926 where he would later research, teach, and eventually become a prestigious board member. While at Harvard, he invented his prototype for the Skinner Box. Also, a fellow student Fred Keller, convinced Skinner he could make an experimental science from the study of behavior. This led Skinner to join Keller and they created different tools for small experiments. After graduation, he unsuccessfully tried to write a great novel while he lived with his parents, which he later called the Dark Years. He soon became disillusioned with his literary skills despite encouragement from widely renowned literary genius Robert Frost and concluded that he had little world experience and no strong personal perspective from which to write. His encounter with John B. Watson's "Behaviorism" led him into graduate study in psychology and to the development of his own operant behaviorism.
Skinner received a Ph.D. from Harvard in 1931, and remained there as a researcher until 1936. He then taught at the University of Minnesota at Minneapolis and later at Indiana University, where he was chair of the psychology department from 1946–1947, before returning to Harvard as a tenured professor in 1948. He remained at Harvard for the rest of his life. In 1973 Skinner was one of the signers of the Humanist Manifesto II.
In 1936, Skinner married Yvonne Blue. The couple had two daughters, Julie (m. Vargas) and Deborah (m. Buzan). He died of leukemia on August 18, 1990, and is buried in Mount Auburn Cemetery, Cambridge, Massachusetts. Skinner continued to write and work until just before his death. A few days before Skinner died, he was given a lifetime achievement award by the American Psychological Association and delivered a 15-minute address concerning his work.
A controversial figure, Skinner has been depicted in many different ways. Much of Skinner’s criticism derived from his penchant for attempting to apply science proven in laboratory environments at a universal level.
Theory.
Skinner called his particular brand of behaviorism "Radical" behaviorism. Radical behaviorism is the philosophy of the science of behavior. It seeks to understand behavior as a function of environmental histories of reinforcing consequences. Such a functional analysis makes it capable of producing technologies of behavior (see Applied behavior analysis). This applied behaviorism lies on the opposite side of the ideological spectrum as the field of cognitive science. Unlike less austere behaviorism, it does not accept private events such as thinking, perceptions, and unobservable emotions in a causal account of an organism's behavior
 The position can be stated as follows: what is felt or introspectively observed is not some nonphysical world of consciousness, mind, or mental life but the observer's own body. This does not mean, as I shall show later, that introspection is a kind of psychological research, nor does it mean (and this is the heart of the argument) that what are felt or introspectively observed are the causes of the behavior. An organism behaves as it does because of its current structure, but most of this is out of reach of introspection. At the moment we must content ourselves, as the methodological behaviorist insists, with a person's genetic and environment histories. What are introspectively observed are certain collateral products of those histories.
In this way we repair the major damage wrought by mentalism. When what a person does [is] attributed to what is going on inside him, investigation is brought to an end. Why explain the explanation? For twenty five hundred years people have been preoccupied with feelings and mental life, but only recently has any interest been shown in a more precise analysis of the role of the environment. Ignorance of that role led in the first place to mental fictions, and it has been perpetuated by the explanatory practices to which they gave rise.
Skinner stood at the opposite position from humanistic psychology for his whole career, and denied humans possessing freedom and dignity as well as evidenced in his book Beyond Freedom and Dignity. Most of his theories were supposed to be based on self-observation, which caused him to become a supporter for behaviorism. Much of this self-observed theory stemmed from Thorndike’s Puzzle Box, a direct antecedent to Skinner’s Box. The psychologist further expanded on Thorndike’s earlier work by introducing the concept of Reinforcement to Thorndike’s Law of Effect. Skinner was an advocate of behavioral engineering and he thought that people should be controlled through the systematic allocation of external rewards. Skinner believed that behavior is maintained from one condition to another through similar or same consequences across these situations. In short, behaviors are causal factors that are influenced by the consequences. His contribution to the understanding of behavior influenced many other scientists to explain social behavior and contingencies.
Reinforcement is a central concept in Behaviorism, and was seen as a central mechanism in the shaping and control of behavior. A common misconception is that negative reinforcement is synonymous with punishment. This misconception is rather pervasive, and is commonly found in even scholarly accounts of Skinner and his contributions. To be clear, while positive reinforcement is the strengthening of behavior by the application of some event (e.g., praise after some behavior is performed), negative reinforcement is the strengthening of behavior by the removal or avoidance of some aversive event (e.g., opening and raising an umbrella over your head on a rainy day is reinforced by the cessation of rain falling on you).
Both types of reinforcement strengthen behavior, or increase the probability of a behavior reoccurring; the difference is in whether the reinforcing event is something applied (positive reinforcement) or something removed or avoided (negative reinforcement). Punishment and extinction have the effect of weakening behavior, or decreasing the future probability of a behavior's occurrence, by the application of an aversive stimulus/event (positive punishment or punishment by contingent stimulation), removal of a desirable stimulus (negative punishment or punishment by contingent withdrawal), or the absence of a rewarding stimulus, which causes the behavior to stop (extinction).
Skinner also sought to understand the application of his theory in the broadest behavioral context as it applies to living organisms, namely natural selection.
Schedules of reinforcement.
Part of Skinner's analysis of behavior involved not only the power of a single instance of reinforcement, but the effects of particular schedules of reinforcement over time.
The most notable schedules of reinforcement presented by Skinner were interval (fixed or variable) and ratio (fixed or variable).
Both FI and VI tend to produce slow, methodical responses because the reinforcements follow a time scale that is independent of how many responses occur. 
VR produce slightly higher rates of responding than FR because organism doesn’t know when next reinforcement is. The higher the ratio, the higher the response rate tends to be.
Air crib.
In an effort to help his wife cope with the day-to-day tasks of child rearing due to the birth of their second child, Skinner thought he might be able to improve upon the standard crib. He invented the 'air-crib' to meet this challenge. An 'air-crib' (also known as a 'baby tender' or humorously as an 'heir conditioner') is an easily cleaned, temperature and humidity-controlled crib Skinner designed to assist in the raising of babies.
Skinner designed this initial, preliminary prototype of the Air-Crib because he thought it would help parents who were awakened by their crying babies at night due to cold temperatures, and a need for essential clothing, or sheets. Despite allegations to the contrary, Skinner’s daughter Deborah claims to never have felt abused or neglected by use of the Air-Crib. He thought doing so would alleviate “troublesome” environmental issues.
It was one of his most controversial inventions, and was popularly mischaracterized as cruel and experimental. The crib was often compared to his operant conditioning chamber, crudely known as the "Skinner Box." This association with a system of experimentation and pellet rewards quashed any success. It was designed to make early childcare simpler (by reducing laundry, diaper rash, cradle cap, etc.), while encouraging the baby to be more confident, mobile, comfortable, healthy and therefore less prone to cry. Babies sleep and will sometimes play in air cribs but apart from newborns, most of a baby's waking hours will be spent out of the crib. Reportedly it had some success in these goals. Air-cribs were later unsuccessfully commercially manufactured by several companies.
A 2004 book by Lauren Slater, entitled "" caused much controversy by mentioning the common rumors that Skinner had used his baby daughter Deborah in some of his experiments and that she had subsequently committed suicide. Although Slater's book stated that the rumors were false, Slater also allowed the reader to believe that Deborah had disappeared. A reviewer in "The Observer" in March 2004 then misquoted Slater's books as supporting the rumors. This review was read by Deborah Skinner (now Deborah Buzan, an artist and writer living in London) who wrote a vehement riposte in "The Guardian".
Operant conditioning chamber.
While a researcher at Harvard, B. F. Skinner invented the operant conditioning chamber, popularly referred to as the Skinner box, to measure responses of organisms (most often, rats and pigeons) and their orderly interactions with the environment. The box had a lever and a food tray, and a hungry rat could get food delivered to the tray pressing the lever. Skinner observed that when a rat was put in the box, it would wander around, sniffing and exploring, and would usually press the bar by accident, at which point a food pellet would drop into the tray. After that happened, the rate of bar pressing would increase dramatically and remain high until the rat was no longer hungry.
Skinner discovered that consequences for the organism played a large role in how the organism responded in certain situations. For instance, when the rat would pull the lever it would receive food. Subsequently, the rat made frequent pulls on the lever. Negative reinforcement was also exemplified by Skinner placing rats into an electrified chamber that delivered unpleasant shocks. Levers to cut the power were placed inside these boxes. By running a current through the “operant conditioning chamber,” Skinner noticed that the rats, after accidentally pressing the lever in a frantic bid to escape, quickly learned the effects of implementing the lever and consequently used this knowledge to stop the currents both during and prior to electrical shock. These two learned responses are known as Escape Learning and Avoidance Learning.
The operant chamber for pigeons involves a plastic disc in which the pigeon pecks in order to open a drawer filled with grains. The Skinner box led to the principle of reinforcement, which is the probability of something occurring based on the consequences of a behavior.
This device was an example of his lifelong ability to invent useful devices, which included whimsical devices in his childhood to the "cumulative recorder" to measure the rate of response of organisms in an operant chamber. Even in old age, Skinner invented a "Thinking Aid" to assist in writing.
Cumulative recorder.
The cumulative recorder is an instrument used to automatically record behavior graphically. Its graphing mechanism consisted of a rotating drum of paper equipped with a marking needle. The needle would start at the bottom of the page and the drum would turn the roll of paper horizontally. This cumulative recorder was used for the Skinner box to record the rat's behavior. This apparatus produced consistent and accurate records of behavior.
Teaching machine.
The teaching machine was a mechanical device whose purpose was to administer a curriculum of programmed instruction. In one incarnation, it housed a list of questions, and a mechanism through which the learner could respond to each question. Upon delivering a correct answer, the learner would be rewarded.
Skinner advocated the use of teaching machines for a broad range of students (e.g., preschool aged to adult) and instructional purposes (e.g., reading and music). Another of the multiple machines he envisioned could teach rhythm:
The teaching machine had such instructional potential because it provided immediate and regular reinforcement that maintained students’ interest, as the “material in the machine [was] always novel” (Skinner, 1961, p. 387). In this way, a student’s attention could be maintained without the use of aversive controls. The efficiency of the teaching machine resulted from its automatic provision of reinforcement, individualized pace setting, and a coherent instructional sequence for the student. It engaged students and allowed them to learn by doing.
Teaching machines, though perhaps rudimentary, were not rigid instruments of instruction. They could be adjusted and improved based upon reports of students’ performance. For example, if a student’s report showed numerous incorrect responses, then the machine could be reprogrammed to provide less advanced prompts or questions- the idea being that students acquire behaviors most efficiently when their error rate is minimized. Along these lines, multiple choice formats were not best suited for teaching machines because contingencies of reinforcement would be left to chance; moreover, this format could increase student mistakes and induce erroneous behaviors.
Not only useful in teaching explicit skills, machines could also promote the development of a repertoire of behaviors Skinner called self-management. Self-management refers to how students think- how they attend to the environment with the view of responding appropriately to stimuli. Machines give students the opportunity to first pay attention before receiving a reward as reinforcement. This is in stark contrast with what Skinner noticed as the classroom practice of initially capturing students’ attention (e.g., with a lively video) and delivering a reward (e.g., entertainment) before they have actually done attended- a practice which actually counters the development of self-management and fails to correctly apply reinforcements for correct behavior.
What Skinner referred to as a teaching machine would probably be akin to a computer software program today that provided highly structured and incremental instruction. Skinner’s influence on such machines is undeniable. He was the first to pioneer the use of machines in the classroom, especially at the primary level. Today teaching machines such as Language Labs have been incorporated into modern education. Though it was just one of a number of inventions, it embodies much of Skinner’s theory of learning and has wide-reaching implications for education in general and classroom instruction in particular.
There has been a resurgence of interest in the notion of the teaching machine and its relationship to adaptive learning systems of the early 21st Century
Pigeon-guided missile.
The US Navy required a weapon effective against the German "Bismarck" class battleships. Although missile and TV technology existed, the size of the primitive guidance systems available rendered any weapon ineffective. Project Pigeon was potentially an extremely simple and effective solution, but despite an effective demonstration it was abandoned when more conventional solutions became available and in particular the radar system. The project centered on dividing the nose cone of a missile into three compartments, and encasing a pigeon in each. Each compartment used a lens to project an image of what was in front of the missile onto a screen. The pigeons would peck toward the object, thereby directing the missile.
Skinner complained "our problem was no one would take us seriously."
The point is perhaps best explained in terms of human psychology (i.e., few people would trust a pigeon to guide a missile no matter how reliable it proved).
Verbal summator.
A device for discovering "latent speech", Skinner experimented with a device he called the "verbal summator." Although Skinner was not a supporter of the concepts of personal assessment or projective testing, he eventually created a tool that was similar to an auditory version of the Rorschach inkblots. This was later known as the verbal summator. This invention was created in order to project subconscious thoughts, much like the inkblots have done. He used this device to create data for his verbal behavior theory. Other researchers later saw this device as a chance to conduct research and for applied purposes for research and applied purposes. The concept of the verbal summator sparked interest within the scientific community and eventually led to other new tests: the “tautophone test, the auditory apperception test, and the Azzageddi test” and has inspired many others.
"Verbal Behavior".
Challenged by Alfred North Whitehead during a casual discussion while at Harvard to provide an account of a randomly provided piece of verbal behavior, Skinner set about attempting to extend his then-new functional, inductive, approach to the complexity of human verbal behavior. Developed over two decades, his work appeared as the culmination of the William James lectures in the book "Verbal Behavior". Although Noam Chomsky was highly critical of "Verbal Behavior", he conceded that "S-R psychology" (which Skinner's system was most certainly not: operant conditioning consists of a stimulus (or antecedent) (S) emitting a response (R) which then becomes more or less likely in the future dependent upon its consequence (C)) was a reason for giving it "a review." "Verbal Behavior" had an uncharacteristically slow reception, partly as a result of Chomsky's review, paired with Skinner's neglect to address or rebut any of Chomsky's condemnations. Skinner's peers may have been slow to adopt and consider the conventions within "Verbal Behavior" due to its lack of experimental evidence—unlike the empirical density that marked Skinner's previous work. However, Skinner's functional analysis of verbal behavior has seen a resurgence of interest in applied settings.
Influence on education.
Skinner influenced education as well as psychology in both his ideology and literature. In Skinner’s view, education has two major purposes: (1) to teach repertoires of both verbal and nonverbal behavior; and (2) to encourage students to display an interest in instruction. He endeavored to bring students’ behavior under the control of the environment by reinforcing it only when particular stimuli were present. Because he believed that human behavior could be affected by small consequences, something as simple as “the opportunity to move forward after completing one stage of an activity” could prove reinforcing (Skinner, 1961, p. 380). Skinner favored active learning in the sense that students were not merely passive recipients of information doled out by teachers. He was convinced that a student had to take action; “to acquire behavior, the student must engage in behavior” (Skinner, 1961, p. 389).
Moreover, Skinner was quoted as saying "Teachers must learn how to teach ... they need only to be taught more effective ways of teaching." Skinner asserted that positive reinforcement is more effective at changing and establishing behavior than punishment, with obvious implications for the then widespread practice of rote learning and punitive discipline in education. Skinner also suggests that the main thing people learn from being punished is how to avoid punishment.
In "The Technology of Teaching", Skinner has a chapter on why teachers fail (pages 93–113): Essentially he says that teachers have not been given an in-depth understanding of teaching and learning. Without knowing the science underpinning teaching, teachers fall back on procedures that work poorly or not at all, such as:
Skinner suggests that any age-appropriate skill can be taught. The steps are
Skinner's views on education are extensively presented in his book "The Technology of Teaching". It is also reflected in Fred S. Keller's "Personalized System of Instruction" and Ogden R. Lindsley's "Precision Teaching".
Skinner associated punishment with avoidance. For example, he thought a child may be forced to practice playing his instrument as a form of seemingly productive discipline. This child would then associate practicing with punishment and thus learn to hate and avoid practicing the instrument. Additionally, teachers who use educational activities to punish children could cause inclinations towards rebellious behavior such as vandalism and opposition to education.
"Walden Two" and "Beyond Freedom and Dignity".
Skinner is popularly known mainly for his books "Walden Two" and "Beyond Freedom and Dignity" for which he made the cover of "TIME" Magazine. The former describes a visit to a fictional "experimental community" in 1940s United States, where the productivity and happiness of the citizens is far in advance of that in the outside world because of their practice of scientific social planning and use of operant conditioning in the raising of children.
"Walden Two", like Thoreau's "Walden", champions a lifestyle that does not support war or foster competition and social strife. It encourages a lifestyle of minimal consumption, rich social relationships, personal happiness, satisfying work and leisure. In 1967, Kat Kinkade founded the intentional community Twin Oaks, using Walden Two as a blueprint. The community is still in existence today and continues to use the Planner-Manager system and other aspects of the original book in its self-governance.
In "Beyond Freedom and Dignity", Skinner suggests that a technology of behavior could help to make a better society. We would, however, have to accept that an autonomous agent is not the driving force of our actions. Skinner offers alternatives to punishment and challenges his readers to use science and modern technology to construct a better society.
Political views.
Skinner's political writings emphasized his hopes that an effective and human science of behavioral control – a technology of human behavior – could help problems unsolved by earlier approaches or aggravated by advances in technology such as the atomic bomb. One of Skinner's stated goals was to prevent humanity from destroying itself. He comprehended political control as aversive or non-aversive, with the purpose to control a population. Skinner supported the use of positive reinforcement as a means of coercion, citing Jean-Jacques Rousseau's novel "" as an example of freedom literature that "did not fear the power of positive reinforcement".
Skinner's book, "Walden Two", presents a vision of a decentralized, localized society, which applies a practical, scientific approach and futuristically advanced behavioral expertise to peacefully deal with social problems. Skinner's utopia is both a thought experiment and a rhetorical piece. In his book, Skinner answers the problem that exists in many utopian novels – "What is the Good Life?" In "Walden Two", the answer is a life of friendship, health, art, a healthy balance between work and leisure, a minimum of unpleasantness, and a feeling that one has made worthwhile contributions to a society in which resources are ensured, in part, by a lack of consumption. 
The world ethos was to be achieved through behavioral technology, which could offer alternatives to coercion, as good science applied correctly would help society, and allow all people to cooperate with each other peacefully. Skinner described his novel as "my New Atlantis", in reference to Bacon's utopia. He opposed corporal punishment in the school, and wrote a letter to the California Senate that helped lead it to a ban on spanking.
Superstitious pigeons.
One of Skinner's experiments examined the formation of superstition in one of his favorite experimental animals, the pigeon. Skinner placed a series of hungry pigeons in a cage attached to an automatic mechanism that delivered food to the pigeon "at regular intervals with no reference whatsoever to the bird's behavior." He discovered that the pigeons associated the delivery of the food with whatever chance actions they had been performing as it was delivered, and that they subsequently continued to perform these same actions.
Skinner suggested that the pigeons behaved as if they were influencing the automatic mechanism with their "rituals" and that this experiment shed light on human behavior:
Modern behavioral psychologists have disputed Skinner's "superstition" explanation for the behaviors he recorded. Subsequent research (e.g. Staddon and Simmelhag, 1971), while finding similar behavior, failed to find support for Skinner's "adventitious reinforcement" explanation for it. By looking at the timing of different behaviors within the interval, Staddon and Simmelhag were able to distinguish two classes of behavior: the "terminal response", which occurred in anticipation of food, and "interim responses", that occurred earlier in the interfood interval and were rarely contiguous with food. Terminal responses seem to reflect classical (as opposed to operant) conditioning, rather than adventitious reinforcement, guided by a process like that observed in 1968 by Brown and Jenkins in their "autoshaping" procedures. The causation of interim activities (such as the schedule-induced polydipsia seen in a similar situation with rats) also cannot be traced to adventitious reinforcement and its details are still obscure (Staddon, 1977).
This experiment was also repeated on humans, in a less controlled manner, on the popular British TV series "Trick or Treat", leading to similar conclusions to Skinner.
B.F. Skinner Quotations.
"I do not admire myself as a person. My successes do not override my shortcomings"
"Ethical control may survive in small groups, but the control of the population as a whole must be delegated to specialists—to police, priests, owners, teachers, therapists, and so on, with their specialized reinforcers and their codified contingencies"
"It is a mistake to suppose that the whole issue is how to free man. The issue is to improve the way in which he is controlled"
"Education is what survives when what has been learnt has been forgotten"
"As the senses grow dull, the stimulating environment becomes less clear. When reinforcing consequences no longer follow, we are bored, discouraged and depressed."
Criticism.
J. E. R. Staddon.
As understood by Skinner, ascribing "dignity" to individuals involves giving them credit for their actions. To say "Skinner is brilliant" means that Skinner is an originating force. If Skinner's determinist theory is right, he is merely the focus of his environment. He is not an originating force and he had no choice in saying the things he said or doing the things he did. Skinner's environment and genetics both allowed and compelled him to write his book. Similarly, the environment and genetic potentials of the advocates of freedom and dignity cause them to resist the reality that their own activities are deterministically grounded. J. E. R. Staddon ("The New Behaviorism", 2001) has argued the compatibilist position, that Skinner's determinism is not in any way contradictory to traditional notions of reward and punishment, as he believed.
Noam Chomsky.
Perhaps Skinner's best known critic, Noam Chomsky published a review of Skinner's "Verbal Behavior" two years after it was published. The 1959 review became better known than the book itself. Chomsky's review has been credited with launching the cognitive movement in psychology and other disciplines. Skinner, who rarely responded directly to critics, never formally replied to Chomsky's critique. Many years later, Kenneth MacCorquodale's reply was endorsed by Skinner.
Chomsky also reviewed Skinner's "Beyond Freedom and Dignity", using the same basic motives as his "Verbal Behavior" review. Among Chomsky's criticisms were that Skinner's laboratory work could not be extended to humans, that when it was extended to humans it represented 'scientistic' behavior attempting to emulate science but which was not scientific, that Skinner was not a scientist because he rejected the hypothetico-deductive model of theory testing, and that Skinner had no science of behavior.
Psychodynamic psychology.
Skinner has been repeatedly criticized for his supposed animosity towards Freud, psychoanalysis, and psychodynamic psychology. There is clear evidence, however, that Skinner shared several of Freud's assumptions, and that he was influenced by Freudian points of view in more than one field, among them the analysis of defense mechanisms, such as repression. To study such phenomena, Skinner even designed his own projective test.
TV Guide.
In the controversial books "Beyond Freedom and Dignity" (1971) and "Walden II" (1946/1968), Skinner laid out his vision of utopian society in which behavior was controlled by the judicious application of the principle of reinforcement. In those books, he put forth the simple but stunning claim that our subjective sense of free will is an illusion and that when we think that we are exercising free will, we are actually responding to present and past patterns of reinforcement. We do things in the present that have been rewarding in the past, and our sense of "choosing" to do them is nothing more than an illusion. Skinner argued that his insights could be used to increase human well-being and solve social problems. Not surprisingly, that claim sparked an outcry from critics who believed that Skinner was giving away one of our most cherished attributes − free will − and calling for a repressive society that manipulated people for its own ends. The criticism extended to "TV Guide", which featured an interview with Skinner and called his ideas "the taming of mankind through a system of dog obedience schools for all."
List of awards and positions.
Honorary degrees.
Skinner received honorary degrees from:
In popular culture.
Writer of "The Simpsons" Jon Vitti named Principal Skinner character after behavioral psychologist B. F. Skinner.

</doc>
<doc id="4869" url="http://en.wikipedia.org/wiki?curid=4869" title="Bill">
Bill

Bill may refer to:

</doc>
<doc id="4870" url="http://en.wikipedia.org/wiki?curid=4870" title="Bill Macy">
Bill Macy

Bill Macy (born Wolf Martin Garber; May 18, 1922) is an American television, film and stage actor, born in Revere, Massachusetts, to Mollie (née Friedopfer) and Michael Garber, a manufacturer. He was raised in Brooklyn, New York and worked as a cab driver before pursuing acting.
Macy is best known for playing Walter Findlay, the long-suffering husband of the title character on the 1970s television situation comedy "Maude". He was also an original cast member of the long-running theatrical revue "Oh! Calcutta!" He has made more than 70 appearances on film and television, including a memorable role as the co-inventor of the 'Opti-grab' in the 1979 Steve Martin comedy "The Jerk", and as the head television writer in "My Favorite Year" (1982).
He appeared in the popular television movie "Perry Mason & The Case Of The Murdered Madame" (1987) as banker Richard Wilson, who, along with three other bankers, employed the murder victim's public relation company prior to her murder. 
He appeared occasionally on "Seinfeld" as one of the residents of the Florida retirement community in which Jerry Seinfeld's parents lived. He also appeared on the short-lived sitcom "Back to You". He made a guest appearance as a patient on "Chicago Hope" and as an aging gambler on the series "Las Vegas". In 2006 he made an appearance on "My Name is Earl" in the second-season episode, "Van Hickey" as an elderly patient in a nursing home who claims he "once tongue-kissed a Jamaican woman."
Macy is only five days younger than actress Bea Arthur, his co-star in "Maude".

</doc>
<doc id="4871" url="http://en.wikipedia.org/wiki?curid=4871" title="Bob Knight">
Bob Knight

Robert Montgomery "Bob" Knight (born October 25, 1940) is a retired American basketball coach. Nicknamed "The General," Knight won 902 NCAA Division I men's college basketball games, the most all-time at the time of his retirement and currently third all-time behind his former player, Mike Krzyzewski of Duke University and Jim Boeheim of Syracuse University. Knight is best known as the head coach of the Indiana Hoosiers from 1971–2000. He also coached at Texas Tech (2001–2008) and at Army (1965–1971).
While at Indiana, Knight led his teams to three NCAA championships, one National Invitation Tournament (NIT) championship, and 11 Big Ten Conference championships. He received National Coach of the Year honors four times and Big Ten Coach of the Year honors eight times. In 1984, he coached the USA men's Olympic team to a gold medal, becoming one of only three basketball coaches to win an NCAA title, NIT title, and an Olympic gold medal.
Knight was one of college basketball's most successful and innovative coaches, having perfected and popularized the motion offense. He has also been praised for running clean programs (none of his teams were ever sanctioned by the NCAA for recruiting violations) and graduating most of his players. However, Knight has also attracted controversy; he famously threw a chair across the court during a game, was once arrested for assault, and regularly displayed a combative nature during encounters with members of the press. Knight remains "the object of near fanatical devotion" from his former players and Indiana fans.
In 2008, Knight joined ESPN as a men's college basketball studio analyst during Championship Week and for coverage of the NCAA Tournament. For the 2008–09 season, he joined ESPN as a part-time color commentator as well as continuing his studio analyst duties.
Playing career.
Knight was born in Massillon, Ohio and grew up in Orrville, Ohio. Knight began his career as a player at Orrville High School. He continued under Basketball Hall of Fame coach Fred Taylor at Ohio State in 1958. Despite being a star player in high school, he played a reserve role as a forward on the 1960 Ohio State Buckeyes team that won the NCAA Championship and featured future Hall of Fame players John Havlicek and Jerry Lucas. The Buckeyes lost to the Cincinnati Bearcats in each of the next two NCAA Championship games, of which Knight was also a part.
Due in part to the star power of those Ohio State teams, playing time was usually scarce for Knight, but that did not prevent him from making an impact. In the 1961 NCAA Championship game, Knight came off the bench with 1:41 on the clock and Cincinnati leading Ohio State, 61-59. In the words of then-Ohio State assistant coach Frank Truitt,
Knight got the ball in the left front court and faked a drive into the middle. Then [he] crossed over like he worked on it all his life and drove right in and laid it up. That tied the game for us, and Knight ran clear across the floor like a 100-yard dash sprinter and ran right at me and said, 'See there, coach, I should have been in that game a long time ago!'
To which Truitt replied, "Sit down, you hot dog. You're lucky you're even on the floor."
In addition to lettering in basketball at Ohio State, it has been claimed that Knight also lettered in football and baseball; however, the official list of Ohio State football letter earners does not include Knight. Knight graduated with a degree in history and government in 1962.
Army Black Knights.
After graduation in 1962, Knight coached junior varsity basketball at Cuyahoga Falls High School in Ohio for one year. Knight then enlisted in the United States Army and accepted an assistant coaching position with the Army Black Knights in 1963, where, two years later, he was named head coach at the relatively young age of 24. In six seasons at West Point, Knight won 102 games, with his first as a head coach coming against Worcester Polytechnic Institute. One of his players was Mike Krzyzewski, who would later serve as his assistant before becoming a Hall of Fame head coach at Duke. Mike Silliman was also another of Knight's players at Army, and Knight was quoted as saying, "Mike Silliman is the best player I have ever coached."
Indiana University Hoosiers.
In 1971, Indiana University hired Knight as head coach. Because of his time spent coaching at Army and his disciplinarian nature, Knight earned the nickname "The General." During his 29 years as head coach at Indiana, the Hoosiers won 662 games, including 22 seasons of 20 or more wins, while losing 239, a .735 winning percentage. In 24 NCAA tournament appearances at Indiana, Hoosier teams under Knight won 42 of 63 games (.667), winning titles in 1976, 1981, and 1987, while losing in the semi-finals in 1973 and 1992.
1970s.
In 1972–73, Knight's second year as coach, Indiana won the Big Ten championship and reached the Final Four, but lost to UCLA. The following season, 1973–74, Indiana once again captured a Big Ten title. In the two following seasons, 1974–75 and 1975–76, Knight's teams were undefeated in the regular season and won 37-consecutive Big Ten games on their way to their fourth conference title in a row. The 1974–75 Hoosiers swept the entire Big Ten by an average of 22.8 points per game. However, in an 83–82 win against Purdue they lost consensus All-American forward Scott May to a broken left arm. With May playing just 7 minutes, the No. 1 Hoosiers lost to Kentucky, 92–90, in the Midwest Regional. The following season, 1975–76, the Hoosiers went the entire season and 1976 NCAA tournament without a single loss, beating Michigan, 86–68, in the title game. Immediately after the game, Knight lamented that "it should have been two." The 1976 Hoosiers remains the last undefeated NCAA Division I men's basketball team. Indiana won the 1979 NIT championship.
1980s.
The 1979–80 Hoosiers won the Big Ten championship and advanced to the 1980 Sweet Sixteen. The following season, in 1980–81, star-guard Isiah Thomas and Knight's Hoosiers once again won a conference title and won the 1981 NCAA tournament, Knight's second national title. In 1982–1983, Knight's No. 1 ranked Hoosiers were favorites to win another national championship. However with an injury to All-American Ted Kitchel mid-season, the Hoosiers' prospects were grim. Knight asked for fan support to rally around the team, which ultimately won the Big Ten championship. Nevertheless in the tournament Kitchel's absence was felt and the team lost to Kentucky in the 1983 Sweet Sixteen.
Knight and his 1985–86 team were profiled in a best-selling book "A Season on the Brink". To write it, Knight granted author John Feinstein almost unprecedented access to the Indiana basketball program, as well as insights into Knight's private life. Feinstein depicted a coach who is quick with a violent temper, but also one who never cheated and strictly followed all of the NCAA's rules. The following season, in 1986–87, Knight won a share of the Big Ten title and his third national championship against Syracuse in the 1987 NCAA tournament. In the 1988–1989 season the Hoosiers again won a Big Ten championship.
1990s.
From 1990–91 through 1992–93, the Hoosiers posted 87 victories, the most by any Big Ten team in a three-year span, breaking the mark of 86 set by Knight's Indiana teams of 1974–76. Teams from these three seasons spent all but two of the 53 poll weeks in the top 10, and 38 of them in the top 5. They captured two Big Ten crowns in 1990–91 and 1992–93, and during the 1991–92 season reached the Final Four. During the 1992–93 season, the 31–4 Hoosiers finished the season at the top of the AP Poll, but were defeated by Kansas in the Elite Eight.
Throughout the mid and late 1990s Knight continued to experience success with continual NCAA tournament appearances and a minimum of 19 wins each season. However, 1993 would be Knight's last conference championship and 1994 would be his last trip to the Sweet Sixteen.
Dismissal.
On March 14, 2000, just before Indiana was to begin play in the NCAA tournament, the CNN Sports Illustrated network ran a piece on Knight in which former player Neil Reed claimed he had been choked by Knight in a 1997 practice. Knight denied the claims in the story. However, less than a month later, the network aired a tape of an Indiana practice from 1997 that appeared to show Knight placing his hand on the neck of Reed.
In response, Indiana University president Myles Brand announced that he had adopted a "zero tolerance" policy with regard to Knight's behavior. Later in the year, in September 2000, Indiana freshman Kent Harvey reportedly said, "Hey, Knight, what's up?" to Knight. According to Harvey, Knight then grabbed him by the arm and lectured him for not showing him respect, insisting that Harvey address him as either "Mr. Knight" or "Coach Knight" instead of simply "Knight." Brand stated that this incident was only one of numerous complaints that occurred after the zero-tolerance policy had been put into place. Brand asked Knight to resign on September 10, and when Knight refused, Brand relieved him of his coaching duties effective immediately. Knight's dismissal was met with outrage from students. That night, thousands of Indiana students marched from Indiana University's Assembly Hall to Brand's home, burning Brand in effigy.
Harvey was supported by some and vilified by many who claim he had intentionally set up Knight. Kent Harvey's stepfather, Mark Shaw, was a former Bloomington-area radio talk show host and Knight critic. On September 13, Knight said goodbye to a crowd of some 6,000 supporters in Dunn Meadow at Indiana University. He asked that they not hold a grudge against Harvey and that they continue to support the basketball team. Knight's firing made national headlines, including the cover of "Sports Illustrated" and around the clock coverage on ESPN.
International coaching.
In 1979 Knight guided the United States Pan American team to a gold medal in Puerto Rico. In 1984 Knight led the U.S. national team to a gold medal in the Olympic Games as coach of the 1984 basketball team (coaches do not receive medals in the Olympics). Players on the team included Michael Jordan and Knight's Indiana player and protege Steve Alford.
Texas Tech Red Raiders.
After taking a season off following his dismissal from Indiana, all the while on the lookout for vacancies, Knight accepted the head coaching job at Texas Tech, though his hiring was opposed by a group of faculty led by Walter Schaller. At the press conference introducing him, Knight quipped, "This is without question the most comfortable red sweater I've had on in six years."
Knight quickly improved the program, which had not been to an NCAA tournament since 1996. He led the team to postseason appearances in each of his first four years at the school (three NCAA Championship tournaments and one NIT). After a rough 2006 season, the team improved in 2007, finishing 21–13 and again making it to the NCAA Championship tournament, where it lost to Boston College in the first round. The best performance by the Red Raiders under Knight came in 2005 when they advanced as far as the Sweet Sixteen. In both 2006 and 2007 under Knight, Texas Tech defeated two Top 10-ranked teams in consecutive weeks. During Knight's first six years at Texas Tech, the Red Raiders won 126 games, an average of 21 wins per season.
On February 4, 2008, Knight retired as head coach of the Texas Tech Red Raiders. His son Pat Knight, the head coach designate since 2005, was immediately named as his successor. The younger Knight stated that, after many years of coaching, his father was exhausted and ready to retire. Just after achieving his 900th win, Knight handed the job over to Pat in the mid-season in part to allow him to get acquainted with coaching the team earlier, instead of having him wait until October, the start of the next season. Following retirement Knight continued living in Lubbock.
Life after coaching.
In 2008, Knight was hired as a studio analyst and occasional color commentator by ESPN. In November 2012, he called an Indiana men's basketball game for the first time, something he had previously refused to do. Current Indiana men's basketball coach Tom Crean has reached out to Knight in an attempt to get him to visit the school again. Knight has thus far rebuffed all attempts to bring him back to Bloomington.
Coaching philosophy.
Knight was an innovator of the motion offense, which he perfected and popularized. The system emphasizes post players setting screens and perimeter players passing the ball until a teammate becomes open for an uncontested jump shot or lay-up. This required players to be unselfish, disciplined, and effective in setting and using screens to get open. 
Knight's motion offense didn't take shape until his time at Indiana. Prior to that, at Army, he ran a "reverse action" that involved reversing the ball from one side of the floor to the other and screening along with it. According to Knight, it was a "West Coast offense" that Pete Newell used exclusively during his coaching career. After being exposed to the Princeton offense, Knight instilled more cutting with the offense he employed, which evolved into the motion offense that he ran for most of his career. Knight continued to develop the offense, instituting different cuts over the years and putting his players in different scenarios.
Knight was well known for the extreme preparation he put into each game and practice. He was often quoted as saying, "Most people have the will to win, few have the will to prepare to win." Often during practice, Knight would instruct his players to a certain spot on the floor and give them options of what to do based on how the defense might react. In contrast to set plays, Knight's offense was designed to react according to the defense.
The 3-point shot was adopted nationally in the NCAA in 1986, mid-way through Knight's coaching career. Although he opposed the rule change throughout his life, it did compliment his offense well by improving the spacing on the floor. Knight's offense also emphasized a two-count. Players in the post are expected to try and post in the paint for two seconds and if they don't receive the ball they go set a screen. Players with the ball are expected to hold the ball for two seconds to see where they are going to take it. Screens are supposed to be held for two seconds, as well.
On defense Knight was known for emphasizing tenacious "man-to-man" defense where defenders contest every pass and every shot, and to help teammates when needed. However, Knight has also incorporated a zone defense periodically after eschewing that defense for the first two decades of his coaching career.
Knight's coaching also included a firm emphasis on academics. All but four of his four-year players completed degrees, a ratio of nearly 98 percent. Nearly 80 percent of his players graduated compared to the national average of 42 percent for Division I schools.
Legacy.
Accomplishments.
Bob Knight's all time coaching record is 902–371. His 902 wins in NCAA Division I men's college basketball games is third all-time to Jim Boeheim, and Knight's former player [Mike Krzyzewski. Knight achieved his 880th career win on January 1, 2007 and passed retired North Carolina coach Dean Smith for most career victories, a title he held until his win total was surpassed by Krzyzewski on November 15, 2011. Knight is the youngest coach to reach 200 (age 35), 300 (age 40) and 400 (age 44) wins. He was also among the youngest to reach other milestones of 500 (age 48) and 600 (age 52) wins.
Texas Tech's participation in the 2007 NCAA Tournament gave Knight more NCAA tournament appearances than any other coach. He is the only coach to win the NCAA, the NIT, an Olympic Gold medal, and a Pan American Games Gold medal. Knight is also one of only three people, along with Dean Smith and Joe B. Hall, who had both played on and coached a winning NCAA championship basketball team.
Recognition.
Knight received a number of personal honors during and after his coaching career. He was named the National Coach of the Year four times (1975, 1976, 1987, 1989) and Big Ten Coach of the Year eight times (1973, 1975, 1976, 1980, 1981, 1989, 1992, 1993). In 1975 he was a unanimous selection as National Coach of the Year, an honor he was accorded again in 1976 by the Associated Press, United Press International, and "Basketball Weekly". In 1987 he was the first person to be honored with the Naismith Coach of the Year Award. In 1989 he garnered National Coach of the Year honors by the AP, UPI, and the United States Basketball Writers Association. Knight was inducted into the Basketball Hall of Fame in 1991.
On November 17, 2006, Knight was recognized for his impact on college basketball as a member of the founding class of the National Collegiate Basketball Hall of Fame. The following year he was the recipient of the Naismith Award for Men's Outstanding Contribution to Basketball. Knight was also inducted into the Army Sports Hall of Fame (Class of 2008) and the Indiana Hoosiers athletics Hall of Fame (Class of 2009). In August 2003, he was honored as the first inductee in The Vince Lombardi Titletown Legends.
Coaching tree.
A number of assistant coaches, players, and managers of Knight have gone on to be coaches. Among them are Hall of Fame Duke coach Mike Krzyzewski, UCLA coach Steve Alford and NBA coaches Randy Wittman, Mike Woodson, Keith Smart, Evansville Coach Marty Simmons, St. Louis Coach Jim Crews and Lawrence Frank.
In the media.
Books about Knight.
In 1986 author John Feinstein published "A Season on the Brink", which detailed the 1985–86 season of the Indiana Hoosiers. Granted almost unprecedented access to the Indiana basketball program, as well as insights into Knight's private life, the book quickly became a major best-seller and spawned a new genre, as a legion of imitators wrote works covering a single year of a sports franchise. In the book Feinstein depicts a coach who is quick with a violent temper, but also one who never cheats and strictly follows all of the NCAA's rules.
Two years later author Joan Mellen penned the book "Bob Knight: His Own Man" (ISBN 0-380-70809-4), in part to rebut Feinstein's "A Season on the Brink". Mellen deals with seemingly all the causes celebres in Knight's career and presents the view that he is more sinned against than sinning.
A number of close associates and friends of Knight have also written books about him. Former player and current UCLA head basketball coach Steve Alford wrote "Playing for Knight: My Six Seasons with Bobby Knight", published in 1990.
Knight's autobiography, written with longtime friend and sports journalist Bob Hammel, was titled "Knight: My Story" and published in 2003. Three years later Steve Delsohn and Mark Heisler wrote "Bob Knight: An Unauthorized Biography".
Film and television.
Knight has appeared or been featured in numerous films and television productions. In 1994 a feature film titled "Blue Chips" featured a character named Pete Bell, a volatile but honest college basketball coach under pressure to win who decides to blatantly violate NCAA rules to field a competitive team after a sub-par season. It starred Nick Nolte as Bell and NBA star Shaquille O'Neal as Neon Bodeaux, a once-in-a-lifetime player Bell woos to his school with gifts and other perks. The coach's temper and wardrobe were modeled after Knight's, though at no time had Knight been known to illegally recruit. Knight himself appears in the movie and coaches against Nolte in the film's climactic game.
ESPN's first feature-length film was "A Season on the Brink", a 2002 TV adaptation from John Feinstein's book. In the movie Knight is played by veteran character actor Brian Dennehy. ESPN also featured Knight in a reality show titled "Knight School", which followed a handful of Texas Tech students as they competed for the right to join the basketball team as a non-scholarship player.
Knight made a cameo appearance as himself in the 2003 film "Anger Management". In 2008, Knight appeared in a commercial as part of Volkswagen's Das Auto series where Max, a 1964 black Beetle interviews famous people. When Knight talked about Volkswagen winning the best resale value award in 2008, Max replied, "At least one of us is winning a title this year." This prompted Knight to throw his chair off the stage and walk out saying, "I may not be retired."
Knight also made an appearance in a TV commercial for with fellow coaches Mike Krzyzewski, Rick Pitino, and Roy Williams, in a parody of Tom Cruise in "Risky Business".
In 2009, Knight produced three instructional coaching DVD libraries—on motion offense, man-to-man defense, and instilling mental toughness—with Championship Productions.
Family and charity.
Knight married Nancy Lou on April 17, 1963 and the two divorced in 1985. Together they had two sons, Tim and Pat. Pat played at Indiana from 1991–95 and served as head coach at Lamar from the time of his father's retirement until his dismissal in 2014. In 1988 Knight married Karen Vieth Edgar, a former Oklahoma high school basketball coach.
Knight has a high regard for education and has made generous donations to the schools he has been a part of, particularly libraries. At Indiana University Knight endowed two chairs, one in history and one in law. He also raised nearly $5 million for the Indiana University library system by championing a library fund to support the library's activities. The fund was ultimately named in his honor.
When Knight came to Texas Tech in 2001, he gave $10,000 to the library, the first gift to the Coach Knight Library Fund which has now collected over $300,000. On November 29, 2007, the Texas Tech library honored this with "A Legacy of Giving: The Bob Knight Exhibit".
Head coaching record.
Source:
Source:

</doc>
<doc id="4874" url="http://en.wikipedia.org/wiki?curid=4874" title="Black metal">
Black metal

Black metal is an extreme subgenre of heavy metal music. Common traits include fast tempos, shrieked vocals, highly distorted guitars played with tremolo picking, blast beat drumming, raw (lo-fi) recording and unconventional song structures.
During the 1980s, several thrash metal and death metal bands formed a prototype for black metal. This so-called "first wave" included bands such as Venom, Bathory, Hellhammer and Celtic Frost. A "second wave" arose in the early 1990s, spearheaded by Norwegian bands such as Mayhem, Burzum, Darkthrone, Immortal and Emperor. The early Norwegian black metal scene developed the style of their forebears into a distinct genre. Norwegian-inspired black metal scenes emerged throughout Europe and North America, although some other scenes developed their own styles independently.
Initially a synonym for "Satanic metal", black metal is often met with hostility from mainstream culture. Many artists express extreme anti-Christian and misanthropic views, and several of the genre's "second wave" pioneers have been convicted for church burnings and murder. There is also a small neo-Nazi movement within black metal, although it has been shunned by most prominent artists.
Characteristics.
Although nowadays, 'black metal' often refers to the Norwegian style with high-pitched or raspy vocals and raw production, it has also been used for bands as different as Death SS, Mercyful Fate, Mayhem, Blasphemy, and the Greek and Finnish bands that emerged around the same time as the Norwegian scene.
Instrumentation.
Norwegian-inspired black metal guitarists usually favor high-pitched guitar tones and heavy distortion. The guitar is usually played with much use of fast (un-muted) tremolo picking. Guitarists often use dissonance—along with specific scales, intervals and chord progressions—to create a sense of dread. The tritone or flat-fifth is often used, for example. Guitar solos and low guitar tunings are rare in black metal.
The bass guitar is seldom used to play stand-alone melodies. It is not uncommon for the bass guitar to be muted against the guitar, or for it to homophonically follow the bass lines of the guitar. Typically, drumming is fast and uses double-bass, blast beats, or both.
Black metal songs often stray from conventional song structure and often lack clear verse-chorus sections. Instead, many black metal songs contain lengthy and repetitive instrumental sections.
The Greek style—established by Rotting Christ, Varathron and Necromantia—has more traditional heavy metal and death metal traits than Norwegian black metal.
Vocals and lyrics.
Traditional black metal bands usually use high-pitched and raspy vocals which include shrieking, screaming and snarling. This vocal style was influenced by Quorthon of Bathory, and is one of the traits that distinguishes the vocals of many traditional black metal artists from those of death metal, which usually uses low-pitched growls.
Black metal was originally used as a term for extreme metal bands with Satanic and anti-Christian lyrics; today, the most common lyrical theme is opposition to Christianity and other organized religions. As part of this, many artists write lyrics that could be seen to promote atheism, antitheism, paganism or Satanism. The anti-Christianity of secular or pagan artists is often linked to the Christianization of their countries. Other oft-explored themes are depression, nihilism, misanthropy, death and other dark topics. However, over time, many artists have begun to focus more on topics like winter, nature, mythology, folklore, philosophy and fantasy. (For more information about black metal lyrics, see the ideology section below.)
Production.
Low-cost production quality was typical for early black metal artists with low budgets, where recordings would often be done in their homes or basements. Even when they were able to raise their production quality, many artists chose to keep making low fidelity (lo-fi) recordings. The reason for this was to stay true to the genre's underground roots and to make the music sound more "raw" and "cold". One of the better-known examples of this is the album "Transilvanian Hunger" by Darkthrone – a band whom Johnathan Selzer of "Terrorizer" magazine says "represent the DIY aspect of black metal". Many have claimed that, originally, black metal was not meant to attract a big audience. Trelldom and God Seed vocalist Gaahl said that during its early years, "black metal was never meant to reach an audience, it was purely for our own satisfaction".
Imagery and performances.
Many bands choose not to play live. Those who do maintain that these "live performances are not for entertainment or spectacle. Sincerity, authenticity and extremity are valued above all else." Some bands consider their concerts to be rituals and often make use of stage props and theatrics. Many bands, such as Mayhem and Gorgoroth, are noted for their controversial shows, which have featured impaled animal heads, mock crucifixions, medieval weaponry and band members doused in animal blood.
Black metal artists often appear dressed in black with combat boots, bullet belts, spiked wristbands and inverted crosses/pentagrams to reinforce their anti-Christian or anti-religious stance. However, the most stand-out trait is their use of corpse paint – black and white makeup (sometimes mixed with real or fake blood), which is used to create a corpse-like appearance.
In the early 1990s, most pioneering black metal artists used simple black-and-white pictures or writing on their record covers. This could have been meant as a reaction against death metal bands, who at that time had begun to use brightly colored album artwork. Most underground black metal artists have continued this style. In the main, black metal album covers are usually atmospheric or provocative; some feature natural or fantasy landscapes (for example Burzum's "Filosofem" and Emperor's "In the Nightside Eclipse") while others are violent, perverted, sacrilegious and iconoclastic (for example Marduk's "Fuck Me Jesus" and Dimmu Borgir's "In Sorte Diaboli").
History.
The following depiction follows a classification according to which pioneers like Venom, Bathory and Hellhammer were part of a "first wave" and the "second wave" was begun by the early Norwegian scene; especially by Mayhem vocalist Dead's suicide, Mayhem's leader Euronymous, who founded the Norwegian scene after Dead's suicide, and Darkthrone's album "A Blaze in the Northern Sky". There are also other definitions according to which other albums like Sarcófago's "I.N.R.I." or Samael's "Worship Him" began the "second wave".
First wave.
The first wave of black metal refers to those bands during the 1980s who influenced the black metal sound and formed a prototype for the genre. They were often speed metal or thrash metal bands.
The term "black metal" was coined by the English band Venom with their second album "Black Metal" (1982). Although deemed thrash metal rather than black metal by today's standards, the album's lyrics and imagery focused more on anti-Christian and Satanic themes than any before it. Their music was fast, unpolished in production and with raspy or grunted vocals. Venom's members also adopted pseudonyms, a practice that would become widespread among black metal musicians.
Another major influence on black metal was the Swedish band Bathory. The band, led by Thomas Forsberg (aka 'Quorthon'), created "the blueprint for Scandinavian black metal". Not only was Bathory's music dark, fast, heavily distorted, lo-fi and with anti-Christian themes, Quorthon was also the first to use the "shrieked" vocals that came to define black metal. The band played in this style on their first four albums: "Bathory" (1984), "The Return of the Darkness and Evil" (1985), "Under the Sign of the Black Mark" (1987) and "Blood Fire Death" (1988). With "Blood Fire Death" and the two following albums, Bathory pioneered the style that would become known as Viking metal.
Hellhammer from Switzerland "made truly raw and brutal music" with Satanic lyrics, and became an important influence on later black metal; "Their simple yet effective riffs and fast guitar sound were groundbreaking, anticipating the later trademark sound of early Swedish death metal". In 1984, members of Hellhammer formed Celtic Frost, whose music "explored more orchestral and experimental territories. The lyrics also became more personal, with topics about inner feelings and majestic stories. But for a couple of years, Celtic Frost was one of the world's most extreme and original metal bands, with a huge impact on the mid-90's black metal scene". Tom G. Warrior of Hellhammer and Celtic Frost credited English hardcore punk band Discharge as "a revolution, much like Venom", saying, "When I heard the first two Discharge records, I was blown away. I was just starting to play an instrument and I had no idea you could go so far."
The Danish band Mercyful Fate influenced the Norwegian scene with their imagery and lyrics. Frontman King Diamond, who wore ghoulish black-and-white facepaint on stage, inspired what became known as "corpse paint".
Other artists usually considered part of this movement include Kreator, Sodom and Destruction (from Germany), Bulldozer and Death SS (from Italy), whose vocalist Steve Sylvester was a member of the Ordo Templi Orientis.
End of the first wave.
In 1987, in the fifth issue of his "Slayer" fanzine, Metalion wrote that "the latest fad of Black/Satanic bands seems to be over", the tradition being continued by a few bands like Incubus and Morbid Angel (from the United States), Sabbat (from Great Britain), Tormentor (from Hungary), Sarcófago (from Brazil), Grotesque and Treblinka/early Tiamat (from Sweden).
Other early black metal bands include Sabbat (formed 1983 in Japan), Parabellum (formed 1983 in Colombia), Salem (formed 1985 in Israel) and Mortuary Drape (formed 1986 in Italy). Japanese band Sigh formed in 1990 and was in regular contact with key members of the Norwegian scene. Their debut album, "Scorn Defeat", became "a cult classic in the black metal world".
In the years before the Norwegian black metal scene arose, important recordings were released by Root and Master's Hammer (from Czechoslovakia), Von (from the United States), Rotting Christ (from Greece), Samael (from Switzerland) and Blasphemy (from Canada), whose debut album "Fallen Angel of Doom" (1990) is considered one of the most influential records for the war metal style (also known as war black metal or bestial black metal). Fenriz of the Norwegian band Darkthrone called Master's Hammer's debut album "Ritual" "the first Norwegian black metal album, even though they are from Czechoslovakia".
In 1990 and 1991, Northern European metallers began to release music influenced by these bands or the older ones from the first wave. In Sweden this included Marduk, Dissection, Nifelheim and Abruptum. In Finland, there emerged a scene that mixed first wave black metal influences with elements of death metal and grindcore; this included Beherit, Archgoat and Impaled Nazarene, whose debut album "Tol Cormpt Norz Norz Norz" "Rock Hard" journalist Wolf-Rüdiger Mühlmann considers a part of war metal's roots. Bands such as Demoncy and Profanatica emerged during this time in the United States, when death metal was more popular among extreme metal fans. The Norwegian band Mayhem's concert in Leipzig with Eminenz and Manos in 1990, later released as "Live in Leipzig", was said to have had a strong influence on the East German scene and is even called the unofficial beginning of German black metal.
Second wave.
The second wave of black metal began in the early 1990s and was spearheaded by the Norwegian black metal scene. During 1990–1993 a number of Norwegian artists began performing and releasing a new kind of black metal music; this included Mayhem, Thorns, Burzum, Darkthrone, Immortal, Satyricon, Emperor, Enslaved, Carpathian Forest and Gorgoroth. They developed the style of their 1980s forebears into a distinct genre. This was partly thanks to a new kind of guitar playing developed by Snorre 'Blackthorn' Ruch of Stigma Diabolicum/Thorns and Øystein 'Euronymous' Aarseth of Mayhem. Fenriz of Darkthrone has credited them with this innovation in a number of interviews. He described it as being "derived from Bathory" and noted that "those kinds of riffs became the new order for a lot of bands in the '90s". Some members of these Norwegian bands would be responsible for a spate of crimes and controversy, including church burnings and murder (see below). The scene was bitterly opposed to Christianity and organized religion as a whole. In interviews during the early 1990s, members of the scene presented themselves as misanthropic Devil worshippers who wanted to spread hatred, sorrow and evil. When asked why such statements were made to the press, Ihsahn of Emperor said that this "was very much to create fear among people" and "to be in opposition to society". More detail about the scene's ideologies can be found in the ideology section. Visually, the dark themes of their music was complemented with corpsepaint, which became a way for many black metal artists to distinguish themselves from other metal bands of the era.
Helvete and Deathlike Silence.
During May–June 1991, Euronymous of Mayhem opened an independent record shop named "Helvete" (Norwegian for 'hell') in Oslo. It quickly became the focal point of Norway's emerging black metal scene and a meeting place for many of its musicians; especially the members of Mayhem, Burzum, Emperor and Thorns. Jon 'Metalion' Kristiansen, writer of the fanzine "Slayer", said that the opening of Helvete was "the creation of the whole Norwegian Black Metal scene". In its basement, Euronymous founded an independent record label named Deathlike Silence Productions. With the rising popularity of his band and others like it, the underground success of Euronymous's label is often credited for encouraging other record labels, who had previously shunned black metal acts, to then reconsider and release their material.
Dead's suicide.
On 8 April 1991, Mayhem vocalist Per Yngve Ohlin (who called himself 'Dead') committed suicide while alone in a house shared by the band. Fellow musicians described Dead as odd, introverted and depressed. Before going onstage he went to great lengths to make himself look like a corpse and would cut his arms while singing. Mayhem's drummer, Hellhammer, said that Dead was the first to wear the distinctive 'corpse paint' that became widespread in the scene.
He was found with slit wrists and a shotgun wound to the head. The weapon belonged to Mayhem's guitarist, Euronymous, which lead to rumours that the shotgun was left there to be used by Dead. Dead's suicide note apologized for firing the weapon indoors and ended: "Excuse all the blood". Before calling the police, Euronymous went to a nearby shop and bought a disposable camera with which he photographed the body, after re-arranging some items. One of these photographs was later used as the cover of a bootleg live album: "Dawn of the Black Hearts".
In time, rumors spread that Euronymous had made a stew with bits of Dead's brain and had made necklaces with bits of his skull. Euronymous allegedly gave some of these necklaces to musicians he deemed worthy. He used Dead's suicide to foster Mayhem's 'evil' image and claimed Dead had killed himself because extreme metal had become 'trendy' and commercialized. Mayhem bassist Jørn 'Necrobutcher' Stubberud noted that "people became more aware of the [black metal] scene after Dead had shot himself [...] I think it was Dead's suicide that really changed the scene".
Two other members of the early Norwegian scene would later commit suicide: Erik 'Grim' Brødreskift (of Immortal, Borknagar, Gorgoroth) in 1999 and Espen 'Storm' Andersen (of Strid) in 2001.
Church burnings.
Musicians and fans of the Norwegian black metal scene took part in over 50 arsons of Christian churches in Norway from June 1992 to 1996. Some of the buildings were hundreds of years old and seen as important historical landmarks. One of the first and most notable was Norway's Fantoft stave church, which police believed was burnt by Varg Vikernes of the one-man band Burzum. The cover of Burzum's EP "Aske" (Norwegian for 'ashes') is a photograph of the Fantoft stave church after its destruction. In May 1994, he was found guilty for burning down Holmenkollen Chapel, Skjold Church and Åsane Church. To coincide with the release of Mayhem's "De Mysteriis Dom Sathanas", Vikernes and Euronymous had also allegedly plotted to bomb Nidaros Cathedral, which appears on the album cover. The musicians Faust, Samoth, (both of Emperor) and Jørn Inge Tunsberg (of Hades Almighty) were also convicted for church arsons. Members of the Swedish scene started to burn churches in 1993.
Many of those convicted for the church burnings have said that their actions were a symbolic "retaliation" against Christianity in Norway. Mayhem drummer Hellhammer said he had called for attacks on mosques and Hindu temples. Today, opinions on the church burnings differ within the black metal community. Guitarist Infernus and former vocalist Gaahl of the band Gorgoroth have praised the church burnings in interviews, with the latter saying "there should have been more of them, and there will be more of them". However, Necrobutcher and Kjetil Manheim of Mayhem have condemned the church burnings, with the latter claiming "It was just people trying to gain acceptance within a strict group [the black metal scene] ... they wanted some sort of approval and status". Watain vocalist Erik Danielsson respected the attacks, but said of those responsible: "the only Christianity they defeated was the last piece of Christianity within themselves. Which is a very good beginning, of course".
Murder of Euronymous.
In early 1993, animosity arose between Euronymous and Vikernes. On the night of 10 August 1993, Varg Vikernes (of Burzum) and Snorre 'Blackthorn' Ruch (of Thorns) drove from Bergen to Euronymous's apartment in Oslo. Upon their arrival a confrontation began and Vikernes fatally stabbed Euronymous. His body was found on the stairs outside the apartment with 23 cut wounds – two to the head, five to the neck, and sixteen to the back.
It has been speculated that the murder was the result of either a power struggle, a financial dispute over Burzum records or an attempt at "outdoing" a stabbing in Lillehammer the year before by Faust. Vikernes denies all of these, claiming that he attacked Euronymous in self-defense. He says that Euronymous had plotted to stun him with an electroshock weapon, tie him up and torture him to death while videotaping the event. He said Euronymous planned to use a meeting about an unsigned contract to ambush him. On the night of the murder, Vikernes claims he intended to hand Euronymous the signed contract and "tell him to fuck off", but that Euronymous "panicked" and attacked him first. He also claims that most of the cut wounds were caused by broken glass Euronymous had fallen on during the struggle. The self-defense story is doubted by Faust and other members of the scene.
Vikernes was arrested on 19 August 1993 in Bergen. Many other members of the scene were taken in for questioning around the same time. Some of them confessed to their crimes and implicated others. In May 1994, Vikernes was sentenced to 21 years in prison (Norway's maximum penalty) for the murder of Euronymous, the arson of four churches, and for the theft and storage of 150 kg of explosives. However, he only confessed to the latter. Two churches were burnt the day he was sentenced, "presumably as a statement of symbolic support". Vikernes smiled when his verdict was read and the picture was widely reprinted in the news media. Blackthorn was sentenced to eight years in prison for being an accomplice to the murder. That month saw the release of Mayhem's album "De Mysteriis Dom Sathanas", which featured Euronymous on guitar and Vikernes on bass guitar. Before the release, Euronymous's family had asked Mayhem's drummer, Hellhammer, to remove the bass tracks recorded by Vikernes. Hellhammer said: "I thought it was appropriate that the murderer and victim were on the same record. I put word out that I was re-recording the bass parts, but I never did". In 2003, Vikernes failed to return to Tønsberg prison after being given a short leave. He was re-arrested shortly after while driving a stolen car with various weapons. Vikernes was released on parole in 2009.
The second wave outside Norway.
Black metal scenes also emerged on the European mainland during the early 1990s, inspired by the Norwegian scene or the older bands, or both. In Poland, a scene was spearheaded by Graveland and Behemoth. In France, a close-knit group of musicians known as Les Légions Noires emerged; this included artists such as Mütiilation, Vlad Tepes, Belketre and Torgeist. In Belgium, there were acts such as Ancient Rites and Enthroned. Bands such as Black Funeral, Grand Belial's Key and Judas Iscariot emerged during this time in the United States.
A notable black metal group in England at the time was Cradle of Filth, who released three demos in a black/death metal style with symphonic flourishes, followed by a studio album, which featured a then-unusual hybrid style of black and gothic metal. The band then abandoned black metal for gothic metal, becoming one of the most successful extreme metal bands to date. John Serba of AllMusic commented that their first album "made waves in the early black metal scene, putting Cradle of Filth on the tips of metalheads' tongues, whether in praise of the band's brazen attempts to break the black metal mold or in derision for its 'commercialization' of an underground phenomenon that was proud of its grimy heritage [...]". Another English band called Necropolis never released any music, but "began a desecratory assault against churches and cemeteries in their area" and "almost caused Black Metal to be banned in Britain as a result".
The controversy surrounding Absurd drew attention to the German black metal scene. In 1993, the members murdered a boy from their school, Sandro Beyer. A photo of Beyer's gravestone is on the cover of one of their demos, "Thuringian Pagan Madness", along with pro-Nazi statements. It was recorded in prison and released in Poland by Graveland drummer Capricornus. The band's early music, however, was more influenced by Oi! and Rock Against Communism (RAC) than by black metal, and described as being "more akin to '60s garage punk than some of the […] Black Metal of their contemporaries". Alexander von Meilenwald from German band Nagelfar considers Ungod's 1993 debut "Circle of the Seven Infernal Pacts", Desaster's 1994 demo "Lost in the Ages", Tha-Norr's 1995 album "Wolfenzeitalter", Lunar Aurora's 1996 debut "Weltengänger" and Katharsis's 2000 debut "666" to be the most important recordings for the German scene. He said they were "not necessarily the best German releases, but they all kicked off something".
After the second wave.
In the beginning of the second wave, the different scenes developed their own styles; as Alan 'A. A. Nemtheanga' Averill says, "you had the Greek sound and the Finnish sound, and the Norwegian sound, and there was German bands and Swiss bands and that kind of thing". By the mid-1990s, the style of the Norwegian scene was being adopted by bands worldwide, and in 1998, "Kerrang!" journalist Malcolm Dome said that "black metal as we know it in 1998 owes more to Norway and to Scandinavia than any other particular country". Newer black metal bands also began raising their production quality and introducing additional instruments such as synthesizers and even full-symphony orchestras.
By the late 1990s, the underground deemed many of the Norwegian pioneers, like Emperor, Immortal, Dimmu Borgir, Ancient, Covenant/The Kovenant, and Satyricon, to have commercialized or sold out to the mainstream media and "big bastard labels".
After Euronymous's death, "[s]ome bands went more towards the Viking metal and epic style, while some bands went deeper into the abyss". Since 1993, the Swedish scene had carried out church burnings, grave desecrations and other violent acts. In 1995, Jon Nödtveidt of Dissection joined the Misanthropic Luciferian Order (MLO). In 1997, he and another MLO member were arrested and charged with shooting dead a 37-year-old man. It was said he was killed "out of anger" because he had "harassed" the two men. Nödtveidt received a 10-year sentence. As the victim was a homosexual immigrant, Dissection was accused of being a Nazi band, but Nödtveidt denied this and dismissed racism and nationalism. The Swedish band Shining, founded in 1996, began writing music almost exclusively about depression and suicide, musically inspired by Strid and by Burzum's albums "Hvis lyset tar oss" and "Filosofem". Vocalist Niklas Kvarforth wanted to "force feed" his listeners "with self-destructive and suicidal imagery and lyrics". In the beginning he used the term "suicidal black metal" for his music. However, he stopped using the term in 2001 because it had begun to be used by a slew of other bands, whom he felt had misinterpreted his vision and were using the music as a kind of therapy rather than a weapon against the listener as Kvarforth intended. He said that he "wouldn't call Shining a black metal band" and called the "suicidal black metal" term a "foolish idea".
According to Erik Danielsson, when his band Watain formed in 1998 there were very few bands who took black metal as seriously as the early Norwegian scene had. A newer generation of Swedish Satanic bands like Watain and Ondskapt, supposedly inspired by Ofermod, the new band of Nefandus member Belfagor, put this scene "into a new light". Kvarforth said, "It seems like people actually [got] afraid again". "The current Swedish black metal scene has a particularly ambitious and articulate understanding of mysticism and its validity to black metal. Many Swedish black metal bands, most notably Watain and Dissection, are [or were] affiliated with the Temple of the Black Light, or Misanthropic Luciferian Order […], a Theistic, Gnostic, Satanic organization based in Sweden." Upon his release in 2004, Jon Nödtveidt restarted Dissection with new members whom he felt were able to "stand behind and live up to the demands of Dissection's Satanic concept". He started calling Dissection "the sonic propaganda unit of the MLO" and released a third full-length album, "Reinkaos". The lyrics contain magical formulae from the "Liber Azerate" and are based on the organization's teachings. After the album's release and a few concerts, Nödtveidt said that he had "reached the limitations of music as a tool for expressing what I want to express, for myself and the handful of others that I care about" and disbanded Dissection before committing suicide.
A part of the underground scene adopted a Jungian interpretation of the church burnings and other acts of the early scene as the re-emergence of ancient archetypes, which Kadmon of Allerseelen and the authors of "Lords of Chaos" had implied in their writings, and mixed this interpretation with Paganism and an interest in Nationalism. Varg Vikernes was seen as "an ideological messiah" by some, although Vikernes had disassociated himself from black metal and his neo-Nazism had nothing to do with that subculture. This led to the rise of National Socialist black metal (NSBM), which Hendrik Möbus of Absurd calls "the logical conclusion" of the Norwegian black metal "movement". Other parts of the scene oppose NSBM as it is "indelibly linked with Asá Trŭ and opposed to Satanism", or look upon Nazism "with vague skepticism and indifference". Members of the NSBM scene, among others, see the Norwegian bands as poseurs whose "ideology is cheap", although they still respect Vikernes and Burzum, whom Grand Belial's Key vocalist Richard Mills called "the only Norwegian band that remains unapologetic and literally convicted of his beliefs".
In France, besides Les Légions Noires (The Black Legions), a NSBM scene arose. Members of French band Funeral desecrated a grave in Toulon in June 1996, and a 19-year-old black metal fan stabbed a priest to death in Mulhouse on Christmas Eve 1996. According to MkM of Antaeus, the early French scene "was quite easy to divide: either you were NSBM and you had the support from zine and the audience, or you were part of the black legions and you had that 'cult' aura". Many French bands, like Deathspell Omega and Aosoth, have an avantgarde approach and a disharmonic sound that is representative of that scene.
In Australia, a scene led by bands like Deströyer 666, Vomitor, Hobbs' Angel of Death, Nocturnal Graves and Gospel of the Horns arose. This scene's typical style is a mixture of old school black metal and raw thrash metal influenced by old Celtic Frost, Bathory, Venom and Sodom but also with its own elements.
In Sicily, a scene was created by people like Agghiastru, Rosario Badalamenti and Nadur called Mediterranean scene and led by Inchiuvatu. They decided to use the Sicilian language for the lyrics and were influenced by Mediterranean folk music<ref name="Grind Zone n°5/2007">"Grind Zone", n°5/2007.</ref> and the Italian progressive rock of the seventies. This will to create a "Mediterranean way to black metal" in the mid nineties was also shared by bands like Moonspell and Nightfall.
The early American black metal bands remained underground. Some of them—like Grand Belial's Key and Judas Iscariot—joined an international NSBM organization called the Pagan Front, although Judas Iscariot sole member Akhenaten left the organization. Other bands like Averse Sefira never had any link with Nazism. The US bands have no common style. Many were musically inspired by Burzum but did not necessarily adopt Vikernes's ideas. Profanatica's music is close to death metal, while Demoncy were accused of ripping off Gorgoroth riffs. There also emerged bands like Xasthur and Leviathan (whose music is inspired by Burzum and whose lyrics focus on topics like suicide), Nachtmystium, Krallice, Wolves in the Throne Room (a band linked to the crust punk scene and the environmental movement), and Liturgy (whose frontman Hunter Hunt-Hendrix wants to replace traditional black metal's "death and atrophy" with "life and hypertrophy"). These bands eschew black metal's traditional lyrical content for "something more Whitman-esque" and have been rejected by some traditional black metallers for their ideologies and the post-rock and shoegazing influences some of them have adopted.
The year 1993 saw the formation of Melechesh in Jerusalem, "undoubtedly the first overtly anti-Christian band to exist in one of the holiest cities in the world". Melechesh began as a straightforward black metal act with their first foray into folk metal occurring on the title track of their 1996 EP "The Siege of Lachish". Their subsequent albums saw the group straddling the boundaries between black, death, and thrash metal. A younger band, Arallu, was formed in the late 1990s and has relationships with both Melechesh and Salem. Both Melechesh and Arallu perform a style they describe as "Mesopotamian Black Metal", a blend of black metal with Mesopotamian folk music.
Since the 2000s, a number of anti-Islamic and anti-religious black metal bands—whose members come from Muslim backgrounds—have emerged in the Middle East. Janaza, believed to be Iraq's first female black metal artist, released the demo "Burning Quran Ceremony" in 2010. Its frontwoman, Anahita, said that her parents and brother were killed by a suicide bomb during the Iraq War. Another Iraqi band, Seeds of Iblis, released their debut EP "Jihad Against Islam" in 2011 through French label Legion of Death. These bands, along with Tadnees (from Saudi Arabia), False Allah (from Bahrain) and Mosque of Satan (from Lebanon), style themselves as the "Arabic Anti-Islamic Legion". Another Lebanese band, Ayat, drew much attention with their debut album "Six Years of Dormant Hatred", released through North American label Moribund Records in 2008.
Stylistic divisions.
Regarding the sound of black metal, there are two conflicting groups within the genre: "those that stay true to the genre's roots, and those that introduce progressive elements". The former believe that the music should always be minimalist – performed only with the standard guitar-bass-drums setup and recorded in a low fidelity style. One supporter of this train of thought is Blake Judd of Nachtmystium, who has rejected labeling his band black metal for its departure from the genre's typical sound. Snorre Ruch of Thorns, on the other hand, has said that modern black metal is "too narrow" and believes that this was "not the idea at the beginning".
Since the 1990s, different styles of black metal have emerged and some have melded Norwegian-style black metal with other genres.
Ideology.
Bands that were part of the 'first wave' had 'Satanic' imagery and lyrics, although most of them were not Satanists. However, for the early 'second wave' bands in Norway, Satanism as an ideology defined black metal. Another part of the scene adopted Paganism, "often coupled with nationalism", although the early Pagan bands did not call themselves 'black metal'. Bands associating themselves with black metal are generally opposed to Christianity and the other major religions. Arguably, this is the only shared belief among those calling their music black metal. Artists who oppose Christianity tend to promote atheism, antitheism, paganism or Satanism. Many artists also write lyrics that appear to be nihilistic and misanthropic.
An article in the Chronicles of Chaos webzine noted that "An overriding feature of almost all black metal is the fascination with the past". Regarding this, Aaron Weaver from Wolves in the Throne Room said: "I think that black metal is an artistic movement that is critiquing modernity on a fundamental level, saying that the modern world view is missing something". As part of this, some black metal artists have written about or focused on the ancient pre-Christian cultures of their homelands. Sam Dunn noted of the Norwegian scene that "unlike any other heavy metal scene, the culture and the place is incorporated into the music and imagery". In a Norwegian documentary, Fenriz stated that "black metal is individualism above all", and artists tend to be supportive of individualism, although followers of Euronymous tended towards support of anti-individualism. According to Benjamin Hedge Olson's master thesis, "Black Metal is characterized by a conflict between radical individualism and group identity and by an attempt to accept both polarities simultaneously".
Olson writes that some artists hold a belief similar to transcendentalists. They try to leave or "transcend" their physical form and receive knowledge from a higher being. They are dissatisfied with a "world that they feel is devoid of spiritual and cultural significance", want to rise above it and challenge "this secularism with religious fanaticism". Olson calls the concerts by these bands "musical rituals designed to achieve both scenic solidarity and mystical transcendence" and considers the "[a]cknowledgment that the performers, through ritual performance, have transcended their mundane, physical forms and taken on a spiritual persona associated with the deity" as a typical step of black metal concerts he attended.
Some prominent musicians within the scene hold that black metal does not need to represent any particular ideology. For example, Jan Axel Blomberg (Hellhammer) said in an interview with "Metal Library" that "In my opinion, black metal today is just music". Likewise, Sigurd Wongraven has said that black metal "doesn't necessarily have to be all Satanic, as long as it's dark". Eric Horner commented: "Though many bands base it on Satanic belief, I disagree that it is the only way to be 'black metal'. Black metal to me is pure emotion and individuality with a real vibe to it". However, an article in Metalion's "Slayer" fanzine attacked musicians that "care more about their guitars than the actual essence onto which the whole concept was and is based upon", and insisted that "the music itself doesn't come as the first priority".
Satanism.
Black metal was originally a term for extreme metal bands with Satanic lyrics and imagery. However, most of the 'first wave' bands (including Venom, who coined the term 'black metal') were not Satanists and merely used it to provoke. One of the few exceptions was Mercyful Fate singer and Church of Satan member King Diamond, whom Michael Moynihan calls "one of the only performers of the '80s Satanic metal who was more than just a poseur using a devilish image for shock value".
In the early 1990s, many Norwegian black metalers presented themselves as misanthropic Devil worshippers who wanted to spread hatred, sorrow and evil. Mayhem's Euronymous was the key figure behind this ideology. They attacked the Church of Satan for its "freedom and life-loving" views. The theistic Satanism they espoused was an inversion of Christianity. Benjamin Hedge Olson wrote that they "transform[ed] Venom's quasi-Satanic stage theatrics into a form of cultural expression unique from other forms of metal or Satanism" and "abandoned the mundane identities and ambitions of other forms of metal in favor of religious and ideological fanaticism". Euronymous professed to be in favor of totalitarianism and against individualism, compassion, peace, happiness and fun. When asked why such statements were made to the press, Ihsahn of Emperor said that this "was very much to create fear among people". He added that the scene wanted "to be in opposition to society" and focused "more on just being 'evil' than having a real Satanic philosophy". According to "Lords of Chaos", many who knew Euronymous—such as Kjetil Manheim, Vikernes and Blackthorn—say that his "extreme Satanic image" was an act. Mortiis, however, said that Euronymous "was such a devil worshipper you wouldn't believe it", and Metalion (who knew Euronymous since 1985 and considered him to be his best friend) said that Euronymous "was always telling what he thought [...] worshipping death and being extreme". Tenebris (allegedly Jon Nödtveidt) from the Misanthropic Luciferian Order wrote that the Norwegian scene was mainly concerned with ideological Satanism and "vanished with his death in '93". As for the other scene members, Sanna Fridh says that there is no evidence to support their early claims of being Devil worshippers, and Leif A. Lier, who led the police investigation after Euronymous's death, said he and his men had not met one Satanist. Faust said: "For some people it [Satanism] was bloody serious, but to a lot of them it was all a big hype". At the time, bands who were not theistic Satanists were not deemed 'black metal' by Euronymous and some other scene members (like Faust) and bands with a Norwegian style, but without Satanic lyrics, tended to use other terms for their music. Today there are still prominent musicians – such as Infernus, Arioch,Nornagest and Erik Danielsson – who state that black metal bands must be theistic Satanists. Some bands like the reformed Dissection and Watain insist that all members must be of the same Satanic belief, whereas Michael W. Ford of Black Funeral and MkM of Antaeus believe black metal must be Satanic but not all band members need to be Satanists. Some bands have moved from Satanism to Paganism; as black metal traditionally is defined by Satanism, "for many 'purist' black metallers, this latter move disqualifies a band as 'black', placing it instead beneath a variety of other modifiers: pagan, Viking, troll, forest, and the like".
Others shun the belief in Satan, seeing it as "Judeo-Christian" in origin, and regard Satanists as perpetuating, and playing a part in, the "Judeo-Christian" worldview. Quorthon of Bathory said that he used 'Satan' and 'Satanism' to provoke and attack Christianity. However, with his third and fourth albums he began "attacking Christianity from a different angle", realizing that Satanism is a "Christian product" and seeing them both as "religious hocus-pocus". Nevertheless, some artists use Satan as a symbol or metaphor for their beliefs. This includes LaVeyan Satanists (who are atheist) and others. Vocalist Gaahl, who considers himself a Norse Shaman, said: "We use the word 'Satanist' because it is Christian world and we have to speak their language [...] When I use the word 'Satan', it means the natural order, the will of a man, the will to grow, the will to become the superman". Varg Vikernes called himself a Satanist in early interviews but "now downplays his former interest in Satanism", saying he was using Satan as a symbol for Odin as the 'adversary' of the Christian God. He started seeing Satanism as an introduction to Paganism.
Unblack metal.
Unblack metal, or Christian black metal, is music that sounds musically similar to black metal but with artists, lyrics, and imagery that promote Christianity. The Australian band Horde's debut album "Hellig Usvart", released through Nuclear Blast in 1994, is often credited as being the first Christian black metal album, although the sole member, known as Anonymous, has stated that "there were similar [unblack] bands prior to Horde, even in Norway", referring to such bands as Antestor, who formed in 1990, although prior to 1993 they were a death/doom band bearing a different name, Crush Evil. "Hellig Usvart" caused great controversy in the black metal scene, and death threats were sent to Nuclear Blast Records headquarters demanding them to release the members' names. The name of Anonymous was later revealed as Jayson Sherlock, a drummer for the bands Mortification and Paramaecium.
Many in the black metal scene see "Christian black metal" as an oxymoron. On the British black metal documentary "" (2007), all interviewed musicians stated, when asked about the matter, that black metal cannot be Christian. The term "Christian black metal" drew mocking replies from black metal musicians, for example Martin Walkyier of the English metal band Sabbat commented: "'Christian black metal?' What do they do? Do they build churches? Do they repair them? "(laughs)"". In fact, the early unblack metal groups Horde and Antestor refused to call their music "black metal" because they felt that the term was strongly associated with Satanism. Horde called its music "holy unblack metal," and Antestor preferred to call their music "sorrow metal" instead. Horde member Jayson Sherlock posted on Facebook that he did not understand how Christians can play black metal music. "For the life of me, I will never understand why Christians think they can play Black Metal. I really don't think they understand what true Black Metal is." However, many current unblack metal bands, such as Crimson Moonlight, feel that black metal has now changed from an ideological movement to a purely musical genre, and that is why they also call their own music black metal.
National Socialist black metal.
National Socialist black metal (NSBM) is black metal music by artists who promote National Socialist (Nazi) or similar beliefs through their lyrics and imagery. The ideology of such bands is typically a mix of paganism, white supremacy, white separatism and antisemitism. However, some bands meld these beliefs with Satanism or occultism, rather than paganism. NSBM is not seen as a distinct genre, but as a neo-völkisch movement or subculture within black metal. Varg Vikernes was the first to bring such views into the scene. Although his music has always been non-political, he began to express such views in writings and interviews after his arrest in 1993. However, he has since distanced himself from the NSBM scene and, although he still holds such beliefs, he refers to himself as an "Odalist" rather than a National Socialist or fascist. Some black metal bands have made references to Nazi Germany for shock value, causing them to be wrongly labeled as NSBM.
NSBM artists are a small minority within black metal, according to Mattias Gardell and Benjamin Hedge Olson. They have been criticized by some prominent and influential black metal musicians – including Jon Nödtveidt, Tormentor, King ov Hell, Infernus, Lord Ahriman, Emperor Magus Caligula, Richard Lederer, Michael W. Ford, and the members of Arkhon Infaustus. Some liken Nazism to Christianity in that it is authoritarian, collectivist, and a "herd mentality". Olson writes that the shunning of Nazism within the scene "has nothing to do with notions of a 'universal humanity' or a rejection of hate" but that Nazism is shunned "because its hatred is too specific and exclusive".

</doc>
<doc id="4875" url="http://en.wikipedia.org/wiki?curid=4875" title="Bin Laden (disambiguation)">
Bin Laden (disambiguation)

bin Laden () is an Arabic language surname synonymous with Osama bin Laden (1957–2011), it may also pertain to the Saudi Binladin Group, a holding company for the assets of the bin Laden family, and other notable members of Osama's family.

</doc>
<doc id="4876" url="http://en.wikipedia.org/wiki?curid=4876" title="Blizzard Entertainment">
Blizzard Entertainment

Blizzard Entertainment, Inc. is an American video game developer and publisher founded on February 8, 1991, under the name Silicon & Synapse by three graduates of UCLA, Michael Morhaime, Allen Adham and Frank Pearce, and is currently a subsidiary of American company Activision Blizzard. Based in Irvine, California, the company originally concentrated primarily on the creation of game ports for other studios before beginning development of their own software in 1993 with the development of games like "Rock n' Roll Racing" and "The Lost Vikings". In 1994 the company became Blizzard Entertainment, Inc. before being acquired by distributor Davidson & Associates and later by Vivendi. Shortly thereafter, Blizzard shipped their breakthrough hit "".
Blizzard went on to create several successful video games, including the "Warcraft" sequels, "StarCraft," and "Diablo" series, and the MMORPG "World of Warcraft". Their most recent projects include "Diablo III", World of Warcraft's fifth expansion, "Warlords of Draenor", and the first expansion of "StarCraft II", "Heart of the Swarm".
On July 9, 2008, Activision officially merged with Vivendi Games, culminating in the inclusion of the Blizzard brand name in the title of the resulting holding company. On July 25, 2013, Activision Blizzard announced the purchase of 429 million shares from owner Vivendi. As a result, Activision Blizzard became an independent company.
Blizzard Entertainment offers events to meet players and to announce games: the BlizzCon in California, United States, and the Blizzard Worldwide Invitational in other countries, such as Paris, France and Seoul, South Korea.
History.
Blizzard Entertainment was founded by Michael Morhaime, Allen Adham, and Frank Pearce as Silicon & Synapse on February 8th 1991, a year after all three had received their bachelor's degrees from UCLA. In the early days the company focused on creating game ports for other studios. Ports include titles such as "J.R.R. Tolkien's The Lord of the Rings, Vol. I" and "Battle Chess II: Chinese Chess". In 1993, the company developed games such as "Rock n' Roll Racing" and "The Lost Vikings" (published by Interplay Productions).
In early 1994 they were acquired by distributor Davidson & Associates for $6.75 million. The same year the company briefly changed its name to Chaos Studios, before finally settling on Blizzard Entertainment after it was discovered that another company with the Chaos name already existed. Shortly thereafter, Blizzard shipped their breakthrough hit "".
Blizzard has changed hands several times since then: Davidson was acquired along with Sierra On-Line by a company called CUC International in 1996; CUC then merged with a hotel, real-estate, and car-rental franchiser called HFS Corporation to form Cendant in 1997. In 1998 it became apparent that CUC had engaged in accounting fraud for years before the merger; Cendant's stock lost 80% of its value over the next six months in the ensuing widely discussed accounting scandal. The company sold its consumer software operations, Sierra On-line which included Blizzard, to French publisher Havas in 1998, the same year Havas was purchased by Vivendi. Blizzard was part of the Vivendi Games group of Vivendi. In July 2008 Vivendi Games merged with Activision, using Blizzard's name in the resulting company, Activision Blizzard.
In 1996, Blizzard acquired Condor Games, which had been working on the game "Diablo" for Blizzard at the time. Condor was renamed Blizzard North, and has since developed hit games "Diablo", "Diablo II", and its expansion pack "". Blizzard North was located in San Mateo, California; the company originated in Redwood City, California.
Blizzard launched their online gaming service "Battle.net" in January 1997 with the release of their action-RPG "Diablo". In 2002, Blizzard was able to reacquire rights for three of its earlier Silicon & Synapse titles from Interplay Entertainment and re-release them under Game Boy Advance. In 2004, Blizzard opened European offices in the Paris suburb of Vélizy, Yvelines, France, responsible for the European in-game support of "World of Warcraft". On November 23, 2004, Blizzard released World of Warcraft, its MMORPG offering. On May 16, 2005, Blizzard announced the acquisition of Swingin' Ape Studios, a video game developer which had been developing "". The company was then merged into Blizzard's other teams after "StarCraft: Ghost" was "postponed indefinitely". On August 1, 2005, Blizzard announced the consolidation of Blizzard North into the headquarters at 131 Theory in UC Irvine's University Research Park in Irvine, California. In 2007, Blizzard moved their headquarters to 16215 Alton Parkway in Irvine, California.
"World of Warcraft" was the fourth released game set in the fantasy "Warcraft" universe, which was first introduced by ' in 1994. Blizzard announced "World of Warcraft" on September 2, 2001. The game was released on November 23, 2004, on the 10th anniversary of the "Warcraft" franchise.
The first expansion set of the game, ', was released on January 16, 2007. The second expansion set, ', was released on November 13, 2008. The third expansion set, "" entered into closed beta testing in late June 2010 and was released to the public on December 7, 2010.
With more than 10 million monthly subscriptions in October 2010, "World of Warcraft" is currently the world's most-subscribed MMORPG, and holds the Guinness World Record for the most popular MMORPG by subscribers. In April 2008, "World of Warcraft" was estimated to hold 62 percent of the MMORPG subscription market. In 2008, Blizzard was honored at the 59th Annual Technology & Engineering Emmy Awards for the creation of "World of Warcraft". Mike Morhaime accepted the award.
As the website "Gamasutra" in February 2012 writes, Blizzard Entertainment will lay off around 600 employees "in order to address the changing needs of our company", as the Blizzard CEO and co-founder Mike Morhaime in a statement said. Blizzard has an office in Austin, Texas as well as in numerous countries around the globe. As of 2012 Blizzard has 4,700 employees across 11 cities, including nearly 1,700 located at their headquarters in Irvine, California.
Games.
Main franchises.
Currently, Blizzard has three main franchises in the gaming industry: "Warcraft", "Diablo", and "StarCraft". Multiple games have been released for each of these game series, and other media based around the intellectual property for each franchise. Numerous books have been written to expand the story for each series, along with media such as comics and collectible card games. Blizzard Entertainment announced in 2006 that they will be producing a "Warcraft" live-action movie. The movie is currently in development, financed and produced by Legendary Pictures, Atlas Entertainment and Film4.
Unreleased games.
Notable unreleased titles include ', which was cancelled on May 22, 1998, "Shattered Nations", and ', which was "Postponed indefinitely" on March 24, 2006 after being in development hell for much of its lifespan. After seven years of development, Blizzard revealed the cancellation of an unannounced MMO codenamed "Titan" on September 23, 2014. The company also has a history of declining to set release dates, choosing to instead take as much time as needed, generally saying a given product is "done when it's done."
"Pax Imperia II" was originally announced as a title to be published by Blizzard. Blizzard eventually dropped "Pax Imperia II", though, when it decided it might be in conflict with their other space strategy project, which became known as "StarCraft". THQ eventually contracted with Heliotrope and released the game in 1997 as "".
Technology.
Warden client.
Blizzard has made use of a special form of software known as the 'Warden Client'. The Warden client is known to be used with Blizzard's Online Games such as "Diablo" and World of "Warcraft", and the Terms of Service contain a clause consenting to the Warden software's RAM scans while a Blizzard game is running.
The Warden client scans a small portion of the code segment of running processes in order to determine whether any third-party programs are running. The goal of this is to detect and address players who may be attempting to run unsigned code or third party programs in the game. This determination of third party programs is made by hashing the scanned strings and comparing the hashed value to a list of hashes assumed to correspond to banned third party programs. The Warden's reliability in correctly discerning legitimate versus illegitimate actions was called into question when a large scale incident happened when many Linux users were banned after an update to Warden caused it to incorrectly detect Cedega as a cheat program. Blizzard issued a statement claiming they had correctly identified and restored all accounts and credited them with 20 days play.
Warden scans all processes running on a computer, not just the World of Warcraft game, and could possibly run across what would be considered private information and other personally identifiable information. It is because of these peripheral scans that Warden has been accused of being spyware and has run afoul of controversy among privacy advocates.
Battle.net 2.0.
Blizzard released its revamped Battle.net service in 2009. This service allows people who have purchased Blizzard products ("StarCraft", ', "Diablo II", and ', as well as their expansions) to download digital copies of games they have purchased, without needing any physical media.
On November 11, 2009, Blizzard required all "World of Warcraft" accounts to switch over to Battle.net Accounts. This transition now means that all current Blizzard titles can be accessed, downloaded, and played with a singular Battle.net login.
Battle.net 2.0 is the new platform for matchmaking service for Blizzard games which offers players a host of additional features. Players will now be able to track their friend's achievements, view match history, avatars, etc. Players will also be able to unlock a wide range of achievements (rewards for completing game content) for Blizzard games. This means players can enjoy new avatars, decals, badges etc, for their accounts.
The service also allows players to chat simultaneously with players from other Blizzard games. For example, players no longer need to create multiple user names or accounts for most Blizzard products. StarCraft II, Diablo III, Hearthstone, World of Warcraft and the Battle.net Launcher support the ability for users to cross communicate. This means that a player could be in a game of StarCraft II, and he/she may send or receive messages from friends playing World of Warcraft or Diablo III. To enable cross game communication players need to become either Battletag or Real ID friends.
Privacy controversy and Real ID.
On July 6, 2010, Blizzard announced that they were changing the way their forums worked to require that users identify themselves with their real name. The reaction from the community was overwhelmingly negative with multiple game magazines calling the change "foolhardy" and an "Epic Fail". It resulted in a significant user response on the Blizzard forums, including one thread on the issue reaching over 11,000 replies. This included personal details of a Blizzard employee who gave his real name "to show it wasn't a big deal". Shortly after revealing his real name, forum users posted personal information including his phone number, picture, age, and home address.
Some technology media outlets suggested that displaying real names through Real ID is a good idea and would benefit both Battle.net and the Blizzard community. But others were worried that Blizzard were opening their fans up to real-life dangers such as stalking, harassment, and employment issues, since a simple Internet search by someone's employer can reveal their online activities.
Blizzard initially responded to some of the concerns by saying that the changes would not be retroactive to previous posts, that parents could set up the system so that minors cannot post, and that posting to the forums is optional. However, due to the huge negative response, Blizzard President Michael Morhaime issued a statement rescinding the plan to use real names on Blizzard's forums for the time being.
Apart from the negative side effects of Real ID relating to privacy, the new addition boasts new features for current Blizzard titles. For instance, real names for friends, cross-realm and cross-game chat, rich presence and broadcasts are included with the Real ID system.
Real names for friends: Real ID friends will appear under their real-life names on friends lists. This means that when chatting, communicating in-game, or viewing a character's profile, the user will be able to retrieve the player's account name as opposed to the character name. Blizzard offers the rationalization that "this saves the hassle of remembering multiple character names in order to communicate" but fail to mention that this means you can always be found when playing the game, no matter what characters or even server you are currently logged in on.
Cross-realm and cross-game chat: With Real ID, friends can chat cross-realm and cross-faction in World of Warcraft. Prior to Real ID this was not possible as players needed to be on the same server and same faction in order to communicate. Furthermore, cross game chat was not available to players playing different Blizzard titles. This is no longer the case as Real ID allows players to chat across different Blizzard games like Starcraft 2 to Diablo 3 to World of Warcraft.
Rich Presence: This feature allows players to track and monitor what their friends are playing in real time. This means when they open their friends list they will be able to view the current game their friend might be playing.
Broadcasts: Allows players to broadcast a short status message for their friends to see. This means a player can make his or her status busy, available, etc. In additon, a player can send out short messages to update any change of plans which can be viewed by all friends.
Legal disputes.
"StarCraft" privacy lawsuit.
In 1998 Donald P. Driscoll, an Albany, California attorney filed a suit on behalf of Intervention, Inc., a California consumer group against Blizzard Entertainment for "unlawful business practices" for the action of collecting data from a user's computer without their permission.
"FreeCraft".
On June 20, 2003, Blizzard issued a cease and desist letter to the developers of an open source clone of the Warcraft engine called "FreeCraft", claiming trademark infringement. This hobby project had the same gameplay and characters as "Warcraft II", but came with different graphics and music.
As well as a similar name, "FreeCraft" enabled gamers to use "Warcraft II" graphics, provided they had the "Warcraft II" CD. The programmers of the clone shut down their site without challenge. Soon after that the developers regrouped to continue the work by the name of "Stratagus".
World of Warcraft Private Server Complications.
On December 5, 2008, Blizzard issued a cease and desist letter to many administrators of high population World of Warcraft private servers (essentially slightly altered hosting servers of the actual World of Warcraft game, that players do not have to pay for). Blizzard used the Digital Millennium Copyright Act to influence many private servers to fully shut down and cease to exist. In 2008 a private server by the name of ChaosCrusade was served with a DMCA notification.
Founder Electronics infringement lawsuit.
On August 14, 2007, Beijing University Founder Electronics Co., Ltd. sued Blizzard Entertainment Limited for copyright infringement claiming 100 million yuan in damages. The lawsuit alleged the Chinese edition of World of Warcraft reproduced a number of Chinese typefaces made by Founder Electronics without permission.
MDY Industries, LLC v. Blizzard Entertainment, Inc..
On July 14, 2008, the U.S. District of Arizona ruled on the case MDY Industries, LLC v. Blizzard Entertainment, Inc. The Court found that MDY was liable for copyright infringement since users of its Glider bot program were breaking the End User License Agreement and Terms of Use for "World of Warcraft". MDY Industries appealed the judgment of the district court, and a judgment was delivered by the Ninth Circuit Court of Appeals on December 14, 2010, in which the summary judgment against MDY for contributory copyright infringement was reversed. Nevertheless, they ruled that the bot violated the DMCA and the case was sent back to the district court for review in light of this decision.
Blizzard Entertainment, Inc. v. Valve Corporation.
Shortly after Valve Corporation filed its trademark for "Dota" to secure the franchising rights for "Dota 2", DotA-Allstars, LLC, run by former contributors to the games's predecessor, "Defense of the Ancients", filed an opposing trademark in August 2010. DotA All-Stars, LLC was sold to Blizzard Entertainment in 2011. After the opposition was overruled in Valve's favor, Blizzard filed an opposition against Valve in November 2011, citing their license agreement with developers, as well as their ownership of DotA-Allstars, LLC. Blizzard conceded their case in May 2012, however, giving Valve undisputed commercial rights to "Dota", while Blizzard would rename their "StarCraft II: Heart of the Swarm" mod "Blizzard All-Stars", which would become the stand-alone game, "Heroes of the Storm".
Related companies.
Over the years, some former Blizzard employees have moved on and established gaming companies of their own:

</doc>
<doc id="4878" url="http://en.wikipedia.org/wiki?curid=4878" title="Robert Bellarmine">
Robert Bellarmine

Saint Robert Bellarmine, S.J. (; 4 October 1542 – 17 September 1621) was an Italian Jesuit and a Cardinal of the Catholic Church. He was one of the most important figures in the Counter-Reformation. He was canonized in 1930 and named a Doctor of the Church. Bellarmine is also widely remembered for his role in the Galileo affair.
Early life.
Bellarmine was born at Montepulciano, the son of noble, albeit impoverished, parents, Vincenzo Bellarmino and his wife Cinzia Cervini, who was the sister of Pope Marcellus II. As a boy he knew Virgil by heart and composed a number of poems in Italian and Latin. One of his hymns, on Mary Magdalene, is included in the Breviary.
He entered the Roman novitiate in 1560, remaining in Rome three years. He then went to a Jesuit house at Mondovì, in Piedmont, where he learned Greek. While at Mondovì, he came to the attention of Francesco Adorno, the local Jesuit Provincial Superior, who sent him to the University of Padua.
Career.
Bellarmine's systematic study of theology began at Padua in 1567 and 1568, where his teachers were adherents of Thomism. In 1569 he was sent to finish it at the University of Leuven in Flanders. There he was ordained, and obtained a reputation both as a professor and a preacher. He was the first Jesuit to teach at the university, where the subject of his course was the "Summa Theologica" of Thomas Aquinas. His residence in Leuven lasted seven years. In poor health, in 1576 he made a journey to Italy. Here he remained, commissioned by Pope Gregory XIII to lecture on polemical theology in the new Roman College, now known as the Pontifical Gregorian University. 
New duties after 1589.
Until 1589, Bellarmine was occupied as professor of theology. After the murder in that year of Henry III of France, Pope Sixtus V sent Enrico Caetani as legate to Paris to negotiate with the Catholic League of France, and chose Bellarmine to accompany him as theologian. He was in the city during its siege by Henry of Navarre.
The next pope, Clement VIII, set great store by him. He was made rector of the Roman College in 1592, examiner of bishops in 1598, and cardinal in 1599. Immediately after his appointment as Cardinal, Pope Clement made him a Cardinal Inquisitor, in which capacity he served as one of the judges at the trial of Giordano Bruno, and concurred in the decision which condemned Bruno to be burned at the stake as a heretic.
In 1602 he was made archbishop of Capua. He had written against pluralism and non-residence of bishops within their dioceses. As bishop he put into effect the reforming decrees of the Council of Trent. He received some votes in the 1605 conclaves which elected Pope Leo XI, Pope Paul V, and in 1621 when Pope Gregory XV was elected, but only in the second conclave of 1605 was he "papabile". 
The Galileo case.
In 1616, on the orders of Paul V, Bellarmine summoned Galileo, notified him of a forthcoming decree of the Congregation of the Index condemning the Copernican doctrine of the mobility of the Earth and the immobility of the Sun, and ordered him to abandon it. Galileo agreed to do so.
When Galileo later complained of rumors to the effect that he had been forced to abjure and do penance, Bellarmine wrote out a certificate denying the rumors, stating that Galileo had merely been notified of the decree and informed that, as a consequence of it, the Copernican doctrine could not be "defended or held". Cardinal Bellarmine believed such a demonstration could not be found because it would contradict the unanimous consent of the Fathers' scriptural exegesis, to which the Council of Trent, in 1546, defined all Catholics must adhere.
Bellarmine wrote to heliocentrist Paolo Antonio Foscarini:the Council [of Trent] prohibits interpreting Scripture against the common consensus of the Holy Fathers; and if Your Paternity wants to read not only the Holy Fathers, but also the modern commentaries on Genesis, the Psalms, Ecclesiastes, and Joshua, you will find all agreeing in the literal interpretation that the sun is in heaven and turns around the earth with great speed, and that the earth is very far from heaven and sits motionless at the center of the world.andI say that if there were a true demonstration that the sun is at the center of the world and the earth in the third heaven, and that the sun does not circle the earth but the earth circles the sun, then one would have to proceed with great care in explaining the Scriptures that appear contrary, and say rather that we do not understand them than that what is demonstrated is false. But I will not believe that there is such a demonstration, until it is shown me. Nor is it the same to demonstrate that by assuming the sun to be at the center and the earth in heaven one can save the appearances, and to demonstrate that in truth the sun is at the center and the earth in heaven; for I believe the first demonstration may be available, but I have very great doubts about the second, and in case of doubt one must not abandon the Holy Scripture as interpreted by the Holy Fathers.
In 1633, nearly twelve years after Bellarmine's death, Galileo was again called before the Inquisition in this matter.
Modern physicist Pierre Duhem "suggests that in one respect, at least, Bellarmine had shown himself a better scientist than Galileo by disallowing the possibility of a 'strict proof of the earth's motion,' on the grounds that an astronomical theory merely 'saves the appearances' without necessarily revealing what 'really happens.'"
Death.
In his old age he was bishop of Montepulciano for four years, after which he retired to the Jesuit college of St. Andrew in Rome, where he died on 17 September 1621, aged 78.
Works.
Bellarmine's books bear the stamp of their period; the effort for literary elegance (so-called "maraviglia") had given place to a desire to pile up as much material as possible, to embrace the whole field of human knowledge, and incorporate it into theology. His controversial works provoked many replies, and were studied for some decades after his death. At Leuven he made extensive studies in the Church Fathers and scholastic theologians, which gave him the material for his book "De scriptoribus ecclesiasticis" (Rome, 1613). It was later revised and enlarged by Sirmond, Labbeus, and Casimir Oudin. Bellarmine wrote the preface to the new Sixto-Clementine Vulgate. 
Dogmatics.
From his research grew his "Disputationes de controversiis christianae fidei" (also called "Disputationes"), first published at Ingolstadt in 1581–1593. This major work was the earliest attempt to systematize the various religious controversies of the time. Bellarmine devoted eleven years to it while at the Roman College. The first volume of the "Disputationes" treats of the Word of God, of Christ, and of the Pope; the second of the authority of ecumenical councils, and of the Church, whether militant, expectant, or triumphant; the third of the sacraments; and the fourth of Divine grace, free will, justification, and good works. 
Venetian Interdict.
Under Pope Paul V (reigned 1605–1621), a major conflict arose between Venice and the Papacy. Paolo Sarpi, as spokesman for the Republic of Venice, protested against the papal interdict, and reasserted the principles of the Council of Constance and of the Council of Basel, denying the pope's authority in secular matters. Bellarmine wrote three rejoinders to the Venetian theologians, and may have warned Sarpi of an impending murderous attack. 
Allegiance oath controversy and papal authority.
Bellarmine also became involved in controversy with King James I of England. From a point of principle for English Catholics, this debate drew in figures from much of Western Europe. It raised the profile of both protagonists, King James as a champion of his own restricted Calvinist Protestantism, and Bellarmine for Tridentine Catholicism.
Devotional works.
During his retirement, he wrote several short books intended to help ordinary people in their spiritual life: "De ascensione mentis in Deum per scalas rerum creatorum opusculum" ("The Mind's Ascent to God") (1614) which was translated into English as "Jacob's Ladder" (1638) without acknowledgement by Henry Isaacson, "The Art of Dying Well" (1619) (in Latin, English translation under this title by Edward Coffin), and "The Seven Words on the Cross".
Canonization and final resting place.
Bellarmine was canonized by Pope Pius XI in 1930; the following year he was declared a Doctor of the Church. His remains, in a cardinal's red robes, are displayed behind glass under a side altar in the Church of Saint Ignatius, the chapel of the Roman College, next to the body of his student, St. Aloysius Gonzaga, as he himself had wished. In the General Roman Calendar Saint Robert Bellarmine's feast day is on 17 September, the day of his death; but some continue to use pre-1969 calendars, in which for 37 years his feast day was on 13 May. The rank assigned to his feast has been "double" (1932–1959), "third-class feast" (1960–1968), and since the 1969 revision "memorial".
Legacy.
Bellarmine University in Louisville, Kentucky is named after him, as are Bellarmine College Preparatory in San Jose, California and Bellarmine Preparatory School in Tacoma, Washington. Fairfield University has a Bellarmine Hall dedicated to the saint.
St. Robert Bellarmine, a church in the New Orleans suburb of Arabi, Louisiana, was destroyed shortly after its completion by Hurricane Betsy in 1965. The church was put completely underwater in 2005 by Hurricane Katrina and took on more water from Hurricane Rita less than a month later. The parish was permanently dissolved by the Roman Catholic Archdiocese of New Orleans immediately after Rita, and its territory was absorbed by Our Lady of Prompt Succor in neighboring Chalmette. 
The Roman Catholic "Archdiocese of Chicago" has both a local territorial parish and a parochial school named in honor of Robert Bellarmine. The parish is located on the Northwest side of Chicago in the Jefferson Park neighborhood at 4646. N. Austin Ave. Chicago, IL 60630 "St. Robert Bellarmine Parish" was founded in 1930. For more information about the parish, please visit: http://srb-chicago.org
The Roman Catholic Archdiocese of Philadelphia has a St. Robert Bellarmine Parish based in Warrington Pa.. The parishes was founded in 1968. http://saintrobertwarrington.org

</doc>
<doc id="4880" url="http://en.wikipedia.org/wiki?curid=4880" title="Bildungsroman">
Bildungsroman

In literary criticism, a Bildungsroman (; ), novel of formation, novel of education, or coming-of-age story (though it may also be known as a subset of the coming-of-age story) is a literary genre that focuses on the psychological and moral growth of the protagonist from youth to adulthood (coming of age), and in which, therefore, character change is extremely important.
History.
The term was coined in 1819 by philologist Karl Morgenstern in his university lectures, and later famously reprised by Wilhelm Dilthey, who legitimized it in 1870 and popularized it in 1905. The genre is further characterized by a number of formal, topical, and thematic features. The term coming-of-age novel is sometimes used interchangeably with "Bildungsroman", but its use is usually wider and less technical.
The birth of the "Bildungsroman" is normally dated to the publication "Wilhelm Meister's Apprenticeship" by Johann Wolfgang Goethe in 1795–96. Although the "Bildungsroman" arose in Germany, it has had extensive influence first in Europe and later throughout the world. Thomas Carlyle translated Goethe’s novel into English, and after its publication in 1824, many British authors wrote novels inspired by it. In the 20th century, it has spread to Germany, Britain, France, and several other countries around the globe.
The genre translates fairly directly into cinematic form, the coming-of-age film.
Plot outline.
A "Bildungsroman" relates the growing up or "coming of age" of a sensitive person who goes in search of answers to life's questions with the expectation that these will result from gaining experience of the world. The genre evolved from folklore tales of a dunce or youngest son going out in the world to seek his fortune. Usually in the beginning of the story there is an emotional loss which makes the protagonist leave on his journey. In a Bildungsroman, the goal is maturity, and the protagonist achieves it gradually and with difficulty. The genre often features a main conflict between the main character and society. Typically, the values of society are gradually accepted by the protagonist and he/she is ultimately accepted into society — the protagonist's mistakes and disappointments are over. In some works, the protagonist is able to reach out and help others after having achieved maturity.
There are many variations and subgenres of "Bildungsroman" that focus on the growth of an individual. An "Entwicklungsroman" ("development novel") is a story of general growth rather than self-cultivation. An "Erziehungsroman" ("education novel") focuses on training and formal schooling, while a "Künstlerroman" ("artist novel") is about the development of an artist and shows a growth of the self.

</doc>
<doc id="4881" url="http://en.wikipedia.org/wiki?curid=4881" title="Bachelor">
Bachelor

A bachelor is a man who is neither married nor cohabitating and who lives independently outside of his parents' home or other institutional setting. 
Origin and medieval usage.
The word is from Anglo-Norman "bacheler" (later suffixal change to "bachelier"; cf. "escolier" "student", from earlier "escoler"), a young squire in training. The ultimate source of the word is uncertain, it may be from Medieval Latin "baccalari(u)s" "vassal farmer" or "farm hand" (cf. Provençal "bacalar", Tuscan "bacalaro" "squire"), i.e. one who tends a "baccalaria", a term for a grazing farm (from "bacca" "cow"), or it may be from Latin "baculum" "a stick" (as the knight-in-training would practice with a wooden club before receiving his sword).
The Old French term crossed into English around 1300, referring to one belonging to the lowest stage of knighthood. Knights bachelor were either poor vassals who could not afford to take the field under their own banner, or knights too young to support the responsibility and dignity of knights banneret.
From the 14th century, the term was also used for a junior member of a guild (otherwise known as "yeomen") or university; hence, an ecclesiastic of an inferior grade, for example, a young monk or even recently appointed canon; and also an inferior grade in scholarship, i.e. one holding a "bachelor's degree" In this sense the word "baccalarius" or "baccalaureus" first appears at the University of Paris in the 13th century, in the system of degrees established under the auspices of Pope Gregory IX, as applied to scholars still in "statu pupillari". 
Thus there were two classes of baccalarii: the "baccalarii cursores", theological candidates passed for admission to the divinity course; and the "baccalarii dispositi", who, having completed this course, were entitled to proceed to the higher degrees.
Use for "unmarried man" in the 19th and 20th centuries.
In the Victorian era, the term eligible bachelor was used in the context of upper class matchmaking, denoting a young man who was not only unmarried and eligible for marriage, but also considered "eligible" in financial and societal terms for the prospective bride under discussion. Also in the Victorian era, the term "confirmed bachelor" denoted a man who was resolute to remain unmarried.
By the later 19th century, the term "bachelor" had acquired the general sense of "unmarried man". The expression bachelor party is recorded 1882. In 1895, a feminine equivalent "bachelor-girl" was coined, replaced in US English by the somewhat humorous "bachelorette" by the mid-1930s. After World War II, this terminology came to be seen as antiquated, and has been mostly replaced by the gender-neutral term "single" (first recorded 1964). In England and Wales, the term "bachelor" remained the official term used for the purpose of marriage registration until 2005, when it was abolished in favour of "single."
In certain Gulf Arab countries, "bachelor" can refer to men who are single as well as immigrant men married to a spouse residing in their country of origin (due to the high added cost of sponsoring a spouse onsite), and a colloquial term "executive bachelor" is also used in rental and sharing accommodation advertisements to indicate availability to white-collar bachelors in particular.

</doc>
<doc id="4882" url="http://en.wikipedia.org/wiki?curid=4882" title="Background radiation">
Background radiation

Background radiation is the ubiquitous ionizing radiation that people on the planet Earth are exposed to, including natural and artificial sources.
Both natural and artificial background radiation varies depending on location and altitude.
Natural background radiation.
Radioactive material is found throughout nature. Detectable amounts occur naturally in soil, rocks, water, air, and vegetation, from which it is inhaled and ingested into the body. In addition to this "internal exposure", humans also receive "external exposure" from radioactive materials that remain outside the body and from cosmic radiation from space. The worldwide average natural dose to humans is about 2.4 millisievert (mSv) per year. This is four times more than the worldwide average artificial radiation exposure, which in the year 2008 amounted to about 0.6 mSv per year. In some rich countries like the US and Japan, artificial exposure is, on average, greater than the natural exposure, due to greater access to medical imaging. In Europe, average natural background exposure by country ranges from under 2 mSv annually in the United Kingdom to more than 7 mSv annually for some groups of people in Finland.
Air.
The biggest source of natural background radiation is airborne radon, a radioactive gas that emanates from the ground. Radon and its isotopes, parent radionuclides, and decay products all contribute to an average inhaled dose of 1.26 mSv/a. Radon is unevenly distributed and varies with weather, such that much higher doses apply to many areas of the world, where it represents a significant health hazard. Concentrations over 500 times higher than the world average have been found inside buildings in Scandinavia, the United States, Iran, and the Czech Republic. Radon is a decay product of uranium, which is relatively common in the Earth's crust, but more concentrated in ore-bearing rocks scattered around the world. Radon seeps out of these ores into the atmosphere or into ground water or infiltrates into buildings. It can be inhaled into the lungs, along with its decay products, where they will reside for a period of time after exposure.
Although radon is naturally occurring, exposure can be enhanced or diminished by human activity, notably house construction. A poorly sealed basement in an otherwise well insulated house can result in the accumulation of radon within the dwelling, exposing its residents to high concentrations. The widespread construction of well insulated and sealed homes in the northern industrialized world has led to radon becoming the primary source of background radiation in some localities in northern North America and Europe. Since it is heavier than air, radon tends to collect in basements and mines. Basement sealing and suction ventilation reduce exposure. Some building materials, for example lightweight concrete with alum shale, phosphogypsum and Italian tuff, may emanate radon if they contain radium and are porous to gas.
Radiation exposure from radon is indirect. Radon has a short half-life (4 days) and decays into other solid particulate radium-series radioactive nuclides. These radioactive particles are inhaled and remain lodged in the lungs, causing continued exposure. Radon is thus the second leading cause of lung cancer after smoking, and accounts for 15,000 to 22,000 cancer deaths per year in the US alone.
About 100,000 Bq/m3 of radon was found in Stanley Watras's basement in 1984. He and his neighbours in Boyertown, Pennsylvania, United States may hold the record for the most radioactive dwellings in the world. International radiation protection organizations estimate that a committed dose may be calculated by multiplying the equilibrium equivalent concentration (EEC) of radon by a factor of 8 to 9 and the EEC of thoron by a factor of 40 .
Most of the atmospheric background is caused by radon and its decay products. The gamma spectrum shows prominent peaks at 609, 1120, and 1764 keV, belonging to bismuth-214, a radon decay product. The atmospheric background varies greatly with wind direction and meteorological conditions. Radon also can be released from the ground in bursts and then form "radon clouds" capable of traveling tens of kilometers.
Cosmic radiation.
The Earth and all living things on it are constantly bombarded by radiation from outer space. This radiation primarily consists of positively charged ions from protons to iron and larger nuclei derived sources outside our solar system. This radiation interacts with atoms in the atmosphere to create an air shower of secondary radiation, including X-rays, muons, protons, alpha particles, pions, electrons, and neutrons. The immediate dose from cosmic radiation is largely from muons, neutrons, and electrons, and this dose varies in different parts of the world based largely on the geomagnetic field and altitude. This radiation is much more intense in the upper troposphere, around 10 km altitude, and is thus of particular concern for airline crews and frequent passengers, who spend many hours per year in this environment. During their flights airline crews typically get an extra dose on the order of 2.2 mSv (220 mrem) per year.
Similarly, cosmic rays cause higher background exposure in astronauts than in humans on the surface of Earth. Astronauts in low orbits, such as in the International Space Station or the Space Shuttle, are partially shielded by the magnetic field of the Earth, but also suffer from the Van Allen radiation belt which accumulates cosmic rays and results from the Earth's magnetic field. Outside low Earth orbit, as experienced by the Apollo astronauts who traveled to the Moon, this background radiation is much more intense, and represents a considerable obstacle to potential future long term human exploration of the moon or Mars.
Cosmic rays also cause elemental transmutation in the atmosphere, in which secondary radiation generated by the cosmic rays combines with atomic nuclei in the atmosphere to generate different nuclides. Many so-called cosmogenic nuclides can be produced, but probably the most notable is carbon-14, which is produced by interactions with nitrogen atoms. These cosmogenic nuclides eventually reach the Earth's surface and can be incorporated into living organisms. The production of these nuclides varies slightly with short-term variations in solar cosmic ray flux, but is considered practically constant over long scales of thousands to millions of years. The constant production, incorporation into organisms and relatively short half-life of carbon-14 are the principles used in radiocarbon dating of ancient biological materials such as wooden artifacts or human remains.
The cosmic radiation at sea level usually manifests as 511 keV gamma rays from annihilation of positrons created by nuclear reactions of high energy particles and gamma rays. The intensity of cosmic ray background increases rapidly with altitude, and at few kilometers above sea the cosmic rays dominate the spectrum and drown the other natural sources. At higher altitudes there is also the contribution of continuous bremsstrahlung spectrum.
Terrestrial sources.
Terrestrial radiation, for the purpose of the table above, only includes sources that remain external to the body. The major radionuclides of concern are potassium, uranium and thorium and their decay products, some of which, like radium and radon are intensely radioactive but occur in low concentrations. Most of these sources have been decreasing, due to radioactive decay since the formation of the Earth, because there is no significant amount currently transported to the Earth. Thus, the present activity on earth from uranium-238 is only half as much as it originally was because of its 4.5 billion year half-life, and potassium-40 (half-life 1.25 billion years) is only at about 8% of original activity. The effects on humans of the actual diminishment (due to decay) of these isotopes is minimal however. This is because humans evolved too recently for the difference in activity over a fraction of a half-life to be significant. Put another way, human history is so short in comparison to a half-life of a billion years, that the activity of these long-lived isotopes has been effectively constant throughout our time on this planet.
In addition, many shorter half-life and thus more intensely radioactive isotopes have not decayed out of the terrestrial environment, however, because of natural on-going production of them. Examples of these are radium-226 (decay product of uranium-238) and radon-222 (a decay product of radium-226).
Thorium and uranium primarily undergo alpha and beta decay, and aren't easily detectable. However, many of their daughter products are strong gamma emitters. Thorium-232 is detectable via a 239 keV peak from lead-212, 511, 583 and 2614 keV from thallium-208, and 911 and 969 keV from actinium-228. Uranium-233 is similar but lacks the actinium-228 peak, which distinguishes it from thorium-232. Uranium-238 manifests as 609, 1120, and 1764 keV peaks of bismuth-214 ("cf." the same peak for atmospheric radon). Potassium-40 is detectable directly via its 1461 keV gamma peak.
Above sea and bodies of water the terrestrial background tends to be about 10 times lower. At coastal areas and over fresh water additional contribution is possible from dispersed sediment.
Food and water.
Some of the essential elements that make up the human body, mainly potassium and carbon, have radioactive isotopes that add significantly to our background radiation dose. An average human contains about 30 milligrams of potassium-40 (40K) and about 10 nanograms (10−8 g) of carbon-14 (14C), which has a decay half-life of 5,730 years. Excluding internal contamination by external radioactive material, the largest component of internal radiation exposure from biologically functional components of the human body is from potassium-40. The decay of about 4,000 nuclei of 40K per second makes potassium the largest source of radiation in terms of number of decaying atoms. The energy of beta particles produced by 40K is also about 10 times more powerful than the beta particles from 14C decay.
14C is present in the human body at a level of 3700 Bq with a biological half-life of 40 days. There are about 1,200 beta particles per second produced by the decay of 14C. However, a 14C atom is in the genetic information of about half the cells, while potassium is not a component of DNA. The decay of a 14C atom inside DNA in one person happens about 50 times per second, changing a carbon atom to one of nitrogen. The global average internal dose from radionuclides other than radon and its decay products is 0.29 mSv/a, of which 0.17 mSv/a comes from 40K, 0.12 mSv/a comes from the uranium and thorium series, and 12 μSv/a comes from 14C.
Areas with high NBR.
Some areas have greater dosage than the country-wide averages. In the world in general, exceptionally high natural background locales include Ramsar in Iran, Guarapari in Brazil, Karunagappalli in India, Arkaroola, South Australia and Yangjiang in China.
The highest level of purely natural radiation ever recorded on the Earth's surface was 90 µGy/h on a Brazilian black beach ("areia preta" in Portuguese) composed of monazite. This rate would convert to 0.8 Gy/a for year-round continuous exposure, but in fact the levels vary seasonally and are much lower in the nearest residences. The record measurement has not been duplicated and is omitted from UNSCEAR's latest reports. Nearby tourist beaches in Guarapari and Cumuruxatiba were later evaluated at 14 and 15 µGy/h.
The highest background radiation in an inhabited area is found in Ramsar, primarily due to the use of local naturally radioactive limestone as a building material. The 1000 most exposed residents receive an average external effective radiation dose of 6 mSv per year, (0.6 rem/yr,) six times more than the ICRP recommended limit for exposure to the public from artificial sources. They additionally receive a substantial internal dose from radon. Record radiation levels were found in a house where the effective dose due to ambient radiation fields was 131 mSv/a, (13.1 rem/yr) and the internal committed dose from radon was 72 mSv/a (7.2 rem/yr). This unique case is over 80 times higher than the world average natural human exposure to radiation.
Epidemiological studies are underway to identify health effects associated with the high radiation levels in Ramsar. It is much too early to draw statistically significant conclusions. While so far support for beneficial effects of chronic radiation (like longer lifespan) has not been observed, a protective and adaptive effect is suggested by at least one study whose authors nonetheless caution that data from Ramsar are not yet sufficiently strong to relax existing regulatory dose limits.
Photoelectric.
Background radiation doses in the immediate vicinities of particles of high atomic number materials, within the human body, have a small enhancement due to the photoelectric effect.
Neutron background.
Most of the natural neutron background is a product of cosmic rays interacting with the atmosphere. The neutron energy peaks at around 1 MeV and rapidly drops above. At sea level, the production of neutrons is about 20 neutrons per second per kilogram of material interacting with the cosmic rays (or, about 100-300 neutrons per square meter per second). The flux is dependent on geomagnetic latitude, with a maximum at about 45 degrees. At solar minimums, due to lower solar magnetic field shielding, the flux is about twice as high vs the solar maximum. It also dramatically increases during solar flares. In the vicinity of larger heavier objects, e.g. buildings or ships, the neutron flux measures higher; this is known as "cosmic ray induced neutron signature", or "ship effect" as it was first detected with ships at sea.
Artificial background radiation.
Medical.
The global average human exposure to artificial radiation is 0.6 mSv/a, primarily from medical imaging. This medical component can range much higher, with an average of 3 mSv per year across the USA population. Other human contributors include smoking, air travel, radioactive building materials, historical nuclear weapons testing, nuclear power accidents and nuclear industry operation.
A typical chest x-ray delivers 0.02 mSv (2 mrem) of effective dose. A dental x-ray delivers a dose of 5 to 10 µSv The average American receives about 3 mSv of diagnostic medical dose per year; countries with the lowest levels of health care receive almost none. Radiation treatment for various diseases also accounts for some dose, both in individuals and in those around them.
Consumer items.
Cigarettes contain polonium-210, originating from the decay products of radon, which stick to tobacco leaves. Heavy smoking results in a radiation dose of 160 mSv/year to localized spots at the bifurcations of segmental bronchi in the lungs from the decay of polonium-210. This dose is not readily comparable to the radiation protection limits, since the latter deal with whole body doses, while the dose from smoking is delivered to a very small portion of the body.
Air travel causes increased exposure to cosmic radiation. The average extra dose to flight personnel is 2.19 mSv/year.
Atmospheric nuclear testing.
Frequent above-ground nuclear explosions between the 1940s and 1960s scattered a substantial amount of radioactive contamination. Some of this contamination is local, rendering the immediate surroundings highly radioactive, while some of it is carried longer distances as nuclear fallout; some of this material is dispersed worldwide. The increase in background radiation due to these tests peaked in 1963 at about 0.15 mSv per year worldwide, or about 7% of average background dose from all sources. The Limited Test Ban Treaty of 1963 prohibited above-ground tests, thus by the year 2000 the worldwide dose from these tests has decreased to only 0.005 mSv per year.
Occupational exposure.
The ICRP recommends limiting occupational radiation exposure to 50 mSv (5 rem) per year, and 100 mSv (10 rem) in 5 years.
At an IAEA conference in 2002, it was recommended that occupational doses below 1–2 mSv per year do not warrant regulatory scrutiny.
Nuclear accidents.
Under normal circumstances, nuclear reactors release small amounts of radioactive gases, which cause negligibly-small radiation exposures to the public. Events classified on the International Nuclear Event Scale as incidents typically do not release any additional radioactive substances into the environment. Large releases of radioactivity from nuclear reactors are extremely rare. Until the present day, there were two major civilian accidents - the Chernobyl accident and the Fukushima I nuclear accidents - which caused substantial contamination. The Chernobyl accident was the only one to cause immediate deaths.
Total doses from the Chernobyl accident ranged from 10 to 50 mSv over 20 years for the inhabitants of the affected areas, with most of the dose received in the first years after the disaster, and over 100 mSv for liquidators. There were 28 deaths from acute radiation syndrome.
Total doses from the Fukushima I accidents were between 1 and 15 mSv for the inhabitants of the affected areas. Thyroid doses for children were below 50 mSv. 167 cleanup workers received doses above 100 mSv, with 6 of them receiving more than 250 mSv (the Japanese exposure limit for emergency response workers).
The average dose from the Three Mile Island accident was 0.01 mSv.
Non-civilian: In addition to the civilian accidents described above, several accidents at early nuclear weapons facilities - such as the Windscale fire, the contamination of the Techa River by the nuclear waste from the Mayak compound, and the Kyshtym disaster at the same compound - released substantial radioactivity into the environment. The Windscale fire resulted in thyroid doses of 5-20 mSv for adults and 10-60 mSv for children. The doses from the accidents at Mayak are unknown.
Nuclear fuel cycle.
The Nuclear Regulatory Commission, the United States Environmental Protection Agency, and other U.S. and international agencies, require that licensees limit radiation exposure to individual members of the public to 1 mSv (100 mrem) per year.
Other.
Coal plants emit radiation in the form of radioactive fly ash which is inhaled and ingested by neighbours, and incorporated into crops. A 1978 paper from Oak Ridge National Laboratory estimated that coal-fired power plants of that time may contribute a whole-body committed dose of 19 µSv/a to their immediate neighbours in a radius of 500 m. The United Nations Scientific Committee on the Effects of Atomic Radiation's 1988 report estimated the committed dose 1 km away to be 20 µSv/a for older plants or 1 µSv/a for newer plants with improved fly ash capture, but was unable to confirm these numbers by test. When coal is burned, uranium, thorium and all the uranium daughters accumulated by disintegration — radium, radon, polonium — are released. Radioactive materials previously buried underground in coal deposits are released as fly ash or, if fly ash is captured, may be incorporated into concrete manufactured with fly ash.
Other usage.
In other contexts, background radiation may simply be any radiation that is pervasive, whether ionizing or not. A particular example of this is the cosmic microwave background radiation, a nearly uniform glow that fills the sky in the microwave part of the spectrum; stars, galaxies and other objects of interest in radio astronomy stand out against this background.
In a laboratory, background radiation refers to the measured value from any sources that affect an instrument when a radiation source sample is not being measured. This background rate, which must be established as a stable value by multiple measurements, usually before and after sample measurement, is subtracted from the rate measured when the sample is being measured.
Background radiation for occupational doses measured for workers is all radiation dose that is not measured by radiation dose measurement instruments in potential occupational exposure conditions. This includes both "natural background radiation" and any medical radiation doses. This value is not typically measured or known from surveys, such that variations in the total dose to individual workers is not known. This can be a significant confounding factor in assessing radiation exposure effects in a population of workers who may have significantly different natural background and medical radiation doses. This is most significant when the occupational doses are very low.

</doc>
<doc id="4884" url="http://en.wikipedia.org/wiki?curid=4884" title="Balmoral">
Balmoral

Balmoral may refer to:

</doc>
<doc id="4885" url="http://en.wikipedia.org/wiki?curid=4885" title="Bannock">
Bannock

Bannock has more than one meaning:

</doc>
<doc id="4886" url="http://en.wikipedia.org/wiki?curid=4886" title="Banquo">
Banquo

Lord Banquo, Thane of Lochaber, is a character in William Shakespeare's 1606 play "Macbeth". In the play, he is at first an ally to Macbeth (both are generals in the King's army) and they are together when they meet the Three Witches. After prophesying that Macbeth will become king, the witches tell Banquo that he will not be king himself, but that his descendants will be. Later, Macbeth in his lust for power sees Banquo as a threat and has him murdered; Banquo's son, Fleance, escapes. Banquo's ghost returns in a later scene, causing Macbeth to react with alarm during a public feast.
Shakespeare borrowed the character of Banquo from "Holinshed's Chronicles", a history of Britain published by Raphael Holinshed in 1587. In "Chronicles" Banquo is an accomplice to Macbeth in the murder of the king, rather than a loyal subject of the king who is seen as an enemy by Macbeth. Shakespeare may have changed this aspect of his character in order to please King James, who was thought at the time to be a descendant of the real Banquo. Critics often interpret Banquo's role in the play as being a foil to Macbeth, resisting evil where Macbeth embraces it. Sometimes, however, his motives are unclear, and some critics question his purity. He does nothing to accuse Macbeth of murdering the king, even though he has reason to believe Macbeth is responsible. 
Source.
Shakespeare often used Raphael Holinshed's "Chronicles of England, Scotland, and Ireland"—commonly known as "Holinshed's Chronicles"—as a source for his plays, and in "Macbeth" he borrows from several of the tales in that work. Holinshed portrays Banquo as a historical figure: he is an accomplice in Mac Bethad mac Findlaích's (Macbeth's) murder of Donnchad mac Crínáin (King Duncan) and plays an important part in ensuring that Macbeth, not Máel Coluim mac Donnchada (Malcolm), takes the throne in the coup that follows. Holinshed in turn used an earlier work, the "Scotorum Historiae" (1526–7) by Hector Boece, as his source. Boece's work is the first known record of Banquo and his son Fleance; and scholars such as David Bevington generally consider them fictional characters invented by Boece. In Shakespeare's day, however, they were considered historical figures of great repute, and the king, James I, based his claim to the throne in part on a descent from Banquo. The House of Stuart was descended from Walter fitz Alan, the first High Steward of Scotland, and he was believed to have been the grandson of Fleance and Gruffydd ap Llywelyn's daughter, Nesta verch Gruffydd. In reality Walter fitz Alan was the son of Alan fitz Flaad, a Breton knight.
Unlike his sources, Shakespeare gives Banquo no role in the King's murder, making it a deed committed solely by Macbeth and his wife. Why Shakespeare's Banquo is so different from the character described by Holinshed and Boece is not known, though critics have proposed several possible explanations. First among them is the risk associated with portraying the king's ancestor as a murderer and conspirator in the plot to overthrow a rightful king, as well as the author's desire to flatter a powerful patron. But Shakespeare may also simply have altered Banquo's character because there was no dramatic need for another accomplice to the murder. There was, however, a need to provide a dramatic contrast to Macbeth; a role that many scholars argue is filled by Banquo. Similarly, when Jean de Schelandre wrote about Banquo in his "Stuartide" in 1611, he also changed the character by portraying him as a noble and honourable man—the critic D. W. Maskell describes him as “…Schelandre's paragon of valour and virtue”—probably for reasons similar to Shakespeare's.
Banquo's role in the coup that follows the murder is harder to explain. Banquo's loyalty to Macbeth, rather than Malcolm, after Duncan's death makes him a passive accomplice in the coup: Malcolm, as Prince of Cumberland, is the rightful heir to the throne and Macbeth a usurper. Daniel Amneus, however, argues that when Ross and Angus bring King Duncan's praise, and the news that Macbeth has been granted the title of Thane of Cawdor, the "greater honor" he ascribes to Macbeth is actually his title as Prince of Cumberland. If Macbeth, rather than Malcolm, is Prince of Cumberland then Macbeth would be next in line to the throne and no coup would be needed, effectively removing this ambiguity from Banquo's character.
Role in the play.
Banquo is in a third of the play's scenes, as both a human and a ghost. As significant as he is to the plot, he has fewer lines than the relatively insignificant Ross, a Scottish nobleman who survives the play. In the second scene of the play, King Duncan describes the manner in which Macbeth, Thane of Glamis, and Banquo, Thane of Lochaber, bravely led his army against invaders, fighting side by side. In the next scene, Banquo and Macbeth, returning from the battle together, encounter the Three Witches, who predict that Macbeth will become Thane of Cawdor, and then king. Banquo, skeptical of the witches, challenges them to predict his own future, and they foretell that Banquo will never himself take the throne, but will beget a line of kings. Banquo remains skeptical after the encounter, wondering aloud if evil can ever speak the truth. He warns Macbeth that evil will offer men a small, hopeful truth only in order to catch them in a deadly trap.
When Macbeth kills the king and takes the throne, Banquo—the only one aware of this encounter with the witches—reserves judgment for God. He is unsure whether Macbeth committed regicide to gain the throne, but muses in a soliloquy that "I fear / Thou play'dst most foully for 't". He offers his respects to the new King Macbeth and pledges loyalty. Later, worried that Banquo's descendants and not his own will rule Scotland, Macbeth sends men to kill Banquo and his son Fleance. During the melee, Banquo holds off the assailants so that Fleance can escape, but is himself killed. The ghost of Banquo later returns to haunt Macbeth at the banquet in act three, scene four. A terrified Macbeth sees him, while the apparition is invisible to his guests. He appears again to Macbeth in a vision granted by the Three Witches, wherein Macbeth sees a long line of kings descended from Banquo.
Analysis.
Foil to Macbeth.
Many scholars see Banquo as a foil and a contrast to Macbeth. Macbeth, for example, eagerly accepts the Three Witches' prophecy as true and seeks to help it along. Banquo, on the other hand, doubts the prophecies and the intentions of these seemingly evil creatures. Whereas Macbeth places his hope in the prediction that he will be king, Banquo argues that evil only offers gifts that lead to destruction. Banquo steadily resists the temptations of evil within the play, praying to heaven for help, while Macbeth seeks darkness, and prays that evil powers will aid him. This is visible in act two; after Banquo sees Duncan to bed, he says: "There's husbandry in heaven, / Their candles are all out". This premonition of the coming darkness in association with Macbeth's murders is repeated just before Banquo is killed: "it will be rain to-night", Banquo tells his son Fleance.
Banquo's status as a contrast to Macbeth makes for some tense moments in the play. In act two, scene one, Banquo meets his son Fleance and asks him to take both his sword and his dagger ("Hold, take my sword ... Take thee that too"). He also explains that he has been having trouble sleeping due to "cursed thoughts that nature / gives way to in repose!" On Macbeth's approach, he demands the sword returned to him quickly. Scholars have interpreted this to mean that Banquo has been dreaming of murdering the king as Macbeth's accomplice in order to take the throne for his own family, as the Three Witches prophesied to him. In this reading, his good nature is so revolted by these thoughts that he gives his sword and dagger to Fleance to be sure they do not come true, but is so nervous at Macbeth's approach that he demands them back. Other scholars have responded that Banquo's dreams have less to do with killing the king and more to do with Macbeth. They argue that Banquo is merely setting aside his sword for the night. Then, when Macbeth approaches, Banquo, having had dreams about Macbeth's deeds, takes back his sword as a precaution in this case.
Macbeth eventually sees that Banquo can no longer be trusted to aid him in his evil, and considers his friend a threat to his newly acquired throne. Thus he has him murdered. Banquo's ability to live on in different ways is another oppositional force, in this case to Macbeth's impending death. His spirit lives on in Fleance, his son, and in his ghostly presence at the banquet.
Ghost scenes.
When Macbeth returns to the witches later in the play, they show him an apparition of the murdered Banquo, along with eight of his descendants. The scene carries deep significance: King James, on the throne when "Macbeth" was written, was believed to be separated from Banquo by nine generations. What Shakespeare writes here thus amounts to a strong support of James' right to the throne by lineage, and for audiences of Shakespeare's day, a very real fulfillment of the witches' prophecy to Banquo that his sons would take the throne. This apparition is also deeply unsettling to Macbeth, who not only wants the throne for himself, but also desires to father a line of kings.
Banquo's other appearance as a ghost during the banquet scene serves as an indicator of Macbeth's conscience returning to plague his thoughts. Banquo's triumph over death appears symbolically, insofar as he literally takes Macbeth's seat during the feast. Shocked, Macbeth uses words appropriate to the metaphor of usurpation, describing Banquo as "crowned" with wounds. The spirit drains Macbeth's manhood along with the blood from his cheeks; as soon as Banquo's form vanishes, Macbeth announces: "Why, so; being gone, / I am a man again."
Like the vision of Banquo's lineage, the banquet scene has also been the subject of criticism. Critics have questioned whether not one, but perhaps two ghosts appear in this scene: Banquo and Duncan. Scholars arguing that Duncan attends the banquet state that Macbeth's lines to the Ghost could apply equally well to the slain king. "Thou canst not say I did it", for example, can mean that Macbeth is not the man who actually killed Banquo, or it can mean that Duncan, who was asleep when Macbeth killed him, cannot claim to have seen his killer. To add to the confusion, some lines Macbeth directs to the ghost, such as "Thy bones are marrowless", cannot rightly be said of Banquo, who has only recently died.
Scholars debate whether Macbeth's vision of Banquo is real or a hallucination. Macbeth had already seen a hallucination before killing Duncan: a knife hovering in the air. Several performances of the play have even ignored the stage direction to have the Ghost of Banquo enter at all, heightening the sense that Macbeth is growing mad, since the audience cannot see what he claims to see. Scholars opposing this view claim that while the dagger is unusual, ghosts of murdered victims are more believable, having a basis in the audience's superstitions. Spirits in other Shakespeare plays—notably "Hamlet" and "Midsummer Night's Dream"—exist in ambiguous forms, occasionally even calling into question their own presence.
Performances and interpretations.
Banquo's role, especially in the banquet ghost scene, has been subject to a variety of interpretations and mediums. Shakespeare's text states: "Enter Ghost of Banquo, and sits in Macbeth's place." Several television versions have altered this slightly, having Banquo appear suddenly in the chair, rather than walking onstage and into it. Special effects and camera tricks also allow producers to make the ghost disappear and reappear, highlighting the fact that "only" Macbeth can see it.
Stage directors, unaided by post-production effects and camera tricks, have used other methods to depict the ghost. In the late 19th century, elaborate productions of the play staged by Henry Irving employed a wide variety of approaches for this task. In 1877 a green silhouette was used to create a ghostlike image; ten years later a trick chair was used to allow an actor to appear in the middle of the scene, and then again from the midst of the audience. In 1895 a shaft of blue light served to indicate the presence of Banquo's spirit. In 1933 a Russian director named Theodore Komisarjevsky staged a modern retelling of the play (Banquo and Macbeth were told of their future through palmistry); he used Macbeth's shadow as the ghost.
Film adaptations have approached Banquo's character in a variety of ways. In 1936 Orson Welles helped produce an African-American cast of the play, including Canada Lee in the role of Banquo. Akira Kurosawa's 1957 adaptation "Throne of Blood" makes the character into Capitan Miki (played by Minoru Chiaki), slain by Macbeth's equivalent (Captain Washizu) when his wife explains that she is with child. News of Miki's death does not reach Washizu until after he has seen the ghost in the banquet scene. In Roman Polanski's 1971 adaptation, Banquo is played by acclaimed stage actor Martin Shaw, in a style reminiscent of earlier stage performances. Polanski's version also emphasises Banquo's objection to Macbeth's ascendency by showing him remaining silent as the other thanes around him hail Macbeth as king. in the 1990 telling of Macbeth in a New York Mafia crime family setting, "Men of Respect", the character of Banquo is named "Bankie Como" and played by American actor Dennis Farina.

</doc>
<doc id="4887" url="http://en.wikipedia.org/wiki?curid=4887" title="British Army">
British Army

The British Army is the land warfare branch of the British Armed Forces. The English Army, founded in 1660, was succeeded in 1707 by the new British Army, incorporating existing Scottish regiments. It was administered by the War Office from London, which was subsumed into the Ministry of Defence in 1964. The professional head of the British Army is the Chief of the General Staff.
The full-time element of the British Army is referred to as the "Regular Army" and has been since the creation of the reservist "Territorial Force" in 1908. All members of the Army swear (or affirm) allegiance to the monarch as commander-in-chief. However, the Bill of Rights of 1689 requires Parliamentary consent for the Crown to maintain a standing army in peacetime. Parliament approves the continued existence of the Army by passing an Armed Forces Act at least once every five years. In contrast to the Royal Navy, Royal Marines and Royal Air Force, the British Army does not include "Royal" in its title because, after a historic struggle between Parliament and monarchy, the British Army has always been answerable to Parliament rather than the Monarch. Many of the Army's constituent regiments and corps have been granted the "Royal" prefix and have members of the Royal Family occupying senior honorary positions within some regiments.
Throughout its history, the British Army has seen action in a number of major wars involving the world's great powers, including the Seven Years' War, the Napoleonic Wars, the Crimean War, the First World War and Second World War. Repeatedly emerging victorious from these decisive wars allowed Britain to influence world events with its policies and establish itself as one of the world's leading military and economic powers. Today, the British Army is deployed in several countries, including as an expeditionary force and a United Nations peacekeeping force. Additionally, the British Army maintains five permanent overseas postings. 
History.
Shortly after the Act of Union in 1707 the English Army and Scottish and Irish regiments were amalgamated to form the British Army. The order of seniority of the most senior line regiments in the British Army is based on the order of seniority in the English army. Scottish and Irish regiments were only allowed to take a rank in the English army from the date of their arrival in England or the date when they were first placed on the English establishment. For example, in 1694 a board of general officers was convened to decide the rank of English, Irish and Scots regiments serving in the Netherlands; the regiment that became known as the Scots Greys were designated as the 4th Dragoons because there were three English regiments raised prior to 1688 when the Scots Greys were first placed on the English establishment. In 1713, when a new board of general officers was convened to decide upon the rank of several regiments, the seniority of the Scots Greys was reassessed and based on their entry into England in June 1685. At that time there was only one English regiment of dragoons, and so after some delay the Scots Greys obtained the rank of 2nd Dragoons in the British Army.
Following William and Mary's accession to the throne, England involved itself in the War of the Grand Alliance, primarily to prevent a French invasion restoring Mary's father, James II. Following the union of England and Scotland in 1707, and the creation of the United Kingdom of Great Britain and Ireland in 1801, British foreign policy on the continent was to contain expansion by its competitor powers such as France and Spain. Spain, in the previous two centuries, had been the dominant global power, and the chief threat to England's early transatlantic ambitions, but was now waning. The territorial ambitions of the French, however, led to the War of the Spanish Succession and the Napoleonic Wars. Russian activity led to the Crimean War. After 1745, recruits were increasingly drawn from Scotland; by the mid-1760s between one fifth and one third of officers were from Scotland.
From the time of the end of the Seven Years' War in 1763, Great Britain, and its successor, the United Kingdom, was one of the leading military and economic powers of the world.
Early British Empire.
The British Empire expanded in this time to include colonies, protectorates, and Dominions throughout the Americas, Africa, Asia and Australasia. Although the Royal Navy is widely regarded as having been vital for the rise of the British Empire, and British dominance of the world, the British Army played an important role in the colonisation of India and other regions. Typical tasks included garrisoning the colonies, capturing strategically important territories, and participating in actions to pacify colonial borders, provide support to allied governments, suppress Britain's rivals, and protect against foreign powers and hostile natives.
British soldiers also helped capture strategically important territories, allowing the empire to expand. The army was also involved in numerous wars to pacify the borders, or to prop up friendly governments, and thereby keep other, competitive, empires away from the British Empire's borders. Among these actions were the Seven Years' War, the American Revolutionary War, the Napoleonic Wars, the First and Second Opium Wars, the Boxer Rebellion, the New Zealand wars, the Sepoy Rebellion of 1857, the First and Second Boer Wars, the Fenian raids, the Irish War of Independence, its serial interventions into Afghanistan (which were meant to maintain a friendly buffer state between British India and the Russian Empire), and the Crimean War (to keep the Russian Empire at a safe distance by coming to Turkey's aid).
As had its predecessor, the English Army, the British Army fought Spain, France, and the Netherlands for supremacy in North America and the West Indies. With native and provincial assistance, the Army conquered New France in the North American theatre of the Seven Years' War and subsequently suppressed a Native American uprising in Pontiac's War. The British Army suffered defeat in the American War of Independence, losing the Thirteen Colonies but holding on to Canada.
The British Army was heavily involved in the Napoleonic Wars and served in multiple campaigns across Europe (including continuous deployment in the Peninsular War), the Caribbean, North Africa and later in North America. The war between the British and the First French Empire of Napoleon Bonaparte stretched around the world and at its peak, in 1813, the regular army contained over 250,000 men. A Coalition of Anglo-Dutch and Prussian Armies under the Duke of Wellington and Field Marshal von Blücher defeated Napoleon at the Battle of Waterloo in 1815.
The English had been involved, both politically and militarily, in Ireland since being given the Lordship of Ireland by the Pope in 1171. The campaign of the English republican Protector, Oliver Cromwell, involved uncompromising treatment of the Irish towns (most notably Drogheda and Wexford) that had supported the Royalists during the English Civil War. The English Army (and subsequently the British Army) stayed in Ireland primarily to suppress numerous Irish revolts and campaigns for independence. In addition to its ongoing conflict with ethnic Irish nationalists, it was faced with the prospect of battling Anglo-Irish and Ulster Scots peoples in Ireland, angered primarily by unfavourable taxation of Irish produce imported into Britain, who, alongside other Irish groups, had raised their own volunteer army and threatened to emulate the American colonists if their conditions were not met. Having learnt from their experience in America, the British government sought a political solution. The British Army found itself fighting Irish rebels, both Protestant and Catholic, primarily in Ulster and Leinster (Wolfe Tone's United Irishmen) in the 1798 rebellion.
In addition to battling the armies of other European Empires (and of its former colonies, the United States, in the American War of 1812), in the battle for global supremacy, the British Army fought the Chinese in the First and Second Opium Wars, and the Boxer Rebellion, Māori tribes in the first of the New Zealand Wars, Nawab Shiraj-ud-Daula's forces and British East India Company mutineers in the Sepoy Rebellion of 1857, the Boers in the First and Second Boer Wars, Irish Fenians in Canada during the Fenian raids and Irish separatists in the Anglo-Irish War.
The vastly increasing demands of imperial expansion, and the inadequacies and inefficiencies of the underfunded, post-Napoleonic Wars British Army, and of the Militia, Yeomanry, and Volunteer Force, led to the Cardwell and Childers Reforms of the late 19th century, which gave the British Army its modern shape, and redefined its regimental system. The Haldane Reforms of 1907 formally created the Territorial Force as the Army's volunteer reserve component by merging and reorganising the Volunteer Force, Militia, and Yeomanry.
World wars.
Great Britain's dominance of the world had been challenged by numerous other powers; in the 20th century, most notably Germany. A century before, it was still vying with Napoleonic France for pre-eminence in Europe and around the world, and Hannoverian Britain's natural allies were the various Kingdoms and principalities of Northern Germany. By the middle of the 19th century, Britain and France were allied in preventing Russia's appropriation of the Ottoman Empire (although it was the fear of French invasion that led, shortly after, to the creation of the Volunteer Force). By the first decade of the 20th century, however, the UK was allied with France (by the Entente Cordiale) and Russia (which had its own secret agreement with France of mutual support in any war against the Prussian-led German Empire and the Austro-Hungarian Empire), and when the First World War broke out in 1914, the British Army sent the British Expeditionary Force to France and Belgium to prevent Germany from occupying these countries. The British Army created the Mediterranean Expeditionary Force in Egypt and sent it to Gallipoli in an unsuccessful attempt to capture Constantinople and secure a sea route to Russia. After the retreat from Gallipoli nearly 400,000 men in 13 divisions from the Mediterranean Expeditionary Force and the Force in Egypt formed a strategic reserve in Egypt called the Egyptian Expeditionary Force. With most of the strategic reserve sent to the Western Front, an Egyptian Expeditionary Force of two British infantry and one Australian and New Zealand mounted division in the Eastern Force, successfully defend the Suez Canal and Romani in 1916 from German and Ottoman incursions. This force captured the Sinai and garrisoned the extended lines of communication, but in early 1917 their advance was stopped at Gaza until towards the end of the year when a greatly enlarged force of infantry and mounted troops captured Beersheba, most of southern Palestine and Jerusalem. Allenby's force, now including Indian Army units which replaced a number of British units sent to the Western Front, captured the southern Jordan Valley in 1918 and carried out two major, but unsuccessful attacks to Amman and Es Salt and occupied part of the Jordan Valley, during preparations for his final successful assault in September at the Battle of Megiddo. As a result of the Egyptian Expeditionary Force's capture of two Ottoman armies, an armistice with the Ottoman Empire was signed on 31 October 1918.
The war would be the most devastating in British military history, with near 800,000 men killed and over 2 million wounded. In the early part of the war, the professional force of the BEF was virtually destroyed and, by turns, a volunteer (and then conscripted) force replaced it. Major battles included the Battle of the Somme. Advances in technology saw advent of the tank, with the creation of the Royal Tank Regiment, and advances in aircraft design, with the creation of the Royal Flying Corps, which were to be decisive in future battles. Trench warfare dominated strategy on the Western Front, and the use of chemical and poison gases added to the devastation.
The Second World War broke out in 1939 with the German invasion of Poland. British assurances to the Polish led the British Empire to declare war on Germany. Again an Expeditionary Force was sent to France, only to be hastily evacuated as the German forces swept through the Low Countries and across France in 1940. Only the Dunkirk evacuation saved the entire Expeditionary Force from capture. Later, however, the British would have spectacular success defeating the Italians and Germans at the Battle of El Alamein in North Africa, and in the D-Day invasion of Normandy with the help of American, Canadian, Australian, New Zealand, Indian and Free French forces. Almost half of the Allied soldiers on D-day were British. In the Far East, the British army battled the Japanese in Burma. The Second World War saw the British Army develop its Special Air Service, Commando units and the Parachute Regiment.
Postcolonial era.
After the end of the Second World War, the British Army was significantly reduced in size, although National Service continued until 1960. This period also saw the process of decolonisation commence with the partition and independence of India and Pakistan, followed by the independence of British colonies in Africa and Asia. Accordingly the army's strength was further reduced, in recognition of Britain's reduced role in world affairs, outlined in the 1957 Defence White Paper. This was despite major actions in Korea in the early 1950s and Suez in 1956. A large force of British troops also remained in Germany, facing the threat of Soviet invasion. The British Army of the Rhine was the Germany garrison formation, with the main fighting force being I (BR) Corps. The Cold War saw significant technological advances in warfare and the Army saw more technologically advanced weapons systems come into service.
Despite the decline of the British Empire, the Army was still deployed around the world, fighting wars in Aden, Indonesia, Cyprus, Kenya and Malaya. In 1982 the British Army, alongside the Royal Marines, helped to recapture the Falkland Islands during the Falklands conflict against Argentina.
In the three decades following 1969, the Army was heavily deployed in Northern Ireland, to support the Royal Ulster Constabulary (later the Police Service of Northern Ireland) in their conflict with republican paramilitary groups, called Operation Banner. The locally recruited Ulster Defence Regiment was formed, later becoming home service battalions in the Royal Irish Regiment in 1992, before being disbanded in 2006. Over 700 soldiers were killed during the Troubles. Following the IRA ceasefires between 1994 and 1996 and since 1997, demilitarisation has taken place as part of the peace process, reducing the military presence from 30,000 to 5,000 troops. On 25 June 2007, the Second Battalion Princess of Wales's Royal Regiment vacated the Army complex at Bessbrook Mill in Armagh. This is part of the 'normalisation' programme in Northern Ireland in response to the IRA's declared end to its activities.
Today.
Personnel.
The British Army is purely a professional force since National service came to an end. The full-time element of the British Army is referred to as the "Regular Army" since the creation of the reservist "Territorial Force" in 1908. The size and structure of the British Army is continually evolving, but on 1 December 2013, the British Army employed; 95,800 Regulars, 3,130 Gurkhas and 26,500 Army Reservists for a combined component strength of 125,430 personnel.
The future transformation of the British Army is referred to as "Army 2020", which is the result of the Strategic Defence and Security Review (SDSR) in October 2010 and a number of following reviews and modifications thereafter. According to the Ministry of Defence, Army 2020 will "ensure that the British Army remains the most capable Army in its class" and enable "it to better meet the security challenges of the 2020s and beyond". Initially, the SDSR outlined a reduction of the Regular British Army by 7,000 to a trained strength of 95,000 personnel by 2015. However, following a further independent review on the future structure of the British Army, "Future Reserves 2020", it was announced that the Regular Army will be reduced to a trained strength of 82,000 while the Army Reserve will be increased to a trained strength of around 30,000 personnel. There will of course be an added margin for soldiers in training. This reform will bring the ratio of regular and part-time personnel of the British Army in line with US and Canadian allies. Perhaps the most important aspect of Army 2020 is that the Army Reserve will become "fully integrated" with the Regular Army and "better prepared" for overseas deployments and operations. However, budget cuts have made Britain a "hostile recruiting environment", with barely half the required number of new reservists actually signing up.
In addition to the active elements of the British Army (Regular and Army Reserve), all ex-Regular Army personnel remain liable to be recalled for duty in a time of need, this is known as the Regular Reserve. The Regular Reserve is separated into two categories: A and D. Category A is mandatory, with the length of time serving in category A depending on time spent in Regular service. Category D is voluntary and consists of personnel who are no-longer required to serve in category A. Regular Reserves in both category A and D serve under a fixed-term reserve contract and are liable to report for training or service overseas and at home. These contracts are similar in nature to those of the Army Reserve. The Long Term reserve is also part of the Regular Reserve but excludes personnel serving in categories A and D. Unlike the other reserves the Long Term reserve do not serve under a contract of any sort, instead they retain a "statutory liability for service" and may be recalled to service under Section 52 of the Reserve Forces Act (RFA) 1996 (until the age of 55). In 2007 there were 121,800 Regular Reserves of the British Army, of which, 33,760 served in categories A and D. Publications since April 2013 no longer report the entire strength of the Regular Reserve, instead they only give a figure for the Regular Reserves serving in categories A and D only. They had a reported strength of 31,300 personnel in 2013.
The table below shows historical personnel trends of the British Army from 1750 to 2014. The Army Reserve – or Territorial Army, as it was known then – did not come into existence until 1908.
Equipment.
Infantry
The basic infantry weapon of the British Army is the L85A2 assault rifle, sometimes equipped with an L17A2 underbarrel grenade launcher. The rifle has several variants, such as the L86A2, the Light Support Weapon (LSW), and the L22A2 carbine issued to tank crews. Support fire is provided by the FN Minimi light machine gun and the L7 General Purpose Machine Gun (GPMG); indirect fire by 51 and 81 mm mortars. Sniper rifles used include the L118A1 7.62 mm, the L115A3 and the AW50F, all produced by Accuracy International. Some units use the L82A1 .50 calibre Barrett sniper rifle. More recently, the L128A1 (Benelli M4) 'combat shotgun' has been adopted, and is intended for close quarters combat in Afghanistan.
Armour
The British Army's main battle tank is the Challenger 2. Other armoured vehicles include the Supacat "Jackal" MWMIK and the Iveco "Panther" CLV. The Warrior Infantry Fighting Vehicle is the primary armoured personnel carrier, although many variants of the Combat Vehicle Reconnaissance (tracked) are used too, as well as the Saxon APC and the FV430 series, which is now having its engines and armour replaced and returned to front-line service as the Bulldog. The British Army commonly uses the Land Rover Wolf and Land Rover Defender.
Artillery
The Army uses three main artillery systems: the Multi Launch Rocket System (MLRS), AS-90 and L118. The MLRS was first used operationally in Operation Granby and has a range of . The AS-90 is a 155 mm self-propelled gun. The L118 Light Gun is a 105 mm towed gun used primarily in support of 16 Air Assault Brigade, 19 Light Brigade and 3 Commando Brigade (Royal Marines). The Rapier FSC Missile System is the Army's primary battlefield air defence system, widely deployed since the Falklands War and the Starstreak HVM (High Velocity Missile) is a surface-to-air weapon, launched either by a single soldier or from a vehicle-mounted launcher.
Army Aviation
The Army Air Corps (AAC) provides direct aviation support for the Army, although the RAF also contributes by providing support helicopters. The primary attack helicopter is the Westland WAH-64 Apache, a license-built, modified version of the US AH-64 Apache, which replaced the Westland Lynx AH7 in the anti-tank role. The Lynx remains in service as an armed escort, surveillance and light utility helicopter. Other types are used in specialised roles e.g. the Westland Gazelle as a light surveillance aircraft and the Bell 212 for support in specific Jungle / 'hot and high' environments The Eurocopter AS 365N Dauphin is used for Special Operations Aviation and the Britten-Norman Islander is a light fixed-wing aircraft used for airborne reconnaissance and command and control.
Recent and current conflicts.
Persian Gulf War.
The ending of the Cold War saw a significant cut in manpower, as outlined in the Options for Change review. Despite this, the Army has been deployed in an increasingly global role, and contributed 50,000 troops to the coalition force that fought Iraq in the Persian Gulf War. British forces were put in control of Kuwait after it was liberated.
47 British Military personnel died during the Persian Gulf War.
Balkans conflicts.
The British Army was deployed to Yugoslavia in 1992; initially this force formed part of the United Nations Protection Force. In 1995 command was transferred to IFOR and then to SFOR. Currently troops are under the command of EUFOR. Over 10,000 troops were sent. In 1999 British forces under the command of SFOR were sent to Kosovo during the conflict there. Command was subsequently transferred to KFOR. Between early 1993 and June 2010, 72 British military personnel died on operations in the former Yugoslavian countries of Bosnia, Kosovo and Macedonia.
War in Afghanistan.
In November 2001 the United Kingdom, as a part of Operation Enduring Freedom with the United States, invaded Afghanistan to topple the Taliban. The 3rd Division were deployed in Kabul, to assist in the liberation of the troubled capital. The Royal Marines' 3 Commando Brigade (part of the Royal Navy but including a number of Army units), also swept the mountains. The British Army is today concentrating on fighting Taliban forces and bringing security to Helmand province. Approximately 7,900 British troops (including marines, airmen and sailors) are currently in Afghanistan, making it the second largest force after the US. Around 500 extra British troops were deployed in 2009, bringing the British Army deployment total up to 9,500 (excluding Special Forces). In December 2012 the Prime Minister, David Cameron, announced that 3,800 troops – almost half of the force serving in Helmand Province – would be withdrawn during 2013 with numbers to fall to approximately 5,200. By March 2014 troop levels were down to 4,000. Combat operations are projected to end in 2014. Between 2001 and 5 March 2014 a total of 448 British military personnel have died on operations in Afghanistan.
Iraq War.
In 2003 the United Kingdom was a major contributor to the invasion of Iraq, sending a force that would reach 46,000 military personnel. The British Army controlled the southern regions of Iraq and maintained a peace-keeping presence in the city of Basra until their withdrawal on 30 April 2009. 179 British Military personnel have died on operations in Iraq. All of the remaining British troops were fully withdrawn from Iraq after the Iraqi government refused to extend their mandate.
The Troubles.
Although having permanent garrisons there, the British Army was initially deployed in a peacekeeping role – codenamed "Operation Banner" – in Northern Ireland in the wake of Unionist attacks on Nationalist communities in Derry and Belfast and to prevent further Loyalist attacks on Catholic communities, under Operation Banner between 1969 and 2007 in support of the Royal Ulster Constabulary (RUC) and its successor, the Police Service of Northern Ireland (PSNI). There has been a steady reduction in the number of troops deployed in Northern Ireland since the Good Friday Agreement was signed in 1998. In 2005, after the Provisional Irish Republican Army announced an end to its armed conflict in Northern Ireland, the British Army dismantled posts and withdrew many troops, and restored troop levels to that of a peace-time garrison.
Operation Banner ended at midnight on 31 July 2007, bringing to an end some 38 years of continuous deployment, making it the longest in the British Army's history. An internal British Army document released in 2007 stated that the British Army had failed to defeat the IRA but had made it impossible for them to win through the use of violence. Operation Helvetic replaced Operation Banner in 2007 maintaining fewer servicemen in a much more benign environment. From 1971 to 1997 a total of 763 British military personnel were killed during the "Troubles". Some 300 deaths during the conflict were attributed to the British Army, including paramilitary and civilians. A total of 303 RUC officers were killed in the same time period. In March 2009, two soldiers and a Police Officer were killed in separate dissident republican attacks in Northern Ireland.
Formation and structure.
The structure of the British Army is complex, due to the different origins of its various constituent parts. It is broadly split into the Regular Army (full-time Officers/soldiers and units) and the Army Reserve (Spare-time Officers/soldiers and units).
In terms of its military structure, it has two parallel organisations, one administrative and one operational.
Administrative
Operational
Structure of units.
The standard operational units are structured as follows, although various units have their own structure, conventions, names and sizes:
Corps are made up of two or more divisions, but now are rarely deployed as a purely national formation due to the size of the British Army.
In place of a Battalion, a task-specific Battlegroup may be formed. A battlegroup is grown around the core of either an armoured regiment or infantry battalion, and has other units added or removed from it as necessary for its purpose. It results in a mixed formation of armour, infantry, artillery, engineers and support units, typically consisting of between 600 and 700 soldiers under the command of a Lieutenant Colonel.
A number of elements of the British Army use alternative terms for battalion, company and platoon. These include the Royal Armoured Corps, Corps of Royal Engineers, Royal Logistic Corps, and the Royal Corps of Signals who use regiment (battalion), squadron (company) and troop (platoon). The Royal Artillery are unique in using the term regiment in place of both corps and battalion, they also replace company with battery and platoon with troop.
Divisions.
The British Army currently has two operational divisions.
There are also a some ten brigades which are not part of any division report directly into Support Command.
Rapid Reaction Force.
16 Air Assault Brigade forms the bulk of the Army's rapid reaction force.
Combat support.
Force Troops Command, or FTC, forms the basis of the Army's Combat support, containing units ranging from artillery to military police.
Aviation components.
The British Army operates alongside the Royal Air Force and the Fleet Air Arm as part of Joint Helicopter Command, but the army also has its own Army Air Corps. Military helicopters of all three services are commanded by Joint Helicopter Command, a joint 2 star headquarters operating under HQ Land Forces.
Special forces.
The British Army contributes two of the three special forces formations within the UK Directorate of Special Forces; the Special Air Service Regiment and the Special Reconnaissance Regiment. The most famous formation is the Special Air Service Regiment. The SAS comprises one regular Regiment and two Army Reserve Regiments.
The regular Regiment, 22 SAS, has its headquarters and depot located in Hereford and consists of five squadrons: A, B, D, G and Reserve with a training wing. The two reserve SAS Regiments (21 SAS and 23 SAS) have a more limited role, to provide depth to the UKSF group through the provision of Individual and collective augmentation to the regular component of UKSF and standalone elements up to task group (Regimental) level focused on support and influence (S&I) operations to assist conflict stabilisation.
The Special Reconnaissance Regiment (SRR), formed in 2005 from existing assets, undertakes close reconnaissance and special surveillance tasks. Special Forces Support Group were formed around 1st Battalion the Parachute Regiment, with attached Royal Marines and RAF Regiment assets, the unit is also open to any member of the HM Armed Forces. The Special Forces Support Group are under the Operational Control of Director Special Forces to provide operational manoeuvre support to the elements of United Kingdom Special Forces.
British Overseas Territories military units.
Numerous military units were raised historically in British territories, including self-governing and Crown colonies, and protectorates. Whereas the Dominions, such as Canada and Australia, had their own armies before achieving complete independence, units raised in those territories which remained part of the realm of the UK were, and are, ultimately under the control of the UK government, and do not constitute separate armies. The UK retains responsibility for the defence of all of the fourteen remaining British Overseas Territories. Although the Cayman Islands premier has stated the desire to raise a Cayman Islands Defence Force when it can be afforded (it currently has only a cadet corps), becoming the fifth, only four of the remaining British Overseas Territories retain locally-raised regiments:
Royal Navy and RAF ground units.
The other armed services have their own infantry units which are not part of the British Army. The Royal Marines are amphibious light infantry forming part of the Naval Service, and the Royal Air Force has the RAF Regiment used for airfield defence, force protection duties and Forward Air Control.
Recruitment.
The Army mainly recruits within the United Kingdom; it normally has a recruitment target of around 12,000 soldiers per year. Low unemployment in Britain has resulted in the Army having difficulty in meeting its target. In the early years of the 21st century there has been a marked increase in the number of recruits from other (mostly Commonwealth) countries. In 2006 overseas recruitment, mostly in Commonwealth countries, generated more than 6,000 soldiers from 54 nations; together with the 3,000 Gurkhas, 10% of the British Army is a foreign national.
The Ministry of Defence now caps the number of recruits from Commonwealth countries, although this will not affect the Gurkhas. If the trend continues 10% of the army will be from Commonwealth countries before 2012. The cap is in place as some fear the army's British character is being diluted, and employing too many could make the army seen as employing mercenaries. The minimum recruitment age is 16 years (after the end of GCSEs), although soldiers may not serve on operations below 18 years; the maximum recruitment age was raised in January 2007 from 26 to 33 years. The normal term of engagement is 22 years, and, once enlisted, soldiers are not normally permitted to leave until they have served at least 4 years.
There has been a strong and continuing tradition of recruiting from Ireland including what is now the Republic of Ireland. Over 200,000 Irish soldiers fought in the First World War. More than 60,000 Irishmen from what was then the Irish Free State (now the Republic of Ireland) and 38,000 from Northern Ireland served in the Second World War, all volunteered.
Oath of allegiance.
All soldiers must take an oath of allegiance upon joining the Army, a process known as "attestation". Those who wish to swear by God use the following words:
Others replace the words "swear by Almighty God" with "solemnly, sincerely and truly declare and affirm". Under the reign of another monarch, the name of the monarch and all pronouns with gender are replaced appropriately.
Flags and ensigns.
The British Army does not have its own specific ensign for the whole Army, unlike the Royal Navy, which uses the White Ensign, and the RAF, which uses the Royal Air Force Ensign. Instead, the Army has different flags and ensigns, some for the entire army and many for the different regiments and corps. The official flag of the Army as a whole is the Union Flag, flown in a ratio of 3:5. A non-ceremonial flag also exists, which is used at recruiting events, military events and exhibitions. It also flies from the Ministry of Defence building in Whitehall.
Whilst at war, the Union Flag is always used, and this flag represents the Army on The Cenotaph at Whitehall in London (the UK's memorial to its war dead).
The British Army has throughout its history operated ships, ports and myriad boats. Boats, Landing Craft and Ports are still operated by the Army and ensigns exists for vessels commanded by the Army. The Royal Logistic Corps operates a large fleet of vessels from its base at Marchwood near Southampton. The Royal Engineers has had fleets since the introduction of diving in 1838 and was granted an ensign following the foundation of the Royal Engineers Submarine Mining Service in 1871, where it operated sea mine laying ships, before transfer of the trade to the Royal Navy. The Corps maintains a Blue Ensign defaced by the crest of the Board of Ordnance from where the Corps developed, which it flies from its fleet and shore establishments that routinely operate boats.
Each Foot Guards and line regiment (excluding The Rifles and Royal Gurkha Rifles (RGR)) also has its own flags, known as Colours—normally a Regimental Colour and a Queen's Colour. The design of different Regimental Colours vary but typically the colour has the Regiment's badge in the centre. The RGR carry the Queen's Truncheon in place of Colours.
Ranks, specialisms and insignia.
Every regiment and corps has its own distinctive insignia, such as cap badge, beret, tactical recognition flash and stable belt.
Throughout the army there are many official specialisms. They do not affect rank, but they do affect pay bands:
Tommy Atkins and other nicknames.
A long established nickname for a British soldier has been "Tommy Atkins" or "Tommy" for short. The origins are obscure but most probably derive from a specimen army form circulated by the Adjutant-General Sir Harry Calvert to all units in 1815 where the blanks had been filled in with the particulars of a Private Thomas Atkins, No 6 Company, 23rd Regiment of Foot. German soldiers in both world wars would usually refer to their British opponents as "Tommys". Present-day British soldiers are often referred to as "Toms" or just "Tom". The British Army magazine "Soldier" has a regular cartoon strip, "Tom", featuring the everyday life of a British soldier. Outside the services, soldiers are generally known as "squaddies" by the British popular press, and the general public.
Another nickname which applies only to soldiers in Scottish regiments is "Jock", derived from the fact that in Scotland the common Christian name John is often changed to Jock in the vernacular.
Welsh soldiers are occasionally referred to as "Taffy" or just "Taff". This may only apply to those from the Taff-Ely Valley in South Wales, where a large portion of men, left unemployed from the decline of the coal industry in the area, enlisted during the First and Second World Wars. Alternatively, it is derived from the supposed Welsh pronunciation of "Dafydd"—the vernacular form of Dave or Davey, the patron Saint of Wales being Saint David. As a nickname for the Welsh it has existed since 1699.
Irish soldiers are referred to as "Paddy" or "Mick".
Junior officers in the army are sometimes known as "Ruperts" by the Other ranks especially for those from a privileged background. This nickname is believed to be derived from the children's comic book character Rupert Bear who epitomises traditional public school values and from the purported preponderance of that particular forename amongst young men from a public school background.

</doc>
<doc id="4888" url="http://en.wikipedia.org/wiki?curid=4888" title="Bruin">
Bruin

Bruin is folk term used for brown bears (from the Dutch "bruin" meaning brown). It is often mistakenly said to mean a pregnant bear. Bruin or Bruins or BRUIN may refer to the following:
Surname.
Bruin and Bruins are Dutch surnames that can be equivalent to Brown (surname) or be patronymic (Bruin/Bruijn was a form of Bruno).

</doc>
<doc id="4890" url="http://en.wikipedia.org/wiki?curid=4890" title="Bayesian probability">
Bayesian probability

Bayesian probability is one of the different interpretations of the concept of probability. The Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, i.e., the propositions whose truth or falsity is uncertain.
Bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies some prior probability, which is then updated in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.
In contrast to interpreting probability as the "frequency" or "propensity" of some phenomenon, Bayesian probability is a quantity that we assign for the purpose of representing a state of knowledge, or a state of belief. In the Bayesian view, a probability is assigned to a hypothesis, whereas under the frequentist view, a hypothesis is typically tested without being assigned a probability.
The term "Bayesian" refers to the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of Bayesian inference. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability.
Broadly speaking, there are two views on Bayesian probability that interpret the "probability" concept in different ways. According to the "objectivist view", the rules of Bayesian statistics can be justified by requirements of rationality and consistency and interpreted as an extension of logic. According to the "subjectivist view", probability quantifies a "personal belief". Many modern machine learning methods are based on objectivist Bayesian principles.
Bayesian methodology.
Bayesian methods are characterized by the following concepts and procedures:
Objective and subjective Bayesian probabilities.
Broadly speaking, there are two views on Bayesian probability that interpret the 'probability' concept in different ways. For objectivists, "probability" objectively measures the plausibility of propositions, i.e. the probability of a proposition corresponds to a reasonable belief everyone (even a "robot") sharing the same knowledge should share in accordance with the rules of Bayesian statistics, which can be justified by requirements of rationality and consistency. For subjectivists, probability corresponds to a 'personal belief'. For subjectivists, rationality and coherence constrain the probabilities a subject may have, but allow for substantial variation within those constraints. The objective and subjective variants of Bayesian probability differ mainly in their interpretation and construction of the prior probability.
History.
The term "Bayesian" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem in a paper titled "An Essay towards solving a Problem in the Doctrine of Chances". In that special case, the prior and posterior distributions were Beta distributions and the data came from Bernoulli trials. It was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called "inverse probability" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, "inverse probability" was largely supplanted by a collection of methods that came to be called frequentist statistics.
In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to "objective" and "subjective" currents in Bayesian practice. In the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. No subjective decisions need to be involved. In contrast, "subjectivist" statisticians deny the possibility of fully objective analysis for the general case.
In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications. Despite the growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics. Nonetheless, Bayesian methods are widely accepted and used, such as in the field of machine learning.
Justification of Bayesian probabilities.
The use of Bayesian probabilities as the basis of Bayesian inference has been supported by several arguments, such as the Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.
Axiomatic approach.
Richard T. Cox showed that Bayesian updating follows from several axioms, including two functional equations and a controversial hypothesis of differentiability. It is known that Cox's 1961 development (mainly copied by Jaynes) is non-rigorous, and in fact a counterexample has been found by Halpern. The assumption of differentiability or even continuity is questionable since the Boolean algebra of statements may only be finite. Other axiomatizations have been suggested by various authors to make the theory more rigorous.
Dutch book approach.
The Dutch book argument was proposed by de Finetti, and is based on betting. A Dutch book is made when a clever gambler places a set of bets that guarantee a profit, no matter what the outcome of the bets. If a bookmaker follows the rules of the Bayesian calculus in the construction of his odds, a Dutch book cannot be made.
However, Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. For example, Hacking writes "And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."
In fact, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "probability kinematics" following the publication of Richard C. Jeffreys' rule, which is itself regarded as Bayesian ). The additional hypotheses sufficient to (uniquely) specify Bayesian updating are substantial, complicated, and unsatisfactory.
Decision theory approach.
A decision-theoretic justification of the use of Bayesian inference (and hence of Bayesian probabilities) was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures. Conversely, every Bayesian procedure is admissible.
Personal probabilities and objective methods for constructing priors.
Following the work on expected utility theory of Ramsey and von Neumann, decision-theorists have accounted for rational behavior using a probability distribution for the agent. Johann Pfanzagl completed the "Theory of Games and Economic Behavior" by providing an axiomatization of subjective probability and utility, a task left uncompleted by von Neumann and Oskar Morgenstern: their original theory supposed that all the agents had the same probability distribution, as a convenience. Pfanzagl's axiomatization was endorsed by Oskar Morgenstern: "Von Neumann and I have anticipated" the question whether probabilities "might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility together with a number for the probabilities (cf. p. 19 of The Theory of Games and Economic Behavior). We did not carry this out; it was demonstrated by Pfanzagl ... with all the necessary rigor".
Ramsey and Savage noted that the individual agent's probability distribution could be objectively studied in experiments. The role of judgment and disagreement in science has been recognized since Aristotle and even more clearly with Francis Bacon. The objectivity of science lies not in the psychology of individual scientists, but in the process of science and especially in statistical methods, as noted by C. S. Peirce. Recall that the objective methods for falsifying propositions about personal probabilities have been used for a half century, as noted previously. Procedures for testing hypotheses about probabilities (using finite samples) are due to Ramsey (1931) and de Finetti (1931, 1937, 1964, 1970). Both Bruno de Finetti and Frank P. Ramsey acknowledge their debts to pragmatic philosophy, particularly (for Ramsey) to Charles S. Peirce.
The "Ramsey test" for evaluating probability distributions is implementable in theory, and has kept experimental psychologists occupied for a half century.
This work demonstrates that Bayesian-probability propositions can be falsified, and so meet an empirical criterion of Charles S. Peirce, whose work inspired Ramsey. (This falsifiability-criterion was popularized by Karl Popper.)
Modern work on the experimental evaluation of personal probabilities uses the randomization, blinding, and Boolean-decision procedures of the Peirce-Jastrow experiment. Since individuals act according to different probability judgments, these agents' probabilities are "personal" (but amenable to objective study).
Personal probabilities are problematic for science and for some applications where decision-makers lack the knowledge or time to specify an informed probability-distribution (on which they are prepared to act). To meet the needs of science and of human limitations, Bayesian statisticians have developed "objective" methods for specifying prior probabilities.
Indeed, some Bayesians have argued the prior state of knowledge defines "the" (unique) prior probability-distribution for "regular" statistical problems; cf. well-posed problems. Finding the right method for constructing such "objective" priors (for appropriate classes of regular problems) has been the quest of statistical theorists from Laplace to John Maynard Keynes, Harold Jeffreys, and Edwin Thompson Jaynes: These theorists and their successors have suggested several methods for constructing "objective" priors:
Each of these methods contributes useful priors for "regular" one-parameter problems, and each prior can handle some challenging statistical models (with "irregularity" or several parameters). Each of these methods has been useful in Bayesian practice. Indeed, methods for constructing "objective" (alternatively, "default" or "ignorance") priors have been developed by avowed subjective (or "personal") Bayesians like James Berger (Duke University) and José-Miguel Bernardo (Universitat de València), simply because such priors are needed for Bayesian practice, particularly in science. The quest for "the universal method for constructing priors" continues to attract statistical theorists.
Thus, the Bayesian statistician needs either to use informed priors (using relevant expertise or previous data) or to choose among the competing methods for constructing "objective" priors.

</doc>
<doc id="4892" url="http://en.wikipedia.org/wiki?curid=4892" title="Bert Bell">
Bert Bell

De Benneville "Bert" Bell (February 25, 1895 – October 11, 1959) was the National Football League (NFL) commissioner from 1945 until his death in 1959. As commissioner, he introduced competitive parity into the NFL to improve the league's commercial viability and promote its popularity, and he helped make the NFL the most financially sound sports enterprise and preeminent sports attraction in the United States (US). He was posthumously inducted into the charter class of the Pro Football Hall of Fame.
Bell played football at the University of Pennsylvania, where as quarterback, he led his team to an appearance in the 1917 Rose Bowl. After being drafted into the US Army during World War I, he returned to complete his collegiate career at Penn and went on to become an assistant football coach with the Quakers in the 1920s. During the Great Depression, he was an assistant coach for the Temple Owls and a co-founder and co-owner of the Philadelphia Eagles.
With the Eagles, Bell led the way in cooperating with the other NFL owners to establish the National Football League Draft in order to afford the weakest teams the first opportunity to sign the best available players. He subsequently became sole proprietor of the Eagles, but the franchise suffered financially. Eventually, he sold the team and bought a share in the Pittsburgh Steelers. During World War II, Bell astutely argued against the league suspending operations until the war's conclusion.
After the war, he was elected NFL commissioner and sold his ownership in the Steelers. As commissioner, he implemented a proactive anti-gambling policy, negotiated a merger with the All-America Football Conference (AAFC), and unilaterally crafted the entire league schedule with an emphasis on enhancing the dramatic effect of late-season matches. During the Golden Age of Television, he tailored the game's rules to strengthen its appeal to mass media and enforced a policy of blacking out local broadcasts of home contests to safeguard ticket receipts. Amid criticism from franchise owners and under pressure from Congress, he unilaterally recognized the NFLPA and facilitated in the development of the first pension plan for the players. He survived to oversee the "Greatest Game Ever Played" and to envision what the league would become in the future.
Early life (1895–1932).
Bell was born de Benneville Bell, on February 25, 1895, in Philadelphia to John C. Bell and Fleurette de Benneville Myers. His father was an attorney who served a term as the Pennsylvania Attorney General. His older brother, John C. Jr., was born in 1892. Bert's parents were very wealthy, and his mother's lineage predated the American Revolutionary War. His father, a Quaker of the University of Pennsylvania (class of 1884) during the early days of American football, accompanied him to his first football game when Bell was six years old. Thereafter, Bell regularly engaged in football games with childhood friends.
In 1904, Bell matriculated at the Episcopal Academy, the Delancey School from 1909 to 1911 and then the Haverford School until 1914. About this time, his father was installed as athletics director at Penn and helped form the NCAA. At Haverford, Bell captained the school's football, basketball, and baseball teams, and "was awarded The Yale Cup [for being] 'The pupil who has done the most to promote athletics in the school.'" Although he excelled at baseball, his devotion was to football. His father, who was named a trustee at Penn in 1911, said of Bell's plans for college, "Bert will go to Penn or he will go to hell."
University of Pennsylvania (1914–1919).
Bell entered Penn in the fall of 1914 as an English major and joined Phi Kappa Sigma. In a rare occurrence for a sophomore, he became the starting quarterback for Penn's coach George H. Brooke. On the team, he also was as a defender, punter, and punt returner. After the team's 3–0 start, Bell temporarily shared possession of his quarterbacking duties until he subsequently reclaimed them later in the season, as Penn finished with a record of 3–5–2.
Prior to Penn's 1916 season, his mother died while he was en route to her bedside. Nevertheless, he started the first game for the Quakers under new coach Bob Folwell, but mixed results left him platooned for the rest of the season. Penn finished with a record of 7–2–1. However, the Quakers secured an invitation to the 1917 Rose Bowl against the Oregon Ducks. Although the best offensive gain for Penn during their 20-14 loss to Oregon was a 20-yard run by Bell, he was replaced late in the game at quarterback after throwing an interception.
In the 1917 season, Bell led Penn to a 9–2–0 record. Afterwards, he registered with a Mobile Hospital Unit of the US Army for World War I and was deployed to France in May 1918. As a result of his unit participating in hazardous duty, it received a congratulatory letter for bravery from General John J. Pershing, and Bell was promoted to first sergeant. After the war, Bell returned to the United States in March 1919. He returned to Penn as captain of the team in the fall and again performed erratically. The Quakers finished 1919 with a 6–2–1 record. Academically, his aversion to attending classes forced him to withdraw from Penn without a degree in early 1920. His collegiate days ended with his having been a borderline All-American, but this period of his life had proven that he "possessed the qualities of a leader."
Early career (1920–1932).
Bell assembled the Stanley Professionals in Chicago in 1920, but he disbanded it prior to playing any games because of negative publicity received by Chicago due the Black Sox Scandal. He joined John Heisman's staff at Penn as an assistant coach in 1920, and Bell would remain there for several years. At Penn, he was well regarded as a football coach, and after its 1924 season, he drew offers for, but declined, head-coaching assignments at other universities. At least as early as 1926, his avocation was socializing and frequenting Saratoga Race Course, where he counted as friends Tim Mara, Art Rooney, and George Preston Marshall. In 1928, Bell tendered his resignation at Penn in protest over the emphasis of in-season scrimmages during practices by Lud Wray, a fellow assistant coach. Bell's resignation was accommodated prior to the start of the 1929 season.
Bell was then an employee of the Ritz-Carlton in Philadelphia. At one point, he tried his hand as a stock broker and lost $50,000 (presently, $) during the Wall Street Crash of 1929. His father bailed him out of his deprivation, and he returned to working at the Ritz. From 1930 until 1932, he was a backfield coach for the Temple Owls football team. In 1932, Marshall tried to coax Bell into buying the rights to a NFL franchise, but Bell disparaged the league and ridiculed the idea. When Pop Warner was hired to coach Temple for the 1933 season, Warner chose to hire his own assistants and Bell was let go.
NFL career.
Philadelphia Eagles (1933–1940).
By early 1933, Bell's opinion on the NFL had changed, and he wanted to become an owner of a team based in Philadelphia. After being advised by the NFL that a prerequisite to a franchise being rendered in Philadelphia was that the Pennsylvania Blue Laws would have to be mollified, he was the "force majeure" in lobbying to getting the laws deprecated. He borrowed funds from Frances Upton, partnered with Wray, and he procured the rights to a franchise in Philadelphia which he christened as the Philadelphia Eagles.
After the inaugural 1933 Philadelphia Eagles season, Bell married Upton at St. Madeleine Sophie Roman Catholic Church in Philadelphia. Days later, his suggestion to bestow the winner of the NFL championship game with the Ed Thorp Memorial Trophy was affirmed. In 1934, the Eagles finished with a 4–7 record, The Eagles' inability to seriously challenge other teams made it difficult to sell tickets, and his failure to sign a talented college prospect led him to adduce that the only way to bring stability to the league was to institute a draft to ensure the weakest teams had an advantage in signing the preeminent players. In 1935, his proposal for a draft was accepted, and in February 1936, the first draft kicked off, at which he acted as Master of Ceremonies. Later that month, his first child, Bert Jr., was born.
In the Eagles' first three years, the partners exhausted $85,000 (presently, $), and at a public auction, Bell became sole owner of the Eagles with a bid of $4,500 (presently, $). Austerity measures forced him to supplant Wray as head coach of the Eagles, wherein Bell led the Eagles to an 1–11 finish, their worst record ever (as of the 2015 season). In December, an application for a franchise in Los Angeles was obstructed by Bell and Pittsburgh Steelers owner Rooney as they deemed it too far of a distance to travel for games. During the Eagles' 2-8-1 1937 season, his second child, John "Upton", was born. In the Eagles' first profitable season, 1938, they posted a 5–6 record. The Eagles finished 1–9–1 in 1939 and 1–10 in 1940.
Pittsburgh Steelers (1940–1945).
In December 1940, Bell conciliated the sale of Rooney's Steelers to Alexis Thompson, and then Rooney acquired half of Bell's interest in the Eagles. In a series of events known as the "Pennsylvania Polka", Rooney and Bell exchanged their entire Eagles roster and their "territorial rights" in Philadelphia to Thompson for his entire Steelers roster and his rights in Pittsburgh. Ostensibly, Rooney had provided assistance to Bell by rewarding him with a 20% commission on the sale of the Steelers. Bell became the Steelers head coach and Rooney became the general manager.
During the training camp of the Pittsburgh's inaugural season with the nickname Steelers, Bell was buoyant with optimism about the team's prospect, but he became crestfallen after Rooney denigrated the squad and flippantly remarked that they looked like the "[s]ame old Steelers" (SOS). After losing the first two games of the 1941 season, Rooney compelled Bell into resigning as head coach. Bell's coaching career ended with a 10–46–2 record, and for coaches with at least five years in the NFL, it is the worst record ever (as of the 2012 season). His first daughter and last child, Jane Upton, was born several months after the season's conclusion.
By 1943, 40% of the NFL rosters had been drafted into the United States Armed Forces for World War II. The resulting difficulty in fielding a full-strength squad led some owners to recommend the league should shut down until the war ended. Bell auspiciously argued against this as he feared they might not be able to resume operations easily after the war, and since Major League Baseball was continuing unabated, then they should also.
Throughout Bell's affiliation with the Steelers, he suffered monetarily and Rooney bought an increasing allotment of the franchise from him. Compounding Bell's problems, Arch Ward organized the All-America Football Conference (AAFC) in 1944 to displace the NFL's sovereignty in professional football. Ward's AAFC promptly began luring players to join the league, which resulted in salaries being driven up drastically. In Bill Dudley's contract proceedings with the Steelers, he attributed Bell's anxiety during the negotiations to the rivalry from the AAFC. Furthermore, by the end of 1945, the Steelers were in their most economically perilous situation in its history.
NFL commissioner (1946–1959).
Election, Hapes-Filchock, and the NFL schedule (1946–1948).
Elmer Layden was appointed the first NFL commissioner in 1941, but Ward appeared as dictating his hiring. Layden tendered his resignation for personal reasons January 1946. Bell, who was not well respected in Pittsburgh, was elected to replace him. He received a three-year contract at $20,000 ($) per year, and transacted a sale of his stake in the Steelers to Rooney, albeit for a price Bell did not construe was full-value. He was then immediately placed at the center of a controversy wherein the owners denied Dan Reeves permission to relocate the Cleveland Rams to Los Angeles. Bell moderated a settlement, and, as a result, the Los Angeles Rams were formed. As a precondition to the Rams leasing the Los Angeles Coliseum, they signed Kenny Washington, which marked the beginning of the end of racial segregation on the field, but also caused "'all hell to break loose'" amidst the owners.
The drawing up of a regular season schedule had been a perennial source of contention among the NFL owners since the league's inception. The crux of the problem was the scheduling of games meant weighing the interest of owners who, early in the season, wanted their franchises to confront teams that drew the largest crowds, versus owners who wanted to play the weaker franchises to pad their team's win-loss record. The resultant impasse coerced the owners, in 1946, to confer upon Bell the sole discretion in developing the league's schedule. He utilized this responsibility to, early in the season, pit the weaker teams against other weak teams, and the strong teams against other strong teams. His goal was to augment game attendances by keeping the difference in team standings to a minimum as deep into the season as possible.
On the eve of the 1946 championship game, Bell was notified that Merle Hapes and Frank Filchock of the New York Giants had been implicated in a bribing scandal. Filchock was sanctioned by Bell to play in the game but Hapes was suspended. At the next NFL owners' meeting, Bell was worried the repercussions from this event would lead to his firing. However he was pleasantly surprised to learn that his contract would be elevated to five years at $30,000 per year. Reinvigorated with renewed support, he persuaded the owners to allow him to put sudden-death overtime into the playoffs.
Subsequently, he wrote an anti-gambling resolution into the league constitution, which empowered him with the ability to permanently ban any NFL associated personnel for betting on a game or for withholding information on a game being possibly fixed. Furthermore, to obstruct gamblers from getting inside information, he secreted the names of officials he would assign to games, and he directed each team to promulgate a precursory injury report which listed anyone who might not participate in a game. Eventually, he lobbied to get every state in the US to criminalize the fixing of sporting events and put employees on the payroll of the NFL to investigate potential betting scams.
AAFC-NFL merger (1948–1950).
The NFL's struggle against the AAFC generated stress on wages, attendance, marketing, and by 1949, it had prevented the NFL for showing a profit for three consecutive years. Bell and representatives from both leagues met to attempt a merger, but their efforts were fruitless. In an unrelated matter, he apprised the owners that attendance records had shown televising games locally had a negative impact on the sale of home tickets. Nevertheless, he actualized the NFL's first television contract—the 1949 championship game. Simultaneously, he dealt with a lawsuit from Bill Radovich, who had been blacklisted for leaving the Lions and gaining employment with the AAFC. Bell and the owners were advised by John C. Jr. that this lawsuit was potentially not winnable, and the ramifications from the outcome of the case weighed heavily on Bell.
One of the primary impediments in an AAFC-NFL merger was the supposed violation of "territorial rights" claimed by Marshall. Eventually, Bell gathered enough support to effectuate a compromise with the AAFC. In late 1949, the leagues merged, and Bell would stay on as commissioner with his contract extended from five to ten years as three AAFC teams (the Cleveland Browns, San Francisco 49ers, and Baltimore Colts) were subsumed. Seeking to capitalize on the publicity of the residual rivalry, he utilized "exquisite dramatic" and business sense and allocated the 1950 opening game to a contest between the 1949 champion Eagles versus the perennial AAFC champion Browns. Feeling financially secure after the merger, he purchased his first home for himself and his family in Narberth, Pennsylvania.
Marketing of the NFL (1950–1956).
In 1950, Bell originated a blackout rule into the NFL which forbid all teams to televise their home games within a 75 mile radius of their stadium - except for the Rams. Consequently, the United States Department of Justice (DOJ) opened an investigation into a violation of the Sherman Antitrust Act. Ensuingly, the Rams attendance for 1950 dropped off by 50%, and this signaled a potential financial disaster. In 1951, he licensed the DuMont Television Network to air the championship games for the next five years, and he stipulated that teams were free to develop their own television contracts independently.
However, preceding 1951 season, he reimposed the blackout rule on all teams in the league. The DOJ filed suit over this and Bell publicly retorted, "You can't give fans a game for free on TV and also expect them to go to the ballpark"; nevertheless, the suit was ordered to trial for January 1952. After the 1951 season ended, he gained unilateral control over the setting of a television strategy for the NFL. He negotiated a deal with DuMont, which granted it the rights to nationally broadcast one regular season game every week, and he directed that the income from this contract was to be shared equally between all the teams. In the DOJ's case, the judge ruled that the blackout policy was legal, but both Bell, and the franchises collectively, were enjoined from negotiating a TV contract; Bell was ecstatic. Later that year, Bell forced one of the owners of the Cleveland Browns to sell all of his shares in the team after Bell determined the owner had bet on Browns' football games. Although he hated to fly, at some indeterminate point, he visited the training camps of every team and lectured on the danger gamblers posed to the league.
Bell authorized a Pro Bowl to be held at the end of each season in order to showcase the talents of the best players. But in the early 1950s, on the field activities sometimes denigrated to borderline "assault and battery" with teams' star players being viciously targeted by opposing players. He answered charges the league was too savage by saying, "'I have never seen a maliciously dirty football player in my life and I don't believe there are any.'" Nevertheless, he ordered broadcasts to follow a strict rule of conduct whereby TV announcers would not be permitted to criticize the game, and neither fights, nor injuries, could be televised by virtue in his belief that announcers were "'salesman for professional football [and] we do not want kids believing that engaging in fights is the way to play football.'"
Bell was criticized for censoring TV broadcasts, a charge he dismissed as not pertinent because he believed he was not impeding the print media but only advertising a product. After CBS and NBC gained the rights to broadcast the games in 1956, he advised the franchises to avoid criticizing the games or the officials, and forewarned that TV would give "'us our greatest opportunity to sell the NFL and everyone must present to the public the greatest games ... combined with the finest sportsmanship.'" This relationship with television was the beginning of the NFL's rise to becoming America's most popular sport.
Compromise with the NFLPA (1956–1957).
In Radovich v. National Football League, the Supreme Court ruled in Radovich's favor and declared the NFL was subject to antitrust laws, and the implication was that the legality of the draft and reserve clause were dubious. Bell pressed a case in the media that the NFL should be exempted from antitrust regulations and proffered the league was a sport and not a business. He invited an investigation from Congress with respect to the court's ruling. The House Judiciary committee, chaired by Emanuel Celler—who believed the draft was illegal and should be abolished, convened in July 1957 to discuss the ramifications of the Radovich decision. Red Grange and Bell testified at the committee's solicitation and argued the draft was essential to the sport's success. Representatives of the NFLPA contradicted these statements and said the draft and the reserve clause were anti-labor, and it seemed as if Congress was going to accept their position. Faced with Congressional opposition, Bell formally recognized the NFLPA and declared he would negotiate with its representatives.
However, Bell was speaking only for himself and without the auspices of the owners. At the next owners' meeting, Rooney admonished they either had to recognize the NFLPA or remove Bell as commissioner. In order to do this, they had to agree in a vote that required a "super-majority". Bell unsuccessfully attempted to persuade the owners to permit the NFLPA to act as a bargaining agent for the players. However, he did reach a compromise with the owners to get them to acquiesce to some of the NFLPA's requests for salary standards and health benefits.
Final days (1958–1959).
For the 1958 season, the durations of timeouts was extended from 60 to 90 seconds and Bell mandated officials call a few "TV timeouts" during each game — a change which triggered criticism from sportswriters. The 1958 championship game became the first NFL championship game decided in overtime and it was considered to be the greatest football game ever played. The game further increased football's marketability to television advertising, and the drama associated with overtime was the catalyst. Years later, after witnessing Bell openly crying after the game, Raymond Berry attributed it to Bell's realization of the impact the game would have on the prevalence of the sport.
The death of Mara in February 1959 unsettled Bell and he experienced a heart attack later that month. He converted to Catholicism in the summer of 1959 because of the lifelong urging of his wife, Mara's death, and his enduring friendship with Rooney, a practicing Catholic. Bell was advised by his doctor to avoid going to football games, to which he quipped, "I'd rather die watching football than in my bed with my boots off." Bell and his children attended an Eagles game at Franklin Field in October 1959. The Eagles held complimentary box seats for him and guests to watch the game, but he preferred to buy his own tickets and sit with the other fans. Sitting behind the end zone during the fourth quarter of the game, he suffered a heart attack and passed away later that day.
Afterwards, he was remembered as "a man of buoyant joviality, with a rough and ready wit, laughter and genuine humility and honesty, clearly innocent of pretense and [pretension]." His funeral was held at Narberth's St. Margaret Roman Catholic Church and Monsignor Cornelius P. Brennan delivered the eulogy, as close friends and admirers attended the mass. Dominic Olejniczak and all the extant owners of the NFL franchises were pallbearers. Bell was interred at Cavalry Cemetery in West Conshohocken, Pennsylvania.
Legacy and honors.
Bell was inducted into the Professional Football Hall of Fame, the Penn Athletics Hall of Fame, the Philadelphia Sports Hall of Fame, and Haverford's Athletic Hall of Fame. The Maxwell Football Club, which he founded in 1937, has presented the best NFL player of the year with the Bert Bell Award since 1959. The Bert Bell Benefit Bowl was exhibited in his honor from 1960 through 1969.
Though his career spanned the desegregation and reintegration of the NFL, as an owner, he never had an African American on any of his teams, but Bert Jr. believed the mere discussion of whether his father was prejudiced was absurd. Bell's handling of the merger with the AAFC was acclaimed as a personal triumph. Although he did not have the wherewithal to prevent the wholesale betting on games, he was proactive in ensuring games were not tampered with by gamblers, and he created the foundation of the contemporary NFL anti-gambling policy.
Bell was criticized as being too strict with his refusal to let sold-out games to be televised locally. Nevertheless, his balancing of television broadcasts against protecting game attendance made the NFL the "healthiest professional sport in America", and he was the "leading protagonist in pro football's evolution into America's major sport." He had understood that the league needed a cooperative television contract with revenue-sharing, but he failed to overcome the obstacles to achieve it. He was portrayed by sportswriters as ensuring the owners treated the players fairly, and his decision to recognize the NFLPA in the face of adversity from owners was a "master stroke" in thwarting Congressional intervention. After he initiated terms for a pension plan with the players in 1959, little progress was made with the NFLPA, however, the first players' pension plan-the Bert Bell National Football League Retirement Plan, was approved in 1962.
Bell's implementation of the draft did not show immediate results, but it was "the single greatest contributor to the [league]'s prosperity" in its first eighty-four years. His original version of the draft was later ruled unconstitutional, but his anchoring of the success of the league to competitive balance has been "hailed by contemporaries and sports historians". Bell had often said, "[o]n any given Sunday, any team in the NFL can beat any other team."

</doc>
