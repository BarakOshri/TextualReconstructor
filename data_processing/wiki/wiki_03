<doc id="844" url="http://en.wikipedia.org/wiki?curid=844" title="Amsterdam">
Amsterdam

Amsterdam (; ) is the capital city of and most populous within the Kingdom of the Netherlands. Its status as the Dutch capital is mandated by the Constitution of the Netherlands though it is not the seat of the Dutch government, which is The Hague. Amsterdam has a population of within the city-proper, in the urban region and in the greater metropolitan area. The city itself, contrary to the bureau of statistics in The Hague, calculates the number of inhabitants in the greater metropolitan area to be 2,332,773. The city is located in the province of North Holland in the west of the country. It comprises much of the northern part of the Randstad, one of the larger conurbations in Europe, with a population of approximately 7 million.
Amsterdam's name derives from "Amstelredamme", indicative of the city's origin as a dam of the river Amstel. Originating as a small fishing village in the late 12th century, Amsterdam became one of the most important ports in the world during the Dutch Golden Age (17th century), a result of its innovative developments in trade. During that time, the city was the leading center for finance and diamonds. In the 19th and 20th centuries, the city expanded, and many new neighborhoods and suburbs were planned and built. The 17th-century canals of Amsterdam and the 19–20th century Defence Line of Amsterdam are on the UNESCO World Heritage List.
As the commercial capital of the Netherlands and one of the top financial centres in Europe, Amsterdam is considered an alpha world city by the Globalization and World Cities (GaWC) study group. The city is also the cultural capital of the Netherlands. Many large Dutch institutions have their headquarters there, and seven of the world's 500 largest companies, including Philips and ING, are based in the city. In 2012, Amsterdam was ranked the second best city in which to live by the Economist Intelligence Unit (EIU) and 12th globally on quality of living by Mercer. The city was previously ranked 3rd in innovation by 2thinknow in the Innovation Cities Index 2009.
The Amsterdam Stock Exchange, the oldest stock exchange in the world, is located in the city center. Amsterdam's main attractions, including its historic canals, the Rijksmuseum, the Van Gogh Museum, Stedelijk Museum, Hermitage Amsterdam, Anne Frank House, Amsterdam Museum, its red-light district, and its many cannabis coffee shops draw more than 3.66 million international visitors annually.
History.
Etymology.
Shortly after the floods of 1170 and 1173 locals of the river Amstel vicinity built a bridge over- and a dam across the river, hence giving its name to the village: "Aemstelredamme". The earliest recorded use of the name "Aemstelredamme" (Amsterdam) comes from a document dated October 27, 1275. Inhabitants of the village, by this document, were exempted from paying a bridge toll in the County of Holland by Count Floris V.
This meant it had been allowed to the inhabitants of the village of Aemstelredamme to travel freely through the County of Holland without having to pay toll at bridges, locks and dams all through the county. The certificate describes the inhabitants as "homines manentes apud Amestelledamme" (people living near "Amestelledamme"). By 1327, the name had developed into "Aemsterdam".
Founding and Middle Ages.
Amsterdam's founding is relatively recent compared with much older Dutch cities such as Nijmegen, Rotterdam, and Utrecht. In October 2008, historical geographer Chris de Bont suggested that the land around Amsterdam was being reclaimed as early as the late 10th century. This does not necessarily mean that there was already a settlement then since reclamation of land may not have been for farming—it may have been for peat, used as fuel.
Amsterdam was granted city rights in either 1300 or 1306. From the 14th century on, Amsterdam flourished, largely because of trade with the Hanseatic League. In 1345, an alleged Eucharistic miracle in the Kalverstraat rendered the city an important place of pilgrimage until the adoption of the Protestant faith. The "Stille Omgang"—a silent procession in civil attire—is today a remnant of the rich pilgrimage history.
Conflict with Spain.
In the 16th century, the Dutch rebelled against Philip II of Spain and his successors. The main reasons for the uprising were the imposition of new taxes, the tenth penny, and the religious persecution of Protestants by the Spanish Inquisition. The revolt escalated into the Eighty Years' War, which ultimately led to Dutch independence. Strongly pushed by Dutch Revolt leader William the Silent, the Dutch Republic became known for its relative religious tolerance. Jews from the Iberian Peninsula, Huguenots from France, prosperous merchants and printers from Flanders, and economic and religious refugees from the Spanish-controlled parts of the Low Countries found safety in Amsterdam. The influx of Flemish printers and the city's intellectual tolerance made Amsterdam a centre for the European free press.
Center of the Dutch Golden Age.
The 17th century is considered Amsterdam's "Golden Age", during which it became the wealthiest city in the world. Ships sailed from Amsterdam to the Baltic Sea, North America, and Africa, as well as present-day Indonesia, India, Sri Lanka, and Brazil, forming the basis of a worldwide trading network. Amsterdam's merchants had the largest share in both the Dutch East India Company and the Dutch West India Company. These companies acquired overseas possessions that later became Dutch colonies. Amsterdam was Europe's most important point for the shipment of goods and was the leading Financial Centre of the world. In 1602, the Amsterdam office of the Dutch East India Company became the world's first stock exchange by trading in its own shares.
Decline and modernization.
Amsterdam's prosperity declined during the 18th and early 19th centuries. The wars of the Dutch Republic with England and France took their toll on Amsterdam. During the Napoleonic Wars, Amsterdam's significance reached its lowest point, with Holland being absorbed into the French Empire. However, the later establishment of the United Kingdom of the Netherlands in 1815 marked a turning point.
The end of the 19th century is sometimes called Amsterdam's second Golden Age. New museums, a train station, and the Concertgebouw were built; in this same time, the Industrial Revolution reached the city. The Amsterdam-Rhine Canal was dug to give Amsterdam a direct connection to the Rhine, and the North Sea Canal was dug to give the port a shorter connection to the North Sea. Both projects dramatically improved commerce with the rest of Europe and the world. In 1906, Joseph Conrad gave a brief description of Amsterdam as seen from the seaside, in "".
Twentieth century.
Shortly before the First World War, the city began expanding, and new suburbs were built. Even though the Netherlands remained neutral in this war, Amsterdam suffered a food shortage, and heating fuel became scarce. The shortages sparked riots in which several people were killed. These riots are known as the "Aardappeloproer" (Potato rebellion). People started looting stores and warehouses in order to get supplies, mainly food.
After landflood in 1916 the depleted municipalities, Durgerdam, Holysloot, Zunderdorp and Schellingwoude, all lying north of Amsterdam, were, on their own request, annexed to the city on 1-1-1921.
Germany invaded the Netherlands on 10 May 1940 and took control of the country. Some Amsterdam citizens sheltered Jews, thereby exposing themselves and their families to the high risk of being imprisoned or sent to concentration camps. More than 100,000 Dutch Jews were deported to Nazi concentration camps of which some 60.000 lived in Amsterdam. Perhaps the most famous deportee was the young Jewish girl Anne Frank, who died in the Bergen-Belsen concentration camp. At the end of the Second World War, communication with the rest of the country broke down, and food and fuel became scarce. Many citizens traveled to the countryside to forage. Dogs, cats, raw sugar beets, and Tulip bulbs—cooked to a pulp—were consumed to stay alive. Most of the trees in Amsterdam were cut down for fuel, and all the wood was taken from the apartments of deported Jews.
Many new suburbs, such as Osdorp, Slotervaart, "Slotermeer", and "Geuzenveld", were built in the years after the Second World War.
These suburbs contained many public parks and wide, open spaces, and the new buildings provided improved housing conditions with larger and brighter rooms, gardens, and balconies. Because of the war and other incidents of the 20th century, almost the entire city centre had fallen into disrepair. As society was changing, politicians and other influential figures made plans to redesign large parts of it. There was an increasing demand for office buildings and new roads as the automobile became available to most common people. A metro started operating in 1977 between the new suburb of Bijlmer and the centre of Amsterdam. Further plans were to build a new highway above the metro to connect the Central Station and city centre with other parts of the city.
The incorporated large-scale demolitions began in Amsterdam's formerly Jewish neighbourhood. Smaller streets, such as the "Jodenbreestraat", were widened and saw almost all of their houses demolished. During the destruction's peak, the "Nieuwmarktrellen" (Nieuwmarkt riots) broke out, where people expressed their fury about the demolition caused by the restructuring of the city.
As a result, the demolition was stopped, and the highway was never built, with only the metro being finished. Only a few streets remained widened. The new city hall was built on the almost completely demolished "Waterlooplein". Meanwhile, large private organisations, such as "Stadsherstel Amsterdam", were founded with the aim of restoring the entire city centre. Although the success of this struggle is visible today, efforts for further restoration are still ongoing. The entire city centre has reattained its former splendor and, as a whole, is now a protected area. Many of its buildings have become monuments, and in July 2010 the "Grachtengordel" (Herengracht, Keizersgracht, and Prinsengracht) was added to the UNESCO World Heritage List.
Twenty-first century.
At the beginning of the new millennium, social problems such as safety, ethnic discrimination and segregation between religious and social groups began to develop. Forty-five percent of the population of Amsterdam has non-Dutch parents. Large social groups come from Suriname, the Dutch Antilles, Morocco and Turkey. Amsterdam is characterized by its (perceived) social tolerance and diversity. The former mayor of Amsterdam, Job Cohen, and his alderman for integration Ahmed Aboutaleb (Now mayor of Rotterdam) formulated a policy of "keeping things together" which involves social dialogue, tolerance and harsh measures against those who break the law.
Geography.
Amsterdam is located in the western Netherlands, in the province of North Holland. The river Amstel terminates in the city centre and connects to a large number of canals that eventually terminate in the IJ. Amsterdam is situated 2 metres below sea level. The surrounding land is flat as it is formed of large polders. A man made forest, Amsterdamse Bos, is situated southwest. Amsterdam is connected to the North Sea through the long North Sea Canal.
Amsterdam is intensely urbanized, as is the Amsterdam metropolitan area surrounding the city. Comprising of land, the city proper has 4,457 inhabitants per km2 and 2,275 houses per km2. Parks and nature reserves make up 12% of Amsterdam's land area.
Venice of the North.
Amsterdam is home to more than one hundred kilometers of canals. The three main canals are Prinsengracht, Herengracht and Keizersgracht, all 3 of which are navigable by boat. In the Middle Ages, Amsterdam was surrounded by a moat, called the Singel, which now forms the innermost ring in the city, and makes the city center a horseshoe shape. The city is also served by a seaport. It is often nicknamed the "Venice of the North," due to its division into approximately 90 islands, which are linked by more than 1,200 bridges.
Climate.
Amsterdam has an oceanic climate (Köppen climate classification "Cfb"), strongly influenced by its proximity to the North Sea to the west, with prevailing westerly winds. Both winters and summers are considered mild, although occasionally quite cool. Amsterdam, as well as most of the North Holland province, lies in USDA Hardiness zone 8b. Frosts mainly occur during spells of easterly or northeasterly winds from the inner European continent. Even then, because Amsterdam is surrounded on three sides by large bodies of water, as well as having a significant heat-island effect, nights rarely fall below , while it could easily be in Hilversum, southeast. Summers are moderately warm but rarely hot. The average daily high in August is , and or higher is only measured on average on 2.5 days, placing Amsterdam in AHS Heat Zone 2. The record extremes range from to .
Days with more than of precipitation are common, on average 133 days per year. Amsterdam's average annual precipitation is , more than what is measured at Amsterdam Schiphol Airport. A large part of this precipitation falls as light rain or brief showers. Cloudy and damp days are common during the cooler months of October through March.
Demographics.
Amsterdam has a population of 810,084 inhabitants within city limits. On 1 January 2012, the ethnic makeup of Amsterdam was 49.5% of Dutch ancestry and 50.5% of foreign origin.
In the 16th and 17th century non-Dutch immigrants to
Amsterdam were mostly Huguenots, Flemings, Sephardi Jews and Westphalians. Huguenots came after the Edict of Fontainebleau in 1685, while the Flemish Protestants came during the Eighty Years' War. The Westphalians came to Amsterdam mostly for economic reasons – their influx continued through the 18th and 19th centuries. Before the Second World War, 10% of the city population was Jewish. Just twenty percent of them survived the Shoah.
The first mass immigration in the 20th century were by people from Indonesia, who came to Amsterdam after the independence of the Dutch East Indies in the 1940s and 1950s. In the 1960s guest workers from Turkey, Morocco, Italy and Spain emigrated to Amsterdam. After the independence of Suriname in 1975, a large wave of Surinamese settled in Amsterdam, mostly in the Bijlmer area. Other immigrants, including refugees asylum seekers and illegal immigrants, came from Europe, America, Asia, and Africa. In the 1970s and 1980s, many 'old' Amsterdammers moved to 'new' cities like Almere and Purmerend, prompted by the third planological bill of the Dutch government. This bill promoted suburbanisation and arranged for new developments in so called "groeikernen", literally "cores of growth". Young professionals and artists moved into neighbourhoods de Pijp and the Jordaan abandoned by these Amsterdammers. The non-Western immigrants settled mostly in the social housing projects in Amsterdam-West and the Bijlmer. Today, people of non-Western origin make up approximately one-third of the population of Amsterdam, and more than 50% of children. A tendence to ethnical segregation is clearly visible, with people of non-Western origins, considered as a separate group by Statistics Netherlands, concentrating in specific neighborhoods especially in Nieuw-West, Zeeburg, Bijlmer and in certain areas of Amsterdam-Noord.
The largest religious group are Christians (17% in 2000), who are divided between Roman Catholics and Protestants. The next largest religion is Islam (14% in 2000), most of whose followers are Sunni.
In 1578 the previously Roman Catholic city of Amsterdam joined the revolt against Spanish rule, late in comparison to other major northern Dutch cities. In line with Protestant procedure of that time, all churches were converted to Protestant worship. Calvinism became the dominant religion, and although Catholicism was not forbidden and priests allowed to serve, the Catholic hierarchy was prohibited. This led to the establishment of "schuilkerken", covert churches, behind seemingly ordinary canal side house fronts. One example is the current debate centre de Rode Hoed.
A large influx of foreigners of many religions came to 17th-century Amsterdam, in particular Sefardic Jews from Spain and Portugal, Huguenots from France, and Protestants from the Southern Netherlands. This led to the establishment of many non-Dutch-speaking religious churches. In 1603, the first notification was made of Jewish religious service. In 1639, the first synagogue was consecrated. The Jews came to call the town Jerusalem of the West, a reference to their sense of belonging there.
As they became established in the city, other Christian denominations used converted Catholic chapels to conduct their own services. The oldest English-language church congregation in the world outside the United Kingdom is found at the Begijnhof. Regular services there are still offered in English under the auspices of the Church of Scotland. The Huguenots accounted for nearly 20% of Amsterdam's inhabitants in 1700. Being Calvinists, they soon integrated into the Dutch Reformed Church, though often retaining their own congregations. Some, commonly referred by the moniker 'Walloon', are recognizable today as they offer occasional services in French.
In the second half of the 17th century, Amsterdam experienced an influx of Ashkenazim, Jews from Central and Eastern Europe, which continued into the 19th century. Jews often fled the pogroms in those areas. The first Ashkenazi who arrived in Amsterdam were refugees from the Chmielnicki Uprising in Poland and the Thirty Years' War. They not only founded their own synagogues, but had a strong influence on the 'Amsterdam dialect' adding a large Yiddish local vocabulary.
Despite an absence of an official Jewish ghetto, most Jews preferred to live in the eastern part of the old medieval heart of the city. The main street of this Jewish neighborhood was the "Jodenbreestraat". The neighborhood comprised the "Waterlooplein" and the Nieuwmarkt. Buildings in this neighborhood fell into disrepair after the Second World War, and a large section of the neighbourhood was demolished during the construction of the subway. This led to riots, and as a result the original plans for large-scale reconstruction were abandoned and the neighborhood was rebuilt with smaller-scale residence buildings on the basis of its original layout.
Catholic churches in Amsterdam have been constructed since the restoration of the episcopal hierarchy in 1853. One of the principal architects behind the city's Catholic churches, Cuypers, was also responsible for the Amsterdam Central Station and the Rijksmuseum, which led to a refusal of Protestant King William III to open 'that monastery'.
In 1924, the Roman Catholic Church of the Netherlands hosted the International Eucharistic Congress in Amsterdam, and numerous Catholic prelates visited the city, where festivities were held in churches and stadiums. Catholic processions on the public streets, however, were still forbidden under law at the time. Only in the 20th century was Amsterdam's relation to Catholicism normalized, but despite its far larger population size, the Catholic clergy chose to place its episcopal see of the city in the nearby provincial town of Haarlem.
In recent times, religious demographics in Amsterdam have been changed by large-scale immigration from former colonies. Immigrants from Suriname have introduced Evangelical Protestantism and Lutheranism, from the Hernhutter variety; Hinduism has been introduced mainly from Suriname; and several distinct branches of Islam have been brought from various parts of the world. Islam is now the largest non-Christian religion in Amsterdam. The large community of Ghanaian and Nigerian immigrants have established African churches, often in parking garages in the Bijlmer area, where many have settled. In addition, a broad array of other religious movements have established congregations, including Buddhism, Confucianism and Hinduism.
Although the saying "Leef en laat leven" or "Live and let live" summarizes the Dutch and especially the Amsterdam open and tolerant society, the increased influx of many races, religions, and cultures after the Second World War, has on a number of occasions strained social relations. With 176 different nationalities, Amsterdam is home to one of the widest varieties of nationalities of any city in the world. The share of the population of immigrant ancestry in the city proper now is about 50%.
The city has been at times marked by ethnic tension. In 2004 film director Theo van Gogh was murdered by an Islamic extremist in Amsterdam. Among others, in line with attitude changes in Dutch politics towards certain (especially Islamic) minorities Turkish-language and Arabic-language TV channels have been dropped from the basic cable TV package. In recent years, politicians are actively discouraged against campaigning in minority languages. In the previous local elections politicians were criticized by current Amsterdam mayor Mr van der Laan (then minister of Integration) for distributing election leaflets in minority languages and in some cases leaflets were collected. Due to this anti-Multicultural stand, van der Laan has been accused of hypocrisy by its own party's PvdA main candidate. Also during the same period, possibly due to his belief in integration via (possibly not always voluntary) assimilation, Amsterdam has been one of the municipalities in the Netherlands which provided immigrants with extensive and free Dutch-language courses, which have benefited many immigrants.
Cityscape and architecture.
Amsterdam fans out south from the Amsterdam Centraal railway station. The oldest area of the town is known as "de Wallen" (the quays). It lies to the east of Damrak and contains the city's famous red light district. To the south of de Wallen is the old Jewish quarter of Waterlooplein. The medieval and colonial age canals of Amsterdam, known as "Grachten", embraces the heart of the city where homes have interesting gables. Beyond the Grachtengordel are the former working class areas of Jordaan and de Pijp. The Museumplein with the city's major museums, the Vondelpark, a 19th-century park named after the Dutch writer Joost van den Vondel, and the Plantage neighbourhood, with the zoo, are also located outside the Grachtengordel.
Several parts of the city and the surrounding urban area are polders. This can be recognised by the suffix "-meer" which means "lake", as in Aalsmeer, Bijlmermeer, Haarlemmermeer, and Watergraafsmeer.
Canals.
The Amsterdam canal system is the result of conscious city planning. In the early 17th century, when immigration was at a peak, a comprehensive plan was developed that was based on four concentric half-circles of canals with their ends emerging at the IJ bay. Known as the "Grachtengordel", three of the canals were mostly for residential development: the "Herengracht" (where "Heren" refers to "Heren Regeerders van de stad Amsterdam" (ruling lords of Amsterdam), and "gracht" means canal, so the name can be roughly translated as "Canal of the lords"), "Keizersgracht" (Emperor's Canal), and "Prinsengracht" (Prince's Canal). The fourth and outermost canal is the "Singelgracht", which is often not mentioned on maps, because it is a collective name for all canals in the outer ring. The Singelgracht should not be confused with the oldest and most inner canal "Singel". The canals served for defence, water management and transport. The defences took the form of a moat and earthen dikes, with gates at transit points, but otherwise no masonry superstructures. The original plans have been lost, so historians, such as Ed Taverne, need to speculate on the original intentions: it is thought that the considerations of the layout were purely practical and defensive rather than ornamental.
Construction started in 1613 and proceeded from west to east, across the breadth of the layout, like a gigantic windshield wiper as the historian Geert Mak calls it – and "not" from the centre outwards, as a popular myth has it. The canal construction in the southern sector was completed by 1656. Subsequently, the construction of residential buildings proceeded slowly. The eastern part of the concentric canal plan, covering the area between the Amstel river and the IJ bay, has never been implemented. In the following centuries, the land was used for parks, senior citizens' homes, theatres, other public facilities, and waterways without much planning.
Over the years, several canals have been filled in, becoming streets or squares, such as the Nieuwezijds Voorburgwal and the Spui.
Expansion.
After the development of Amsterdam's canals in the 17th century, the city did not grow beyond its borders for two centuries. During the 19th century, Samuel Sarphati devised a plan based on the grandeur of Paris and London at that time. The plan envisaged the construction of new houses, public buildings and streets just outside the "grachtengordel". The main aim of the plan, however, was to improve public health. Although the plan did not expand the city, it did produce some of the largest public buildings to date, like the "Paleis voor Volksvlijt".
Following Sarphati, "Van Niftrik" and "Kalff" designed an entire ring of 19th century neighbourhoods surrounding the city’s centre, with the city preserving the ownership of all land outside the 17th century limit, thus firmly controlling development. Most of these neighbourhoods became home to the working class.
In response to overcrowding, two plans were designed at the beginning of the 20th century which were very different from anything Amsterdam had ever seen before: "Plan Zuid", designed by the architect Berlage, and "West". These plans involved the development of new neighbourhoods consisting of "housing blocks" for all social classes.
After the Second World War, large new neighbourhoods were built in the western, southeastern, and northern parts of the city. These new neighbourhoods were built to relieve the city's shortage of living space and give people affordable houses with modern conveniences. The neighbourhoods consisted mainly of large housing blocks situated among green spaces, connected to wide roads, making the neighbourhoods easily accessible by motor car. The western suburbs which were built in that period are collectively called the "Westelijke Tuinsteden". The area to the southeast of the city built during the same period is known as the "Bijlmer".
Architecture.
Amsterdam has a rich architectural history. The oldest building in Amsterdam is the Oude Kerk (Old Church), at the heart of the Wallen, consecrated in 1306. The oldest wooden building is "het Houten Huys" at the Begijnhof.
It was constructed around 1425 and is one of only two existing wooden buildings. It is also one of the few examples of Gothic architecture in Amsterdam.
In the 16th century, wooden buildings were razed and replaced with brick ones. During this period, many buildings were constructed in the architectural style of the Renaissance. Buildings of this period are very recognisable with their stepped gable façades, which is the common Dutch Renaissance style. Amsterdam quickly developed its own Renaissance architecture. These buildings were built according to the principles of the architect Hendrick de Keyser. One of the most striking buildings designed by Hendrick de Keyer is the Westerkerk. In the 17th century baroque architecture became very popular, as it was elsewhere in Europe. This roughly coincided with Amsterdam’s Golden Age. The leading architects of this style in Amsterdam were Jacob van Campen, Philips Vingboons and Daniel Stalpaert.
Philip Vingboons designed splendid merchants' houses throughout the city. A famous building in baroque style in Amsterdam is the Royal Palace on Dam Square. Throughout the 18th century, Amsterdam was heavily influenced by French culture. This is reflected in the architecture of that period. Around 1815, architects broke with the baroque style and started building in different neo-styles. Most Gothic style buildings date from that era and are therefore said to be built in a neo-gothic style. At the end of the 19th century, the Jugendstil or Art Nouveau style became popular and many new buildings were constructed in this architectural style. Since Amsterdam expanded rapidly during this period, new buildings adjacent to the city centre were also built in this style. The houses in the vicinity of the Museum Square in Amsterdam Oud-Zuid are an example of Jugendstil. The last style that was popular in Amsterdam before the modern era was Art Deco. Amsterdam had its own version of the style, which was called the Amsterdamse School. Whole districts were built this style, such as the "Rivierenbuurt". A notable feature of the façades of buildings designed in Amsterdamse School is that they are highly decorated and ornate, with oddly shaped windows and doors.
The old city centre is the focal point of all the architectural styles before the end of the 19th century.
Jugendstil and Georgian are mostly found outside the city’s centre in the neighbourhoods built in the early
20th century, although there are also some striking examples of these styles in the city centre.
Most historic buildings in the city centre and nearby are houses, such as the famous merchants' houses lining the canals.
Parks and recreational areas.
Amsterdam has many parks, open spaces, and squares throughout the city. Vondelpark, the largest park in the city, is located in the Oud-Zuid borough and is named after the 17th century Amsterdam author, Joost van den Vondel. Yearly, the park has around 10 million visitors. In the park is an open air theatre, a playground and several horeca facilities. In the Zuid borough, is Beatrixpark, named after Queen Beatrix. Between Amsterdam and Amstelveen is the Amsterdamse Bos (Amsterdam Forest), the largest recreational area in Amsterdam. Annually, almost 4.5 million people visit the park, which has a size of 1,000 hectares and is approximately three times the size of Central Park. Amstelpark in the Zuid borough houses the Rieker windmill, which dates to 1636. Other parks include Sarphatipark in the De Pijp neighborhood, Oosterpark in the Oost borough, and Westerpark in the Westerpark neighborhood. The city has four beaches, the Nemo Beach, Citybeach "Het stenen hoofd" (Silodam), Blijburg, and one in Amsterdam-Noord.
The city has many open squares (plein in Dutch). The namesake of the city as the site of the original dam, Dam Square, is the main town square and has the Royal Palace and National Monument. Museumplein hosts various museums, including the Rijksmuseum, Van Gogh Museum, and Stedelijk Museum. Other squares include Rembrandtplein, Muntplein, Nieuwmarkt, Leidseplein, Spui, and Waterlooplein.
Economy.
Amsterdam is the financial and business capital of the Netherlands.
Amsterdam is currently one of the best European cities in which to locate an international business. It is ranked fifth in this category and is only surpassed by London, Paris, Frankfurt and Barcelona. Many large corporations and banks have their headquarters in Amsterdam, including Akzo Nobel, Heineken International, ING Group, TomTom, Delta Lloyd Group and Philips. KPMG International's global headquarters is located in nearby Amstelveen, where many non-Dutch companies have settled as well, because surrounding communities allow full land ownership, contrary to Amsterdam's land-lease system.
Though many small offices are still located on the old canals, companies are increasingly relocating outside the city centre. The Zuidas (English: South Axis) has become the new financial and legal hub. The five largest law firms of the Netherlands, a number of Dutch subsidiaries of large consulting firms like Boston Consulting Group and Accenture, and the World Trade Center Amsterdam are also located in Zuidas.
There are three other smaller financial districts in Amsterdam. The first is the area surrounding Amsterdam Sloterdijk railway station, where several newspapers like De Telegraaf have their offices. Also, Deloitte, the municipal public transport company ("Gemeentelijk Vervoersbedrijf") and the Dutch tax offices ("Belastingdienst") are located there. The second Financial District is the area surrounding Amsterdam Arena. The third is the area surrounding Amsterdam Amstel railway station. The tallest building in Amsterdam, the Rembrandt Tower, is situated there, as is the headquarters of Philips.
The Amsterdam Stock Exchange (AEX), now part of Euronext, is the world's oldest stock exchange and is one of Europe's largest bourses. It is situated near Dam Square in the city's centre.
Together with Eindhoven (Brainport) and Rotterdam (Seaport), Amsterdam (Airport) forms the foundation of the Dutch economy.
Tourism.
Amsterdam is one of the most popular tourist destinations in Europe, receiving more than 4.63 million international visitors annually, this is excluding the 16 million day trippers visiting the city every year. The number of visitors has been growing steadily over the past decade. This can be attributed to an increasing number of European visitors. Two thirds of the hotels are located in the city's centre. Hotels with 4 or 5 stars contribute 42% of the total beds available and 41% of the overnight stays in Amsterdam. The room occupation rate was 78% in 2006, up from 70% in 2005. The majority of tourists (74%) originate from Europe. The largest group of non-European visitors come from the United States, accounting for 14% of the total. Certain years have a theme in Amsterdam to attract extra tourists. For example, the year 2006 was designated "Rembrandt 400", to celebrate the 400th birthday of Rembrandt van Rijn. Some hotels offer special arrangements or activities during these years. The average number of guests per year staying at the four campsites around the city range from 12,000 to 65,000.
Red light district.
De Wallen, also known as Walletjes or Rosse Buurt, is a designated area for legalised prostitution and is Amsterdam's largest and most well known red-light district. This neighborhood has become a famous attraction for tourists. It consists of a network of roads and alleys containing several hundred small, one-room apartments rented by sex workers who offer their services from behind a window or glass door, typically illuminated with red lights.
Retail.
Shops in Amsterdam range from large department stores such as De Bijenkorf founded in 1870 and Maison de Bonneterie a Parisian style store founded in 1889, to small specialty shops. Amsterdam's high-end shops are found in the streets "Pieter Cornelisz Hooftstraat" and "Cornelis Schuytstraat", which are located in the vicinity of the Vondelpark. One of Amsterdam's busiest high streets is the narrow, medieval "Kalverstraat" in the heart of the city. Other shopping areas include the "Negen Straatjes" and Haarlemmerdijk and Haarlemmerstraat. "Negen Straatjes" are nine narrow streets within the "Grachtengordel", the concentric canal system of Amsterdam. The Negen Straatjes differ from other shopping districts with the presence of a large diversity of privately owned shops. The Haarlemmerstraat and Haarlemmerdijk were voted best shopping street in the Netherlands in 2011. These streets have as the "Negen Straatjes" a large diversity of privately owned shops. But as the "Negen Straatjes" are dominated by fashion stores the Haarlemmerstraat and Haarlemmerdijk offer a very wide variety of all kinds of stores, just to name some specialties: candy and other food related stores, lingerie, sneakers, wedding clothing, interior shops, books, Italian deli's, racing and mountain bikes, skatewear, etc.
The city also features a large number of open-air markets such as the "Albert Cuypmarkt", "Westerstraat-markt", "Ten Katemarkt", and "Dappermarkt". Some of these markets are held on a daily basis, like the Albert Cuypmarkt and the Dappermarkt. Others, like the Westerstraatmarkt, are held on a weekly basis.
Fashion.
Fashion brands like G-star, Gsus, BlueBlood, Iris van Herpen, 10 feet and Warmenhoven & Venderbos, and fashion designers like Mart Visser, Viktor & Rolf, Sheila de Vries, Marlies Dekkers and Frans Molenaar are based in Amsterdam. Modelling agencies Elite Models, Touche models and Tony Jones have opened branches in Amsterdam. Fashion models like Yfke Sturm, Doutzen Kroes and Kim Noorda started their careers in Amsterdam. Amsterdam has its garment centre in the World Fashion Center. Buildings which formerly housed brothels in the red light district have been converted to ateliers for young fashion designers, AKA eagle fuel. Fashion photographers Inez van Lamsweerde and Vinoodh Matadin were born in Amsterdam.
Culture.
During the later part of the 16th century Amsterdam's Rederijkerskamer (Chamber of Rhetoric) organized contests between different Chambers in the reading of poetry and drama. In 1638, Amsterdam opened its first theatre. Ballet performances were given in this theatre as early as 1642. In the 18th century, French theatre became popular. While Amsterdam was under the influence of German music in the 19th century there were few national opera productions; the Hollandse Opera of Amsterdam was built in 1888 for the specific purpose of promoting Dutch opera. In the 19th century, popular culture was centred on the Nes area in Amsterdam (mainly vaudeville and music-hall). The metronome, one of the most important advances in European classical music, was invented here in 1812 by Dietrich Nikolaus Winkel. At the end of this century, the Rijksmuseum and were built. In 1888, the Concertgebouworkest was established. With the 20th century came cinema, radio and television. Though most studios are located in Hilversum and Aalsmeer, Amsterdam's influence on programming is very strong. Many people who work in the television industry live in Amsterdam. Also, the headquarters of the Dutch SBS Broadcasting Group is located in Amsterdam.
Museums.
The most important museums of Amsterdam are located on the Museumplein ("Museum Square"), located at the southwestern side of the Rijksmuseum. It was created in the last quarter of the 19th century on the grounds of the former World's fair. The northeastern part of the square is bordered by the very large Rijksmuseum. In front of the Rijksmuseum on the square itself is a long, rectangular pond. This is transformed into an ice rink in winter. The northwestern part of the square is bordered by the Van Gogh Museum, Stedelijk Museum, House of Bols Cocktail & Genever Experience and Coster Diamonds. The southwestern border of the Museum Square is the Van Baerlestraat, which is a major thoroughfare in this part of Amsterdam. The Concertgebouw is situated across this street from the square. To the southeast of the square are situated a number of large houses, one of which contains the American consulate. A parking garage can be found underneath the square, as well as a supermarket. "Het Museumplein" is covered almost entirely with a lawn, except for the northeastern part of the square which is covered with gravel. The current appearance of the square was realized in 1999, when the square was remodeled. The square itself is the most prominent site in Amsterdam for festivals and outdoor concerts, especially in the summer. Plans were made in 2008 to remodel the square again, because many inhabitants of Amsterdam are not happy with its current appearance.
The Rijksmuseum possesses the largest and most important collection of classical Dutch art.
It opened in 1885. Its collection consists of nearly one million objects. The artist most associated with Amsterdam is Rembrandt, whose work, and the work of his pupils, is displayed in the Rijksmuseum. Rembrandt's masterpiece De Nachtwacht ("The Night Watch") is one of top pieces of art of the museum. It also houses paintings from artists like Van der Helst, Vermeer, Frans Hals, Ferdinand Bol, Albert Cuyp, Jacob van Ruisdael and Paulus Potter. Aside from paintings, the collection consists of a large variety of decorative art. This ranges from Delftware to giant dollhouses from the 17th century. The architect of the gothic revival building was P.J.H. Cuypers. The museum underwent a 10 year, 375 million euro renovation starting in 2003. The full collection was reopened to the public on April 13, 2013 and the Rijksmuseum has established itself as the most visited museum in Amsterdam with 2.2 million visitors in 2013.
Van Gogh lived in Amsterdam for a short while and there is a museum dedicated to his work. The museum is housed in one of the few modern buildings in this area of Amsterdam. The building was designed by Gerrit Rietveld. This building is where the permanent collection is displayed. A new building was added to the museum in 1999. This building, known as the performance wing, was designed by Japanese architect Kisho Kurokawa. Its purpose is to house temporary exhibitions of the museum. Some of Van Gogh's most famous paintings, like the Aardappeleters ("The Potato Eaters") and Zonnebloemen ("Sunflowers"), are present in the collection. The Van Gogh museum is the second most visited museum in Amsterdam, with 1.4 million annual visitors.
Next to the Van Gogh museum stands the Stedelijk Museum. This is Amsterdam's most important museum of modern art . The museum is as old as the square it borders and was opened in 1895. The permanent collection consists of works of art from artists like Piet Mondriaan, Karel Appel, and Kazimir Malevich. After renovations lasting several years the museum opened in September 2012 with a new composite extension that has been called "The Bathtub" due to its resemblance to one.
Amsterdam contains many other museums throughout the city. They range from small museums such as the Verzetsmuseum ("Resistance Museum"), the Anne Frank Huis ("Anne Frank House"), and the Rembrandthuis ("Rembrandt House"), to the very large, like the Tropenmuseum ("Museum of the Tropics"), Amsterdam Museum (formerly known as "Amsterdams Historisch Museum", "Amsterdam Historical Museum"), Hermitage Amsterdam (a dependency of the Hermitage Museum of Saint Petersburg) and the Joods Historisch Museum ("Jewish Historical Museum"). The modern-styled NEMO (museum) is dedicated to child-friendly science exhibitions.
Music.
Amsterdam's musical culture includes a large collection of songs which treat the city nostalgically and lovingly. The 1949 song "Aan de Amsterdamse grachten" ("On the canals of Amsterdam") was performed and recorded by many artists, including John Kraaijkamp sr.; the best-known version is probably that by Wim Sonneveld (1962). In the 1950s Johnny Jordaan rose to fame with "Geef mij maar Amsterdam" ("I prefer Amsterdam"), which praises the city above all others (explicitly Paris); Jordaan sang especially about his own neighborhood, the Jordaan ("Bij ons in de Jordaan"). Colleagues and contemporaries of Johnny include Tante Leen, Zwarte Riek, and Manke Nelis. Other notable Amsterdam songs are "Amsterdam" by Jacques Brel (1964) and "Deze Stad" by De Dijk (1989). A 2011 poll by Amsterdam paper "Het Parool" found, somewhat surprisingly, that Trio Bier's "Oude Wolf" was voted "Amsterdams lijflied". Notable Amsterdam bands from the modern era include the Osdorp Posse and The Ex.
The Heineken Music Hall is a concert hall located near the Amsterdam ArenA. Its main purpose is to serve as a podium for pop concerts for big audiences. Many famous international artists have performed there. Two other notable venues, Paradiso and the Melkweg are located near the Leidseplein. Both focus on broad programming, ranging from indie rock to hip hop, R&B, and other popular genres. Other more subculturally focused music venues are OCCII, OT301, De Nieuwe Anita, Winston Kingdom and Zaal 100. Jazz has a strong following in Amsterdam, with the Bimhuis being the premier venue. In 2012, Ziggo Dome was opened, also near Amsterdam ArenA, a state of the art indoor music arena.
The Heineken Music Hall is also host to many electronic dance music festivals, alongside many other venues. Armin van Buuren and Tiesto, some of the world's leading Trance DJ's hail from the Netherlands and perform frequently in Amsterdam. Each year in October, the city hosts the Amsterdam Dance Event (ADE) which is one of the leading electronic music conferences and one of the biggest club festivals for electronic music in the world. Another popular dance festival is 5daysoff, which takes place in the venues Paradiso and Melkweg. In summer time there are several big outdoor dance parties in or nearby Amsterdam, such as Awakenings, Dance Valley, Mystery Land, Loveland, A Day at the Park, Welcome to the Future, and Valtifest.
Amsterdam has a world-class symphony orchestra, the Royal Concertgebouw Orchestra. Their home is the Concertgebouw, which is across the Van Baerlestraat from the Museum Square. It is considered by critics to be a concert hall with some of the best acoustics in the world. The building contains three halls, Grote Zaal, Kleine Zaal, and Spiegelzaal. Some nine hundred concerts and other events per year take place in the Concertgebouw, for a public of over 700,000, making it one of the most-visited concert halls in the world. The opera house of Amsterdam is situated adjacent to the city hall. Therefore, the two buildings combined are often called the Stopera, (a word originally coined by protesters against it very construction: "Stop the Opera[-house]"). This huge modern complex, opened in 1986, lies in the former Jewish neighbourhood at "Waterlooplein" next to the river Amstel. The "Stopera" is the homebase of Dutch National Opera, Dutch National Ballet and the Holland Symfonia. Muziekgebouw aan 't IJ is a concert hall, which is situated in the IJ near the central station. Its concerts perform mostly modern classical music. Located adjacent to it, is the "Bimhuis", a concert hall for improvised and Jazz music.
Performing arts.
Amsterdam has three main theatre buildings.
The Stadsschouwburg Amsterdam at the Leidseplein is the home base of Toneelgroep Amsterdam. The current building dates from 1894. Most plays are performed in the Grote Zaal (Great Hall). The normal program of events encompasses all sorts of theatrical forms. The Stadsschouwburg is currently being renovated and expanded. The third theater space, to be operated jointly with next door Melkweg, will open in late 2009 or early 2010.
Dutch National Opera & Ballet (formerly known as "Het Muziektheater"), dating from 1986, is the principal opera house and home to Dutch National Opera and Dutch National Ballet.
Royal Theatre Carré was built as a permanent circus theatre in 1887 and is currently mainly used for musicals, cabaret performances and pop concerts.
The recently re-opened DeLaMar Theater houses the more commercial plays and musicals.
The Netherlands has a tradition of cabaret or "kleinkunst", which combines music, storytelling, commentary, theatre and comedy. Cabaret dates back to the 1930s and artists like Wim Kan, Wim Sonneveld and Toon Hermans were pioneers of this form of art in the Netherlands. In Amsterdam is the Kleinkunstacademie (English: Cabaret Academy). Contemporary popular artists are Youp van 't Hek, Freek de Jonge, Herman Finkers, Hans Teeuwen, Theo Maassen, Herman van Veen, Najib Amhali, Raoul Heertje, Jörgen Raymann, Brigitte Kaandorp and Comedytrain. The English spoken comedy scene was established with the founding of Boom Chicago in 1993. They have their own theatre at Leidseplein.
Nightlife.
Amsterdam is famous for its vibrant and diverse nightlife. Amsterdam has many "cafés" (bars). They range from large and modern to small and cozy. The typical "Bruine Kroeg" (brown "café") breathe a more old fashioned atmosphere with dimmed lights, candles, and somewhat older clientele. Most "cafés" have terraces in summertime. A common sight on the Leidseplein during summer is a square full of terraces packed with people drinking beer or wine. Many restaurants can be found in Amsterdam as well. Since Amsterdam is a multicultural city, a lot of different ethnic restaurants can be found. Restaurants range from being rather luxurious and expensive to being ordinary and affordable. Amsterdam also possesses many discothèques. The two main nightlife areas for tourists are the Leidseplein and the Rembrandtplein. The Paradiso, Melkweg and Sugar Factory are cultural centres, which turn into discothèques on some nights. Examples of discothèques near the Rembrandtplein are the Escape and Club Home. Also noteworthy are Panama, Hotel Arena (East), The Sand and The Powerzone. Bimhuis located near the Central Station, with its rich programming hosting the best in the field is considered one of the best jazz clubs in the world. The Reguliersdwarsstraat is the main street for the LGBT community and nightlife.
Festivals.
In 2008, there were 140 festivals and events in Amsterdam.
Famous festivals and events in Amsterdam include: Koninginnedag (In 2013 Koningsdag since the crowning of king Willem-Alexander) (Queen's Day - King's Day); the Holland Festival for the performing arts; the yearly Prinsengrachtconcert (classical concerto on the Prinsen canal) in August; the 'Stille Omgang' (a silent Roman Catholic evening procession held every March); Amsterdam Gay Pride; The Cannabis Cup; and the Uitmarkt. On Koninginnedag—that was held each year on 30 April—hundreds of thousands of people travel to Amsterdam to celebrate with the city's residents and Koningsdag is held on the 27th of April. The entire city becomes overcrowded with people buying products from the "freemarket," or visiting one of the many music concerts.
The yearly Holland Festival attracts international artists and visitors from all over Europe. Amsterdam Gay Pride is a yearly local LGBT parade of boats in Amsterdam's canals, held on the first Saturday in August. The Gay Pride event is a frequent source of both criticism and praise. The annual Uitmarkt is a three-day cultural event at the start of the cultural season in late August. It offers previews of many different artists, such as musicians and poets, who perform on podia.
Sports.
Amsterdam is home of the "Eredivisie" football club Ajax Amsterdam. The stadium Amsterdam ArenA is the home of Ajax. It is located in the south-east of the city next to the new Amsterdam Bijlmer ArenA railway station. Before moving to their current location in 1996, Ajax played their regular matches in De Meer Stadion.
In 1928, Amsterdam hosted the Summer Olympics. The Olympic Stadium built for the occasion has been completely restored and is now used for cultural and sporting events, such as the Amsterdam Marathon. In 1920, Amsterdam assisted in hosting some of the sailing events for the Summer Olympics held in neighboring Antwerp, Belgium by hosting events at Buiten Y.
The city holds the Dam to Dam Run, a 10-mile race from Amsterdam to Zaandam, as well as the Amsterdam Marathon.
The ice hockey team Amstel Tijgers play in the Jaap Eden ice rink. The team competes in the Dutch ice hockey premier league. Speed skating championships have been held on the 400-metre lane of this ice rink.
Amsterdam holds two American Football franchises: the Amsterdam Crusaders and the Amsterdam Panthers.
The Amsterdam Pirates baseball team competes in the Dutch Major League. There are three field hockey teams: Amsterdam, Pinoké and Hurley, who play their matches around the Wagener Stadium in the nearby city of Amstelveen. The basketball team MyGuide Amsterdam competes in the Dutch premier division and play their games in the Sporthallen Zuid.
There is one rugbyclub in Amsterdam, which also hosts sports training classes such as RTC(Rugby Talenten Centrum or Rugby Talent Centre) and the National Rugby stadium.
Since 1999 the city of Amsterdam honours the best sportsmen and women at the Amsterdam Sports Awards. Boxer Raymond Joval and field hockey midfielder Carole Thate were the first to receive the awards, in 1999.
Government.
 The city of Amsterdam is a municipality under the Dutch Municipalities Act. It is governed by a directly elected municipal council, a municipal executive board and a mayor. Since 1981, the municipality of Amsterdam has gradually been divided into semi-autonomous boroughs, called "stadsdelen" or 'districts'. Over time, a total of 15 boroughs were created. In May 2010, under a major reform, the number of Amsterdam boroughs was reduced to eight: Amsterdam-Centrum covering the city centre including the canal belt, Amsterdam-Noord consisting of the neighborhoods north of the IJ lake, Amsterdam-Oost in the east, Amsterdam-Zuid in the south, Amsterdam-West in the west, Amsterdam Nieuw-West in the far west, Amsterdam Zuidoost in the southeast, and Westpoort covering the Port of Amsterdam area.
City government.
As with all Dutch municipalities, Amsterdam is governed by a directly elected municipal council, a municipal executive board and a mayor ("burgemeester"). The mayor is a member of the municipal executive board, but also has individual responsibilies in maintaining public order. In July 2010, Eberhard van der Laan (Labour Party) was appointed mayor of Amsterdam by the national government for a six-year term after being nominated by the Amsterdam municipal council. After the 2014 municipal council elections, a governing majority of D66, VVD and SP was formed - the first coalition without the Labour Party since World War II. Next to the mayor, the municipal executive board consists of eight "wethouders" ('alderpersons') appointed by the municipal council: four D66 alderpersons, two VVD alderpersons and two SP alderpersons.
Unlike most other Dutch municipalities, Amsterdam is subdivided into eight boroughs, called "stadsdelen" or 'districts', a system that was implemented gradually in the 1980s to improve local governance. The boroughs are responsible for many activities that had previously been run by the central city. In 2010, the number of Amsterdam boroughs reached fifteen. Fourteen of those had their own district council ("deelraad"), elected by a popular vote. The fifteenth, Westpoort, covers the harbour of Amsterdam and had very few residents. Therefore, it was governed by the central municipal council. Under the borough system, municipal decisions are made at borough level, except for those affairs pertaining to the whole city such as major infrastructure projects, which are the jurisdiction of the central municipal authorities. In 2010, the borough system was restructured, in which many smaller boroughs merged into larger boroughs. In 2014, under a reform of the Dutch Municipalities Act, the Amsterdam boroughs lost much of their autonomous status, as their district councils were abolished. The municipal council of Amsterdam voted to maintain the borough system by replacing the district councils with smaller, but still directly elected district committees ("bestuurscommissies"). Under a municipal ordinance, the new district committees were granted responsibilities through delegation of regulatory and executive powers by the central municipal council.
Metropolitan area.
 "Amsterdam" is usually understood to refer to the municipality of Amsterdam. Colloquially, some areas within the municipality, such as the town of Durgerdam, may not be considered part of Amsterdam.
Statistics Netherlands uses three other definitions of Amsterdam: metropolitan agglomeration Amsterdam ("Grootstedelijke Agglomeratie Amsterdam", not to be confused with "Grootstedelijk Gebied Amsterdam", a synonym of "Groot Amsterdam"), Greater Amsterdam ("Groot Amsterdam", a COROP region) and the urban region Amsterdam ("Stadsgewest Amsterdam"). The Amsterdam Department for Research and Statistics uses a fourth conurbation, namely the "Stadsregio Amsterdam" ('City Region of Amsterdam'). The city region is similar to Greater Amsterdam but includes the municipalities of Zaanstad and Wormerland. It excludes Graft-De Rijp.
The smallest of these areas is the municipality of Amsterdam with a population of 802,938 in 2013. The metropolitan agglomeration had a population of 1,096,042 in 2013. It includes the municipalities of Zaanstad, Wormerland, Oostzaan, Diemen and Amstelveen only, as well as the municipality of Amsterdam. Greater Amsterdam includes 15 municipalities, and had a population of 1,293,208 in 2013. Though much larger in area, the population of this area is only slightly larger, because the definition excludes the relatively populous municipality of Zaanstad. The largest area by population, the Amsterdam Metropolitan Area (Dutch: Metropoolregio Amsterdam), has a population of 2,33 million. It includes for instance Zaanstad, Wormerveer, Muiden, Abcoude, Haarlem, Almere and Lelystad but excludes Graft-De Rijp. Amsterdam is part of the conglomerate metropolitan area Randstad, with a total population of 6,659,300 inhabitants.
Of these various metropolitan area configurations, only the "Stadsregio Amsterdam" (City Region of Amsterdam) has a formal governmental status. Its responsibities include regional spatial planning and the metropolitan public transport concessions.
National capital.
Under the Dutch Constitution, Amsterdam is the capital of the Netherlands. Since the 1983 constitutional revision, the constitution mentions "Amsterdam" and "capital" in chapter 2, article 32: The king's confirmation by oath and his coronation take place in "the capital Amsterdam" ("de hoofdstad Amsterdam"). Previous versions of the constitution only mentioned "the city of Amsterdam" ("de stad Amsterdam"). For a royal investiture, therefore, the States General of the Netherlands (the Dutch Parliament) meets for a ceremonial joint session in Amsterdam. The ceremony traditionally takes place at the Nieuwe Kerk on Dam Square, immediately after the former monarch has signed the act of abdication at the nearby Royal Palace of Amsterdam. Normally, however, the Parliament sits in The Hague, the city which has historically been the seat of the Dutch government, the Dutch monarchy, and the Dutch supreme court. Foreign embassies are also located in The Hague.
Symbols.
The coat of arms of Amsterdam is composed of several historical elements. First and centre are three St Andrew's crosses, aligned in a vertical band on the city's shield (although Amsterdam's patron saint was Saint Nicholas). These St Andrew's crosses can also be found on the cityshields of neighbours Amstelveen and Ouder-Amstel. This part of the coat of arms is the basis of the flag of Amsterdam, flown by the city government, but also as civil ensign for ships registered in Amsterdam. Second is the Imperial Crown of Austria. In 1489, out of gratitude for services and loans, Maximilian I awarded Amsterdam the right to adorn its coat of arms with the king's crown. Then, in 1508, this was replaced with Maximilian's imperial crown when he was crowned Holy Roman Emperor. In the early years of the 17th century, Maximilian's crown in Amsterdam's coat of arms was again replaced, this time with the crown of Emperor Rudolph II, a crown that became the Imperial Crown of Austria. The lions date from the late 16th century, when city and province became part of the Republic of the Seven United Netherlands. Last came the city's official motto: "Heldhaftig, Vastberaden, Barmhartig" ("Heroic, Determined, Merciful"), bestowed on the city in 1947 by Queen Wilhelmina, in recognition of the city's bravery during the Second World War.
Transport.
Metro, tram, bus.
Currently, there are sixteen tram routes and four metro routes, with a fifth route to be added when the North/South metro line is completed (due in 2017). All are operated by municipal public transport operator GVB, which also runs the city bus network.
Three, fare-free GVB ferries carry pedestrians and cyclists across the IJ lake to the borough of Amsterdam-Noord, and two fare-charging ferries run east and west along the harbour. There are also privately operated water taxis, a water bus, a boat sharing operation, electric rental boats (Boaty) and canal cruises, that transport people along Amsterdam's waterways.
Regional buses, and some suburban buses, are operated by Connexxion and EBS. International coach services are provided by Eurolines from Amsterdam Amstel railway station, IDBUS from Amsterdam Sloterdijk railway station, and Megabus from "Zuiderzeeweg" in the east of the city.
Car.
Amsterdam was intended in 1932 to be the hub, a kind of Kilometre Zero, of the highway system of the Netherlands, with freeways numbered One to Eight planned to originate from the city. The outbreak of the Second World War and shifting priorities led to the current situation, where only roads A1, A2, and A4 originate from Amsterdam according to the original plan. The A3 road to Rotterdam was cancelled in 1970 in order to conserve the Groene Hart. Road A8, leading north to Zaandam and the A10 Ringroad were opened between 1968 and 1974. Besides the A1, A2, A4 and A8, several freeways, such as the A7 and A6, carry traffic mainly bound for Amsterdam.
The A10 ringroad surrounding the city connects Amsterdam with the Dutch national network of freeways. Interchanges on the A10 allow cars to enter the city by transferring to one of the 18 "city roads", numbered S101 through to S118. These city roads are regional roads without grade separation, and sometimes without a central reservation. Most are accessible by cyclists. The S100 "Centrumring" is a smaller ringroad circumnavigating the city's centre.
In the city centre, driving a car is discouraged. Parking fees are expensive, and many streets are closed to cars or are one-way. The local government sponsors carsharing and carpooling initiatives such as "Autodelen" and "Meerijden.nu".
National rail.
Amsterdam is served by ten stations of the Nederlandse Spoorwegen (Dutch Railways). Six are intercity stops: Sloterdijk, Zuid, Amstel, Bijlmer ArenA, Lelylaan and Amsterdam Centraal. The stations for local services are: RAI, Holendrecht, Muiderpoort and Science Park. Amsterdam Centraal is also an international railway station. From the station there are regular services to destinations such as Austria, Belarus, Belgium, the Czech Republic, Denmark, France, Germany, Hungary, Poland, Russia and Switzerland. Among these trains are international trains of the Nederlandse Spoorwegen (Amsterdam-Berlin) and the Thalys (Amsterdam-Brussels-Paris/Lille), CityNightLine, and InterCityExpress (Amsterdam-Cologne-Frankfurt).
Airport.
Amsterdam Airport Schiphol is less than 20 minutes by train from Amsterdam Centraal railway station and is also served by domestic and international intercity trains, such as Thalys and Intercity Brussel. Schiphol is the largest airport in the Netherlands, the fourth largest in Europe, and the fourteenth largest in the world in terms of passengers. It handles about 50 million passengers per year and is the home base of four airlines, KLM, transavia.com, Martinair and Arkefly. As of 2013, Schiphol was the sixth busiest airport in the world measured by international passenger numbers.
Cycling.
Amsterdam is one of the most bicycle-friendly large cities in the world and is a centre of bicycle culture with good facilities for cyclists such as bike paths and bike racks, and several guarded bike storage garages ("fietsenstalling") which can be used for a nominal fee. In 2013, there were about 1,200,000 bicycles in Amsterdam outnumbering the amount of citizens in the city. Theft is widespread – in 2011, about 83,000 bicycles were stolen in Amsterdam. Bicycles are used by all socio-economic groups because of their convenience, Amsterdam's small size, the of bike paths, the flat terrain, and the arguable inconvenience of driving an automobile.
Education.
Amsterdam has two universities: the University of Amsterdam (Universiteit van Amsterdam), and the VU University Amsterdam (Vrije Universiteit or "VU"). Other institutions for higher education include an art school – Gerrit Rietveld Academie, a university of applied sciences - the Hogeschool van Amsterdam, and the Amsterdamse Hogeschool voor de Kunsten. Amsterdam's International Institute of Social History is one of the world's largest documentary and research institutions concerning social history, and especially the history of the labour movement. Amsterdam's Hortus Botanicus, founded in the early 17th century, is one of the oldest botanical gardens in the world, with many old and rare specimens, among them the coffee plant that served as the parent for the entire coffee culture in Central and South America.
Some of Amsterdam's primary schools base their teachings on particular pedagogic theories like the various Montessori schools. The biggest Montessori High School in Amsterdam is the Montessori Lyceum Amsterdam. Many schools, however, are based on religion. This used to be primarily Roman Catholicism and various Protestant denominations, but with the influx of Muslim immigrants there has been a rise in the number of Islamic schools. Jewish schools can be found in the southern suburbs of Amsterdam.
Amsterdam is noted for having five independent grammar schools (Dutch: gymnasia), the Vossius Gymnasium, Barlaeus Gymnasium, St. Ignatius Gymnasium, Het 4e Gymnasium and the Cygnus Gymnasium where a classical curriculum including Latin and classical Greek is taught. Though believed until recently by many to be an anachronistic and elitist concept that would soon die out, the gymnasia have recently experienced a revival, leading to the formation of a fourth and fifth grammar school in which the three aforementioned schools participate. Most secondary schools in Amsterdam offer a variety of different levels of education in the same school. The city also has various colleges ranging from art and design to politics and economics which are mostly also available for students coming from other countries.
Media.
Amsterdam is a prominent center for national and international media. Some locally based newspapers include Het Parool, a national daily paper; De Telegraaf, the largest Dutch daily newspaper; the daily newspapers Trouw, De Volkskrant and NRC Handelsblad; De Groene Amsterdammer, a weekly newspaper; the free newspapers Sp!ts, Metro, and The Holland Times (printed in English).
Amsterdam is home to the Netherlands' second-largest commercial TV group SBS Broadcasting Group, consisting of TV-stations SBS 6, Net 5 and Veronica. However, Amsterdam is not being considered 'the media city of the Netherlands'. The town of Hilversum, 30 km (19 mi) south-east of Amsterdam, has been crowned with this unofficial title. Hilversum is the principal center for radio and television broadcasting in the Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based there. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS, as well as to the studios and offices of all the Dutch public broadcasting organizations and many commercial TV production companies.
Amsterdam is also featured in John Green's book 'The Fault in Our Stars,' which has also been made in a film, and part of the film takes place in Amsterdam.
Housing.
The housing market is heavily regulated. In Amsterdam, 55% of existing housing and 30% of new housing is owned by Housing Associations, which are Government sponsored entities.
Squat properties are common throughout Amsterdam, due to property law strongly favouring tenants. A number of these squats have become well known, such as OT301, Paradiso, Vrankrijk (closed down by city government), and the Binnenpret, and several are now businesses, such as health clubs and licensed restaurants.
International relations.
Amsterdam is twinned with the following cities:

</doc>
<doc id="846" url="http://en.wikipedia.org/wiki?curid=846" title="Museum of Work">
Museum of Work

The Museum of Work, or "Arbetets museum", is a museum located in Norrköping, Sweden. The museum can be found in the 19th century building "The Iron" in the Motala ström river in central Norrköping.

</doc>
<doc id="848" url="http://en.wikipedia.org/wiki?curid=848" title="Audi">
Audi

Audi AG () is a German automobile manufacturer that designs, engineers, produces, markets and distributes automobiles. Audi oversees worldwide operations from its headquarters in Ingolstadt, Bavaria, Germany. Audi-branded vehicles are produced in nine production facilities worldwide.
Audi has been a majority owned (99.55%) subsidiary of Volkswagen Group since 1966, following a phased purchase of Audi AG's predecessor, Auto Union, from Daimler-Benz. Volkswagen relaunched the Audi brand with the 1965 introduction of the Audi F103 series.
The company name is based on the Latin translation of the surname of the founder, August Horch. "Horch", meaning "listen" in German, becomes "audi" in Latin. The four rings of the Audi logo each represent one of four car companies that banded together to create Audi's predecessor company, Auto Union. Audi's slogan is Vorsprung durch Technik, meaning "Advancement through Technology". Recently in the United States, Audi has updated the slogan to "Truth in Engineering". Audi is a member of the "German Big 3" luxury automakers, along with BMW and Mercedes-Benz, which are the three best-selling luxury automakers in the world.
History.
Birth of the company and its name.
Originally in 1885, automobile company Wanderer was established, later becoming a branch of Audi AG. Another company, NSU, which also later merged into Audi, was founded during this time, and later supplied the chassis for Gottlieb Daimler's four-wheeler.
On 14 November 1899, August Horch (1868–1951) established the company A. Horch & Cie. in the Ehrenfeld district of Cologne. Three years later in 1902 he moved with his company to Reichenbach im Vogtland. On May, 10th, 1904 he founded the August Horch & Cie. Motorwagenwerke AG, a joint-stock company in Zwickau (State of Saxony).
After troubles with Horch chief financial officer, August Horch left Motorwagenwerke and founded in Zwickau on 16 July 1909, his second company, the August Horch Automobilwerke GmbH. His former partners sued him for trademark infringement. The German Reichsgericht (Supreme Court) in Leipzig, eventually determined that the Horch brand belonged to his former company.
Since August Horch was banned from using "Horch" as a trade name in his new car business, he called a meeting with close business friends, Paul and Franz Fikentscher from Zwickau, Germany. At the apartment of Franz Fikentscher, they discussed how to come up with a new name for the company. During this meeting, Franz's son was quietly studying Latin in a corner of the room. Several times he looked like he was on the verge of saying something but would just swallow his words and continue working, until he finally blurted out, "Father – "audiatur et altera pars"... wouldn't it be a good idea to call it "audi" instead of "horch"?" "Horch!" in German means "Hark!" or "hear", which is "Audi" in the singular imperative form of "audire" – "to listen" – in Latin. The idea was enthusiastically accepted by everyone attending the meeting. On 25 April 1910 the Audi Automobilwerke GmbH Zwickau (from 1915 on Audiwerke AG Zwickau) was entered in the company's register of Zwickau registration court.
The first Audi automobile, the Audi Type A 10/ Sport-Phaeton, was produced in the same year, followed by the successor Type B 10/28PS in the same year.
Audi started with a 2,612 cc inline-four engine model Type A, followed by a 3,564 cc model, as well as 4,680 cc and 5,720 cc models. These cars were successful even in sporting events. The first six-cylinder model Type M, 4,655 cc appeared in 1924.
August Horch left the "Audiwerke" in 1920 for a high position at the ministry of transport, but he was still involved with Audi as a member of the board of trustees. In September 1921, Audi became the first German car manufacturer to present a production car, the Audi Type K, with left-handed drive. Left-hand drive spread and established dominance during the 1920s because it provided a better view of oncoming traffic, making overtaking safer.
The merger of the four companies under the logo of four rings.
In August 1928, Jørgen Rasmussen, the owner of Dampf-Kraft-Wagen (DKW), acquired the majority of shares in Audiwerke AG. In the same year, Rasmussen bought the remains of the U.S. automobile manufacturer Rickenbacker, including the manufacturing equipment for eight-cylinder engines. These engines were used in "Audi Zwickau" and "Audi Dresden" models that were launched in 1929. At the same time, six-cylinder and four-cylinder (the "four" with a Peugeot engine) models were manufactured. Audi cars of that era were luxurious cars equipped with special bodywork.
In 1932, Audi merged with Horch, DKW, and Wanderer, to form Auto Union AG, Chemnitz. It was during this period that the company offered the Audi Front that became the first European car to combine a six-cylinder engine with front-wheel drive. It used a powertrain shared with the Wanderer, but turned 180-degrees, so that the drive shaft faced the front.
Before World War II, Auto Union used the four interlinked rings that make up the Audi badge today, representing these four brands. This badge was used, however, only on Auto Union racing cars in that period while the member companies used their own names and emblems. The technological development became more and more concentrated and some Audi models were propelled by Horch or Wanderer built engines.
Reflecting the economic pressures of the time, Auto Union concentrated increasingly on smaller cars through the 1930s, so that by 1938 the company's DKW brand accounted for 17.9% of the German car market, while Audi held only 0.1%. After the final few Audis were delivered in 1939 the "Audi" name disappeared completely from the new car market for more than two decades.
Post-World War II.
Like most German manufacturing, at the onset of World War II the Auto Union plants were retooled for military production, and were a target for allied bombing during the war which left them damaged.
Overrun by the Soviet Army in 1945, on the orders of the Soviet Union military administration the factories were dismantled as part of war reparations. Following this, the company's entire assets were expropriated without compensation. On 17 August 1948, Auto Union AG of Chemnitz was deleted from the commercial register. These actions had the effect of liquidating Germany's Auto Union AG. The remains of the Audi plant of Zwickau became the VEB (for "People Owned Enterprise") or AWZ (in English: Automobile Works Zwickau).
The former Audi factory in Zwickau restarted assembly of the pre-war-models in 1949. These DKW models were renamed to IFA F8 and IFA F9 and were similar to the West German versions. West and East German models were equipped with the traditional and renowned DKW two-stroke engines. The Zwickau plant manufactured the infamous Trabant until 1991, when it came under Volkswagen control—effectively bringing it under the same umbrella as Audi since 1945.
New Auto Union unit.
A new West German headquartered Auto Union was launched in Ingolstadt, Bavaria with loans from the Bavarian state government and Marshall Plan aid. The reformed company was launched 3 September 1949 and continued DKW's tradition of producing front-wheel drive vehicles with two-stroke engines. This included production of a small but sturdy 125 cc motorcycle and a DKW delivery van, the DKW F 89 L at Ingolstadt. The Ingolstadt site was large, consisting of an extensive complex of formerly military buildings which was suitable for administration as well as vehicle warehousing and distribution, but at this stage there was at Ingolstadt no dedicated plant suitable for mass production of automobiles: for manufacturing the company's first post-war mass-market passenger car plant capacity in Düsseldorf was rented from Rheinmetall-Borsig. It was only ten years later, after the company had attracted an investor that funds became available for construction of major car plant at the Ingolstadt head office site.
In 1958, in response to pressure from Friedrich Flick, then their largest single shareholder, Daimler-Benz took an 87% holding in the Auto Union company, and this was increased to a 100% holding in 1959. However, small two-stroke cars were not the focus of Daimler-Benz's interests, and while the early 1960s saw major investment in new Mercedes models and in a state of the art factory for Auto Union's, the company's aging model range at this time did not benefit from the economic boom of the early 1960s to the same extent as competitor manufacturers such as Volkswagen and Opel. The decision to dispose of the Auto Union business was based on its lack of profitability. Ironically, by the time they sold the business, it also included a large new factory and near production-ready modern four-stroke engine, which would enable the Auto Union business, under a new owner, to embark on a period of profitable growth, now producing not Auto Unions or DKWs, but using the "Audi" name, resurrected in 1965 after a 25 year gap. Under the terms of the sale, Daimler-Benz retained the old Düsseldorf plant, which survives to the present day as a centre for Mercedes-Benz commercial vehicle assembly.
In 1964, Volkswagen acquired a 50% holding in the business, which included the new factory in Ingolstadt and the trademark rights of the Auto Union. Eighteen months later, Volkswagen bought complete control of Ingolstadt, and by 1966 were using the spare capacity of the Ingolstadt plant to assemble an additional 60,000 Volkswagen Beetles per year. Two-stroke engines became less popular during the 1960s as customers were more attracted to the smoother four-stroke engines. In September 1965, the DKW F102 was fitted with a four-stroke engine and a facelift for the car's front and rear. Volkswagen dumped the DKW brand because of its associations with two-stroke technology, and having classified the model internally as the F103, sold it simply as the "Audi." Later developments of the model were named after their horsepower ratings and sold as the Audi 60, 75, 80, and Super 90, selling until 1972. Initially, Volkswagen was hostile to the idea of Auto Union as a standalone entity producing its own models having acquired the company merely to boost its own production capacity through the Ingolstadt assembly plant. Then VW chief Heinz Nordhoff explicitly forbade Auto Union from any further product development. Fearing that the company's heritage would disappear underneath VW badge engineering, Auto Union engineers under the leadership of Ludwig Kraus developed the first Audi 100 in secret, without Nordhoff's knowledge. When presented with a finished prototype, Nordhoff was so impressed he authorised the car for production, which when launched in 1968, went on to be a huge success. With this, the resurrection of the Audi brand was now complete, this being followed by the first generation Audi 80 in 1972, which would in turn provide a template for VW's new front-wheel-drive water-cooled range which debuted from the mid-1970s onward.
In 1969, Auto Union merged with NSU, based in Neckarsulm, near Stuttgart. In the 1950s, NSU had been the world's largest manufacturer of motorcycles, but had moved on to produce small cars like the NSU Prinz, the TT and TTS versions of which are still popular as vintage race cars. NSU then focused on new rotary engines based on the ideas of Felix Wankel. In 1967, the new NSU Ro 80 was a car well ahead of its time in technical details such as aerodynamics, light weight, and safety. However, teething problems with the rotary engines put an end to the independence of NSU. The Neckarsulm plant is now used to produce the larger Audi models A6 and A8. The Neckarsulm factory is also home of the quattro GmbH, a subsidiary responsible for development and production of Audi high-performance models: the R8 and the "RS" model range.
The mid-sized car that NSU had been working on, the K70, was intended to slot between the rear-engined Prinz models and the futuristic NSU Ro 80. However, Volkswagen took the K70 for its own range, spelling the end of NSU as a separate brand.
Modern era.
The new merged company was known as Audi NSU Auto Union AG, and saw the emergence of Audi as a separate brand for the first time since the pre-war era. Volkswagen introduced the Audi brand to the United States for the 1970 model year.
The first new car of this regime was the Audi 100 of 1968. This was soon joined by the Audi 80/Fox (which formed the basis for the 1973 Volkswagen Passat) in 1972 and the Audi 50 (later rebadged as the Volkswagen Polo) in 1974. The Audi 50 was a seminal design because it was the first incarnation of the Golf/Polo concept, one that led to a hugely successful world car.
The Audi image at this time was a conservative one, and so, a proposal from chassis engineer Jörg Bensinger was accepted to develop the four-wheel drive technology in Volkswagen's Iltis military vehicle for an Audi performance car and rally racing car. The performance car, introduced in 1980, was named the "Audi Quattro", a turbocharged coupé which was also the first German large-scale production vehicle to feature permanent all-wheel drive through a centre differential. Commonly referred to as the "Ur-Quattro" (the "Ur-" prefix is a German augmentative used, in this case, to mean "original" and is also applied to the first generation of Audi's S4 and S6 Sport Saloons, as in "UrS4" and "UrS6"), few of these vehicles were produced (all hand-built by a single team), but the model was a great success in rallying. Prominent wins proved the viability of all-wheel drive racecars, and the Audi name became associated with advances in automotive technology.
In 1985, with the Auto Union and NSU brands effectively dead, the company's official name was now shortened to simply Audi AG.
In 1986, as the Passat-based Audi 80 was beginning to develop a kind of "grandfather's car" image, the "type 89" was introduced. This completely new development sold extremely well. However, its modern and dynamic exterior belied the low performance of its base engine, and its base package was quite spartan (even the passenger-side mirror was an option.) In 1987, Audi put forward a new and very elegant Audi 90, which had a much superior set of standard features. In the early 1990s, sales began to slump for the Audi 80 series, and some basic construction problems started to surface.
In the early part of the 21st century, Audi set forth on a German racetrack to claim and maintain several world records, such as top speed endurance. This effort was in-line with the company's heritage from the 1930s racing era Silver Arrows.
Through the early 1990s, Audi began to shift its target market upscale to compete against German automakers Mercedes-Benz and BMW. This began with the release of the Audi V8 in 1990. It was essentially a new engine fitted to the Audi 100/200, but with noticeable bodywork differences. Most obvious was the new grille that was now incorporated in the bonnet.
By 1991, Audi had the four-cylinder Audi 80, the 5-cylinder Audi 90 and Audi 100, the turbocharged Audi 200 and the Audi V8. There was also a coupe version of the 80/90 with both 4- and 5-cylinder engines.
Although the five-cylinder engine was a successful and robust powerplant, it was still a little too different for the target market. With the introduction of an all-new Audi 100 in 1992, Audi introduced a 2.8L V6 engine. This engine was also fitted to a face-lifted Audi 80 (all 80 and 90 models were now badged 80 except for the USA), giving this model a choice of four-, five-, and six-cylinder engines, in Saloon, Coupé and Cabriolet body styles.
The five-cylinder was soon dropped as a major engine choice; however, a turbocharged version remained. The engine, initially fitted to the 200 quattro 20V of 1991, was a derivative of the engine fitted to the Sport Quattro. It was fitted to the Audi Coupé, and named the S2 and also to the Audi 100 body, and named the S4. These two models were the beginning of the mass-produced S series of performance cars.
Audi 5000 unintended acceleration allegations.
Sales in the United States fell after a series of recalls from 1982 to 1987 of Audi 5000 models associated with reported incidents of sudden unintended acceleration linked to six deaths and 700 accidents. At the time, NHTSA was investigating 50 car models from 20 manufacturers for sudden surges of power.
A "60 Minutes" report aired 23 November 1986, featuring interviews with six people who had sued Audi after reporting unintended acceleration, showing an Audi 5000 ostensibly suffering a problem when the brake pedal was pushed. Subsequent investigation revealed that "60 Minutes" had engineered the failure – fitting a canister of compressed air on the passenger-side floor, linked via a hose to a hole drilled into the transmission.
Audi contended, prior to findings by outside investigators, that the problems were caused by driver error, specifically pedal misapplication. Subsequently, the National Highway Traffic Safety Administration (NHTSA) concluded that the majority of unintended acceleration cases, including all the ones that prompted the "60 Minutes" report, were caused by driver error such as confusion of pedals. CBS did not acknowledge the test results of involved government agencies, but did acknowledge the similar results of another study.
In a review study published in 2012, NHTSA summarized its past findings about the Audi unintended acceleration problems: "Once an unintended acceleration had begun, in the Audi 5000, due to a failure in the idle-stabilizer system (producing an initial acceleration of 0.3g), pedal misapplication resulting from panic, confusion, or unfamiliarity with the Audi 5000 contributed to the severity of the incident."
This summary is consistent with the conclusions of NHTSA's most technical analysis at the time: "Audi idle-stabilization systems were prone to defects which resulted in excessive idle speeds and brief unanticipated accelerations of up to 0.3g [which is similar in magnitude to an emergency stop in a subway car]. These accelerations could not be the sole cause of [(long-duration) sudden acceleration incidents (SAI)], but might have triggered some SAIs by startling the driver. The defective idle-stabilization system performed a type of electronic throttle control. Significantly: multiple "intermittent malfunctions of the electronic control unit were observed and recorded ... and [were also observed and] reported by Transport Canada."
With a series of recall campaigns, Audi made several modifications; the first adjusted the distance between the brake and accelerator pedal on automatic-transmission models. Later repairs, of 250,000 cars dating back to 1978, added a device requiring the driver to press the brake pedal before shifting out of park. A legacy of the Audi 5000 and other reported cases of sudden unintended acceleration are intricate gear stick patterns and brake interlock mechanisms to prevent inadvertent shifting into forward or reverse. It is unclear how the defects in the idle-stabilization system were addressed.
Audi's U.S. sales, which had reached 74,061 in 1985, dropped to 12,283 in 1991 and remained level for three years. – with resale values falling dramatically. Audi subsequently offered increased warranty protection and renamed the affected models – with the "5000" becoming the "100" and "200" in 1989 – and only reached the same sales levels again by model year 2000.
A 2010 "BusinessWeek" article – outlining possible parallels between Audi's experience and 2009–2010 Toyota vehicle recalls – noted a class-action lawsuit filed in 1987 by about 7,500 Audi 5000-model owners remains unsettled and is currently being contested in county court in Chicago after appeals at the Illinois state and U.S. federal levels.
Model introductions.
In the mid-to-late 1990s, Audi introduced new technologies including the use of aluminum construction. Produced from 1999 to 2005, the Audi A2 was a futuristic super mini, born from the Al2 concept, with many features that helped regain consumer confidence, like the aluminium space frame, which was a first in production car design. In the A2 Audi further expanded their TDI technology through the use of frugal three-cylinder engines. The A2 was extremely aerodynamic and was designed around a wind tunnel. The Audi A2 was criticised for its high price and was never really a sales success but it planted Audi as a cutting-edge manufacturer. The model, a Mercedes-Benz A-Class competitor, sold relatively well in Europe. However, the A2 was discontinued in 2005 and Audi decided not to develop an immediate replacement.
The next major model change came in 1995 when the Audi A4 replaced the Audi 80. The new nomenclature scheme was applied to the Audi 100 to become the Audi A6 (with a minor facelift). This also meant the S4 became the S6 and a new S4 was introduced in the A4 body. The S2 was discontinued. The Audi Cabriolet continued on (based on the Audi 80 platform) until 1999, gaining the engine upgrades along the way. A new A3 hatchback model (sharing the Volkswagen Golf Mk4's platform) was introduced to the range in 1996, and the radical Audi TT coupé and roadster were debuted in 1998 based on the same underpinnings.
The engines available throughout the range were now a 1.4 L, 1.6 L and 1.8 L four-cylinder, 1.8 L four-cylinder turbo, 2.6 L and 2.8 L V6, 2.2 L turbo-charged five-cylinder and the 4.2 L V8 engine. The V6s were replaced by new 2.4 L and 2.8 L 30V V6s in 1998, with marked improvement in power, torque and smoothness. Further engines were added along the way, including a 3.7 L V8 and 6.0 L W12 engine for the A8.
Audi AG today.
Audi's sales grew strongly in the 2000s, with deliveries to customers increasing from 653,000 in 2000 to 1,003,000 in 2008. The largest sales increases came from Eastern Europe (+19.3%), Africa (+17.2%) and the Middle East (+58.5%). China in particular has become a key market, representing 108,000 out of 705,000 cars delivered in the first three quarters of 2009. One factor for its popularity in China is that Audis have become the car of choice for purchase by the Chinese government for officials, and purchases by the government are responsible for 20% of its sales in China. As of late 2009, Audi's operating profit of €1.17-billion ($1.85-billion) made it the biggest contributor to parent Volkswagen Group's nine-month operating profit of €1.5-billion, while the other marques in Group such as Bentley and SEAT had suffered considerable losses. May 2011 saw record sales for Audi of America with the new Audi A7 and Audi A3 TDI Clean Diesel. In May 2012, Audi reported a 10% increase in its sales—from 408 units to 480 in the last year alone.
Audi manufactures vehicles in seven plants around the world, some of which are shared with other VW Group marques although many sub-assemblies such as engines and transmissions are manufactured within other Volkswagen Group plants.
Audi's two principal assembly plants are:
Outside of Germany, Audi produces vehicles at:
In September 2012, Audi announced the construction of its first North American manufacturing plant in Puebla, Mexico. This plant is expected to be operative in 2016 and produce the second generation Q5.
From 2002 up to 2003, Audi headed the Audi Brand Group, a subdivision of the Volkswagen Group's Automotive Division consisting of Audi, Lamborghini and SEAT, that was focused on sporty values, with the marques' product vehicles and performance being under the higher responsibility of the Audi brand.
In 2014 Audi UK falsely claimed that the Audi A7, A8, and R8 were Euro NCAP safety tested, all achieving five out of five stars. In fact none were tested.
Technology.
Bodyshells.
Audi produces 100% galvanised cars to prevent corrosion, and was the first mass-market vehicle to do so, following introduction of the process by Porsche, c. 1975. Along with other precautionary measures, the full-body zinc coating has proved to be very effective in preventing rust. The body's resulting durability even surpassed Audi's own expectations, causing the manufacturer to extend its original 10-year warranty against corrosion perforation to currently 12 years (except for aluminium bodies which do not rust).
Space frame.
Audi introduced a new series of vehicles in the mid-1990s and continues to pursue new technology and high performance. An all-aluminium car was brought forward by Audi, and in 1994 the Audi A8 was launched, which introduced aluminium space frame technology (called "Audi Space Frame" or ASF) which saves weight and improves torsion rigidity compared to a conventional steel frame. Prior to that effort, Audi used examples of the Type 44 chassis fabricated out of aluminium as test-beds for the technique. The disadvantage of the aluminium frame is that it is very expensive to repair and requires a specialized aluminium bodyshop. The weight reduction is somewhat offset by the quattro four-wheel drive system which is standard in most markets. Nonetheless, the A8 is usually the lightest all-wheel drive car in the full-size luxury segment, also having best-in-class fuel economy. The Audi A2, Audi TT and Audi R8 also use Audi Space Frame designs.
Drivetrains.
Layout.
For most of its lineup (excluding the A3, A1, and TT models), Audi has not adopted the transverse engine layout which is typically found in economy cars (such as Peugeot and Citroën), since that would limit the type and power of engines that can be installed. In order to be able to mount powerful engines (such as a V8 engine in the Audi S4 and Audi RS4, as well as the W12 engine in the Audi A8L W12), Audi has usually engineered its more expensive cars with a longitudinally front-mounted engine, in an "overhung" position, over the front wheels in front of the axle line. While this allows for the easy adoption of all-wheel drive, it goes against the ideal 50:50 weight distribution.
In all its post Volkswagen-era models, Audi has firmly refused to adopt the traditional rear-wheel drive layout favored by its two arch rivals Mercedes-Benz and BMW, favoring either front-wheel drive or all-wheel drive. The majority of Audi's lineup in the United States features all-wheel drive standard on most of its expensive vehicles (only the entry-level trims of the A4 and A6 are available with front-wheel drive), in contrast to Mercedes-Benz and BMW whose lineup treats all-wheel drive as an option. BMW did not offer all-wheel drive on its V8-powered cars (as opposed to crossover SUVs) until the 2010 BMW 7 Series and 2011 BMW 5 Series, while the Audi A8 has had all-wheel drive available/standard since the 1990s. Regarding high-performance variants, Audi S and RS models have always had all-wheel drive, unlike their direct rivals from BMW M and Mercedes-AMG whose cars are rear-wheel drive only (although their performance crossover SUVs are all-wheel drive).
Audi has recently applied the "quattro" badge to models such as the A3 and TT which do not use the Torsen-based system as in prior years with a mechanical center differential, but with the Haldex Traction electro-mechanical clutch AWD system.
Engines.
In the 1980s, Audi, along with Volvo, was the champion of the inline-five cylinder, 2.1/2.2 L engine as a longer-lasting alternative to more traditional six-cylinder engines. This engine was used not only in production cars but also in their race cars. The 2.1 L inline five-cylinder engine was used as a base for the rally cars in the 1980s, providing well over after modification. Before 1990, there were engines produced with a displacement between 2.0 L and 2.3 L. This range of engine capacity allowed for both fuel economy and power.
For the ultra-luxury version of its Audi A8 fullsize luxury flagship sedan, the Audi A8L W12, Audi uses the Volkswagen Group W12 engine instead of the conventional V12 engine favored by rivals Mercedes-Benz and BMW. The W12 engine configuration (also known as a "WR12") is created by forming two imaginary narrow-angle 15° VR6 engines at an angle of 72°, and the narrow angle of each set of cylinders allows just two overhead camshafts to drive each pair of banks, so just four are needed in total. The advantage of the W12 engine is its compact packaging, allowing Audi to build a 12-cylinder sedan with all-wheel drive, whereas a conventional V12 engine could only have a rear-wheel drive configuration as it would have no space in the engine bay for a differential and other components required to power the front wheels. In fact, the 6.0 L W12 in the Audi A8L W12 is smaller in overall dimensions than the 4.2 L V8 that powers the Audi A8 4.2 variants. The 2011 Audi A8 debuted a revised 6.3-litre version of the W12 (WR12) engine with .
Fuel Stratified Injection.
New models of the A3, A4, A6 and A8 have been introduced, with the ageing 1.8-litre engine now having been replaced by new Fuel Stratified Injection (FSI) engines. Nearly every petroleum burning model in the range now incorporates this fuel-saving technology.
Direct-Shift Gearbox.
At the turn of the century, Volkswagen introduced the Direct-Shift Gearbox (DSG), a type of dual clutch transmission. It is an automated semi-automatic transmission, drivable like a conventional automatic transmission. Based on the gearbox found in the Group B S1, the system includes dual electrohydraulically controlled clutches instead of a torque converter. This is implemented in some VW Golfs, Audi A3, Audi A4 and TT models where DSG is called S-tronic.
LED daytime running lights.
Beginning in 2006, Audi has implemented white LED technology as daytime running lights (DRL) in their products. The distinctive shape of the DRLs has become a trademark of sorts. LEDs were first introduced on the Audi A8 W12, the world's first production car to have LED DRLs, and have since spread throughout the entire model range. The LEDs are present on some Audi billboards.
Since 2010, Audi has also offered the LED technology in low- and high-beam headlights.
Multi Media Interface.
Audi has recently started offering a computerised control system for its cars, called Multi Media Interface (MMI). This came amid criticism of BMW's iDrive control. It is essentially a rotating control knob and 'segment' buttons – designed to control all in-car entertainment devices (radio, CD changer, iPod, TV tuner), satellite navigation, heating and ventilation, and other car controls with a screen. MMI was widely reported to be a considerable improvement on BMW's iDrive, although BMW has since made their iDrive more user-friendly.
MMI has been generally well-received, as it requires less menu-surfing with its segment buttons around a central knob, along with 'main function' direct access buttons – with shortcuts to the radio or phone functions. The screen, either colour or monochrome, is mounted on the upright dashboard, and on the A4 (new), A5, A6, A8, and Q7, the controls are mounted horizontally.
An "MMI-like" system is also available on the A3, TT, A4 (B7), and R8 models – when equipped with the Audi Navigation System Plus (RNS-E) satellite navigation system.
Models.
Current model range.
The following tables list Audi production vehicles that are sold as of 2014:
S and RS models.
RS Q3 Concept
Electric vehicles.
Audi is planning an alliance with the Japanese electronics giant Sanyo to develop a pilot hybrid electric project for the Volkswagen Group. The alliance could result in Sanyo batteries and other electronic components being used in future models of the Volkswagen Group. Concept electric vehicles unveiled to date include the Audi A1 Sportback Concept, Audi A4 TDI Concept E, and the fully electric Audi e-tron Concept Supercar.
Motorsport.
Audi has competed in various forms of motorsports. Audi's tradition in motorsport began with their former company Auto Union in the 1930s. In the 1990s, Audi found success in the Touring and Super Touring categories of motor racing after success in circuit racing in North America.
Rallying.
In 1980, Audi released the Quattro, a four-wheel drive (4WD) turbocharged car that went on to win rallies and races worldwide. It is considered one of the most significant rally cars of all time, because it was one of the first to take advantage of the then-recently changed rules which allowed the use of four-wheel drive in competition racing. Many critics doubted the viability of four-wheel drive racers, thinking them to be too heavy and complex, yet the Quattro was to become a successful car. Leading its first rally it went off the road, however the rally world had been served notice 4WD was the future. The Quattro went on to achieve much success in the World Rally Championship. It won the 1983 (Hannu Mikkola) and the 1984 (Stig Blomqvist) drivers' titles, and brought Audi the manufacturers' title in 1982 and 1984.
In 1984, Audi launched the short-wheelbase Sport Quattro which dominated rally races in Monte Carlo and Sweden, with Audi taking all podium places, but succumbed to problems further into WRC contention. In 1985, after another season mired in mediocre finishes, Walter Röhrl finished the season in his Sport Quattro S1, and helped place Audi second in the manufacturers' points. Audi also received rally honours in the Hong Kong to Beijing rally in that same year. Michèle Mouton, the only female driver to win a round of the World Rally Championship and a driver for Audi, took the Sport Quattro S1, now simply called the "S1", and raced in the Pikes Peak International Hill Climb. The climb race pits a driver and car to drive to the summit of the Pikes Peak mountain in Colorado, and in 1985, Michèle Mouton set a new record of 11:25.39, and being the first woman to set a Pikes Peak record. In 1986, Audi formally left international rally racing following an accident in Portugal involving driver Joaquim Santos in his Ford RS200. Santos swerved to avoid hitting spectators in the road, and left the track into the crowd of spectators on the side, killing three and injuring 30. Bobby Unser used an Audi in that same year to claim a new record for the Pikes Peak Hill Climb at 11:09.22.
In 1987, Walter Röhrl claimed the title for Audi setting a new Pikes Peak International Hill Climb record of 10:47.85 in his Audi S1, which he had retired from the WRC two years earlier. The Audi S1 employed Audi's time-tested inline-five-cylinder turbocharged engine, with the final version generating . The engine was mated to a six-speed gearbox and ran on Audi's famous four-wheel drive system. All of Audi's top drivers drove this car; Hannu Mikkola, Stig Blomqvist, Walter Röhrl and Michèle Mouton. This Audi S1 started the range of Audi 'S' cars, which now represents an increased level of sports-performance equipment within the mainstream Audi model range.
In the USA.
As Audi moved away from rallying and into circuit racing, they chose to move first into America with the Trans-Am in 1988.
In 1989, Audi moved to International Motor Sports Association (IMSA) GTO with the Audi 90, however as they avoided the two major endurance events (Daytona and Sebring) despite winning on a regular basis, they would lose out on the title.
Touring cars.
In 1990, having completed their objective to market cars in North America, Audi returned to Europe, turning first to the Deutsche Tourenwagen Meisterschaft (DTM) series with the Audi V8, and then in 1993, being unwilling to build cars for the new formula, they turned their attention to the fast-growing Super Touring series, which are a series of national championships. Audi first entered in the French Supertourisme and Italian Superturismo. In the following year, Audi would switch to the German Super Tourenwagen Cup (known as STW), and then to British Touring Car Championship (BTCC) the year after that.
The Fédération Internationale de l'Automobile (FIA), having difficulty regulating the quattro four-wheel drive system, and the impact it had on the competitors, would eventually ban all four-wheel drive cars from competing in 1998, but by then, Audi switched all their works efforts to sports car racing.
By 2000, Audi would still compete in the US with their RS4 for the SCCA Speed World GT Challenge, through dealer/team Champion Racing competing against Corvettes, Vipers, and smaller BMWs (where it is one of the few series to permit 4WD cars). In 2003, Champion Racing entered an RS6. Once again, the quattro four-wheel drive was superior, and Champion Audi won the championship. They returned in 2004 to defend their title, but a newcomer, Cadillac with the new Omega Chassis CTS-V, gave them a run for their money. After four victories in a row, the Audis were sanctioned with several negative changes that deeply affected the car's performance. Namely, added ballast weights, and Champion Audi deciding to go with different tyres, and reducing the boost pressure of the turbocharger.
In 2004, after years of competing with the TT-R in the revitalised DTM series, with privateer team Abt Racing/Christian Abt taking the 2002 title with Laurent Aïello, Audi returned as a full factory effort to touring car racing by entering two factory supported Joest Racing A4 DTM cars.
24 Hours of Le Mans.
Audi began racing prototype sportscars in 1999, debuting at the Le Mans 24 hour. Two car concepts were developed and raced in their first season - the Audi R8R (open-cockpit 'roadster' prototype) and the Audi R8C (closed-cockpit 'coupé' GT-prototype). The R8R scored a credible podium on its racing debut at Le Mans and was the concept which Audi continued to develop into the 2000 season due to favourable rules for open-cockpit prototypes.
However, most of the competitors (such as BMW, Toyota, Mercedes and Nissan) retired at the end of 1999.
The factory-supported Joest Racing team won at Le Mans three times in a row with the Audi R8 (2000–2002), as well as winning every race in the American Le Mans Series in its first year. Audi also sold the car to customer teams such as Champion Racing.
In 2003, two Bentley Speed 8s, with engines designed by Audi, and driven by Joest drivers "loaned" to the fellow Volkswagen Group company, competed in the GTP class, and finished the race in the top two positions, while the Champion Racing R8 finished third overall, and first in the LMP900 class. Audi returned to the winner's podium at the 2004 race, with the top three finishers all driving R8s: Audi Sport Japan Team Goh finished first, Audi Sport UK Veloqx second, and Champion Racing third.
At the 2005 24 Hours of Le Mans, Champion Racing entered two R8s, along with an R8 from the Audi PlayStation Team Oreca. The R8s (which were built to old LMP900 regulations) received a narrower air inlet restrictor, reducing power, and an additional of weight compared to the newer LMP1 chassis. On average, the R8s were about 2–3 seconds off pace compared to the Pescarolo–Judd. But with a team of excellent drivers and experience, both Champion R8s were able to take first and third, while the Oreca team took fourth. The Champion team was also the first American team to win Le Mans since the Gulf Ford GTs in 1967. This also ends the long era of the R8; however, its replacement for 2006, called the Audi R10 TDI, was unveiled on 13 December 2005.
The R10 TDI employed many new and innovative features, the most notable being the twin-turbocharged direct injection diesel engine. It was first raced in the 2006 12 Hours of Sebring as a race-test in preparation for the 2006 24 Hours of Le Mans, which it later went on to win. Audi has been on the forefront of sports car racing, claiming a historic win in the first diesel sports car at 12 Hours of Sebring (the car was developed with a Diesel engine due to ACO regulations that favor diesel engines). As well as winning the 24 Hours of Le Mans in 2006 making history, the R10 TDI has also shown its capabilities by beating the Peugeot 908 HDi FAP in , and beating Peugeot again in , (however Peugeot won the 24h in 2009) and, in a podium clean-sweep by proving its reliability throughout the race (compared to all four 908 entries retired before the end of the race) while breaking a new distance record (set way back by the Porsche 917K of Martini Racing in ), in with the R15 TDI Plus.
Audi's sports car racing success would continue with the Audi R18's victory at the 2011 24 Hours of Le Mans. Audi Sport Team Joest's Benoît Tréluyer earned Audi their first pole position in five years while the team's sister car locked out the front row. Early accidents eliminated two of Audi's three entries, but the sole remaining Audi R18 TDI of Tréluyer, Marcel Fässler, and André Lotterer held off the trio of Peugeot 908s to claim victory by a margin of 13.8 seconds.
American Le Mans Series.
Audi entered a factory racing team run by Joest Racing into the American Le Mans Series under the Audi Sport North America name in 2000. This was a successful operation with the team winning on its debut in the series at the 2000 12 Hours of Sebring. Factory backed Audi R8s were the dominant car in ALMS taking 25 victories between 2000 and the end of the 2002 season. In 2003 Audi sold customer cars to Champion Racing as well as continuing to race the factory Audi Sport North America team. Champion Racing won many races as a private team running Audi R8s and eventually replaced Team Joest as the Audi Sport North America between 2006 and 2008. Since 2009 Audi has not taken part in full American Le Mans Series Championships, but has competed in the series opening races at Sebring, using the 12 hour race as a test for Le Mans, and also as part of the 2012 FIA World Endurance Championship season calendar.
European Le Mans Series.
Audi participated in the 2003 1000km of Le Mans which was a one-off sports car race in preparation for the 2004 European Le Mans Series. The factory team Audi Sport UK won races and the championship in the 2004 season but Audi was unable to match their sweeping success of Audi Sport North America in the American Le Mans Series, partly due to the arrival of a factory competitor in LMP1, Peugeot. The French manufacturer's 908 HDi FAP became the car to beat in the series from 2008 onwards with 20 LMP wins. However, Audi were able to secure the championship in 2008 even though Peugeot scored more race victories in the season.
World Endurance Championship.
2012.
In 2012, the FIA sanctioned a World Endurance Championship which would be organised by the ACO as a continuation of the ILMC. Audi competed won the first WEC race at Sebring and followed this up with a further three successive wins, including the 2012 24 hours of Le Mans. Audi scored a final 5th victory in the 2012 WEC in Bahrain and were able to win the inaugural WEC Manufacturers' Championship.
2013.
As defending champions, Audi once again entered the Audi R18 e-tron quattro chassis into the 2013 WEC and the team won the first five consecutive races, including the 2013 24 hours of le Mans. The victory at Round 5, Circuit of the Americas, was of particular significance as it marked the 100th win for Audi in Le Mans prototypes. Audi secured their second consecutive WEC Manufacturers' Championship at Round 6 after taking second place and half points in the red-flagged Fuji race.
2014.
For the 2014 season Audi entered a redesigned and upgraded R18 e-tron quattro which featured a 2 MJ energy recovery system. As defending champions, Audi would once again face a challenge in LMP1 from Toyota, and additionally from Porsche who returned to endurance racing after a 16 year absence. The season opening 6hrs of Silverstone was a disaster for Audi who saw both cars retire from the race, marking the first time that an Audi car has failed to score a podium in a World Endurance Championship race.
Formula E.
Audi will provide factory support to a Formula E team in partnership with DTM team Abt Sportsline. This team will be called Audi Sport Abt Formula E Team in the inaugural 2014/15 Formula E season. On 13 February 2014 the team announced its driver line up as Daniel Abt and World Endurance Championship driver Lucas di Grassi.
Marketing.
Branding.
The Audi emblem is four overlapping rings that represent the four marques of Auto Union. The Audi emblem symbolises the amalgamation of Audi with DKW, Horch and Wanderer: the first ring from the left represents Audi, the second represents DKW, third is Horch, and the fourth and last ring Wanderer.
Its similarity to the Olympic rings caused the International Olympic Committee to sue Audi in International Trademark Court in 1995, to which they lost.
As part of Audi's centennial celebration in 2009, the company updated the logo, changing the font to left-aligned Audi Type, and altering the shading for the overlapping rings. The revised logo was designed by Rayan Abdullah.
Audi developed a Corporate Sound concept, with Audi Sound Studio designed for producing the Corporate Sound. The Corporate Sound project began with sound agency Klangerfinder GmbH & Co KG and s12 GmbH. Audio samples were created in Klangerfinder's sound studio in Stuttgart, becoming part of Audi Sound Studio collection. Other Audi Sound Studio components include The Brand Music Pool, The Brand Voice. Audi also developed Sound Branding Toolkit including certain instruments, sound themes, rhythm and car sounds which all are supposed to reflect the AUDI sound character.
Audi started using a beating heart sound trademark beginning in 1996. An updated heartbeat sound logo, developed by agencies KLANGERFINDER GmbH & Co KG of Stuttgart and S12 GmbH of Munich, was first used in 2010 in an Audi A8 commercial with the slogan "The Art of Progress."
Slogans.
Audi's corporate tagline is "Vorsprung durch Technik", meaning "Progress through Technology". The German-language tagline is used in many European countries, including the United Kingdom, and in other markets, such as Latin America, Oceania and parts of Asia including Japan. A few years ago, the North American tagline was "Innovation through technology", but in Canada the German tagline "Vorsprung durch Technik" was used in advertising. More recently, however, Audi has updated the tagline to "Truth in Engineering" in the U.S.
Typography.
Audi Sans (based on Univers Extended) was originally created in 1997 by Ole Schäfer for MetaDesign. MetaDesign was later commissioned for a new corporate typeface called Audi Type, designed by Paul van der Laan and Pieter van Rosmalen of Bold Monday. The font began to appear in Audi's 2009 products and marketing materials.
Sponsorships.
Audi is a strong partner of different kinds of sports. In football, long partnerships exist between Audi and domestic clubs including FC Bayern Munich, Hamburger SV, 1. FC Nuremberg, Hertha Berlin, and Borussia Mönchengladbach and international clubs including Chelsea FC, Real Madrid CF, FC Barcelona, AC Milan, Ajax Amsterdam, Queens Park Rangers F.C. and Perspolis F.C.. Audi also sponsors winter sports: The Audi FIS Alpine Ski World Cup is named after the company. Additionally, Audi supports the German Ski Association (DSV) as well as the alpine skiing national teams of Switzerland, Sweden, Finland, France, Liechtenstein, Italy, Austria and the US. For almost two decades Audi fosters golf sport: for example with the Audi quattro Cup and the HypoVereinsbank Ladies German Open presented by Audi. In sailing, Audi is engaged in the Medcup regatta and supports the team Luna Rossa during the Louis Vuitton Pacific Series and also is the primary sponsor of the Melges 20 sailboat. Further, Audi sponsors the regional teams ERC Ingolstadt (hockey) and FC Ingolstadt 04 (soccer).
In 2009, the year of Audis 100th anniversary, the company organises the Audi Cup for the first time. Audi also sponsor the New York Yankees as well. In October 2010 they agreed to a three sponsorship year-deal with Everton. Audi also sponsors the England Polo Team and holds the Audi Polo Awards.
Multitronic campaign.
In 2001, Audi promoted the new multitronic continuously variable transmission with television commercials throughout Europe, featuring an impersonator of musician and actor Elvis Presley. A prototypical dashboard figure – later named "Wackel-Elvis" ("Wobble Elvis" or "Wobbly Elvis") – appeared in the commercials to demonstrate the smooth ride in an Audi equipped with the multitronic transmission. The dashboard figure was originally intended for use in the commercials only, but after they aired the demand for Wackel-Elvis fans grew among fans and the figure was mass-produced in China and marketed by Audi in their factory outlet store.
Audi TDI.
As part of Audi's attempt to promote its Diesel technology in 2009, the company began Audi Mileage Marathon. The driving tour featured a fleet of 23 Audi TDI vehicles from 4 models (Audi Q7 3.0 TDI, Audi Q5 3.0 TDI, Audi A4 3.0 TDI, Audi A3 Sportback 2.0 TDI with S tronic transmission) travelling across the American continent from New York to Los Angeles, passing major cities like Chicago, Dallas and Las Vegas during the 13 daily stages, as well as natural wonders including the Rocky Mountains, Death Valley and the Grand Canyon.
As part of 2014 model year Audi TDI vehicles launch in the US, 3 television commercials ("The Station", "Future", "Range") were produced. In the 60-second 'The Station' ad, a woman at a fueling station reaches for the diesel pump to fill up her Audi A6. In a dramatic fashion, unsuspecting onlookers race towards her and they can't imagine the luxury vehicle is in fact a diesel. The spot ends with the tagline "It's time to rethink diesel – join the club." "The Station" appeared on primetime network and cable 2013 fall programming including Agents of S.H.I.E.L.D, Modern Family, The Big Bang Theory, Hostages, Sons of Anarchy and NBC NFL Sunday Night Football. The 15-second "Range" ad demonstrates the potential to drive from New York to Chicago on a single tank of gas, covering a range of approximately 790 miles. In the 15-second "Future" ad, viewers see the potential for clean diesel as today's leading alternative fuel solution and an intelligent choice for those on the leading-edge. Audi TDI provides drivers with 30% better fuel economy and range without compromises on performance and design. In addition to the three new television spots, Audi also tried to dispel the most common myths of diesel – gas station availability, the smell and perception associated with an older generation of diesel vehicles, weak performance – in a series of four online video shorts that would roll out over the next two months on the Audi YouTube channel (http://www.youtube.com/audiusa). The spots also will appear on The Washington Post and Slate.com in a custom user-generated content hub through 2013-10-31. In addition to standard and high-impact ads, the content hub features custom videos, articles and infographics, along with relevant social conversations. The Audi TDI clean diesel campaign also features print ads that reinforce the message "the future of fuel is here now." Print ads would roll out in select automotive buff books in fall 2013. 'The Station' ad was premiered in Canada in September 2013. 'The Station' (also called 'The Moment of Truth') ad was produced by Venables Bell & Partners, Biscuit Filmworks, Final Cut.
As part of 2014 model year Audi TDI vehicles launch in the US, the 'Truth in 48' driving challenge took place from Audi Pacific dealership at Los Angeles to New York in 48 hours or less, began at 9 a.m. PDT on 2013-09-07. The Coast-to-coast attempt used 2014 Audi A6 TDI and Audi A7 TDI and a 2014 Audi Q5 TDI crossover as the support vehicle, with teams of eight noted hypermilers and four journalists.
Audi e-tron.
The next phase of technology Audi is developing is the e-tron electric drive powertrain system. They have shown several concept cars , each with different levels of size and performance. The original e-tron concept shown at the 2009 Frankfurt motor show is based on the platform of the R8 and has been scheduled for limited production. Power is provided by electric motors at all four wheels. The second concept was shown at the 2010 Detroit Motor Show. Power is provided by two electric motors at the rear axle. This concept is also considered to be the direction for a future mid-engined gas-powered 2-seat performance coupe. The Audi A1 e-tron concept, based on the Audi A1 production model, is a hybrid vehicle with a range extending Wankel rotary engine to provide power after the initial charge of the battery is depleted. It is the only concept of the three to have range extending capability. The car is powered through the front wheels, always using electric power.
It is all set to be displayed at the Auto Expo 2012 in New Delhi, India, from 5 January. Powered by a 1.4 litre engine, and can cover a distance up to 54 km s on a single charge. The e-tron was also shown in the 2013 blockbuster film Iron Man 3 and was driven by Tony Stark (Iron Man).
In video games.
In PlayStation Home, the PlayStation 3's online community-based service, Audi has supported Home by releasing a dedicated Home space in the European version of Home. Audi is the first carmaker to develop a space for Home. On 17 December 2009, Audi released the Audi Space as two spaces; the Audi Home Terminal and the Audi Vertical Run. The Audi Home Terminal features an Audi TV channel delivering video content, an Internet Browser feature, and a view of a city. The Audi Vertical Run is where users can access the mini-game Vertical Run, a futuristic mini-game featuring Audi's e-tron concept. Players collect energy and race for the highest possible speeds and the fastest players earn a place in the Audi apartments located in a large tower in the centre of the Audi Space. In both the Home Terminal and Vertical Run spaces, there are teleports where users can teleport back and forth between the two spaces. Audi has stated that additional content will be added in 2010.

</doc>
<doc id="849" url="http://en.wikipedia.org/wiki?curid=849" title="Aircraft">
Aircraft

An aircraft is a machine that is able to fly by gaining support from the air, or, in general, the atmosphere of a planet. It counters the force of gravity by using either static lift or by using the dynamic lift of an airfoil, or in a few cases the downward thrust from jet engines.
The human activity that surrounds aircraft is called "aviation". Crewed aircraft are flown by an onboard pilot, but unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers. Aircraft may be classified by different criteria, such as lift type, propulsion, usage and others.
History.
Flying model craft and stories of manned flight go back many centuries, however the first manned ascent – and safe descent – in modern times took place by hot-air balloon in the 18th century. Each of the two World Wars led to great technical advances. Consequently the history of aircraft can be divided into five eras:
Methods of lift.
Lighter than air – aerostats.
Aerostats use buoyancy to float in the air in much the same way that ships float on the water. They are characterized by one or more large gasbags or canopies, filled with a relatively low-density gas such as helium, hydrogen, or hot air, which is less dense than the surrounding air. When the weight of this is added to the weight of the aircraft structure, it adds up to the same weight as the air that the craft displaces.
Small hot-air balloons called sky lanterns date back to the 3rd century BC, and were only the second type of aircraft to fly, the first being kites.
A balloon was originally any aerostat, while the term airship was used for large, powered aircraft designs – usually fixed-wing – though none had yet been built. The advent of powered balloons, called dirigible balloons, and later of rigid hulls allowing a great increase in size, began to change the way these words were used. Huge powered aerostats, characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags, were produced, the Zeppelins being the largest and most famous. There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships, so "airship" came to be synonymous with these aircraft. Then several accidents, such as the Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a "balloon" is an unpowered aerostat and an "airship" is a powered one.
A powered, steerable aerostat is called a "dirigible". Sometimes this term is applied only to non-rigid balloons, and sometimes "dirigible balloon" is regarded as the definition of an airship (which may then be rigid or non-rigid). Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back. These soon became known as "blimps". During the Second World War, this shape was widely adopted for tethered balloons; in windy weather, this both reduces the strain on the tether and stabilizes the balloon. The nickname "blimp" was adopted along with the shape. In modern times, any small dirigible or airship is called a blimp, though a blimp may be unpowered as well as powered.
Heavier-than-air – aerodynes.
Heavier-than-air aircraft, such as airplanes, must find some way to push air or gas downwards, so that a reaction occurs (by Newton's laws of motion) to push the aircraft upwards. This dynamic movement through the air is the origin of the term "aerodyne". There are two ways to produce dynamic upthrust: aerodynamic lift, and powered lift in the form of engine thrust.
Aerodynamic lift involving wings is the most common, with fixed-wing aircraft being kept in the air by the forward movement of wings, and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air must flow over the wing and generate lift. A "flexible wing" is a wing made of fabric or thin sheet material, often stretched over a rigid frame. A "kite" is tethered to the ground and relies on the speed of the wind over its wings, which may be flexible or rigid, fixed, or rotary.
With powered lift, the aircraft directs its engine thrust vertically downward. V/STOL aircraft, such as the Harrier Jump Jet and F-35B take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight.
A pure rocket is not usually regarded as an aerodyne, because it does not depend on the air for its lift (and can even fly into space); however, many aerodynamic lift vehicles have been powered or assisted by rocket motors. Rocket-powered missiles that obtain aerodynamic lift at very high speed due to airflow over their bodies are a marginal case.
Fixed-wing.
The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings, a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift. Kites were the first kind of aircraft to fly, and were invented in China around 500 BC. Much aerodynamic research was done with kites before test aircraft, wind tunnels, and computer modelling programs became available.
The first heavier-than-air craft capable of controlled free-flight were gliders. A glider designed by Cayley carried out the first true manned, controlled flight in 1853.
Practical, powered, fixed-wing aircraft (the aeroplane or airplane) were invented by Wilbur and Orville Wright. Besides the method of propulsion, fixed-wing aircraft are in general characterized by their wing configuration. The most important wing characteristics are:
A variable geometry aircraft can change its wing configuration during flight.
A "flying wing" has no fuselage, though it may have small blisters or pods. The opposite of this is a "lifting body", which has no wings, though it may have small stabilizing and control surfaces.
Wing-in-ground-effect vehicles may be considered as fixed-wing aircraft. They "fly" efficiently close to the surface of the ground or water, like conventional aircraft during takeoff. An example is the Russian ekranoplan (nicknamed the "Caspian Sea Monster"). Man-powered aircraft also rely on ground effect to remain airborne with a minimal pilot power, but this is only because they are so underpowered — in fact, the airframe is capable of flying higher.
Rotorcraft.
Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section blades (a "rotary wing") to provide lift. Types include helicopters, autogyros, and various hybrids such as gyrodynes and compound rotorcraft.
"Helicopters" have a rotor turned by an engine-driven shaft. The rotor pushes air downward to create lift. By tilting the rotor forward, the downward flow is tilted backward, producing thrust for forward flight. Some helicopters have more than one rotor and a few have rotors turned by gas jets at the tips.
"Autogyros" have unpowered rotors, with a separate power plant to provide thrust. The rotor is tilted backward. As the autogyro moves forward, air blows upward across the rotor, making it spin. This spinning increases the speed of airflow over the rotor, to provide lift. Rotor kites are unpowered autogyros, which are towed to give them forward speed or tethered to a static anchor in high-wind for kited flight.
"Cyclogyros" rotate their wings about a horizontal axis.
"Compound rotorcraft" have wings that provide some or all of the lift in forward flight. They are nowadays classified as "powered lift" types and not as rotorcraft. "Tiltrotor" aircraft (such as the V-22 Osprey), tiltwing, tailsitter, and coleopter aircraft have their rotors/propellers horizontal for vertical flight and vertical for forward flight.
Propulsion.
Unpowered aircraft.
Gliders are heavier-than-air aircraft that do not employ propulsion once airborne. Take-off may be by launching forward and downward from a high location, or by pulling into the air on a tow-line, either by a ground-based winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its forward air speed and lift, it must descend in relation to the air (but not necessarily in relation to the ground). Many gliders can 'soar' – gain height from updrafts such as thermal currents. The first practical, controllable example was designed and built by the British scientist and pioneer George Cayley, whom many recognise as the first aeronautical engineer. Common examples of gliders are sailplanes, hang gliders and paragliders.
Balloons drift with the wind, though normally the pilot can control the altitude, either by heating the air or by releasing ballast, giving some directional control (since the wind direction changes with altitude). A wing-shaped hybrid balloon can glide directionally when rising or falling; but a spherically shaped balloon does not have such directional control.
Kites are aircraft that are tethered to the ground or other object (fixed or mobile) that maintains tension in the tether or kite line; they rely on virtual or real wind blowing over and under them to generate lift and drag. Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting deflections, and can be lighter-than-air, neutrally buoyant, or heavier-than-air.
Powered aircraft.
Powered aircraft have one or more onboard sources of mechanical power, typically aircraft engines although rubber and manpower have also been used. Most aircraft engines are either lightweight piston engines or gas turbines. Engine fuel is stored in tanks, usually in the wings but larger aircraft also have additional fuel tanks in the fuselage.
Propeller aircraft.
Propeller aircraft use one or more propellers (airscrews) to create thrust in a forward direction. The propeller is usually mounted in front of the power source in "tractor configuration" but can be mounted behind in "pusher configuration". Variations of propeller layout include "contra-rotating propellers" and "ducted fans".
Many kinds of power plant have been used to drive propellers. Early airships used man power or steam engines. The more practical internal combustion piston engine was used for virtually all fixed-wing aircraft until World War II and is still used in many smaller aircraft. Some types use turbine engines to drive a propeller in the form of a turboprop or propfan. Human-powered flight has been achieved, but has not become a practical means of transport. Unmanned aircraft and models have also used power sources such as electric motors and rubber bands.
Jet aircraft.
Jet aircraft use airbreathing jet engines, which take in air, burn fuel with it in a combustion chamber, and accelerate the exhaust rearwards to provide thrust.
Turbojet and turbofan engines use a spinning turbine to drive one or more fans, which provide additional thrust. An afterburner may be used to inject extra fuel into the hot exhaust, especially on military "fast jets". Use of a turbine is not absolutely necessary: other designs include the pulse jet and ramjet. These mechanically simple designs cannot work when stationary, so the aircraft must be launched to flying speed by some other method. Other variants have also been used, including the motorjet and hybrids such as the Pratt & Whitney J58, which can convert between turbojet and ramjet operation.
Compared to propellers, jet engines can provide much higher thrust, higher speeds and, above about , greater efficiency. They are also much more fuel-efficient than rockets. As a consequence nearly all large, high-speed or high-altitude aircraft use jet engines.
Rotorcraft.
Some rotorcraft, such as helicopters, have a powered rotary wing or "rotor", where the rotor disc can be angled slightly forward so that a proportion of its lift is directed forwards. The rotor may, like a propeller, be powered by a variety of methods such as a piston engine or turbine. Experiments have also used jet nozzles at the rotor blade tips.
Design and construction.
Aircraft are designed according to many factors such as customer and manufacturer demand, safety protocols and physical and economic constraints. For many types of aircraft the design process is regulated by national airworthiness authorities.
The key parts of an aircraft are generally divided into three categories:
Structure.
The approach to structural design varies widely between different types of aircraft. Some, such as paragliders, comprise only flexible materials that act in tension and rely on aerodynamic pressure to hold their shape. A balloon similarly relies on internal gas pressure but may have a rigid basket or gondola slung below it to carry its payload. Early aircraft, including airships, often employed flexible doped aircraft fabric covering to give a reasonably smooth aeroshell stretched over a rigid frame. Later aircraft employed semi-monocoque techniques, where the skin of the aircraft is stiff enough to share much of the flight loads. In a true monocoque design there is no internal structure left.
The key structural parts of an aircraft depend on what type it is.
Aerostats.
Lighter-than-air types are characterised by one or more gasbags, typically with a supporting structure of flexible cables or a rigid framework called its hull. Other elements such as engines or a gondola may also be attached to the supporting structure.
Aerodynes.
Heavier-than-air types are characterised by one or more wings and a central fuselage. The fuselage typically also carries a tail or empennage for stability and control, and an undercarriage for takeoff and landing. Engines may be located on the fuselage or wings. On a fixed-wing aircraft the wings are rigidly attached to the fuselage, while on a rotorcraft the wings are attached to a rotating vertical shaft. Smaller designs sometimes use flexible materials for part or all of the structure, held in place either by a rigid frame or by air pressure. The fixed parts of the structure comprise the airframe.
Avionics.
The avionics comprise the flight control systems and related equipment, including the cockpit instrumentation, navigation, radar, monitoring, and communication systems.
Flight characteristics.
Flight envelope.
The flight envelope of an aircraft refers to its capabilities in terms of airspeed and load factor or altitude. The term can also refer to other measurements such as maneuverability. When a craft is pushed, for instance by diving it at high speeds, it is said to be flown "outside the envelope", something considered unsafe.
Range.
The range is the distance an aircraft can fly between takeoff and landing, as limited by the time it can remain airborne.
For a powered aircraft the time limit is determined by the fuel load and rate of consumption.
For an unpowered aircraft, the maximum flight time is limited by factors such as weather conditions and pilot endurance. Many aircraft types are restricted to daylight hours, while balloons are limited by their supply of lifting gas. The range can be seen as the average ground speed multiplied by the maximum time in the air.
Flight dynamics.
Flight dynamics is the science of air vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation around three axes about the vehicle's center of mass, known as "pitch", "roll", and "yaw" (quite different from their use as Tait-Bryan angles).
Flight dynamics is concerned with the stability and control of an aircraft's rotation about each of these axes.
Stability.
An aircraft that is unstable tends to diverge from its current flight path and so is difficult to fly. An aircraft which is very stable tends to stay on its current flight path and is difficult to manoeuvre. So it is important for any design to achieve the desired degree of stability. Since the widespread use of digital computers, it is becoming increasingly common for designs to be inherently unstable and to rely on computerised control systems to provide artificial stability.
A fixed wing is typically unstable in pitch, roll and yaw. Pitch and yaw stabilities of conventional fixed wing designs need horizontal and vertical stabilisers, which act in a similar way to the feathers on an arrow. These stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise the flight dynamics of pitch and yaw. They are usually mounted on the tail section (empennage), although in the canard layout, the main aft wing replaces the canard foreplane as pitch stabilizer. tandem and Tailless aircraft rely on the same general rule to achieve stability, the aft surface being the stabilising one.
A rotary wing is typically unstable in yaw, requiring a vertical stabiliser.
A balloon is typically very stable in pitch and roll due to the way the payload is hung underneath.
Control.
Flight control surfaces enable the pilot to control an aircraft's flight attitude and are usually part of the wing or mounted on, or integral with, the associated stabilizing surface. Their development was a critical advance in the history of aircraft, which had until that point been uncontrollable in flight.
Aerospace engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the aerodynamic center of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft, causing the aircraft to pitch up or down. Control systems are also sometimes used to increase or decrease drag, for example to slow the aircraft to a safe speed for landing.
The two main aerodynamic forces acting on any aircraft are lift supporting it in the air and drag opposing its motion. Control surfaces or other techniques may also be used to affect these forces directly, without inducing any rotation.
Impacts of aircraft use.
Aircraft permit long distance, high speed travel and may be the more fuel efficient mode of transportation in some circumstances. Aircraft have environmental and climate impacts beyond fuel efficiency considerations, however. They are also relatively noisy compared to other forms of travel and high altitude aircraft generate contrails, which experimental evidence suggests may alter weather patterns.
Uses for aircraft.
Aircraft are produced in several different types optimized for various uses; military aircraft, which includes not just combat types but many types of supporting aircraft, and civil aircraft, which include all non-military types, experimental and model.
Military.
A military aircraft is any aircraft that is operated by a legal or insurrectionary armed service of any type. Military aircraft can be either combat or non-combat:
Most military aircraft are powered heavier-than-air types. Other types such as gliders and balloons have also been used as military aircraft; for example, balloons were used for observation during the American Civil War and World War I, and military gliders were used during World War II to land troops.
Civil.
Civil aircraft divide into "commercial" and "general" types, however there are some overlaps.
Commercial aircraft include types designed for scheduled and charter airline flights, carrying passengers, mail and other cargo. The larger passenger-carrying types are the airliners, the largest of which are wide-body aircraft. Some of the smaller types are also used in general aviation, and some of the larger types are used as VIP aircraft.
General aviation is a catch-all covering other kinds of private (where the pilot is not paid for time or expenses) and commercial use, and involving a wide range of aircraft types such as business jets (bizjets), trainers, homebuilt, gliders, warbirds and hot air balloons to name a few. The vast majority of aircraft today are general aviation types.
Experimental.
An experimental aircraft is one that has not been fully proven in flight, or one that carries an FAA airworthiness certificate in the "Experimental" category. Often, this implies that new aerospace technologies are being tested on the aircraft, although the term also refers to amateur- and kit-built aircraft; many of which are based on proven designs.
Model.
A model aircraft is a small unmanned type made to fly for fun, for static display, for aerodynamic research or for other purposes. A scale model is a replica of some larger design.
External links.
History
Information

</doc>
<doc id="851" url="http://en.wikipedia.org/wiki?curid=851" title="Alfred Nobel">
Alfred Nobel

Alfred Bernhard Nobel ( ; 21 October 1833 – 10 December 1896) was a Swedish chemist, engineer, innovator, and armaments manufacturer.
He was the inventor of dynamite. Nobel also owned Bofors, which he had redirected from its previous role as primarily an iron and steel producer to a major manufacturer of cannon and other armaments. Nobel held 350 different patents, dynamite being the most famous. His fortune was used posthumously to institute the Nobel Prizes. The synthetic element nobelium was named after him. His name also survives in modern-day companies such as Dynamit Nobel and AkzoNobel, which are descendants of or mergers with companies Nobel himself established.
Life and career.
Born in Stockholm, Alfred Nobel was the fourth son of Immanuel Nobel (1801–1872), an inventor and engineer, and Karolina Andriette (Ahlsell) Nobel (1805–1889). The couple married in 1827 and had eight children. The family was impoverished, and only Alfred and his three brothers survived past childhood. Through his father, Alfred Nobel was a descendant of the Swedish scientist Olaus Rudbeck (1630–1702), and in his turn the boy was interested in engineering, particularly explosives, learning the basic principles from his father at a young age. Alfred Nobel's interest in technology was inherited from his father, an alumnus of Royal Institute of Technology in Stockholm.
Following various business failures, Nobel's father moved to Saint Petersburg in 1837 and grew successful there as a manufacturer of machine tools and explosives. He invented modern plywood and started work on the "torpedo". In 1842, the family joined him in the city. Now prosperous, his parents were able to send Nobel to private tutors and the boy excelled in his studies, particularly in chemistry and languages, achieving fluency in English, French, German, and Russian. For 18 months, from 1841 to 1842, Nobel went to the only school he ever attended as a child, the Jacobs Apologistic School in Stockholm.
As a young man, Nobel studied with chemist Nikolai Zinin; then, in 1850, went to Paris to further the work; and, at 18, he went to the United States for four years to study chemistry, collaborating for a short period under inventor John Ericsson, who designed the American Civil War ironclad "USS Monitor". Nobel filed his first patent, for a gas meter, in 1857.
The family factory produced armaments for the Crimean War (1853–1856); but, had difficulty switching back to regular domestic production when the fighting ended and they filed for bankruptcy. In 1859, Nobel's father left his factory in the care of the second son, Ludvig Nobel (1831–1888), who greatly improved the business. Nobel and his parents returned to Sweden from Russia and Nobel devoted himself to the study of explosives, and especially to the safe manufacture and use of nitroglycerine (discovered in 1847 by Ascanio Sobrero, one of his fellow students under Théophile-Jules Pelouze at the University of Turin). Nobel invented a detonator in 1863; and, in 1865, he designed the blasting cap.
On 3 September 1864, a shed, used for the preparation of nitroglycerin, exploded at the factory in Heleneborg Stockholm, killing five people, including Nobel's younger brother Emil. Dogged by more minor accidents but unfazed, Nobel went on to build further factories, focusing on improving the stability of the explosives he was developing. Nobel invented dynamite in 1867, a substance easier and safer to handle than the more unstable nitroglycerin. Dynamite was patented in the US and the UK and was used extensively in mining and the building of transport networks internationally. In 1875 Nobel invented gelignite, more stable and powerful than dynamite, and in 1887 patented ballistite, a forerunner of cordite.
Nobel was elected a member of the Royal Swedish Academy of Sciences in 1884, the same institution that would later select laureates for two of the Nobel prizes, and he received an honorary doctorate from Uppsala University in 1893.
Nobel's brothers Ludvig and Robert exploited oilfields along the Caspian Sea and became hugely rich in their own right. Nobel invested in these and amassed great wealth through the development of these new oil regions. During his life Nobel issued 350 patents internationally and by his death had established 90 armaments factories, despite his belief in pacifism.
In 1888, the death of his brother Ludvig caused several newspapers to publish obituaries of Alfred in error. A French obituary stated "Le marchand de la mort est mort" "("The merchant of death is dead")".
In 1891, following the death of his mother and his brother Ludvig and the end of a longstanding relationship, Nobel moved from Paris to San Remo, Italy. Suffering from angina, Nobel died at home, of a cerebral hemorrhage in 1896. Unbeknownst to his family, friends or colleagues, he had left most of his wealth in trust, in order to fund the awards that would become known as the Nobel Prizes. He is buried in Norra begravningsplatsen in Stockholm.
Personal life.
Through baptism and confirmation Alfred Nobel was Lutheran and during his Paris years he frequented regularly the Church of Sweden Abroad led by pastor Nathan Söderblom who would in 1930 also be the recipient of the Nobel Peace Prize.
Nobel travelled for much of his business life, maintaining companies in various countries in Europe and North America and keeping a permanent home in Paris from 1873 to 1891. He remained a solitary character, given to periods of depression. Though Nobel remained unmarried, his biographers note that he had at least three loves. Nobel's first love was in Russia with a girl named Alexandra, who rejected his proposal. In 1876 Austro-Bohemian Countess Bertha Kinsky became Alfred Nobel's secretary. But after only a brief stay she left him to marry her previous lover, Baron Arthur Gundaccar von Suttner. Though her personal contact with Alfred Nobel had been brief, she corresponded with him until his death in 1896, and it is believed that she was a major influence in his decision to include a peace prize among those prizes provided in his will. Bertha von Suttner was awarded the 1905 Nobel Peace prize, 'for her sincere peace activities'.
Nobel's third and longest-lasting relationship was with Sofie Hess from Vienna, whom he met in 1876. The liaison lasted for 18 years. After his death, according to his biographers Evlanoff, Fluor, and Fant, Nobel's letters were locked within the Nobel Institute in Stockholm. They were released only in 1955, to be included with other biographical data.
Despite the lack of formal secondary and tertiary level education, Nobel gained proficiency in six languages: Swedish, French, Russian, English, German and Italian. He also developed sufficient literary skill to write poetry in English. His "Nemesis", a prose tragedy in four acts about Beatrice Cenci, partly inspired by Percy Bysshe Shelley's "The Cenci", was printed while he was dying. The entire stock except for three copies was destroyed immediately after his death, being regarded as scandalous and blasphemous. The first surviving edition (bilingual Swedish–Esperanto) was published in Sweden in 2003. The play has been translated into Slovenian via the Esperanto version and into French. In 2010 it was published in Russia in another bilingual (Russian–Esperanto) edition.
Inventions.
Nobel found that when nitroglycerin was incorporated in an absorbent inert substance like "kieselguhr" (diatomaceous earth) it became safer and more convenient to handle, and this mixture he patented in 1867 as 'dynamite'. Nobel demonstrated his explosive for the first time that year, at a quarry in Redhill, Surrey, England. In order to help reestablish his name and improve the image of his business from the earlier controversies associated with the dangerous explosives, Nobel had also considered naming the highly powerful substance "Nobel's Safety Powder", but settled with Dynamite instead, referring to the Greek word for 'power'.
Nobel later on combined nitroglycerin with various nitrocellulose compounds, similar to collodion, but settled on a more efficient recipe combining another nitrate explosive, and obtained a transparent, jelly-like substance, which was a more powerful explosive than dynamite. 'Gelignite', or blasting gelatin, as it was named, was patented in 1876; and was followed by a host of similar combinations, modified by the addition of potassium nitrate and various other substances. Gelignite was more stable, transportable and conveniently formed to fit into bored holes, like those used in drilling and mining, than the previously used compounds and was adopted as the standard technology for mining in the Age of Engineering bringing Nobel a great amount of financial success, though at a significant cost to his health. An off-shoot of this research resulted in Nobel's invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant.
Nobel Prizes.
In 1888 Alfred's brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred's obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated, "" ("The merchant of death is dead") and went on to say, "Dr. Alfred Nobel, who became rich by finding ways to kill more people faster than ever before, died yesterday." Alfred was disappointed with what he read and concerned with how he would be remembered.
On 27 November 1895, at the Swedish-Norwegian Club in Paris, Nobel signed his last will and testament and set aside the bulk of his estate to establish the Nobel Prizes, to be awarded annually without distinction of nationality. After taxes and bequests to individuals, Nobel's will allocated 94% of his total assets, 31,225,000 Swedish kronor, to establish the five Nobel Prizes. This converted to GBP £1,687,837 at the time. In 2012, the capital was worth around SEK 3.1 billion (USD 472 million, EUR 337 million), which is almost twice the amount of the initial capital, taking inflation into account.
The first three of these prizes are awarded for eminence in physical science, in chemistry and in medical science or physiology; the fourth is for literary work "in an ideal direction" and the fifth prize is to be given to the person or society that renders the greatest service to the cause of international fraternity, in the suppression or reduction of standing armies, or in the establishment or furtherance of peace congresses.
The formulation for the literary prize being given for a work "in an ideal direction" (' in Swedish), is cryptic and has caused much confusion. For many years, the Swedish Academy interpreted "ideal" as "idealistic" (') and used it as a reason not to give the prize to important but less Romantic authors, such as Henrik Ibsen and Leo Tolstoy. This interpretation has since been revised, and the prize has been awarded to, for example, Dario Fo and José Saramago, who do not belong to the camp of literary idealism.
There was room for interpretation by the bodies he had named for deciding on the physical sciences and chemistry prizes, given that he had not consulted them before making the will. In his one-page testament, he stipulated that the money go to discoveries or inventions in the physical sciences and to discoveries or improvements in chemistry. He had opened the door to technological awards, but had not left instructions on how to deal with the distinction between science and technology. Since the deciding bodies he had chosen were more concerned with the former, the prizes went to scientists more often than engineers, technicians or other inventors.
In 2001, Alfred Nobel's great-grandnephew, Peter Nobel (b. 1931), asked the Bank of Sweden to differentiate its award to economists given "in Alfred Nobel's memory" from the five other awards. This request added to the controversy over whether the Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel is actually a "Nobel Prize".
Monuments.
The "Monument to Alfred Nobel" (, ) in Saint Petersburg is located along the Bolshaya Nevka River on Petrogradskaya Embankment. It was dedicated in 1991 to mark the 90th anniversary of the first Nobel Prize presentation. Diplomat and Professor Arkady Melua initiators of creation of the monument (1989). Professor A. Melua has provided funds for the establishment of the monument (, 1990–1991). The abstract metal sculpture was designed by local artists Sergey Alipov and Pavel Shevchenko, and appears to be an explosion or branches of a tree. Petrogradskaya Embankment is the street where the Nobel's family lived until 1859.

</doc>
<doc id="852" url="http://en.wikipedia.org/wiki?curid=852" title="Alexander Graham Bell">
Alexander Graham Bell

Alexander Graham Bell (March 3, 1847 – August 2, 1922) was an eminent Scottish-born scientist, inventor, engineer and innovator who is credited with inventing the first practical telephone.
Bell's father, grandfather, and brother had all been associated with work on elocution and speech, and both his mother and wife were deaf, profoundly influencing Bell's life's work. His research on hearing and speech further led him to experiment with hearing devices which eventually culminated in Bell being awarded the first U.S. patent for the telephone in 1876. Bell considered his most famous invention an intrusion on his real work as a scientist and refused to have a telephone in his study.
Many other inventions marked Bell's later life, including groundbreaking work in optical telecommunications, hydrofoils and aeronautics. In 1888, Bell became one of the founding members of the National Geographic Society.
Early life.
Alexander Bell was born in Edinburgh, Scotland, on March 3, 1847. The family home was at 16 South Charlotte Street, and has a stone inscription, marking it as Alexander Graham Bell's birthplace. He had two brothers: Melville James Bell (1845–70) and Edward Charles Bell (1848–67). Both of his brothers died of tuberculosis. His father was Professor Alexander Melville Bell, and his mother was Eliza Grace (née Symonds). Although he was born "Alexander", at age 10, he made a plea to his father to have a middle name like his two brothers. For his 11th birthday, his father acquiesced and allowed him to adopt the middle name "Graham", chosen out of admiration for Alexander Graham, a Canadian being treated by his father and boarder who had become a family friend. To close relatives and friends he remained "Aleck" which his father continued to call him into later life.
First invention.
As a child, young Alexander displayed a natural curiosity about his world, resulting in gathering botanical specimens as well as experimenting even at an early age. His best friend was Ben Herdman, a neighbor whose family operated a flour mill, the scene of many forays. Young Aleck asked what needed to be done at the mill. He was told wheat had to be dehusked through a laborious process and at the age of 12, Bell built a homemade device that combined rotating paddles with sets of nail brushes, creating a simple dehusking machine that was put into operation and used steadily for a number of years. In return, John Herdman gave both boys the run of a small workshop in which to "invent".
From his early years, Bell showed a sensitive nature and a talent for art, poetry and music that was encouraged by his mother. With no formal training, he mastered the piano and became the family's pianist. Despite being normally quiet and introspective, he reveled in mimicry and "voice tricks" akin to ventriloquism that continually entertained family guests during their occasional visits. Bell was also deeply affected by his mother's gradual deafness, (she began to lose her hearing when he was 12) and learned a manual finger language so he could sit at her side and tap out silently the conversations swirling around the family parlour. He also developed a technique of speaking in clear, modulated tones directly into his mother's forehead wherein she would hear him with reasonable clarity. Bell's preoccupation with his mother's deafness led him to study acoustics.
His family was long associated with the teaching of elocution: his grandfather, Alexander Bell, in London, his uncle in Dublin, and his father, in Edinburgh, were all elocutionists. His father published a variety of works on the subject, several of which are still well known, especially his "The Standard Elocutionist" (1860), which appeared in Edinburgh in 1868. "The Standard Elocutionist" appeared in 168 British editions and sold over a quarter of a million copies in the United States alone. In this treatise, his father explains his methods of how to instruct deaf-mutes (as they were then known) to articulate words and read other people's lip movements to decipher meaning. Aleck's father taught him and his brothers not only to write Visible Speech but to identify any symbol and its accompanying sound. Aleck became so proficient that he became a part of his father's public demonstrations and astounded audiences with his abilities. He could decipher Visible Speech representing virtually every language, including Latin, Scottish Gaelic and even Sanskrit, accurately reciting written tracts without any prior knowledge of their pronunciation.
Education.
As a young child, Bell, like his brothers, received his early schooling at home from his father. At an early age, however, he was enrolled at the Royal High School, Edinburgh, Scotland, which he left at age 15, completing only the first four forms. His school record was undistinguished, marked by absenteeism and lacklustre grades. His main interest remained in the sciences, especially biology, while he treated other school subjects with indifference, to the dismay of his demanding father. Upon leaving school, Bell travelled to London to live with his grandfather, Alexander Bell. During the year he spent with his grandfather, a love of learning was born, with long hours spent in serious discussion and study. The elder Bell took great efforts to have his young pupil learn to speak clearly and with conviction, the attributes that his pupil would need to become a teacher himself. At age 16, Bell secured a position as a "pupil-teacher" of elocution and music, in Weston House Academy, at Elgin, Moray, Scotland. Although he was enrolled as a student in Latin and Greek, he instructed classes himself in return for board and £10 per session. The following year, he attended the University of Edinburgh; joining his older brother Melville who had enrolled there the previous year. In 1868, not long before he departed for Canada with his family, Aleck completed his matriculation exams and was accepted for admission to the University of London.
First experiments with sound.
Bell's father encouraged Aleck's interest in speech and, in 1863, took his sons to see a unique automaton, developed by Sir Charles Wheatstone based on the earlier work of Baron Wolfgang von Kempelen. The rudimentary "mechanical man" simulated a human voice. Aleck was fascinated by the machine and after he obtained a copy of von Kempelen's book, published in German, and had laboriously translated it, he and his older brother Melville built their own automaton head. Their father, highly interested in their project, offered to pay for any supplies and spurred the boys on with the enticement of a "big prize" if they were successful. While his brother constructed the throat and larynx, Aleck tackled the more difficult task of recreating a realistic skull. His efforts resulted in a remarkably lifelike head that could "speak", albeit only a few words. The boys would carefully adjust the "lips" and when a bellows forced air through the windpipe, a very recognizable "Mama" ensued, to the delight of neighbors who came to see the Bell invention.
Intrigued by the results of the automaton, Bell continued to experiment with a live subject, the family's Skye Terrier, "Trouve". After he taught it to growl continuously, Aleck would reach into its mouth and manipulate the dog's lips and vocal cords to produce a crude-sounding "Ow ah oo ga ma ma". With little convincing, visitors believed his dog could articulate "How are you grandma?" More indicative of his playful nature, his experiments convinced onlookers that they saw a "talking dog". However, these initial forays into experimentation with sound led Bell to undertake his first serious work on the transmission of sound, using tuning forks to explore resonance.
At the age of 19, he wrote a report on his work and sent it to philologist Alexander Ellis, a colleague of his father (who would later be portrayed as Professor Henry Higgins in "Pygmalion"). Ellis immediately wrote back indicating that the experiments were similar to existing work in Germany, and also lent Aleck a copy of Hermann von Helmholtz's work, "The Sensations of Tone as a Physiological Basis for the Theory of Music".
Dismayed to find that groundbreaking work had already been undertaken by Helmholtz who had conveyed vowel sounds by means of a similar tuning fork "contraption", he pored over the German scientist's book. Working from his own erroneous mistranslation of a French edition, Aleck fortuitously then made a deduction that would be the underpinning of all his future work on transmitting sound, reporting: "Without knowing much about the subject, it seemed to me that if vowel sounds could be produced by electrical means, so could consonants, so could articulate speech." He also later remarked: "I thought that Helmholtz had done it ... and that my failure was due only to my ignorance of electricity. It was a valuable blunder ... If I had been able to read German in those days, I might never have commenced my experiments!"
Family tragedy.
In 1865, when the Bell family moved to London, Bell returned to Weston House as an assistant master and, in his spare hours, continued experiments on sound using a minimum of laboratory equipment. Bell concentrated on experimenting with electricity to convey sound and later installed a telegraph wire from his room in Somerset College to that of a friend. Throughout late 1867, his health faltered mainly through exhaustion. His younger brother, Edward "Ted," was similarly bed-ridden, suffering from tuberculosis. While Bell recovered (by then referring to himself in correspondence as "A.G. Bell") and served the next year as an instructor at Somerset College, Bath, England, his brother's condition deteriorated. Edward would never recover. Upon his brother's death, Bell returned home in 1867. His older brother Melville had married and moved out. With aspirations to obtain a degree at the University College London, Bell considered his next years as preparation for the degree examinations, devoting his spare time at his family's residence to studying.
Helping his father in Visible Speech demonstrations and lectures brought Bell to Susanna E. Hull's private school for the deaf in South Kensington, London. His first two pupils were "deaf mute" girls who made remarkable progress under his tutelage. While his older brother seemed to achieve success on many fronts including opening his own elocution school, applying for a patent on an invention, and starting a family, Bell continued as a teacher. However, in May 1870, Melville died from complications due to tuberculosis, causing a family crisis. His father had also suffered a debilitating illness earlier in life and had been restored to health by a convalescence in Newfoundland. Bell's parents embarked upon a long-planned move when they realized that their remaining son was also sickly. Acting decisively, Alexander Melville Bell asked Bell to arrange for the sale of all the family property, conclude all of his brother's affairs (Bell took over his last student, curing a pronounced lisp), and join his father and mother in setting out for the "New World". Reluctantly, Bell also had to conclude a relationship with Marie Eccleston, who, as he had surmised, was not prepared to leave England with him.
Canada.
In 1870, at age 23, Bell, his brother's widow, Caroline (Margaret Ottaway), and his parents travelled on the SS "Nestorian" to Canada. After landing at Quebec City the Bells transferred to another steamer to Montreal and then boarded a train to Paris, Ontario, to stay with the Reverend Thomas Henderson, a family friend. After a brief stay with the Hendersons, the Bell family purchased a farm of at Tutelo Heights (now called Tutela Heights), near Brantford, Ontario. The property consisted of an orchard, large farm house, stable, pigsty, hen-house and a carriage house, which bordered the Grand River.
At the homestead, Bell set up his own workshop in the converted carriage house near to what he called his "dreaming place", a large hollow nestled in trees at the back of the property above the river. Despite his frail condition upon arriving in Canada, Bell found the climate and environs to his liking, and rapidly improved. He continued his interest in the study of the human voice and when he discovered the Six Nations Reserve across the river at Onondaga, he learned the Mohawk language and translated its unwritten vocabulary into Visible Speech symbols. For his work, Bell was awarded the title of Honorary Chief and participated in a ceremony where he donned a Mohawk headdress and danced traditional dances.
After setting up his workshop, Bell continued experiments based on Helmholtz's work with electricity and sound. He also modified a melodeon (a type of pump organ) so that it could transmit its music electrically over a distance. Once the family was settled in, both Bell and his father made plans to establish a teaching practice and in 1871, he accompanied his father to Montreal, where Melville was offered a position to teach his System of Visible Speech.
Work with the deaf.
Bell's father was invited by Sarah Fuller, principal of the Boston School for Deaf Mutes (which continues today as the public Horace Mann School for the Deaf), in Boston, Massachusetts, to introduce the Visible Speech System by providing training for Fuller's instructors, but he declined the post in favor of his son. Traveling to Boston in April 1871, Bell proved successful in training the school's instructors. He was subsequently asked to repeat the program at the American Asylum for Deaf-mutes in Hartford, Connecticut, and the Clarke School for the Deaf in Northampton, Massachusetts.
Returning home to Brantford after six months abroad, Bell continued his experiments with his "harmonic telegraph". The basic concept behind his device was that messages could be sent through a single wire if each message was transmitted at a different pitch, but work on both the transmitter and receiver was needed. 
Unsure of his future, he first contemplated returning to London to complete his studies, but decided to return to Boston as a teacher. His father helped him set up his private practice by contacting Gardiner Greene Hubbard, the president of the Clarke School for the Deaf for a recommendation. Teaching his father's system, in October 1872, Alexander Bell opened his "School of Vocal Physiology and Mechanics of Speech" in Boston, which attracted a large number of deaf pupils, with his first class numbering 30 students. While he was working as a private tutor, one of his most famous pupils was Helen Keller, who came to him as a young child unable to see, hear, or speak. She was later to say that Bell dedicated his life to the penetration of that "inhuman silence which separates and estranges." In 1893, Keller performed the sod-breaking ceremony for the construction of the new Bell's new Volta Bureau, dedicated to "the increase and diffusion of knowledge relating to the deaf".
Several influential people of the time, including Bell, viewed deafness as something that should be eradicated, and also believed that with resources and effort they could teach the deaf to speak and avoid the use of sign language, thus enabling their integration within the wider society from which many were often being excluded. However in several schools, children were mistreated, for example by having their hands tied behind their backs so they could not communicate by signing—the only language they knew—in an attempt to force them to attempt oral communication. Owing to his efforts to suppress the teaching of sign language, Bell is often viewed negatively by those embracing deaf culture.
Continuing experimentation.
In the following year, Bell became professor of Vocal Physiology and Elocution at the Boston University School of Oratory. During this period, he alternated between Boston and Brantford, spending summers in his Canadian home. At Boston University, Bell was "swept up" by the excitement engendered by the many scientists and inventors residing in the city. He continued his research in sound and endeavored to find a way to transmit musical notes and articulate speech, but although absorbed by his experiments, he found it difficult to devote enough time to experimentation. While days and evenings were occupied by his teaching and private classes, Bell began to stay awake late into the night, running experiment after experiment in rented facilities at his boarding house. Keeping "night owl" hours, he worried that his work would be discovered and took great pains to lock up his notebooks and laboratory equipment. Bell had a specially made table where he could place his notes and equipment inside a locking cover. Worse still, his health deteriorated as he suffered severe headaches. Returning to Boston in fall 1873, Bell made a fateful decision to concentrate on his experiments in sound.
Deciding to give up his lucrative private Boston practice, Bell retained only two students, six-year-old "Georgie" Sanders, deaf from birth, and 15-year-old Mabel Hubbard. Each pupil would play an important role in the next developments. George's father, Thomas Sanders, a wealthy businessman, offered Bell a place to stay in nearby Salem with Georgie's grandmother, complete with a room to "experiment". Although the offer was made by George's mother and followed the year-long arrangement in 1872 where her son and his nurse had moved to quarters next to Bell's boarding house, it was clear that Mr. Sanders was backing the proposal. The arrangement was for teacher and student to continue their work together, with free room and board thrown in. Mabel was a bright, attractive girl who was ten years Bell's junior, but became the object of his affection. Having lost her hearing after a near-fatal bout of scarlet fever close to her fifth birthday, she had learned to read lips but her father, Gardiner Greene Hubbard, Bell's benefactor and personal friend, wanted her to work directly with her teacher.
Telephone.
By 1874, Bell's initial work on the harmonic telegraph had entered a formative stage, with progress made both at his new Boston "laboratory" (a rented facility) and at his family home in Canada a big success. While working that summer in Brantford, Bell experimented with a "phonautograph", a pen-like machine that could draw shapes of sound waves on smoked glass by tracing their vibrations. Bell thought it might be possible to generate undulating electrical currents that corresponded to sound waves. Bell also thought that multiple metal reeds tuned to different frequencies like a harp would be able to convert the undulating currents back into sound. But he had no working model to demonstrate the feasibility of these ideas.
In 1874, telegraph message traffic was rapidly expanding and in the words of Western Union President William Orton, had become "the nervous system of commerce". Orton had contracted with inventors Thomas Edison and Elisha Gray to find a way to send multiple telegraph messages on each telegraph line to avoid the great cost of constructing new lines. When Bell mentioned to Gardiner Hubbard and Thomas Sanders that he was working on a method of sending multiple tones on a telegraph wire using a multi-reed device, the two wealthy patrons began to financially support Bell's experiments. Patent matters would be handled by Hubbard's patent attorney, Anthony Pollok.
In March 1875, Bell and Pollok visited the famous scientist Joseph Henry, who was then director of the Smithsonian Institution, and asked Henry's advice on the electrical multi-reed apparatus that Bell hoped would transmit the human voice by telegraph. Henry replied that Bell had "the germ of a great invention". When Bell said that he did not have the necessary knowledge, Henry replied, "Get it!" That declaration greatly encouraged Bell to keep trying, even though he did not have the equipment needed to continue his experiments, nor the ability to create a working model of his ideas. However, a chance meeting in 1874 between Bell and Thomas A. Watson, an experienced electrical designer and mechanic at the electrical machine shop of Charles Williams, changed all that.
With financial support from Sanders and Hubbard, Bell hired Thomas Watson as his assistant, and the two of them experimented with acoustic telegraphy. On June 2, 1875, Watson accidentally plucked one of the reeds and Bell, at the receiving end of the wire, heard the overtones of the reed; overtones that would be necessary for transmitting speech. That demonstrated to Bell that only one reed or armature was necessary, not multiple reeds. This led to the "gallows" sound-powered telephone, which could transmit indistinct, voice-like sounds, but not clear speech.
The race to the patent office.
In 1875, Bell developed an acoustic telegraph and drew up a patent application for it. Since he had agreed to share U.S. profits with his investors Gardiner Hubbard and Thomas Sanders, Bell requested that an associate in Ontario, George Brown, attempt to patent it in Britain, instructing his lawyers to apply for a patent in the U.S. only after they received word from Britain (Britain would issue patents only for discoveries not previously patented elsewhere).
Meanwhile, Elisha Gray was also experimenting with acoustic telegraphy and thought of a way to transmit speech using a water transmitter. On February 14, 1876, Gray filed a caveat with the U.S. Patent Office for a telephone design that used a water transmitter. That same morning, Bell's lawyer filed Bell's application with the patent office. There is considerable debate about who arrived first and Gray later challenged the primacy of Bell's patent. Bell was in Boston on February 14 and did not arrive in Washington until February 26.
Bell's patent 174,465, was issued to Bell on March 7, 1876, by the U.S. Patent Office. Bell's patent covered "the method of, and apparatus for, transmitting vocal or other sounds telegraphically ... by causing electrical undulations, similar in form to the vibrations of the air accompanying the said vocal or other sound" Bell returned to Boston the same day and the next day resumed work, drawing in his notebook a diagram similar to that in Gray's patent caveat.
On March 10, 1876, three days after his patent was issued, Bell succeeded in getting his telephone to work, using a liquid transmitter similar to Gray's design. Vibration of the diaphragm caused a needle to vibrate in the water, varying the electrical resistance in the circuit. When Bell spoke the famous sentence "Mr. Watson—Come here—I want to see you" into the liquid transmitter, Watson, listening at the receiving end in an adjoining room, heard the words clearly.
Although Bell was, and still is, accused of stealing the telephone from Gray, Bell used Gray's water transmitter design only after Bell's patent had been granted, and only as a proof of concept scientific experiment, to prove to his own satisfaction that intelligible "articulate speech" (Bell's words) could be electrically transmitted. After March 1876, Bell focused on improving the electromagnetic telephone and never used Gray's liquid transmitter in public demonstrations or commercial use.
The question of priority for the variable resistance feature of the telephone was raised by the examiner before he approved Bell's patent application. He told Bell that his claim for the variable resistance feature was also described in Gray's caveat. Bell pointed to a variable resistance device in Bell's previous application in which Bell described a cup of mercury, not water. Bell had filed the mercury application at the patent office a year earlier on February 25, 1875, long before Elisha Gray described the water device. In addition, Gray abandoned his caveat, and because he did not contest Bell's priority, the examiner approved Bell's patent on March 3, 1876. Gray had reinvented the variable resistance telephone, but Bell was the first to write down the idea and the first to test it in a telephone.
The patent examiner, Zenas Fisk Wilber, later stated in an affidavit that he was an alcoholic who was much in debt to Bell's lawyer, Marcellus Bailey, with whom he had served in the Civil War. He claimed he showed Gray's patent caveat to Bailey. Wilber also claimed (after Bell arrived in Washington D.C. from Boston) that he showed Gray's caveat to Bell and that Bell paid him $100. Bell claimed they discussed the patent only in general terms, although in a letter to Gray, Bell admitted that he learned some of the technical details. Bell denied in an affidavit that he ever gave Wilber any money.
Later developments.
Continuing his experiments in Brantford, Bell brought home a working model of his telephone. On August 3, 1876, from the telegraph office in Mount Pleasant five miles (eight km) away from Brantford, Bell sent a tentative telegram indicating that he was ready. With curious onlookers packed into the office as witnesses, faint voices were heard replying. The following night, he amazed guests as well as his family when a message was received at the Bell home from Brantford, four miles (six km) distant, along an improvised wire strung up along telegraph lines and fences, and laid through a tunnel. This time, guests at the household distinctly heard people in Brantford reading and singing. These experiments clearly proved that the telephone could work over long distances.
Bell and his partners, Hubbard and Sanders, offered to sell the patent outright to Western Union for $100,000. The president of Western Union balked, countering that the telephone was nothing but a toy. Two years later, he told colleagues that if he could get the patent for $25 million he would consider it a bargain. By then, the Bell company no longer wanted to sell the patent. Bell's investors would become millionaires, while he fared well from residuals and at one point had assets of nearly one million dollars.
Bell began a series of public demonstrations and lectures to introduce the new invention to the scientific community as well as the general public. A short time later, his demonstration of an early telephone prototype at the 1876 Centennial Exposition in Philadelphia brought the telephone to international attention. Influential visitors to the exhibition included Emperor Pedro II of Brazil. Later Bell had the opportunity to demonstrate the invention personally to Sir William Thomson (later, Lord Kelvin), a renowned Scottish scientist, as well as to Queen Victoria, who had requested a private audience at Osborne House, her Isle of Wight home. She called the demonstration "most extraordinary". The enthusiasm surrounding Bell's public displays laid the groundwork for universal acceptance of the revolutionary device.
The Bell Telephone Company was created in 1877, and by 1886, more than 150,000 people in the U.S. owned telephones. Bell Company engineers made numerous other improvements to the telephone, which emerged as one of the most successful products ever. In 1879, the Bell company acquired Edison's patents for the carbon microphone from Western Union. This made the telephone practical for longer distances, and it was no longer necessary to shout to be heard at the receiving telephone.
In January 1915, Bell made the first ceremonial transcontinental telephone call. Calling from the AT&T head office at 15 Dey Street in New York City, Bell was heard by Thomas Watson at 333 Grant Avenue in San Francisco. The "New York Times" reported:
Competitors.
As is sometimes common in scientific discoveries, simultaneous developments can occur, as evidenced by a number of inventors who were at work on the telephone. Over a period of 18 years, the Bell Telephone Company faced 587 court challenges to its patents, including five that went to the U.S. Supreme Court, but none was successful in establishing priority over the original Bell patent and the Bell Telephone Company never lost a case that had proceeded to a final trial stage. Bell's laboratory notes and family letters were the key to establishing a long lineage to his experiments. The Bell company lawyers successfully fought off myriad lawsuits generated initially around the challenges by Elisha Gray and Amos Dolbear. In personal correspondence to Bell, both Gray and Dolbear had acknowledged his prior work, which considerably weakened their later claims.
On January 13, 1887, the U,S. Government moved to annul the patent issued to Bell on the grounds of fraud and misrepresentation. After a series of decisions and reversals, the Bell company won a decision in the Supreme Court, though a couple of the original claims from the lower court cases were left undecided. By the time that the trial wound its way through nine years of legal battles, the U.S. prosecuting attorney had died and the two Bell patents (No. 174,465 and dated March 7, 1876 and No. 186,787 dated January 30, 1877) were no longer in effect, although the presiding judges agreed to continue the proceedings due to the case's importance as a "precedent". With a change in administration and charges of conflict of interest (on both sides) arising from the original trial, the US Attorney General dropped the lawsuit on November 30, 1897 leaving several issues undecided on the merits.
During a deposition filed for the 1887 trial, Italian inventor Antonio Meucci also claimed to have created the first working model of a telephone in Italy in 1834. In 1886, in the first of three cases in which he was involved, Meucci took the stand as a witness in the hopes of establishing his invention's priority. Meucci's evidence in this case was disputed due to a lack of material evidence for his inventions as his working models were purportedly lost at the laboratory of American District Telegraph (ADT) of New York, which was later incorporated as a subsidiary of Western Union in 1901. Meucci's work, like many other inventors of the period, was based on earlier acoustic principles and despite evidence of earlier experiments, the final case involving Meucci was eventually dropped upon Meucci's death. However, due to the efforts of Congressman Vito Fossella, the U.S. House of Representatives on June 11, 2002 stated that Meucci's "work in the invention of the telephone should be acknowledged", even though this did not put an end to a still contentious issue. Some modern scholars do not agree with the claims that Bell's work on the telephone was influenced by Meucci's inventions. 
The value of the Bell patent was acknowledged throughout the world, and patent applications were made in most major countries, but when Bell had delayed the German patent application, the electrical firm of Siemens & Halske (S&H) managed to set up a rival manufacturer of Bell telephones under their own patent. The Siemens company produced near-identical copies of the Bell telephone without having to pay royalties. The establishment of the International Bell Telephone Company in Brussels, Belgium in 1880, as well as a series of agreements in other countries eventually consolidated a global telephone operation. The strain put on Bell by his constant appearances in court, necessitated by the legal battles, eventually resulted in his resignation from the company.
Family life.
On July 11, 1877, a few days after the Bell Telephone Company was established, Bell married Mabel Hubbard (1857–1923) at the Hubbard estate in Cambridge, Massachusetts. His wedding present to his bride was to turn over 1,487 of his 1,497 shares in the newly formed Bell Telephone Company. Shortly thereafter, the newlyweds embarked on a year-long honeymoon in Europe. During that excursion, Alec took a handmade model of his telephone with him, making it a "working holiday". The courtship had begun years earlier; however, Alexander waited until he was more financially secure before marrying. Although the telephone appeared to be an "instant" success, it was not initially a profitable venture and Bell's main sources of income were from lectures until after 1897. One unusual request exacted by his fiancée was that he use "Alec" rather than the family's earlier familiar name of "Aleck". From 1876, he would sign his name "Alec Bell". They had four children: Elsie May Bell (1878–1964) who married Gilbert Grosvenor of National Geographic fame, Marian Hubbard Bell (1880–1962) who was referred to as "Daisy", and two sons who died in infancy (Edward in 1881 and Robert in 1883). The Bell family home was in Cambridge, Massachusetts, until 1880 when Bell's father-in-law bought a house in Washington, D.C., and later in 1882 bought a home in the same city for Bell's family, so that they could be with him while he attended to the numerous court cases involving patent disputes.
Bell was a British subject throughout his early life in Scotland and later in Canada until 1882, when he became a naturalized citizen of the United States. In 1915, he characterized his status as: "I am not one of those hyphenated Americans who claim allegiance to two countries." Despite this declaration, Bell has been proudly claimed as a "native son" by all three countries he resided in: the United States, Canada and the United Kingdom.
By 1885, a new summer retreat was contemplated. That summer, the Bells had a vacation on Cape Breton Island in Nova Scotia, spending time at the small village of Baddeck. Returning in 1886, Bell started building an estate on a point across from Baddeck, overlooking Bras d'Or Lake. By 1889, a large house, christened "The Lodge" was completed and two years later, a larger complex of buildings, including a new laboratory, were begun that the Bells would name Beinn Bhreagh (Gaelic: "beautiful mountain") after Alec's ancestral Scottish highlands. Bell also built the Bell Boatyard on the estate, employing up to 40 people building experimental craft as well as wartime lifeboats and workboats for the Royal Canadian Navy and pleasure craft for the Bell family. An enthusiastic boater, Bell and his family sailed a rowed a long series of vessels on Bras d'Or Lake, ordering additional vessels from the H.W. Embree and Sons boatyard in Port Hawkesbury, Nova Scotia. In his final, and some of his most productive years, Bell split his residency between Washington, D.C., where he and his family initially resided for most of the year, and at Beinn Bhreagh where they spent increasing amounts of time.
Until the end of his life, Bell and his family would alternate between the two homes, but "Beinn Bhreagh" would, over the next 30 years, become more than a summer home as Bell became so absorbed in his experiments that his annual stays lengthened. Both Mabel and Alec became immersed in the Baddeck community and were accepted by the villagers as "their own". The Bells were still in residence at "Beinn Bhreagh" when the Halifax Explosion occurred on December 6, 1917. Mabel and Alec mobilized the community to help victims in Halifax.
Later inventions.
Although Alexander Graham Bell is most often associated with the invention of the telephone, his interests were extremely varied. According to one of his biographers, Charlotte Gray, Bell's work ranged "unfettered across the scientific landscape" and he often went to bed voraciously reading the "Encyclopædia Britannica", scouring it for new areas of interest. The range of Bell's inventive genius is represented only in part by the 18 patents granted in his name alone and the 12 he shared with his collaborators. These included 14 for the telephone and telegraph, four for the photophone, one for the phonograph, five for aerial vehicles, four for "hydroairplanes" and two for selenium cells. Bell's inventions spanned a wide range of interests and included a metal jacket to assist in breathing, the audiometer to detect minor hearing problems, a device to locate icebergs, investigations on how to separate salt from seawater, and work on finding alternative fuels.
Bell worked extensively in medical research and invented techniques for teaching speech to the deaf. During his Volta Laboratory period, Bell and his associates considered impressing a magnetic field on a record as a means of reproducing sound. Although the trio briefly experimented with the concept, they could not develop a workable prototype. They abandoned the idea, never realizing they had glimpsed a basic principle which would one day find its application in the tape recorder, the hard disc and floppy disc drive and other magnetic media.
Bell's own home used a primitive form of air conditioning, in which fans blew currents of air across great blocks of ice. He also anticipated modern concerns with fuel shortages and industrial pollution. Methane gas, he reasoned, could be produced from the waste of farms and factories. At his Canadian estate in Nova Scotia, he experimented with composting toilets and devices to capture water from the atmosphere. In a magazine interview published shortly before his death, he reflected on the possibility of using solar panels to heat houses.
Photophone.
Bell and his assistant Charles Sumner Tainter jointly invented a wireless telephone, named a photophone, which allowed for the transmission of both sounds and normal human conversations on a beam of light. Both men later became full associates in the Volta Laboratory Association.
On June 21, 1880, Bell's assistant transmitted a wireless voice telephone message a considerable distance, from the roof of the Franklin School in Washington, D.C., to Bell at the window of his laboratory, some away, 19 years before the first voice radio transmissions.
Bell believed the photophone's principles were his life's "greatest achievement", telling a reporter shortly before his death that the photophone was "the greatest invention [I have] ever made, greater than the telephone". The photophone was a precursor to the fiber-optic communication systems which achieved popular worldwide usage in the 1980s. Its master patent was issued in December 1880, many decades before the photophone's principles came into popular use.
Metal detector.
Bell is also credited with the invention of the metal detector in 1881. The device was quickly put together in an attempt to find the bullet in the body of U.S. President James Garfield. According to some accounts, the metal detector worked flawlessly in tests but did not find the assassin's bullet partly because the metal bed frame on which the President was lying disturbed the instrument, resulting in static. The president's surgeons, who were skeptical of the device, ignored Bell's requests to move the president to a bed not fitted with metal springs. Alternatively, although Bell had detected a slight sound on his first test, the bullet may have been lodged too deeply to be detected by the crude apparatus.
Bell's own detailed account, presented to the American Association for the Advancement of Science in 1882, differs in several particulars from most of the many and varied versions now in circulation, most notably by concluding that extraneous metal was not to blame for failure to locate the bullet. Perplexed by the peculiar results he had obtained during an examination of Garfield, Bell "...proceeded to the Executive Mansion the next morning...to ascertain from the surgeons whether they were perfectly sure that all metal had been removed from the neighborhood of the bed. It was then recollected that underneath the horse-hair mattress on which the President lay was another mattress composed of steel wires. Upon obtaining a duplicate, the mattress was found to consist of a sort of net of woven steel wires, with large meshes. The extent of the [area that produced a response from the detector] having been so small, as compared with the area of the bed, it seemed reasonable to conclude that the steel mattress had produced no detrimental effect." In a footnote, Bell adds that "The death of President Garfield and the subsequent "post-mortem" examination, however, proved that the bullet was at too great a distance from the surface to have affected our apparatus."
Hydrofoils.
The March 1906 "Scientific American" article by American pioneer William E. Meacham explained the basic principle of hydrofoils and hydroplanes. Bell considered the invention of the hydroplane as a very significant achievement. Based on information gained from that article he began to sketch concepts of what is now called a hydrofoil boat. Bell and assistant Frederick W. "Casey" Baldwin began hydrofoil experimentation in the summer of 1908 as a possible aid to airplane takeoff from water. Baldwin studied the work of the Italian inventor Enrico Forlanini and began testing models. This led him and Bell to the development of practical hydrofoil watercraft.
During his world tour of 1910–11, Bell and Baldwin met with Forlanini in France. They had rides in the Forlanini hydrofoil boat over Lake Maggiore. Baldwin described it as being as smooth as flying. On returning to Baddeck, a number of initial concepts were built as experimental models, including the "Dhonnas Beag", the first self-propelled Bell-Baldwin hydrofoil. The experimental boats were essentially proof-of-concept prototypes that culminated in the more substantial HD-4, powered by Renault engines. A top speed of was achieved, with the hydrofoil exhibiting rapid acceleration, good stability and steering along with the ability to take waves without difficulty. In 1913, Dr. Bell hired Walter Pinaud, a Sydney yacht designer and builder as well as the proprietor of Pinaud's Yacht Yard in Westmount, Nova Scotia to work on the pontoons of the HD-4. Pinaud soon took over the boatyard at Bell Laboratories on Beinn Bhreagh, Bell's estate near Baddeck, Nova Scotia. Pinaud's experience in boat-building enabled him to make useful design changes to the HD-4. After the First World War, work began again on the HD-4. Bell's report to the U.S. Navy permitted him to obtain two engines in July 1919. On September 9, 1919, the HD-4 set a world marine speed record of , a record which stood for ten years.
Aeronautics.
In 1891, Bell had begun experiments to develop motor-powered heavier-than-air aircraft. The AEA was first formed as Bell shared the vision to fly with his wife, who advised him to seek "young" help as Alexander was at the graceful age of 60.
In 1898, Bell experimented with tetrahedral box kites and wings constructed of multiple compound tetrahedral kites covered in maroon silk. The tetrahedral wings were named "Cygnet" I, II and III, and were flown both unmanned and manned ("Cygnet I" crashed during a flight carrying Selfridge) in the period from 1907–1912. Some of Bell's kites are on display at the Alexander Graham Bell National Historic Site.
Bell was a supporter of aerospace engineering research through the Aerial Experiment Association (AEA), officially formed at Baddeck, Nova Scotia, in October 1907 at the suggestion of his wife Mabel and with her financial support after the sale of some of her real estate. The AEA was headed by Bell and the founding members were four young men: American Glenn H. Curtiss, a motorcycle manufacturer at the time and who held the title "world's fastest man", having ridden his self-constructed motor bicycle around in the shortest time, and who was later awarded the Scientific American Trophy for the first official one-kilometre flight in the Western hemisphere, and who later became a world-renowned airplane manufacturer; Lieutenant Thomas Selfridge, an official observer from the U.S. Federal government and the only person in the army who believed aviation was the future; Frederick W. Baldwin, the first Canadian and first British subject to pilot a public flight in Hammondsport, New York, and J.A.D. McCurdy —Baldwin and McCurdy being new engineering graduates from the University of Toronto.
The AEA's work progressed to heavier-than-air machines, applying their knowledge of kites to gliders. Moving to Hammondsport, the group then designed and built the "Red Wing", framed in bamboo and covered in red silk and powered by a small air-cooled engine. On March 12, 1908, over Keuka Lake, the biplane lifted off on the first public flight in North America. The innovations that were incorporated into this design included a cockpit enclosure and tail rudder (later variations on the original design would add ailerons as a means of control). One of the AEA's inventions, a practical wingtip form of the aileron, was to become a standard component on all aircraft. The "White Wing" and "June Bug" were to follow and by the end of 1908, over 150 flights without mishap had been accomplished. However, the AEA had depleted its initial reserves and only a $15,000 grant from Mrs. Bell allowed it to continue with experiments.
Their final aircraft design, the "Silver Dart" embodied all of the advancements found in the earlier machines. On February 23, 1909, Bell was present as the "Silver Dart" flown by J.A.D. McCurdy from the frozen ice of Bras d'Or, made the first aircraft flight in Canada. Bell had worried that the flight was too dangerous and had arranged for a doctor to be on hand. With the successful flight, the AEA disbanded and the "Silver Dart" would revert to Baldwin and McCurdy who began the Canadian Aerodrome Company and would later demonstrate the aircraft to the Canadian Army.
Eugenics.
Bell was connected with the eugenics movement in the United States. In his lecture "Memoir upon the formation of a deaf variety of the human race" presented to the National Academy of Sciences on November 13, 1883 he noted that congenitally deaf parents were more likely to produce deaf children and tentatively suggested that couples where both parties were deaf should not marry. However, it was his hobby of livestock breeding which led to his appointment to biologist David Starr Jordan's Committee on Eugenics, under the auspices of the American Breeders Association. The committee unequivocally extended the principle to man. From 1912 until 1918 he was the chairman of the board of scientific advisers to the Eugenics Record Office associated with Cold Spring Harbor Laboratory in New York, and regularly attended meetings. In 1921, he was the honorary president of the Second International Congress of Eugenics held under the auspices of the American Museum of Natural History in New York. Organisations such as these advocated passing laws (with success in some states) that established the compulsory sterilization of people deemed to be, as Bell called them, a "defective variety of the human race". By the late 1930s, about half the states in the U.S. had eugenics laws, and California's compulsory sterilization law was used as a model for that of Nazi Germany.
Legacy and honors.
Honors and tributes flowed to Bell in increasing numbers as his most famous invention became ubiquitous and his personal fame grew. Bell received numerous honorary degrees from colleges and universities, to the point that the requests almost became burdensome. During his life he also received dozens of major awards, medals and other tributes. These included statuary monuments to both him and the new form of communication his telephone created, notably the Bell Telephone Memorial erected in his honor in "Alexander Graham Bell Gardens" in Brantford, Ontario, in 1917.
A large number of Bell's writings, personal correspondence, notebooks, papers and other documents reside at both the United States Library of Congress Manuscript Division (as the "Alexander Graham Bell Family Papers"), and at the Alexander Graham Bell Institute, Cape Breton University, Nova Scotia; major portions of which are available for online viewing.
A number of historic sites and other marks commemorate Bell in North America and Europe, including the first telephone companies of the United States and Canada. Among the major sites are:
In 1880, Bell received the Volta Prize with a purse of 50,000 francs (approximately US$ in today's dollars) for the invention of the telephone from the Académie française, representing the French government. Among the luminaries who judged were Victor Hugo and Alexandre Dumas. The Volta Prize was conceived by Napoleon Bonaparte in 1801, and named in honor of Alessandro Volta, with Bell receiving the third grand prize in its history. Since Bell was becoming increasingly affluent, he used his prize money to create endowment funds (the 'Volta Fund') and institutions in and around the United States capital of Washington, D.C.. These included the prestigious" 'Volta Laboratory Association' "(1880), also known as the" Volta Laboratory "and as the" 'Alexander Graham Bell Laboratory', "and which eventually led to the Volta Bureau (1887) as a center for studies on deafness which is still in operation in Georgetown, Washington, D.C. The Volta Laboratory became an experimental facility devoted to scientific discovery, and the very next year it improved Edison's phonograph by substituting wax for tinfoil as the recording medium and incising the recording rather than indenting it, key upgrades that Edison himself later adopted. The laboratory was also the site where he and his associate invented his "proudest achievement", "the photophone", the "optical telephone" which presaged fibre optical telecommunications, while the Volta Bureau would later evolve into the Alexander Graham Bell Association for the Deaf and Hard of Hearing (the AG Bell), a leading center for the research and pedagogy of deafness.
In partnership with Gardiner Hubbard, Bell helped establish the publication Science during the early 1880s. In 1888, Bell was one of the founding members of the National Geographic Society and became its second president (1897–1904), and also became a Regent of the Smithsonian Institution (1898–1922). The French government conferred on him the decoration of the Légion d'honneur (Legion of Honor); the Royal Society of Arts in London awarded him the Albert Medal in 1902; the University of Würzburg, Bavaria, granted him a PhD, and he was awarded the Franklin Institute's Elliott Cresson Medal in 1912. He was one of the founders of the American Institute of Electrical Engineers in 1884, and served as its president from 1891–92. Bell was later awarded the AIEE's Edison Medal in 1914 "For meritorious achievement in the invention of the telephone".
The "bel" (B) and the smaller "decibel" (dB) are units of measurement of sound intensity invented by Bell Labs and named after him. Since 1976 the IEEE's Alexander Graham Bell Medal has been awarded to honor outstanding contributions in the field of telecommunications.
In 1936 the US Patent Office declared Bell first on its list of the country's greatest inventors, leading to the US Post Office issuing a commemorative stamp honoring Bell in 1940 as part of its 'Famous Americans Series'. The First Day of Issue ceremony was held on October 28 in Boston, Massachusetts, the city where Bell spent considerable time on research and working with the deaf. The Bell stamp became very popular and sold out in little time. The stamp became, and remains to this day, the most valuable one of the series.
The 150th anniversary of Bell's birth in 1997 was marked by a special issue of commemorative £1 banknotes from the Royal Bank of Scotland. The illustrations on the reverse of the note include Bell's face in profile, his signature, and objects from Bell's life and career: users of the telephone over the ages; an audio wave signal; a diagram of a telephone receiver; geometric shapes from engineering structures; representations of sign language and the phonetic alphabet; the geese which helped him to understand flight; and the sheep which he studied to understand genetics. Additionally, the Government of Canada honored Bell in 1997 with a C$100 gold coin, in tribute also to the 150th anniversary of his birth, and with a silver dollar coin in 2009 in honor of the 100th anniversary of flight in Canada. That first flight was made by an airplane designed under Dr. Bell's tutelage, named the Silver Dart. Bell's image, and also those of his many inventions have graced paper money, coinage and postal stamps in numerous countries worldwide for many dozens of years.
Alexander Graham Bell was ranked 57th among the 100 Greatest Britons (2002) in an official BBC nationwide poll, and among the Top Ten Greatest Canadians (2004), and the 100 Greatest Americans (2005). In 2006 Bell was also named as one of the 10 greatest Scottish scientists in history after having been listed in the National Library of Scotland's 'Scottish Science Hall of Fame'. Bell's name is still widely known and used as part of the names of dozens of educational institutes, corporate namesakes, street and place names around the world.
Honorary degrees.
Alexander Graham Bell, who could not complete the university program of his youth, received at least a dozen honorary degrees from academic institutions, including eight honorary LL.D.s (Doctorate of Laws), two Ph.D.s, a D.Sc. and an M.D.: 
Death.
Bell died of complications arising from diabetes on August 2, 1922, at his private estate, Beinn Bhreagh, Nova Scotia, at age 75. Bell had also been afflicted with pernicious anemia. His last view of the land he had inhabited was by moonlight on his mountain estate at 2:00 A.M. While tending to him after his long illness, Mabel, his wife, whispered, "Don't leave me." By way of reply, Bell traced the sign for "no" in the air —and then he died.
On learning of Bell's death, the Canadian Prime Minister, Mackenzie King, cabled Mrs. Bell, saying:
"[The Government expresses] to you our sense of the world's loss in the death of your distinguished husband. It will ever be a source of pride to our country that the great invention, with which his name is immortally associated, is a part of its history. On the behalf of the citizens of Canada, may I extend to you an expression of our combined gratitude and sympathy.
Bell's coffin was constructed of Beinn Bhreagh pine by his laboratory staff, lined with the same red silk fabric used in his tetrahedral kite experiments. To help celebrate his life, his wife asked guests not to wear black (the traditional funeral color) while attending his service, during which soloist Jean MacDonald sang a verse of Robert Louis Stevenson's "Requiem":
"Under a wide and starry sky,<br>
"Dig the grave and let me lie.<br>
"Glad did I live and gladly die<br>
"And I laid me down with a will.
Upon the conclusion of Bell's funeral, "every phone on the continent of North America was silenced in honor of the man who had given to mankind the means for direct communication at a distance".
Dr. Alexander Graham Bell was buried atop Beinn Bhreagh mountain, on his estate where he had resided increasingly for the last 35 years of his life, overlooking Bras d'Or Lake. He was survived by his wife Mabel, his two daughters, Elsie May and Marian, and nine of his grandchildren.
External links.
Patents.
"U.S. patent images in TIFF format"
Multimedia.
 

</doc>
<doc id="854" url="http://en.wikipedia.org/wiki?curid=854" title="Anatolia">
Anatolia

Anatolia (from Greek , ' — "east" or "(sun)rise"; in modern ), in geography known as Asia Minor (from ' — "small Asia"), Asian Turkey, Anatolian peninsula, or Anatolian plateau, denotes the westernmost protrusion of Asia, which makes up the majority of the Republic of Turkey.
The region is bounded by the Black Sea to the north, the Mediterranean Sea to the south, and the Aegean Sea to the west. The Sea of Marmara forms a connection between the Black and Aegean Seas through the Bosphorus and Dardanelles straits and separates Anatolia from Thrace on the European mainland.
Traditionally, Anatolia is considered to extend in the east to a line between the Gulf of İskenderun and the Black Sea, or to what is historically known as the Armenian Highlands (Armenia Major). That would approximately correspond to the western two-thirds of the Asian part of Turkey. However, since Anatolia is now often considered to be synonymous with Asian Turkey, almost the entire country, its eastern and southeastern borders are widely taken to be the Turkish borders with the neighboring countries, which are Georgia, Armenia, Azerbaijan, Iran, Iraq, and Syria, in clockwise direction.
Definition.
The Anatolian peninsula, also called Asia Minor, is bounded by the Black Sea to the north, the Mediterranean Sea to the south, the Aegean Sea to the west, and the Sea of Marmara to the northwest, which separates Anatolia from Thrace in Europe.
Traditionally, Anatolia is considered to extend in the east to an indefinite line running from the Gulf of İskenderun to the Black Sea, coterminous with the Anatolian Plateau. This traditional geographical definition is used, for example, in the latest edition of "Merriam-Webster's Geographical Dictionary", as well as the archeological community. Under this definition, Anatolia is bounded to the East by the Armenian Highland, and the Euphrates before that river bends to the southeast to enter Mesopotamia. To the southeast, it is bounded by the ranges that separate it from the Orontes valley in Syria (region) and the Mesopotamian plain.
However, following the establishment of the Republic of Turkey, Anatolia was defined by the Turkish government as being effectively co-terminous with Asian Turkey. Turkey's First Geography Congress in 1941 created two regions to the east of the Gulf of Iskenderun-Black Sea line named the Eastern Anatolia Region and the Southeastern Anatolia Region, the former largely corresponding to the western part of the Armenian Highland, the latter to the northern part of the Mesopotamian plain. This wider definition of Anatolia has gained widespread currency outside of Turkey and has, for instance, been adopted by "Encyclopædia Britannica" and other encyclopedic and general reference publications.
Etymology.
The oldest known reference to Anatolia – as “Land of the Hatti” – was found on Mesopotamian cuneiform tablets from the period of the Akkadian Empire (2350–2150 BC). The first name the Greeks used for the Anatolian peninsula was Ἀσία (Asía), presumably after the name of the Assuwa league in western Anatolia. As the name of Asia came to be extended to other areas east of the Mediterranean, the name for Anatolia was specified as Μικρὰ Ἀσία ("Mikrá Asía") or Asia Minor, meaning “Lesser Asia”, in Late Antiquity.
The name "Anatolia" derives from the Greek ("") meaning “the East” or more literally “sunrise”, comparable to the Latin derived terms “levant” and “orient”. The precise reference of this term has varied over time, perhaps originally referring to the Aeolian, Ionian and Dorian colonies on the west coast of Asia Minor. In the Byzantine Empire, the Anatolic Theme (Aνατολικόν θέμα) was a "theme" covering the western and central parts of Turkey’s present-day Central Anatolia Region. The modern Turkish form of Anatolia is "Anadolu", which again derives from the Greek name Aνατολή ("Anatolḗ"). The Russian male name Anatoly and the French Anatole share the same linguistic origin.
In English the name of "Turkey" for ancient Anatolia first appeared c. 1369. It is derived from the Medieval Latin "Turchia" (meaning “Land of the Turks”, Turkish "Türkiye"), which was originally used by the Europeans to define the Seljuk controlled parts of Anatolia after the Battle of Manzikert.
History.
Prehistory and antiquity.
Human habitation in Anatolia dates back to the Paleolithic. Neolithic Anatolia has been proposed as the homeland of the Indo-European language family, although linguists tend to favour a later origin in the steppes north of the Black Sea. However, it is clear that the Indo-European Anatolian languages have been spoken in Anatolia since at least the 19th century BC.
Eastern Anatolia contains the oldest known monumental structures. Those at Göbekli Tepe, for example, were built by hunter-gatherers a thousand years before the development of agriculture. Eastern Anatolia, alongside Mesopotamia and the Levant, was a heartland of the Neolithic Revolution, one of the earliest areas in which humans domesticated plants and animals. Neolithic sites such as Çatalhöyük, Çayönü, Nevalı Çori and Hacilar represent the world's oldest known agricultural towns.
The earliest historical records of Anatolia stem from the southeast of the region and are from the Mesopotamia-based Akkadian Empire during the reign of Sargon of Akkad in the 24th century BC. Scholars generally believe the earliest indigenous populations of Anatolia were the Hattians and Hurrians. The Hattians spoke a language of unclear affiliation, and the Hurrian language belongs to a small family called Hurro-Urartian, all these languages now being extinct; relationships with indigenous languages of the Caucasus have been proposed but are not generally accepted. The region was famous for exporting raw materials, and areas of Hattian- and Hurrian-populated southeast Anatolia were colonised by the Akkadians.
After the fall of the Akkadian empire in the mid-21st century BC, the Assyrians, who were the northern branch of the Akkadian people, colonised parts of the region between the 21st and mid-18th centuries BC and claimed its resources, notably silver. One of the numerous cuneiform records dated circa 20th century BC, found in Anatolia at the Assyrian colony of Kanesh, uses an advanced system of trading computations and credit lines.
Unlike the Semitic Akkadians and their descendants, the Assyrians, whose Anatolian possessions were peripheral to their core lands in Mesopotamia, the Hittites were centred at Hattusa in north-central Anatolia by 2000 BC. They were speakers of an Indo-European language known as the "language of Nesa". Originating from Nesa, they conquered Hattusa in the 18th century BC, imposing themselves over Hattian- and Hurrian-speaking populations.
The Hittites adopted the cuneiform written script, invented in Mesopotamia. During the Late Bronze Age circa 2000 BC, they created an empire, the Hittite New Kingdom, which reached its height in the 14th century BC, controlling much of Asia Minor. The empire included a large part of Anatolia, northwestern Syria and northwest upper Mesopotamia. They failed to reach the Anatolian coasts of the Black Sea, however, as another non-Indo-European people, the Kaskians, had established a kingdom there in the 17th century BC, displacing earlier Palaic speaking Indo-Europeans. Much of the history of the Hittite Empire concerned war with the rival empires of Egypt, Assyria and the Mitanni.
The Mitanni Empire was also an Indo-European (and Hurrian)-speaking and Anatolian-based empire. The Mitanni appeared in the 17th century BC and spoke an Indo-Aryan language related to the Indo-European languages eventually to be found in India.
The Egyptians eventually withdrew from the region after failing to gain the upper hand over the Hittites and becoming wary of the power of Assyria, which had destroyed the Mitanni Empire. The Assyrians and Hittites were then left to battle over control of eastern and southern Anatolia and colonial territories in Syria. The Assyrians had better success than the Egyptians, annexing much Hittite (and Hurrian) territory in these regions.
After 1180 BC, the Hittite empire disintegrated into several independent "Neo-Hittite" states, subsequent to losing much territory to the Middle Assyrian Empire and being finally overrun by the Phrygians, another Indo-European people who are believed to have migrated from the Balkans. The Phrygian expansion into southeast Anatolia was eventually halted by the Assyrians, who controlled that region.
Ancient Anatolia is subdivided by modern scholars into regions named after the Indo-European (and largely Hittite, Luwian or Greek speaking) peoples that occupied them, such as Lydia, Lycia, Caria, Mysia, Bithynia, Phrygia, Galatia, Lycaonia, Pisidia, Paphlagonia, Cilicia, and Cappadocia.
Semitic Arameans encroached over the borders of south central Anatolia in the century or so after the fall of the Hittite empire, and some of the Neo-Hittite states in this region became an amalgam of Hittites and Arameans. These became known as Syro-Hittite states.
In central and western Anatolia, another Indo-European people, the Luwians, came to the fore, circa 2000 BC. Their language was closely related to Hittite. The general consensus amongst scholars is that Luwian was spoken—to a greater or lesser degree—across a large area of western Anatolia, including (possibly) Wilusa (Troy), the Seha River Land (to be identified with the Hermos and/or Kaikos valley), and the kingdom of Mira-Kuwaliya with its core territory of the Maeander valley. From the 9th century BC, Luwian regions coalesced into a number of states such as Lydia, Caria and Lycia, all of which had Hellenic influence.
The north western coasts of Anatolia were inhabited by Greeks of the Achaean/Mycenaean culture from the 20th century BC, related to the Greeks of south eastern Europe and the Aegean.
Beginning with the Bronze Age collapse at the end of the 2nd millennium BC, the west coast of Anatolia was settled by Ionian Greeks, usurping the area of the related but earlier Mycenaean Greeks. Over several centuries, numerous Ancient Greek city-states were established on the coasts of Anatolia. Greeks started Western philosophy on the western coast of Anatolia (Pre-Socratic philosophy).
Hurrian kingdoms, such as Nairi and the powerful state of Urartu arose in northeastern Anatolia from the 10th century BC, before eventually falling to the Assyrians. During the same period the Georgian states of Colchis and Tabal arose around the Black Sea and central Anatolia respectively.
From the late 8th century BC, a new wave of Indo-European-speaking raiders entered northern and northeast Anatolia: the Cimmerians and Scythians. The Cimmerians overran Phrygia and the Scythians threatened to do the same to Urartu and Lydia, before both were finally checked by the Assyrians.
From the 10th to late 7th centuries BC, much of Anatolia (particularly the east, central, southwestern and southeastern regions) fell to the Neo Assyrian Empire, including all of the Neo-Hittite and Syro-Hittite states, Phrygia, Urartu, Nairi, Tabal, Cilicia, Commagene, Caria, Lydia, the Cimmerians and Scythians and swathes of Cappadocia.
The Assyrian empire collapsed due to a bitter series of civil wars followed by a combined attack by Medes, Persians, Scythians and their own Babylonian relations. The last Assyrian city to fall was Harran in southeast Anatolia. This city was the birthplace of the last king of Babylon, the Assyrian Nabonidus and his son and regent Belshazzar. Much of the region then fell to the short-lived Iran-based Median Empire, with the Babylonians and Scythians briefly appropriating some territory.
Anatolia is known as the birthplace of minted coinage (as opposed to unminted coinage, which first appears in Mesopotamia at a much earlier date) as a medium of exchange, some time in the 7th century BC in Lydia. The use of minted coins continued to flourish during the Greek and Roman eras.
During the 6th century BC, most of Anatolia was conquered by the Persian Achaemenid Empire, the Persians having usurped the Medes as the dominant dynasty in Iran. Also in the 6th century BC, the Indo-European Armenians founded the Orontid Dynasty in Urartu. In 499 BC, the Ionian city-states on the west coast of Anatolia rebelled against Persian rule. The Ionian Revolt, as it became known, initiated the Greco-Persian Wars, which ended in a Greek victory in 449 BC, and the Ionian cities regained their independence.
In 334 BC, the Macedonian Greek king Alexander the Great conquered the peninsula. Alexander's conquest opened up the interior of Asia Minor to Greek settlement and influence. Following the death of Alexander and the breakup of his empire, Anatolia was ruled by a series of Hellenistic kingdoms, such as the Attalids of Pergamum and the Seleucids, the latter controlling most of Anatolia. A period of peaceful Hellenization followed, such that the local Anatolian languages had been supplanted by Greek by the 1st century BC. In 133 BC the last Attalid king bequeathed his kingdom to the Roman Republic, and western and central Anatolia came under Roman control, but Hellenistic culture remained predominant. During the 1st century BC the Armenians established the powerful Armenian kingdom under Tigran who reigned throughout much of eastern Anatolia between the Caspian, Black, and Mediterranean seas. Areas of the southeast such as Harran and the Hakkari mountains continued to be inhabited by remnants of the Assyrians, but these regions remained under Parthian and then Sassanid Persian rule.
Medieval and Renaissance periods.
After the division of the Roman Empire, Anatolia became part of the East Roman, or Byzantine Empire. Anatolia was one of the first places where Christianity spread, so that by the 4th century AD, western and central Anatolia were overwhelmingly Christian and Greek-speaking. For the next 600 years, while Imperial possessions in Europe were subjected to barbarian invasions, Anatolia would be the center of the Hellenic world. Byzantine control was challenged by Arab raids starting in the 8th century (see Byzantine–Arab Wars), but in the 9th and 10th century a resurgent Byzantine Empire regained its lost territories and even expanded beyond its traditional borders, into Armenia and Syria (ancient Aram). In the 10 years following the Battle of Manzikert in 1071, the Seljuk Turks from Central Asia established themselves over large areas of Anatolia, with particular concentrations around the north western rim. The Turkish language and the Islamic religion were gradually introduced as a result of the Seljuk conquest, and this period marks the start of Anatolia's slow transition from predominantly Christian and Greek-speaking, to predominantly Muslim and Turkish-speaking (although some ethnic groups such as Armenians, Greeks, Assyrians remained numerous and retained Christianity and their native languages). In the following century, the Byzantines managed to reassert their control in western and northern Anatolia. Control of Anatolia was then split between the Byzantine Empire and the Seljuk Sultanate of Rûm, with the Byzantine holdings gradually being reduced. In 1255, the Mongols swept through eastern and central Anatolia, and would remain until 1335. The Ilkhanate garrison was stationed near Ankara.
By the end of the 14th century, most of Anatolia was controlled by various Anatolian beyliks. Smyrna fell in 1330, and the last Byzantine stronghold in Anatolia, Philadelphia, fell in 1390. The Turkmen Beyliks were under the control of the Mongols, at least nominally, through declining Seljuk Sultans. The Beyliks did not mint coins in the names of their own leaders while they remained under the suzerainty of the Mongol Ilkhanids. The Osmanli ruler Osman I was the first Turkish ruler who minted coins in his own name in 1320s, for it bears the legend "Minted by Osman son of Ertugul". Since the minting of coins was a prerogative accorded in Islamic practice only to a sovereign, it can be considered that Osmanli became formally independent from the Mongol Khans.
After the decline of the Ilkhanate from 1335–1353, the Mongol Empire's legacy in the region was the Uyghur Eretna Dynasty that was overthrown by Kadi Burhan al-Din in 1381. Among the Turkmen leaders the Ottomans emerged as great power under Osman and his son Orhan I. The Anatolian beyliks were in turn absorbed into the rising Ottoman Empire during the 15th century. The Ottomans completed the conquest of the peninsula in 1517 with the taking of Halicarnassus (modern Bodrum) from the Knights of Saint John.
Modern times.
With the beginning of the slow decline of the Ottoman Empire in the early 19th century, and as a result of the expansionist policies of Czarist Russia in the Caucasus, many Muslim nations and groups in that region, mainly Circassians, Tatars, Azeris, Lezgis, Chechens and several Turkic groups left their ancestral homelands and settled in Anatolia. As the Ottoman Empire further shrank in the Balkan regions and then fragmented during the Balkan Wars, much of the non-Christian populations of its former possessions, mainly Balkan Muslims (Bosnians, Albanians, Turks, Muslim Bulgarians and Greek Muslims such as the Vallahades from Greek Macedonia, Bulgaria, Northern Macedonia), were resettled in various parts of Anatolia, mostly in formerly Christian villages throughout Anatolia.
A continuous reverse migration occurred since the early 19th century, when Greeks from Anatolia, Constantinopole and Pontus area migrated toward the newly independent Kingdom of Greece, and also towards the United States, southern part of the Russian Empire, Latin America and rest of Europe. Following the Treaty of Turkmenchay (1828) and the incorporation of the Eastern Armenia into the Russian Empire, another reverse migration involved the large Armenian population of Anatolia, which recorded significant migration rates from Western Armenia (Eastern Anatolia) toward the Russian Empire, especially toward the newly established Armenian provinces of the empire.
Anatolia remained multi-ethnic until the early 20th century (see the rise of nationalism under the Ottoman Empire). During World War I, the Armenian Genocide, the Greek genocide (especially in Pontus), and the Assyrian Genocide almost entirely removed the ancient indigenous communities of Armenian and Assyrian populations in Anatolia, as well as a large part of its ethnic Greek population. Following the Greco-Turkish War of 1919-1922, all remaining ethnic Anatolian Greeks were forced out during the 1923 population exchange between Greece and Turkey. Since the foundation of the Republic of Turkey in 1923, Anatolia has become Turkey, its inhabitants being mainly Turks and Kurds (see demographics of Turkey and history of Turkey).
Geography.
Geology.
Anatolia's terrain is structurally complex. A central massif composed of uplifted blocks and downfolded troughs, covered by recent deposits and giving the appearance of a plateau with rough terrain, is wedged between two folded mountain ranges that converge in the east. True lowland is confined to a few narrow coastal strips along the Aegean, Mediterranean, and Black Sea coasts. Flat or gently sloping land is rare and largely confined to the deltas of the Kızıl River, the coastal plains of Çukurova and the valley floors of the Gediz River and the Büyük Menderes River as well as some interior high plains in Anatolia, mainly around Lake Tuz (Salt Lake) and the Konya Basin ("Konya Ovasi").
Climate.
Anatolia has a varied range of climates. The central plateau is characterized by a continental climate, with hot summers and cold snowy winters. The south and west coasts enjoy a typical Mediterranean climate, with mild rainy winters, and warm dry summers. The Black Sea and Marmara coasts have temperate oceanic climate, with cool foggy summers and much rainfall throughout the year.
Ecoregions.
There is a diverse number of plant and animal communities.
The mountains and coastal plain of northern Anatolia experiences humid and mild climate. There are temperate broadleaf, mixed and coniferous forests. The central and eastern plateau, with its drier continental climate, has deciduous forests and forest steppes. Western and southern Anatolia, which have a Mediterranean climate, contain Mediterranean forests, woodlands, and scrub ecoregions.
Demographics.
Almost 80% of the people currently residing in Anatolia are Turks. Kurds constitute a major community in southeastern Anatolia, and are the largest ethnic minority. Abkhazians, Albanians, Arabs, Arameans, Armenians, Assyrians, Azerbaijanis, Bosniaks, Circassians, Gagauz, Georgians, Serbs, Greeks, Hemshin, Jews, Laz, Levantines, Pomaks, Zazas and a number of other ethnic groups also live in Anatolia in smaller numbers.

</doc>
<doc id="856" url="http://en.wikipedia.org/wiki?curid=856" title="Apple Inc.">
Apple Inc.

Apple Inc. is an American multinational corporation headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, online services, and personal computers. Its best-known hardware products are the Mac line of computers, the iPod media player, the iPhone smartphone, and the iPad tablet computer. Its online services include iCloud, iTunes Store, and App Store. Apple's consumer software includes the OS X and iOS operating systems, the iTunes media browser, the Safari web browser, and the iLife and iWork creativity and productivity suites.
Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne on April 1, 1976, to develop and sell personal computers. It was incorporated as Apple Computer, Inc. on January 3, 1977, and was renamed as Apple Inc. on January 9, 2007, to reflect its shifted focus towards consumer electronics.
Apple is the world's second-largest information technology company by revenue after Samsung Electronics, and the world's second-largest mobile phone maker after Samsung. "Fortune" magazine named Apple the most admired company in the United States in 2008, and in the world from 2008 to 2012. On September 30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand in the Omnicom Group's "Best Global Brands" report. However, the company has received criticism for its contractors' labor practices, as well as for its own environmental and business practices.
As of June 2014, Apple maintains 425 retail stores in fourteen countries, as well as the online Apple Store and iTunes Store, the latter of which is the world's largest music retailer. Apple is the largest publicly traded corporation in the world by market capitalization, with an estimated market capitalization of $446 billion by January 2014. As of September 29, 2012, the company had 72,800 permanent full-time employees and 3,300 temporary full-time employees worldwide. Its worldwide annual revenue in 2013 totaled $170 billion. As of Q1 2014, Apple's five-year growth average is 39% for top line growth and 45% for bottom line growth. In May 2013, Apple entered the top ten of the Fortune 500 list of companies for the first time, rising 11 places above its 2012 ranking to take the sixth position.
In 2014, according to the Interbrand ranking, Apple is the world's most valuable brand with a valuation of 118.9 billion. . 
History.
1976–80: Founding and incorporation.
Apple was established on April 1, 1976, by Steve Jobs, Steve Wozniak and Ronald Wayne to sell the Apple I personal computer kit. The Apple I kits were a computer single handedly designed and hand-built by Wozniak and first shown to the public at the Homebrew Computer Club. The Apple I was sold as a motherboard (with CPU, RAM, and basic textual-video chips), which is less than what is today considered a complete personal computer. The Apple I went on sale in July 1976 and was market-priced at $666.66 ($ in 2015 dollars, adjusted for inflation).
Apple was incorporated January 3, 1977, without Wayne, who sold his share of the company back to Jobs and Wozniak for $800. Multimillionaire Mike Markkula provided essential business expertise and funding of $250,000 during the incorporation of Apple.
During the first five years of operations, revenues doubled every four months, an average growth rate of 700%.
The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at the first West Coast Computer Faire. It differed from its major rivals, the TRS-80 and Commodore PET, because of its character cell-based color graphics and an open architecture. While early models used ordinary cassette tapes as storage devices, they were superseded by the introduction of a 5 1/4 inch floppy disk drive and interface, the Disk II.
The Apple II was chosen to be the desktop platform for the first "killer app" of the business world, VisiCalc, a spreadsheet program. VisiCalc created a business market for the Apple II and gave home users compatibility with the office, an additional reason to buy an Apple II. Apple was a distant third place to Commodore and Tandy until VisiCalc came along.
By the end of the 1970s, Apple had a staff of computer designers and a production line. The company introduced the Apple III in May 1980 in an attempt to compete with IBM and Microsoft in the business and corporate computing market.
Jobs and several Apple employees, including Jef Raskin, visited Xerox PARC in December 1979 to see the Xerox Alto. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares (800,000 split-adjusted shares) of Apple at the pre-IPO price of $10 a share. Jobs was immediately convinced that all future computers would use a graphical user interface (GUI), and development of a GUI began for the Apple Lisa.
On December 12, 1980, Apple went public at $22 per share, generating more capital than any IPO since Ford Motor Company in 1956 and instantly creating more millionaires (about 300) than any company in history.
1981–89: Success with Macintosh.
Apple began working on the Apple Lisa in 1978. In 1982, Jobs was pushed from the Lisa team due to infighting. Jobs took over Jef Raskin's low-cost-computer project, the Macintosh. A race broke out between the Lisa team and the Macintosh team over which product would ship first. Lisa won the race in 1983 and became the first personal computer sold to the public with a GUI, but was a commercial failure due to its high price tag and limited software titles.
In 1984, Apple launched the Macintosh. Its debut was announced by the now famous $1.5 million television commercial "1984". It was directed by Ridley Scott and was aired during the third quarter of Super Bowl XVIII on January 22, 1984. It is now hailed as a watershed event for Apple's success and a "masterpiece".
The Macintosh initially sold well, but follow-up sales were not strong due to its high price and limited range of software titles. The Macintosh was the first personal computer to be sold without a programming language at all.
The machine's fortunes changed with the introduction of the LaserWriter, the first PostScript laser printer to be sold at a reasonable price, and PageMaker, an early desktop publishing package. It has been suggested that the combination of these three products was responsible for the creation of the desktop publishing market. The Mac was particularly powerful in the desktop publishing market due to its advanced graphics capabilities, which had necessarily been built in to create the intuitive Macintosh GUI.
In 1985 a power struggle developed between Jobs and CEO John Sculley, who had been hired two years earlier. The Apple board of directors instructed Sculley to "contain" Jobs and limit his ability to launch expensive forays into untested products. Rather than submit to Sculley's direction, Jobs attempted to oust him from his leadership role at Apple. Sculley found out that Jobs had been attempting to organize a coup and called a board meeting at which Apple's board of directors sided with Sculley and removed Jobs from his managerial duties. Jobs resigned from Apple and founded NeXT Inc. the same year.
The Macintosh Portable was introduced in 1989 and was designed to be just as powerful as a desktop Macintosh, but weighed a bulky with a 12-hour battery life. After the Macintosh Portable, Apple introduced the PowerBook in 1991. The same year, Apple introduced System 7, a major upgrade to the operating system which added color to the interface and introduced new networking capabilities. It remained the architectural basis for Mac OS until 2001.
The success of the PowerBook and other products brought increasing revenue. For some time, Apple was doing incredibly well, introducing fresh new products and generating increasing profits in the process. The magazine "MacAddict" named the period between 1989 and 1991 as the "first golden age" of the Macintosh.
Following the success of the Macintosh LC, Apple introduced the Centris line, a low-end Quadra, and the ill-fated Performa line that was sold with an overwhelming number of configurations and software bundles to avoid competing with the various consumer outlets such as Sears, Price Club, and Wal-Mart (the primary dealers for these models). Consumers ended up confused and did not understand the difference between models.
1990–99: Decline and restructuring.
During this time Apple experimented with a number of other failed consumer targeted products including digital cameras, portable CD audio players, speakers, video consoles, and TV appliances. Enormous resources were also invested in the problem-plagued Newton division based on John Sculley's unrealistic market forecasts. Ultimately, none of these products helped, as Apple's market share and stock prices continued to slide.
Apple saw the Apple II series as too expensive to produce, while taking away sales from the low-end Macintosh. In 1990, Apple released the Macintosh LC with a single expansion slot for the Apple IIe Card to migrate Apple II users to the Macintosh platform. Apple stopped selling the Apple IIe in 1993.
Microsoft continued to gain market share with Windows focusing on delivering software to cheap commodity personal computers while Apple was delivering a richly engineered, but expensive, experience. Apple relied on high profit margins and never developed a clear response. Instead, they sued Microsoft for using a graphical user interface similar to the Apple Lisa in "Apple Computer, Inc. v. Microsoft Corporation". The lawsuit dragged on for years before it was finally dismissed. At the same time, a series of major product flops and missed deadlines sullied Apple's reputation, and Sculley was replaced as CEO by Michael Spindler.
By the early 1990s, Apple was developing alternative platforms to the Macintosh, such as the A/UX. Apple had also begun to experiment in providing a Mac-only online portal which they called eWorld, developed in collaboration with America Online and designed as a Mac-friendly alternative to other online services such as CompuServe. The Macintosh platform was itself becoming outdated because it was not built for multitasking, and several important software routines were programmed directly into the hardware. In addition, Apple was facing competition from OS/2 and UNIX vendors such as Sun Microsystems. The Macintosh would need to be replaced by a new platform, or reworked to run on more powerful hardware.
In 1994, Apple allied with IBM and Motorola in the AIM alliance. The goal was to create a new computing platform (the PowerPC Reference Platform), which would use IBM and Motorola hardware coupled with Apple's software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind, thus countering Microsoft. The same year, Apple introduced the Power Macintosh, the first of many Apple computers to use Motorola's PowerPC processor.
In 1996, Michael Spindler was replaced by Gil Amelio as CEO. Gil Amelio made many changes at Apple, including extensive layoffs. After numerous failed attempts to improve Mac OS, first with the Taligent project, then later with Copland and Gershwin, Amelio chose to purchase NeXT and its NeXTSTEP operating system, bringing Steve Jobs back to Apple as an advisor. On July 9, 1997, Amelio was ousted by the board of directors after overseeing a three-year record-low stock price and crippling financial losses. Jobs acted as the interim CEO and began restructuring the company's product line; it was during this period that Jobs identified Jonathan Ive's design talent and the pair worked collaboratively to rebuild Apple's status.
At the 1997 Macworld Expo, Jobs announced that Apple would join Microsoft to release new versions of Microsoft Office for the Macintosh, and that Microsoft had made a $150 million investment in non-voting Apple stock. On November 10, 1997, Apple introduced the Apple Online Store, tied to a new build-to-order manufacturing strategy.
On August 15, 1998, Apple introduced a new all-in-one computer reminiscent of the Macintosh 128K: the iMac. The iMac design team was led by Ive, who would later design the iPod and the iPhone. The iMac featured modern technology and a unique design, and sold almost 800,000 units in its first five months.
Through this period, Apple purchased several companies to create a portfolio of professional and consumer-oriented digital production software. In 1998, Apple purchased Macromedia's Final Cut Pro software, signaling its expansion into the digital video editing market. The following year, Apple released two video editing products: iMovie for consumers and, for professionals, Final Cut Pro, which has gone on to be a significant video-editing program, with 800,000 registered users in early 2007. In 2002, Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for their music productivity application Logic, which led to the development of their consumer-level GarageBand application. iPhoto's release the same year completed the iLife suite.
2000–06: Return to profitability.
Mac OS X, based on NeXT's OPENSTEP and BSD Unix, was released on March 24, 2001 after several years of development. Aimed at consumers and professionals alike, Mac OS X aimed to combine the stability, reliability and security of Unix with the ease of use afforded by an overhauled user interface. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications within Mac OS X as the Classic environment, which meant that users were able to continue running their old applications.
On May 19, 2001, Apple opened the first official Apple Retail Stores in Virginia and California. On July 9, they bought Spruce Technologies, a DVD authoring company. On October 23 of the same year, Apple debuted the iPod portable digital audio player, first sold on November 10. The product was phenomenally successful — over 100 million units were sold within six years. In 2003, Apple's iTunes Store was introduced, offering online music downloads for $0.99 a song and integration with the iPod. The service quickly became the market leader in online music services, with over 5 billion downloads by June 19, 2008.
Since 2001, Apple's design team has progressively abandoned the use of translucent colored plastics first used in the iMac G3. This began with the PowerBook, made with titanium, and was followed by the iBook's white polycarbonate structure and the flat-panel iMac.
At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs announced that Apple would begin producing Intel-based Mac computers in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the transition to Intel chips for the entire Mac product line—over one year sooner than announced. The Power Mac, iBook and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, "The Wall Street Journal" reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.
Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. In January 2006, Apple's market cap surpassed that of Dell. Nine years prior, Dell's CEO Michael Dell said that if he ran Apple he would "shut it down and give the money back to the shareholders." Although Apple's market share in computers had grown, it remained far behind competitors using Microsoft Windows, accounting for about 8% of desktops and laptops in the US.
2007–10: Success with mobile devices.
Apple achieved widespread success with its iPhone, iPod Touch and iPad products, which introduced innovations in mobile phones, portable music players and personal computers respectively. In addition, the implementation of a store for the purchase of software applications represented a new business model. Touch screens had been invented and seen in mobile devices before, but Apple was the first to achieve mass market adoption of such a user interface that included particular pre-programmed touch gestures.
Delivering his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would from that point on be known as Apple Inc., because computers were no longer the main focus of the company, which had shifted its emphasis to mobile electronic devices. The event also saw the announcement of the iPhone and the Apple TV.
The following day, Apple shares hit $97.80, an all-time high at that point. In May, Apple's share price passed the $100 mark.
In an article posted on Apple's website on February 6, 2007, Jobs wrote that Apple would be willing to sell music on the iTunes Store without digital rights management (DRM), thereby allowing tracks to be played on third-party players, if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May 2007. Other record labels eventually followed suit and Apple published a press release in January 2009 to announce the corresponding changes to the iTunes Store.
In July 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and registered an average daily revenue of $1 million, with Jobs speculating in August 2008 that the App Store could become a billion-dollar business for Apple. By October 2008 Apple was the third-largest mobile handset supplier in the world due to the popularity of the iPhone.
On December 16, 2008, Apple announced that 2009 would be the last year the corporation would be attending the Macworld Expo, after more than 20 years of attendance. The announcement also explained that senior vice president of Worldwide Product Marketing Philip Schiller would deliver the 2009 keynote address in lieu of the expected Jobs. The official press release explained that Apple was "scaling back" on trade shows generally, with Macworld Tokyo and Apple Expo in Paris, France two other events that the corporation had ceased attendance at. The enormous success of the Apple Retail Stores and website had rendered trade shows into a minor promotional channel and was cited as the primary reason for the change.
On January 14, 2009, an internal memo from Jobs announced that he would be taking a six-month medical leave of absence from Apple until the end of June 2009, during which time he would focus on his health. In the email, Jobs also stated that he realized "the curiosity over my personal health continues to be a distraction not only for me and my family, but everyone else at Apple as well," further explaining that the break would allow the company "to focus on delivering extraordinary products." Despite Jobs's absence, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with a revenue of $8.16 billion and a profit of $1.21 billion.
After years of speculation and multiple rumored "leaks", Apple announced a large screen, tablet-like media device known as the iPad on January 27, 2010. The iPad runs the same touch based operating system that the iPhone uses and many of the same iPhone apps are compatible with the iPad. This gave the iPad a large app catalog on launch even with very little development time before the release. Later that year on April 3, 2010, the iPad was launched in the US and sold more than 300,000 units on that day, reaching 500,000 by the end of the first week. In May of the same year, Apple's market cap exceeded that of competitor Microsoft for the first time since 1989.
Apple released the iPhone 4, which introduced video calling, multitasking, and a new uninsulated stainless steel design, which acts as the phone's antenna. Because of this antenna implementation, some iPhone 4 users reported a reduction in signal strength when the phone is held in specific ways. After a large amount of media coverage including mainstream news organizations, Apple held a press conference where they offered buyers a free rubber 'bumper' case, which had been proven to eliminate the signal reduction issue. Later that year Apple again refreshed its iPod line of MP3 players which introduced a multi-touch iPod Nano, iPod Touch with FaceTime, and iPod Shuffle with buttons which brought back the buttons of earlier generations.
In October 2010, Apple shares hit an all-time high, eclipsing $300. Additionally, on October 20, Apple updated their MacBook Air laptop, iLife suite of applications, and unveiled Mac OS X Lion, the last version with the name "Mac OS X". On January 6, 2011, the company opened their Mac App Store, a digital software distribution platform, similar to the existing iOS App Store. Apple was featured in the documentary "Something Ventured" which premiered in 2011.
2011–12: Steve Jobs's death.
On January 17, 2011, Jobs announced in an internal Apple memo that he would take another medical leave of absence, for an indefinite period, to allow him to focus on his health. Chief operating officer Tim Cook assumed Jobs's day-to-day operations at Apple, although Jobs would still remain "involved in major strategic decisions for the company." Apple became the most valuable consumer-facing brand in the world. In June 2011, Steve Jobs surprisingly took the stage and unveiled iCloud, an online storage and syncing service for music, photos, files and software which replaced MobileMe, Apple's previous attempt at content syncing.
This would be the last product launch Jobs would attend before his death. It has been argued that Apple has achieved such efficiency in its supply chain that the company operates as a monopsony (one buyer, many sellers), in that it can dictate terms to its suppliers. In July 2011, due to the American debt-ceiling crisis, Apple's financial reserves were briefly larger than those of the U.S. Government.
On August 24, 2011, Jobs resigned his position as CEO of Apple. He was replaced by Tim Cook and Jobs became Apple's chairman. Prior to this, Apple did not have a chairman and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson became Chairman of the Board in November.
On October 4, 2011, Apple announced the iPhone 4S, which included an improved camera with 1080p video recording, a dual core A5 chip capable of 7 times faster graphics than the A4, an "intelligent software assistant" named Siri, and cloud-sourced data with iCloud. (The iPhone 4S was officially released on October 14, 2011.)
On October 5, 2011, Apple announced that Jobs had died, marking the end of an era for Apple Inc.
On October 29, 2011, Apple purchased C3 Technologies, a mapping company, for $240 million, becoming the third mapping company Apple has purchased. On January 10, 2012, Apple paid $500 million to acquire Anobit, an Israeli hardware company that developed and supplies a proprietary memory signal processing technology that improves the performance of flash-memory used in iPhones and iPads.
On January 19, 2012, Apple's Phil Schiller introduced iBooks Textbooks for iOS and iBook Author for Mac OS X in New York City. This was the first major announcement by Apple since the passing of Steve Jobs, who stated in his biography that he wanted to reinvent the textbook and education. The third-generation iPad was announced on March 7, 2012. It includes a Retina display, a new CPU, a five megapixel camera, and 1080p video recording.
On July 24, 2012, during a conference call with investors, Tim Cook said that he loved India, but that Apple was going to expect larger opportunities outside of India, citing the reason as the 30% sourcing requirement from India.
On August 20, 2012, Apple's rising stock rose the company's value to a world-record $624 billion. This beat the non-inflation-adjusted record for market capitalization set by Microsoft in 1999. On August 24, 2012, a US jury ruled that Samsung should pay Apple $1.05 billion (£665m) in damages in an intellectual property lawsuit. Samsung said they will appeal the court ruling. Samsung subsequently prevailed on its motion to vacate this damages award, which the Court reduced by $450 million. The Court further granted Samsung's request for a new trial.
On September 12, 2012, Apple unveiled the iPhone 5, featuring an enlarged screen, more powerful processors, and running iOS 6. The latter includes a new mapping application (replacing Google Maps) that has attracted some criticism. It was made available on September 21, 2012, and became Apple's biggest iPhone launch, with over 2 million pre-orders pushing back the delivery date to late October.
On October 23, 2012, Apple unveiled the iPad Mini, which features a 7.9-inch screen in contrast to the iPad's 9.7-inch screen. Apple also released a third-generation 13-inch MacBook Pro with a Retina display; the fourth-generation iPad, featuring a faster processor and a Lightning dock connector; and new iMac and Mac Mini computers. After the launch of Apple's iPad Mini and fourth generation iPad on November 3, 2012, Apple had sold 3 million iPads in three days of the launch.
On November 10, 2012, Apple confirmed a global settlement that would dismiss all lawsuits between Apple and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million a year from this deal with HTC.
In December 2012, in a TV interview for NBC's "Rock Center" and also aired on the "Today" morning show, Apple CEO Tim Cook said that in 2013 the company will produce one of its existing lines of Mac computers in the United States. In January 2013, Cook stated that he expected China to overtake the US as Apple's biggest market.
2013–present: Acquisitions and expansion.
In March 2013, Apple filed a patent for an augmented reality (AR) system that can identify objects in a live video stream and present information corresponding to these objects through a computer-generated information layer overlaid on top of the real-world image.
At the Worldwide Developer's Conference on June 10, 2013, Apple announced the seventh iOS operating system alongside OS X Mavericks, the tenth version of Mac OS X, and a new Internet radio service called iTunes Radio. iTunes Radio, iOS 7 and OS X Mavericks were released fall 2013. The radio service features more than 200 stations according to company's statement.
On July 2, 2013, Apple recruited Paul Deneve, Belgian President and CEO of Yves Saint Laurent, to Apple's top ranks. A spokesperson for the company stated, "We're thrilled to welcome Paul Deneve to Apple. He'll be working on special projects as a vice president reporting directly to Tim Cook."
Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook attended a closed-door summit held by President Obama on August 8, 2013 in regard to government surveillance and the Internet in the wake of the Edward Snowden NSA incident.
A report on August 22, 2013 confirmed that Apple acquired Embark Inc., a small Silicon Valley-based mapping company. Embark builds free transit apps to help smartphone users navigate public transportation in U.S. cities such as New York, San Francisco and Chicago. Following the confirmation of the acquisition, an Apple spokesperson explained, "Apple buys smaller technology companies from time to time, and we generally do not discuss our purpose or plans." In November 2012, Embark claimed that over 500,000 people used its apps.
An anonymous Apple employee revealed to the "Bloomberg" media publication that the opening of a Tokyo, Japan store is planned for 2014. The construction of the store will be completed in February 2014, but as of August 29, 2013, Takashi Takebayashi, a Tokyo-based spokesman for Apple, has not made any comment to the media. A Japanese analyst has stated, "For Apple, the Japanese market is appealing in terms of quantity and price. There is room to expand tablet sales and a possibility the Japanese market expands if Apple’s mobile carrier partners increase.
On October 1, 2013, Apple India executives unveiled a plan to expand further into the Indian market, following Cook's acknowledgment of the country in July 2013 when sales results showed that iPhone sales in India grew 400% during the second quarter of 2013. In attendance at the confidential meeting were 20 CEOs and senior executives from telecom and electronic retail companies.
A mid-October 2013 announcement revealed that Burberry executive Angela Ahrendts will commence as a senior vice president at Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost eight years and, during her tenure, sales increased to about US$3.2 billion (2 billion pounds) and shares gained more than threefold. In a company wide memo sent on the morning of October 15, 2013, Cook explained the decision to hire Ahrendts:
She [Ahrendts] shares our values and our focus on innovation. She places the same strong emphasis as we do on the customer experience. She cares deeply about people and embraces our view that our most important resource and our soul is our people. She believes in enriching the lives of others and she is wicked smart.
On November 24, 2013, Apple Inc. confirmed the purchase of PrimeSense, an Israeli 3D sensing company based in Tel Aviv. In the following month, Apple Inc. purchased social analytics firm Topsy, one of a small number of firms with real-time access to the messages that appear on Twitter (every tweet published since 2006 is within its scope). Debra Aho Williamson, an analyst with EMarketer Inc., explained: “A key point is they are one of the few companies that has access to the Twitter fire hose and can do real-time analysis of the trends and discussions happening on Twitter.” While an exact amount is unknown, the deal was apparently worth more than US$200 million, according to people with knowledge of the secret deal, and Apple spokespeople refused to disclose a purpose at the time of the acquisition.
On December 6, 2013, Apple Inc. launched iBeacon technology across its 254 U.S. retail stores. Using Bluetooth wireless technology, iBeacon senses the user's exact location within the Apple store and sends the user messages about products, events and other information, tailored to the user's location. iBeacon works as long as the user has downloaded the Apple Store app and has expressly permitted Apple to track them.
Apple Inc. reported that the company sold 51 million iPhones in the Q1 of 2014 (an all-time quarterly record), compared to 47.8 million in the year-ago quarter. Apple also sold 26 million iPads during the quarter, also an all-time quarterly record, compared to 22.9 million in the year-ago quarter. The Company sold 4.8 million Macs, compared to 4.1 million in the year-ago quarter.
On February 4, 2014, Cook met with Abdullah Gül, the President of Turkey, in Ankara to discuss the company's involvement in the Fatih project. Cook also confirmed that Turkey's first Apple Retail Store would be opened in Istanbul in April 2014.
During the proceedings of the "Apple Inc. v. Samsung Electronics Co., Ltd." lawsuits, a previously confidential email, written by Jobs a year before his death, was presented and its content became publicly available in early April 2014. With a subject line that reads "Top 100 – A," the email was sent only to the company's 100 most senior employees and outlines Jobs's vision of Apple Inc.'s future under 10 subheadings, including a "2011 Strategy." Notably, Jobs declares a "Holy War with Google" for 2011 and schedules a "new campus" for 2015.
On May 28, 2014, Apple confirmed its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats Electronics—producer of the "Beats by Dr. Dre" line of headphones and speaker products, and operator of the music streaming service Beats Music—for $3 billion. Iovine felt that Beats had always "belonged" with Apple, as the company modeled itself after Apple's "unmatched ability to marry culture and technology." In regards to the deal, Tim Cook stated that "Music is such an important part of all of our lives and holds a special place within our hearts at Apple. That's why we have kept investing in music and are bringing together these extraordinary teams so we can continue to create the most innovative music products and services in the world." As a result of the acquisition, Apple plans to offer Beats' products through its retail outlets and resellers, but the company has not made any further indications about how Beats will be integrated into Apple's product line.
An Apple representative confirmed to the media in August 2014 that Anand Lal Shimpi, editor and publisher of the "AnandTech" website, was recruited by Apple without elaborating on Lal Shimpi's role. Lal Shimpi informed visitors to the "AnandTech" website that new editor in chief Ryan Smith would be responsible for the publication, as he had retired from the tech publishing field.
Products.
Mac.
Apple sells a variety of computer accessories for Macs, including Thunderbolt Display, Magic Mouse, Magic Trackpad, Wireless Keyboard, Battery Charger, the AirPort wireless networking products, and Time Capsule.
iPad.
On January 27, 2010, Apple introduced their much-anticipated media tablet, the iPad, running a modified version of iOS. It offers multi-touch interaction with multimedia formats including newspapers, magazines, ebooks, textbooks, photos, movies, videos of TV shows, music, word processing documents, spreadsheets, video games, and most existing iPhone apps. It also includes a mobile version of Safari for web browsing, as well as access to the App Store, iTunes Library, iBookstore, Contacts, and Notes. Content is downloadable via Wi-Fi and optional 3G service or synced through the user's computer. AT&T was initially the sole U.S. provider of 3G wireless access for the iPad.
On March 2, 2011, Apple introduced the iPad 2, which had a faster processor and a camera on the front and back. It also added support for optional 3G service provided by Verizon in addition to the existing offering by AT&T. However, the availability of the iPad 2 has been limited as a result of the devastating earthquake and ensuing tsunami in Japan in March 2011.
On March 7, 2012, Apple introduced the third-generation iPad, marketed as "the new iPad". It added LTE service from AT&T or Verizon, the upgraded A5X processor, and the Retina display (2048 by 1536 resolution), originally implemented on the iPhone 4 and iPhone 4S. The dimensions and form factor remained relatively unchanged, with the new iPad being a fraction thicker and heavier than the previous version, and minor positioning changes.
On October 23, 2012, Apple's fourth-generation iPad came out, marketed as the "iPad with Retina display". It added the upgraded A6X processor and replaced the traditional 30-pin dock connector with the all-digital Lightning connector. The iPad Mini was also introduced, with a reduced 7.9-inch display and featuring much of the same internal specifications as the iPad 2.
On October 22, 2013, Apple introduced the iPad Air. It added the new 64 bit Apple-A7 processor. The iPad mini with Retina Display was also introduced, featuring the Apple-A7 processor as well.
Since its launch, iPad users have downloaded three billion apps, while the total number of App Store downloads is over 25 billion.
iPod.
On October 23, 2001, Apple introduced the iPod digital music player. Several updated models have since been introduced, and the iPod brand is now the market leader in portable music players by a significant margin, with more than 350 million units shipped . Apple has partnered with Nike to offer the Nike+iPod Sports Kit, enabling runners to synchronize and monitor their runs with iTunes and the Nike+ website.
Apple currently sells four variants of the iPod:
iPhone.
At the Macworld Conference & Expo in January 2007, Steve Jobs introduced the long-anticipated iPhone, a convergence of an Internet-enabled smartphone and iPod. The first-generation iPhone was released on June 29, 2007 for $499 (4 GB) and $599 (8 GB) with an AT&T contract. On February 5, 2008, it was updated to have 16 GB of memory, in addition to the 8 GB and 4 GB models. It combined a 2.5G quad band GSM and EDGE cellular phone with features found in handheld devices, running scaled-down versions of Apple's Mac OS X (dubbed iPhone OS, later renamed iOS), with various Mac OS X applications such as Safari and Mail. It also includes web-based and Dashboard apps such as Google Maps and Weather. The iPhone features a touchscreen display, Bluetooth, and Wi-Fi (both "b" and "g").
At Worldwide Developers Conference (WWDC) on June 9, 2008, Apple announced the iPhone 3G. It was released on July 11, 2008, with a reduced price of $199 for the 8 GB version, and $299 for the 16 GB version. This version added support for 3G networking and assisted-GPS navigation. The flat silver back and large antenna square of the original model were eliminated in favor of a curved glossy black or white back. Software capabilities were improved with the release of the App Store, providing applications for download that were compatible with the iPhone. On April 24, 2009, the App Store surpassed one billion downloads. At WWDC on June 8, 2009, Apple announced the iPhone 3GS. It provided an incremental update to the device, including faster internal components, support for faster 3G speeds, video recording capability, and voice control.
At WWDC on June 7, 2010, Apple announced the redesigned iPhone 4. It features a 960x640 display, the Apple A4 processor also used in the iPad, a gyroscope for enhanced gaming, 5MP camera with LED flash, front-facing VGA camera and FaceTime video calling. Shortly after its release, reception issues were discovered by consumers, due to the stainless steel band around the edge of the device, which also serves as the phone's cellular signal and Wi-Fi antenna. The issue was corrected by a "Bumper Case" distributed by Apple for free to all owners for a few months. In June 2011, Apple overtook Nokia to become the world's biggest smartphone maker by volume.
On October 4, 2011, Apple unveiled the iPhone 4S, which was first released on October 14, 2011. It features the Apple A5 processor, and is the first model offered by Sprint (joining AT&T and Verizon Wireless as the United States carriers offering iPhone models). On October 19, 2011, Apple announced an agreement with C Spire Wireless to sell the iPhone 4S with that carrier in the near future, marking the first time the iPhone was officially supported on a regional carrier's network. Another notable feature of the iPhone 4S was Siri voice assistant technology, which Apple had acquired in 2010, as well as other features, including an updated 8MP camera with new optics. Apple sold 4 million iPhone 4S phones in the first three days of availability.
On September 12, 2012, Apple introduced the iPhone 5. It added a 4-inch display, 4G LTE connectivity, and the upgraded Apple A6 chip, among several other improvements. Two million iPhones were sold in the first twenty-four hours of pre-ordering and over five million handsets were sold in the first three days of its launch.
A patent filed in July 2013 revealed the development of a new iPhone battery system that uses location data in combination with data on the user's habits to moderate the handsets power settings accordingly. Apple is working towards a power management system that will provide features such as the ability of the iPhone to estimate the length of time a user will be away from a power source to modify energy usage and a detection function that adjusts the charging rate to best suit the type of power source that is being used.
In March 2013, one of the largest cellular phone companies in the United States, T-Mobile, announced that it would begin selling the iPhone 5 on April 12. The announcement of the iPhone came with the announcement that the company would begin implementing 4G cellular service for its users.
Upon the launch of the iPhone 5S and iPhone 5C, Apple sold over nine million devices in the first three days of its launch, which sets a new record for first-weekend smartphone sales. This was the first time that Apple has simultaneously launched two models and the inclusion of China in the list of markets contributed to the record sales result.
On October 15, 2013, U.S. Cellular, the United States fifth largest cell phone provider, announced that it would begin to carry the iPhone. It is the last of the five major carriers, including AT&T, Verizon, Sprint, and T-Mobile to offer the phone. The phone went on sale on November 8 at U.S. Cellular stores around the country. The finalization of a deal between Apple and China Mobile, the world's largest mobile network, was announced in late December 2013. The multi-year agreement provides iPhone access to over 760 million China Mobile subscribers.
In a March 2014 interview, Ive used the iPhone as an example of Apple's ethos of creating high-quality, life-changing products, explaining that they are comparatively expensive due to the intensive effort that is used to make them:
We don’t take so long and make the way we make for fiscal reasons ... Quite the reverse. The body is made from a single piece of machined aluminium ... The whole thing is polished first to a mirror finish and then is very finely textured, except for the Apple logo. The chamfers [smoothed-off edges] are cut with diamond-tipped cutters. The cutters don’t usually last very long, so we had to figure out a way of mass-manufacturing long-lasting ones. The camera cover is sapphire crystal. Look at the details around the sim-card slot. It’s extraordinary!
Apple TV.
At the 2007 Macworld conference, Jobs demonstrated the Apple TV, (previously known as the iTV), a set-top video device intended to bridge the sale of content from iTunes with high-definition televisions. The device links up to a user's TV and syncs, either via Wi-Fi or a wired network, with one computer's iTunes library and streams from an additional four. The Apple TV originally incorporated a 40 GB hard drive for storage, includes outputs for HDMI and component video, and plays video at a maximum resolution of 720p. On May 31, 2007 a 160 GB drive was released alongside the existing 40 GB model and on January 15, 2008 a software update was released, which allowed media to be purchased directly from the Apple TV.
In September 2009, Apple discontinued the original 40 GB Apple TV and now continues to produce and sell the 160 GB Apple TV. On September 1, 2010, alongside the release of the new line of iPod devices for the year, Apple released a completely redesigned Apple TV. The new device is 1/4 the size, runs quieter, and replaces the need for a hard drive with media streaming from any iTunes library on the network along with 8 GB of flash memory to cache media downloaded. Apple with the Apple TV has added another device to its portfolio that runs on its A4 processor along with the iPad and the iPhone. The memory included in the device is the half of the iPhone 4 at 256 MB; the same as the iPad, iPhone 3GS, third and fourth-generation iPod Touch.
It has HDMI out as the only video out source. Features include access to the iTunes Store to rent movies and TV shows (purchasing has been discontinued), streaming from internet video sources, including YouTube and Netflix, and media streaming from an iTunes library. Apple also reduced the price of the device to $99. A third generation of the device was introduced at an Apple event on March 7, 2012, with new features such as higher resolution (1080p) and a new user interface.
Apple Watch.
The Apple Watch is a smartwatch announced by Tim Cook on September 9, 2014. It has fitness tracking capabilities similar to Fitbit. It requires an iPhone 5 or above to work. The release date is early 2015, starting at $349.
Software.
Apple develops its own operating system to run on Macs, OS X, the latest version being OS X Mavericks (version 10.9). Apple also independently develops computer software titles for its OS X operating system. Much of the software Apple develops is bundled with its computers. An example of this is the consumer-oriented iLife software package that bundles iMovie, iPhoto and GarageBand. For presentation, page layout and word processing, iWork is available, which includes Keynote, Pages, and Numbers. iTunes, QuickTime media player, and Software Update are available as free downloads for both OS X and Windows.
Apple also offers a range of professional software titles. Their range of server software includes the operating system OS X Server; Apple Remote Desktop, a remote systems management application; and Xsan, a Storage Area Network file system. For the professional creative market, there is Aperture for professional RAW-format photo processing; Final Cut Pro, a video production suite; Logic Pro, a comprehensive music toolkit; and Motion, an advanced effects composition program.
Apple also offers online services with iCloud, which provides cloud storage and syncing for a wide range of data, including email, contacts, calendars, photos and documents. It also offers iOS device backup, and is able to integrate directly with third-party apps for even greater functionality. iCloud is the fourth generation of online services provided by Apple, and was preceded by MobileMe, .Mac and iTools, all which met varying degrees of success.
Corporate identity.
Logo.
According to Steve Jobs, Apple was so named because Jobs was coming back from an apple farm, and he was on a fruitarian diet. He thought the name was "fun, spirited and not intimidating".
Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting under an apple tree. It was almost immediately replaced by Rob Janoff's "rainbow Apple", the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. Janoff presented Jobs with several different monochromatic themes for the "bitten" logo, and Jobs immediately took a liking to it. While Jobs liked the logo, he insisted it be in color to humanize the company. The logo was designed with a bite so that it would not be confused with a cherry. The colored stripes were conceived to make the logo more accessible, and to represent the fact the Apple II could generate graphics in color. This logo is often erroneously referred to as a tribute to Alan Turing, with the bite mark a reference to his method of suicide. Both Janoff and Apple deny any homage to Turing in the design of the logo.
On August 27, 1999 (the following year after the iMac G3 was introduced), Apple officially dropped the rainbow scheme and began to use monochromatic themes, nearly identical in shape to its previous rainbow incarnation, on various products, packaging and advertising. An Aqua-themed version of the monochrome logo was used from 1999 to 2003, and a Glass-themed version was used from 2007 to 2013. With the release of iOS 7 and OS X Mavericks in late 2013, the logo appears flat and white with no glossy effects.
Steve Jobs and Steve Wozniak were Beatles fans, but Apple Inc. had trademark issues with Apple Corps Ltd., a multimedia company started by the Beatles in 1967, involving their name and logo. This resulted in a series of lawsuits and tension between the two companies. These issues ended with settling of their most recent lawsuit in 2007.
Advertising.
Apple's first slogan, "Byte into an Apple", was coined in the late 1970s. From 1997 to 2002, the slogan "Think Different" was used in advertising campaigns, and is still closely associated with Apple. Apple also has slogans for specific product lines — for example, "iThink, therefore iMac" was used in 1998 to promote the iMac, and "Say hello to iPhone" has been used in iPhone advertisements. "Hello" was also used to introduce the original Macintosh, Newton, iMac ("hello (again)"), and iPod.
Since the introduction of the Macintosh in 1984 with the 1984 Super Bowl commercial to the more modern 'Get a Mac' adverts, Apple has been recognized in the past for its efforts towards effective advertising and marketing for its products, though its advertising was criticized for the claims made by some later campaigns, particularly the 2005 Power Mac ads and iPhone ads in Britain.
Apple's product commercials gained fame for launching musicians into stardom as a result of their eye-popping graphics and catchy tunes. First, the company popularized Canadian singer Feist's "1234" song in its ad campaign. Later, Apple used the song "New Soul" by French-Israeli singer-songwriter Yael Naïm to promote the MacBook Air. The debut single shot to the top of the charts and sold hundreds of thousands of copies in a span of weeks.
Brand loyalty.
Apple's brand loyalty is considered unusual for any product. At one time, Apple evangelists were actively engaged by the company, but this was after the phenomenon was already firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism "something that was stumbled upon," while Ive explained in 2014 that "People have an incredibly personal relationship" with Apple's products.
Apple supports the continuing existence of a network of Mac User Groups in many areas where Mac computers are available. Mac users previously meet at the European Apple Expo and the San Francisco Macworld Conference & Expo trade shows, where Apple traditionally introduced new products each year, to both the industry and public, until Apple pulled out of both events—the conferences continue, but Apple is not officially represented at either event. Mac developers, in turn, continue to gather at the annual Apple Worldwide Developers Conference.
Apple Store openings can draw crowds of thousands, with some waiting in line as much as a day before the opening or flying in from other countries for the event. The New York City Fifth Avenue "Cube" store had a line as long as half a mile; a few Mac fans took the opportunity of the setting to propose marriage. The Ginza opening in Tokyo was estimated in the thousands with a line exceeding eight city blocks.
John Sculley told "The Guardian" newspaper in 1997: "People talk about technology, but Apple was a marketing company. It was the marketing company of the decade." Research in 2002 by NetRatings indicate that the average Apple consumer was usually more affluent and better educated than other PC company consumers. The research indicated that this correlation could stem from the fact that on average Apple Inc. products are more expensive than other PC products.
In response to a query about the devotion of loyal Apple consumers, Ive responded:
What people are responding to is much bigger than the object. They are responding to something rare — a group of people who do more than simply make something work, they make the very best products they possibly can. It’s a demonstration against thoughtlessness and carelessness.
Home page.
The Apple website home page has been used to commemorate, or pay tribute to, milestones and events outside of Apple's product offerings:
Corporate affairs.
During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s, beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since adopted USB, AGP, HyperTransport, Wi-Fi, and other industry standards in its computers. FireWire is an Apple-originated standard that was widely adopted across the industry after it was standardized as IEEE 1394.
Headquarters.
Apple Inc.'s world corporate headquarters are located in the middle of Silicon Valley, at 1–6 Infinite Loop, Cupertino, California. This Apple campus has six buildings that total and was built in 1993 by Sobrato Development Cos.
In 2006, Apple announced its intention to build a second campus on assembled from various contiguous plots (east of N Wolfe Road between Pruneridge Avenue and Vallco Parkway). Later acquisitions increased this to 175 acres. The new campus, also in Cupertino, will be about east of the current campus. The new campus building will be designed by Norman Foster.
On October 15, 2013, the Cupertino City Council approved the proposed "spaceship" design campus. On June 7, 2011, Jobs had given a presentation to Cupertino City Council, detailing the architectural design of the new building and its environs. The new campus is planned to house up to 13,000 employees in one central four-storied circular building (with a café for 3,000 sitting people integrated) surrounded by extensive landscape (with parking mainly underground and the rest centralized in a parking structure). The new campus will be built on the former HP headquarters, next to Interstate 280. On the morning of the announcement, Apple CEO Cook tweeted "Our home for innovation and creativity for decades to come. Cupertino City Council Gives Unanimous Approval for Apple's New Campus." The 2.8 million square foot facility, which will include Jobs's original designs for a fitness center and corporate auditorium, will be able to house 14,000 employees, with enough parking space to accommodate almost every employee.
Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland. The facility, which opened in 1980, was Apple's first location outside of the United States. Apple Sales International, which deals with all of Apple's international sales outside of the USA, is located at Apple's campus in Cork along with Apple Distribution International, which similarly deals with Apple's international distribution network.
On April 20, 2012, Apple added 500 new jobs at its European headquarters, increasing the total workforce from around 2,800 to 3,300 employees. The company will build a new office block on its Hollyhill Campus to accommodate the additional staff.
Corporate culture.
Apple was one of several highly successful companies founded in the 1970s that bucked the traditional notions of what a corporate culture should look like in organizational hierarchy (flat versus tall, casual versus formal attire, etc.). Other highly successful firms with similar cultural aspects from the same period include Southwest Airlines and Microsoft. Originally, the company stood in opposition to staid competitors like IBM by default, thanks to the influence of its founders; Jobs often walked around the office barefoot even after Apple was a Fortune 500 company. By the time of the "1984" TV ad, this trait had become a key way the company attempted to differentiate itself from its competitors. According to a 2011 report in "Fortune," this has resulted in a corporate culture more akin to a startup rather than a multinational corporation.
As the company has grown and been led by a series of chief executives, each with his own idea of what Apple should be, some of its original character has arguably been lost, but Apple still has a reputation for fostering individuality and excellence that reliably draws talented people into its employ. This was especially after Jobs's return. To recognize the best of its employees, Apple created the Apple Fellows program, awarding individuals who made extraordinary technical or leadership contributions to personal computing while at the company. The Apple Fellowship has so far been awarded to a few individuals including Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, and Steve Wozniak.
Apple is also known for strictly enforcing accountability. Each project has a "directly responsible individual," or "DRI" in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Numerous employees of Apple have cited that projects without Jobs's involvement often took longer than projects with his involvement.
At Apple, employees are specialists who are not exposed to functions outside their area of expertise. Jobs saw this as a means of having "best-in-class" employees in every role. For instance, Ron Johnson—Senior Vice President of Retail Operations until November 1, 2011—was responsible for site selection, in-store service, and store layout, yet he had no control of the inventory in his stores (which is done company wide by Cook, who has a background in supply-chain management). This is the opposite of General Electric's corporate culture, which aims to create well-rounded managers.
The company's manufacturing, procurement and logistics enables it to execute massive product launches without having to maintain large, profit-sapping inventories. In 2011, Apple's profit margins were 40 percent, compared with between 10 and 20 percent for most other hardware companies. Cook's catchphrase to describe his focus on the company's operational arm is: “Nobody wants to buy sour milk”.
The company previously advertised its products as being made in America up until the late 1990s; however, as a result of outsourcing initiatives in the 2000s, almost all of its manufacturing is now handled abroad. According to a report by the "New York Times", Apple insiders "believe the vast scale of overseas factories as well as the flexibility, diligence and industrial skills of foreign workers have so outpaced their American counterparts that “Made in the U.S.A.” is no longer a viable option for most Apple products".
Unlike other major U.S. companies, Apple provides a relatively simple compensation policy for executives, which does not include perks that other CEOs enjoy (such as country club fees and private use of company aircraft). The company typically grants stock options to executives every other year.
A media article published in July 2013 provided details about Apple's "At-Home Apple Advisors" customer support program that serves as the corporation's call center. The advisors are employed within the U.S. and work remotely after undergoing a four-week training program that also serves as a testing period. The advisors earn between US$9 and $12 per hour, and receive intensive management to ensure a high quality of customer support.
Retail outlets.
In 1999 Jobs retained Eight Inc. as the strategic retail design partner to begin creating the Apple retail store, and began the search for senior retail and development executives. Tim Kobe of Eight Inc. prepared an "Apple Retail" white paper for Jobs, outlining the ability of separate Apple retail stores to directly drive the Apple brand experience—Kobe used their recently completed work with The North Face and Nike as a basis for the white paper.
The first two Apple Stores opened on May 19, 2001: the first opening was at Tysons Corner, Virginia, and the Glendale, California store, at the Glendale Galleria, opened later in the day due to the time zone difference. More than 7,700 people visited Apple’s first two stores in the opening weekend, spending a total of US$599,000.
After the first Apple Store opened, Apple sold third-party accessories, such as Nikon and Canon digital cameras. Adobe, one of Apple's oldest software partners, also sells its Mac-compatible software, as does Microsoft, who sells Microsoft Office for the Mac.
The books of John Wiley & Sons, the publishers of the "For Dummies" series of instructional books, were banned from Apple Stores in 2005, because Jobs disagreed with their decision to publish an unauthorized Jobs biography called "iCon". After the launch of the iBookstore, Apple stopped selling physical books, both online and at the Apple Retail Stores.
Litigation.
Apple has been a participant in various legal proceedings and claims since it began operation and, like its competitors and peers, engages in litigation (trying legal cases before the courts) in its normal course of business for a variety of reasons. In particular, Apple is known for and promotes itself as actively and aggressively enforcing its intellectual property interests.
Some examples include "Apple v. Samsung", "Apple v. Microsoft", "Motorola Mobility v. Apple Inc.", "Apple Corps v. Apple Computer".
Finance.
In its fiscal year ending in September 2011, Apple Inc. reported a total of $108 billion in annual revenues – a significant increase from its 2010 revenues of $65 billion – and nearly $82 billion in cash reserves. Apple achieved these results while losing market share in certain product categories. On March 19, 2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth quarter of 2012, per approval by their board of directors.
On September 2012, Apple reached a record share price of more than $705 and closed at above 700. With 936,596,000 outstanding shares (as of June 30, 2012), it had a market capitalization of about $660 billion. At the time, this was the highest nominal market capitalization ever reached by a publicly traded company, surpassing a record set by Microsoft in 1999.
Environmental record.
Climate change and clean energy.
On April 21, 2011, Greenpeace released a report highlighting the fact that data centers consumed up to 2% of all global electricity and this amount was projected to increase. Phil Radford of Greenpeace said “we are concerned that this new explosion in electricity use could lock us into old, polluting energy sources instead of the clean energy available today.” On April 17, 2012, following a Greenpeace protest of Apple, Apple Inc. released a statement committing to ending its use of coal and shifting to 100% clean energy. By 2013 Apple was using 100% renewable energy to power their data centers, and overall 75% of its power came from renewable sources.
In 2010, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple a score of 52 points out of a possible 100, which puts Apple in their top category "Striding". This was an increase from May 2008, when Climate Counts only gave Apple 11 points out of 100, which placed the company last among electronics companies, at which time Climate Counts also labeled Apple with a "stuck icon", adding that Apple at the time was "a choice to avoid for the climate conscious consumer".
Toxics.
Greenpeace has campaigned against Apple because of various environmental issues, including a global end-of-life take-back plan, non-recyclable hardware components and toxins within iPhone hardware. Since 2003 Greenpeace has campaigned against Apple's use of particular chemicals in its products, more specifically, the inclusion of PVC and BFRs in their devices. On May 2, 2007, Steve Jobs released a report announcing plans to eliminate PVC and BFRs by the end of 2008. Apple has since eliminated PVC and BFRs from its product range, becoming the first laptop manufacturer to do so.
In the first edition of the Greenpeace 'Green Electronics Guide', released in August 2006, Apple only scored 2.7/10.
The Environmental Protection Agency rates Apple highest amongst producers of notebooks, and fairly well compared to producers of desktop computers and LCD displays.
In June 2007, Apple upgraded the MacBook Pro, replacing cold cathode fluorescent lamp (CCFL) backlit LCD displays with mercury-free LED backlit LCD displays and arsenic-free glass, and has since done this for all notebooks. Apple has also left out BFRs and PVCs in various internal components. Apple offers information about emissions, materials, and electrical usage concerning each product.
In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, BFRs and had an efficient power adapter.
In October 2009, Apple upgraded the iMac and MacBook, replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays with mercury-free LED backlit LCD displays and arsenic-free glass. This means all Apple computers have mercury free LED backlit displays, arsenic-free glass and are without PVC cables. All Apple computers also have EPEAT Gold status.
In October 2011, Chinese authorities ordered an Apple supplier to close part of its plant in Suzhou after residents living nearby raised significant environmental concerns.
In November 2011, Apple featured in Greenpeace's Guide to Greener Electronics, which ranks electronics manufacturers on sustainability, climate and energy policy, and how "green" their products are. The company ranked fourth of fifteen electronics companies (moving up five places from the previous year) with a score of 4.6/10 down from 4.9. Greenpeace praises Apple's sustainability, noting that the company exceeded its 70% global recycling goal in 2010. It continues to score well on the products rating with all Apple products now being free of PVC vinyl plastic and brominated flame retardants. However, the guide criticizes Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data and for not setting out any targets to reduce emissions. In January 2012, Apple requested that their cable maker, Volex, begin producing halogen-free USB and power cables.
In June 2012, Apple Inc. withdrew its products from the Electronic Product Environmental Assessment Tool (EPEAT) certification system, but reversed this decision in July.
Labor practices.
Manufacturing.
In 2006, the "Mail on Sunday" reported on the working conditions that existed at factories in China where the contract manufacturers Foxconn and Inventec produced the iPod. The article stated that one complex of factories that assembles the iPod (among other items) had over 200,000 workers that lived and worked in the factory, with employees regularly working more than 60 hours per week. The article also reported that workers made around $100 per month and were required to live pay for rent and food from the company, which generally amounted to a little over half of workers' earnings.
Apple immediately launched an investigation and worked with their manufacturers to ensure acceptable working conditions. In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply. Yearly progress reports have been published since 2008. In 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens. One worker claimed that he and his coworkers had not been informed of possible occupational illnesses. After a spate of suicides in a Foxconn facility in China making iPads and iPhones, albeit at a lower rate than in China as a whole, workers were forced to sign a legally binding document guaranteeing that they would not kill themselves.
In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.
Workers in factories producing Apple products have also been exposed to n-hexane, a neurotoxin that is a cheaper alternative than alcohol for cleaning the products.
In 2013, China Labor Watch said it found violations of the law and of Apple's pledges about working conditions at facilities operated by Pegatron, including discrimination against ethnic minorities and women, withholding employees' pay, excessive work hours, poor living conditions, health and safety problems and pollution.
Tax practices.
Apple created subsidiaries in low-tax places such as the Republic of Ireland, the Netherlands, Luxembourg and the British Virgin Islands to cut the taxes it pays around the world. According to the "New York Times," in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes. In the late 1980s Apple was a pioneer of an accounting technique known as the "Double Irish With a Dutch Sandwich," which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.
British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporation tax. He followed this research by calling on the Chancellor of the Exchequer George Osborne to force these multinationals, which also included Google and The Coca-Cola Company, to state the effective rate of tax they pay on their UK revenues. Elphicke also said that government contracts should be withheld from multinationals who do not pay their fair share of UK tax.
In June 2014 the European Commissioner for Competition launched an investigation of Apple's tax practices in Ireland, as part of a wider probe of multi-national companies' tax arrangements in various European countries.
Charitable causes.
As of 2014, Apple is listed as a partner of the Product RED campaign, together with other brands such as Nike, Girl, American Express and Converse. The campaign's mission is to prevent the transmission of HIV from mother to child by 2015 (its byline is "Fighting For An AIDS Free Generation").
In November 2012, Apple donated $2.5 million to the American Red Cross to aid relief efforts after Hurricane Sandy.
References.
Sources

</doc>
<doc id="857" url="http://en.wikipedia.org/wiki?curid=857" title="Aberdeenshire">
Aberdeenshire

Aberdeenshire () is one of the 32 council areas of Scotland, and a lieutenancy area.
The Aberdeenshire council area does not include the City of Aberdeen, a separate council area, from which its name derives. Together, the modern council area and the city form historic Aberdeenshire, one of the counties of Scotland formerly used for local government and still used as a registration county.
Aberdeenshire Council is headquartered at Woodhill House, in Aberdeen, making it the only Scottish council whose headquarters are based outwith its jurisdiction. Aberdeenshire borders Angus and Perth and Kinross to the south, and Highland and Moray to the west.
Traditionally, it has been economically dependent upon the primary sector (agriculture, fishing, and forestry) and related
processing industries. Over the last 40 years, the development of the oil and gas industry and associated service sector has broadened Aberdeenshire's economic base, and contributed to a rapid population growth of some 50%. since 1975, while the land covered represents 8% of Scotland's overall territory. It covers an area of .
History.
Aberdeenshire has a rich prehistoric and historic heritage. It is the locus of a large number of Neolithic and Bronze Age archaeological sites, including Longman Hill, Kempstone Hill, Catto Long Barrow and Cairn Lee. The area was settled in the Bronze Age by the Beaker culture, who arrived from the south around 2000-1800 BC. Stone circles and cairns are predominantly from this era. In the Iron Age, hill forts were built. Around the 1st century AD, the Taexali people, which have left little history were believed to have resided along the coast. The Picts were the next documented inhabitants of the area, and were no later than 800-900 AD. The Romans also were in the area during this period, as they left signs at Kintore. Christianity influenced the inhabitants early on, and there were Celtic monasteries at Old Deer and Monymusk. Since medieval times there have been a number of crossings of the Mounth (a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven) through present day Aberdeenshire from the Scottish Lowlands to the Highlands. Some of the most well known and historically important trackways are the Causey Mounth and Elsick Mounth.
Aberdeenshire played an important role in the fighting between the Scottish clans. Clan MacBeth and the Clan Canmore were 2 of the larger clans. Lumphanan saw Macbeth fall in 1057. During the Anglo-Norman penetration, other families arrives such as House of Balliol, Clan Bruce, and Clan Cumming (Comyn). When the fighting amongst these newcomers resulted in the Scottish Wars of Independence, the English king Edward I traveled across the area twice, in 1296 and 1303. In 1307, Robert the Bruce was victorious near Inverurie. Along with his victory came new families, namely the Forbeses and the Gordons. These new families set the stage for the upcoming rivalries during the 14th and 15th centuries. This rivalry grew worse when religion became a focal point, as the Gordon family adhered to Catholicism and the Forbes to Protestantism. Three universities were founded in the area prior to the 17th century, King's College in Old Aberdeen (1494), Marischal College in Aberdeen (1593), and the University of Fraserburgh (1597).
After the end of the Revolution of 1688, there was a generally peaceful period, which was interrupted only by such fleeting events such as the Rising of 1715 and the Rising of 1745, which in turn led to the end of the ascendancy of Episcopalianism, and the feudal power of landowners and in turn lead to the era of increased agricultural and industrial progress. During the 17th century, Aberdeenshire was the location of more fighting, centered around the Marquess of Montrose and the English Civil Wars. This period also saw increased wealth due to the increase in trade with Germany, Poland, and the Low Countries.
The present council area is named after the historic county of Aberdeen, which had different boundaries and was abolished in 1975 under the Local Government (Scotland) Act 1973. It was replaced by Grampian Regional Council and five district councils: Banff and Buchan, Gordon, Kincardine and Deeside, Moray and the City of Aberdeen. The current Aberdeenshire consists of all of former Aberdeenshire, former Kincardineshire and the northeast portions of Banffshire. Local government functions were shared between the two levels. In 1996, under the Local Government etc (Scotland) Act 1994, the Banff and Buchan district, Gordon district and Kincardine and Deeside district were merged to form the present Aberdeenshire council area, with the other two districts becoming autonomous council areas.
Demographics.
The population of the council area has risen over 50% since 1971 to approximately 247,600, representing 4.7% of Scotland's total. Aberdeenshire's population has increased by 9.1% since 2001, while Scotland's total population grew by only 3.8%.
The census lists a relatively high proportion of under 16s and slightly less people of working-age compared with the Scottish average. The twelve biggest settlements in Aberdeenshire (with 2011 population estimates) are:
Economy.
Aberdeenshire's Gross Domestic Product (GDP) is estimated at £3,496m (2011), representing 5.2% of the Scottish total. Aberdeenshire's economy is closely linked to Aberdeen City's (GDP £7,906m) and in 2011 the region as a whole was calculated to contribute 16.8% of Scotland's GDP. Between 2012 and 2014 the combined Aberdeenshire and Aberdeen City economic forecast GDP growth rate is 6.8%, the highest growth rate of any local council area and above the Scottish rate of 4.8%.
A significant proportion of Aberdeenshire's working residents commute to Aberdeen City for work, varying from 11.5% from Fraserburgh to 65% from Westhill.
Average Gross Weekly Earnings (for full-time employees employed in work places in Aberdeenshire in 2011) are £570.60. This is lower than the Scottish average by £4.10 and a fall of 2.6% on the 2010 figure. The average gross weekly pay of people resident in Aberdeenshire is much higher, at £641.90, as many people commute out
of Aberdeenshire, principally into Aberdeen City.
Total employment (excluding farm data) in Aberdeenshire is estimated at 93,700 employees (Business Register and
Employment Survey 2009). The majority of employees work within the service sector, predominantly in public administration, education and health. Almost 19% of employment is within the public sector. Aberdeenshire's economy remains closely linked to Aberdeen City's and the North Sea oil industry, with many employees in oil related jobs.
The average monthly unemployment (claimant count) rate for Aberdeenshire in 2011 was 1.5%. This is lower than the average rates for Aberdeen City (2.3%), Scotland (4.2%) and the UK (3.8%).
Governance and politics.
The council has 68 councillors, elected in 19 multi-member wards by Single Transferable Vote. The 2012 elections resulted in the following representation:
The overall political composition of the council, following subsequent defections and by-elections, is as follows:
The Council's Revenue Budget for 2012/13 totals approx £548 million. The Education, Learning and Leisure Service takes the largest share of budget (52.3%), followed by Housing and Social Work (24.3%), Infrastructure Services (15.9%), Joint Boards (such as Fire and Police) and Misc services (7.9%) and Trading Activities (0.4%).
21.5% of the revenue is raised locally through the Council Tax. Average Band D Council Tax is £1,141 (2012/13), no change on the previous year.
The current chief executive of the Council is Colin D Mackenzie and the elected Council Leader is Jim Gifford. Aberdeenshire also has a Provost, who is Councillor Jill Webster.
The council has devolved power to six area committees: Banff and Buchan; Buchan; Formartine; Garioch; Marr; and Kincardine and Mearns. Each area committee takes decisions on local issues such as planning applications, and the split is meant to reflect the diverse circumstances of each area. ()
Notable features.
The following significant structures or places are within Aberdeenshire:
Hydrology and climate.
There are numerous rivers and burns in Aberdeenshire, including Cowie Water, Carron Water, Burn of Muchalls, River Dee, River Don, River Ury, River Ythan, Water of Feugh, Burn of Myrehouse, Laeca Burn and Luther Water. Numerous bays and estuaries are found along the seacoast of Aberdeenshire, including Banff Bay, Ythan Estuary, Stonehaven Bay and Thornyhive Bay. Aberdeenshire is in the rain shadow of the Grampians, therefore it is a generally dry climate, with portions of the coast, receiving of moisture annually. Summers are mild and winters are typically cold in Aberdeenshire; Coastal temperatures are moderated by the North Sea such that coastal areas are typically cooler in the summer and warmer in winter than inland locations. Coastal areas are also subject to haar, or coastal fog.

</doc>
<doc id="859" url="http://en.wikipedia.org/wiki?curid=859" title="Aztlan Underground">
Aztlan Underground

Aztlan Underground is a fusion band from Los Angeles. Since early 1989, Aztlan Underground has played Rapcore. Indigenous drums, flutes, and rattles are commonplace in its musical compositions.
This unique sound is the backdrop for the band's message of dignity for indigenous people, all of humanity, and Earth. Aztlan Underground has been cultivating a grass roots audience across the country, which has become a large and loyal underground following. Their music includes spoken word pieces and elements of punk, hip hop, rock, funk, jazz, and indigenous music, among others.
The artists are Chenek "DJ Bean" (turntables, samples and percussion), Yaotl (vocals, indigenous percussion), Joe "Peps" (bass, rattles), Alonzo Beas (guitars, synth), Caxo (drums, indigenous percussion), and Bulldog (vocals, flute).
Aztlan Underground appeared on television on Culture Clash on Fox in 1993, was part of "Breaking Out", a concert on pay per view in 1998, and was featured in the independent films "Algun Dia" and "Frontierlandia".
The band has been mentioned or featured in various newspapers and magazines: the Vancouver Sun, Northshore News (Vancouver, Canada newspaper), New Times (Los Angeles weekly entertainment newspaper), BLU Magazine (underground hip hop magazine), BAM Magazine (Southern California), La Banda Elastica Magazine, and the Los Angeles Times Calendar section. It is also the subject of a chapter in "It's Not About A Salary", by Brian Cross. They also opened for Rage Against the Machine in Mexico City.
It was nominated in the New Times 1998 "Best Latin Influenced" category, the BAM Magazine 1999 "Best Rock en Español" category, and the LA Weekly 1999 "Best Hip Hop" category.
Aztlan Underground were signed to a Basque record label in 1999 which enabled them to tour Spain extensively and perform in France and Portugal.
Other parts of the world that Aztlan Underground have performed include Canada, Australia, and Venezuela.
The band completed their third album and released it exclusively digital on August 29, 2009. The band is set to begin writing a new record this year.
Aztlan Underground were nominated for four Native American Music Award categories for the Nammys 2010. See Nammys.com
Discography.
"Decolonize".
Year:1995
"Sub-Verses".
Year:1998
"Aztlan Underground".
Year:2009

</doc>
<doc id="863" url="http://en.wikipedia.org/wiki?curid=863" title="American Civil War">
American Civil War

The American Civil War, widely known in the United States as simply the Civil War as well as other sectional names, was fought from 1861 to 1865. Seven Southern slave states individually declared their secession from the United States and formed the Confederate States of America, known as the "Confederacy" or the "South". They grew to include eleven states, and although they claimed thirteen states and additional western territories, the Confederacy was never recognized by a foreign country. The states that did not declare secession were known as the "Union" or the "North". The war had its origin in the fractious issue of slavery, especially the extension of slavery into the western territories. After four years of bloody combat that left over 600,000 Union and Confederate soldiers dead, and destroyed much of the South's infrastructure, the Confederacy collapsed, slavery was abolished, and the difficult Reconstruction process of restoring national unity and guaranteeing civil rights to the freed slaves began.
In the 1860 presidential election, Republicans, led by Abraham Lincoln, opposed the expansion of slavery into US territories. Lincoln won, but before his inauguration on March 4, 1861, seven slave states with cotton-based economies formed the Confederacy. The first six to secede had the highest proportions of slaves in their populations, a total of 48.8% for the six. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln's inaugural address declared his administration would not initiate civil war. Eight remaining slave states continued to reject calls for secession. Confederate forces seized numerous federal forts within territory claimed by the Confederacy. A peace conference failed to find a compromise, and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene; none did and none recognized the new Confederate States of America.
Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter, a key fort held by Union troops in South Carolina. Lincoln called for every state to provide troops to retake the fort; consequently, four more slave states joined the Confederacy, bringing their total to eleven. Lincoln soon controlled the border states, after arresting state legislators and suspending habeas corpus, ignoring the ruling of the Supreme Court's Chief Justice that such suspension was unconstitutional, and established a naval blockade that crippled the southern economy. The Eastern Theater was inconclusive in 1861–62. The autumn 1862 Confederate campaign into Maryland (a Union state) ended with Confederate retreat at the Battle of Antietam, dissuading British intervention. To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies, and the Union siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. In the Western Theater, William T. Sherman drove east to capture Atlanta and marched to the sea, destroying Confederate infrastructure along the way. The Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the protracted Siege of Petersburg. The besieged Confederate army eventually abandoned Richmond, seeking to regroup at Appomattox Court House, though there they found themselves surrounded by union forces. This led to Lee's surrender to Grant on April 9, 1865. All Confederate generals surrendered by that summer.
The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed World War I. It remains the deadliest war in American history, resulting in the deaths of an estimated 750,000 soldiers and an undetermined number of civilian casualties. One estimate of the death toll is that ten percent of all Northern males 20–45 years old, and 30 percent of all Southern white males aged 18–40 perished. From 1861 to 1865 about 620,000 soldiers lost their lives.
Causes of secession.
The causes of the Civil War were complex and have been controversial since the war began. The issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war. Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln had won without carrying a single Southern state, many Southern whites felt that disunion had become their only option, because they felt as if they were losing representation, which hampered their ability to promote pro-slavery acts and policies.
Slavery.
The slavery issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with Republicanism in the United States, or a state-based property system protected by the Constitution. The strategy of the anti-slavery forces was containment — to stop the expansion and thus put slavery on a path to gradual extinction. To slave holding interests in the South, this strategy was perceived as infringing upon their Constitutional rights. Slavery was being phased out of existence in the North and was fading in the border states and urban areas, but was expanding in highly profitable cotton districts of the south.
Despite compromises in 1820 and 1850, the slavery issues exploded in the 1850s. Causes include controversy over admitting Missouri as a slave state in 1820, the acquisition of Texas as a slave state in 1845 and the status of slavery in western territories won as a result of the Mexican–American War and the resulting Compromise of 1850. Following the U.S. victory over Mexico, Northerners attempted to exclude slavery from conquered territories in the Wilmot Proviso; although it passed the House, it failed in the Senate. Northern (and British) readers recoiled in anger at the horrors of slavery as described in the novel and play "Uncle Tom's Cabin" (1852) by abolitionist Harriet Beecher Stowe. Irreconcilable disagreements over slavery ended the Whig and Know Nothing political parties, and later split the Democratic Party between North and South, while the new Republican Party angered slavery interests by demanding an end to its expansion. Most observers believed that without expansion slavery would eventually die out; Lincoln argued this in 1845 and 1858.
Meanwhile, the South of the 1850s saw an increasing number of slaves leave the border states through sale, manumission and escape. During this same period, slave-holding border states had more free African-Americans and European immigrants than the lower South, which increased Southern fears that slavery was threatened with rapid extinction in this area. With tobacco and cotton wearing out the soil, the South believed it needed to expand slavery. Some advocates for the Southern states argued in favor of reopening the international slave trade to populate territory that was to be newly opened to slavery. Southern demands for a slave code to ensure slavery in the territories repeatedly split the Democratic Party between North and South by widening margins.
To settle the dispute over slavery expansion, Abolitionists and proslavery elements sent their partisans into Kansas, both using ballots and bullets. In the 1850s, a miniature civil war in Bleeding Kansas led pro-South Presidents Franklin Pierce and James Buchanan to attempt a forced admission of Kansas as a slave state through vote fraud. The 1857 Congressional rejection of the pro-slavery Lecompton Constitution was the first multi-party solid-North vote, and that solid vote was anti-slavery to support the democratic majority voting in the Kansas Territory. Violence on behalf of Southern honor reached the floor of the Senate in 1856 when a Southern Congressman, Preston Brooks, physically assaulted Republican Senator Charles Sumner when he ridiculed prominent slaveholders as pimps for slavery.
The earlier political party structure failed to make accommodation among sectional differences. Disagreements over slavery caused the Whig and "Know-Nothing" parties to collapse. In 1860, the last national political party, the Democratic Party, split along sectional lines. Anti-slavery Northerners mobilized in 1860 behind moderate Abraham Lincoln because he was most likely to carry the doubtful western states. In 1857, the Supreme Court's "Dred Scott" decision ended the Congressional compromise for Popular Sovereignty in Kansas. According to the court, slavery in the territories was a property right of any settler, regardless of the majority there. Chief Justice Taney's decision said that slaves were "... so far inferior that they had no rights which the white man was bound to respect". The decision overturned the Missouri Compromise, which banned slavery in territory north of the 36°30' parallel.
Republicans denounced the "Dred Scott" decision and promised to overturn it; Abraham Lincoln warned that the next "Dred Scott" decision could threaten the Northern states with slavery. The Republican party platform called slavery "a national evil", and Lincoln believed it would die a natural death if it were contained. The Democrat Stephen A. Douglas developed the Freeport Doctrine to appeal to North and South. Douglas argued, Congress could not decide either for or against slavery before a territory was settled. Nonetheless, the anti-slavery majority in Kansas could stop slavery with its own local laws if their police laws did not protect slavery introduction. Most 1850 political battles followed the arguments of Lincoln and Douglas, focusing on the issue of slavery expansion in the territories.
But political debate was cut short throughout the South with Northern abolitionist John Brown's 1859 raid at Harpers Ferry Armory in an attempt to incite slave insurrections. The Southern political defense of slavery transformed into widespread expansion of local militias for armed defense of their "peculiar" domestic institution. Lincoln's assessment of the political issue for the 1860 elections was that, "This question of Slavery was more important than any other; indeed, so much more important has it become that no other national question can even get a hearing just at present." The Republicans gained majorities in both House and Senate for the first time since the 1856 elections, they were to be seated in numbers that Lincoln might use to govern, a national parliamentary majority even before pro-slavery House and Senate seats were vacated. Meanwhile, Southern Vice President, Alexander Stephens, in the "Cornerstone Speech", declared the new confederate "Constitution has put at rest forever all the agitating questions relating to our peculiar institutions—African slavery as it exists among us—the proper status of the negro in our form of civilization. This was the immediate cause of the late rupture and present revolution." The Republican administration enacted the Confiscation Acts that set conditions for emancipation of slaves prior to the official proclamation of emancipation. Likewise, Lincoln had previously condemned slavery and called for its "extinction."
Considering the relative weight given to causes of the Civil War by contemporary actors, historians such as Chandra Manning argue that both Union and Confederate fighting soldiers believed that slavery caused the Civil War. Union men mainly believed the war was to emancipate the slaves. Confederates fought to protect southern society, and slavery as an integral part of it. Addressing the causes, Eric Foner would relate a historical context with multidimensional political, social and economic variables. The several causes united in the moment by a consolidating nationalism. A social movement that was individualist, egalitarian and perfectionist grew to a political democratic majority attacking slavery, and slavery's defense in the Southern pre-industrial traditional society brought the two sides to war.
States' rights.
Everyone agreed that states had certain rights—but did those rights carry over when a citizen left that state? The Southern position was that citizens of every state had the right to take their property anywhere in the U.S. and not have it taken away—specifically they could bring their slaves anywhere and they would remain slaves. Northerners rejected this "right" because it would violate the right of a free state to outlaw slavery within its borders. Republicans committed to ending the expansion of slavery were among those opposed to any such right to bring slaves and slavery into the free states and territories. The "Dred Scott" Supreme Court decision of 1857 bolstered the Southern case within territories, and angered the North.
Secondly, the South argued that each state had the right to secede—leave the Union—at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a "perpetual union". Historian James McPherson writes concerning states' rights and other non-slavery explanations:
Sectionalism.
Sectionalism refers to the different economies, social structure, customs and political values of the North and South. It increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for the poor whites. The South expanded into rich new lands in the Southwest (from Alabama to Texas).
However, slavery declined in the border states and could barely survive in cities and industrial areas (it was fading out in cities such as Baltimore, Louisville, and St. Louis), so a South based on slavery was rural and non-industrial. On the other hand, as the demand for cotton grew, the price of slaves soared. Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.
Fears of slave revolts and abolitionist propaganda made the South militantly hostile to abolitionism. Southerners complained that it was the North that was changing, and was prone to new "isms", while the South remained true to historic republican values of the Founding Fathers (many of whom owned slaves, including Washington, Jefferson, and Madison). Lincoln said that Republicans were following the tradition of the framers of the Constitution (including the Northwest Ordinance and the Missouri Compromise) by preventing expansion of slavery.
In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations. Industrialization meant that seven European immigrants out of eight settled in the North. The movement of twice as many whites leaving the South for the North as vice versa contributed to the South's defensive-aggressive political behavior.
Protectionism.
Historically, southern slave-holding states, because of their low cost manual labor, had little perceived need for mechanization, and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.
The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The South had no complaints but the low rates angered Northern industrialists and factory workers, especially in Pennsylvania, who demanded protection for their growing iron industry. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were finally enacted in 1861 after Southerners resigned their seats in Congress.
Historians in the 1920s emphasized the tariff issue but since the 1950s they have minimized it, noting that few Southerners in 1860–61 said it was of central importance to them. Some secessionist documents do mention the tariff issue, though not nearly as often as the preservation of slavery.
Slave power and free soil.
Antislavery forces in the North identified the "Slave Power" as a direct threat to republican values. They argued that rich slave owners were using political power to take control of the Presidency, Congress and the Supreme Court, thus threatening the rights of the citizens of the North.
"Free soil" was a Northern demand that the new lands opening up in the west be available to independent yeoman farmers and not be bought out by rich slave owners who would buy up the best land and work it with slaves, forcing the white farmers onto marginal lands. This was the basis of the Free Soil Party of 1848, and a main theme of the Republican Party. Free Soilers and Republicans demanded a homestead law that would give government land to settlers; it was defeated by Southerners who feared it would attract to the west European immigrants and poor Southern whites.
Territorial crisis.
Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. Of the states carved out of these territories by 1845, all had entered the union as slave states: Louisiana, Missouri, Arkansas, Florida and Texas, as well as the southern portions of Alabama and Mississippi. These were balanced by new free states created within the U.S.' original boundary east of the Mississippi River, and the free state of Iowa in 1846. With the conquest of northern Mexico, including California in 1848, slaveholding interests looked forward to the institution flourishing in much of these lands as well. Southerners also anticipated garnering slaves and slave states in Cuba and Central America. Northern free soil interests vigorously sought to curtail any further expansion of slave soil. It was these territorial disputes that the proslavery and antislavery forces collided over. The Compromise of 1850 over California, tried again to reach some political settlement on these issues.
The existence of slavery in the southern states was far less politically polarizing than the explosive question of the territorial expansion of the institution westward. Moreover, Americans were informed by two well-established readings of the Constitution regarding human bondage: first, that the slave states had complete autonomy over the institution within their boundaries, and second, that the domestic slave trade – trade among the states – was immune to federal interference. The only feasible strategy available to attack slavery was to restrict its expansion into the new territories. Slaveholding interests fully grasped the danger that this strategy posed to them. Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."
By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly. Two of the "conservative" doctrines emphasized the written text and historical precedents of the founding document (specifically, the Northwest Ordinance and the Missouri Compromise), while the other two doctrines developed arguments that transcended the Constitution.
The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the historical designation of free and slave apportionments in territories (as done in the Missouri Compromise) should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.
The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance – that slavery could be excluded altogether (as done in the Northwest Ordinance) in a territory at the discretion of Congress – with one caveat: the due process clause of the Fifth Amendment must apply. In other words, Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.
Of the two doctrines that rejected federal authority, one was articulated by northern Democrat of Illinois Senator Stephen A. Douglas, and the other by southern Democratic Senator Jefferson Davis of Mississippi and Vice-President John C. Breckinridge of Kentucky.
Douglas proclaimed the doctrine of territorial or "popular" sovereignty, which declared that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery – a purely local matter. Congress, having created the territory, was barred, according to Douglas, from exercising any authority in domestic matters. To do so would violate historic traditions of self-government, implicit in the US Constitution. The Kansas-Nebraska Act of 1854 legislated this doctrine. In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until after the 1860 elections, when southern senators began to leave.
The fourth in this quartet is the theory of state sovereignty ("states' rights"), also known as the "Calhoun doctrine", named after the South Carolinian political theorist and statesman John C. Calhoun. Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the Federal Union under the US Constitution – and not merely as an argument for secession. The basic premise was that all authority regarding matters of slavery in the territories resided in each state. The role of the federal government was merely to enable the implementation of state laws when residents of the states entered the territories. The Calhoun doctrine asserted that the federal government in the territories was only the agent of the several sovereign states, and hence incapable of forbidding the bringing into any territory of anything that was legal property in any state. State sovereignty, in other words, gave the laws of the slaveholding states "extra-jurisdictional" effect.
"States' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority. As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power."
By 1860, these four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the US Constitution.
National elections.
Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in their sense of country as an important example to the world of a national republic of political liberty and personal rights. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, division and redivision in the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state. In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure—first in their political institutions, then in their civil life together.
Nationalism and honor.
Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy. C. Vann Woodward said of the latter group, "A great slave society ... had grown up and miraculously flourished in the heart of a thoroughly bourgeois and partly puritanical republic. It had renounced its bourgeois origins and elaborated and painfully rationalized its institutional, legal, metaphysical, and religious defenses ... When the crisis came it chose to fight. It proved to be the death struggle of a society, which went down in ruins." Perceived insults to Southern collective honor included the enormous popularity of "Uncle Tom's Cabin" (1852) and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.
While the South moved toward a Southern nationalism, leaders in the North were also becoming more nationally minded, and rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion ... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence." The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.
Lincoln's election.
The election of Lincoln in November 1860 was the final trigger for secession. Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed.
Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.
Secession and war begins.
Resolves and developments.
Secession of South Carolina.
South Carolina did more to advance nullification and secession than any other Southern state. South Carolina adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union" on December 24, 1860. It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution.
Secession winter.
Before Lincoln took office, seven states had declared their secession from the Union. They established a Southern government, the Confederate States of America on February 4, 1861. They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "... was intended to be perpetual," but that, "The power by force of arms to compel a State to remain in the Union," was not among the "... enumerated powers granted to Congress." One quarter of the U.S. Army—the entire garrison in Texas—was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.
As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts),
the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.
States align.
Confederate states.
Seven Deep South cotton states seceded by February 1861, starting with South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas. These seven states formed the Confederate States of America (February 4, 1861), with Jefferson Davis as president, and a governmental structure closely modeled on the U.S. Constitution.
Following the attack on Fort Sumter, President Lincoln called for a volunteer army from each state. Within two months, an additional four Southern slave states declared their secession and joined the Confederacy: Virginia, Arkansas, North Carolina and Tennessee. The northwestern portion of Virginia subsequently seceded from Virginia, joining the Union as the new state of West Virginia on June 20, 1863. By the end of 1861, Missouri and Kentucky were effectively under Union control, with Confederate state governments in exile.
Among the ordinances of secession passed by the individual states, those of three – Texas, Alabama, and Virginia – specifically mentioned the plight of the 'slaveholding states' at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures.
However, at least four states – South Carolina,
Mississippi,
Georgia,
and Texas
– also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the influence over the northern states of the movement to abolish slavery, something regarded as a Constitutional right by the slaveholding states.
Union states.
Twenty-three states remained loyal to the Union: California, Connecticut, Delaware, Illinois, Indiana, Iowa, Kansas, Kentucky, Maine, Maryland, Massachusetts, Michigan, Minnesota, Missouri, New Hampshire, New Jersey, New York, Ohio, Oregon, Pennsylvania, Rhode Island, Vermont, and Wisconsin. During the war, Nevada and West Virginia joined as new states of the Union. Tennessee and Louisiana were returned to Union military control early in the war.
The territories of Colorado, Dakota, Nebraska, Nevada, New Mexico, Utah, and Washington fought on the Union side. Several slave-holding Native American tribes supported the Confederacy, giving the Indian Territory (now Oklahoma) a small, bloody civil war.
Border states.
The border states in the Union were West Virginia (which separated from Virginia and became a new state), and four of the five northernmost slave states (Maryland, Delaware, Missouri, and Kentucky).
Maryland had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. (Maryland's legislature voted to stay in the Union, but also rejected hostilities with the South, voting to close Maryland's rail lines to prevent them from being used for war.) Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North. Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened. All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus, (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.
In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. ("See also: Missouri secession"). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.
Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.
After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34% approved the statehood bill (96% approving). The inclusion of 24 secessionist counties in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war. Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000–22,000 soldiers to both the Confederacy and the Union.
A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.
Beginning the war.
Lincoln's victory in the presidential election of 1860 triggered South Carolina's declaration of secession from the Union in December, and six more states did so by February 1861. A pre-war February Peace Conference of 1861 met in Washington, Lincoln sneaking into town to stay in the Conference's hotel its last three days. The attempt failed at resolving the crisis, but the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.
Lincoln's policy.
Since December, secessionists with and without state forces had seized Federal Court Houses, U.S. Treasury mints and post offices. Southern governors ordered militia mobilization, seized most of the federal forts and cannon within their boundaries and U.S. armories of infantry weapons. The governors in big-state Republican strongholds of Massachusetts, New York, and Pennsylvania quietly began buying weapons and training militia units themselves. President Buchanan protested seizure of Federal property, but made no military response apart from a failed attempt on January 9, 1861 to resupply Fort Sumter using the ship "Star of the West", which was fired upon by South Carolina forces and turned back before it reached the fort.
On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a "more perfect union" than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void". He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. Marshals and Judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia and North Carolina. In Lincoln's Inaugural, U.S. policy would only collect import duties at its ports, there could be no serious injury to justify revolution in the politics of four years. His speech closed with a plea for restoration of the bonds of union.
The South sent delegations to Washington and offered to pay for the federal properties and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government. Secretary of State William Seward who at that time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed. President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina's Fort Sumter.
Battle of Fort Sumter.
Ft. Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. forts garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12–13, forcing its capitulation. On April 15, Lincoln's Secretary of War then called on Governors for 75,000 volunteers to recapture the fort and other federal property.
Northerners rallied behind Lincoln's call for all the states to send troops to recapture the forts and to preserve the Union, citing presidential powers given by the Militia Acts of 1792. With the scale of the rebellion apparently small so far, Lincoln called for 75,000 volunteers for 90 days. Several Northern governors began to move forces the next day, and Secessionists seized Liberty Arsenal in Liberty, Missouri the next week. Two weeks later, on May 3, 1861, Lincoln called for an additional 42,034 volunteers for a period of three years.
Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond. 
The War.
The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, and many more minor actions and skirmishes. In the scales of world military history, both sides fighting were characterized by their bitter intensity and high casualties. "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy's soldier. 
Mobilization.
As the first seven states began organizing a Confederacy in Montgomery, the entire US army numbered 16,000. However, Northern governors had begun to mobilize their militias. The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. After Fort Sumter, Lincoln called out 75,000 three-month volunteers, by May Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.
In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law—conscription—as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt. The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.
When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft. Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.
North and South, the draft laws were highly unpopular. An estimated 120,000 men evaded conscription in the North, many of them fleeing to Canada, and another 280,000 Northern soldiers deserted during the war, along with at least 100,000 Southerners, or about 10% all together. However, desertion was a common event in the 19th century; in the peacetime Army about 15% of the soldiers deserted every year. In the South, many men deserted temporarily to take care of their families, then returned to their units. In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.
From a tiny frontier force in 1860, in a few years the Union and Confederates armies had grown into the "largest and most efficient armies in the world." European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan's assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.
Motivation.
Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:
Prisoners.
At the start of the civil war a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile they were held in camps run by their own army where they were paid but not allowed to perform any military duties. The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that about 56,000 of the 409,000 POWs died in prisons during the War, accounting for nearly 10% of the conflict's fatalities.
Naval war.
The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396. Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy. Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.
Union blockade.
By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible. Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion however demanded an immediate attack by the army to capture Richmond.
In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake it was too late. "King Cotton" was dead, as the South could export less than 10% of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.
Modern navy evolves.
The Civil War prompted the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union's naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries. Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union's own ironclad warships, they were unsuccessful.
The Confederacy experimented with a submarine, which did not work well, and with building an ironclad ship, the CSS "Virginia", which was based on rebuilding a sunken Union ship, the "Merrimac". On its first foray on March 8, 1862, the "Virginia" decimated the Union's wooden fleet, but the next day the first Union ironclad, the USS "Monitor", arrived to challenge it. The Battle of the Ironclads was a draw, but it marks the worldwide transition to ironclad warships.
The Confederacy lost the "Virginia" when the ship was scuttled to prevent capture, and the Union built many copies of the "Monitor". Lacking the technology to build effective warships, the Confederacy attempted to obtain warships from Britain.
Blockade runners.
British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. The ships were so small that only a small amount of cotton went out. When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released. The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Historians agree that the blockade was a major factor in ruining the Confederate economy. However, Wise argues that they provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.
Economic impact.
Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well. The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.
To fight an offensive war the Confederacy purchased ships from Britain, converted them to warships, and raided American merchants ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested. After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.
Rivers.
The 1862 Union strategy called for simultaneous advances along four axes. McClellan would lead the main thrust in Virginia towards Richmond. Ohio forces were to advance through Kentucky into Tennessee, the Missouri Department would drive south along the Mississippi River, and the westernmost attack would originate from Kansas.
Ulysses Grant used river transport and Andrew Foote's gunboats of the Western Flotilla to threaten the Confederacy's "Gibraltar of the West" at Columbus, Kentucky. Grant was rebuffed at Belmont, but cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.
In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action. They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry and Fort Donelson, and supplied Grant's forces as he moved into Tennessee. At Shiloh, (Pittsburg Landing) in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory – the first battle with the high casualty rates that would repeat over and over. Memphis fell to Union forces and became a key base for further advances south along the Mississippi River. In April 1862, US Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederates abandoned the city, which gave the Union a critical anchor in the deep South.
Naval forces assisted Grant in his long, complex campaign that resulted in the surrender of Vicksburg in July 1863, and full Union control of the Mississippi soon after.
Eastern theater.
Because of the fierce resistance of a few initial Confederate forces at Manassas, Virginia, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces there was halted in the First Battle of Bull Run, or "First Manassas". McDowell's troops were forced back to Washington, D.C., by the Confederates under the command of Generals Joseph E. Johnston and P. G. T. Beauregard. It was in this battle that Confederate General Thomas Jackson received the nickname of "Stonewall" because he stood like a stone wall against Union troops.
Alarmed at the loss, and in an attempt to prevent more slave states from leaving the Union, the U.S. Congress passed the Crittenden-Johnson Resolution on July 25 of that year, which stated that the war was being fought to preserve the Union and not to end slavery.
Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign, Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat. The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South. McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops.
Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history. Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.
When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg on December 13, 1862, when over 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker.
Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863. Gen. Stonewall Jackson was mortally wounded by his own men during the battle and subsequently died of complications. Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863). This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000). However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant.
Western theater.
While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge. Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization.
The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans, which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.
General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state. Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.
The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged.
The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh; and the Battle of Vicksburg, which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga, driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.
Trans-Mississippi.
Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control. Roving Confederate bands such as Quantrill's Raiders terrorized the countryside, striking both military installations and civilian settlements. The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.
By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.
Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out inside the tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union. The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.
After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him. Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.
End of war.
Conquest of Virginia.
At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war. This was total war not in killing civilians but rather in destroying homes, farms, and railroads. Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.
Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.
Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.
Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president. Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.
Leaving Atlanta, and his base of supplies, Sherman's army marched with an unknown destination, laying waste to about 20% of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army.
Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire parameter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west and after a defeat at Sayler's Creek.
Confederacy surrenders.
Initially, Lee was not intending to surrender, but rather to regroup at the village of Appomattox Court House, where supplies were to be waiting, and to continue the war. Grant chased Lee, and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and so he surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House.
In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller. On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them. President Johnson officially declared a virtual end to the insurrection on May 9, 1865. Confederate President Jefferson Davis was captured the following day. On June 23, 1865, Cherokee leader Stand Watie was the last Confederate General to surrender his forces.
Diplomacy.
Europe in the 1860s was more fragmented than it had been since before the American Revolution. France was in a weakened state while Britain was still shocked by its own poor performance in the Crimean War. France was unable or unwilling to support either side without Britain, where popular support remained with the Union though elite opinion was more varied. They were further distracted by Germany and Italy, who were experiencing unification troubles, and by Russia, who was almost unflinching in their support for the Union.
Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators. The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.
Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860–62 crop failures in Europe made the North's grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half. When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.
Charles Francis Adams proved particularly adept as minister to Britain for the U.S. and Britain was reluctant to boldly challenge the blockade. The Confederacy purchased several warships from commercial ship builders in Britain (CSS "Alabama", CSS Shenandoah, CSS Tennessee, CSS Tallahassee, CSS Florida and some others). The most famous, the CSS "Alabama", did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for European politicians, especially in Britain (which had abolished slavery in her colonies in 1834).
War loomed in late 1861 between the U.S. and Britain over the Trent Affair, involving the U.S. Navy's boarding of a British mail steamer to seize two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation—though even such an offer would have risked war with the U.S. Lord Palmerston reportedly read "Uncle Tom's Cabin" three times when deciding on this.
The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.
Victory and aftermath.
Results.
The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty. The Freedmen did indeed get their freedom, their citizenship, and control of their lives, their families and their churches.
Historians have debated whether the Confederacy could have won the war. Most scholars, such as James McPherson, argue that Confederate victory was at least possible. McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.
Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win. Lincoln was not a military dictator, and could only continue to fight the war as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.
Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat. Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back ... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don't think the South ever had a chance to win that War."
A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it,"people did not will hard enough and long enough to win." The black Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederates army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict. However, most historians reject the argument. James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864-5, he says most Confederate soldiers were fighting hard. Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up – some few deserters – plenty tired of war, but the masses determined to fight it out."
Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. Although Lincoln's approach to emancipation was slow, the Emancipation Proclamation was an effective use of the President's war powers. The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities.
Lincoln's naval blockade was 95% effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.
Costs.
The war produced about 1,030,000 casualties (3% of the population), including about 620,000 soldier deaths—two-thirds by disease, and 50,000 civilians. Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20% higher than traditionally estimated, and possibly as high as 850,000. The war accounted for roughly as many American deaths as all American deaths in other U.S. wars combined.
Based on 1860 census figures, 8% of all white males aged 13 to 43 died in the war, including 6% in the North and 18% in the South. About 56,000 soldiers died in prison camps during the War. An estimated 60,000 men lost limbs in the war.
Confederate death toll estimates vary considerably. Union army dead, amounting to 15% of the over two million who served, was broken down as follows:
Black troops accounted for 10% of the Union death toll, they amounted to 15% of disease deaths but less than 3% of those killed in battle.
Losses can be viewed as high considering that the defeat of Mexico in 1846–48 resulted in fewer than 2,000 soldiers killed in battle. One reason for the high number of battle deaths during the war was the use of Napoleonic tactics, such as charging. With the advent of more accurate rifled barrels, Minié balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined the better part of World War I.
The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 18, 1865) by the Thirteenth Amendment.
The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40% of that of the North, a condition that lasted until well into the 20th century. Southern influence in the US federal government, previously considerable, was greatly diminished until the latter half of the 20th century. The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.
Emancipation.
Issue of Slavery During the War.
While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee's army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery. Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal. Lincoln's decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans. By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.
Emancipation Proclamation.
The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army. About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.
During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game." Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.
At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Frémont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected. But only the District of Columbia accepted Lincoln's gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat". Lincoln laid the groundwork for public support in an open letter published letter to abolitionist Horace Greeley's newspaper.
In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors' Conference added support for the proclamation. Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong ... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling ... I claim not to have controlled events, but confess plainly that events have controlled me."
Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.
Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty. The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France. By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.
Texas v. White.
In "Texas v. White", the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States of America; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.
Reconstruction.
Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863 and continued to 1877. It comprised multiple complex methods to resolve the war, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to guarantee the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery—and prevent semi-slavery status.
President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded strong proof that Confederate nationalism was dead and the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson's work. They used the Army to dissolve Southern state governments and hold new elections with Freedmen voting. The result was a Republican coalition that took power in ten states for varying lengths of time, staying in power with the help of U.S. Army units and black voters. Grant was elected president in 1868 and continued the Radical policies. Meanwhile the Freedmen's Bureau, started by Lincoln in 1865 to help the freed slaves, played a major role in helping the blacks and arranging work for them. In opposition paramilitary groups such as the first Ku Klux Klan used violence to thwart these efforts.
The "Liberal Republicans" argued the war goals had been achieved and Reconstruction should end. They ran a ticket in 1872 but were decisively defeated as Grant was reelected. In 1874, Democrats took control of Congress and opposed any more reconstruction. The disputed 1876 elections were resolved by the Compromise of 1877, which put Republican Rutherford B. Hayes in the White House. He pulled out the last federal troops and the last Republican state governments in the South collapsed, marking the end of Civil War and Reconstruction.
Memory and historiography.
The Civil War is one of the central events in America's collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war's aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war. The last theme includes moral evaluations of racism and slavery, heroism in combat and behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.
Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:
In sharp contrast, Black preachers interpreted the Civil War as:
Lost Cause.
Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations. Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery. Some appeals highlight cultural differences North and South. The military conflict by Confederate actors is idealized. In any case, secession was said to be lawful. The two important political legacies flowed from the adoption of the Lost Cause analysis were that it facilitated the reunification of the North and the South, and it excused the “virulent racism” of the 19th century, sacrificing African-American progress to a white man’s reunification. But the Lost Cause legacy to history is “a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter” in every instance.
Beardian historiography.
The most influential interpretation of the Civil War was the Beardian approach presented by Charles A. Beard and Mary R. Beard in "The Rise of American Civilization" (1927). It was highly influential among historians and the general public until the civil rights era of the 1950s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really a:
The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.
Civil War commemoration.
The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary.
Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as "Birth of a Nation" (1915), "Gone with the Wind" (1939), and "Lincoln (2012)".
See also.
General reference
Union
Confederacy
Ethnic articles
Topical articles
National articles

</doc>
<doc id="864" url="http://en.wikipedia.org/wiki?curid=864" title="Andy Warhol">
Andy Warhol

Andy Warhol (; August 6, 1928 – February 22, 1987) was an American artist who was a leading figure in the visual art movement known as pop art. His works explore the relationship between artistic expression, celebrity culture and advertisement that flourished by the 1960s. After a successful career as a commercial illustrator, Warhol became a renowned and sometimes controversial artist. The Andy Warhol Museum in his native city, Pittsburgh, Pennsylvania, holds an extensive permanent collection of art and archives. It is the largest museum in the United States dedicated to a single artist.
Warhol's art encompassed many forms of media, including hand drawing, painting, printmaking, photography, silk screening, sculpture, film, and music. He was also a pioneer in computer-generated art using Amiga computers that were introduced in 1984, two years before his death. He founded "Interview Magazine" and was the author of numerous books, including "The Philosophy of Andy Warhol" and "". He managed and produced the Velvet Underground, a rock band which had a strong influence on the evolution of punk rock music. He is also notable as a gay man who lived openly as such before the gay liberation movement. His studio, The Factory, was a famous gathering place that brought together distinguished intellectuals, drag queens, playwrights, Bohemian street people, Hollywood celebrities, and wealthy patrons.
Warhol has been the subject of numerous retrospective exhibitions, books, and feature and documentary films. He coined the widely used expression "15 minutes of fame". Many of his creations are very collectible and highly valuable. The highest price ever paid for a Warhol painting is US$105 million for a 1963 canvas titled "Silver Car Crash (Double Disaster)". A 2009 article in "The Economist" described Warhol as the "bellwether of the art market". Warhol's works include some of the most expensive paintings ever sold.
Early life (1928–1949).
Andy Warhol ("né" Andrej Varhola, Jr.) was born on August 6, 1928 in Pittsburgh, Pennsylvania. He was the fourth child of Andrej Varhola (Americanized as Andrew Warhola, Sr., 1889–1942) and Júlia ("née" Zavacká, 1892–1972), whose first child was born in their homeland and died before their move to the U.S. Andy had two older brothers, Paul (June 26, 1922 - January 30, 2014) and John Warhola (May 31, 1925 – December 24, 2010).
His parents were working-class Lemko emigrants from Mikó (now called Miková), located in today's northeastern Slovakia, part of the former Austro-Hungarian Empire. Warhol's father immigrated to the United States in 1914, and his mother joined him in 1921, after the death of Warhol's grandparents. Warhol's father worked in a coal mine. The family lived at 55 Beelen Street and later at 3252 Dawson Street in the Oakland neighborhood of Pittsburgh. The family was Byzantine Catholic and attended St. John Chrysostom Byzantine Catholic Church. Andy Warhol had two older brothers—Pavol (Paul), the oldest, was born before the family emigrated; Ján was born in Pittsburgh. Pavol's son, James Warhola, became a successful children's book illustrator. About 1939, he started to collect autographed cards of film stars.
In third grade, Warhol had Sydenham's chorea (also known as St. Vitus' Dance), the nervous system disease that causes involuntary movements of the extremities, which is believed to be a complication of scarlet fever which causes skin pigmentation blotchiness. He became a hypochondriac, developing a fear of hospitals and doctors. Often bedridden as a child, he became an outcast at school and bonded with his mother. At times when he was confined to bed, he drew, listened to the radio and collected pictures of movie stars around his bed. Warhol later described this period as very important in the development of his personality, skill-set and preferences. When Warhol was 13, his father died in an accident.
As a teenager, Warhol graduated from Schenley High School in 1945. After graduating from high school, his intentions were to study art education at the University of Pittsburgh in the hope of becoming an art teacher, but his plans changed and he enrolled in the Carnegie Institute of Technology in Pittsburgh, where he studied commercial art. In 1949, he moved to New York City and began a career in magazine illustration and advertising. In 1949, he earned a Bachelor of Fine Arts in pictorial design.
1950s.
During the 1950s, Warhol gained fame for his whimsical ink drawings of shoe advertisements. These were done in a loose, blotted-ink style, and figured in some of his earliest showings at the Bodley Gallery in New York. With the concurrent rapid expansion of the record industry and the introduction of the vinyl record, Hi-Fi, and stereophonic recordings, RCA Records hired Warhol, along with another freelance artist, Sid Maurer, to design album covers and promotional materials.
Warhol was an early adopter of the silk screen printmaking process as a technique for making paintings. His earliest silkscreening in painting involved hand-drawn images though this soon progressed to the use of photographically derived silkscreening in paintings. Prior to entering the field of fine art, Warhol's commercial art background also involved innovative techniques for image making that were somewhat related to printmaking techniques. When rendering commercial objects for advertising Warhol devised a technique that resulted in a characteristic image. His imagery used in advertising was often executed by means of applying ink to paper and then blotting the ink while still wet. This was akin to a printmaking process on the most rudimentary scale.
Warhol's work both as a commercial artist and later a fine artist displays a casual approach to image making, in which chance plays a role and mistakes and unintentional marks are tolerated. The resulting imagery in both Warhol's commercial art and later in his fine art endeavors is often replete with imperfection—smudges and smears can often be found. In his book "POPism" Warhol writes, "When you do something exactly wrong, you always turn up something."
1960s.
He began exhibiting his work during the 1950s. He held exhibitions at the Hugo Gallery, and the Bodley Gallery in New York City and in California his first West Coast gallery exhibition was on July 9, 1962, in the Ferus Gallery of Los Angeles. The exhibition marked his West Coast debut of pop art.
Andy Warhol's first New York solo pop art exhibition was hosted at Eleanor Ward's Stable Gallery November 6–24, 1962. The exhibit included the works "Marilyn Diptych", "100 Soup Cans", "100 Coke Bottles," and "100 Dollar Bills". At the Stable Gallery exhibit, the artist met for the first time poet John Giorno who would star in Warhol's first film, "Sleep", in 1963.
It was during the 1960s that Warhol began to make paintings of iconic American objects such as dollar bills, mushroom clouds, electric chairs, Campbell's Soup Cans, Coca-Cola bottles, celebrities such as Marilyn Monroe, Elvis Presley, Marlon Brando, Troy Donahue, Muhammad Ali, and Elizabeth Taylor, as well as newspaper headlines or photographs of police dogs attacking civil rights protesters. During these years, he founded his studio, "The Factory" and gathered about him a wide range of artists, writers, musicians, and underground celebrities. His work became popular and controversial. Warhol had this to say about Coca Cola:
New York's Museum of Modern Art hosted a Symposium on pop art in December 1962 during which artists like Warhol were attacked for "capitulating" to consumerism. Critics were scandalized by Warhol's open embrace of market culture. This symposium set the tone for Warhol's reception. Throughout the decade it became increasingly clear that there had been a profound change in the culture of the art world, and that Warhol was at the center of that shift.
A pivotal event was the 1964 exhibit "The American Supermarket", a show held in Paul Bianchini's Upper East Side gallery. The show was presented as a typical U.S. small supermarket environment, except that everything in it—from the produce, canned goods, meat, posters on the wall, etc.—was created by six prominent pop artists of the time, among them the controversial (and like-minded) Billy Apple, Mary Inman, and Robert Watts. Warhol's painting of a can of Campbell's soup cost $1,500 while each autographed can sold for $6. The exhibit was one of the first mass events that directly confronted the general public with both pop art and the perennial question of what art is.
As an advertisement illustrator in the 1950s, Warhol used assistants to increase his productivity. Collaboration would remain a defining (and controversial) aspect of his working methods throughout his career; this was particularly true in the 1960s. One of the most important collaborators during this period was Gerard Malanga. Malanga assisted the artist with the production of silkscreens, films, sculpture, and other works at "The Factory," Warhol's aluminum foil-and-silver-paint-lined studio on 47th Street (later moved to Broadway). Other members of Warhol's Factory crowd included Freddie Herko, Ondine, Ronald Tavel, Mary Woronov, Billy Name, and Brigid Berlin (from whom he apparently got the idea to tape-record his phone conversations).
During the 1960s, Warhol also groomed a retinue of bohemian and counterculture eccentrics upon whom he bestowed the designation "Superstars", including Nico, Joe Dallesandro, Edie Sedgwick, Viva, Ultra Violet, Holly Woodlawn, Jackie Curtis, and Candy Darling. These people all participated in the Factory films, and some—like Berlin—remained friends with Warhol until his death. Important figures in the New York underground art/cinema world, such as writer John Giorno and film-maker Jack Smith, also appear in Warhol films of the 1960s, revealing Warhol's connections to a diverse range of artistic scenes during this time.
Attempted murder (1968).
On June 3, 1968, radical feminist writer Valerie Solanas shot Warhol and Mario Amaya, art critic and curator, at Warhol's studio. Before the shooting, Solanas had been a marginal figure in the Factory scene. She authored in 1967 the "S.C.U.M. Manifesto", a separatist feminist tract that advocated the elimination of men; and appeared in the 1968 Warhol film "I, a Man". Earlier on the day of the attack, Solanas had been turned away from the Factory after asking for the return of a script she had given to Warhol. The script had apparently been misplaced.
Amaya received only minor injuries and was released from the hospital later the same day. Warhol was seriously wounded by the attack and barely survived: surgeons opened his chest and massaged his heart to help stimulate its movement again. He suffered physical effects for the rest of his life, including being required to wear a surgical corset. The shooting had a profound effect on Warhol's life and art.
Solanas was arrested the day after the assault. By way of explanation, she said that Warhol "had too much control over my life." She was eventually sentenced to three years under the control of the Department of Corrections. After the shooting, the Factory scene became much more tightly controlled, and for many the "Factory 60s" ended.
Warhol had this to say about the attack: "Before I was shot, I always thought that I was more half-there than all-there—I always suspected that I was watching TV instead of living life. People sometimes say that the way things happen in movies is unreal, but actually it's the way things happen in life that's unreal. The movies make emotions look so strong and real, whereas when things really do happen to you, it's like watching television—you don't feel anything. Right when I was being shot and ever since, I knew that I was watching television. The channels switch, but it's all television."
1970s.
Compared to the success and scandal of Warhol's work in the 1960s, the 1970s were a much quieter decade, as he became more entrepreneurial. According to Bob Colacello, Warhol devoted much of his time to rounding up new, rich patrons for portrait commissions—including Shah of Iran Mohammad Reza Pahlavi, his wife Empress Farah Pahlavi, his sister Princess Ashraf Pahlavi, Mick Jagger, Liza Minnelli, John Lennon, Diana Ross, and Brigitte Bardot. Warhol's famous portrait of Chinese Communist leader Mao Zedong was created in 1973. He also founded, with Gerard Malanga, "Interview" magazine, and published "The Philosophy of Andy Warhol" (1975). An idea expressed in the book: "Making money is art, and working is art and good business is the best art." 
Warhol used to socialize at various nightspots in New York City, including Max's Kansas City; and, later in the 1970s, Studio 54. He was generally regarded as quiet, shy, and a meticulous observer. Art critic Robert Hughes called him "the white mole of Union Square."
With his longtime friend Stuart Pivar, Warhol founded the New York Academy of Art in 1979.
1980s.
Warhol had a re-emergence of critical and financial success in the 1980s, partially due to his affiliation and friendships with a number of prolific younger artists, who were dominating the "bull market" of 1980s New York art: Jean-Michel Basquiat, Julian Schnabel, David Salle and other so-called Neo-Expressionists, as well as members of the Transavantgarde movement in Europe, including Francesco Clemente and Enzo Cucchi.
By this period, Warhol was being criticized for becoming merely a "business artist". In 1979, reviewers disliked his exhibits of portraits of 1970s personalities and celebrities, calling them superficial, facile and commercial, with no depth or indication of the significance of the subjects. They also criticized his 1980 exhibit of 10 portraits at the Jewish Museum in New York, entitled "Jewish Geniuses", which Warhol—who was uninterested in Judaism and Jews—had described in his diary as "They're going to sell." In hindsight, however, some critics have come to view Warhol's superficiality and commerciality as "the most brilliant mirror of our times," contending that "Warhol had captured something irresistible about the zeitgeist of American culture in the 1970s."
Warhol also had an appreciation for intense Hollywood glamour. He once said: "I love Los Angeles. I love Hollywood. They're so beautiful. Everything's plastic, but I love plastic. I want to be plastic."
Death.
Warhol died in New York City at 6:32 am on February 22, 1987. According to news reports, he had been making good recovery from a routine gallbladder surgery at New York Hospital before dying in his sleep from a sudden post-operative cardiac arrhythmia. Prior to his diagnosis and operation, Warhol delayed having his recurring gallbladder problems checked, as he was afraid to enter hospitals and see doctors. His family sued the hospital for inadequate care, saying that the arrhythmia was caused by improper care and water intoxication. The malpractice case was quickly settled out of court; Warhol's family received an undisclosed sum of money.
Warhol's body was taken back to Pittsburgh by his brothers for burial. The wake was at Thomas P. Kunsak Funeral Home and was an open-coffin ceremony. The coffin was a solid bronze casket with gold plated rails and white upholstery. Warhol was dressed in a black cashmere suit, a paisley tie, a platinum wig, and sunglasses. He was posed holding a small prayer book and a red rose. The funeral liturgy was held at the Holy Ghost Byzantine Catholic Church on Pittsburgh's North Side. The eulogy was given by Monsignor Peter Tay. Yoko Ono and John Richardson were speakers. The coffin was covered with white roses and asparagus ferns. After the liturgy, the coffin was driven to St. John the Baptist Byzantine Catholic Cemetery in Bethel Park, a south suburb of Pittsburgh.
At the grave, the priest said a brief prayer and sprinkled holy water on the casket. Before the coffin was lowered, Paige Powell dropped a copy of "Interview" magazine, an "Interview" T-shirt, and a bottle of the Estee Lauder perfume "Beautiful" into the grave. Warhol was buried next to his mother and father. A memorial service was held in Manhattan for Warhol on April 1, 1987, at St. Patrick's Cathedral, New York.
Foundation.
Warhol's will dictated that his entire estate — with the exception of a few modest legacies to family members — would go to create a foundation dedicated to the "advancement of the visual arts". Warhol had so many possessions that it took Sotheby's nine days to auction his estate after his death; the auction grossed more than US$20 million.
In 1987, in accordance with Warhol's will, the Andy Warhol Foundation for the Visual Arts began. The foundation serves as the estate of Andy Warhol, but also has a mission "to foster innovative artistic expression and the creative process" and is "focused primarily on supporting work of a challenging and often experimental nature."
The Artists Rights Society is the U.S. copyright representative for the Andy Warhol Foundation for the Visual Arts for all Warhol works with the exception of Warhol film stills. The U.S. copyright representative for Warhol film stills is the Warhol Museum in Pittsburgh. Additionally, the Andy Warhol Foundation for the Visual Arts has agreements in place for its image archive. All digital images of Warhol are exclusively managed by Corbis, while all transparency images of Warhol are managed by Art Resource.
The Andy Warhol Foundation released its "20th Anniversary Annual Report" as a three-volume set in 2007: Vol. I, 1987–2007; Vol. II, Grants & Exhibitions; and Vol. III, Legacy Program. The Foundation remains one of the largest grant-giving organizations for the visual arts in the U.S.
Works.
Paintings.
By the beginning of the 1960s, Warhol had become a very successful commercial illustrator. His detailed and elegant drawings for I. Miller shoes were particularly popular. They consisted mainly of "blotted ink" drawings (or monoprints), a technique which he applied in much of his early art. Although many artists of this period worked in commercial art, most did so discreetly. Warhol was so successful, however, that his profile as an illustrator seemed to undermine his efforts to be taken seriously as an artist.
Pop art was an experimental form that several artists were independently adopting; some of these pioneers, such as Roy Lichtenstein, would later become synonymous with the movement. Warhol, who would become famous as the "Pope of Pop", turned to this new style, where popular subjects could be part of the artist's palette. His early paintings show images taken from cartoons and advertisements, hand-painted with paint drips. Marilyn Monroe was a pop art painting that Warhol had done and it was very popular. Those drips emulated the style of successful abstract expressionists (such as Willem de Kooning). Warhol's first pop art paintings were displayed in April 1961, serving as the backdrop for New York Department Store Bronwit Teller's window display. This was the same stage his Pop Art contemporaries Jasper Johns, James Rosenquist and Robert Rauschenberg had also once graced. Eventually, Warhol pared his image vocabulary down to the icon itself—to brand names, celebrities, dollar signs—and removed all traces of the artist's "hand" in the production of his paintings.
To him, part of defining a niche was defining his subject matter. Cartoons were already being used by Lichtenstein, typography by Jasper Johns, and so on; Warhol wanted a distinguishing subject. His friends suggested he should paint the things he loved the most. It was the gallerist Muriel Latow who came up with the ideas for both the soup cans and Warhol's dollar paintings. On November 23, 1961 Warhol wrote Latow a check for $50 which, according to the 2009 Warhol biography, "Pop, The Genius of Warhol", was payment for coming up with the idea of the soup cans as subject matter.
For his first major exhibition Warhol painted his famous cans of Campbell's Soup, which he claimed to have had for lunch for most of his life. The work sold for $10,000 at an auction on November 17, 1971, at Sotheby's New York.
He loved celebrities, so he painted them as well. From these beginnings he developed his later style and subjects. Instead of working on a signature subject matter, as he started out to do, he worked more and more on a signature style, slowly eliminating the handmade from the artistic process. Warhol frequently used silk-screening; his later drawings were traced from slide projections. At the height of his fame as a painter, Warhol had several assistants who produced his silk-screen multiples, following his directions to make different versions and variations.
In 1979, Warhol was commissioned by BMW to paint a Group 4 race version of the then elite supercar BMW M1 for the fourth installment in the BMW Art Car Project. Unlike the three artists before him, Warhol declined the use of a small scale practice model, instead opting to immediately paint directly onto the full scale automobile. It was indicated that Warhol spent only a total of 23 minutes to paint the entire car.
Warhol produced both comic and serious works; his subject could be a soup can or an electric chair. Warhol used the same techniques—silkscreens, reproduced serially, and often painted with bright colors—whether he painted celebrities, everyday objects, or images of suicide, car crashes, and disasters, as in the 1962–1963 "Death and Disaster" series. The "Death and Disaster" paintings included "Red Car Crash", "Purple Jumping Man", and "Orange Disaster." One of these paintings, the diptych "Silver Car Crash", became the highest priced work of his when it sold at Sotheby's Contemporary Art Auction on Wednesday, November 13, 2013 for $105.4 million.
Some of Warhol's work, as well as his own personality, has been described as being Keatonesque. Warhol has been described as playing dumb to the media. He sometimes refused to explain his work. He has suggested that all one needs to know about his work is "already there 'on the surface.'"
His Rorschach inkblots are intended as pop comments on art and what art could be. His cow wallpaper (literally, wallpaper with a cow motif) and his oxidation paintings (canvases prepared with copper paint that was then oxidized with urine) are also noteworthy in this context. Equally noteworthy is the way these works—and their means of production—mirrored the atmosphere at Andy's New York "Factory". Biographer Bob Colacello provides some details on Andy's "piss paintings":
Warhol's first portrait of "Basquiat" (1982) is a black photosilkscreen over an oxidized copper "piss painting".
After many years of silkscreen, oxidation, photography, etc., Warhol returned to painting with a brush in hand in a series of over 50 large collaborative works done with Jean-Michel Basquiat between 1984 and 1986. Despite negative criticism when these were first shown, Warhol called some of them "masterpieces," and they were influential for his later work.
The influence of the large collaborations with Basquiat can be seen in Warhol's "The Last Supper" cycle, his last and possibly his largest series.
Andy Warhol was commissioned in 1984 by the gallerist Alexander Iolas to produce work based on Leonardo da Vinci's "The Last Supper" for an exhibition at the old refectory of the Palazzo delle Stelline in Milan, opposite from the Santa Maria delle Grazie where Leonardo da Vinci's mural can be seen.
Warhol exceeded the demands of the commission and produced nearly 100 variations on the theme, mostly silkscreens and paintings, and among them a collaborative sculpture with Basquiat, the "Ten Punching Bags (Last Supper)".
The Milan exhibition that opened in January 1987 with a set of 22 silk-screens, was the last exhibition for both the artist and the gallerist.
The series of "The Last Supper" was seen by some as "arguably his greatest," but by others as "wishy-washy, religiose" and "spiritless." It is also the largest series of religious-themed works by any U.S. artist.
At the time of his death, Warhol was working on "Cars", a series of paintings for Mercedes-Benz.
A self-portrait by Andy Warhol (1963–1964), which sold in New York at the May Post-War and Contemporary evening sale in Christie's, fetched $38.4 million.
On May 9, 2012, his classic painting "Double Elvis (Ferus Type)" sold at auction at Sotheby's in New York for US$33 million. With commission, the sale price totaled US$37,042,500, short of the $50 million that Sotheby's had predicted the painting might bring. The piece (silkscreen ink and spray paint on canvas) shows Elvis Presley in a gunslinger pose. It was first exhibited in 1963 at the Ferus Gallery in Los Angeles. Warhol made 22 versions of the "Double Elvis," nine of which are held in museums.
On Wednesday, November 13, 2013 his "Silver Car Crash (Double Disaster) diptych sold at Sotheby's Contemporary Art Auction for $105.4 million, a new record for the famed pop artist (pre-auction estimates at $80 million). Created in 1963, this work has only been seen in public once in the past 26 years.
Films.
Warhol worked across a wide range of media—painting, photography, drawing, and sculpture. In addition, he was a highly prolific filmmaker. Between 1963 and 1968, he made more than 60 films, plus some 500 short black-and-white "screen test" portraits of Factory visitors. One of his most famous films, "Sleep", monitors poet John Giorno sleeping for six hours. The 35-minute film "Blow Job" is one continuous shot of the face of DeVeren Bookwalter supposedly receiving oral sex from filmmaker Willard Maas, although the camera never tilts down to see this. Another, "Empire" (1964), consists of eight hours of footage of the Empire State Building in New York City at dusk. The film "Eat" consists of a man eating a mushroom for 45 minutes. Warhol attended the 1962 premiere of the static composition by LaMonte Young called "Trio for Strings" and subsequently created his famous series of static films including "Kiss", "Eat", and "Sleep" (for which Young initially was commissioned to provide music). Uwe Husslein cites filmmaker Jonas Mekas, who accompanied Warhol to the Trio premiere, and who claims Warhol's static films were directly inspired by the performance.
"Batman Dracula" is a 1964 film that was produced and directed by Warhol, without the permission of DC Comics. It was screened only at his art exhibits. A fan of the Batman series, Warhol's movie was an "homage" to the series, and is considered the first appearance of a blatantly campy Batman. The film was until recently thought to have been lost, until scenes from the picture were shown at some length in the 2006 documentary "Jack Smith and the Destruction of Atlantis".
Warhol's 1965 film "Vinyl" is an adaptation of Anthony Burgess' popular dystopian novel "A Clockwork Orange". Others record improvised encounters between Factory regulars such as Brigid Berlin, Viva, Edie Sedgwick, Candy Darling, Holly Woodlawn, Ondine, Nico, and Jackie Curtis. Legendary underground artist Jack Smith appears in the film "Camp".
His most popular and critically successful film was "Chelsea Girls" (1966). The film was highly innovative in that it consisted of two 16 mm-films being projected simultaneously, with two different stories being shown in tandem. From the projection booth, the sound would be raised for one film to elucidate that "story" while it was lowered for the other. The multiplication of images evoked Warhol's seminal silk-screen works of the early 1960s.
Other important films include "Bike Boy", "My Hustler", "The Nude Restaurant", and "Lonesome Cowboys", a raunchy pseudo-western. These and other titles document gay underground and camp culture, and continue to feature prominently in scholarship about sexuality and art. "Blue Movie"—a film in which Warhol superstar Viva makes love and fools around in bed with a man for 33 minutes of the film's playing-time—was Warhol's last film as director. The film was at the time scandalous for its frank approach to a sexual encounter. For many years Viva refused to allow it to be screened. It was publicly screened in New York in 2005 for the first time in over thirty years.
After his June 3, 1968, shooting, a reclusive Warhol relinquished his personal involvement in filmmaking. His acolyte and assistant director, Paul Morrissey, took over the film-making chores for the Factory collective, steering Warhol-branded cinema towards more mainstream, narrative-based, B-movie exploitation fare with "Flesh", "Trash", and "Heat". All of these films, including the later "Andy Warhol's Dracula" and "Andy Warhol's Frankenstein", were far more mainstream than anything Warhol as a director had attempted. These latter "Warhol" films starred Joe Dallesandro—more of a Morrissey star than a true Warhol superstar.
In the early 1970s, most of the films directed by Warhol were pulled out of circulation by Warhol and the people around him who ran his business. After Warhol's death, the films were slowly restored by the Whitney Museum and are occasionally projected at museums and film festivals. Few of the Warhol-directed films are available on video or DVD.
Music.
In the mid-1960s, Warhol adopted the band the Velvet Underground, making them a crucial element of the Exploding Plastic Inevitable multimedia performance art show. Warhol, with Paul Morrissey, acted as the band's manager, introducing them to Nico (who would perform with the band at Warhol's request). In 1966 he "produced" their first album "The Velvet Underground & Nico", as well as providing its album art. His actual participation in the album's production amounted to simply paying for the studio time. After the band's first album, Warhol and band leader Lou Reed started to disagree more about the direction the band should take, and their artistic friendship ended. In 1989, after Warhol's death, Reed and John Cale re-united for the first time since 1972 to write, perform, record and release the concept album "Songs for Drella", a tribute to Warhol.
Warhol designed many album covers for various artists starting with the photographic cover of John Wallowitch's debut album, "This Is John Wallowitch!!!" (1964). He designed the cover art for the Rolling Stones albums "Sticky Fingers" (1971) and "Love You Live" (1977), and the John Cale albums "The Academy in Peril" (1972) and "Honi Soit" in 1981. One of Warhol's last works was a portrait of Aretha Franklin for the cover of her 1986 gold album "Aretha", which was done in the style of the "Reigning Queens" series he had completed the year before.
Warhol strongly influenced the new wave/punk rock band Devo, as well as David Bowie. Bowie recorded a song called "Andy Warhol" for his 1971 album "Hunky Dory". Lou Reed wrote the song "Andy's Chest", about Valerie Solanas, the woman who shot Warhol, in 1968. He recorded it with the Velvet Underground, and this version was released on the VU album in 1985. Bowie would later play Warhol in the 1996 movie, "Basquiat". Bowie recalled how meeting Warhol in real life helped him in the role, and recounted his early meetings with him:
Books and print.
Beginning in the early 1950s, Warhol produced several unbound portfolios of his work.
The first of several bound self-published books by Warhol was "25 Cats Name Sam and One Blue Pussy", printed in 1954 by Seymour Berlin on Arches brand watermarked paper using his blotted line technique for the lithographs. The original edition was limited to 190 numbered, hand colored copies, using Dr. Martin's ink washes. Most of these were given by Warhol as gifts to clients and friends. Copy No. 4, inscribed "Jerry" on the front cover and given to Geraldine Stutz, was used for a facsimile printing in 1987 and the original was auctioned in May 2006 for US $35,000 by Doyle New York.
Other self-published books by Warhol include:
After gaining fame, Warhol "wrote" several books that were commercially published:
Warhol created the fashion magazine "Interview" that is still published today. The loopy title script on the cover is thought to be either his own handwriting or that of his mother, Julia Warhola, who would often do text work for his early commercial pieces.
Other media.
Although Andy Warhol is most known for his paintings and films, he authored works in many different media.
Producer and product.
Warhol had assistance in producing his paintings. This is also true of his film-making and commercial enterprises.
He founded the gossip magazine "Interview", a stage for celebrities he "endorsed" and a business staffed by his friends. He collaborated with others on all of his books (some of which were written with Pat Hackett.) He adopted the young painter Jean-Michel Basquiat, and the band The Velvet Underground, presenting them to the public as his latest interest, and collaborating with them. One might even say that he produced people (as in the Warholian "Superstar" and the Warholian portrait). He endorsed products, appeared in commercials, and made frequent celebrity guest appearances on television shows and in films (he appeared in everything from "Love Boat" to "Saturday Night Live" and the Richard Pryor movie, "Dynamite Chicken").
In this respect Warhol was a fan of "Art Business" and "Business Art"—he, in fact, wrote about his interest in thinking about art as business in "The Philosophy of Andy Warhol from A to B and Back Again".
Personal life.
Sexuality.
Warhol was gay. When interviewed in 1980, he indicated that he was still a virgin—biographer Bob Colacello who was present at the interview felt it was probably true and that what little sex he had was probably "a mixture of voyeurism and masturbation—to use his [Andy's] word "abstract"". Warhol's assertion of virginity would seem to be contradicted by an incident recounted by one biographer, his hospital treatment in 1960 for condylomata, a sexually transmitted disease. The fact that Warhol's homosexuality influenced his work and shaped his relationship to the art world is a major subject of scholarship on the artist and is an issue that Warhol himself addressed in interviews, in conversation with his contemporaries, and in his publications ("e.g.", "Popism: The Warhol 1960s"). Throughout his career, Warhol produced erotic photography and drawings of male nudes. Many of his most famous works (portraits of Liza Minnelli, Judy Garland, and Elizabeth Taylor, and films like "Blow Job," "My Hustler" and "Lonesome Cowboys") draw from gay underground culture and/or openly explore the complexity of sexuality and desire. As has been addressed by a range of scholars, many of his films premiered in gay porn theaters.
The first works that Warhol submitted to a fine art gallery, homoerotic drawings of male nudes, were rejected for being too openly gay. In "Popism", furthermore, the artist recalls a conversation with the film maker Emile de Antonio about the difficulty Warhol had being accepted socially by the then more famous (but closeted) gay artists Jasper Johns and Robert Rauschenberg. De Antonio explained that Warhol was "too swish and that upsets them." In response to this, Warhol writes, "There was nothing I could say to that. It was all too true. So I decided I just wasn't going to care, because those were all the things that I didn't want to change anyway, that I didn't think I 'should' want to change ... Other people could change their attitudes but not me". In exploring Warhol's biography, many turn to this period—the late 1950s and early 1960s—as a key moment in the development of his persona. Some have suggested that his frequent refusal to comment on his work, to speak about himself (confining himself in interviews to responses like "Um, no" and "Um, yes", and often allowing others to speak for him)—and even the evolution of his pop style—can be traced to the years when Warhol was first dismissed by the inner circles of the New York art world.
Religious beliefs.
Warhol was a practicing Ruthenian Catholic. He regularly volunteered at homeless shelters in New York, particularly during the busier times of the year, and described himself as a religious person. Many of Warhol's later works depicted religious subjects, including two series, "Details of Renaissance Paintings" (1984) and "The Last Supper" (1986). In addition, a body of religious-themed works was found posthumously in his estate.
During his life, Warhol regularly attended Mass, and the priest at Warhol's church, Saint Vincent Ferrer, said that the artist went there almost daily, although he was not observed taking communion or going to confession and sat or knelt in the pews at the back. The priest thought he was afraid of being recognized; Warhol said he was self-conscious about being seen in a Roman Rite church crossing himself "in the Orthodox way" (right to left instead of the reverse).
His art is noticeably influenced by the eastern Christian tradition which was so evident in his places of worship.
Warhol's brother has described the artist as "really religious, but he didn't want people to know about that because [it was] private". Despite the private nature of his faith, in Warhol's eulogy John Richardson depicted it as devout: "To my certain knowledge, he was responsible for at least one conversion. He took considerable pride in financing his nephew's studies for the priesthood".
Collections.
Warhol was an avid collector. His friends referred to his numerous collections, which filled not only his four-story townhouse, but also a nearby storage unit, as "Andy's Stuff." The true extent of his collections was not discovered until after his death, when the Andy Warhol Museum in Pittsburgh took in 641 boxes of his "Stuff."
Warhol's collections included airplane menus, unpaid invoices, pizza dough, pornographic pulp novels, newspapers, stamps, supermarket flyers, and cookie jars, among other eccentricities. One of his main collections was his wigs. Warhol owned over forty and felt very protective of his hairpieces which were sewn by a New York wig-maker from hair imported from Italy. In 1985 a girl snatched Warhol's wig off his head. It was later discovered in Warhol's diary entry for that day that he wrote "I don't know what held me back from pushing her over the balcony."
Another item found in Warhol's boxes at the museum in Pittsburgh was a mummified human foot from Ancient Egypt. The curator of anthropology at Carnegie Museum of Natural History felt that Warhol most likely found it at a flea market.
Movies about Warhol.
Dramatic portrayals.
Warhol appeared as himself in the film "Cocaine Cowboys" (1979).
After his death, Warhol was portrayed by Crispin Glover in Oliver Stone's film "The Doors" (1991), by David Bowie in Julian Schnabel's film "Basquiat" (1996), and by Jared Harris in Mary Harronls film "I Shot Andy Warhol" (1996).
Warhol appears as a character in Michael Daugherty's opera "Jackie O" (1997). Actor makes a brief cameo as Warhol in "" (1997).
Many films by avant-garde cineast Jonas Mekas have caught the moments of Warhol's life. Sean Gregory Sullivan depicted Warhol in the film "54" (1998). Guy Pearce portrayed Warhol in the film, "Factory Girl" (2007), about Edie Sedgwick's life. Actor Greg Travis portrays Warhol in a brief scene from the film "Watchmen" (2009).
In the film "Men in Black III" (2012) Andy Warhol turns out to really be undercover MIB Agent W (played by Bill Hader). Warhol is throwing a party at The Factory in 1969, where he is looked up by MIB Agents K and J (J from the future). Agent W is desperate to end his undercover job ( "I'm so out of ideas I'm painting soup cans and bananas, for Christ sakes!" and "You gotta fake my death, okay? I can't listen to sitar music anymore.")
Gus Van Sant was planning a version of Warhol's life with River Phoenix in the lead role just before Phoenix's death in 1993.
Honors.
In 2002, the U.S. Postal Service issued an 18-cent stamp commemorating Warhol. Designed by Richard Sheaff of Scottsdale, Arizona, the stamp was unveiled at a ceremony at The Andy Warhol Museum and features Warhol's painting "Self-Portrait, 1964".

</doc>
<doc id="868" url="http://en.wikipedia.org/wiki?curid=868" title="Alp Arslan">
Alp Arslan

Alp Arslan (Persian: آلپ ارسلان; full name: "Diya ad-Dunya wa ad-Din Adud ad-Dawlah Abu Shuja Muhammad Alp Arslan ibn Dawud") (20 January 1029 – 15 December 1072) was the second Sultan of the Seljuk Empire and great-grandson of Seljuk, the eponymous founder of the dynasty. His real name was "Muhammad bin Dawud Chaghri", and for his military prowess, personal valour, and fighting skills he obtained the name "Alp Arslan", which means "Heroic Lion" in Turkish.
Career.
Alp Arslan succeeded his father Çağrı Bey as governor of Khorasan in 1059. His uncle Tughril died and was succeeded by Suleiman, Arslan's brother. Arslan and his uncle Kutalmish both contested this succession. Arslan defeated Kutalmish for the throne and succeeded on 27 April 1064 as sultan of Great Seljuq, thus becoming sole monarch of Persia from the river Oxus to the Tigris.
In consolidating his empire and subduing contending factions, Arslan was ably assisted by Nizam al-Mulk, his vizier, and one of the most eminent statesmen in early Muslim history. With peace and security established in his dominions, Arslan convoked an assembly of the states and declared his son Malik Shah I his heir and successor. With the hope of capturing Caesarea Mazaca, the capital of Cappadocia, he placed himself at the head of the Turkish cavalry, crossed the Euphrates, and entered and invaded the city. Along with Nizam al-Mulk, he then marched into Armenia and Georgia, which he conquered in 1064.
Byzantine struggle.
En route to Syria in 1068, Alp Arslan Oush invaded the Byzantine Empire. The Emperor Romanos IV Diogenes, assuming command in person, met the invaders in Cilicia. In three arduous campaigns, the Turks were defeated in detail and driven across the Euphrates in 1070. The first two campaigns were conducted by the emperor himself, while the third was directed by Manuel Comnenos, great-uncle of Emperor Manuel Comnenos.
In 1071 Romanos again took the field and advanced into Armenia with possibly 30,000 men, including a contingent of Cuman Turks as well as contingents of Franks and Normans, under Ursel de Baieul. At Manzikert, on the Murat River, north of Lake Van, Diogenes was met by Alp Arslan. The sultan proposed terms of peace, which were rejected by the emperor, and the two forces waged the Battle of Manzikert. The Cuman mercenaries among the Byzantine forces immediately defected to the Turkish side. Seeing this, "the Western mercenaries rode off and took no part in the battle." To be exact, Romanos was betrayed by general Andronikos Doukas, son of the Caesar (Romanos's stepson), who pronounced him dead and rode off with a large part of the Byzantine forces at a critical moment. The Byzantines were totally routed.
Emperor Romanos IV was himself taken prisoner and conducted into the presence of Alp Arslan. After a ritual humiliation, Arslan treated him with generosity. After peace terms were agreed to, Arslan dismissed the Emperor, loaded with presents and respectfully attended by a military guard. The following conversation is said to have taken place after Romanos was brought as a prisoner before the Sultan:
Alp Arslan's victories changed the balance in near Asia completely in favour of the Seljuq Turks and Sunni Muslims. While the Byzantine Empire was to continue for nearly four more centuries, and the Crusades would contest the issue for some time, the victory at Manzikert signalled the beginning of Turkish ascendancy in Anatolia. Most historians, including Edward Gibbon, date the defeat at Manzikert as the beginning of the end of the Eastern Roman Empire. Certainly the entry of Turkic farmers following their horsemen ended the themes in Anatolia that had furnished the Empire with men and treasure.
State organization.
Alp Arslan's strength lay in the military realm. Domestic affairs were handled by his able vizier, Nizam al-Mulk, the founder of the administrative organization that characterized and strengthened the sultanate during the reigns of Alp Arslan and his son, Malik Shah. Military fiefs, governed by Seljuq princes, were established to provide support for the soldiery and to accommodate the nomadic Turks to the established Anatolian agricultural scene. This type of military fiefdom enabled the nomadic Turks to draw on the resources of the sedentary Persians, Turks, and other established cultures within the Seljuq realm, and allowed Alp Arslan to field a huge standing army without depending on tribute from conquest to pay his soldiers. He not only had enough food from his subjects to maintain his military, but the taxes collected from traders and merchants added to his coffers sufficiently to fund his continuous wars.
According to the poet Saadi Shirazi: 
Arslan possessed a fort, which raised at the height of Alwand, from all were those within its walls, for its roads were a labyrinth, like the curls of a bride. From a learned traveler Arslan once inquired: "Didst thou ever, in thy wanderings, see a fort as strong as this?" "Splendid it is," was the travelers reply, "but methinks not it confers much strength. Before thee, did not other kings possess it for a while, then pass away? After thee, will not other kings assume control, and eat the fruits of the tree of thy hope?"
In the estimation of the wise, the world is a false gem that passes each moment from one hand to another. (the fort was sacked by the Mongols led by Hulagu).
Suleiman ibn Kutalmish was the son of the contender for Arslan's throne; he was appointed governor of the north-western provinces and assigned to completing the invasion of Anatolia. An explanation for this choice can only be conjectured from Ibn al-Athir’s account of the battle between Alp-Arslan and Kutalmish, in which he writes that Alp-Arslan wept for the latter's death and greatly mourned the loss of his kinsman.
Death.
After Manzikert, the dominion of Alp Arslan extended over much of western Asia. He soon prepared to march for the conquest of Turkestan, the original seat of his ancestors. With a powerful army he advanced to the banks of the Oxus. Before he could pass the river with safety, however, it was necessary to subdue certain fortresses, one of which was for several days vigorously defended by the governor, Yussuf el-Harezmi, a Khwarezmian. He was obliged to surrender, however, and was carried as a prisoner before the sultan, who condemned him to death. Yussuf, in desperation, drew his dagger and rushed upon the sultan. Alp Arslan, who took great pride in his reputation as the foremost archer of his time, motioned to his guards not to interfere. He drew his bow, but his foot slipped, the arrow glanced aside, and he received the assassin's dagger in his breast. Alp Arslan died from this wound four days later, on 25 November 1072, in his 42nd year, and he was taken to Merv to be buried next to his father, Chaghri Beg. Upon his tomb lies the following inscription:
As he lay dying, Alp Arslan whispered to his son that his vanity had killed him. "Alas," he is recorded to have said, "surrounded by great warriors devoted to my cause, guarded night and day by them, I should have allowed them to do their job. I had been warned against trying to protect myself, and against letting my courage get in the way of my good sense. I forgot those warnings, and here I lie, dying in agony. Remember well the lessons learned, and do not allow your vanity to overreach your good sense..."
Legacy.
Alp Arslan's conquest of Anatolia from the Byzantines is also seen as one of the pivotal precursors to the launch of the crusades.
From 2002 to July 2008 under Turkmen calendar reform, the month of August was named after Alp Arslan.

</doc>
<doc id="869" url="http://en.wikipedia.org/wiki?curid=869" title="American Film Institute">
American Film Institute

The American Film Institute (AFI) is a film organization that educates filmmakers and honors the heritage of the moving picture arts. AFI is supported by private funding and public membership.
The institute is composed of leaders from the film, entertainment, business and academic communities. A Board of Trustees chaired by Sir Howard Stringer and a Board of Directors chaired by Robert Daly guide the organization, which is led by President and CEO Bob Gazzale. Prior leaders were founding director George Stevens, Jr. (from 1967 to 1980) and Jean Picker Firstenberg (from 1980 to 2007).
AFI educational and cultural programs include:
History.
The American Film Institute was founded by a 1965 presidential mandate announced in the Rose Garden of the White House by Lyndon B. Johnson – to establish a national arts organization to preserve the legacy of American film heritage, educate the next generation of filmmakers and honor the artists and their work. Two years later, in 1967, AFI was established, supported by the National Endowment for the Arts, the Motion Picture Association of America and the Ford Foundation.
The original 22-member Board of Trustees included Chair Gregory Peck and Vice Chair Sidney Poitier as well as Francis Ford Coppola, Arthur Schlesinger, Jr., Jack Valenti and other representatives from the arts and academia.[1]
The institute established a training program for filmmakers known then as the Center for Advanced Film Studies. Also created in the early years were a repertory film exhibition program at the Kennedy Center for the Performing Arts and the AFI Catalog of Feature Films — a scholarly source for American film history.
The institute moved to its current eight-acre Hollywood campus in 1981. The film training program grew into the AFI Conservatory, an accredited graduate school.
AFI moved its presentation of first-run and auteur films from the Kennedy Center to the historic 1938 Art Deco AFI Silver Theatre and Cultural Center, which now hosts two major film festivals – AFI Fest and AFI Docs –making AFI the largest nonprofit film exhibitor in the world.
AFI educates audiences and recognizes artistic excellence through its awards programs and 10 Top 10 Lists.
AFI Conservatory.
In 1969, the institute established the Center for Advanced Film Studies at Greystone, the Doheny Mansion in Beverly Hills, California. The first class included filmmakers Terrence Malick, David Lynch, Caleb Deschanel and Paul Schrader. That program grew into the AFI Conservatory, an accredited graduate film school located in the hills above Hollywood, California, providing training in six filmmaking disciplines: cinematography, directing, editing, producing, production design and screenwriting. Mirroring a professional production environment, Fellows collaborate to make more films than any other graduate level program. Admission to AFI Conservatory is highly selective, with a maximum of 140 graduates per year.
In 2013, Emmy and Oscar-winning director, producer and screenwriter James L. Brooks ("As Good as It Gets", "Broadcast News", "Terms of Endearment") joined AFI as Artistic Director of the AFI Conservatory where he provides leadership for the film program.
Brooks' artistic role at the AFI Conservatory has a rich legacy that includes Daniel Petrie, Jr., Robert Wise and Frank Pierson. Award-winning director Bob Mandel serves as Dean of the AFI Conservatory.
Notable alumni.
AFI Conservatory's alumni have careers in film, television and on the web. They have been recognized with all of the major industry awards – Academy Awards, Emmy Awards, guild awards, and the Tony Award.
Among the alumni of AFI are Darren Aronofsky ("Requiem for a Dream", "Black Swan"), Todd Cherniawsky ("Oz the Great and Powerful", "Avatar"), Keith Cunningham ("Bridesmaids"), Janusz Kamiński ("Lincoln", "Schindler's List", "Saving Private Ryan"), Heidi Levitt ("The Artist"), Matthew Libatique ("Noah", "Black Swan"), David Lynch ("Mulholland Drive", "Blue Velvet"), Terrence Malick ("Days of Heaven", "The Thin Red Line", "The Tree of Life"), Wally Pfister ("Memento", "The Dark Knight", "Inception"), Robert Richardson ("Platoon", "JFK", "Django Unchained") and many others.
AFI programs.
AFI Catalog of Feature Films.
The AFI Catalog, started in 1968, is a web-based filmographic database. A research tool for film historians, the catalog consists of entries on more than 60,000 feature films and 17,000 short films produced from 1893–2011, as well as AFI Awards Outstanding Movies of the Year from 2000 through 2010.
AFI Awards.
Each year the AFI Awards honor the ten outstanding films and the ten outstanding television programs. The Awards are announced in December and a private luncheon for award honorees takes place the following January.
The AFI Awards were first announced in 2000.
AFI 100 Years… series.
The AFI 100 Years... series, which ran from 1998 to 2008 and created jury-selected lists of America's best movies in categories such as Musicals, Laughs and Thrills, prompted new generations to experience classic American films. The juries consisted of over 1,500 artists, scholars, critics and historians, with movies selected based on the film's popularity over time, historical significance and cultural impact. "Citizen Kane" was voted the greatest American film twice.
AFI film festivals.
AFI operates two film festivals: AFI Fest in Los Angeles, and AFI Docs (formally known as Silverdocs) in Silver Spring, Maryland, and Washington, D.C.
AFI Fest.
AFI Fest is the American Film Institute's annual celebration of artistic excellence. The festival is a showcase for the best festival films of the year and an opportunity for master filmmakers and emerging artists to come together with audiences in the movie capital of the world. AFI Fest is the only festival of its stature that is free to the public. The Academy of Motion Picture Arts and Sciences recognizes AFI Fest as a qualifying festival for the Short Films category for the annual Academy Awards.
The festival has paid tribute to numerous influential filmmakers and artists over the years, including Agnès Varda, Pedro Almodóvar and David Lynch as Guest Artistic Directors, and has screened scores of films that have produced Oscar nominations and wins.
The American Film Market (AFM) is the market partner of AFI Fest. Audi is the festival's presenting sponsor. Additional sponsors include American Airlines and Stella Artois.
AFI Docs.
Held annually in June, AFI Docs (formerly Silverdocs) is a documentary festival in Washington, D.C.. The festival attracts over 27,000 documentary enthusiasts.
AFI Silver Theatre and Cultural Center.
The AFI Silver Theatre and Cultural Center is a moving image exhibition, education and cultural center located in Silver Spring, Maryland. Anchored by the restoration of noted architect John Eberson's historic 1938 Silver Theatre, it features 32,000 square feet of new construction housing two stadium theatres, office and meeting space, and reception and exhibit areas.
The AFI Silver Theatre and Cultural Center presents film and video programming, augmented by filmmaker interviews, panels, discussions,and musical performances.
The AFI Directing Workshop for Women.
The Directing Workshop for Women is a training program committed to educating and mentoring participants in an effort to increase the number of women working professionally in screen directing. In this tuition-free program, each participant is required to complete a short film by the end of the year-long program.
Alumnae of the program include Maya Angelou, Anne Bancroft, Dyan Cannon, Ellen Burstyn, Jennifer Getzinger, Lesli Linka Glatter and Nancy Malone.
AFI Directors Series.
AFI released a set of hour-long programs reviewing the career of acclaimed directors. The Directors Series content was copyrighted in 1997 by Media Entertainment Inc and The American Film Institute, and the VHS and DVDs were released between 1999 and 2001 on Winstar TV and Video.
Directors featured included:

</doc>
<doc id="872" url="http://en.wikipedia.org/wiki?curid=872" title="Akira Kurosawa">
Akira Kurosawa

 was a Japanese film director, screenwriter, producer, and editor. Regarded as one of the most important and influential filmmakers in the history of cinema, Kurosawa directed 30 films in a career spanning 57 years.
Kurosawa entered the Japanese film industry in 1936, following a brief stint as a painter. After years of working on numerous films as an assistant director and scriptwriter, he made his debut as a director in 1943, during World War II, with the popular action film "Sanshiro Sugata" (a.k.a. "Judo Saga"). After the war, the critically acclaimed "Drunken Angel" (1948), in which Kurosawa cast then-unknown actor Toshiro Mifune in a starring role, cemented the director's reputation as one of the most important young filmmakers in Japan. The two men would go on to collaborate on another 15 films. His wife Yōko Yaguchi was also an actress in one of his films.
"Rashomon", which premiered in Tokyo in August 1950, and which also starred Mifune, became, on September 10, 1951, the surprise winner of the Golden Lion at the Venice Film Festival and was subsequently released in Europe and North America. The commercial and critical success of this film opened up Western film markets for the first time to the products of the Japanese film industry, which in turn led to international recognition for other Japanese filmmakers. Throughout the 1950s and early 1960s, Kurosawa directed approximately a film a year, including a number of highly regarded films such as "Ikiru" (1952), "Seven Samurai" (1954) and "Yojimbo" (1961). After the mid-1960s, he became much less prolific, but his later work—including his final two epics, "Kagemusha" (1980) and "Ran" (1985)—continued to win awards, including the Palme d'Or for "Kagemusha", though more often abroad than in Japan.
In 1990, he accepted the Academy Award for Lifetime Achievement. Posthumously, he was named "Asian of the Century" in the "Arts, Literature, and Culture" category by "AsianWeek" magazine and CNN, cited as "one of the [five] people who contributed most to the betterment of Asia in the past 100 years".
Life and career.
Childhood and youth (1910–1935).
Kurosawa was born on 23 March 1910 in Ōimachi in the Ōmori district of Tokyo. His father Isamu, a member of a former samurai family from Akita Prefecture, worked as the director of the Army's Physical Education Institute's lower secondary school, while his mother Shima came from a merchant's family living in Osaka. Akira was the eighth and youngest child of the moderately wealthy family, with two of his siblings already grown up at the time of his birth and one deceased, leaving Kurosawa to grow up with three sisters and a brother.
In addition to promoting physical exercise, Isamu Kurosawa was open to western traditions and considered theater and motion pictures to have educational merit. He encouraged his children to watch films; young Akira viewed his first movies at the age of six. An important formative influence was his elementary school teacher Mr Tachikawa, whose progressive educational practices ignited in his young pupil first a love of drawing and then an interest in education in general. During this time, the boy also studied calligraphy and Kendo swordsmanship.
Another major childhood influence was Heigo Kurosawa, Akira's older brother by four years. In the aftermath of the Great Kantō earthquake of 1923, which devastated Tokyo, Heigo took the 13-year-old Akira to view the devastation. When the younger brother wanted to look away from the human corpses and animal carcasses scattered everywhere, Heigo forbade him to do so, instead encouraging Akira to face his fears by confronting them directly. Some commentators have suggested that this incident would influence Kurosawa's later artistic career, as the director was seldom hesitant to confront unpleasant truths in his work.
Heigo was academically gifted, but soon after failing to secure a place in Tokyo's foremost high school, he began to detach himself from the rest of the family, preferring to concentrate on his interest in foreign literature. In the late 1920s, Heigo became a benshi (silent film narrator) for Tokyo theaters showing foreign films, and quickly made a name for himself. Akira, who at this point planned to become a painter, moved in with him, and the two brothers became inseparable. Through Heigo, Akira devoured not only films but also theater and circus performances, while exhibiting his paintings and working for the left-wing Proletarian Artists' League. However, he was never able to make a living with his art, and, as he began to perceive most of the proletarian movement as "putting unfulfilled political ideals directly onto the canvas", he lost his enthusiasm for painting.
With the increasing production of talking pictures in the early 1930s, film narrators like Heigo began to lose work, and Akira moved back in with his parents. In July 1933, Heigo committed suicide. Kurosawa has commented on the lasting sense of loss he felt at his brother's death and the chapter of his autobiography ("Something Like an Autobiography") that describes it—written nearly half a century after the event—is titled, "A Story I Don't Want to Tell." Only four months later, Kurosawa's eldest brother also died, leaving Akira, at age 23, the only one of the Kurosawa brothers still living, together with his three surviving sisters.
Director in training (1935–1941).
In 1935, the new film studio Photo Chemical Laboratories, known as P.C.L. (which later became the major studio, Toho), advertised for assistant directors. Although he had demonstrated no previous interest in film as a profession, Kurosawa submitted the required essay, which asked applicants to discuss the fundamental deficiencies of Japanese films and find ways to overcome them. His half-mocking view was that if the deficiencies were fundamental, there was no way to correct them. Kurosawa's essay earned him a call to take the follow-up exams, and director Kajirō Yamamoto, who was among the examiners, took a liking to Kurosawa and insisted that the studio hire him. The 25-year-old Kurosawa joined P.C.L. in February 1936.
During his five years as an assistant director, Kurosawa worked under numerous directors, but by far the most important figure in his development was Kajiro Yamamoto. Of his 24 films as A.D., he worked on 17 under Yamamoto, many of them comedies featuring the popular actor Kenichi Enomoto, known as "Enoken." Yamamoto nurtured Kurosawa's talent, promoting him directly from third assistant director to chief assistant director after a year. Kurosawa's responsibilities increased, and he worked at tasks ranging from stage construction and film development to location scouting, script polishing, rehearsals, lighting, dubbing, editing and second-unit directing. In the last of Kurosawa's films as an assistant director, "Horse" ("Uma", 1941), Kurosawa took over most of the production, as Yamamoto was occupied with the shooting of another film.
One important piece of advice Yamamoto gave Kurosawa was that a good director needed to master screenwriting. Kurosawa soon realized that the potential earnings from his scripts were much higher than what he was paid as an assistant director. Kurosawa would later write or co-write all of his own films. He also frequently wrote screenplays for other directors such as for Satsuo Yamamoto's film, Tsubasa no gaika. This outside scriptwriting would serve Kurosawa as a lucrative sideline lasting well into the 1960s, long after he became world-famous.
Wartime films and marriage (1942–1945).
In the two years following the release of "Horse" in 1941, Kurosawa searched for a story he could use to launch his directing career. Towards the end of 1942, about a year after the beginning of Japan's war with the United States, novelist Tsuneo Tomita published his Musashi Miyamoto inspired judo novel, "Sanshiro Sugata", the advertisements for which intrigued Kurosawa. He bought the book on its publication day, devoured it in one sitting, and immediately asked Toho to secure the film rights. Kurosawa's initial instinct proved correct as, within a few days, three other major Japanese studios also offered to buy the rights. Toho prevailed, and Kurosawa began pre-production on his debut work as director.
Shooting of "Sanshiro Sugata" began on location in Yokohama in December 1942. Production proceeded smoothly, but getting the completed film past the censors was an entirely different matter. The censorship considered the work too "British-American" (an accusation tantamount, at that time, to a charge of treason), and it was only through the intervention of director Yasujirō Ozu, who championed the film, that "Sanshiro Sugata" was finally accepted for release on March 25, 1943. (Kurosawa had just turned 33.) The movie became both a critical and commercial success. Nevertheless, the censorship office would later decide to cut out some 18 minutes of footage, much of which is now considered lost.
He next turned to the subject of wartime female factory workers in "The Most Beautiful", a propaganda film which he shot in a semi-documentary style in early 1944. In order to coax realistic performances from his actresses, the director had them live in a real factory during the shoot, eat the factory food and call each other by their character names. He would use similar methods with his performers throughout his career.
During production, the actress playing the leader of the factory workers, Yōko Yaguchi, was chosen by her colleagues to present their demands to the director. She and Kurosawa were constantly at loggerheads, and it was through these arguments that the two, paradoxically, became close. They married on May 21, 1945, with Yaguchi two months pregnant (she never resumed her acting career), and the couple would remain together until her death in 1985. They would have two children: a son, Hisao, born December 20, 1945, who would serve as producer on some of his father's last projects, and Kazuko, born April 29, 1954, who would become a costume designer.
Shortly before his marriage, Kurosawa was pressured by the studio against his will to direct a sequel to his debut film. The often blatantly propagandistic "Sanshiro Sugata Part II", which premiered in May 1945, is generally considered one of his very weakest pictures.
Kurosawa decided to write the script for a film that would be both censor-friendly and less expensive to produce. "The Men Who Tread on the Tiger's Tail", based on the Kabuki play "Kanjinchō" and starring the comedian Enoken, with whom Kurosawa had often worked during his assistant director days, was completed in September 1945. By this time, Japan had surrendered and the occupation of Japan had begun. The new American censors interpreted the values allegedly promoted in the picture as overly "feudal" and banned the work. (It would not be released until 1952, the year another Kurosawa film, "Ikiru", was also released.) Ironically, while in production, the film had already been savaged by Japanese wartime censors as too Western and "democratic" (they particularly disliked the comic porter played by Enoken), so the movie most probably would not have seen the light of day even if the war had continued beyond its completion.
First postwar works (1946–1950).
The war now ended, Kurosawa, absorbing the democratic ideals of the Occupation, sought to make films that would establish a new respect towards the individual and the self. The first such film, "No Regrets for Our Youth" (1946), inspired by both the 1933 Takigawa incident and the Hotsumi Ozaki wartime spy case, criticized Japan's prewar regime for its political oppression. Atypically for the director, the heroic central character is a woman, Yukie (Setsuko Hara), born into upper-middle-class privilege, who comes to question her values in a time of political crisis. The original script had to be extensively rewritten and, because of its controversial theme (and because the protagonist was a woman), the completed work divided critics, but it nevertheless managed to win the approval of audiences, who turned variations on the film's title ("No regrets for...") into something of a postwar catchphrase.
His next film, "One Wonderful Sunday" premiered in July 1947 to mixed reviews. It is a relatively uncomplicated and sentimental love story dealing with an impoverished postwar couple trying to enjoy, within the devastation of postwar Tokyo, their one weekly day off. The movie bears the influence of Frank Capra, D. W. Griffith and F. W. Murnau. Another film released in 1947 with Kurosawa's involvement was the action-adventure thriller, "Snow Trail", directed by Senkichi Taniguchi from Kurosawa's screenplay. It marked the debut of the intense young actor Toshiro Mifune. It was Kurosawa who, with his mentor Yamamoto, had intervened to persuade Toho to sign Mifune, during an audition in which the young man greatly impressed Kurosawa, but managed to alienate most of the other judges.
"Drunken Angel" is often considered the director's first major work. Although the script, like all of Kurosawa's occupation-era works, had to go through forced rewrites due to American censorship, Kurosawa felt that this was the first film in which he was able to express himself freely. A grittily realistic story of a doctor who tries to save a gangster (yakuza) with tuberculosis, it was also the director's first film with Toshiro Mifune, who would proceed to play either the main or a major character in all but one ("Ikiru") of the director's next 16 films. While Mifune was not cast as the protagonist in "Drunken Angel", his explosive performance as the gangster so dominates the drama that he shifted the focus from the title character, the alcoholic doctor played by Takashi Shimura, who had already appeared in several Kurosawa movies. However, Kurosawa did not want to smother the young actor's immense vitality, and Mifune's rebellious character electrified audiences in much the way that Marlon Brando's defiant stance would startle American film audiences a few years later. The film premiered in Tokyo in April 1948 to rave reviews and was chosen by the prestigious Kinema Junpo critics poll as the best film of its year, the first of three Kurosawa movies to be so honored.
Kurosawa, with producer Sōjirō Motoki and fellow directors and friends Kajiro Yamamoto, Mikio Naruse and Senkichi Taniguchi, formed a new independent production unit called Film Art Association (Eiga Geijutsu Kyōkai). For this organization's debut work, and first film for Daiei studios, Kurosawa turned to a contemporary play by Kazuo Kikuta and, together with Taniguchi, adapted it for the screen. "The Quiet Duel" starred Toshiro Mifune as an idealistic young doctor struggling with syphilis, a deliberate attempt by Kurosawa to break the actor away from being typecast as gangsters. Released in March 1949, it was a box office success, but is generally considered one of the director's lesser achievements.
His second film of 1949, also produced by Film Art Association and released by Shintoho, was "Stray Dog". The most celebrated of Kurosawa's works from this period, it is a detective movie (perhaps the first important Japanese film in that genre) that explores the mood of Japan during its painful postwar recovery through the story of a young detective, played by Mifune, and his obsession over his handgun, stolen by a penniless young man who proceeds to use it to rob and murder. Adapted from an unpublished novel by Kurosawa in the style of a favorite writer of his, Georges Simenon, it was the director's first collaboration with screenwriter Ryuzo Kikushima, who would later help to script eight other Kurosawa films. A famous, virtually wordless sequence, lasting over eight minutes, shows the detective, disguised as an impoverished veteran, wandering the streets in search of the gun thief; it employed actual documentary footage of war-ravaged Tokyo neighborhoods shot by Kurosawa's friend, Ishirō Honda, the future director of "Gojira" (aka, "Godzilla"). The film is considered a precursor to the contemporary police procedural and buddy cop film genres.
"Scandal", released by Shochiku in April 1950, was inspired by the director's personal experiences with, and anger towards, Japanese yellow journalism. The work is an ambitious mixture of courtroom drama and social problem film about free speech and personal responsibility, but even Kurosawa regarded the finished product as dramatically unfocused and unsatisfactory, and almost all critics agree.
However, it would be Kurosawa's "second" film of 1950, "Rashomon", that would ultimately win him a whole new audience.
International recognition (1950–1958).
After finishing "Scandal", Kurosawa was approached by Daiei studios, which asked the director to make another film for them. Kurosawa picked a script by an aspiring young screenwriter, Shinobu Hashimoto. (They would eventually write nine films together.) It was based on Ryūnosuke Akutagawa's experimental short story "In a Grove", which recounts the murder of a samurai and the rape of his wife from various different and conflicting points-of-view. Kurosawa saw potential in the script, and with Hashimoto's help, polished and expanded it and then pitched it to Daiei, who were happy to accept the project due to its low budget.
Shooting of "Rashomon" began on July 7, 1950 and, after extensive location work in the primeval forest of Nara, wrapped on August 17. Just one week was spent in hurried post-production, hampered by a studio fire, and the finished film premiered at Tokyo's Imperial Theatre on August 25, expanding nationwide the following day. The movie was met by lukewarm reviews, with many critics puzzled by its unique theme and treatment, but it was nevertheless a moderate financial success for Daiei.
Kurosawa's next film, for Shochiku, was "The Idiot", an adaptation of the novel by the director's favorite writer, Fyodor Dostoyevsky. The filmmaker relocated the story from Russia to Hokkaido, but it is otherwise very faithful to the original, a fact seen by many critics as detrimental to the work. A studio-mandated edit shortened it from Kurosawa's original cut of 265 minutes (nearly four-and-a-half hours) to just 166 minutes, making the resulting narrative exceedingly difficult to follow. It is widely considered today to be one of the director's least successful works. Contemporary reviews were very negative, but the film was a moderate success at the box office, largely because of the popularity of one of its stars, Setsuko Hara.
Meanwhile, unbeknownst to Kurosawa, "Rashomon" had been entered in the prestigious Venice Film Festival, due to the efforts of Giuliana Stramigioli, a Japan-based representative of an Italian film company, who had seen and admired the movie and convinced Daiei to submit it. On September 10, 1951, "Rashomon" was awarded the festival's highest prize, the Golden Lion, shocking not only Daiei but the international film world, which at the time was largely unaware of Japan's decades-old cinematic tradition.
After Daiei very briefly exhibited a subtitled print of the film in Los Angeles, RKO purchased distribution rights to "Rashomon" in the United States. The company was taking a considerable gamble. It had put out only one prior subtitled film in the American market, and the only previous Japanese talkie commercially released in New York had been Mikio Naruse's comedy, "Wife! Be Like a Rose", in 1937: a critical and box-office flop. However, "Rashomon"'s commercial run, greatly helped by strong reviews from critics and even the columnist Ed Sullivan, was very successful. (It earned $35,000 in its first three weeks at a single New York theater, an almost unheard-of sum at the time.) This success in turn led to a vogue in America for Japanese movies throughout the 1950s, replacing the enthusiasm for Italian neorealist cinema. (The film was also released, by other distributors, in France, West Germany, Denmark, Sweden and Finland.) Among the Japanese filmmakers whose work, as a result, began to win festival prizes and commercial release in the West were Kenji Mizoguchi ("The Life of Oharu", "Ugetsu", "Sansho the Bailiff") and, somewhat later, Yasujirō Ozu ("Tokyo Story", "An Autumn Afternoon")—artists highly respected in Japan but, prior to this period, almost totally unknown in the West. Later generations of Japanese filmmakers who would find acclaim outside Japan—from Nagisa Oshima and Shohei Imamura to Juzo Itami, Takeshi Kitano and Takashi Miike—were able to pass through the door that Kurosawa was the very first to open.
His career boosted by his sudden international fame, Kurosawa, now reunited with his original film studio, Toho (which would go on to produce his next 11 films), set to work on his next project, "Ikiru". The movie stars Takashi Shimura as a cancer-ridden Tokyo bureaucrat, Watanabe, on a final quest for meaning before his death. For the screenplay, Kurosawa brought in Hashimoto as well as writer Hideo Oguni, who would go on to co-write 12 Kurosawa films. Despite the work's grim subject matter, the screenwriters took a satirical approach, which some have compared to the work of Brecht, to both the bureaucratic world of its hero and the U.S. cultural colonization of Japan. (American pop songs figure prominently in the film.) Because of this strategy, the filmmakers are usually credited with saving the picture from the kind of sentimentality common to dramas about characters with terminal illnesses. "Ikiru" opened in October 1952 to rave reviews—it won Kurosawa his second Kinema Junpo "Best Film" award—and enormous box office success. It remains the most acclaimed of all the artist's films set in the modern era.
In December 1952, Kurosawa took his "Ikiru" screenwriters, Shinobu Hashimoto and Hideo Oguni, for a forty-five day secluded residence at an inn to create the screenplay for his next movie, "Seven Samurai". The ensemble work was Kurosawa's first proper samurai film, the genre for which he would become most famous. The simple story, about a poor farming village in Sengoku period Japan that hires a group of samurai to defend it against an impending attack by bandits, was given a full epic treatment, with a huge cast (largely consisting of veterans of previous Kurosawa productions) and meticulously detailed action, stretching out to almost three-and-a-half hours of screen time.
Three months were spent in pre-production and a month in rehearsals. Shooting took up 148 days spread over almost a year, interrupted by production and financing troubles and Kurosawa's health problems. The film finally opened in April 1954, half a year behind its original release date and about three times over budget, making it at the time the most expensive Japanese film ever made. (However, by Hollywood standards, it was a quite modestly budgeted production, even for that time). The film received positive critical reaction and became a big hit, quickly making back the money invested in it and providing the studio with a product that they could, and did, market internationally—though with extensive edits. Over time—and with the theatrical and home video releases of the uncut version—its reputation has steadily grown. It is now regarded by some commentators as the greatest Japanese film ever made, and in 1979, a poll of Japanese film critics also voted it the best Japanese film ever made.
In 1954, nuclear tests in the Pacific were causing radioactive rainstorms in Japan and one particular incident in March had exposed a Japanese fishing boat to nuclear fallout, with disastrous results. It is in this anxious atmosphere that Kurosawa's next film, "Record of a Living Being", was conceived. The story concerned an elderly factory owner (Toshiro Mifune) so terrified of the prospect of a nuclear attack that he becomes determined to move his entire extended family (both legal and extra-marital) to what he imagines is the safety of a farm in Brazil. Production went much more smoothly than the director's previous film, but a few days before shooting ended, Kurosawa's composer, collaborator and close friend Fumio Hayasaka passed away (of tuberculosis) at the age of only 41. The film's score was finished by Hayasaka's student, Masaru Sato, who would go on to score all of Kurosawa's next eight films. "Record of a Living Being" opened in November 1955 to mixed reviews and muted audience reaction, becoming the first Kurosawa film to lose money during its original theatrical run. Today, it is considered by many to be among the finest films dealing with the psychological effects of the global nuclear stalemate.
Kurosawa's next project, "Throne of Blood", a lavishly produced adaptation of William Shakespeare's "Macbeth"—set, like "Seven Samurai", in the Sengoku Era—represented an ambitious transposition of the English work into an Asian context. Kurosawa instructed his leading actress, Isuzu Yamada, to regard the work as if it were a cinematic version of a "Japanese" rather than a European literary classic. Appropriately, the acting of the players, particularly Yamada, draws heavily on the stylized techniques of the Noh theater. It was filmed in 1956 and released in January 1957 to a slightly less negative domestic response than had been the case with the director's previous film. Abroad, "Throne of Blood", regardless of the liberties it takes with its source material, quickly earned a place among the most celebrated Shakespeare adaptations.
Another adaptation of a classic European theatrical work followed almost immediately, with production of "The Lower Depths", based on a play by Maxim Gorky, taking place in May and June 1957. In contrast to the gigantic scope and sweep of "Throne of Blood", "The Lower Depths" was shot on only two confined sets, the better to emphasize the restricted nature of the characters' lives. Though faithful to the play, this adaptation of Russian material to a completely Japanese setting—in this case, the late Edo period—unlike his earlier "The Idiot", was regarded as artistically successful. The film premiered in September 1957, receiving a mixed response similar to that of "Throne of Blood". However, some critics rank it among the director's most underrated works.
Kurosawa's three consecutive movies after "Seven Samurai" had not managed to capture Japanese audiences in the way that that film had. The mood of the director's work had been growing increasingly pessimistic and dark, with the possibility of redemption through personal responsibility now very much questioned, particularly in "Throne of Blood" and "The Lower Depths". He recognized this, and deliberately aimed for a more light-hearted and entertaining film for his next production, while switching to the new widescreen format that had been gaining popularity in Japan. The resulting film, "The Hidden Fortress", is an action-adventure comedy-drama about a medieval princess, her loyal general and two peasants who all need to travel through enemy lines in order to reach their home region. Released in December 1958, "The Hidden Fortress" became an enormous box office success in Japan and was warmly received by critics. Today, the film is considered one of Kurosawa's most lightweight efforts, though it remains popular, not least because it is one of several major influences (as George Lucas himself has conceded) on Lucas' hugely popular 1977 space opera, "".
Birth of a company and the end of an era (1959–1965).
Starting with "Rashomon", Kurosawa's productions had become increasingly large in scope and so had the director's budgets. Toho, concerned about this development, suggested that he might help finance his own works, therefore making the studio's potential losses smaller, while in turn allowing himself more artistic freedom as co-producer. Kurosawa agreed, and the Kurosawa Production Company was established in April 1959, with Toho as majority shareholder.
Despite now risking his own money, Kurosawa chose a story more directly critical of the Japanese business and political elites than any of his previous works. "The Bad Sleep Well", based on a script by Kurosawa's nephew Mike Inoue, is a revenge drama about a young man who climbs the hierarchy of a corrupt Japanese company with the intention of exposing the men responsible for his father's death. Its theme proved topical: while the film was in production, mass demonstrations were held against the new U.S.-Japan Security treaty, which was seen by many Japanese, particularly the young, as threatening the country's democracy by giving too much power to corporations and politicians. The film opened in September 1960 to positive critical reaction and modest box office success. The 25-minute opening sequence, depicting a corporate wedding reception interrupted by reporters and police (who arrest an executive for corruption), is widely regarded as one of Kurosawa's most skillfully executed set pieces, but the remainder of the film is often perceived as disappointing by comparison. The movie has also been criticized for employing the conventional Kurosawan hero to combat a social evil that cannot be resolved through the actions of individuals, however courageous or cunning.
"Yojimbo" ("The Bodyguard"), Kurosawa Production's second film, centers on a masterless samurai, Sanjuro, who strolls into a 19th-century town ruled by two opposing violent factions and provokes them into destroying each other. The director used this work to play with many genre conventions, particularly the Western, while at the same time offering an unprecedentedly (for the Japanese screen) graphic portrayal of violence. Some commentators have seen the Sanjuro character in this film as a fantasy figure who magically reverses the historical triumph of the corrupt merchant class over the samurai class. Featuring Tatsuya Nakadai in his first major role in a Kurosawa movie, and with innovative photography by Kazuo Miyagawa (who shot "Rashomon") and Takao Saito, the film premiered in April 1961 and was an immense success at the box office, earning more than any previous Kurosawa film. Critical reaction was equally positive, and the film proved a major influence on its genre in Japan, ushering in a new era of violent samurai films, known as "cruel films" ("zankoku eiga"). The movie and its blackly comic tone were also widely imitated abroad. Sergio Leone's "A Fistful of Dollars" was a virtual (unauthorized) scene-by-scene remake.
Following the success of "Yojimbo", Kurosawa found himself under pressure from Toho to create a sequel. Kurosawa turned to a script he had written before "Yojimbo", reworking it to include the hero of his previous film. "Sanjuro" was the first of three Kurosawa films to be adapted from the work of the writer Shūgorō Yamamoto (the others would be "Red Beard" and "Dodeskaden"). It is lighter in tone and closer to a conventional period film than "Yojimbo", though its story of a power struggle within a samurai clan is portrayed with strongly comic undertones. The film opened on January 1, 1962, quickly surpassing "Yojimbo"'s box office success and garnering positive reviews.
Kurosawa had meanwhile instructed Toho to purchase the film rights to "King's Ransom", a novel about a kidnapping written by American author and screenwriter Evan Hunter, under his pseudonym of Ed McBain, as one of his 87th Precinct series of crime books. The director intended to create a work condemning kidnapping, which he considered one of the very worst crimes. The suspense film, titled "High and Low", was shot during the latter half of 1962 and released in March 1963. It broke Kurosawa's box office record (the third film in a row to do so), became the highest grossing Japanese film of the year, and won glowing reviews. However, his triumph was somewhat tarnished when, ironically, the film was blamed for a wave of kidnappings which occurred in Japan about this time (he himself received kidnapping threats directed at his young daughter, Kazuko). "High and Low" is considered by many commentators to be among the director's strongest works.
Kurosawa quickly moved on to his next project, "Red Beard". Based on a short story collection by Shūgorō Yamamoto and incorporating elements from Dostoyevsky's novel "The Insulted and Injured", it is a period film, set in a mid-19th century clinic for the poor, in which Kurosawa's humanist themes receive perhaps their fullest statement. A conceited and materialistic, foreign-trained young doctor, Yasumoto, is forced to become an intern at the clinic under the stern tutelage of Doctor Niide, known as "Akahige" ("Red Beard"), played by Mifune. Although he resists Red Beard initially, Yasumoto comes to admire his wisdom and courage, and to perceive the patients at the clinic, whom he at first despised, as worthy of compassion and dignity.
Yūzō Kayama, who plays Yasumoto, was an extremely popular film and music star at the time, particularly for his "Young Guy" ("Wakadaishō") series of musical comedies, so signing him to appear in the film virtually guaranteed Kurosawa strong box-office. The shoot, the filmmaker's longest ever, lasted well over a year (after five months of pre-production), and wrapped in spring 1965, leaving the director, his crew and his actors exhausted. "Red Beard" premiered in April 1965, becoming the year's highest-grossing Japanese production and the third (and last) Kurosawa film to top the prestigious Kinema Jumpo yearly critics poll. It remains one of Kurosawa's best-known and most-loved works in his native country. Outside Japan, critics have been much more divided. Most commentators concede its technical merits and some praise it as among Kurosawa's best, while others insist that it lacks complexity and genuine narrative power, with still others claiming that it represents a retreat from the artist's previous commitment to social and political change.
The film marked something of an end of an era for its creator. The director himself recognized this at the time of its release, telling critic Donald Richie that a cycle of some kind had just come to an end and that his future films and production methods would be different. His prediction proved quite accurate. Beginning in the late 1950s, television began increasingly to dominate the leisure time of the formerly large and loyal Japanese cinema audience. And as film company revenues dropped, so did their appetite for risk—particularly the risk represented by Kurosawa's costly production methods.
"Red Beard" also marked the midway point, chronologically, in the artist's career. During his previous twenty-nine years in the film industry (which includes his five years as assistant director), he had directed twenty-three films, while during the remaining twenty-eight years, for many and complex reasons, he would complete only seven more. Also, for reasons never adequately explained, "Red Beard" would be his final film starring Toshiro Mifune. Yu Fujiki, an actor who worked on "The Lower Depths", observed, regarding the closeness of the two men on the set, "Mr. Kurosawa's heart was in Mr. Mifune's body." Donald Richie has described the rapport between them as a unique "symbiosis." Virtually all critics agree that the strongest period of Kurosawa's career was the one between 1950 and 1965—bookended by "Rashomon" and "Red Beard"—and that it is not a coincidence that this phase corresponds almost exactly to the time that he and Mifune worked together.
Hollywood detour (1966–1968).
When Kurosawa's exclusive contract with Toho came to an end in 1966, the 56-year-old director was seriously contemplating change. Observing the troubled state of the domestic film industry, and having already received dozens of offers from abroad, the idea of working outside Japan appealed to him as never before.
For his first foreign project, Kurosawa chose a story based on a Life magazine article. The Embassy Pictures action thriller, to be filmed in English and called simply "Runaway Train", would have been his first in color. But the language barrier proved a major problem, and the English version of the screenplay was not even finished by the time filming was to begin in autumn 1966. The shoot, which required snow, was moved to autumn 1967, then canceled in 1968. Almost twenty years later, another foreigner working in Hollywood, Andrei Konchalovsky, would finally make "Runaway Train", though from a script totally different from Kurosawa's.
The director meanwhile had become involved in a much more ambitious Hollywood project. "Tora! Tora! Tora!", produced by 20th Century Fox and Kurosawa Production, would be a portrayal of the Japanese attack on Pearl Harbor from both the American and the Japanese points-of-view, with Kurosawa helming the Japanese half and an English-speaking filmmaker directing the American half. He spent several months working on the script with Ryuzo Kikushima and Hideo Oguni, but very soon the project began to unravel. The director chosen to film the American sequences turned out not to be the prestigious English filmmaker David Lean, as the producers had led Kurosawa to believe, but the much less celebrated special effects expert, Richard Fleischer. The budget was also cut, and the screen time allocated for the Japanese segment would now be no longer than 90 minutes—a major problem, considering that Kurosawa's script ran over four hours. After numerous revisions, a more or less finalized cut screenplay was agreed upon in May 1968. Shooting began in early December, but Kurosawa would last only a little over three weeks as director. He struggled to work with an unfamiliar crew and the requirements of a Hollywood production, while his working methods puzzled his American producers, who ultimately concluded that the director must be mentally ill. On Christmas Eve 1968, the Americans announced that Kurosawa had left the production due to "fatigue", effectively firing him. (He was ultimately replaced, for the film's Japanese sequences, with two directors, Kinji Fukasaku and Toshio Masuda.)
"Tora! Tora! Tora!", finally released to unenthusiastic reviews in September 1970, was, as Donald Richie put it, an "almost unmitigated tragedy" in Kurosawa's career. He had spent years of his life on a logistically nightmarish project to which he ultimately did not contribute a foot of film shot by himself. (He had his name removed from the credits, though the script used for the Japanese half was still his and his co-writers'.) He became estranged from his longtime collaborator, writer Ryuzo Kikushima, and never worked with him again. The project had inadvertently exposed corruption in his own production company (a situation reminiscent of his own movie, "The Bad Sleep Well"). His very sanity had been called into question. Worst of all, the Japanese film industry—and perhaps the man himself—began to suspect that he would never make another film.
A difficult decade (1969–1977).
Knowing that his reputation was at stake following the much publicised "Tora! Tora! Tora!" debacle, Kurosawa moved quickly to a new project to prove he was still viable. To his aid came friends and famed directors Keisuke Kinoshita, Masaki Kobayashi and Kon Ichikawa, who together with Kurosawa established in July 1969 a production company called the Club of the Four Knights (Yonki no kai). Although the plan was for the four directors to create a film each, it has been suggested that the real motivation for the other three directors was to make it easier for Kurosawa to successfully complete a film, and therefore find his way back into the business.
The first project proposed and worked on was a period film to be called "Dora-Heita", but when this was deemed too expensive, attention shifted to "Dodesukaden", an adaptation of yet another Shūgorō Yamamoto work, again about the poor and destitute. The film was shot quickly (by Kurosawa's standards) in about nine weeks, with Kurosawa determined to show he was still capable of working quickly and efficiently within a limited budget. For his first work in color, the dynamic editing and complex compositions of his earlier pictures were set aside, with the artist focusing on the creation of a bold, almost surreal palette of primary colors, in order to reveal the toxic environment in which the characters live. It was released in Japan in October 1970, but though a minor critical success, it was greeted with audience indifference. The picture lost money and caused the Club of the Four Knights to dissolve. Initial reception abroad was somewhat more favorable, but "Dodesukaden" has since been typically considered an interesting experiment not comparable to the director's best work.
Unable to secure funding for further work and allegedly suffering from health problems, Kurosawa apparently reached the breaking point: on December 22, 1971, he slit his wrists and throat multiple times. The suicide attempt proved unsuccessful and the director's health recovered fairly quickly, with Kurosawa now taking refuge in domestic life, uncertain if he would ever direct another film.
In early 1973, the Soviet studio Mosfilm approached the filmmaker to ask if he would be interested in working with them. Kurosawa proposed an adaptation of Russian explorer Vladimir Arsenyev's autobiographical work "Dersu Uzala". The book, about a Goldi hunter who lives in harmony with nature until destroyed by encroaching civilization, was one that he had wanted to make since the 1930s. In December 1973, the 63-year-old Kurosawa set off for the Soviet Union with four of his closest aides, beginning a year-and-a-half stay in the country. Shooting began in May 1974 in Siberia, with filming in exceedingly harsh natural conditions proving very difficult and demanding. The picture wrapped in April 1975, with a thoroughly exhausted and homesick Kurosawa returning to Japan and his family in June. "Dersu Uzala" had its world premiere in Japan on August 2, 1975, and did well at the box office. While critical reception in Japan was muted, the film was better reviewed abroad, winning the Golden Prize at the 9th Moscow International Film Festival, as well as an Academy Award for Best Foreign Language Film. Today, critics remain divided over the film: some see it as an example of Kurosawa's alleged artistic decline, while others count it among his finest works.
Although proposals for television projects were submitted to him, he had no interest in working outside the film world. Nevertheless, the hard-drinking director did agree to appear in a series of television ads for Suntory whiskey, which aired in 1976. While fearing that he might never be able to make another film, the director nevertheless continued working on various projects, writing scripts and creating detailed illustrations, intending to leave behind a visual record of his plans in case he would never be able to film his stories.
Two epics (1978–1986).
In 1977, American director George Lucas had released "", a wildly successful science fiction film influenced by Kurosawa's "The Hidden Fortress", among other works. Lucas, like many other New Hollywood directors, revered Kurosawa and considered him a role model, and was shocked to discover that the Japanese filmmaker was unable to secure financing for any new work. The two met in San Francisco in July 1978 to discuss the project Kurosawa considered most financially viable: "Kagemusha", the epic story of a thief hired as the double of a medieval Japanese lord of a great clan. Lucas, enthralled by the screenplay and Kurosawa's illustrations, leveraged his influence over 20th Century Fox to coerce the studio that had fired Kurosawa just ten years earlier to produce "Kagemusha", then recruited fellow fan Francis Ford Coppola as co-producer.
Production began the following April, with Kurosawa in high spirits. Shooting lasted from June 1979 through March 1980 and was plagued with problems, not the least of which was the firing of the original lead actor, Shintaro Katsu—creator of the very popular Zatoichi character—due to an incident in which the actor insisted, against the director's wishes, on videotaping his own performance. (He was replaced by Tatsuya Nakadai, in his first of two consecutive leading roles in a Kurosawa movie.) The film was completed only a few weeks behind schedule and opened in Tokyo in April 1980. It quickly became a massive hit in Japan. The film was also a critical and box office success abroad, winning the coveted Palme d'Or at the 1980 Cannes Film Festival in May, though some critics, then and now, have faulted the film for its alleged coldness. Kurosawa spent much of the rest of the year in Europe and America promoting "Kagemusha", collecting awards and accolades, and exhibiting as art the drawings he had made to serve as storyboards for the film.
The international success of "Kagemusha" allowed Kurosawa to proceed with his next project, "Ran", another epic in a similar vein. The script, partly based on William Shakespeare's "King Lear", depicted a ruthless, bloodthirsty daimyo (warlord), played by Tatsuya Nakadai, who, after foolishly banishing his one loyal son, surrenders his kingdom to his other two sons, who then betray him, thus plunging the entire kingdom into war. As Japanese studios still felt wary about producing another film that would rank among the most expensive ever made in the country, international help was again needed. This time it came from French producer Serge Silberman, who had produced Luis Buñuel's final movies. Filming did not begin until December 1983 and lasted more than a year.
In January 1985, production of "Ran" was halted as Kurosawa's 64-year-old wife Yōko fell ill. She died on February 1. Kurosawa returned to finish his film and "Ran" premiered at the Tokyo Film Festival on May 31, with a wide release the next day. The film was a moderate financial success in Japan, but a larger one abroad and, as he had done with "Kagemusha", Kurosawa embarked on a trip to Europe and America, where he attended the film's premieres in September and October.
"Ran" won several awards in Japan, but was not quite as honored there as many of the director's best films of the 1950s and 1960s had been. The film world was shocked, however, when Japan passed over the film in favor of another as its official entry to compete for an Oscar nomination in the Best Foreign Film category. Both the producer and Kurosawa himself attributed this to a misunderstanding: because of the Academy's arcane rules, no one was sure whether "Ran" qualified as a "Japanese" film, a "French" film (due to its financing), or both, so it was not submitted at all. In response to what at least appeared to be a blatant snub by his own countrymen, the director Sidney Lumet led a successful campaign to have Kurosawa receive an Oscar nomination for Best Director that year (Sydney Pollack ultimately won the award for directing Out of Africa). "Ran"'s costume designer, Emi Wada, won the movie's only Oscar.
"Kagemusha" and "Ran", particularly the latter, are often considered to be among Kurosawa's finest works. After "Ran"'s release, Kurosawa would point to it as his best film, a major change of attitude for the director who, when asked which of his works was his best, had always previously answered "my next one."
Final works and last years (1987–1998).
For his next movie, Kurosawa chose a subject very different from any that he had ever filmed before. While some of his previous pictures (for example, "Drunken Angel" and "Kagemusha") had included brief dream sequences, "Dreams" was to be entirely based upon the director's own dreams. Significantly, for the first time in over forty years, Kurosawa, for this deeply personal project, wrote the screenplay alone. Although its estimated budget was lower than the films immediately preceding it, Japanese studios were still unwilling to back one of his productions, so Kurosawa turned to another famous American fan, Steven Spielberg, who convinced Warner Bros. to buy the international rights to the completed film. This made it easier for Kurosawa's son, Hisao, as co-producer and soon-to-be head of Kurosawa Production, to negotiate a loan in Japan that would cover the film's production costs. Shooting took more than eight months to complete, and "Dreams" premiered at Cannes in May 1990 to a polite but muted reception, similar to the reaction the picture would generate elsewhere in the world.
Kurosawa now turned to a more conventional story with "Rhapsody in August"—the director's first film fully produced in Japan since "Dodeskaden" over twenty years before—which explored the scars of the nuclear bombing which destroyed Nagasaki at the very end of World War II. It was adapted from a Kiyoko Murata novel, but the film's references to the Nagasaki bombing came from the director rather than from the book. This was his only movie to include a role for an American movie star: Richard Gere, who plays a small role as the nephew of the elderly heroine. Shooting took place in early 1991, with the film opening on May 25 that year to a largely negative critical reaction, especially in the United States, where the director was accused of promulgating naïvely anti-American sentiments.
Kurosawa wasted no time moving onto his next project: "Madadayo", or "Not Yet". Based on autobiographical essays by Hyakken Uchida, the film follows the life of a Japanese professor of German through the Second World War and beyond. The narrative centers on yearly birthday celebrations with his former students, during which the protagonist declares his unwillingness to die just yet—a theme that was becoming increasingly relevant for the film's 81-year-old creator. Filming began in February 1992 and wrapped by the end of September. Its release on April 17, 1993, was greeted by an even more disappointed reaction than had been the case with his two preceding works.
Kurosawa nevertheless continued to work. He wrote the original screenplays "The Sea is Watching" in 1993 and "After the Rain" in 1995. While putting finishing touches on the latter work in 1995, Kurosawa slipped and broke the base of his spine. Following the accident, he would use a wheelchair for the rest of his life, putting an end to any hopes of him directing another film. His longtime wish—to die on the set while shooting a movie—was never to be fulfilled.
After his accident, Kurosawa's health began to deteriorate. While his mind remained sharp and lively, his body was giving up, and for the last half year of his life, the director was largely confined to bed, listening to music and watching television at home. On September 6, 1998, Kurosawa died of a stroke in Setagaya, Tokyo, at the age of 88. Kurosawa was survived by his two children and four grandchildren, three from son Hisao's marriage to Hiroko Hayashi and one grandson, actor Takayuki Kato, from his daughter Kazuko Kurosawa.
Posthumous works.
Following Kurosawa's death, several posthumous works based on his unfilmed screenplays have been produced. "After the Rain", directed by Takashi Koizumi, was released in 1998, and "The Sea is Watching", directed by Kei Kumai, premiered in 2002. A script created by the Yonki no Kai ("Club of the Four Knights") (Kurosawa, Keisuke Kinoshita, Masaki Kobayashi, and Kon Ichikawa), around the time that "Dodeskaden" was made, finally was filmed and released (in 2000) as "Dora-Heita", by the only surviving founding member of the club, Kon Ichikawa.
Working methods, style and themes.
Working methods.
All biographical sources, as well as the filmmaker's own comments, indicate that Kurosawa was a completely "hands-on" director, passionately involved in every aspect of the filmmaking process. As one interviewer summarized, "he (co-)writes his scripts, oversees the design, rehearses the actors, sets up all the shots and then does the editing." His active participation extended from the initial concept to the editing and scoring of the final product.
Script.
Kurosawa emphasized time and again that the screenplay was the absolute foundation of a successful film and that, though a mediocre director can sometimes make a passable film out of a "good" script, even an excellent director can never make a good film out of a "bad" script. During the postwar period, he began the practice of collaborating with a rotating group of five screenwriters: Eijirō Hisaita, Ryuzo Kikushima, Shinobu Hashimoto, Hideo Oguni, and Masato Ide. Whichever members of this group happened to be working on a particular film would gather around a table, often at a hot-springs resort, where they would not be distracted by the outside world. ("Seven Samurai", for example, was written in this fashion.) Often they all (except Oguni, who acted as "referee") would work on exactly the same pages of the script, and Kurosawa would choose the best-written version from the different drafts of each particular scene. This method was adopted "so that each contributor might function as a kind of foil, checking the dominance of any one person's point-of-view."
In addition to the actual script, Kurosawa at this stage often produced extensive, fantastically detailed notes to elaborate his vision. For example, for "Seven Samurai", he created six notebooks in which he created (among many other things) detailed biographies of the samurai, including what they wore and ate, how they walked, talked and behaved when greeted, and even how each tied his shoes. For the 101 peasant characters in the film, he created a registry consisting of 23 families and instructed the performers playing these roles to live and work as these "families" for the duration of shooting.
Shooting.
For his early films, although they were consistently well photographed, Kurosawa generally used standard lenses and deep-focus photography. Beginning with "Seven Samurai" (1954), however, Kurosawa's cinematic technique changed drastically with his extensive use of long lens and multiple cameras. The director claimed that he used these lenses and several cameras rolling at once to help the actors—allowing them to be photographed at some distance from the lens, and without any knowledge of which particular camera's image would be utilized in the final cut—making their performances much more natural. (In fact, Tatsuya Nakadai agreed that the multiple cameras greatly helped his performances with the director.) But these changes had a powerful effect as well on the look of the action scenes in that film, particularly the final battle in the rain. Says Stephen Prince: "He can use the telephoto lenses to get under the horses, in between their hooves, to plunge us into the chaos of that battle in a visual way that is really quite unprecedented, both in Kurosawa's own work and in the samurai genre as a whole."
With "The Hidden Fortress", Kurosawa began to utilize the widescreen (anamorphic) process for the first time in his work. These three techniques—long lenses, multiple cameras and widescreen—were in later works fully exploited, even in sequences with little or no overt action, such as the early scenes of "High and Low" that take place in the central character's home, in which they are employed to dramatize tensions and power relationships between the characters within a highly confined space.
For all his films, but particularly for his "jidaigeki", Kurosawa insisted on absolute authenticity of sets, costumes and props. Numerous instances of his fanatical devotion to detail have been recorded, of which the following are only a few examples.
For "Throne of Blood", in the scene where Washizu (Mifune) is attacked with arrows by his own men, the director had archers shoot real arrows, hollowed out and running along wires, toward Toshiro Mifune from a distance of about ten feet, with the actor carefully following chalk marks on the ground to avoid being hit. (Some of the arrows missed him by an inch; the actor, who admitted that he was not merely "acting" terrified in the film, suffered nightmares afterward).
For "Red Beard", to construct the gate for the clinic set, Kurosawa had his assistants dismantle rotten wood from old sets and then create the prop from scratch with this old wood, so the gate would look properly ravaged by time. For the same film, for teacups that appeared in the movie, he ordered his crew to pour fifty years' worth of tea into the cups so they would appear appropriately stained.
For "Ran", art director Yoshirō Muraki, constructing the "third castle" set under the director's supervision, created the "stones" of that castle by having photographs taken of actual stones from a celebrated castle, then painting Styrofoam blocks to exactly resemble those stones and gluing them to the castle "wall" through a process known as "rough-stone piling", which required months of work. Later, before shooting the famous scene in which the castle is attacked and set on fire, in order to prevent the Styrofoam "stones" from melting in the heat, the art department coated the surface with four layers of cement, then painted the colors of the ancient stones onto the cement.
Editing.
Kurosawa both directed and edited most of his films, which is nearly unique among prominent filmmakers. Kurosawa often remarked that he shot a film simply in order to have material to edit, because the editing of a picture was the most important and creatively interesting part of the process for him. Kurosawa's creative team believed that the director's skill with editing was his greatest talent. Hiroshi Nezu, a longtime production supervisor on his films, said, "Among ourselves, we think that he is Toho's best director, that he is Japan's best scenarist, and that he is the best editor in the world. He is most concerned with the flowing quality which a film must have ... The Kurosawa film flows "over" the cut, as it were."
The director's frequent crew member Teruyo Nogami confirms this view. "Akira Kurosawa's editing was exceptional, the inimitable work of a genius ... No one was a match for him." She claimed that Kurosawa carried in his head all the information about all shots filmed, and if, in the editing room, he asked for a piece of film and she handed him the wrong one, he would immediately recognize the error, though she had taken detailed notes on each shot and he had not. She compared his mind to a computer, which could do with edited segments of film what computers do today.
Kurosawa's habitual method was to edit a film daily, bit by bit, during production. This helped particularly when he started using multiple cameras, which resulted in a large amount of film to assemble. "I always edit in the evening if we have a fair amount of footage in the can. After watching the rushes, I usually go to the editing room and work." Because of this practice of editing as he went along, the post-production period for a Kurosawa film could be startlingly brief: "Yojimbo" had its Japanese premiere on April 20, 1961, four days after shooting concluded on April 16.
"Kurosawa-gumi".
Throughout his career, Kurosawa worked constantly with people drawn from the same pool of creative technicians, crew members and actors, popularly known as the "Kurosawa-gumi" (Kurosawa group). The following is a partial list of this group, divided by profession. This information is derived from the IMDB pages for Kurosawa's films and Stuart Galbraith IV's filmography:
Composers: Fumio Hayasaka ("Drunken Angel", "Stray Dog", "Scandal", "Rashomon", "The Idiot", "Ikiru", "Seven Samurai", "Record of a Living Being"); Masaru Sato ("Throne of Blood", "The Lower Depths", "The Hidden Fortress", "The Bad Sleep Well", "Yojimbo", "Sanjuro", "High and Low", "Red Beard"); Tōru Takemitsu ("Dodeskaden", "Ran"); Shin’ichirō Ikebe ("Kagemusha", "Dreams", "Rhapsody in August", "Madadayo").
Cinematographers: Asakazu Nakai ("No Regrets for Our Youth", "One Wonderful Sunday", "Stray Dog", "Ikiru", "Seven Samurai", "Record of a Living Being", "Throne of Blood", "High and Low", "Red Beard", "Dersu Uzala", "Ran"); Kazuo Miyagawa ("Rashomon", "Yojimbo"); Takao Saitō ("Sanjuro", "High and Low", "Red Beard", "Dodeskaden", "Kagemusha", "Ran", "Dreams", "Rhapsody in August", "Madadayo").
Art Department: Yoshirō Muraki served as either assistant art director, art director or production designer for all Kurosawa's films (except for "Dersu Uzala") from "Drunken Angel" until the end of the director's career.
Production Crew: Teruyo Nogami served as script supervisor, production manager, associate director or assistant to the producer on all Kurosawa's films from "Rashomon" to the end of the director's career. Hiroshi Nezu was production supervisor or unit production manager on all the films from "Seven Samurai" to "Dodeskaden", except "Sanjuro". After retiring as a director, Ishirō Honda returned more than 30 years later to work again for his friend and former mentor as a directorial advisor, production coordinator and creative consultant on Kurosawa's last five films ("Kagemusha", "Ran", "Dreams", "Rhapsody in August" and "Madadayo"). Allegedly one segment of "Dreams" was actually directed by Honda following Kurosawa's detailed storyboards.
Actors: "Leading actors": Takashi Shimura (21 films); Toshiro Mifune (16 films), Susumu Fujita (8 films), Tatsuya Nakadai (6 films) and Masayuki Mori (5 films).
"Supporting performers" (in alphabetical order): Minoru Chiaki, Kamatari Fujiwara, Bokuzen Hidari, Fumiko Homma, Hisashi Igawa, Yunosuke Ito, Kyoko Kagawa, Daisuke Kato, Isao Kimura, Kokuten Kodo, Akitake Kono, Yoshio Kosugi, Koji Mitsui, Seiji Miyaguchi, Eiko Miyoshi, Nobuo Nakamura, Akemi Negishi, Denjiro Okochi, Noriko Sengoku, Gen Shimizu, Ichiro Sugai, Haruo Tanaka, Akira Terao, Eijiro Tono, Yoshio Tsuchiya, Kichijiro Ueda, Atsushi Watanabe, Isuzu Yamada, Tsutomu Yamazaki and Yoshitaka Zushi.
Style.
Virtually all commentators have noted Kurosawa's bold, dynamic style, which many have compared to the traditional Hollywood style of narrative moviemaking, one that emphasizes, in the words of one such scholar, "chronological, causal, linear and historical thinking." But it has also been claimed that, from his very first film, the director displayed a technique quite distinct from the seamless style of classic Hollywood. This technique involved a disruptive depiction of screen space through the use of numerous unrepeated camera setups, a disregard for the traditional 180-degree axis of action around which Hollywood scenes have usually been constructed, and an approach in which "narrative time becomes spatialized", with fluid camera movement often replacing conventional editing. The following are some idiosyncratic aspects of the artist's style.
Axial cut.
In his films of the 1940s and 1950s, Kurosawa frequently employs the "axial cut," in which the camera moves closer to, or further away from, the subject, not through the use of tracking shots or dissolves, but through a series of matched jump cuts. For example, in "Sanshiro Sugata II", the hero takes leave of the woman he loves, but then, after walking away a short distance, turns and bows to her, and then, after walking further, turns and bows once more. This sequence of shots is illustrated on film scholar David Bordwell's blog. The three shots are not connected in the film by camera movements or dissolves, but by a series of two jump cuts. The effect is to stress the duration of Sanshiro's departure.
In the opening sequence of "Seven Samurai" in the peasant village, the axial cut is used twice. When the villagers are outdoors, gathered in a circle, weeping and lamenting the imminent arrival of the bandits, they are glimpsed from above in extreme long shot, then, after the cut, in a much closer shot, then in an even closer shot at ground level as the dialogue begins. A few minutes later, when the villagers go to the mill to ask the village elder's advice, there is a long shot of the mill, with a slowly turning wheel in the river, then a closer shot of this wheel, and then a still closer shot of it. (As the mill is where the elder lives, these shots forge a mental association in the viewer's mind between that character and the mill.)
Cutting on motion.
A number of scholars have pointed out Kurosawa's tendency to "cut on motion": that is, to edit a sequence of a character or characters in motion so that an action is depicted in two or more separate shots, rather than one uninterrupted shot. One scholar, as an example, describes a tense scene in "Seven Samurai" in which the samurai Shichirôji, who is standing, wishes to console the peasant Manzo, who is sitting on the ground, and he gets down on one knee to talk to him. Kurosawa chooses to film this simple action in two shots rather than one (cutting between the two only "after" the action of kneeling has begun) to fully convey Shichirôji's humility. Numerous other instances of this device are evident in the movie. "Kurosawa [frequently] breaks up the action, fragments it, in order to create an emotional effect."
Wipe.
A form of cinematic punctuation very strongly identified with Kurosawa is the wipe. This is an effect created through an optical printer, in which, when a scene ends, a line or bar appears to move across the screen, "wiping" away the image while simultaneously revealing the first image of the subsequent scene. As a transitional device, it is used as a substitute for the straight cut or the dissolve (though Kurosawa, of course, often used both of those devices as well). In his mature work, Kurosawa employed the wipe so frequently that it became a kind of signature. For example, one blogger has counted no fewer than 12 instances of the wipe in "Drunken Angel".
There are a number of theories concerning the purpose of this device, which, as James Goodwin notes, was common in silent cinema but became considerably rarer in the more "realistic" sound cinema. Goodwin claims that the wipes in "Rashomon", for instance, fulfill one of three purposes: emphasizing motion in traveling shots, marking narrative shifts in the courtyard scenes and marking temporal ellipses between actions (e.g., between the end of one character's testimony and the beginning of another's). He also points out that in "The Lower Depths", in which Kurosawa completely avoided the use of wipes, the director cleverly manipulated people and props "in order to slide new visual images in and out of view much as a wipe cut does."
An instance of the wipe used as a satirical device can be seen in "Ikiru". A group of women visit the local government office to petition the bureaucrats to turn a waste area into a children's playground. The viewer is then shown a series of point of view shots of various bureaucrats, connected by wipe transitions, each of whom refers the group to another department. Nora Tennessen comments in her blog (which shows one example) that "the wipe technique makes [the sequence] funnier—images of bureaucrats are stacked like cards, each more punctilious than the last."
Image-sound counterpoint.
Kurosawa by all accounts always gave great attention to the soundtracks of his films (Teruyo Nogami's memoir gives many such examples). In the late 1940s, he began to employ music for what he called "counterpoint" to the emotional content of a scene, rather than merely to reinforce the emotion, as Hollywood traditionally did (and still does). The inspiration for this innovation came from a family tragedy. When news reached Kurosawa of his father's death in 1948, he wandered aimlessly through the streets of Tokyo. His sorrow was magnified rather than diminished when he suddenly heard the cheerful, vapid song "The Cuckoo Waltz", and he hurried to escape from this "awful music." He then told his composer, Fumio Hayasaka, with whom he was working on "Drunken Angel", to use "The Cuckoo Waltz" as ironic accompaniment to the scene in which the dying gangster, Matsunaga, sinks to his lowest point in the narrative.
This ironic approach to music can also be found in "Stray Dog", a film released a year after "Drunken Angel". In the climactic scene, the detective Murakami is fighting furiously with the murderer Yusa in a muddy field. The sound of a Mozart piece is suddenly heard, played on the piano by a woman in a nearby house. As one commentator notes, "In contrast to this scene of primitive violence, the serenity of the Mozart is, literally, other-worldly" and "the power of this elemental encounter is heightened by the music." Nor was Kurosawa's "ironic" use of the soundtrack limited to music. One critic observes that, in "Seven Samurai", "During episodes of murder and mayhem, birds chirp in the background, as they do in the first scene when the farmers lament their seemingly hopeless fate."
Recurring themes.
Master–disciple relationship.
Many commentators have noted the frequent occurrence in Kurosawa's work of the complex relationship between an older and a younger man, who serve each other as master and disciple, respectively. This theme was clearly an expression of the director's life experience. "Kurosawa revered his teachers, in particular Kajiro Yamamoto, his mentor at Toho", according to Joan Mellen. "The salutary image of an older person instructing the young evokes always in Kurosawa's films high moments of pathos." The critic Tadao Sato considers the recurring character of the "master" to be a type of surrogate father, whose role it is to witness the young protagonist's moral growth and approve of it.
In his very first film, "Sanshiro Sugata", after the Judo master Yano becomes the title character's teacher and spiritual guide, "the narrative [is] cast in the form of a chronicle studying the stages of the hero's growing mastery and maturity." The master-pupil relationship in the films of the postwar era—as depicted in such works as "Drunken Angel", "Stray Dog", "Seven Samurai", "Red Beard" and "Dersu Uzala"—involves very little direct instruction, but much learning through experience and example; Stephen Prince relates this tendency to the private and nonverbal nature of the concept of Zen enlightenment.
By the time of "Kagemusha", however, according to Prince, the meaning of this relationship has changed. A thief chosen to act as the double of a great lord continues his impersonation even after his master's death: "the relationship has become spectral and is generated from beyond the grave with the master maintaining a ghostly presence. Its end is death, not the renewal of commitment to the living that typified its outcome in earlier films." However, according to the director's biographer, in his final film, "Madadayo"—which deals with a teacher and his relationship with an entire group of ex-pupils—a sunnier vision of the theme emerges. "The students hold an annual party for their professor, attended by dozens of former students, now adults of varying age ... This extended sequence ... expresses, as only Kurosawa can, the simple joys of student-teacher relationships, of kinship, of being alive."
Heroic champion.
Kurosawa's is a "heroic" cinema, a series of dramas (mostly) concerned with the deeds and fates of larger-than-life heroes. Stephen Prince has identified the emergence of the unique Kurosawa protagonist with the immediate post-World War II period. The goal of the American Occupation to replace Japanese feudalism with individualism coincided with the director's artistic and social agenda: "Kurosawa welcomed the changed political climate and sought to fashion his own mature cinematic voice." The Japanese critic Tadao Sato concurs: "With defeat in World War II, many Japanese ... were dumbfounded to find that the government had lied to them and was neither just nor dependable. During this uncertain time Akira Kurosawa, in a series of first-rate films, sustained the people by his consistent assertion that the meaning of life is not dictated by the nation but something each individual should discover for himself through suffering." The filmmaker himself remarked that, during this period, "I felt that without the establishment of the self as a positive value there could be no freedom and no democracy."
The first such postwar hero was, atypically for the artist, a heroine—Yukie, played by Setsuko Hara, in "No Regrets for Our Youth". According to Prince, her "desertion of family and class background to assist a poor village, her perseverance in the face of enormous obstacles, her assumption of responsibility for her own life and for the well-being of others, and her existential loneliness ... are essential to Kurosawan heroism and make of Yukie the first coherent ... example." This "existential loneliness" is also exemplified by Dr. Sanada (Takashi Shimura) in "Drunken Angel": "Kurosawa insists that his heroes take their stand, alone, against tradition and battle for a better world, even if the path there is not clear. Separation from a corrupt social system in order to alleviate human suffering, as Sanada does, is the only honorable course."
Many commentators regard "Seven Samurai" as the ultimate expression of the artist's heroic ideal. Joan Mellen's comments are typical of this view: "Seven Samurai is above all a homage to the samurai class at its most noble ... Samurai for Kurosawa represent the best of Japanese tradition and integrity." Ironically, it is because of, not in spite of, the chaotic times of civil war depicted in the film that the seven rise to greatness. "Kurosawa locates the unexpected benefits no less than the tragedy of this historical moment. The upheaval forces samurai to channel the selflessness of their credo of loyal service into working for peasants." However, this heroism is futile because "there was already rising ... a merchant class which would supplant the warrior aristocracy." So the courage and supreme skill of the central characters will not prevent the ultimate destruction of themselves or their class.
As Kurosawa's career progressed he seemed to find it increasingly difficult to sustain the heroic ideal. As Prince notes, "Kurosawa's is an essentially tragic vision of life, and this sensibility ... impedes his efforts to realize a socially committed mode of filmmaking." Furthermore, the director's ideal of heroism is subverted by history itself: "When history is articulated as it is in "Throne of Blood", as a blind force ... heroism ceases to be a problem or a reality." According to Prince, the filmmaker's vision eventually became so bleak that he would come to view history merely as eternally recurring patterns of violence, within which the individual is depicted as not only unheroic, but utterly helpless (see "Cycles of violence" below).
Nature and weather.
Nature is a crucial element in Kurosawa's films. According to Stephen Prince, "Kurosawa's sensibility, like that of many Japanese artists, is keenly sensitive to the subtleties and beauties of season and scenery." He has never hesitated to exploit climate and weather as plot elements, to the point where they become "active participants in the drama ... The oppressive heat in "Stray Dog" and "Record of a Living Being" is omnipresent and becomes thematized as a signifier of a world disjointed by economic collapse and the atomic threat." The director himself once said, "I like hot summers, cold winters, heavy rains and snows, and I think most of my pictures show this. I like extremes because I find them most alive."
Wind is also a powerful symbol: "The persistent metaphor of Kurosawa's work is that of wind, the winds of change, of fortune and adversity." "The visually flamboyant [final] battle [of "Yojimbo"] takes place in the main street, as huge clouds of dust swirl around the combatants ... The winds that stir the dust ... have brought firearms to the town along with the culture of the West, which will end the warrior tradition."
It is also difficult not to notice the importance of rain to Kurosawa: "Rain in Kurosawa's films is never treated neutrally. When it occurs ... it is never a drizzle or a light mist but always a frenzied downpour, a driving storm." "The final battle [in "Seven Samurai"] is a supreme spiritual and physical struggle, and it is fought in a blinding rainstorm, which enables Kurosawa to visualize an ultimate fusion of social groups ... but this climactic vision of classlessness, with typical Kurosawan ambivalence, has become a vision of horror. The battle is a vortex of swirling rain and mud ... The ultimate fusion of social identity emerges as an expression of hellish chaos."
Cycles of violence.
Beginning with "Throne of Blood" (1957), an obsession with historical cycles of inexorable savage violence—what Stephen Prince calls "the countertradition to the committed, heroic mode of Kurosawa's cinema"—first appears. According to Donald Richie, within the world of that film, "Cause and effect is the only law. Freedom does not exist." and Prince claims that its events "are inscribed in a cycle of time that infinitely repeats." (He uses as evidence the fact that Washizu's lord, unlike the kindly King Duncan of Shakespeare's play, had murdered his own lord years before to seize power, and is then murdered in turn by Washizu (the Macbeth character) for the same reason.) "The fated quality to the action of Macbeth ... was transposed by Kurosawa with a sharpened emphasis upon predetermined action and the crushing of human freedom beneath the laws of karma."
Prince claims that Kurosawa's last epics, "Kagemusha" and particularly "Ran", mark a major turning point in the director's vision of the world. In "Kagemusha", "where once [in the world of his films] the individual [hero] could grasp events tightly and demand that they conform to his or her impulses, now the self is but the epiphenomenon of a ruthless and bloody temporal process, ground to dust beneath the weight and force of history." The following epic, "Ran", is "a relentless chronicle of base lust for power, betrayal of the father by his sons, and pervasive wars and murders." The historical setting of the film is used as "a commentary on what Kurosawa now perceives as the timelessness of human impulses toward violence and self-destruction." "History has given way to a perception of life as a wheel of endless suffering, ever turning, ever repeating", which is compared in many instances in the screenplay with hell. "Kurosawa has found hell to be both the inevitable outcome of human behavior and the appropriate visualization of his own bitterness and disappointment."
Criticisms.
In general.
In the early to mid-1950s, a number of critics belonging to the French New Wave championed the films of the older Japanese master, Kenji Mizoguchi, at the expense of Kurosawa's work. New Wave critic-filmmaker Jacques Rivette, said: "You can compare only what is comparable and that which aims high enough ... [Mizoguchi] seems to be the only Japanese director who is completely Japanese and yet is also the only one that achieves a true universality, that of an individual." According to such French commentators, Mizoguchi seemed, of the two artists, the more authentically Japanese. But at least one film scholar has questioned the validity of this dichotomy between "Japanese" Mizoguchi and "Western" Kurosawa by pointing out that "Mizo" had been as influenced by Western cinema and Western culture in general as Kurosawa, and that this is reflected in his work.
A criticism frequently directed at Kurosawa's films is that the director's preoccupation with ethical and moral themes led him at times to create what some commentators regard as sentimental or naïve work. Speaking of the postwar "slice of life" drama "One Wonderful Sunday", for example, film scholar (and future politician) Audie Bock claimed that not even Kurosawa's celebrated prowess as an editor could save one particular scene from bathos: "The last sequence ... is an excruciating twelve minutes of the boy conducting an imaginary orchestra in an empty amphitheater while his girlfriend appeals directly to the camera for the viewer to join in. Angles and focal lengths change, details of leaves scattering in the wind are intercut, but nothing makes the scene go any faster."
Some controversy exists about the extent to which Kurosawa's films of the Second World War period could be considered propaganda. The cultural historian Peter B. High sees Kurosawa's wartime cinema as part of the propagandistic trend of Japan at war and as an example of many of these wartime conventions. High refers to his second film, "The Most Beautiful", as a "dark and gloomy rendition of the standard formulas of the [home front] genre." Another controversy centers on his alleged refusal to acknowledge Japan's wartime guilt. In one of Kurosawa's last films, "Rhapsody in August", an elderly survivor of the atomic attack on Nagasaki is visited by her half-Japanese, half-American nephew, Clark (Richard Gere), who appears (at least to some viewers) to apologize, as an American, for the city's wartime destruction. The "New York Times" critic Vincent Canby wrote about this film: "A lot of people at Cannes were outraged that the film makes no mention of Pearl Harbor and Japan's atrocities in China ... If Clark can apologize for bombing Nagasaki, why can't Granny apologize for the raid on Pearl Harbor?"
A number of critics have reacted negatively to the female characters in Kurosawa's movies. Joan Mellen, in her examination of this subject, has maintained that, by the time of "Red Beard" (1965), "women in Kurosawa have become not only unreal and incapable of kindness, but totally bereft of autonomy, whether physical, intellectual, or emotional ... Women at their best may only imitate the truths men discover." Kurosawa scholar Stephen Prince concurs with Mellen's view, though less censoriously: "Unlike a male-oriented director like Sam Peckinpah, Kurosawa is not hostile to women, but his general lack of interest in them should be regarded as a major limitation of his work."
In Japan.
In Japan, both critics and other filmmakers have sometimes accused his work of elitism, because of his focus on exceptional, heroic individuals and groups of men. In her commentary on the deluxe DVD edition of "Seven Samurai", Joan Mellen maintains that certain shots of the samurai characters Kambei and Kyuzo, which to her reveal Kurosawa "privileging" these samurai, "support the argument voiced by several Japanese critics that Kurosawa was an elitist ... Kurosawa was hardly a progressive director, they argued, since his peasants could not discover among their own ranks leaders who might rescue the village. Instead, justifying the inequitable class structure of their society and ours, the peasants must rely on the aristocracy, the upper class, and in particular samurai, to ensure their survival ... Kurosawa defended himself against this charge in his interview with me. 'I wanted to say that after everything the peasants were the stronger, closely clinging to the earth ... It was the samurai who were weak because they were being blown by the winds of time.'"
Because of Kurosawa's popularity with European and American audiences from the early 1950s onward, he has not escaped the charge of deliberately catering to the tastes of Westerners to achieve or maintain that popularity. Joan Mellen, recording the violently negative reaction (in the 1970s) of the left-wing director Nagisa Oshima to Kurosawa and his work, states: "That Kurosawa had brought Japanese film to a Western audience meant [to Oshima] that he must be pandering to Western values and politics." Kurosawa always strongly denied pandering to Western tastes: "He has never catered to a foreign audience" writes Audie Bock, "and has condemned those who do."
Kurosawa was often criticized by his countrymen for perceived "arrogant" behavior. It was in Japan that the (initially) disparaging nickname "Kurosawa Tennō"—"The Emperor Kurosawa"—was coined. "Like tennō", Yoshimoto claimed, "Kurosawa is said to cloister himself in his own small world, which is completely cut off from the everyday reality of the majority of Japanese. The nickname tennō is used in this sense to create an image of Kurosawa as a director who abuses his power solely for the purpose of self-indulgence."
Worldwide impact.
Reputation among filmmakers.
Many celebrated directors have been influenced by Kurosawa and/or have expressed admiration for his work. The filmmakers cited below are grouped according to three categories: a) those who, like Kurosawa himself, established international critical reputations in the 1950s and early 1960s; b) the so-called "New Hollywood" directors, that is, American moviemakers who, for the most part, established their reputations in the early to mid-1970s; and c) other Asian directors.
Ingmar Bergman called his own film "The Virgin Spring" "touristic, a lousy imitation of Kurosawa", and added, "At that time my admiration for the Japanese cinema was at its height. I was almost a samurai myself!" Federico Fellini in an interview declared the director "the greatest living example of all that an author of the cinema should be"—despite admitting to having seen only one of his films, "Seven Samurai". Roman Polanski in 1965 cited Kurosawa as one of his three favorite filmmakers (with Fellini and Orson Welles), singling out "Seven Samurai", "Throne of Blood" and "The Hidden Fortress" for praise. Bernardo Bertolucci considered the Japanese master's influence to be seminal: "Kurosawa's movies and "La Dolce Vita" of Fellini are the things that pushed me, sucked me into being a film director."
Kurosawa's "New Hollywood" admirers have included Robert Altman, Francis Ford Coppola, Steven Spielberg, Martin Scorsese, George Lucas, and John Milius. Robert Altman, when he first saw "Rashomon" (during the period when he worked regularly in television rather than feature films), was so impressed by its cinematographer's achievement of shooting several shots with the camera aimed directly at the sun—allegedly it was the first film in which this was done successfully—that he claims he was inspired the very next day to begin incorporating shots of the sun into his television work. It was Coppola who said of Kurosawa, "One thing that distinguishes [him] is that he didn't make one masterpiece or two masterpieces. He made, you know, "eight" masterpieces." Both Spielberg and Scorsese have praised the older man's role as teacher and role model—as a "sensei", to use the Japanese term. Spielberg has declared, "I have learned more from him than from almost any other filmmaker on the face of the earth", while Scorsese remarked, "Let me say it simply: Akira Kurosawa was my master, and ... the master of so many other filmmakers over the years." As already noted above, several of these moviemakers were also instrumental in helping Kurosawa obtain financing for his late films: Lucas and Coppola served as co-producers on "Kagemusha", while the Spielberg name, lent to the 1990 production, "Dreams", helped bring that picture to fruition.
As the first Asian filmmaker to achieve international prominence, Kurosawa has naturally served as an inspiration for other Asian "auteurs". Of "Rashomon", the most famous director of India, Satyajit Ray, said: "The effect of the film on me [upon first seeing it in Calcutta in 1952] was electric. I saw it three times on consecutive days, and wondered each time if there was another film anywhere which gave such sustained and dazzling proof of a director's command over every aspect of film making." Other Asian admirers include the Japanese actor and director Takeshi Kitano, Hong Kong filmmaker John Woo and mainland Chinese director Zhang Yimou, who called Kurosawa "the quintessential Asian director."
Legacy.
Kurosawa Production Co., established in 1959, continues to oversee much of Kurosawa's legacy. The director's son, Hisao Kurosawa, is the current head of the company. Its American subsidiary, Kurosawa Enterprises, is located in Los Angeles. Rights to Kurosawa's works are held by Kurosawa Production and the film studios under which he worked, most notably Toho. Kurosawa Production works closely with the Akira Kurosawa Foundation, established in December 2003 and also run by Hisao Kurosawa. The foundation organizes an annual short film competition and spearheads Kurosawa-related projects, including a recently shelved one to build a memorial museum for the director.
In 1981, the Kurosawa Film Studio was opened in Yokohama; two additional locations have since been launched in Japan. A large collection of archive material, including scanned screenplays, photos and news articles, has been made available through the Akira Kurosawa Digital Archive, a Japanese website maintained by Ryukoku University Digital Archives Research Center in collaboration with Kurosawa Production. Anaheim University's Akira Kurosawa School of Film was launched in spring 2009 with the backing of Kurosawa Production. It offers online programs in digital film making, with headquarters in Anaheim and a learning center in Tokyo.
Two film awards have also been named in Kurosawa's honor. The Akira Kurosawa Award for Lifetime Achievement in Film Directing is awarded during the San Francisco International Film Festival, while the Akira Kurosawa Award is given during the Tokyo International Film Festival. In commemoration of the 100th anniversary of Kurosawa's birth in 2010, a project called AK100 was launched in 2008. The AK100 Project aims to "expose young people who are the representatives of the next generation, and all people everywhere, to the light and spirit of Akira Kurosawa and the wonderful world he created."
Anaheim University in cooperation with the Kurosawa Family established the Anaheim University Akira Kurosawa School of Film to offer online and blended learning programs on Akira Kurosawa and filmmaking.
Filmography.
On home video.
All thirty films directed by Kurosawa are available on DVD worldwide, most of them from more than one distributor and in more than one region code. His films have begun to be released on Blu-ray.
As Writer.
Work with Akira Kurosawa credited as writer, by year:

</doc>
<doc id="874" url="http://en.wikipedia.org/wiki?curid=874" title="Ancient Egypt">
Ancient Egypt

Ancient Egypt was an ancient civilization of Northeastern Africa, concentrated along the lower reaches of the Nile River in what is now the modern country of Egypt. It is one of six civilizations globally to arise independently. Egyptian civilization coalesced around 3150 BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under the first pharaoh. The history of ancient Egypt occurred in a series of stable Kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.
Egypt reached the pinnacle of its power during the New Kingdom, in the Ramesside period where it rivalled the Hittite Empire, Assyrian Empire and Mitanni Empire, after which it entered a period of slow decline. Egypt was invaded or conquered by a succession of foreign powers (such as the Canaanites/Hyksos, Libyans, Nubians, Assyria, Babylonia, Achaemenids and Macedon/Greece) in the Third Intermediate Period of Egypt and Late Period. In the aftermath of Alexander the Great's death, one of his generals, Ptolemy Soter, established himself as the new ruler of Egypt. This Greek Ptolemaic Dynasty ruled Egypt until 30 BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.
The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to defeat foreign enemies and assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.
The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known ships, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Egypt left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A new-found respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.
History.
The Nile has been the lifeline of its region for much of human history. The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization. Nomadic modern human hunter-gatherers began living in the Nile valley through the end of the Middle Pleistocene some 120 thousand years ago. By the late Paleolithic period, the arid climate of Northern Africa became increasingly hot and dry, forcing the populations of the area to concentrate along the river region.
Predynastic period.
In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.
By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badari, which probably originated in the Western Desert; it was known for its high quality ceramics, stone tools, and its use of copper.
The Badari was followed by the Amratian (Naqada I) and Gerzeh (Naqada II) cultures, which brought a number of technological improvements. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes. In Naqada II times, early evidence exists of contact with the Near East, particularly Canaan and the Byblos coast. Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley. Establishing a power center at Hierakonpolis, and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile. They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean and Near East to the east. Royal Nubian burials at Qustul produced artifacts bearing the oldest-known examples of Egyptian dynastic symbols, such as the white crown of Egypt and falcon.
The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience, which was used well into the Roman Period to decorate cups, amulets, and figurines. During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system of hieroglyphs for writing the ancient Egyptian language.
Early Dynastic Period (c. 3050–2686 BC).
 The Early Dynastic Period was approximately contemporary to the early Sumerian-Akkadian civilisation of Mesopotamia and of ancient Elam. The third-century BC Egyptian priest Manetho grouped the long line of pharaohs from Menes to his own time into 30 dynasties, a system still used today. He chose to begin his official history with the king named "Meni" (or "Menes" in Greek) who was believed to have united the two kingdoms of Upper and Lower Egypt (around 3100 BC).
The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the pharaoh Narmer, who is depicted wearing royal regalia on the ceremonial "Narmer Palette," in a symbolic act of unification. In the Early Dynastic Period about 3150 BC, the first of the Dynastic pharaohs solidified control over lower Egypt by establishing a capital at Memphis, from which he could control the labour force and agriculture of the fertile delta region, as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the pharaohs during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified pharaoh after his death. The strong institution of kingship developed by the pharaohs served to legitimize state control over the land, labour, and resources that were essential to the survival and growth of ancient Egyptian civilization.
Old Kingdom (2686–2181 BC).
Major advances in architecture, art, and technology were made during the Old Kingdom, fueled by the increased agricultural productivity and resulting population, made possible by a well-developed central administration. Some of ancient Egypt's crowning achievements, the Giza pyramids and Great Sphinx, were constructed during the Old Kingdom. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order. 
Along with the rising importance of a central administration arose a new class of educated scribes and officials who were granted estates by the pharaoh in payment for their services. Pharaohs also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the pharaoh after his death. Scholars believe that five centuries of these practices slowly eroded the economic power of the pharaoh, and that the economy could no longer afford to support a large centralized administration. As the power of the pharaoh diminished, regional governors called nomarchs began to challenge the supremacy of the pharaoh. This, coupled with severe droughts between 2200 and 2150 BC, is assumed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.
First Intermediate Period (2181–1991 BC).
After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the pharaoh, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes. In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period.
Free from their loyalties to the pharaoh, local rulers began competing with each other for territorial control and political power. By 2160 BC, rulers in Herakleopolis controlled Lower Egypt in the north, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055 BC the northern Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as the Middle Kingdom.
Middle Kingdom (2134–1690 BC).
The pharaohs of the Middle Kingdom restored the country's prosperity and stability, thereby stimulating a resurgence of art, literature, and monumental building projects. Mentuhotep II and his Eleventh Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming kingship at the beginning of the Twelfth Dynasty around 1985 BC, shifted the nation's capital to the city of Itjtawy, located in Faiyum. From Itjtawy, the pharaohs of the Twelfth Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia that was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls-of-the-Ruler", to defend against foreign attack.
With the pharaohs' having secured military and political security and vast agricultural and mineral wealth, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom experienced an increase in expressions of personal piety and what could be called a democratization of the afterlife, in which all people possessed a soul and could be welcomed into the company of the gods after death. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style. The relief and portrait sculpture of the period captured
subtle, individual details that reached new heights of technical perfection.
The last great ruler of the Middle Kingdom, Amenemhat III, allowed Semitic-speaking Canaanite settlers from the Near East into the delta region to provide a sufficient labour force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with severe Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later Thirteenth and Fourteenth dynasties. During this decline, the Canaanite settlers began to seize control of the delta region, eventually coming to power in Egypt as the Hyksos.
Second Intermediate Period (1674–1549 BC) and the Hyksos.
Around 1785 BC, as the power of the Middle Kingdom pharaohs weakened, a Semitic Canaanite people called the Hyksos had already settled in the Eastern Delta town of Avaris, seized control of Egypt, and forced the central government to retreat to Thebes. The pharaoh was treated as a vassal and expected to pay tribute. The Hyksos ("foreign rulers") retained Egyptian models of government and identified as pharaohs, thus integrating Egyptian elements into their culture. They and other Semitic invaders introduced new tools of warfare into Egypt, most notably the composite bow and the horse-drawn chariot.
After their retreat, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos' Nubian allies, the Kushites, to the south of Egypt. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555 BC The pharaohs Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians to the south of Egypt, but failed to defeat the Hyksos. That task fell to Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He established a new dynasty. In the New Kingdom that followed, the military became a central priority for the pharaohs seeking to expand Egypt's borders and attempting to gain mastery of the Near East.
New Kingdom (1549–1069 BC).
The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including the Mitanni Empire, Assyria, and Canaan. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs to the largest empire Egypt had ever seen. Between their reigns, Hatshepsut generally promoted peace and restored trade routes lost during the Hyksos occupation, as well as expanding to new regions. When Tuthmosis III died in 1425 BC, Egypt had an empire extending from Niya in north west Syria to the fourth waterfall of the Nile in Nubia, cementing loyalties and opening access to critical imports such as bronze and wood.
The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The pharaoh Hatshepsut used such hyperbole and grandeur during her reign of almost twenty-two years. Her reign was very successful, marked by an extended period of peace and wealth-building, trading expeditions to Punt, restoration of foreign trade networks, and great building projects, including an elegant mortuary temple that rivaled the Greek architecture of a thousand years later, a colossal pair of obelisks, and a chapel at Karnak. Despite her achievements, Amenhotep II, the heir to Hatshepsut's nephew-stepson Tuthmosis III, sought to erase her legacy near the end of his father's reign and throughout his, touting many of her accomplishments as his. He also tried to change many established traditions that had developed over the centuries, which some suggest was a futile attempt to prevent other women from becoming pharaoh and to curb their influence in the kingdom.
Around 1350 BC, the stability of the New Kingdom seemed threatened further when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun deity Aten as the supreme deity, suppressed the worship of most other deities, and attacked the power of the temple that had become dominated by the priests of Amun in Thebes, whom he saw as corrupt. Moving the capital to the new city of Akhetaten (modern-day Amarna), Akhenaten turned a deaf ear to events in the Near East (where the Hittites, Mitanni, and Assyrians were vying for control). He was devoted to his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned, the priests of Amun soon regained power and returned the capital to Thebes. Under their influence the subsequent pharaohs Tutankhamun, Ay, and Horemheb worked to erase all mention of Akhenaten's heresy, now known as the Amarna Period.
Around 1279 BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history. A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh (in modern Syria) and, after fighting to a stalemate, finally agreed to the first recorded peace treaty, around 1258 BC. With both the Egyptians and Hittite Empire proving unable to gain the upper hand over one another, and both powers also fearful of the expanding Middle Assyrian Empire, Egypt withdrew from much of the Near East. The Hittites were thus left to compete unsuccessfully with the powerful Assyrians and the newly arrived Phrygians.
Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyan Berbers to the west, and the Sea Peoples, a powerful confederation of largely Greek, Luwian and Phoenician/Caananite pirates from the Aegean. Initially, the military was able to repel these invasions, but Egypt eventually lost control of its remaining territories in southern Caanan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, and civil unrest. After regaining their power, the high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.
Third Intermediate Period (1069–653 BC).
Following the death of Ramesses XI in 1078 BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only. During this time, Berber tribes from what was later to be called Libya had been settling in the western delta, and the chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945 BC, founding the Libyan Berber, or Bubastite, dynasty that ruled for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions.
In the mid-ninth century BC, Egypt made a failed attempt to once more gain a foothold in Western Asia. Osorkon II of Egypt, along with a large alliance of nations and peoples, including; Israel, Hamath, Phoenicia/Caanan, the Arabs, Arameans, and neo Hittites among others, engaged in the Battle of Karkar against the powerful Assyrian king Shalmaneser III in 853 BC. However, this coalition of powers failed and the Assyrian Empire continued to dominate Western Asia.
Libyan Berber control began to erode as a rival native dynasty in the delta arose under Leontopolis. Also, the Nubians of the Kushites threatened Egypt from the lands to the south.
Drawing on millennia of interaction (trade, acculturation, occupation, assimilation, and war) with Egypt, the Kushite king Piye left his Nubian capital of Napata and invaded Egypt around 727 BC. Piye easily seized control of Thebes and eventually the Nile Delta. He recorded the episode on his stela of victory. Piye set the stage for subsequent Twenty-fifth dynasty pharaohs, such as Taharqa, to reunite the "Two lands" of Northern and Southern Egypt. The Nile valley empire was as large as it had been since the New Kingdom.
The Twenty-fifth dynasty ushered in a renaissance period for ancient Egypt. Religion, the arts, and architecture were restored to their glorious Old, Middle, and New Kingdom forms. Pharaohs, such as Taharqa, built or restored temples and monuments throughout the Nile valley, including at Memphis, Karnak, Kawa, Jebel Barkal, etc. It was during the Twenty-fifth dynasty that there was the first widespread construction of pyramids (many in modern Sudan) in the Nile Valley since the Middle Kingdom.
Piye made various unsuccessful attempts to extend Egyptian influence in the Near East, then controlled by Assyria. In 720 BC, he sent an army in support a rebellion against Assyria, which was taking place in Philistia and Gaza. However, Piye was defeated by Sargon II and the rebellion failed. In 711 BC, Piye again supported a revolt against the Assyrians by the Israelites of Ashdod and was once again defeated by the Assyrian king Sargon II. Subsequently, Piye was forced from the Near East.
From the 10th century BC onwards, Assyria fought for control of the southern Levant. Frequently, cities and kingdoms of the southern Levant appealed to Egypt for aide in their struggles against the powerful Assyrian army. Taharqa enjoyed some initial success in his attempts to regain a foothold in the Near East. Taharqa aided the Judean King Hezekiah when Hezekiah and Jerusalem was besieged by the Assyrian king, Sennacherib. Scholars disagree on the primary reason for Assyria's abandonment of their siege on Jerusalem. Reasons for the Assyrian withdrawal range from conflict with the Egyptian/Kushite army to divine intervention to surrender to disease. Henry Aubin argues that the Kushite/Egyptian army saved Jerusalem from the Assyrians and prevented the Assyrians from returning to capture Jerusalem for the remainder of Sennacherib's life (20 years). Some argue that disease was the primary reason for failing to actually take the city and Senacherib's annals claim Judah was forced into tribute regardless.
The Assyrians began their invasion of Egypt under king Esarhaddon, successor of Sennacherib. Sennacherib had been murdered by his own sons for destroying the rebellious city of Babylon. In 674 BC, Taharqa defeated Esarhaddon and the Assyrian army outright on Egyptian soil. In 671 BC, Esarhaddon drove the Kushites from Northern Egypt and back to their Nubian homeland. However, the native Egyptian rulers installed by Esarhaddon were unable to retain full control of the whole country for long. Two years later, Taharqa returned from Nubia and seized control of a section of southern Egypt as far north as Memphis. Esarhaddon prepared to return to Egypt and once more eject Taharqa, however he fell ill and died in his capital, Nineveh, before he left Assyria. His successor, Ashurbanipal, sent a general with a small, but well trained army, which defeated Taharqa at Memphis and once more drove him from Egypt. Taharqa died in Nubia two years later.
His successor, Tanutamun, also made a failed attempt to regain Egypt for Nubia. He successfully defeated Necho, the puppet ruler installed by Ashurbanipal, taking Thebes in the process. The Assyrians then sent a large army southwards. Tantamani (Tanutamun) was heavily routed and fled back to Nubia. The Assyrian army sacked Thebes to such an extent it never truly recovered. A native ruler, Psammetichus I was placed on the throne, as a vassal of Ashurbanipal, and the Nubians were never again to pose a threat.
Late Period (672–332 BC).
With no permanent plans for conquest, the Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-sixth Dynasty. By 653 BC, the Saite king Psamtik I (taking advantage of the fact that Assyria was involved in a fierce war conquering Elam and that few Assyrian troops were stationed in Egypt) was able to free Egypt relatively peacefully from Assyrian vassalage with the help of Lydian and Greek mercenaries, the latter of whom were recruited to form Egypt's first navy. Psamtik and his successors however were careful to maintain peaceful relations with Assyria. Greek influence expanded greatly as the city of Naukratis became the home of Greeks in the delta.
In 609 BC Necho II went to war with Babylonia, the Chaldeans, the Medians and the Scythians in an attempt to save Assyria, which after a brutal civil war was being ovverrun by this coalition of powers. However, the attempt to save Egypts former masters failed. The Egyptians delayed intervening too long, and Nineveh had already fallen and King Sin-shar-ishkun was dead by the time Necho II sent his armies northwards. However, Necho easily brushed aside the Israelite army under King Josiah but he and the Assyrians then lost a battle at Harran to the Babylonians, Medes and Scythians. Necho II and Ashur-uballit II of Assyria were finally defeated at Carchemish in Aramea (modern Syria) in 605 BC. The Egyptians remained in the area for some decades, struggling with the Babylonian kings Nabopolassar and Nebuchadnezzar II for control of portions of the former Assyrian Empire in The Levant. However, they were eventually driven back into Egypt, and Nebuchadnezzar II even briefly invaded Egypt itself in 567 BC. The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525 BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians.
Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia (modern Lebanon) in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-seventh dynasty, ended in 402 BC, and from 380–343 BC the Thirtieth Dynasty ruled as the last native royal house of dynastic Egypt, which ended with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-first Dynasty, began in 343 BC, but shortly after, in 332 BC, the Persian ruler Mazaces handed Egypt over to the Macedonian ruler Alexander the Great without a fight.
Ptolemaic dynasty.
In 332 BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Macedonian Ptolemaic dynasty, was based on an Egyptian model and based in the new capital city of Alexandria. The city showcased the power and prestige of Hellenistic rule, and became a seat of learning and culture, centered at the famous Library of Alexandria. The Lighthouse of Alexandria lit the way for the many ships that kept trade flowing through the city—as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.
Hellenistic culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria that formed after the death of Ptolemy IV. In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful Syriac opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.
Roman Period.
Egypt became a province of the Roman Empire in 30 BC, following the defeat of Marc Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the Emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period. Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.
Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued. The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.
From the mid-first century AD, Christianity took root in Egypt as it was seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from Egyptian Religion and Greco-Roman religion and threatened the popular religious traditions. This led to persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303, but eventually Christianity won out. In 391 the Christian Emperor Theodosius introduced legislation that banned pagan rites and closed temples. Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed. As a consequence, Egypt's native religious culture was continually in decline. While the native population certainly continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.
In the fourth century AD, the Roman Empire split into two, and Egypt became part of the Eastern Empire, known as the Byzantine Empire. The Eastern Empire became increasingly "oriental" and "Eastern" in style, as its links with the old Greco-Roman world faded. The Greek system of local government by citizens had now entirely disappeared.
The Sassanid Persians who were involved in a long running and draining war with Byzantium for control of the Near East, Asia Minor, North Africa and the east Mediterranean, briefly recaptured Egypt under King Khosrow II in 618 AD, but were ejected by the Byzantine Emperor Heraclius in 628 AD.
Arab Muslim Period.
An army of 4,000 Arabs led by Amr Ibn Al-Aas was sent by the Caliph Umar, successor to Muhammad, to spread Islamic rule to the west. The Arabs crossed into Egypt from Palestine in December 639 AD, and advanced rapidly into the Nile Delta. The Imperial garrisons, exhausted by constant war with the Persians, retreated into the walled towns, where they successfully held out for a year or more. But the Arabs sent for reinforcements, and in April 641 they captured Alexandria. The Byzantines did assemble a fleet with the aim of recapturing Egypt, and won back Alexandria in 645, but the Muslims retook the city in 646, completing the Arab Muslim conquest of Egypt. Thus ended 975 years of Græco-Roman rule over Egypt.
Local resistance by the native Egyptian Copts however, began to materialize shortly thereafter and would last until at least the ninth century. The Arabs imposed a special tax, known as Jizya, on the Egyptians, who were by this time Coptic Christians. They acquired the status of dhimmis, and all native Egyptians were prohibited from joining the army. The Arabs in the seventh century used the term "quft" to describe the indigenous people of Egypt. Thus, Egyptians became known as Copts, and the non-Chalcedonian Egyptian Church became known as the Coptic Church. The indigenous population of Egypt was gradually and largely "Arabized" and "Islamicized" over the following centuries, However, native Egyptian identity and language survived among the Copts, who spoke the Coptic language, a direct descendant of the Demotic Egyptian (which itself was an evolution of Ancient Egyptian) spoken in the Roman era. Since the eighteenth century, Coptic has mostly been limited to liturgical use and today Coptic is extinct as a primary language. Copts still to this day espouse an Egyptian rather than Arab ethnic identity.
Government and economy.
Administration and commerce.
The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives. At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they houses of worship, but were also responsible for collecting and storing the nation's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods.
Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period, they did use a type of money-barter system, with standard sacks of grain and the "deben", a weight of roughly of copper or silver, forming a common denominator. Workers were paid in grain; a simple laborer might earn 5½ sacks (200 kg or 400 lb) of grain per month, while a foreman might earn 7½ sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140 deben. Grain could be traded for other goods, according to the fixed price list. During the fifth century BC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.
Social status.
Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land. Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system. Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank. The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. Slavery was known in ancient Egypt, but the extent and prevalence of its practice are unclear.
The ancient Egyptians viewed men and women, including people from all social classes except slaves, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress. Although, slaves were mostly used as indentured servants. They were able to buy and sell, or work their way to freedom or nobility, and usually were treated by doctors in the workplace. Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices and opportunities for achievement. Women such as Hatshepsut and Cleopatra VI even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not often take part in official roles in the administration, served only secondary roles in the temples, and were not as likely to be as educated as men. 
Legal system.
The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at. Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes. Local councils of elders, known as "Kenbet" in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes. More serious cases involving murder, major land transactions, and tomb robbery were referred to the "Great Kenbet", over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.
Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family. Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgment by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.
Agriculture.
A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.
Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: "Akhet" (flooding), "Peret" (planting), and "Shemu" (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops. From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.
The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer. Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.
Animals.
The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole. Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry such as ducks, geese, and pigeons were captured in nets and bred on farms, where they were force-fed with dough to fatten them. The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and they provided both honey and wax.
The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by the Hyksos in the Second Intermediate Period, and the camel, although known from the New Kingdom, was not used as a beast of burden until the Late Period. There is also evidence to suggest that elephants were briefly utilized in the Late Period, but largely abandoned due to lack of grazing land. Dogs, cats and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as lions, were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses. During the Predynastic and Late periods, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were bred in large numbers on farms for the purpose of ritual sacrifice.
Natural resources.
Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry. Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster. Ore-bearing rock formations were found in distant, inhospitable wadis in the eastern desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose. Ancient Egyptians were among the first to use minerals such as sulfur as cosmetic substances.
The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai. Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were utilized in the Late Period. High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the eastern desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the eastern desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.
Trade.
The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs. An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty. Narmer had Egyptian pottery produced in Canaan and exported back to Egypt.
By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons. Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil. In exchange for its luxury imports and raw materials, Egypt mainly exported grain, gold, linen, and papyrus, in addition to other finished goods including glass and stone objects.
Language.
Historical development.
The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages. It has the second longest history of any language (after Sumerian), having been written from c. 3200 BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic. Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.
Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian develops prefixal definite and indefinite articles, which replace the older inflectional suffixes. There is a change from the older verb–subject–object word order to subject–verb–object. The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.
Sounds and grammar.
Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Later Egyptian to about nine. The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton is the semantic core of the word 'hear'; its basic conjugation is ', 'he hears'. If the subject is a noun, suffixes are not added to the verb: ', 'the woman hears'.
Adjectives are derived from nouns through a process that Egyptologists call "nisbation" because of its similarity with Arabic. The word order is in verbal and adjectival sentences, and in nominal and adverbial sentences. The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun. Verbs and nouns are negated by the particle "n", but "nn" is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).
Writing.
Hieroglyphic writing dates from c. 3000 BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.
Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs. Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine and Islamic periods in Egypt, but only in 1822, after the discovery of the Rosetta stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs almost fully deciphered.
Literature.
Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the "Per Ankh" institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories. Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300 BC. Later Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as "Sebayt" ("instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example.
The Story of Sinuhe, written in Middle Egyptian, might be the classic of Egyptian literature. Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests. The Instruction of Amenemope is considered a masterpiece of near-eastern literature. Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700 BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Graeco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.
Culture.
Daily life.
Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mud-brick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.
The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin. Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income.
Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and imported lutes and lyres from Asia. The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies.
The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan. The wealthy members of ancient Egyptian society enjoyed hunting and boating as well.
The excavation of the workers' village of Deir el-Madinah has resulted in one of the most thoroughly documented accounts of community life in the ancient world that spans almost four hundred years. There is no comparable site in which the organisation, social interactions, working and living conditions of a community were studied in such detail.
Cuisine.
Egyptian cuisine remained remarkably stable over time; indeed, the cuisine of modern Egypt retains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.
Architecture.
The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the power of the pharaoh. The ancient Egyptians were skilled builders; using simple but effective tools and sighting instruments, architects could build large stone structures with accuracy and precision.
The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mud bricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs. Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of bricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif.
The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Graeco-Roman period. The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs. The Twenty-fifth dynasty was a notable exception, as all Twenty-fifth dynasty pharaohs constructed pyramids.
Art.
The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change. These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures that can also be read as hieroglyphs. Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.
Ancient Egyptian artisans used stone to carve statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed.
Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife. During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.
Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris. The most striking example of a politically driven change in artistic forms comes from the Amarna period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas. This style, known as Amarna art, was quickly and thoroughly erased after Akhenaten's death and replaced by the traditional forms.
Religious beliefs.
Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting myths and stories into a coherent system. These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.
Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos. After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people.
The Egyptians believed that every human being was composed of physical and spiritual parts or "aspects". In addition to the body, each person had a "šwt" (shadow), a "ba" (personality or soul), a "ka" (life-force), and a "name". The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his "ka" and "ba" and become one of the "blessed dead", living on as an "akh", or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth". If deemed worthy, the deceased could continue their existence on earth in spiritual form.
Burial customs.
The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife. Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars.
By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.
Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Beginning in the New Kingdom, books of the dead were included in the grave, along with shabti statues that were believed to perform manual labor for them in the afterlife. Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.
Military.
The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant.
Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the Khopesh was adopted from Asiatic soldiers. The pharaoh was usually depicted in art and literature riding at the head of the army, it has been suggested that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so. although it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops." Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.
Technology, medicine, and mathematics.
Technology.
In technology, medicine and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditional empiricism, as evidenced by the Edwin Smith and Ebers papyri (c. 1600 BC), is first credited to Egypt. The Egyptians created their own alphabet and decimal system.
Faience and glass.
Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known as faience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made of silica, small amounts of lime and soda, and a colorant, typically copper. The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known as Egyptian Blue, also called blue frit, which is produced by fusing (or sintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment.
The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently. It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.
Medicine.
The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare).
The diets of the wealthy were rich in sugars, which promoted periodontal disease. Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence. Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.
Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such as Imhotep, remained famous long after their deaths. Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists. Training of physicians took place at the "Per Ankh" or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments.
Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection, while opium thyme and belladona were used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddess Isis. Moldy bread, honey and copper salts were also used to prevent infection from dirt in burns. Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.
Shipbuilding.
Early Egyptians knew how to assemble planks of wood into a ship hull and had mastered advanced forms of shipbuilding as early as 3000 BC. The Archaeological Institute of America reports that some of the oldest ships yet unearthed are known as the Abydos boats. These are a group of 14 discovered ships in Abydos that were constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor of New York University, woven straps were found to have been used to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000 BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000 BC was long and is now thought to perhaps have belonged to an earlier pharaoh. According to professor O'Connor, the 5,000-year-old ship may have even belonged to Pharaoh Aha.
Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a 43.6-meter vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500 BC, is a full-size surviving example that may have filled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints.
Large seagoing ships are known to have been heavily used by the Egyptians in their trade with the city states of the eastern Mediterranean, especially Byblos (on the coast of modern day Lebanon), and in several expeditions down the Red Sea to the Land of Punt. In fact one of the earliest Egyptian words for a seagoing ship is a "Byblos Ship" which originally defined a class of Egyptian seagoing ships used on the Byblos run;however, by the end of the Old Kingdom, the term had come to include large seagoing ships, whatever their destination.
In 2011 archaeologists from Italy, the United States, and Egypt excavating a dried-up lagoon known as Mersa Gawasis have unearthed traces of an ancient harbor that once launched early voyages like Hatshepsut’s Punt expedition onto the open ocean. Some of the site’s most evocative evidence for the ancient Egyptians’ seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles.
And in 2013 a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Cheops on the Red Sea coast near Wadi el-Jarf (about 110 miles south of Suez) 
Mathematics.
The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system. The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain. Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction, multiplication, and division—use fractions, compute the volumes of boxes and pyramids, and calculate the surface areas of rectangles, triangles, and circles. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations.
Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively. Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to write fractions as the sum of several fractions. For example, they resolved the fraction "two-fifths" into the sum of "one-third" + "one-fifteenth". Standard tables of values facilitated this. Some common fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.
Ancient Egyptian mathematicians had a grasp of the principles underlying the Pythagorean theorem, knowing, for example, that a triangle had a right angle opposite the hypotenuse when its sides were in a 3–4–5 ratio. They were able to estimate the area of a circle by subtracting one-ninth from its diameter and squaring the result:
a reasonable approximation of the formula π"r" 2.
The golden ratio seems to be reflected in many Egyptian constructions, including the pyramids, but its use may have been an unintended consequence of the ancient Egyptian practice of combining the use of knotted ropes with an intuitive sense of proportion and harmony.
Legacy.
The culture and monuments of ancient Egypt have left a lasting legacy on the world. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome. The Romans also imported building materials from Egypt to erect Egyptian style structures. Early historians such as Herodotus, Strabo, and Diodorus Siculus studied and wrote about the land, which Romans came to view as a place of mystery.
During the Middle Ages and the Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi. In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities.
Although the European colonial occupation of Egypt destroyed a significant portion of the country's historical legacy, some foreigners had more positive results. Napoleon, for example, arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the "Description de l'Ėgypte".
In the 20th century AD, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. The Supreme Council of Antiquities now approves and oversees all excavations, which are aimed at finding information rather than treasure. The council also supervises museums and monument reconstruction programs designed to preserve the historical legacy of Egypt.

</doc>
<doc id="875" url="http://en.wikipedia.org/wiki?curid=875" title="Analog Brothers">
Analog Brothers

Analog Brothers were an experimental hip-hop crew featuring Ice Oscillator also known as Ice-T (keyboards, drums, vocals), Keith Korg also known as Kool Keith (bass, strings, vocals), Mark Moog also known as Marc Live (drums, "violyns" and vocals), Silver Synth also known as Black Silver (synthesizer, lazar bell and vocals), and Rex Roland also known as Pimp Rex (keyboards, vocals, production). Its album "Pimp to Eat" featured guest appearances by various members of Rhyme Syndicate, Odd Oberheim, Jacky jasper (who appears as Jacky Jasper on the song "We Sleep Days" and H-Bomb on "War"), D.J. Cisco from S.M., the Synth-a-Size Sisters and Teflon.
While the group only recorded one album together as the Analog Brothers, a few bootlegs of its live concert performances, including freestyles with original lyrics, have occasionally surfaced online. After "Pimp to Eat", the Analog Brothers continued performing together in various line ups. Kool Keith and Marc Live joined with Jacky jasper to release two albums as KHM. Marc Live rapped with Ice T's group SMG. Marc also formed a group with Black Silver called Live Black, but while five of their tracks were released on a demo CD sold at concerts, Live Black's first album has yet to be released.
In 2008, Ice-T and Black Silver toured together as Black Ice, and released an album together called Urban Legends.
In addition to all this, the Analog Brothers continue to make frequent appearances on each other's solo albums.

</doc>
<doc id="876" url="http://en.wikipedia.org/wiki?curid=876" title="Motor neuron disease">
Motor neuron disease

Motor neuron disease (MND) are a small group of neurological disorders that selectively affect motor neurons, the cells that control voluntary muscle activity including speaking, walking, swallowing, and general movement of the body. They are generally progressive in nature, and cause increasing disability and, eventually, death.
Motor neuron disorders or motor neuron diseases are a larger group of disorders that include any disease that affects neurons.
Terminology.
The term motor neuron disease is used inconsistently. Some use it to refer to a group of diseases which includes amyotrophic lateral sclerosis (ALS) and others use it to specifically mean ALS. The other four motor neuron disease are: primary lateral sclerosis, progressive muscular atrophy, progressive bulbar palsy and pseudobulbar palsy. To avoid confusion, the annual scientific research conference dedicated to the study of MND is called the "International ALS/MND Symposium".
While MND refers to a specific subset of similar diseases, there are numerous other diseases of motor neurons that as a group are referred to as motor neuron disorders. Example include spinal muscular atrophy, spinobulbar muscular atrophy, Charcot–Marie–Tooth disease, Guillain–Barré syndrome, and many others.
Classification.
In the most common classification, the term "motor neuron disease" applies to the following five disorders which affect either upper motor neurons (UMN) or lower motor neurons (LMN), or both:
Even though diseases known as spinal muscular atrophies, including the most common spinal muscular atrophy (SMA), affect motor neurons and are classified as MND by the disease terminology classification system Medical Subject Headings (MeSH), they are not classified as such by the tenth International Statistical Classification of Diseases and Related Health Problems (ICD-10) published in 1992 and thus are not discussed in this article.

</doc>
<doc id="877" url="http://en.wikipedia.org/wiki?curid=877" title="Abjad">
Abjad

An abjad is a type of writing system where each symbol always or usually stands for a consonant, leaving the reader to supply the appropriate vowel.
It is a term suggested by Peter T. Daniels to replace the common terms "consonantary", "consonantal alphabet" or "syllabary" to refer to the family of scripts called West Semitic.
Abjad is thought to be based on the first letters (a, b, g, d) found in all Semitic languages such as Phoenician, Syriac, Hebrew, and Arabic. In Arabic, "A" (), "B" (), "" (), "D" () make the word "abjad" which means "alphabet". The modern Arabic word for "alphabet" and "abjad" is interchangeably either " or ". The word "alphabet" in English has a source in Greek language in which the first two letters were "A" (alpha) and "B" (beta), hence "alphabeta" (in Italian, Portuguese and Spanish, "alfabeto", but also called "abecedario", from "a" "b" "c" "d"). In Hebrew the first two letters are "A" (aleph), "B" (bet) hence "alephbet." It is also used to enumerate a list in the same manner that "a, b, c, d" (etc.) are used in the English language.
Etymology.
The name "abjad" (' ) is derived from pronouncing the first letters of the "Arabic" alphabet in order. The ordering (') of Arabic letters used to match that of the older Hebrew, Phoenician and Semitic alphabets; "" (read from right to left: ) or .
The word is also used as a synonym for alphabet in Malay as Malay has many loanwords from Arabic.
Terminology.
According to the formulations of Daniels, abjads differ from alphabets in that only consonants, not vowels, are represented among the basic graphemes. Abjads differ from abugidas, another category invented by Daniels, in that in abjads, the vowel sound is "implied" by phonology, and where vowel marks exist for the system, such as nikkud for Hebrew and ḥarakāt for Arabic, their use is optional and not the dominant (or literate) form. Abugidas mark the vowels (other than the "inherent" vowel) with a diacritic, a minor attachment to the letter, or a standalone glyph. Some abugidas use a special symbol to "suppress" the inherent vowel so that the consonant alone can be properly represented. In a syllabary, a grapheme denotes a complete syllable, that is, either a lone vowel sound or a combination of a vowel sound with one or more consonant sounds.
The antagonism of abjad versus alphabet, as it was formulated by Daniels, has been rejected by other scholars because abjad is also used as a term not only for the Arabic numeral system but, which is most important in terms of historical grammatology, also as term for the alphabetic device (i.e. letter order) of ancient Northwest Semitic scripts in opposition to the 'south Arabian‘ order. This caused fatal effects on terminology in general and especially in (ancient) Semitic philology. Also, it suggests that consonantal alphabets, in opposition to for instance the Greek alphabet, were not yet true alphabets and not yet entirely complete, lacking something important to be a fully working script system. It has also been objected that, as a set of letters, an alphabet is not the mirror of what should be there in a language from a phonemic or even phonological point of view, rather, it is the data stock of what provides maximum efficiency with least effort from a semantic point of view.
Values table.
The "Abjad gadol" (see below) values are:
Origins.
All known abjads belong to the Semitic family of scripts. These scripts are thought to derive from the Proto-Sinaitic alphabet (dated to about 1500 BC), which is thought to derive from Egyptian hieroglyphs; Egyptian writing systems (including hieroglyphic, hieratic, and demotic) used certain characters in abjad-like fashion to supplement the logographic element of the script. The abjad was significantly simpler than the earlier hieroglyphs. The number of distinct glyphs was reduced tremendously at the cost of increased ambiguity.
The first abjad to gain widespread usage was the Phoenician abjad. Unlike other contemporary scripts, such as Cuneiform and Egyptian hieroglyphs, the Phoenician script consisted of only about two dozen symbols. This made the script easy to learn, and Phoenician seafaring merchants took the script wherever they went. Phoenician gave way to a number of new writing systems, including the Greek alphabet and Aramaic, a widely used abjad. The Greek alphabet evolved into the modern western alphabets, such as Latin and Cyrillic, while Aramaic became the ancestor of many modern abjads and abugidas of Asia.
Aramaic spread across Asia, reaching as far as India and may become Brahmi, the ancestral abugida to most modern Indian and Southeast Asian scripts. In the Middle East, Aramaic gave rise to the Hebrew and Nabataean abjads, which retained many of the Aramaic letter forms. The Syriac script was a cursive variation of Aramaic. It is unclear whether the Arabic abjad was derived from Nabatean or Syriac.
Impure abjads.
"Impure" abjads have characters for some vowels, optional vowel diacritics, or both. The term "pure" abjad refers to scripts entirely lacking in vowel indicators. However, most modern abjads, such as Arabic, Hebrew, Aramaic and Avestan, are "impure" abjads, that is, they also contain symbols for some of the vowel phonemes. An example of a "pure" abjad is ancient Phoenician.
Addition of vowels.
In the 9th century BC, the Greeks adapted the Phoenician script for use in their own language. The phonetic structure of the Greek language created too many ambiguities when the vowels went unrepresented, so the script was modified. They did not need letters for the guttural sounds represented by aleph, he, heth or ayin, so these symbols were assigned vocalic values. The letters waw and yod were also used. The Greek alphabet thus became the world's first to have symbols representing vowel sounds.
Abugidas developed along a slightly different route. The basic consonantal symbol was considered to have an inherent "a" vowel sound. Hooks or short lines attached to various parts of the basic letter modify the vowel. In this way, the South Arabian alphabet evolved into the Ge'ez alphabet between the 5th century BC and the 5th century AD. Similarly, around the 3rd century BC, the Brāhmī script developed (from the Aramaic abjad, it has been hypothesized).
The other major family of abugidas, Canadian Aboriginal syllabics, was initially developed in the 1840s by missionary and linguist James Evans for the Cree and Ojibwe languages. Evans used features of Devanagari script and Pitman shorthand to create his initial abugida. Later in the 19th century, other missionaries adapted Evans' system to other Canadian aboriginal languages. Canadian syllabics differ from other abugidas in that the vowel is indicated by rotation of the consonantal symbol, with each vowel having a consistent orientation.
Abjads and the structure of Semitic languages.
The abjad form of writing is well-adapted to the morphological structure of the Semitic languages it was developed to write. This is because words in Semitic languages are formed from a root consisting of (usually) three consonants, the vowels being used to indicate inflectional or derived forms. For instance, according to Classical Arabic and Modern Standard Arabic, the Arabic root ' (to sacrifice) can be derived the forms ' (he sacrificed), ' (you (masculine singular) sacrificed), ' (he slaughtered), ' (he slaughters), and ' (slaughterhouse). In each case, the absence of full glyphs for vowels makes the common root clearer, allowing readers to guess the meaning of unfamiliar words from familiar roots (especially in conjunction with context clues) and improving word recognition while reading for practiced readers.

</doc>
<doc id="878" url="http://en.wikipedia.org/wiki?curid=878" title="Abugida">
Abugida

An abugida (from Ge‘ez አቡጊዳ "’äbugida"), also called an alphasyllabary, is a segmental writing system in which consonant–vowel sequences are written as a unit: each unit is based on a consonant letter, and vowel notation is secondary. This contrasts with a full alphabet, in which vowels have status equal to consonants, and with an abjad, in which vowel marking is absent or optional. (In less formal contexts, all three types of script may be termed alphabets.) Abugidas include the extensive Brahmic family of scripts of South and Southeast Asia.
"Abugida" as a term in linguistics was proposed by Peter T. Daniels in his 1990 typology of writing systems. "’Abugida" is an Ethiopian name for the Ge‘ez script, taken from four letters of that script, "’ä bu gi da", in much the same way that "abecedary" is derived from Latin "a be ce de", and "alphabet" is derived from the names of the two first letters in the Greek alphabet, "alpha" and "beta". As Daniels used the word, an abugida is in contrast with a syllabary, where letters with shared consonants or vowels show no particular resemblance to one another, and also with an alphabet proper, where independent letters are used to denote both consonants and vowels. The term "alphasyllabary" was suggested for the Indic scripts in 1997 by William Bright, following South Asian linguistic usage, to convey the idea that "they share features of both alphabet and syllabary".
Abugidas were long considered to be syllabaries, or intermediate between syllabaries and alphabets, and the term "syllabics" is retained in the name of Canadian Aboriginal Syllabics. Other terms that have been used include "neosyllabary" (Février 1959), "pseudo-alphabet" (Householder 1959), "semisyllabary" (Diringer 1968; a word that has other uses) and "syllabic alphabet" (Coulmas 1996; this term is also a synonym for syllabary).
Description.
In general, a letter of an abugida transcribes a consonant. Letters are written as a linear sequence, in most cases left to right. Vowels are written through modification of these consonantal letters, either by means of diacritics (which may not follow the direction of writing the letters), or by changes in the form of the letter itself.
Vowels not preceded by a consonant may be represented with a zero consonant letter, modified to indicate the vowel, or separate letters for each vowel, that are distinct from the corresponding dependent vowel signs. Consonants not followed by a vowel may be represented with:
There are three principal families of abugidas, depending on whether vowels are indicated by modifying consonants by "diacritics, distortion," or "orientation."
Tāna of the Maldives has dependent vowels and a zero vowel sign, but no inherent vowel.
Indic (Brahmic).
Indic scripts originated in India and spread to Southeast Asia. All surviving Indic scripts are descendants of the Brahmi alphabet. Today they are used in most languages of South Asia (although replaced by Perso-Arabic in Urdu, Kashmiri and some other languages of Pakistan and India) and mainland Southeast Asia (Burma, Thailand, Laos, Cambodia; but not Malaysia or Vietnam). The primary division is into North Indic scripts used in Northern India, Nepal, Tibet and Bhutan, and Southern Indic scripts used in South India, Sri Lanka and Southeast Asia.
South Indic letter forms are very rounded; North Indic less so, though Oriya, Golmol and Litumol of Nepal script are rounded.
Most North Indic scripts' full letters incorporate a horizontal line at the top, with Gujarati and Oriya script as exceptions; South Indic scripts do not.
Indic scripts indicate vowels through dependent vowel signs (diacritics) around the consonants, often including a sign that explicitly indicates the lack of a vowel. If a consonant has no vowel sign, this indicates a default vowel. Vowel diacritics may appear above, below, to the left, to the right, or around the consonant.
The most widely used Indic script is Devanagari, shared by Hindi, Bhojpuri, Marathi, Nepali, and often Sanskrit. A basic letter such as क in Hindi represents a syllable with the default vowel, in this case "ka" (). In some languages, including Hindi, it becomes a final closing consonant at the end of a word, in this case "k". The inherent vowel may be changed by adding vowel mark (diacritics), producing syllables such as कि "ki," कु "ku," के "ke," को "ko." The mora a consonant letter represents, either with or without a marked vowel, is called an "akshara".
In many of the Brahmic scripts, a syllable beginning with a cluster is treated as a single character for purposes of vowel marking, so a vowel marker like "-i," falling before the character it modifies, may appear several positions before the place where it is pronounced. For example, the game cricket in Hindi is क्रिकेट "krikeţ;" the diacritic for appears before the consonant cluster , not before the . A more unusual example is seen in the Batak alphabet: Here the syllable "bim" is written "ba-ma-i-(virama)". That is, the vowel diacritic and virama are both written after the consonants for the whole syllable.
In many abugidas, there is also a diacritic to suppress the inherent vowel, yielding the bare consonant. In Devanagari, क् is "k," and ल् is "l". This is called the "virāma" or "halantam" in Sanskrit. It may be used to form consonant clusters, or to indicate that a consonant occurs at the end of a word. Thus in Sanskrit, a default vowel consonant such as क does not take on a final consonant sound. Instead, it keeps its vowel. For writing two consonants without a vowel in between, instead of using diacritics on the first consonant to remove its vowel, another popular method of special conjunct forms is used in which two or more consonant characters are merged to express a cluster, such as Devanagari: क्ल "kla." (Note that some fonts display this as क् followed by ल, rather than forming a conjunct. This expedient is used by ISCII and South Asian scripts of Unicode.) Thus a closed syllable such as "kal" requires two "aksharas" to write.
The Róng script used for the Lepcha language goes further than other Indic abugidas, in that a single "akshara" can represent a closed syllable: Not only the vowel, but any final consonant is indicated by a diacritic. For example, the syllable [sok] would be written as something like , here with an underring representing and an overcross representing the diacritic for final . Most other Indic abugidas can only indicate a very limited set of final consonants with diacritics, such as or , if they can indicate any at all.
Ethiopic.
In Ethiopic, where the term "abugida" originates, the diacritics have been fused to the consonants to the point that they must be considered modifications of the form of the letters. Children learn each modification separately, as in a syllabary; nonetheless, the graphic similarities between syllables with the same consonant is readily apparent, unlike the case in a true syllabary.
Though now an abugida, the Ge'ez alphabet, until the advent of Christianity ("ca." AD 350), had originally been what would now be termed an "abjad". In the Ge'ez abugida (or "fidel"), the base form of the letter (also known as "fidel") may be altered. For example, ሀ "hä" (base form), ሁ "hu" (with a right-side diacritic that doesn't alter the letter), ሂ "hi" (with a subdiacritic that compresses the consonant, so it is the same height), ህ "hə" or (where the letter is modified with a kink in the left arm).
Canadian Aboriginal syllabics.
In the family known as Canadian Aboriginal syllabics, which was inspired by the Devanagari script of India, vowels are indicated by changing the orientation of the syllabogram. Each vowel has a consistent orientation; for example, Inuktitut ᐱ "pi," ᐳ "pu," ᐸ "pa;" ᑎ "ti," ᑐ "tu," ᑕ "ta". Although there is a vowel inherent in each, all rotations have equal status and none can be identified as basic. Bare consonants are indicated either by separate diacritics, or by superscript versions of the "aksharas"; there is no vowel-killer mark.
Borderline cases.
Vowelled abjads.
Consonantal scripts ("abjads") are normally written without indication of many vowels. However in some contexts like teaching materials or scriptures, Arabic and Hebrew are written with full indication of vowels via diacritic marks ("harakat", "niqqud") making them effectively abugidas. The Brahmic and Ethiopic families are thought to have originated from the Semitic abjads by the addition of vowel marks.
The Arabic scripts used for Kurdish in Iraq and for Uighur in Xinjiang, China, as well as the Hebrew script of Yiddish, are fully vowelled, but because the vowels are written with full letters rather than diacritics and there are no inherent vowels, these are considered alphabets, not abugidas.
Phagspa.
The imperial Mongol script called Phagspa was derived from the Tibetan abugida, but all vowels are written in-line rather than as diacritics. However, it retains the features of having an inherent vowel /a/ and having distinct initial vowel letters.
Pahawh.
Pahawh Hmong is a non-segmental script that indicates syllable onsets and rimes, such as consonant clusters and vowels with final consonants. Thus it is not segmental and cannot be considered an abugida. However, it superficially resembles an abugida with the roles of consonant and vowel reversed. Most syllables are written with two letters in the order rime–onset (typically vowel-consonant), even though they are pronounced as onset-rime (consonant-vowel), rather like the position of the vowel in Devanagari, which is written before the consonant. Pahawh is also unusual in that, while an inherent rime (with mid tone) is unwritten, it also has an inherent onset . For the syllable , which requires one or the other of the inherent sounds to be overt, it is that is written. Thus it is the rime (vowel) which is basic to the system.
Meroitic.
It is difficult to draw a dividing line between abugidas and other segmental scripts. For example, the Meroitic script of ancient Sudan did not indicate an inherent "a" (one symbol stood for both "m" and "ma," for example), and is thus similar to Brahmic family of abugidas. However, the other vowels were indicated with full letters, not diacritics or modification, so the system was essentially an alphabet that did not bother to write the most common vowel.
Shorthand.
Several systems of shorthand use diacritics for vowels, but they do not have an inherent vowel, and are thus more similar to Thaana and Kurdish script than to the Brahmic scripts. The Gabelsberger shorthand system and its derivatives modify the "following" consonant to represent vowels. The Pollard script, which was based on shorthand, also uses diacritics for vowels; the placements of the vowel relative to the consonant indicates tone. Pitman shorthand uses straight strokes and quarter-circle marks in different orientations as the principal "alphabet" of consonants; vowels are shown as light and heavy dots, dashes and other marks in one of 3 possible positions to indicate the various vowel-sounds. However, to increase writing speed, Pitman has rules for using the positioning or choice of consonant signs so that writing vowel-marks can be dispensed with.
Development.
As the term "alphasyllabary" suggests, abugidas have been considered an intermediate step between alphabets and syllabaries. Historically, abugidas appear to have evolved from abjads (vowelless alphabets). They contrast with syllabaries, where there is a distinct symbol for each syllable or consonant-vowel combination, and where these have no systematic similarity to each other, and typically develop directly from logographic scripts. Compare the Devanagari examples above to sets of syllables in the Japanese hiragana syllabary: か "ka", き "ki", く "ku", け "ke", こ "ko" have nothing in common to indicate "k;" while ら "ra", り "ri", る "ru", れ "re", ろ "ro" have neither anything in common for "r", nor anything to indicate that they have the same vowels as the "k" set.
Most Indian and Indochinese abugidas appear to have first been developed from abjads with the and Brāhmī scripts; the abjad in question is usually considered to be the Aramaic one, but while the link between Aramaic and Kharosthi is more or less undisputed, this is not the case with Brahmi. The Kharosthi family does not survive today, but Brahmi's descendants include most of the modern scripts of South and Southeast Asia. Ge'ez derived from a different abjad, the Sabean script of Yemen; the advent of vowels coincided with the introduction of Christianity about AD 350;.
" is encumbered with the drawback of an excessively awkward and cumbrous written character. ... At first glance, an book seems to be all curves, and it takes a second look to notice that there is something inside each." (G.A. Grierson, Linguistic Survey of India, 1903)

</doc>
<doc id="880" url="http://en.wikipedia.org/wiki?curid=880" title="ABBA">
ABBA

ABBA was a Swedish pop group formed in Stockholm in 1972, comprising Agnetha Fältskog, Björn Ulvaeus, Benny Andersson, and Anni-Frid Lyngstad. ABBA is an acronym of the first letters of the band members' first names and is sometimes stylized as the registered trademark ᗅᗺᗷᗅ. The band became one of the most commercially successful acts in the history of popular music, topping the charts worldwide from 1975 to 1982. It also won the Eurovision Song Contest 1974, giving Sweden its first triumph in the history of the contest and being the most successful group ever to take part in the competition.
ABBA has sold over 380 million albums and singles worldwide, which makes it one of the best-selling music artists of all time, and the second best-selling music group of all time. ABBA was the first group to come from a non-English-speaking country that enjoyed consistent success in the charts of English-speaking countries, including the UK, Ireland, the U.S., Canada, Australia, New Zealand, and South Africa. The group also enjoyed significant success in Latin American markets, and recorded a collection of their hit songs in Spanish.
During the band's active years, Fältskog and Ulvaeus were very much in love, and married, as were Lyngstad and Andersson, although both couples later divorced. At the height of their popularity, both relationships were suffering strain which ultimately resulted in the collapse of the Ulvaeus-Fältskog marriage in 1979 and the Andersson-Lyngstad marriage in 1981. These relationship changes were reflected in the group's music, with later compositions including more introspective, brooding, dark lyrics.
After ABBA broke up in late 1982, Andersson and Ulvaeus achieved success writing music for the stage while Lyngstad and Fältskog pursued solo careers with mixed success. ABBA's music declined in popularity until several films, notably "Muriel's Wedding" and "The Adventures of Priscilla, Queen of the Desert", revived interest in the group and the spawning of several tribute bands. In 1999, ABBA's music was adapted into the successful musical "Mamma Mia!" that toured worldwide. A film of the same name, released in 2008, became the highest-grossing film in the United Kingdom that year. The group was inducted into the Rock and Roll Hall of Fame on 15 March 2010.
ABBA were honored at the 50th anniversary celebration of the Eurovision Song Contest in 2005, when their hit "Waterloo" was chosen as the best song in the competition's history.
History.
Before ABBA (1960s).
Benny Andersson (born 16 December 1946 in Stockholm, Sweden) became (at age 18) a member of a popular Swedish pop-rock group the Hep Stars that performed covers, amongst other things, of international hits. The Hep Stars were known as "the Swedish Beatles". They also set up Hep House, their equivalent of Apple Corps. Andersson played the keyboard and eventually started writing original songs for his band, many of which became major hits, including "No Response" that hit number 3 in 1965, "Sunny Girl", "Wedding", and "Consolation", all of which hit number 1 in 1966. Andersson also had a fruitful songwriting collaboration with Lasse Berghagen, with whom he wrote his first Svensktoppen entry "Sagan om lilla Sofie" ("The Story of Little Sophie") in 1968.
Björn Ulvaeus (born 25 April 1945 in Gothenburg/Göteborg, Sweden) also began his musical career at 18 (as a singer and guitarist), when he fronted The Hootenanny Singers, a popular Swedish folk-skiffle group. Ulvaeus started writing English-language songs for his group, and even had a brief solo career alongside. The Hootenanny Singers and The Hep Stars sometimes crossed paths while touring. In June 1966, Ulvaeus and Andersson decided to write a song together. Their first attempt was "Isn't It Easy to Say", a song later recorded by The Hep Stars. Stig Anderson was the manager of The Hootenanny Singers and founder of the Polar Music label. He saw potential in the collaboration, and encouraged them to write more. Both also began playing occasionally with the other's bands on stage and on record, although it was not until 1969 that the pair wrote and produced some of their first real hits together: "Ljuva sextital" ("Sweet Sixties"), recorded by Brita Borg, and The Hep Stars' 1969 hit "Speleman" ("Fiddler").
Andersson wrote and submitted the song "Hej, Clown" for the 1969 Melodifestivalen, the national festival to select the Swedish entry to the Eurovision Song Contest. The song tied for first place, but re-voting relegated Andersson's song to second place. On that occasion Andersson briefly met his future spouse, singer Anni-Frid Lyngstad, who also participated in the contest. A month later, the two had become a couple. As their respective bands began to break up during 1969, Andersson and Ulvaeus teamed up and recorded their first album together in 1970, called "Lycka" ("Happiness"), which included original songs sung by both men. Their spouses were often present in the recording studio, and sometimes added backing vocals; Fältskog even co-wrote a song with the two. Ulvaeus still occasionally recorded and performed with The Hootenanny Singers until the summer of 1974, and Andersson took part in producing their records.
Agnetha Fältskog (born 5 April 1950 in Jönköping, Sweden) sang with a local dance band headed by Bernt Enghardt who sent a demo recording of the band to Karl Gerhard Lundkvist. The demo tape featured a song written and sung by Agnetha: "Jag var så kär". Lundkvist was so impressed with her voice that he was convinced she would be a star. After going through considerable effort to locate the singer, he arranged for Agnetha to come to Stockholm and to record two of her own songs. This led to Agnetha having a number 1 record in Sweden with a self-composed song and selling more than 80,000 copies while she was still only 17. She was soon noticed by the critics and songwriters as a talented singer/songwriter of schlager style songs. Fältskog's main inspiration in her early years were singers such as Connie Francis. Along with her own compositions, she recorded covers of foreign hits and performed them on tours in Swedish folkparks. Most of her biggest hits were self-composed, which was quite unusual for a female singer in the 1960s. Agnetha released four solo LPs between 1968 and 1971. She had many successful singles in the Swedish charts.
During filming of a Swedish TV special in May 1969, Fältskog met Ulvaeus, and they married on 6 July 1971. Fältskog and Ulvaeus eventually were involved in each other's recording sessions, and soon even Andersson and Lyngstad added backing vocals to her third studio album "Som jag är" ("As I Am") (1970). In 1972, Fältskog starred as Mary Magdalene in the original Swedish production of "Jesus Christ Superstar" and attracted favourable reviews. Between 1967 and 1975, Fältskog released five studio albums.
Anni-Frid "Frida" Lyngstad (born 15 November 1945 in Bjørkåsen in Ballangen, Norway) sang from the age of 13 with various dance bands, and worked mainly in a jazz-oriented cabaret style. She also formed her own band, the Anni-Frid Four. In the summer of 1967, she won a national talent competition with "En ledig dag" ("A Day Off") a Swedish version of the bossa nova song "A Day in Portofino", which is included in the EMI compilation "Frida 1967–1972". The first prize was a recording contract with EMI Sweden and to perform live on the most popular TV shows in the country. This TV performance, amongst many others, is included in the 3½ hour documentary "Frida – The DVD". Lyngstad released several schlager style singles on EMI without much success. When Benny Andersson started to produce her recordings in 1971, she had her first number 1 single, "Min egen stad" ("My Own Town") written by Benny featuring all the future ABBA members on backing vocals. Lyngstad toured and performed regularly in the folkpark circuit and made appearances on radio and TV. She met Ulvaeus briefly in 1963 during a talent contest, and Fältskog during a TV show in early 1968.
Lyngstad finally linked up with her future bandmates in 1969. On 1 March 1969, she participated in the Melodifestivalen, where she met Andersson for the first time. A few weeks later they met again during a concert tour in southern Sweden and they soon became a couple. Andersson produced her single "Peter Pan" in September 1969 — her first collaboration with Benny & Björn, as they had written the song. Andersson would then produce Lyngstad's debut studio album, "Frida", which was released in March 1971. Lyngstad also played in several revues and cabaret shows in Stockholm between 1969 and 1973. After ABBA formed, she recorded another successful album in 1975, "Frida ensam", which included a Swedish rendition of "Fernando", a hit on the Swedish radio charts before the English version was released.
First live performance and the start of "Festfolket".
An attempt at combining their talents occurred in April 1970 when the two couples went on holiday together to the island of Cyprus. What started as singing for fun on the beach ended up as an improvised live performance in front of the United Nations soldiers stationed on the island. Andersson and Ulvaeus were at this time recording their first album together, "Lycka", which was to be released in September 1970. Fältskog and Lyngstad added backing vocals on several tracks during June, and the idea of them working together saw them launch a stage act, "Festfolket" (which translates from Swedish to mean both "Party People" and "Engaged Couples") on 1 November 1970 in Gothenburg.
The cabaret show attracted generally negative reviews, except for the performance of the Andersson and Ulvaeus hit "Hej, gamle man" ("Hello, Old Man"); the first Björn and Benny recording to feature all four. They also performed solo numbers from respective albums, but the lukewarm reception convinced the foursome to shelve plans for working together for the time being, and each soon concentrated on individual projects again.
First record together "Hej, gamle man".
"Hej, gamle man", a song about an old Salvation Army soldier, became the quartet's first hit. The record was credited to Björn & Benny and reached number 5 on the sales charts and number 1 on Svensktoppen, staying there for 15 weeks.
It was during 1971 that the four artists began working together more, adding vocals to the others' recordings. Fältskog, Andersson and Ulvaeus toured together in May, while Lyngstad toured on her own. Frequent recording sessions brought the foursome closer together during the summer.
Forming the group (1970 till 1973).
After the 1970 release of "Lycka", two more singles credited to 'Björn & Benny' were released in Sweden, "Det kan ingen doktor hjälpa" ("No Doctor Can Help with That") and "Tänk om jorden vore ung" ("Imagine If the Earth Were Young"), with more prominent vocals by Fältskog and Lyngstad–and moderate chart success.
Fältskog and Ulvaeus, now married, started performing together with Andersson on a regular basis at the Swedish folkparks during the summer of 1971.
Stig Anderson, founder and owner of Polar Music, was determined to break into the mainstream international market with music by Andersson and Ulvaeus. "One day the pair of you will write a song that becomes a worldwide hit", he predicted. Stig Anderson encouraged Ulvaeus and Andersson to write a song for Melodifestivalen, and after two rejected entries in 1971, Andersson and Ulvaeus submitted their new song "Säg det med en sång" ("Say It with a Song") for the 1972 contest, choosing newcomer Lena Anderson to perform. The song came in third place, encouraging Stig Anderson, and became a hit in Sweden.
The first signs of foreign success came as a surprise, as the Andersson and Ulvaeus single "She's My Kind of Girl" was released through Epic Records in Japan in March 1972, giving the duo a Top 10 hit. Two more singles were released in Japan, "En Carousel" ("En Karusell" in Scandinavia, an earlier version of "Merry-Go-Round") and "Love Has Its Ways" (a song they wrote with Kōichi Morita).
First hit as Agnetha, Anni-Frid, Benny & Björn.
Ulvaeus and Andersson persevered with their songwriting and experimented with new sounds and vocal arrangements. "People Need Love" was released in June 1972, featuring guest vocals by the women, who were now given much greater prominence. Stig Anderson released it as a single, credited to "Björn & Benny, Agnetha & Anni-Frid". The song peaked at number 17 in the Swedish combined single and album charts, enough to convince them they were on to something. The single also became the first record to chart for the quartet in the United States, where it peaked at number 114 on the "Cashbox" singles chart and number 117 on the "Record World" singles chart. Labeled as "Björn & Benny (with Svenska Flicka)", it was released there through Playboy Records. However, according to Stig Anderson, "People Need Love" could have been a much bigger American hit, but a small label like Playboy Records did not have the distribution resources to meet the demand for the single from retailers and radio programmers.
The foursome decided to record their first album together in the autumn of 1972, and sessions began on 26 September 1972. The women shared lead vocals on "Nina, Pretty Ballerina" (a top ten hit in Austria) that day, and their voices in harmony for the first time gave the foursome an idea of the quality of their combined talents.
"Ring Ring".
In 1973, the band and their manager Stig Anderson decided to have another try at Melodifestivalen, this time with the song "Ring Ring". The studio sessions were handled by Michael B. Tretow, who experimented with a "wall of sound" production technique that became the wholly new sound. Stig Anderson arranged an English translation of the lyrics by Neil Sedaka and Phil Cody and they thought this would be a surefire winner. However, on 10 February 1973, the song came third in Melodifestivalen, thus it never reached the Eurovision Song Contest itself. Nevertheless, the group released their debut studio album, also called "Ring Ring". The album did well and the "Ring Ring" single was a hit in many parts of Europe and also in South Africa. However, Stig Anderson felt that the true breakthrough could only come with a UK or US hit.
Though Agnetha Fältskog gave birth to her first child in 1973 she was for a shorter period replaced by Inger Brundin on a trip to West Germany.
Official naming.
In early 1973, Stig Anderson, tired of unwieldy names, started to refer to the group privately and publicly as ABBA. At first, this was a play on words, as Abba is also the name of a well-known fish-canning company in Sweden, and itself an acronym. However, since the fish-canners were unknown outside Sweden, Anderson came to believe the name would work in international markets. A competition to find a suitable name for the group was held in a Gothenburg newspaper. The group was impressed with the names "Alibaba", "FABB", and "Baba", but in the end all the entries were ignored and it was officially announced in the summer that the group were to be known as "ABBA". The group negotiated with the canners for the rights to the name. "ABBA" is an acronym formed from the first letters of each group member's first name: Agnetha, Björn, Benny and Anni-Frid. During a promotional photo, Benny flipped his "B" horizontally for fun, and from 1976 onwards the first 'B' in the logo version of the name was "mirror-image" reversed on the band's promotional material and ᗅᗺᗷᗅ became the group's registered trademark.
The first time "ABBA" is found written on paper is on a recording session sheet from the Metronome Studio in Stockholm, dated 16 October 1973. This was first written as "Björn, Benny, Agnetha & Frida", but was subsequently crossed out with "ABBA" written in large letters on top.
The official logo, using the bold version of the News Gothic typeface, was designed by Rune Söderqvist, and appeared for the first time on the "Dancing Queen" single in August 1976, and subsequently on all later original albums and singles. But the idea for the official logo was made by the German photographer Wolfgang Heilemann on a "Dancing Queen" shoot for the teenage magazine Bravo. On the photo, the ABBA members held a giant initial letter of his/her name. After the pictures were made, Heilemann found out that one of the men held his letter backwards as in ᗅᗺᗷᗅ®. They discussed it and the members of ABBA liked it. Following their acquisition of the group's catalogue, Polygram began using variations of the ABBA logo, using a different font and adding a crown emblem to it in 1992 for the first release of the "ABBA Gold: Greatest Hits" compilation. When Universal Music purchased Polygram (and, thus, ABBA's label Polar Music International), control of the group's catalogue was returned to Stockholm. Since then, the original logo has been reinstated on all official products.
Breakthrough (1973–1976).
Eurovision Song Contest 1974.
As the group entered the Melodifestivalen with "Ring Ring" but failed to qualify as the 1973 Swedish entry, Stig Anderson immediately started planning for the 1974 contest.
Ulvaeus, Andersson and Stig Anderson believed in the possibilities of using the Eurovision Song Contest as a way to make the music business aware of them as songwriters, as well as the band itself. In late 1973, they were invited by Swedish television to contribute a song for the Melodifestivalen 1974 and from a number of new songs, the upbeat number "Waterloo" was chosen; the group was now inspired by the growing glam rock scene in England.
ABBA won their national heats on Swedish television on 9 February 1974, and with this third attempt were far more experienced and better prepared for the Eurovision Song Contest. Winning the 1974 Contest on 6 April 1974 gave ABBA the chance to tour Europe and perform on major television shows; thus the band saw the "Waterloo" single chart in many European countries. "Waterloo" was ABBA's first number one single in big markets such as the UK and Germany. In the United States, the song peaked at number six on the "Billboard" Hot 100 chart, paving the way for their first album and their first trip as a group there. Albeit a short promotional visit, it included their first performance on American television, "The Mike Douglas Show". The album "Waterloo" only peaked at number 145 on the "Billboard" 200 chart, but received unanimous high praise from the US critics: "Los Angeles Times" called it "a compelling and fascinating debut album that captures the spirit of mainstream pop quite effectively … an immensely enjoyable and pleasant project", while "Creem" characterized it as "a perfect blend of exceptional, lovable compositions".
ABBA's follow-up single, "Honey, Honey", peaked at number 27 on the US "Billboard" Hot 100, and was a number 2 hit in Germany. However, in the United Kingdom, ABBA's British record label, Epic, decided to re-release a remixed version of "Ring Ring" instead of "Honey, Honey", and a cover version of the latter by Sweet Dreams peaked at number 10. Both records debuted on the UK chart within one week of each other. "Ring Ring" failed to reach the Top 30 in the United Kingdom, increasing growing speculation that the group was simply a Eurovision one-hit wonder.
Post-Eurovision.
In November 1974, ABBA embarked on their first European tour, playing dates in Denmark, West Germany and Austria. It was not as successful as the band had hoped, since most of the venues did not sell out. Due to a lack of demand, they were even forced to cancel a few shows, including a sole concert scheduled in Switzerland. The second leg of the tour, which took them through Scandinavia in January 1975, was very different. They played to full houses everywhere and finally got the reception they had aimed for. Live performances continued during the summer of 1975 when ABBA embarked on a fourteen open-air date tour of Sweden and Finland. Their Stockholm show at the Gröna Lund amusement park had an estimated audience of 19,200. Björn Ulvaeus later said that "If you look at the singles we released straight after Waterloo, we were trying to be more like the Sweet, a semi-glam rock group, which was stupid because we were always a pop group."
In late 1974, "So Long" was released as a single in the United Kingdom but it received no airplay from Radio 1 and failed to chart. In the summer of 1975 ABBA released "I Do, I Do, I Do, I Do, I Do", which again received little airplay on Radio 1 but managed to climb the charts, to number 38. Later in 1975, the release of their self-titled third studio album "ABBA" and single "SOS" brought back their chart presence in the UK, where the single hit number 6 and the album peaked at number 13. "SOS" also became ABBA's second number 1 single in Germany and their third in Australia. Success was further solidified with "Mamma Mia" reaching number 1 in the United Kingdom, Germany and Australia. In the United States, "SOS" peaked at number 10 on the Record World Top 100 singles chart and number 15 on the Billboard Hot 100 chart, picking up the BMI Award along the way as one of the most played songs on American radio in 1975.
The success of the group in the United States had until that time been limited to single releases. By early 1976, the group already had four Top 30 singles on the US charts, but the album market proved to be tough to crack. The eponymous "ABBA " album generated three American hits, but it only peaked at number 165 on the "Cashbox" album chart and number 174 on the "Billboard" 200 chart. Opinions were voiced, by "Creem" in particular, that in the US ABBA had endured "a very sloppy promotional campaign". Nevertheless, the group enjoyed warm reviews from the American press. "Cashbox" went as far as saying that "there is a recurrent thread of taste and artistry inherent in Abba's marketing, creativity and presentation that makes it almost embarrassing to critique their efforts", while "Creem" wrote: "SOS is surrounded on this LP by so many good tunes that the mind boggles".
In Australia, the airing of the music videos for "I Do, I Do, I Do, I Do, I Do" and "Mamma Mia" on the nationally-broadcast TV pop show "Countdown" (which premiered in August 1975) saw the band rapidly gain enormous popularity, and "Countdown" become a key promoter of the group via their distinctive music videos. This started an immense interest for ABBA in Australia, resulting in both the single and album holding down the No. 1 positions on the charts for months.
Superstardom (1976–1981).
In March 1976, the band released the compilation album "Greatest Hits", despite having had only six top 40 hits in the United Kingdom and the United States. Nevertheless, it became their first UK number 1 album, and also took ABBA into the Top 50 on the US album charts for the first time, eventually selling more than a million copies there. At the same time, Germany released a compilation named "The Very Best of ABBA", also becoming a number 1 album there whereas the "Greatest Hits" compilation followed a few months later to number 2 on the German charts, despite all similarities with "The Very Best" album. Also included on "Greatest Hits" was a new single, "Fernando", which went to number 1 in at least thirteen countries worldwide, including the United Kingdom, Germany and Australia, and the single went on to sell over 10 million copies worldwide. In Australia, the song occupied the top position for 14 weeks (and stayed in the chart for 40 weeks), tying with The Beatles' "Hey Jude" for longest-running number one, and making "Fernando" one of the best-selling singles of all time in Australia. That same year, the group received its first international prize, with "Fernando" being chosen as the "Best Studio Recording of 1975". In the United States, "Fernando" reached the Top 10 of the Cashbox Top 100 singles chart and number 13 on the Billboard Hot 100. It also topped the Billboard Adult Contemporary chart, ABBA's first American number one single on any chart.
The group's fourth studio album, "Arrival", a number 1 bestseller in Europe and Australia, represented a new level of accomplishment in both songwriting and studio work, prompting rave reviews from more rock-oriented UK music weeklies such as "Melody Maker" and "New Musical Express", and mostly appreciative notices from US critics. Hit after hit flowed from "Arrival": "Money, Money, Money", another number 1 in Germany and Australia, and "Knowing Me, Knowing You", ABBA's sixth consecutive German number 1 as well as another UK number 1. The real sensation was "Dancing Queen", not only topping the charts in loyal markets the UK, Germany and Australia, but also reaching number 1 in the United States. In South Africa, ABBA had astounding success with "Fernando", "Dancing Queen" and "Knowing Me, Knowing You" being among the top 20 best-selling singles for 1976–77. In 1977, "Arrival" was nominated for the inaugural BRIT Award in the category "Best International Album of the Year". By this time ABBA were popular in the United Kingdom, most of Western Europe, Australia and New Zealand. In "Frida – The DVD", Lyngstad explains how she and Fältskog developed as singers, as ABBA's recordings grew more complex over the years.
The band's popularity in the United States would remain on a comparatively smaller scale, and "Dancing Queen" became the only Billboard Hot 100 number 1 single ABBA had there (they did, however, get three more singles to the number 1 position on other Billboard charts, including Billboard Adult Contemporary and Hot Dance Club Play). Nevertheless, "Arrival" finally became a true breakthrough release for ABBA on the US album market where it peaked at number 20 on the Billboard 200 chart and was certified gold by RIAA.
European and Australian tour.
In January 1977, ABBA embarked on their first major tour. The group's status had changed dramatically and they were clearly regarded as superstars. They opened their much anticipated tour in Oslo, Norway on 28 January, and mounted a lavishly produced spectacle that included a few scenes from their self-written mini-operetta "The Girl with the Golden Hair". The concert attracted immense media attention from across Europe and Australia. They continued the tour through Western Europe visiting Gothenburg, Copenhagen, Berlin, Cologne, Amsterdam, Antwerp, Essen, Hanover, Hamburg, ending with shows in the United Kingdom in Manchester, Birmingham, Glasgow and two sold-out concerts at London's Royal Albert Hall. Tickets for these two shows were available only by mail application and it was later revealed that the box-office received 3.5 million requests for tickets, enough to fill the venue 580 times. Along with praise ("ABBA turn out to be amazingly successful at reproducing their records", wrote "Creem"), there were complaints that "ABBA performed slickly...but with a zero personality coming across from a total of 16 people on stage" ("Melody Maker"). One of the Royal Albert Hall concerts was filmed as a reference for the filming of the Australian tour for what became "", though it is not exactly known how much of the concert was filmed.
After the European leg of the tour, in March 1977, ABBA played 11 dates in Australia before a total of 160,000 people. The opening concert in Sydney at the Sydney Showground on 3 March to an audience of 20,000 was marred by torrential rain with Lyngstad slipping on the wet stage during the concert. However all four members would later recall this concert as most memorable of their career. Upon their arrival in Melbourne, a civic reception was held at the Melbourne Town Hall and ABBA appeared on the balcony to greet an enthusiastic crowd of 6,000. In Melbourne, the group played three concerts at the Sidney Myer Music Bowl with 14,500 at each including the Australian Prime Minister Malcolm Fraser and his family. At the first Melbourne concert, an additional 16,000 people gathered outside the fenced-off area to listen to the concert. In Adelaide, the group performed one concert at West Lakes Football Stadium before 20,000 people with another 10,000 listening outside. During the first of five concerts in Perth, there was a bomb scare with everyone having to evacuate the Entertainment Centre. The trip was accompanied by mass hysteria and unprecedented media attention ("Swedish ABBA stirs box-office in Down Under tour...and the media coverage of the quartet rivals that set to cover the upcoming Royal tour of Australia", wrote "Variety"), and is captured on film in "", directed by Lasse Hallström.
The Australian tour and its subsequent "ABBA: The Movie" produced some ABBA lore, as well. Fältskog's blonde good looks had long made her the band's "pin-up girl", a role she disdained. During the Australian tour, she performed in a skin-tight white jumpsuit, causing one Australian newspaper to use the headline "Agnetha's bottom tops dull show". When asked about this at a news conference, she replied: "Don't they have bottoms in Australia?"
In December 1977, ABBA followed up "Arrival" with the more ambitious fifth album "", released to coincide with the debut of "ABBA: The Movie". Although the album was less well received by UK reviewers, it did spawn more worldwide hits: "The Name of the Game" and "Take a Chance on Me", which both topped the UK charts, and peaked at number 12 and number 3 respectively on the Billboard Hot 100 chart in the US. Although "Take a Chance on Me" did not top the American charts, it proved to be ABBA's biggest hit single there, selling more copies than "Dancing Queen". "The Album" also included "Thank You for the Music", the B-side of "Eagle" in countries where the latter had been released as a single, and was belatedly released as an A-side single in the United Kingdom and Ireland in 1983. "Thank You for the Music" has become one of the best loved and best known ABBA songs without being released as a single during the group's lifetime.
Polar Music Studio formation.
By 1978 ABBA were one of the biggest bands in the world. They converted a vacant movie theatre into the Polar Music Studio, a state-of-the-art studio in Stockholm. The studio was used by several other bands; notably Genesis' "Duke" and Led Zeppelin's "In Through the Out Door" were recorded there. During May, the group went to the United States for a promotional campaign, performing alongside Andy Gibb on Olivia Newton-John's TV show. Recording sessions for the single "Summer Night City" were an uphill struggle, but upon release the song became another hit for the group. The track would set the stage for ABBA's foray into disco with their next album.
On 9 January 1979, the group performed "Chiquitita" at the Music for UNICEF Concert held at the United Nations General Assembly to celebrate UNICEF's Year of the Child. ABBA donated the copyright of this worldwide hit to the UNICEF; see Music for UNICEF Concert. The single was released the following week, and reached number 1 in ten countries.
North American and European tours.
In mid-January 1979, Ulvaeus and Fältskog announced they were getting divorced. The news caused interest from the media, and led to speculation about the band's future. ABBA assured the press and their fan base they were continuing their work as a group, and that the divorce would not affect them. Nonetheless, the media continued to confront them with this in interviews. To escape the media swirl and concentrate on their writing, Andersson and Ulvaeus secretly travelled to Compass Point Studios in Nassau, Bahamas, where for two weeks they prepared their next album's songs in relative quiet.
The group's sixth studio album, "Voulez-Vous", was released in April 1979, the title track of which was recorded at the famous Criteria Studios in Miami, Florida, with the assistance of recording engineer Tom Dowd amongst others. The album topped the charts across Europe and in Japan and Mexico, hit the Top 10 in Canada and Australia and the Top 20 in the United States. None of the singles from the album reached number 1 on the UK charts, but "Chiquitita", "Does Your Mother Know", "Angeleyes" (with "Voulez-Vous", released as a double A-side) and "I Have a Dream" were all UK Top 5 hits. In Canada, "I Have a Dream" became ABBA's second number 1 on the RPM Adult Contemporary chart (after "Fernando" hit the top previously). Also in 1979, the group released their second compilation album, "Greatest Hits Vol. 2", which featured a brand new track: "Gimme! Gimme! Gimme! (A Man After Midnight)", another number 3 hit in both the UK and Germany. In Russia during the late 1970s, the group was paid in oil commodities because of an embargo on the ruble.
On 13 September 1979, ABBA began their at the Northlands Coliseum in Edmonton, Canada, with a full house of 14,000. "The voices of the band, Agnetha's high sauciness combined with round, rich lower tones of Anni-Frid, were excellent...Technically perfect, melodically correct and always in perfect pitch...The soft lower voice of Anni-Frid and the high, edgy vocals of Agnetha were stunning", raved "Edmonton Journal".
During the next four weeks they played a total of 17 sold-out dates, 13 in the United States and four in Canada. The last scheduled ABBA concert in the United States in Washington, D.C. was cancelled due to Fältskog's emotional distress suffered during the flight from New York to Boston, when the group's private plane was subjected to extreme weather conditions and was unable to land for an extended period. They appeared at the Boston Music Hall for the performance 90 minutes late. The tour ended with a show in Toronto, Canada at Maple Leaf Gardens before a capacity crowd of 18,000. "ABBA plays with surprising power and volume; but although they are loud, they're also clear, which does justice to the signature vocal sound...Anyone who's been waiting five years to see Abba will be well satisfied", wrote "Record World".
On 19 October 1979, the tour resumed in Western Europe where the band played 23 sold-out gigs, including six sold-out nights at London's Wembley Arena.
Progression.
In March 1980, ABBA travelled to Japan where upon their arrival at Narita International Airport, they were besieged by thousands of fans. The group played eleven concerts to full houses, including six shows at Tokyo's Budokan. This tour was the last "on the road" adventure of their career.
In the summer of 1980, the group released the single "The Winner Takes It All" the group's eighth UK chart topper (and their first since 1978). The song is widely misunderstood as being written about Ulvaeus and Fältskog's marital tribulations; Ulvaeus wrote the lyrics, but has stated they were not about his own divorce; Fältskog has repeatedly stated she was not the loser in their divorce. In the United States, the single peaked at number 8 on the Billboard Hot 100 chart and became ABBA's second Billboard Adult Contemporary number 1. It was also re-recorded by Andersson and Ulvaeus with a slightly different backing track, by French chanteuse Mireille Mathieu at the end of 1980 – as "Bravo Tu As Gagné", with French lyrics by Alain Boublil. November the same year saw the release of ABBA's seventh album "Super Trouper", which reflected a certain change in ABBA's style with more prominent use of synthesizers and increasingly personal lyrics. It set a record for the most pre-orders ever received for a UK album after one million copies were ordered before release. The 2nd single from the album, "Super Trouper", also hit number 1 in the UK, becoming the group's ninth and final UK chart-topper. Another track from the "Super Trouper" album, "Lay All Your Love on Me", released in 1981 as a 12-inch single only in selected territories, managed to top the Billboard Hot Dance Club Play chart and peaked at number 7 on the UK singles chart becoming, at the time, the highest ever charting 12-inch release in UK chart history.
Also in 1980, ABBA recorded a compilation of Spanish-language versions of their hits called "Gracias Por La Música". This was released in Spanish-speaking countries as well as in Japan and Australia. The album became a major success, and along with the Spanish version of "Chiquitita", this signalled the group's breakthrough in Latin America. "ABBA Oro: Grandes Éxitos", the Spanish equivalent of "ABBA Gold: Greatest Hits", was released in 1999.
Final album and performances (1981–1982).
In January 1981, Ulvaeus married Lena Källersjö, and manager Stig Anderson celebrated his 50th birthday with a party. For this occasion, ABBA recorded the track "Hovas Vittne" (a pun on the Swedish name for Jehovah's Witness and Anderson's birthplace, Hova) as a tribute to him, and released it only on 200 red vinyl copies, to be distributed to the guests attending the party. This single has become a sought-after collectible. In mid-February 1981, Andersson and Lyngstad announced they were filing for divorce. Information surfaced that their marriage had been an uphill struggle for years, and Benny had already met another woman, Mona Nörklit, whom he married in November 1981.
Andersson and Ulvaeus had songwriting sessions during the spring of 1981, and recording sessions began in mid-March. At the end of April, the group recorded a TV special, "Dick Cavett Meets ABBA" with the US talk show host Dick Cavett. "The Visitors", ABBA's eighth and final studio album, showed a songwriting maturity and depth of feeling distinctly lacking from their earlier recordings but still placing the band squarely in the pop genre, with catchy tunes and harmonies. Although not revealed at the time of its release, the album's title track, according to Ulvaeus, refers to the secret meetings held against the approval of totalitarian governments in Soviet-dominated states, while other tracks address topics like failed relationships, the threat of war, ageing, and loss of innocence. The album's only major single release, "One of Us", proved to be the last of ABBA's nine number 1 singles in Germany in December 1981; and the swansong of their sixteen Top 5 singles on the South African chart. "One of Us" was also ABBA's final Top 10 hit in the UK.
Although it topped the album charts across most of Europe, including the UK and Germany, "The Visitors" was not as commercially successful as its predecessors, showing a commercial decline in previously loyal markets such as France, Australia and Japan. A track from the album, "When All Is Said and Done", was released as a single in North America, Australia and New Zealand, and fittingly became ABBA's final Top 40 hit in the US (debuting on the US charts on 31 December 1981), while also reaching the US Adult Contemporary Top 10, and number 4 on the RPM Adult Contemporary chart in Canada. The song's lyrics, as with "The Winner Takes It All" and "One of Us", dealt with the painful experience of separating from a long-term partner, though it looked at the trauma more optimistically. With the now publicised story of Andersson and Lyngstad's divorce, speculation increased of tension within the band. Also released in the United States was the title track of "The Visitors", which hit the Top Ten on the Billboard Hot Dance Club Play chart.
Final recording sessions.
In the spring of 1982, songwriting sessions had started and the group came together for more recordings. Plans were not completely clear, but a new album was discussed and the prospect of a small tour suggested. The recording sessions in May and June 1982 were a struggle, and only three songs were eventually recorded: "You Owe Me One", "I Am the City" and "Just Like That". Andersson and Ulvaeus were not satisfied with the outcome, so the tapes were shelved and the group took a break for the summer.
Back in the studio again in early August, the group had changed plans for the rest of the year: they settled for a Christmas release of a double album compilation of all their past single releases to be named "". New songwriting and recording sessions took place, and during October and December, they released the singles "The Day Before You Came"/"Cassandra" and "Under Attack"/"You Owe Me One", the A-sides of which were included on the compilation album. Neither single made the Top 20 in the United Kingdom, though "The Day Before You Came" became a Top 5 hit in many European countries such as Germany, the Netherlands and Belgium. The album went to number 1 in the UK and Belgium, Top 5 in the Netherlands and Germany and Top 20 in many other countries. "Under Attack", the group's final release before disbanding, was a Top 5 hit in the Netherlands and Belgium.
"I Am the City" and "Just Like That" were left unreleased on "The Singles: The First Ten Years" for possible inclusion on the next projected studio album, though this never came to fruition. "I Am the City" was eventually released on the compilation album "" in 1993, while "Just Like That" has been recycled in new songs with other artists produced by Andersson and Ulvaeus. A reworked version of the verses ended up in the musical "Chess". The chorus section of "Just Like That" was eventually released on a retrospective box set in 1994. Despite a number of requests from fans, Ulvaeus and Andersson are still refusing to release ABBA's version of "Just Like That" in its entirety, even though the complete version surfaced on bootlegs.
The group travelled to London to promote "The Singles: The First Ten Years" in the first week of November 1982, appearing on "Saturday Superstore" and "The Late, Late Breakfast Show", and also to West Germany in the second week, to perform on Show Express. On 19 November 1982, ABBA appeared for the last time in Sweden on the TV programme Nöjesmaskinen, and on 11 December 1982, they made their last performance ever, transmitted to the UK on Noel Edmonds' "The Late, Late Breakfast Show", through a live link from a TV studio in Stockholm.
Final performances.
Andersson and Ulvaeus began collaborating with Tim Rice in early 1983 on writing songs for the musical project "Chess", while Fältskog and Lyngstad both concentrated on international solo careers. While Andersson and Ulvaeus were working on the musical, a further co-operation among the three of them came with the musical "Abbacadabra" that was produced in France for television. It was a children's musical utilising 14 ABBA songs. Alain and Daniel Boublil, who wrote "Les Misérables", had been in touch with Stig Anderson about the project, and the TV musical was aired over Christmas on French TV and later a Dutch version was also broadcast. Boublil previously also wrote the French lyric for Mireille Mathieu's version of "The Winner Takes It All".
Lyngstad, who had recently moved to Paris, participated in the French version, and recorded a single, "Belle", a duet with French singer Daniel Balavoine. The song was a cover of ABBA's 1976 instrumental track "Arrival". As the single "Belle" sold well in France, Cameron Mackintosh wanted to stage an English-language version of the show in London, with the French lyrics translated by David Wood and Don Black; Andersson and Ulvaeus got involved in the project, and contributed with one new song, "The Seeker". "Abbacadabra" premièred on 8 December 1983 at The Lyric Hammersmith Theatre in London, to mixed reviews and full houses for eight weeks, closing on 21 January 1984. Lyngstad was also involved in this production, recording "Belle" in English as "Time", a duet with actor and singer B. A. Robertson: the single sold well, and was produced and recorded by Andersson and Ulvaeus.
Anni-Frid lyngstad performed "I Have A Dream" with a children's choir on French television in 1984 -solo-.
All four members made their final public appearance, as four friends more than as ABBA, in January 1986, when they recorded a video of themselves performing an acoustic version of "Tivedshambo", which was the first song written by their manager, Stig Anderson, for a Swedish TV show honouring Anderson on his 55th birthday. The four had not seen each other for more than two years. That same year they also performed privately at another friend's 40th birthday: their old tour manager, Claes af Geijerstam. They sang a self-written song titled "Der Kleine Franz" that was later to resurface in "Chess". Also in 1986, "ABBA Live" was released, featuring selections of live performances from the group's 1977 and 1979 tours. The four members were guests at the 50th birthday of Görel Hanser in 1999. Hanser was a long-time friend of all four, and also former secretary of Stig Anderson. Honouring Görel, ABBA performed a Swedish birthday song "Med En Enkel Tulipan" a cappella.
Benny Andersson has on several occasions performed old ABBA songs. In June 1992, he and Ulvaeus appeared with U2 at a Stockholm concert, singing the chorus of "Dancing Queen", and a few years later during the final performance of the B & B in Concert in Stockholm, Andersson joined the cast for an encore at the piano. Andersson frequently adds an ABBA song to the playlist when he performs with his BAO band. He also played the piano during new recordings of the ABBA songs "Like an Angel Passing Through My Room" with opera singer Anne Sofie von Otter, and "When All Is Said and Done" with Swede Viktoria Tolstoy. In 2002, Andersson and Ulvaeus both performed an a cappella rendition of the first verse of "Fernando" as they accepted their Ivor Novello award in London. Lyngstad performed and recorded an a cappella version of "Dancing Queen" with the Swedish group The Real Group in 1993, and has also re-recorded "I Have a Dream" with Swiss singer Dan Daniell in 2003.
Breaking up.
ABBA has never officially announced the end of the group, but it has long been considered dissolved. Their final public performance together as ABBA was on the British TV programme "The Late, Late Breakfast Show" (live from Stockholm) on 11 December 1982. In January 1983, Fältskog started recording sessions for a solo album, as Lyngstad had successfully released her album "Something's Going On" some months earlier. Ulvaeus and Andersson, meanwhile, started songwriting sessions for the musical "Chess". In interviews at the time, Björn and Benny denied the split of ABBA ("Who are we without our ladies? Initials of Brigitte Bardot?"), and Lyngstad and Fältskog kept claiming in interviews that ABBA would come together for a new album repeatedly during 1983 and 1984. Internal strife between the group and their manager escalated and the band members sold their shares in Polar Music during 1983. Except for a TV appearance in 1986, the foursome did not come together publicly again until they were reunited at the Swedish premiere of the "Mamma Mia!" movie on 4 July 2008.
In an interview with the "Sunday Telegraph", following the premiere, Ulvaeus and Andersson confirmed that there was nothing that could entice them back on stage again. Ulvaeus said: "We will never appear on stage again. [...] There is simply no motivation to re-group. Money is not a factor and we would like people to remember us as we were. Young, exuberant, full of energy and ambition. I remember Robert Plant saying Led Zeppelin were a cover band now because they cover all their own stuff. I think that hit the nail on the head."
However, on 3 January 2011, Fältskog, who has been long considered to be the most reclusive member of the group and possibly also the major obstacle to any reunion, raised the possibility of reuniting for a one-off engagement. She admitted that she has not yet brought the idea up to the other three members. In April 2013, she reiterated her hopes for reunion during an interview with "Die Zeit", stating: "If they ask me, I'll say yes."
In a May 2013 interview, Fältskog, aged 63 at the time, confirmed that an Abba reunion will never eventuate: "I think we have to accept that it will not happen, because we are too old and each one of us has their own life. Too many years have gone by since we stopped, and there’s really no meaning in putting us together again." Fältskog further explained that the band members remained on amenable terms: "It’s always nice to see each other now and then and to talk a little and to be a little nostalgic." In an April 2014 interview, Fältskog, when asked about whether the band might reunite for a new recording said "It's difficult to talk about this because then all the news stories will be: 'Abba is going to record another song!' But as long as we can sing and play, then why not? I would love to, but it's up to Björn and Benny."
After ABBA.
Benny Andersson and Björn Ulvaeus.
In October 1984, Ulvaeus and Andersson together with lyricist Tim Rice released the musical concept double album "Chess". The singles "One Night in Bangkok" (with vocals by Murray Head and Anders Glenmark ) and "I Know Him So Well" (a duet by Barbara Dickson and Elaine Paige, and later also recorded by both Barbra Streisand and Whitney Houston) were both hugely successful. The former reached number 1 in Australia, Germany, Spain and Switzerland; number 2 in Austria, France and New Zealand; number 3 in Canada, Norway, Sweden and the US, as well as reaching the top 10 in a few other countries. In May 1986, the musical premièred in London's West End, and ran for almost three years. "Chess" also opened on Broadway in April 1988, but closed within two months due to bad reviews. In Stockholm, the composers staged "Chess på svenska" ("Chess in Swedish") in 2003, with some new material, including the musical numbers "Han är en man, han är ett barn" ("He's a Man, He's a Child") and "Glöm mig om du kan" ("Forget Me If You Can"). In 2008, the musical was again revived for a successful staging at London's Royal Albert Hall which was subsequently released on DVD, and then in two successful separate touring productions in the United States and United Kingdom, in 2010.
Andersson and Ulvaeus' next project, "Kristina från Duvemåla", an epic Swedish musical, premiered in Malmö, in southern Sweden in October 1995. The musical ran for five years in Stockholm, and an English version has been in development for some considerable time. It has been reported that a Broadway production is in its earliest stages of pre-production. In the meantime, following some earlier workshops, a full presentation of the English translation of the musical in concert, now with the shortened name of "Kristina", took place to capacity crowds in September 2009 at New York's Carnegie Hall, and in April 2010 at London's Royal Albert Hall, followed by a CD release of the New York recordings.
Since 1983, besides "Chess" and "Kristina från Duvemåla", Benny Andersson has continued writing songs with Ulvaeus. The pair produced two English-language pop albums with Swedish duo Gemini in 1985 and 1987. In 1987, Andersson also released his first solo album on his own label, Mono Music, called "Klinga mina klockor" ("Ring My Bells"), all new material inspired by Swedish folk music – and followed it with his second album titled "November 1989".
In the 1990s, Andersson wrote music for the popular Swedish cabaret quartet Ainbusk Singers, giving them two hits: "Lassie" and "Älska mig" ("Love me"), and later produced "Shapes", an English-language album by the group's Josefin Nilsson with all-new material by Andersson and Ulvaeus. Andersson has also regularly written music for films (most notably to Roy Andersson's "Songs from the Second Floor"). In 2001, Andersson formed his own band, Benny Anderssons Orkester (BAO), which released three successful albums in 2001, 2004 and 2007 respectively. Andersson has the distinction of remaining the longest in the Swedish Radio Svensktoppen charts; the song "Du är min man" ("You Are My Man"), sung by Helen Sjöholm, spent 278 weeks there between 2004 and 2009. Andersson released his third album BAO 3 in October 2007, of new material with his band BAO and vocalists Helen Sjöholm and Tommy Körberg, as well as playing to full houses at two of Sweden's largest concert venues in October and November 2007, with an audience of 14,000.
 Ulvaeus has not appeared on stage performing music since ABBA, but had a reunion with his co-members of the Hootenanny Singers on 16 July 2005 at a music festival in his hometown of Västervik, singing their 1966 hit "Marianne".
Andersson and Ulvaeus have been highly involved in the worldwide productions of the musical "Mamma Mia!", alongside Lyngstad who attends premieres. They were also involved in the production of the successful film version of the musical, which opened in July 2008. Andersson produced the soundtrack utilising many of the musicians ABBA used on their albums and tours. Andersson made a cameo appearance in the movie as a 'fisherman' piano player in the "Dancing Queen" scene, while Ulvaeus is seen as a Greek god playing a lyre during the closing credits.
Andersson and Ulvaeus have continuously been writing new material; most recently the two wrote 7 songs for Anderssons 'BAO' 2011 album 'O Klang Och Jubeltid', performed as usual by vocalists Sjöholm, Körberg and Moreus. In July 2009, 'BAO' released their first international release, now named "The Benny Andersson Band", with the album "The Story of a Heart". The album was a compilation of 14 tracks from Andersson's five Swedish-language releases between 1987 and 2007, including five songs now recorded with lyrics by Ulvaeus in English, and the new title song premiered on BBC2's "Ken Bruce Show". A Swedish-language version of the title track, "Sommaren Du Fick" ("The Summer You Got"), was released as a single in Sweden prior to the English version, with vocals by Helen Sjöholm.
In the spring of 2009, Andersson also released a single recorded by the staff at his privately owned Stockholm hotel "Hotel Rival", titled "2nd Best to None", accompanied by a video showing the staff at work. In 2008, Andersson and Ulvaeus wrote a song for Swedish singer Sissela Kyle, titled "Jag vill bli gammal" ("I Wanna Grow Old"), for her Stockholm stage show "Your Days Are Numbered", which was never recorded and released, but did get a TV performance. Ulvaeus also contributed lyrics to ABBA's 1976 instrumental track "Arrival" for Sarah Brightman's cover version recorded for her 2008 album "Winter Symphony". New English lyrics have also been written for Andersson's 1999 song "Innan Gryningen" (then also named "Millennium Hymn"), with the new title "The Silence of the Dawn" for Barbara Dickson (performed live, but not yet recorded and released). In 2007, they wrote the new song "Han som har vunnit allt" ("He Who's Won It All") for actor/singer Anders Ekborg. Björn wrote English lyrics for two older songs from Benny's solo albums: "I Walk with You Mama" ("Stockholm by Night", 1989) and "After the Rain" ("Efter regnet", 1987) for opera singer Anne Sofie von Otter, for her Andersson tribute album "I Let the Music Speak". Barbara Dickson recorded (but not yet released) a Björn & Benny song called 'The Day The Wall Came Tumbling Down'; the song eventually was released by Australian 'Mamma Mia!' musical star Anne Wood 201 album of ABBA covers, Divine Discontent. As of October 2012, Björn Ulvaeus has mentioned writing new material with Benny for a 'BAO' Christmas release (also mentioned as a BAO 'box'), and Benny is busy writing music for a Swedish language obscure musical, 'Hjälp Sökes' ('Help is Wanted') together with Kristina Lugn and Lars Rudolfsson, premiering 8 February 2013.
Andersson has also written music for a documentary film about Olof Palme, re-recording the track 'Sorgmarch' from his last album throughout the film.
Agnetha Fältskog and Anni-Frid Lyngstad.
Both female members of ABBA pursued solo careers on the international scene after their work with the group. In 1982, Lyngstad chose Genesis drummer and vocalist Phil Collins to produce the album "Something's Going On" and unveiled the hit single and video "I Know There's Something Going On" in the autumn of that year. The single became a number 1 hit in France (where it spent five weeks at the top), Belgium, Switzerland and Costa Rica. The track reached number 3 in Austria, the Netherlands, Norway, Sweden and Poland, and was also a Top 10 hit in Germany, Italy, Finland, South Africa and Australia. Sveriges Television documented this historical event, by filming the whole recording process. The result became a one-hour TV documentary, including interviews with Lyngstad, Collins, Ulvaeus and Andersson as well as all the musicians. This documentary and the promotion videos from the album are included in "Frida - The DVD".
Lyngstad's second solo album after ABBA was called "Shine", produced by Steve Lillywhite. "Shine" was recorded in Paris and released in 1984."Shine" was Lyngstad's final studio album release for twelve years. It featured "Slowly", the last known Andersson-Ulvaeus composition to have been recorded by one of the former female ABBA vocalists to date. The promotion videos and clips for "Shine" are included in "Frida – The DVD".
In 1980, Agnetha Fältskog recorded Nu tändas tusen juleljus (Now a thousand Christmas candles are lit), a Swedish Christmas album along with her 7 year old daughter Linda. The album was released in 1981. Nu tändas tusen julejus, which was Fältskog's first Swedish language recording for the Polar Music label after having left CBS-Cupol, peaked at No. 6 on the Swedish album chart in January 1982,[2] has been re-released on CD by Polar Music/PolyGram/Universal Music all through the 1990s and 2000s and is one of the best-selling Swedish Christmas albums of all time. The album name is derived from one of Scandinavia's best-known Christmas carols.
In 1983, Fältskog released the solo album "Wrap Your Arms Around Me" which achieved platinum sales in Sweden. This included the single "The Heat Is On", which was a hit all over Europe and Scandinavia. It reached number one in Sweden and Norway and number two in the Netherlands and Belgium. In the United States, Fältskog earned a Billboard Top 30 hit with "Can't Shake Loose". In Europe, the single "Wrap Your Arms Around Me" was another successful hit, topping the charts in Belgium and Denmark, reaching the Top 5 in Sweden, the Netherlands and South Africa, and the Top 20 in Germany and France. The album sold 1.2 million copies worldwide. The album was produced by the highly successful producer and songwriter Mike Chapman, also known for his work with The Sweet, Mud, Suzi Quatro, Blondie, Pat Benatar and The Knack.
"It's So Nice to be Rich" was Agnetha's fourth top ten hit in Sweden in 1983. Her duet with Tomas Ledin, "Never Again" was the first one.
Fältskog's second English-language solo album, "Eyes of a Woman", was released in March 1985, peaking at number 2 in Sweden and another platinum seller and performing reasonably well in Europe. The album was produced by Eric Stewart of 10cc. The first single from the album was her self-penned "I Won't Let You Go". Agnetha's duet with Ola Håkansson "The Way You Are" was a number one hit in Sweden in 1986 and was awarded double platinum.
In early 1987, Agnetha recorded an album "Kom följ med I vår karusell" ('Come ride with me on my carousell') with her son Christian. The album contained songs for children and was sung in Swedish. For the album Agnetha recorded duets with her son and with a choir of children. She also recorded a few solo songs. The production was modern and fresh. The single 'Pa Sondag' was much played at the radio and even made the Swedish top 10, unique for a song made for kids to enjoy.
Also in November 1987, Fältskog released her third post-ABBA solo album, the Peter Cetera-produced "I Stand Alone", which also included the Billboard Adult Contemporary duet with Cetera, "I Wasn't the One (Who Said Goodbye)", as well as the European charting singles "The Last Time" and "Let It Shine". The album was extremely successful in Sweden, where it spent eight weeks at number 1 and was awarded double-platinum. Shortly after some minor European promotion for the album in early 1988, Fältskog withdrew from public life and halted her music career. In 1996, she released her autobiography, "As I Am", and a compilation album featuring her solo hits alongside some ABBA classics.
In 2004, she made a successful comeback, releasing the critically acclaimed album "My Colouring Book", which debuted at number 1 in Sweden (achieving triple-platinum status), number 6 in Germany, and number 12 in the UK, winning a silver award, and achieving gold status in Finland. The single "If I Thought You'd Ever Change Your Mind" (a cover of the Cilla Black 1960s song) became Fältskog's biggest solo hit in the United Kingdom, reaching number 11. The single peaked at number 2 in Sweden and was a hit throughout Scandinavia and Europe. A further single, "When You Walk in the Room", was released but met with less success, only peaking at number 34 in the United Kingdom. In January 2007, she sang a live duet on stage with Swedish singer Tommy Körberg at the after party for the final performance of the musical, "Mamma Mia!", in Stockholm, at which Benny Andersson and Björn Ulvaeus were also present.
In 1992, Lyngstad had been asked and chosen to be the chairperson for the environmental organisation "Artister för miljön" (Artists for the Environment) in Sweden. She became chairwoman for this organisation from 1992 to 1995. To mark her interests for the environment, she recorded the Julian Lennon song "Saltwater" and performed it live in Stockholm. She arranged and financed summer camps for poor children in Sweden, focusing on environmental and ecological issues. Her environmental work for this organisation led up to the decision to record again. The album "Djupa andetag" ("Deep Breaths") was released towards the end of 1996 and became a success in Sweden, where it reached number 1. The lyrics for the single from this album, "Även en blomma" ("Even a Flower"), deal with environmental issues. In 2004, Lyngstad recorded a song called "The Sun Will Shine Again", written especially for her and released with former Deep Purple member Jon Lord. The couple made several TV performances with this song in Germany. Lyngstad lives a relatively low-profile life but occasionally appears at a party or charity function. On 26 August 1992, she married Prince Heinrich Ruzzo Reuss von Plauen, of the German Reuss family. Von Plauen died of lymphoma in 1999 at the age of 49. In addition to losing her husband, Lyngstad had also lost her daughter Lise-Lotte in a car crash a year earlier.
On 15 November 2005, Lyngstad's 60th birthday, Universal released the "Frida Box Set", consisting of the solo albums she recorded for the Polar Label. Also included is the 3-hour documentary "Frida – The DVD". On this DVD, which covers her entire singing career, the viewer is guided by Lyngstad herself through the years from her TV debut in Sweden in 1967 to the TV performances she made in Germany in 2004. Many rare clips are included in the set and each performance is explained by Lyngstad herself. The interview with Lyngstad was filmed in the Swiss Alps in summer 2005.
Lyngstad returned to the recording studio in 2010 to record vocals for the Cat Stevens song "Morning Has Broken", for Swedish guitarist Georg Wadenius's October 2010 album "Reconnections". The album, which featured other guest vocalists, reached number 17 in the Swedish charts.
In May 2013, Fältskog released a solo album entitled "A" through the Verve music label. In a promotional interview, Fältskog explained that the album was unplanned and it was after she heard the first three songs that she felt that she "had to do this [record the album]". She also revealed that she completed singing lessons prior to recording "A", as she felt a "a bit rusty" in her throat. Fältskog stated that she would not be undertaking any tours or live performances in support of the album, explaining: "I'm not that young anymore. I don’t have the energy to do that, and also I don’t want to travel too much." The title of the album was conceived of by the studio production team.
In 2004 she recorded an album of 1960s covers who had the most impact on her teenage years as a music contender. "A" has been very successful, earning her 4 Gold Records in UK where it peaked at # 6, Australia, Germany and Sweden. In both UK and Australia it was in the top 100 albums of 2013.
Revival.
The same year the members of ABBA went their separate ways, the French production of a "tribute" show (a children's TV musical named "Abbacadabra" using 14 ABBA songs) spawned new interest in the group's music.
After receiving little attention during the mid-to-late-1980s, ABBA's music experienced a resurgence in the early 1990s due to the UK synth-pop duo Erasure, who released a cover extended play featuring versions of ABBA songs which topped the charts in 1992. As U2 arrived in Stockholm for a concert in June of that year, the band paid homage to ABBA by inviting Björn Ulvaeus and Benny Andersson to join them on stage for a rendition of "Dancing Queen", playing guitar and keyboards. September 1992 saw the release of "", a new compilation album. The single "Dancing Queen" received radio airplay in the UK in summer 1992 to promote the album. The song returned to the Top 20 of the UK singles chart in August that year, this time peaking at number 16.
The enormous interest in the "ABBA Gold: Greatest Hits" compilation saw the release of "" in 1993.
In 1994, two Australian cult films caught the attention of the world's media, both focusing on admiration for ABBA: "The Adventures of Priscilla, Queen of the Desert" and "Muriel's Wedding". The same year, "Thank You for the Music", a four-disc box set comprising all the group's hits and stand-out album tracks, was released with the involvement of all four members. "By the end of the twentieth century", American critic Chuck Klosterman wrote a decade later, "it was far more contrarian to hate ABBA than to love them."
ABBA were soon recognised and embraced by other acts: Evan Dando of The Lemonheads recorded a cover version of "Knowing Me, Knowing You"; Sinéad O'Connor and Boyzone's Stephen Gately have recorded "Chiquitita"; Tanita Tikaram, Blancmange and Steven Wilson paid tribute to "The Day Before You Came". Cliff Richard covered "Lay All Your Love on Me", while Dionne Warwick, Peter Cetera, and Celebrity Skin recorded their versions of "SOS". U.S. alternative-rock musician Marshall Crenshaw has also been known to play a version of "Knowing Me, Knowing You" in concert appearances, while legendary English Latin pop songwriter Richard Daniel Roman has recognized ABBA as a major influence. Swedish metal guitarist Yngwie Malmsteen covered "Gimme! Gimme! Gimme! (A Man After Midnight)" with slightly altered lyrics.
Two different compilation albums of ABBA songs have been released. "ABBA: A Tribute" coincided with the 25th anniversary celebration and featured 17 songs, some of which were recorded especially for this release. Notable tracks include Go West's "One of Us", Army of Lovers "Hasta Mañana", Information Society's "Lay All Your Love on Me", Erasure's "Take a Chance on Me" (with MC Kinky), and Lyngstad's a cappella duet with The Real Group of "Dancing Queen". A second 12-track album was released in 1999, entitled "ABBAMANIA", with proceeds going to the Youth Music charity in England. It featured all new cover versions: notable tracks were by Madness ("Money, Money, Money"), Culture Club ("Voulez-Vous"), The Corrs ("The Winner Takes It All"), Steps ("Lay All Your Love on Me", "I Know Him So Well"), and a medley entitled "Thank ABBA for the Music" performed by several artists and as featured on the Brits Awards that same year.
In 1997, an ABBA tribute group was formed, the ABBA Teens, which was subsequently renamed the A-Teens to allow the group some independence. The group's first album, "The ABBA Generation", consisting solely of ABBA covers reimagined as 1990s pop songs, was a worldwide success and so were subsequent albums. The group disbanded in 2004 due to a grueling schedule and intentions to go solo.
In Sweden, the growing recognition of the legacy of Andersson and Ulvaeus resulted in the 1998 "B & B Concerts": a tribute concert (with Swedish singers who had worked with the songwriters through the years) showcasing not only their ABBA years, but hits both before and after ABBA. The concert was a success, and was ultimately released on CD. It later toured Scandinavia and even went to Beijing in the People's Republic of China for two concerts.
In 2000, ABBA was reported to have turned down an offer of approximately US$1,000,000,000 (one billion US dollars) to do a reunion tour consisting of 100 concerts.
For the 2004 semi-final of the Eurovision Song Contest, staged in Istanbul 30 years after ABBA had won the contest in Brighton, all four members made cameo appearances in a special comedy video made for the interval act, entitled "Our Last Video Ever". Others well-known stars such as Rik Mayall, Cher and Iron Maiden's Eddie also made appearances in the video. It was not included in the official DVD release of the Eurovision Contest, but was issued as a separate DVD release, retitled "The Last Video" at the request of the former ABBA members.
In 2005, all four members of ABBA appeared at the Stockholm premiere of the musical "Mamma Mia!".
On 22 October 2005, at the 50th anniversary celebration of the Eurovision Song Contest, "Waterloo" was chosen as the best song in the competition's history.
On 4 July 2008, all four ABBA members were reunited at the Swedish premiere of the film "Mamma Mia!". It was only the second time all of them had appeared together in public since 1986. During the appearance, they re-emphasized that they intended never to officially reunite, citing the opinion of Robert Plant that the re-formed Led Zeppelin was more like a cover band of itself than the original band. Ulvaeus stated that he wanted the band to be remembered as they were during the peak years of their success.
The compilation album "", originally released in 1992, returned to number one in the UK album charts for the fifth time on 3 August 2008. On 14 August 2008, the "Mamma Mia! The Movie" film soundtrack went to number 1 on the US Billboard charts, ABBA's first US chart-topping album. During the band's heyday the highest album chart position they had ever achieved in America was number 14.
In November 2008, all eight studio albums, together with a ninth of rare tracks, was released as "The Albums". It hit several charts, peaking at number 4 in Sweden and reaching the Top 10 in several other European territories.
In 2008, Sony Computer Entertainment Europe, in collaboration with Universal Music Group Sweden AB, released "SingStar ABBA" on both the PlayStation 2 and PlayStation 3 games consoles, as part of the SingStar music video games. The PS2 version features 20 ABBA songs, while 25 songs feature on the PS3 version.
On 22 January 2009, Fältskog and Lyngstad appeared together on stage to receive the Swedish music award "Rockbjörnen" (for "lifetime achievement"). In an interview, the two women expressed their gratitude for the honorary award and thanked their fans.
On 25 November 2009, PRS for Music announced that the British public voted ABBA as the band they would most like to see re-form.
On 27 January 2010, ABBAWORLD, a 25-room touring exhibition featuring interactive and audiovisual activities, debuted at Earls Court Exhibition Centre in London. According to the exhibition's website, ABBAWORLD is "approved and fully supported" by the band members.
"Mamma Mia" was released as one of the first few non-premium song selections for the online RPG game "Bandmaster". On 17 May 2011, "Gimme! Gimme! Gimme!" was added as a non-premium song selection for the Bandmaster Philippines server. On 15 November 2011, Ubisoft released a dancing game called "" for the Wii.
In January 2012, Universal Music announced the re-release of ABBA's final album "The Visitors", featuring a previously unheard track "From a Twinkling Star to a Passing Angel".
A book entitled "Abba: The Official Photo Book" was published in early 2014 to mark the 40-year anniversary of the band's Eurovision victory. The book reveals that part of the reason for the band's outrageous costumes were the Swedish tax laws at the time that allowed the cost of brazen outfits that were not suitable for public display to be deducted against tax.
Recording process.
Abba were perfectionists in the studio and would work on tracks tirelessly until they got them right, rather than leaving them and coming back to them later.<ref name=abba/ref>ABBA – In Their Own Words, compiled by Rosemary York, 1981, pp 57–65. Omnibus Press ISBN 0-86001-950-0</ref>
The band would create the basic rhythm track with a drummer, guitarist and bass player. All the other arrangements – vocals, other instruments – would be overlaid onto this basic track. The vocals would then be added, and orchestra overdubs were usually left till last.
The women of the band would contribute ideas at the studio stage. Benny and Björn would play them the backing tracks and they would make comments and suggestions. According to Agnetha, the women had the final say in how the lyrics were shaped. Frida says: "When we gather around the piano to get our voices tuned up, we often come up with things we can use in the backing vocals."
After all the vocals and overdubs had been done, the band would take up to five days to mix a song.
Success in the United States.
During their active career, from 1972 to 1982, ABBA placed twenty singles on the "Billboard" Hot 100 fourteen of which made the top 40 (13 on the Cashbox Top 100) and ten of which made the Top 20 on both charts. A total of four of those singles reached the Top 10, including "Dancing Queen" which reached number 1 in April 1977.
While "Fernando" and "SOS" did not break the Top 10 on the Billboard Hot 100 chart, reaching #13 and #15 respectively, they did reach the Top 10 on Cashbox ("Fernando") and Record World ("SOS") charts. Both "Dancing Queen" and "Take A Chance On Me" were certified gold by the Recording Industry Association of America for sales of over one million copies each.
The group also had 12 Top 20 singles on the Billboard Adult Contemporary chart with two of them, "Fernando" and "The Winner Takes It All", reaching number 1. "Lay All Your Love on Me" was ABBA's fourth number 1 single on a Billboard chart, topping the Hot Dance Club Play chart. The singles "Dancing Queen" and "Take a Chance on Me" were certified gold (more than 1 million copies sold) by the RIAA.
Nine ABBA albums made their way into the top half of the "Billboard" 200 album chart, with seven of them reaching the Top 50 and four reaching the Top 20. "ABBA: The Album" was the highest-charting album of the group's career, peaking at No. 14. Five albums received RIAA gold certification (more than 500,000 copies sold), while three acquired platinum status (selling more than one million copies). In 1993, the "ABBA Gold: Greatest Hits" collection was released in the United States and has since become a seven-time platinum best-seller; it climbed to #1 on the Billboard Top Pop Catalog Albums chart and also peaked at number 11 on Billboard Comprehensive Albums chart.
On 15 March 2010, ABBA was inducted into the Rock and Roll Hall of Fame by Bee Gees members Barry Gibb and Robin Gibb. The ceremony was held at The Waldorf Astoria Hotel in New York City. The group was represented by Anni-Frid Lyngstad and Benny Andersson.
Fashion, style, videos, advertising campaigns.
ABBA was widely noted for the colourful and trend-setting costumes its members wore. The reason for the wild costumes was Swedish tax law. The clothes could be deductible only if they could not be worn other than for performances. Choreography by Graham Tainton also contributed to their performance style.
The videos that accompanied some of the band's biggest hits are often cited as being among the earliest examples of the genre. Most of ABBA's videos (and "ABBA: The Movie") were directed by Lasse Hallström, who would later direct the films "My Life as a Dog", "The Cider House Rules" and "Chocolat".
ABBA made videos because their songs were hits in many different countries and personal appearances were not always possible. This was also done in an effort to minimize travelling, particularly to countries that would have required extremely long flights. Fältskog and Ulvaeus had two young children and Fältskog, who was also afraid of flying, was very reluctant to leave her children for such a long time. ABBA's manager, Stig Anderson, realized the potential of showing a simple video clip on television to publicize a single or album, thereby allowing easier and quicker exposure than a concert tour. Some of these videos became classics because of the 1970s-era costumes and early video effects, such as the grouping of the band members in different combinations of pairs, overlapping one singer's profile with the other's full face, and the contrasting of one member against another.
In 1976, ABBA participated in a high-profile advertising campaign by the Matsushita Electric Industrial (today's Panasonic), which was designed to promote the brand National. This campaign was designed initially for Australia, where "National" was still the primary brand used by Matsushita, who had not introduced the "Panasonic" brand to Australia yet despite its widespread use in other parts of the world such as the United States. However, the campaign was also aired in Japan. Five commercials, each approximately one minute long, were produced, each using the "National Song" sung by ABBA, which used the melody and instrumental arrangement of "Fernando", adapted with new lyrics promoting National, and working in several slogans used by National in their advertising.
Political controversy.
In September 2010, band members Andersson and Ulvaeus criticized the right-wing Danish People's Party (DF) for using the ABBA song "Mamma Mia" (with modified lyrics) at rallies. The band threatened to file a lawsuit against the DF, saying they never allowed their music to be used politically and that they had absolutely no interest in supporting the party. Their record label Universal Music later said that no legal action would be taken because an agreement had been reached.
References.
Notes
Bibliography
Further reading

</doc>
<doc id="881" url="http://en.wikipedia.org/wiki?curid=881" title="Allegiance">
Allegiance

An allegiance is a duty of fidelity said to be owed by a subject or a citizen to his/her state or sovereign.
Etymology.
From Middle English "ligeaunce" (see medieval Latin "ligeantia", "a liegance"). The "al-" prefix was probably added through confusion with another legal term, "allegeance", an "allegation" (the French "allegeance" comes from the English). "Allegiance" is formed from "liege," from Old French "liege", "liege, free", of Germanic origin. The connection with Latin "ligare", "to bind," is erroneous.
Usage.
The term "allegiance" was traditionally often used by English legal commentators in a larger sense, divided by them into natural and local, the latter applying to the deference which even a foreigner must pay to the institutions of the country in which he happens to live. However it is in its proper sense, in which it indicates national character and the subjection due to that character, that the word is more important.
In that sense it represents the feudal liege homage, which could be due only to one lord, while simple homage might be due to every lord under whom the person in question held land.
United Kingdom.
The English doctrine, which was at one time adopted in the United States, asserted that allegiance was indelible: "Nemo potest exuere patriam". Accordingly, as the law stood before 1870, every person who by birth or naturalisation satisfied the conditions set forth, though he should be removed in infancy to another country where his family resided, owed an allegiance to the British crown which he could never resign or lose, except by act of parliament or by the recognition of the independence or the cession of the portion of British territory in which he resided. 
This refusal to accept any renunciation of allegiance to the Crown led to conflict with the United States over impressment, and then led to further conflicts even during the War of 1812, when thirteen Irish American prisoners of war were executed as traitors after the Battle of Queenston Heights; Winfield Scott urged American reprisal, but none was carried out. 
Allegiance is the tie which binds the subject to the Sovereign in return for that protection which the Sovereign affords the subject. It was the mutual bond and obligation between monarch and subjects, whereby subjects are called his liege subjects, because they are bound to obey and serve him; and he is called their liege lord, because he should maintain and defend them ("Ex parte Anderson" (1861) 3 El & El 487; 121 ER 525; "China Navigation Co v Attorney-General" (1932) 48 TLR 375; "Attorney-General v Nissan" [1969] 1 All ER 629; "Oppenheimer v Cattermole" [1972] 3 All ER 1106). The duty of the Crown towards its subjects is to govern and protect. The reciprocal duty of the subject towards the Crown is that of allegiance. 
At common law allegiance is a true and faithful obedience of the subject due to his Sovereign. As the subject owes to his king his true and faithful allegiance and obedience, so the Sovereign 
Natural allegiance and obedience is an incident inseparable to every subject, for parte Anderson" (1861) 3 El & El 487; 121 ER 525). Natural-born subjects owe allegiance wherever they may be. Where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)). 
Allegiance is owed both to the Sovereign as a natural person and to the Sovereign in the political capacity ("Re Stepney Election Petition, Isaacson v Durant" (1886) 17 QBD 54 (per Lord Coleridge CJ)). Attachment to the person of the reigning Sovereign is not sufficient. Loyalty requires affection also to the office of the Sovereign, attachment to royalty, attachment to the law and to the constitution of the realm, and he who would, by force or by fraud, endeavour to prostrate that law and constitution, though he may retain his affection for its head, can boast but an imperfect and spurious species of loyalty ("R v O'Connell" (1844) 7 ILR 261). 
There were four kinds of allegiances ("Rittson v Stordy" (1855) 3 Sm & G 230; "De Geer v Stone" (1882) 22 Ch D 243; "Isaacson v Durant" (1886) 54 LT 684; "Gibson, Gavin v Gibson" [1913] 3 KB 379; "Joyce v DPP" [1946] AC 347; "Collingwood v Pace" (1661) O Bridg 410; "Lane v Bennett" (1836) 1 M & W 70; "Lyons Corp v East India Co" (1836) 1 Moo PCC 175; "Birtwhistle v Vardill" (1840) 7 Cl & Fin 895; "R v Lopez, R v Sattler" (1858) Dears & B 525; Ex p Brown (1864) 5 B & S 280); 
(a) "Ligeantia naturalis, absoluta, pura et indefinita", and this originally is due by nature and birthright, and is called "alta ligeantia", and those that owe this are called "subditus natus";
(b) "Ligeantia acquisita", not by nature but by acquisition or denization, being called a denizen, or rather denizon, because they are "subditus datus"; 
(c) "Ligeantia localis", by operation of law, when a friendly alien enters the country, because so long as they are in the country they are within the Sovereign's protection, therefore they owe the Sovereign a local obedience or allegiance ("R v Cowle" (1759) 2 Burr 834; "Low v Routledge" (1865) 1 Ch App 42; "Re Johnson, Roberts v Attorney-General" [1903] 1 Ch 821; "Tingley v Muller" [1917] 2 Ch 144; "Rodriguez v Speyer" [1919] AC 59; "Johnstone v Pedlar" [1921] 2 AC 262; "R v Tucker" (1694) Show Parl Cas 186; "R v Keyn" (1876) 2 Ex D 63; "Re Stepney Election Petn, Isaacson v Durant" (1886) 17 QBD 54);
(d) A legal obedience, where a particular law requires the taking of an oath of allegiance by subject or alien alike.
Natural allegiance was acquired by birth within the Sovereign's dominions (except for the issue of diplomats or of invading forces or of an alien in enemy occupied territory). The natural allegiance and obedience is an incident inseparable to every subject, for as soon as they are born they owe by birthright allegiance and obedience to the Sovereign ("Ex p. Anderson" (1861) 3 E & E 487). A natural-born subject owes allegiance wherever they may be, so that where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)). 
Acquired allegiance was acquired by naturalisation or denization. Denization, or "ligeantia acquisita", appears to be threefold ("Thomas v Sorrel" (1673) 3 Keb 143); 
Local allegiance was due by an alien while in the protection of the Crown. All friendly resident aliens incurred all the obligations of subjects ("The Angelique" (1801) 3 Ch Rob App 7). An alien, coming into a colony also became, temporarily a subject of the Crown, and acquired rights both within and beyond the colony, and these latter rights could not be affected by the laws of that colony ("Routledge v Low" (1868) LR 3 HL 100; 37 LJ Ch 454; 18 LT 874; 16 WR 1081, HL; "Reid v Maxwell" (1886) 2 TLR 790; "Falcon v Famous Players Film Co" [1926] 2 KB 474). 
A resident alien owed allegiance even when the protection of the Crown was withdrawn owing to the occupation of an enemy, because the absence of the Crown's protection was temporary and involuntary ("de Jager v Attorney-Geneneral of Natal" [1907] AC 326). 
Legal allegiance was due when an alien took an oath of allegiance required for a particular office under the Crown. 
By the Naturalisation Act 1870, it was made possible for British subjects to renounce their nationality and allegiance, and the ways in which that nationality is lost are defined. So British subjects voluntarily naturalized in a foreign state are deemed aliens from the time of such naturalization, unless, in the case of persons naturalized before the passing of the act, they have declared their desire to remain British subjects within two years from the passing of the act. Persons who from having been born within British territory are British subjects, but who at birth became under the law of any foreign state subjects of such state, and also persons who though born abroad are British subjects by reason of parentage, may by declarations of alienage get rid of British nationality. Emigration to an uncivilized country leaves British nationality unaffected: indeed the right claimed by all states to follow with their authority their subjects so emigrating is one of the usual and recognized means of colonial expansion.
United States.
The doctrine that no man can cast off his native allegiance without the consent of his sovereign was early abandoned in the United States, and Chief Justice John Rutledge also declared in Talbot v. Janson, "a man may, at the same time, enjoy the rights of citizenship under two governments." On July 27, 1868, the day before the Fourteenth Amendment was adopted, U.S. Congress declared in the preamble of the Expatriation Act that "the right of expatriation is a natural and inherent right of all people, indispensable to the enjoyment of the rights of life, liberty and the pursuit of happiness," and (Section I) one of "the fundamental principles of this government" (United States Revised Statutes, sec. 1999). Every natural-born citizen of a foreign state who is also an American citizen and every natural-born American citizen who is a citizen of a foreign land owes a double allegiance, one to the United States, and one to his homeland (in the event of an immigrant becoming a citizen of the US), or to his adopted land (in the event of an emigrant natural born citizen of the US becoming a citizen of another nation). If these allegiances come into conflict, he or she may be guilty of treason against one or both. If the demands of these two sovereigns upon his duty of allegiance come into conflict, those of the United States have the paramount authority in American law; likewise, those of the foreign land have paramount authority in their legal system. In such a situation, it may be incumbent on the individual to renounce one of his citizenships to avoid possibly being forced into situations where countervailing duties are required of him, such as might occur in the event of war.
Oath of allegiance.
The oath of allegiance is an oath of fidelity to the sovereign taken by all persons holding important public office and as a condition of naturalization. By ancient common law it might be required of all persons above the age of 12, and it was repeatedly used as a test for the disaffected. In England it was first imposed by statute in the reign of Elizabeth I of England (1558) and its form has more than once been altered since. Up to the time of the revolution the promise was, "to be true and faithful to the king and his heirs, and truth and faith to bear of life and limb and terrene honour, and not to know or hear of any ill or damage intended him without defending him therefrom." This was thought to favour the doctrine of absolute non-resistance, and accordingly the convention parliament enacted the form that has been in use since that time – "I do sincerely promise and swear that I will be faithful and bear true allegiance to His Majesty ..."
In Islam.
The word used in the Arabic language for allegiance is "bay'at" (Arabic: بيعة), which means "taking hand". The practice is sanctioned in the Qur'an by Surah 48:10: "Verily, those who give thee their allegiance, they give it but to Allah Himself". The word is used for the oath of allegiance to an emir. It is also used for the initiation ceremony specific to many Sufi orders.

</doc>
<doc id="885" url="http://en.wikipedia.org/wiki?curid=885" title="Altenberg">
Altenberg

Altenberg (German for "old mountain") may refer to:
People.
Any place called Altenberg may have given rise to Altenberg as a family name, such as:

</doc>
<doc id="887" url="http://en.wikipedia.org/wiki?curid=887" title="MessagePad">
MessagePad

The MessagePad is the first series of personal digital assistant devices developed by Apple Computer for the Newton platform in 1993. Some electronic engineering and the manufacture of Apple's MessagePad devices was undertaken in Japan by the Sharp Corporation. The devices were based on the ARM 610 RISC processor and all featured handwriting recognition software and were developed and marketed by Apple. The devices ran the Newton OS.
Details.
Screen and input.
With the MessagePad 120 with Newton OS 2.0, the Newton Keyboard by Apple became available, which can also be used via the dongle on Newton devices with a Newton InterConnect port, most notably the Apple MessagePad 2000/2100 series, as well as the Apple eMate 300.
Newton devices featuring Newton OS 2.1 or higher can be used with the screen turned horizontally ("landscape") as well as vertically ("portrait"). A change of a setting rotates the contents of the display by 90, 180 or 270 degrees. Handwriting recognition still works properly with the display rotated, although display calibration is needed when rotation in any direction is used for the first time or when the Newton device is reset.
Handwriting recognition.
In initial versions (Newton OS 1.x) the handwriting recognition gave extremely mixed results for users and was sometimes inaccurate. The original handwriting recognition engine was called Calligrapher, and was licensed from a Russian company called Paragraph International. Calligrapher's design was quite sophisticated; it attempted to learn the user's natural handwriting, using a database of known words to make guesses as to what the user was writing, and could interpret writing anywhere on the screen, whether hand-printed, in cursive, or a mix of the two. By contrast, Palm Pilot's Graffiti had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter shapes which resembled standard handwriting, but which were modified to be both simple and very easy to differentiate. Palm Computing also released two versions of Graffiti for Newton devices. Ironically, the Newton version sometimes performed better and could also show strokes as they were being written as input was done on the display itself, rather than on a silkscreen area.
For editing text, Newton had a very intuitive system for handwritten editing, such as scratching out words to be deleted, circling text to be selected, or using written carets to mark inserts.
Later releases of the Newton operating system retained the original recognizer for compatibility, but added a hand-printed-text-only (not cursive) recognizer, called "Rosetta", which was developed by Apple, included in version 2.0 of the Newton operating system, and refined in Newton 2.1. Rosetta is generally considered a significant improvement and many reviewers, testers, and most users consider the Newton 2.1 handwriting recognition software better than any of the alternatives even 10 years after it was introduced. Recognition and computation of handwritten horizontal and vertical formulas such as "1 + 2 =" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique part of every Newton device.
The handwriting recognition and parts of the user interface for the Newton are best understood in the context of the broad history of Pen computing, which is quite extensive.
A vital feature of the Newton handwriting recognition system is the modeless error correction. That is, correction done in situ without using a separate window or widget, using a minimum of gestures. If a word is recognized improperly, the user could double-tap the word and a list of alternatives would pop up in a menu under the stylus. Most of the time, the correct word will be in the list. If not, a button at the bottom of the list allows the user to edit individual characters in that word. Other pen gestures could do such things as transpose letters (also in situ). The correction popup also allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve memory and storage space, alternative recognition hypotheses would not be saved indefinitely. If the user returned to a note a week later, for example, they would only see the best match. Error correction in many current handwriting systems provides such functionality but adds more steps to the process, greatly increasing the interruption to a user's workflow that a given correction requires.
User interface.
Text could also be entered by tapping with the stylus on a small on-screen pop-up QWERTY virtual keyboard, although more layouts were developed by users. Newton devices could also accept free-hand "Sketches", "Shapes", and "Ink Text", much like a desktop computer graphics tablet. With "Shapes", Newton could recognize that the user was attempting to draw a circle, a line, a polygon, etc., and it would clean them up into perfect vector representations (with modifiable control points and defined vertices) of what the user was attempting to draw. "Shapes" and "Sketches" could be scaled or deformed once drawn. "Ink text" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes ("ink text" supported word wrap, could be formatted to be bold, italic, etc.). At any time a user could also direct their Newton device to recognize selected "ink text" and turn it into recognized text (deferred recognition). A Newton note (or the notes attached to each contact in Names and each Dates calendar or to-do event) could contain any mix of interleaved text, Ink Text, Shapes, and Sketches.
Connectivity.
The MessagePad 100 series of devices used Macintosh's proprietary serial ports—round Mini-DIN 8 connectors. The MessagePad 2000/2100 models (as well as the eMate 300) have a small, proprietary "Newton InterConnect" port. However, the development of the Newton hardware/software platform was canceled by Steve Jobs on February 27, 1998, so the InterConnect port, while itself very advanced, can only be used to connect a serial dongle. A prototype multi-purpose InterConnect device containing serial, audio in, audio out, and other ports was also discovered. In addition, all Newton devices have infrared connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped with a standard PC Card expansion slot (two on the 2000/2100). This allows native modem and even Ethernet connectivity; Newton users have also written drivers for 802.11b wireless networking cards and ATA-type flash memory cards (including the popular CompactFlash format), as well as for Bluetooth cards. Newton can also dial a phone number through the built-in speaker of the Newton device by simply holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as parallel adapters, PCMCIA cards, or serial modems, the most notable of which is the lightweight Newton Fax Modem released by Apple in 1993. It is powered by 2 AA batteries, and can also be used with a power adapter. It provides data transfer at 2400 bit/s, and can also send and receive fax messages at 9600 and 4800 bit/s respectively.
Power options.
The original Apple MessagePad and MessagePad 100 used four AAA batteries. They were eventually replaced by AA batteries with the release of the Apple MessagePad 110.
The use of 4 AA NiCd (MessagePad 110, 120 and 130) and 4x AA NiMH cells (MP2x00 series, eMate 300) give a runtime of up to 30 hours (MP2100 with two 20 MB Linear Flash memory PC Cards, no backlight usage) and up to 24 hours with backlight on. While adding more weight to the handheld Newton devices than AAA batteries or custom battery packs, the choice of an easily replaceable/rechargeable cell format gives the user a still unsurpassed runtime and flexibility of power supply. This, together with the flash memory used as internal storage starting with the Apple MessagePad 120 (if all cells lost their power, no data was lost due to the non-volatility of this storage), gave birth to the slogan "Newton never dies, it only gets new batteries".
Later efforts and improvements.
The Apple MessagePad 2000/2100, with a vastly improved handwriting recognition system, 162 MHz StrongARM SA-110 RISC processor, Newton OS 2.1, and a better, clearer, backlit screen, attracted critical plaudits. 
Cases.
Apple and third parties marketed several "wallets" (cases) for the handheld Newton devices, which would hold them securely along with the owner's credit cards, driver's license, business cards, and cash. Most also protected the LCD screen. 
Market reception.
Fourteen months after Sculley demoed it at the May 1992, Chicago CES, the MessagePad was first offered for sale on August 2, 1993 at the Boston Macworld Expo. The hottest item at the show, it cost $900. 50,000 MessagePads were sold in the device’s first three months on the market.
The original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries.
Critics also panned the handwriting recognition that was available in the debut models, which had been trumpeted in the Newton's marketing campaign. It was this problem that was skewered in the Doonesbury comic strips in which a written text entry is (erroneously) translated as "Egg Freckles?", as well as in the animated television series The Simpsons. However, the word 'freckles' was not included in the Newton dictionary, although a user could add it themselves. Difficulties were in part caused by the long time requirements for the Calligrapher handwriting recognition software to "learn" the user's handwriting; this process could take anywhere from two weeks to two months.
Another factor which limited the early Newton devices' appeal was that desktop connectivity was not included in the basic retail package, a problem that was later solved with 2.x Newton devices - these were bundled with a serial cable and the appropriate Newton Connection Utilities software.
Later versions of Newton OS offered improved handwriting recognition, quite possibly a leading reason for the continued popularity of the devices among Newton users. Even given the age of the hardware and software, Newtons still demand a sale price on the used market far greater than that of comparatively aged PDAs produced by other companies. In 2006 CNET compared an Apple MessagePad 2000 to a Samsung Q1, and the Newton was declared better. In 2009, CNET compared an Apple MessagePad 2000 to an iPhone, and the Newton was still declared better.
A chain of dedicated Newton only stores called Newton Source existed from 1994 through 1998. Locations included NYC, Los Angeles, San Francisco, Chicago and Boston. The Westwood Village, California, near U.C.L.A. featured the trademark red and yellow lightbulb Newton logo in neon. The stores provided an informative educational venue to learn about the Newton platform in a hands on relaxed fashion. The stores had no traditional computer retail counters and featured oval desktops where interested users could become intimately involved with the Newton product range. The stores were a model for the later Apple Stores. 
Newton device models.
Notes: The eMate 300 actually has ROM chips silk screened with 2.2 on them. Stephanie Mak on her website discusses this:
If one removes all patches to the eMate 300 (by replacing the ROM chip, and then putting in the original one again, as the eMate and the MessagePad 2000/2100 devices erase their memory completely after replacing the chip), the result will be the Newton OS saying that this is version 2.2.00. Also, the Original MessagePad and the MessagePad 100 share the same model number, as they only differ in the ROM chip version. (The OMP has OS versions 1.0 to 1.05, or 1.10 to 1.11, while the MP100 has 1.3 that can be upgraded with various patches.)
Other uses.
There were a number of projects that used the Newton as a portable information device in cultural settings such as museums. For example, Visible Interactive created a walking tour in San Francisco's Chinatown but the most significant effort took place in Malaysia at the Petronas Discovery Center, known as Petrosains.
In 1995, an exhibit design firm, DMCD Inc., was awarded the contract to design a new science museum in the Petronas Towers in Kuala Lumpur. A major factor in the award was the concept that visitors would use a Newton device to access additional information, find out where they were in the museum, listen to audio, see animations, control robots and other media, and to bookmark information for printout at the end of the exhibit.
The device became known as the ARIF, a Malay word for "wise man" or "seer" and it was also an acronym for A Resourceful Informative Friend. Some 400 ARIFS were installed and over 300 are still in use today. The development of the ARIF system was extremely complex and required a team of hardware and software engineers, designers, and writers. ARIF is an ancestor of the PDA systems used in museums today and it boasted features that have not been attempted since.
The Newton was also used in healthcare applications, for example in collecting data directly from patients. Newtons were used as electronic diaries, with patients entering their symptoms and other information concerning their health status on a daily basis. The compact size of the device and its ease of use made it possible for the electronic diaries to be carried around and used in the patients' everyday life setting. This was an early example of electronic patient-reported outcomes (ePRO)

</doc>
<doc id="888" url="http://en.wikipedia.org/wiki?curid=888" title="A. E. van Vogt">
A. E. van Vogt

Alfred Elton van Vogt (; April 26, 1912 – January 26, 2000) was a Canadian-born science fiction author regarded as one of the most popular, influential and complex science fiction writers of the mid-twentieth century: the "Golden Age" of the genre.
Early life and writings.
Van Vogt was born on a farm in Edenburg, a Russian Mennonite community east of Gretna, Manitoba, Canada. Until he was four years old, van Vogt and his family spoke only a dialect of Low German in the home. Van Vogt's father, a lawyer, moved his family several times and his son found these moves difficult, remarking in later life:
After starting his writing career by writing for "true confession" style pulp magazines like "True Story", van Vogt decided to switch to writing something he enjoyed, science fiction. This happened after he casually picked up the August 1938 issue of "Astounding Science Fiction" from a newsstand, and found the story "Who Goes There?". The story inspired him to write "Vault of the Beast", which he would send to the same magazine. It was rejected, but the rejection letter encouraged him to try again. He would then send in a new story called "The Black Destroyer", which was accepted, while a rewritten version of "Vault of the Beast" would be published in 1940.
Van Vogt's first SF publication was inspired by "The Voyage of the Beagle" by Charles Darwin. "The Black Destroyer" was published by John W. Campbell in "Astounding Science Fiction", July 1939, the centennial year of Darwin's journal. It featured a fierce, carnivorous alien, the coeurl, stalking the crew of an exploration spaceship. The second Space Beagle story appeared in December, "Discord in Scarlet". Each was the cover story and was accompanied by interior illustrations, created by Frank Kramer and Paul Orban. (Van Vogt and Kramer thus debuted in the issue of "Astounding" that is sometimes singled out for ushering in the "Golden Age" of science fiction.) The former story served as the inspiration for a number of science fiction movies.
In 1950, the two were combined with two other stories as a fix-up novel, "The Voyage of the Space Beagle" (Simon & Schuster), which was published in at least five European languages by 1955. Positing the need for exobiologists who will appreciate the differences between the inhabitants of other planets and ourselves, it stresses the importance of the civilian rather than military in exploration of other cultures.
Van Vogt's first completed novel, and one of his most famous, is "Slan" (Arkham House, 1946), which Campbell serialized in "Astounding" September to December 1940. Using what became one of van Vogt's recurring themes, it told the story of a 9-year-old superman living in a world in which his kind are slain by "Homo sapiens".
In 1941, van Vogt decided to become a full-time writer, quitting his job at the Canadian Department of National Defence. Extremely prolific for a few years, van Vogt wrote a large number of short stories. In the 1950s, many of them were retrospectively patched together into novels, or "fixups" as he called them, a term which entered the vocabulary of science fiction criticism. When the original stories were related (e.g., "The War against the Rull") this was often successful. When not (e.g., "Quest for the Future") the disparate stories thrown together generally made for a less coherent plot.
Post-war philosophy.
In 1944, van Vogt moved to Hollywood, California, where his writing took on new dimensions after World War II. Van Vogt was always interested in the idea of all-encompassing systems of knowledge (akin to modern meta-systems)—the characters in his very first story used a system called "Nexialism" to analyze the alien's behaviour, and he became interested in the general semantics of Alfred Korzybski.
He subsequently wrote three novels merging these overarching themes, "The World of Null-A" and "The Pawns of Null-A" in the late 1940s, and "Null-A Three" in the early 1980s. "Null-A", or non-Aristotelian logic, refers to the capacity for, and practice of, using intuitive, inductive reasoning (compare fuzzy logic), rather than reflexive, or conditioned, deductive reasoning.
Van Vogt was also profoundly affected by revelations of totalitarian police states that emerged after World War II. He wrote a mainstream novel that was set in Communist China, "The Violent Man" (1962); he said that to research this book he had read 100 books about China. Into this book he incorporated his view of "the violent male type", which he described as a "man who had to be right", a man who "instantly attracts women" and who he said were the men who "run the world".
At the same time, in his fiction, van Vogt was consistently sympathetic to absolute monarchy as a form of government. This was the case, for instance, in the "Weapon Shop" series, the "Mixed Men" series, and in single stories such as "Heir Apparent" (1945), whose protagonist was described as a "benevolent dictator".
Van Vogt systematized his writing method, using scenes of 800 words or so where a new complication was added or something resolved. Several of his stories hinge on temporal conundra, a favorite theme. He stated that he acquired many of his writing techniques from three books: "Narrative Technique" by Thomas Uzzell, "The Only Two Ways to Write a Story" by John Gallishaw, and "Twenty Problems of the Short-Story Writer" by Gallishaw.
He also claimed many of his ideas came from dreams; throughout his writing life he arranged to be awakened every 90 minutes during his sleep period so he could write down his dreams.
In 1950, van Vogt was briefly appointed as head of L. Ron Hubbard's Dianetics operation in California. Dianetics was the secular precursor to Hubbard's Church of Scientology. The operation went broke nine months later, but never went bankrupt, due to van Vogt's arrangements with creditors. Van Vogt and his wife opened their own Dianetics centre, partly financed by his writings, until he "signed off" around 1961. At the time of his interview with Charles Platt, van Vogt was still president of the Californian Association of Dianetic Auditors.
In 1951, he published "The Weapon Shops of Isher", a true science fiction classic with strong political overtones. Between 1950 and 1960, van Vogt produced collections, notable fixups such as: "The Mixed Men" (1952) and "The War Against the Rull" (1959), and the two "Clane" novels, "Empire of the Atom" (1957) and "The Wizard of Linn" (1962), which were inspired (like Asimov's Foundation series) by the fall of the Roman Empire, specifically Claudius. He resumed writing again in the 1960s, mainly at Frederik Pohl's invitation. His later novels included fixups such as "The Beast" (aka "Moonbeast") (1963), "Rogue Ship" (1965), "Quest for the Future" (1970) and "Supermind" (1977); expanded short stories ("The Darkness on Diamondia" (1972), "Future Glitter" (aka "Tyranopolis") (1973); original novels such as "Children of Tomorrow" (1970), "The Battle of Forever" (1971) and "The Anarchistic Colossus" (1977); plus sequels to his classic works, many of which were promised, but only one of which appeared, "Null-A Three" (1984; originally published in French). Several later books were original in Europe, and at least one novel has only ever appeared in Italian, no English version yet published.
On January 26, 2000, van Vogt died in Los Angeles, United States from Alzheimer's disease and was survived by his second wife, the former Lydia Bereginsky.
Critical reception.
Critical opinion about the quality of van Vogt's work has been sharply divided.
One early and articulate critic was Damon Knight. In a chapter-long essay reprinted in "In Search of Wonder," entitled "Cosmic Jerrybuilder: A. E. van Vogt", Knight famously remarked that van Vogt "is no giant; he is a pygmy who has learned to operate an overgrown typewriter". Knight described "The World of Null-A" as "one of the worst allegedly-adult science fiction stories ever published". About van Vogt's writing, Knight said:
About "Empire of the Atom" Knight wrote:
Knight also expressed misgivings about van Vogt's politics, noting that his stories almost invariably present absolute monarchy in a favorable light.
On the other hand, when science fiction author Philip K. Dick was asked which science fiction writers had influenced his work the most, he replied:
Dick also defended van Vogt against Damon Knight’s criticisms:
In a review of "Transfinite: The Essential A.E. van Vogt", science fiction writer Paul Di Filippo said:
In "The John W. Campbell Letters", Campbell says, "The son-of-a-gun gets hold of you in the first paragraph, ties a knot around you, and keeps it tied in every paragraph thereafter—including the ultimate last one".
Harlan Ellison (who began reading van Vogt as a teenager) wrote, "Van was the first writer to shine light on the restricted ways in which I had been taught to view the universe and the human condition".
Writing in 1984 David Hartwell said:
The literary critic Leslie A. Fiedler said something similar:
The American literary critic Fredric Jameson says of van Vogt:
Nevertheless, van Vogt still has his critics. For example Darrell Schweitzer writing to "The New York Review of Science Fiction" in 1999 quoted a passage from the original van Vogt novelette "The Mixed Men", which he was then reading, and remarked:
Recognition.
In 1946, van Vogt and his first wife, Edna Mayne Hull, were Guests of Honor at the fourth World Science Fiction Convention.
In 1980, van Vogt received a "Casper Award" (precursor to the Canadian Prix Aurora Awards) for Lifetime Achievement.
The Science Fiction Writers of America named him its 14th Grand Master in 1995 (presented 1996).
There had been great controversy within SFWA regarding its long wait in bestowing its highest honor (limited to living writers, no more than one annually). Writing an obituary of van Vogt, Robert J. Sawyer, a fellow Canadian writer of science fiction remarked:
It is generally held that the "damnable SFWA politics" concerns Damon Knight, the founder of the SFWA, who abhorred van Vogt's style and politics and thoroughly demolished his literary reputation in the 1950s.
Harlan Ellison was more explicit in 1999 introduction to "Futures Past: The Best Short Fiction of A. E. van Vogt":
In 1996, van Vogt received a Special Award from the World Science Fiction Convention "for six decades of golden age science fiction". That same year, the Science Fiction and Fantasy Hall of Fame inducted him in its inaugural class of two deceased and two living persons, along with writer Jack Williamson (also living) and editors Hugo Gernsback and John W. Campbell.
The works of van Vogt were translated into French by the surrealist Boris Vian ("The World of Null-A" as "Le Monde des Å" in 1958), and van Vogt's works were "viewed as great literature of the surrealist school".
Quotes.
Concerning Theodore Sturgeon's death, van Vogt commented: 
Works.
Novels.
Primary dates represent first publication in book form.

</doc>
<doc id="890" url="http://en.wikipedia.org/wiki?curid=890" title="Anna Kournikova">
Anna Kournikova

Anna Sergeyevna Kournikova (; born 7 June 1981) is a Russian retired professional tennis player. Her appearance and celebrity status made her one of the best known tennis stars worldwide, despite her never winning a WTA singles title. At the peak of her fame, fans looking for images of Kournikova made her name one of the most common search strings on Google Search.
Despite her lack of a title, she reached No. 8 in the world in 2000. She achieved greater success playing doubles, where she was at times the World No. 1 player. With Martina Hingis as her partner, she won Grand Slam titles in Australia in 1999 and 2002. They referred to themselves as the "Spice Girls of Tennis".
Kournikova's professional tennis career ended prematurely at the age of 21 due to serious back and spinal problems, including a herniated disk. She lives in Miami Beach, Florida, and plays in occasional exhibitions and in doubles for the St. Louis Aces of World Team Tennis. She was a new trainer for season 12 of the television show "The Biggest Loser", replacing Jillian Michaels, but did not return for season 13. In addition to her tennis and television work, Kournikova serves as a Global Ambassador for Population Services International's "Five & Alive" program, which addresses health crises facing children under the age of five and their families.
Early life.
Anna Kournikova was born in Moscow, Soviet Union, on 7 June 1981. Her father, Sergei Kournikov (born 1961), a former Greco-Roman wrestling champion, eventually earned a PhD and was a professor at the University of Physical Culture and Sport in Moscow. As of 2001, he was still a part-time martial arts instructor there. Her mother Alla (born 1963) had been a 400-metre runner. Her younger brother, Allan, is a youth golf world champion who was featured in the 2013 documentary film "The Short Game".
Sergei Kournikov has said, "We were young and we liked the clean, physical life, so Anna was in a good environment for sport from the beginning".
Kournikova received her first tennis racquet as a New Year gift in 1986 at age 5. Describing her early regimen, she said, "I played two times a week from age six. It was a children's program. And it was just for fun; my parents didn't know I was going to play professionally, they just wanted me to do something because I had lots of energy. It was only when I started playing well at seven that I went to a professional academy. I would go to school, and then my parents would take me to the club, and I'd spend the rest of the day there just having fun with the kids." In 1986, Kournikova became a member of the Spartak Tennis Club, coached by Larissa Preobrazhenskaya. In 1989, at the age of eight, Kournikova began appearing in junior tournaments, and by the following year, was attracting attention from tennis scouts across the world. Kournikova signed a management deal at age ten and went to Bradenton, Florida, to train at Nick Bollettieri's celebrated tennis academy.
Tennis career.
1989–1997: Early years and breakthrough.
Following her arrival in the United States, Kournikova became prominent on the tennis scene. At 14, she won the European Championships and the Italian Open Junior tournament. She became the youngest player to win the 18-and-under division of the Junior Orange Bowl tennis tournament. By the end of the year, Kournikova was crowned the ITF Junior World Champion U-18 and Junior European Champion U-18.
In 1994, Kournikova received a wild card into ITF tournament in Moscow qualifications, but lost to third seeded Sabine Appelmans. She debuted in professional tennis at 14 in the Fed Cup for Russia, the youngest player ever to participate and win. In 1995, she turned pro, and won two ITF titles, in Midland, Michigan and Rockford, Illinois. The same year Kournikova reached her first WTA Tour doubles final at the Kremlin Cup. Partnering with 1995 Wimbledon girls' champion in both singles and doubles Aleksandra Olsza, they lost to Meredith McGrath and Larisa Neiland.
In 1996, she started playing under a new coach, Ed Nagel. Her six-year tenure with Ed would produce terrific results. At 15, she made her grand slam debut, when she reached the fourth round of the 1996 US Open, only to be stopped by then-top ranked player Steffi Graf, the eventual champion. After this tournament, Kournikova's ranking jumped from No. 144 to debut in the Top 100 at No. 69. Kournikova was a member of the Russian delegation to the 1996 Olympic Games in Atlanta, Georgia. In 1996, she was named WTA Newcomer of the Year, and she was ranked No. 57 in the end of the season.
Kournikova entered the 1997 Australian Open as World No. 67, where she lost in the first round to World No. 12 Amanda Coetzer. At the Italian Open, Kournikova lost to Amanda Coetzer in the second round. However, she reached the semifinals in the doubles partnering with Elena Likhovtseva, before losing to the sixth seeds Mary Joe Fernández and Patricia Tarabini.
At the 1997 French Open, Kournikova made it to the third round before losing to World No. 1 Martina Hingis. She also reached the third round in doubles with Likhovtseva. At the 1997 Wimbledon Championships, Kournikova became only the second woman in the open era to reach the semifinals in her Wimbledon debut, the first being Chris Evert in 1972. There she lost to eventual champion Martina Hingis.
At the 1997 US Open, she lost in the second round to the eleventh seed Irina Spîrlea. Partnering with Likhovtseva, she reached the third round of the women's doubles event. Kournikova played her last WTA Tour event of 1997 at Porsche Tennis Grand Prix in Filderstadt, losing to Amanda Coetzer in the second round of singles, and in the first round of doubles to Lindsay Davenport and Jana Novotná partnering with Likhovtseva. She broke into the top 50 on 19 May, and was ranked No. 32 in singles and No. 41 in doubles at the end of the season.
1998–2000: Success and stardom.
In 1998, Kournikova broke into the WTA's top 20 rankings for the first time, when she was ranked No. 16. At the 1998 Australian Open, Kournikova lost in the third round to World No. 1 player Martina Hingis. She also partnered with Larisa Neiland in women's doubles, and they lost to eventual champions Hingis and Mirjana Lučić in the second round. Although she lost in the second round of the Paris Open to Anke Huber in singles, Kournikova reached her second doubles WTA Tour final, partnering with Larisa Neiland. They lost to Sabine Appelmans and Miriam Oremans. Kournikova and Neiland reached their second consecutive final at the Linz Open, losing to Alexandra Fusai and Nathalie Tauziat. At the Miami Open, Kournikova reached her first WTA Tour singles final, before losing to Venus Williams in the final.
Kournikova then reached two consecutive quarterfinals, at Amelia Island and the Italian Open, losing respectively to Lindsay Davenport and Martina Hingis. At the German Open, she reached the semifinals in both singles and doubles, partnering with Larisa Neiland. At the 1998 French Open Kournikova had her best result at this tournament, making it to the fourth round before losing to Jana Novotná. She also reached her first Grand Slam doubles semifinals, losing with Neiland to Lindsay Davenport and Natasha Zvereva. During her quarterfinals match at the grass-court Eastbourne Open versus Steffi Graf, Kournikova injured her thumb, which would eventually force her to withdraw from the 1998 Wimbledon Championships. However, she won that match, but then withdraw from her semifinals match against Arantxa Sánchez Vicario. Kournikova returned for the Du Maurier Open and made it to the third round, before losing to Conchita Martínez. At the 1998 US Open Kournikova reached the fourth round before losing to Arantxa Sánchez Vicario. Her strong year qualified her for the year-end 1998 WTA Tour Championships, but she lost to Monica Seles in the first round. However, with Seles, she won her first WTA doubles title, in Tokyo, beating Mary Joe Fernández and Arantxa Sánchez Vicario in the final. At the end of the season, she was ranked No. 10 in doubles.
At the start of the 1999 season, Kournikova advanced to the fourth round in singles before losing to Mary Pierce. However, Kournikova won her first doubles Grand Slam title, partnering Martina Hingis. The two defeated Lindsay Davenport and Natasha Zvereva in the final. At the Tier I Family Circle Cup, Kournikova reached her second WTA Tour final, but lost to Martina Hingis. She then defeated Jennifer Capriati, Lindsay Davenport and Patty Schnyder on her route to the Bausch & Lomb Championships semifinals, losing to Ruxandra Dragomir. At The French Open, Kournikova reached the fourth round before losing to eventual champion Steffi Graf. Once the grass-court season commenced in England, Kournikova lost to Nathalie Tauziat in the semifinals in Eastbourne. At Wimbledon, Kournikova lost to Venus Williams in the fourth round. She also reached the final in mixed doubles, partnering with Jonas Björkman, but they lost to Leander Paes and Lisa Raymond. Kournikova again qualified for year-end WTA Tour Championships, but lost to Mary Pierce in the first round, and ended the season as World No. 12.
While Kournikova had a successful singles season, she was even more successful in doubles. After their victory at the Australian Open, she and Martina Hingis won tournaments in Indian Wells, Rome, Eastbourne and the WTA Tour Championshiops, and reached the final of The French Open where they lost to Serena and Venus Williams. Partnering with Elena Likhovtseva, Kournikova also reached the final in Stanford. On 22 November 1999 she reached the World No. 1 ranking in doubles, and ended the season at this ranking. Anna Kournikova and Martina Hingis were presented with the WTA Award for Doubles Team of the Year.
Kournikova opened her 2000 season winning the Gold Coast Open doubles tournament partnering with Julie Halard. She then reached the singles semifinals at the Medibank International Sydney, losing to Lindsay Davenport. At the 2000 Australian Open, she reached the fourth round in singles and the semifinals in doubles. That season, Kournikova reached eight semifinals (Sydney, Scottsdale, Stanford, San Diego, Luxembourg, Leipzig and 2000 WTA Tour Championships), seven quarterfinals (Gold Coast, Tokyo, Amelia Island, Hamburg, Eastbourne, Zürich and Philadelphia) and one final. On 20 November 2000 she broke into top 10 for the first time, reaching No. 8. She was also ranked No. 4 in doubles at the end of the season. Kournikova was once again, more successful in doubles. She reached the final of the 2000 US Open in mixed doubles, partnering with Max Mirnyi, but they lost to Jared Palmer and Arantxa Sánchez Vicario. She also won six doubles titles – Gold Coast (with Julie Halard), Hamburg (with Natasha Zvereva), Filderstadt, Zürich, Philadelphia and the 2000 WTA Tour Championships (with Martina Hingis).
2001–2003: Injuries and final years.
Her 2001 season was dominated by injury, including a left foot stress fracture which forced her withdrawal from twelve tournaments, including the French Open and Wimbledon. She underwent surgery in April. She reached her second career grand slam quarterfinals, at the Australian Open. Kournikova then withdrew from several events due to continuing problems with her left foot and did not return until Leipzig. With Barbara Schett, she won the doubles title in Sydney. She then lost in the finals in Tokyo, partnering with Iroda Tulyaganova, and at San Diego, partnering with Martina Hingis. Hingis and Kournikova also won the Kremlin Cup. At the end of the 2001 season, she was ranked No. 74 in singles and No. 26 in doubles.
Kournikova was quite successful in 2002. She reached the semifinals of Auckland, Tokyo, Acapulco and San Diego, and the final of the China Open, losing to Anna Smashnova. This was Kournikova's last singles final. With Martina Hingis, she lost in the final at Sydney, but they won their second grand slam title together, the Australian Open. They also lost in the quarterfinals of the US Open. With Chanda Rubin, Kournikova played the semifinals of Wimbledon, but they lost to Serena and Venus Williams. Partnering Janet Lee, she won the Shanghai title. At the end of 2002 season, she was ranked No. 35 in singles and No. 11 in doubles.
In 2003, Anna Kournikova collected her first grand slam match victory in two years at the Australian Open. She defeated Henrieta Nagyová in the 1st round, and then lost to Justine Henin-Hardenne in the 2nd round. She withdrew from Tokyo due to a sprained back suffered at the Australian Open and did not return to Tour until Miami. On 9 April, in what would be the final WTA match of her career, Kournikova retired in the 1st round of the Family Circle Cup in Charleston, South Carolina, due to a left adductor strain. Her singles world ranking was 67. She reached the semifinals at the ITF tournament in Sea Island, before withdrawing from a match versus Maria Sharapova due to the adductor injury. She lost in the 1st round of the ITF tournament in Charlottesville. She did not compete for the rest of the season due to a continuing back injury. At the end of the 2003 season and her professional career, she was ranked No. 305 in singles and No. 176 in doubles.
Kournikova's two Grand Slam doubles titles came in 1999 and 2002, both at the Australian Open in the Women's Doubles event with partner Martina Hingis. Kournikova proved a successful doubles player on the professional circuit, winning 16 tournament doubles titles, including two Australian Opens and being a finalist in mixed doubles at the US Open and at Wimbledon, and reaching the No. 1 ranking in doubles in the Women's Tennis Association tour rankings. Her pro career doubles record was 200–71. However, her singles career plateaued after 1999. For the most part, she managed to retain her ranking between 10 and 15 (her career high singles ranking was No.8), but her expected finals breakthrough failed to occur; she only reached four finals out of 130 singles tournaments, never in a Grand Slam event, and never won one.
Her singles record is 209–129. Her final playing years were marred by a string of injuries, especially back injuries, which caused her ranking to erode gradually. As a personality Kournikova was among the most common search strings for both articles and images in her prime.
2004–present: Exhibitions and World Team Tennis.
Kournikova has not played on the WTA Tour since 2003, but still plays exhibition matches for charitable causes. In late 2004, she participated in three events organized by Elton John and by fellow tennis players Serena Williams and Andy Roddick. In January 2005, she played in a doubles charity event for the Indian Ocean tsunami with John McEnroe, Andy Roddick, and Chris Evert. In November 2005, she teamed up with Martina Hingis, playing against Lisa Raymond and Samantha Stosur in the WTT finals for charity. Kournikova is also a member of the St. Louis Aces in the World Team Tennis (WTT), playing doubles only.
In September 2008, Kournikova showed up for the 2008 Nautica Malibu Triathlon held at Zuma Beach in Malibu, California. The Race raised funds for children's Hospital Los Angeles. She won that race for women's K-Swiss team. On 27 September 2008, Kournikova played exhibition mixed doubles matches in Charlotte, North Carolina, partnering with Tim Wilkison and Karel Nováček. Kournikova and Wilkison defeated Jimmy Arias and Chanda Rubin, and then Kournikova and Novacek defeated Rubin and Wilkison.
On 12 October 2008, Anna Kournikova played one exhibition match for the annual charity event, hosted by Billie Jean King and Elton John, and raised more than $400,000 for the Elton John AIDS Foundation and Atlanta AIDS Partnership Fund. She played doubles with Andy Roddick (they were coached by David Chang) versus Martina Navratilova and Jesse Levine (coached by Billie Jean King); Kournikova and Roddick won.
Kournikova competed alongside John McEnroe, Tracy Austin and Jim Courier at the "Legendary Night", which was held on 2 May 2009, at the Turning Stone Event Center in Verona, New York. The exhibition included a mixed doubles match of McEnroe and Austin against Courier and Kournikova.
In 2008, she was named a spokesperson for K-Swiss. In 2005, Kournikova stated that if she were 100% fit, she would like to come back and compete again.
In June 2010, Kournikova reunited with her doubles partner Martina Hingis to participate in competitive tennis for the first time in seven years in the Invitational Ladies Doubles event at Wimbledon. On 29 June 2010 they defeated the British pair Samantha Smith and Anne Hobbs.
Playing style.
As a player, Kournikova was noted for her footspeed and aggressive baseline play, and excellent angles and dropshots; however, her relatively flat, high-risk groundstrokes tended to produce frequent errors, and her serve was sometimes unreliable in singles.
Kournikova plays right-handed with a two-handed backhand. She is a great player at the net. She can hit forceful groundstrokes and also drop shots.
Her playing style fits the profile for a doubles player, and is complemented by her height. She has been compared to such doubles specialists as Pam Shriver and Peter Fleming.
Personal life.
Kournikova was in a relationship with fellow Russian Pavel Bure, an NHL ice hockey player. The two met in 1999 when Kournikova was still linked to Bure's former Russian teammate Sergei Fedorov. Bure and Kournikova were reported to have been engaged in 2000 after a reporter took a photo of them together in a Florida restaurant where Bure supposedly asked Kournikova to marry him. As the story made headlines in Russia, where they were both heavily followed in the media as celebrities, Bure and Kournikova both denied any engagement. Kournikova, 10 years younger than Bure, was 18 years old at the time.
The following year, Kournikova and Fedorov were married in Moscow. Fedorov claimed he and Kournikova were married in 2001, and divorced in 2003. Kournikova's representatives deny any marriage to Fedorov; however, Fedorov's agent Pat Brisson claims that although he does not know when they got married, he knew "Fedorov was married".
Kournikova started dating pop star Enrique Iglesias in late 2001 (she appeared in his video, "Escape"). Kournikova has consistently refused to directly confirm or deny the status of her personal relationships. In June 2008, Iglesias was quoted by the "Daily Star" as having married Kournikova the previous year and subsequently separated. The couple have invested in a $20 million home to be built on a private island in Miami.
Kournikova resides in Miami Beach, Florida.
Media publicity.
Most of Kournikova's fame has come from the publicity surrounding her looks and her personal life. During her debut at the 1996 US Open at the age of 15, the western world noticed her beauty, and soon pictures of her appeared in numerous magazines worldwide.
In 2000, Kournikova became the new face for Berlei's shock absorber sports bras, and appeared in the "only the ball should bounce" billboard campaign. Following that, she was cast by the Farrelly brothers for a minor role in the 2000 film "Me, Myself & Irene" starring Jim Carrey and Renée Zellweger. Photographs of her scantily clad form have appeared in various men's magazines, including one in the much-publicized 2004 "Sports Illustrated Swimsuit Issue", where she posed in bikinis and swimsuits, and in other men's publications such as "FHM" and "Maxim". Kournikova was named one of "People's" 50 Most Beautiful People in 1998 and was voted "hottest female athlete" on ESPN.com. In 2002 she also placed first in "FHM's 100 Sexiest Women in the World" in US and UK editions. By contrast, ESPN—citing the degree of hype as compared to actual accomplishments as a singles player—ranked Kournikova 18th in its "25 Biggest Sports Flops of the Past 25 Years". Kournikova was also ranked No. 1 in the ESPN Classic series "Who's number 1?" when the series featured sport's most overrated athletes.
She continued to be the most searched athlete on the Internet through 2008 even though she had retired from the professional tennis circuit years earlier. After slipping from first to sixth among athletes in 2009, she moved back up to third place among athletes in terms of search popularity in 2010.
In October 2010, Kournikova headed to NBC's "The Biggest Loser" where she led the contestants in a tennis-workout challenge. In May 2011, it was announced that Kournikova would join "The Biggest Loser" as a regular celebrity trainer in season 12. She did not return for season 13.
In November 2010, she became an American citizen. In 2011, "Men's Health" named her one of the "100 Hottest Women of All-Time", ranking her at No. 29.
Influences on popular culture.
A variation of a White Russian made with skim milk is known as an Anna Kournikova. In the lingo of the poker variation Texas Hold 'em, the hole cards Ace–King (unsuited) are sometimes referred to as an "Anna Kournikova", a term introduced by the poker commentator Vince van Patten during a WPT tournament because it "looks great but never wins". A video game featuring Kournikova's licensed appearance, titled "Anna Kournikova's Smash Court Tennis", was developed by Namco and released for the PlayStation in Japan and Europe in November 1998. A computer virus named the Anna Kournikova virus arose on 12 February 2001.
External links.
 
 
 
 

</doc>
<doc id="892" url="http://en.wikipedia.org/wiki?curid=892" title="Alfons Maria Jakob">
Alfons Maria Jakob

Alfons Maria Jakob (2 July 1884, Aschaffenburg/Bavaria–17 October 1931, Hamburg) was a German neurologist who worked in the field of neuropathology.
He was born in Aschaffenburg, Bavaria and educated in medicine at the universities of Munich, Berlin, and Strasbourg, where obtained his doctorate in 1908. In 1909 he commenced clinical work under the psychiatrist Emil Kraepelin and did laboratory work with Franz Nissl and Alois Alzheimer in Munich.
In 1911 he went to Hamburg to work with Theodor Kaes and became head of the laboratory of anatomical pathology at the psychiatric State Hospital Hamburg-Friedrichsberg. Following the death of Kaes in 1913, Jakob succeeded him as prosector. After serving in the German army in World War I, he returned to Hamburg and climbed the academic ladder. He was habilitated in neurology in 1919 and in 1924 became professor of neurology. Under Jakob's guidance the department grew rapidly. He made notable contributions to knowledge on concussion and secondary nerve degeneration and became a doyen of neuropathology.
Jakob published five monographs and more than 75 papers. His neuropathological studies contributed greatly to the delineation of several diseases, including multiple sclerosis and Friedreich's ataxia. He first recognised and described Alper's disease and Creutzfeldt-Jakob disease (the latter with Hans Gerhard Creutzfeldt). He accumulated experience in neurosyphilis, having a 200-bedded ward devoted exclusively to that disorder. Jakob made a lecture tour of the United States and South America where he wrote a paper on the neuropathology of yellow fever.
He suffered from chronic osteomyelitis for the last 7 years of his life. This eventually caused a retroperitoneal abscess and paralytic ileus from which he died following operation.

</doc>
<doc id="894" url="http://en.wikipedia.org/wiki?curid=894" title="Agnosticism">
Agnosticism

Agnosticism is the view that the truth values of certain claims—especially claims about the existence or non-existence of God, as well as other religious and metaphysical claims—are unknown or unknowable.
According to the philosopher William L. Rowe, in the popular sense, an agnostic is someone who neither believes nor disbelieves in the existence of God, whereas a theist and an atheist believe and disbelieve, respectively.
Thomas Henry Huxley, an English biologist, coined the word "agnostic" in 1869. However, earlier thinkers have written works that promoted agnostic points of view. These thinkers include Sanjaya Belatthaputta, a 5th-century BCE Indian philosopher who expressed agnosticism about any afterlife,
Protagoras, a 5th-century BCE Greek philosopher was agnostic about the gods.
The Nasadiya Sukta in the Rigveda is agnostic about the origin of the universe.
Since the time that Huxley coined the term, many other thinkers have extensively written about agnosticism. 
Defining agnosticism.
Thomas Henry Huxley said:
According to philosopher William L. Rowe, in the strict sense, agnosticism is the view that human reason is incapable of providing sufficient rational grounds to justify either the belief that God exists or the belief that God does not exist.
Etymology.
"Agnostic" () was used by Thomas Henry Huxley in a speech at a meeting of the Metaphysical Society in 1869 to describe his philosophy, which rejects all claims of spiritual or mystical knowledge.
Early Christian church leaders used the Greek word "gnosis" (knowledge) to describe "spiritual knowledge". Agnosticism is not to be confused with religious views opposing the ancient religious movement of Gnosticism in particular; Huxley used the term in a broader, more abstract sense.
Huxley identified agnosticism not as a creed but rather as a method of skeptical, evidence-based inquiry.
In recent years, scientific literature dealing with neuroscience and psychology has used the word to mean "not knowable".
In technical and marketing literature, "agnostic" can also mean independence from some parameters—for example, "platform agnostic"
or "hardware agnostic"
Qualifying agnosticism.
Scottish Enlightenment philosopher David Hume contended that meaningful statements about the universe are always qualified by some degree of doubt. He asserted that the fallibility of human beings means that they cannot obtain absolute certainty except in trivial cases where a statement is true by definition (i.e. tautologies such as "all bachelors are unmarried" or "all triangles have three corners").
Types of agnosticism.
A person calling oneself 'agnostic' is stating that they have no opinion on the existence of God, as there is no definitive evidence for or against. Agnosticism has, however, more recently been subdivided into several categories. Variations include:
History.
Hindu philosophy.
Throughout the history of Hinduism there has been a strong tradition of philosophic speculation and skepticism.
The Rig Veda takes an agnostic view on the fundamental question of how the universe and the gods were created. Nasadiya Sukta ("Creation Hymn") in the tenth chapter of the Rig Veda says:
Greek philosophy.
Agnostic thought, in the form of skepticism, emerged as a formal philosophical position in ancient Greece. Its proponents included Protagoras, Pyrrho, Carneades, Sextus Empiricus
and, to some degree, Socrates, who was a strong advocate for a skeptical approach to epistemology.
Pyrrho said that we should refrain from making judgement as we can never know the true reality. According to Pyrrho, having opinion was possible, but certainty and knowledge are impossible.
Carneades was also a skeptic in relation to all knowledge claims. He proposed a probability theory, however. According to him, certainty could never be attained. Protagoras rejected the conventional accounts of the gods. He said:
Hume, Kant, and Kierkegaard.
Aristotle,
Anselm,
Aquinas,
and Descartes
presented arguments attempting to rationally prove the existence of God. The skeptical empiricism of David Hume, the antinomies of Immanuel Kant, and the existential philosophy of Søren Kierkegaard convinced many later philosophers to abandon these attempts, regarding it impossible to construct any unassailable proof for the existence or non-existence of God.
In his 1844 book, "Philosophical Fragments", Kierkegaard writes:
Thomas Henry Huxley.
Agnostic views are as old as philosophical skepticism, but the terms agnostic and agnosticism were created by Huxley to sum up his thoughts on contemporary developments of metaphysics about the "unconditioned" (William Hamilton) and the "unknowable" (Herbert Spencer). Though Huxley began to use the term "agnostic" in 1869, his opinions had taken shape some time before that date. In a letter of September 23, 1860, to Charles Kingsley, Huxley discussed his views extensively:
And again, to the same correspondent, May 6, 1863:
Of the origin of the name agnostic to describe this attitude, Huxley gave the following account:
William Stewart Ross.
William Stewart Ross wrote under the name of Saladin. He championed agnosticism in opposition to the atheism of Charles Bradlaugh as an open-ended spiritual exploration.
In "Why I am an Agnostic" (c. 1889) he claims that agnosticism is "the very reverse of atheism".
Robert G. Ingersoll.
Robert G. Ingersoll, an Illinois lawyer and politician who evolved into a well-known and sought-after orator in 19th-century America, has been referred to as the "Great Agnostic".
In an 1896 lecture titled "Why I Am An Agnostic", Ingersoll related why he was an agnostic:
In the conclusion of the speech he simply sums up the agnostic position as:
Bertrand Russell.
Bertrand Russell's pamphlet, "Why I Am Not a Christian", based on a speech delivered in 1927 and later included in a book of the same title, is considered a classic statement of agnosticism.
He calls upon his readers to "stand on their own two feet and look fair and square at the world with a fearless attitude and a free intelligence".
In 1939, Russell gave a lecture on "The existence and nature of God", in which he characterized himself as an atheist. He said:
However, later in the same lecture, discussing modern non-anthropomorphic concepts of God, Russell states:
In Russell's 1947 pamphlet, "Am I An Atheist or an Agnostic?" (subtitled "A Plea For Tolerance in the Face of New Dogmas"), he ruminates on the problem of what to call himself:
In his 1953 essay, "What Is An Agnostic?" Russell states:
Later in the essay, Russell adds:
Leslie Weatherhead.
In 1965 Christian theologian Leslie Weatherhead published "The Christian Agnostic", in which he argues:
Although radical and unpalatable to conventional theologians, Weatherhead's "agnosticism" falls far short of Huxley's, and short even of "weak agnosticism":
Charles Darwin.
Raised in a religious environment, Charles Darwin studied to be an Anglican clergyman. While eventually doubting parts of his faith, Darwin continued to help in church affairs, even while avoiding church attendance. Darwin stated that it would be "absurd to doubt that a man might be an ardent theist and an evolutionist". Although reticent about his religious views, in 1879 he wrote that "I have never been an atheist in the sense of denying the existence of a God. – I think that generally ... an agnostic would be the most correct description of my state of mind."
Demographics.
Demographic research services normally do not differentiate between various types of non-religious respondents, so agnostics are often classified in the same category as atheists or other non-religious people.
A 2010 survey published in "Encyclopædia Britannica" found that the non-religious people or the agnostics made up about 9.6% of the world's population.
A November–December 2006 poll published in the "Financial Times" gives rates for the United States and five European countries. The rates of agnosticism in the United States were at 14%, while the rates of agnosticism in the European countries surveyed were considerably higher: Italy (20%), Spain (30%), Great Britain (35%), Germany (25%), and France (32%).
A study conducted by the Pew Research Center found that about 16% of the world's people, the third largest group after Christianity and Islam, have no religious affiliation.
According to a 2012 report by the Pew Research Center, agnostics made up 3.3% of the US adult population.
In the "U.S. Religious Landscape Survey", conducted by the Pew Research Center, 55% of agnostic respondents expressed "a belief in God or a universal spirit",
whereas 41% stated that they thought that they felt a tension "being non-religious in a society where most people are religious".
Other studies have placed the estimated percentage of atheists, agnostics, and other nonbelievers in a personal god as low as single digits in Poland, Romania, Cyprus, and some other European countries.
According to the 2011 Australian Bureau of Statistics, 22% of Australians have "no religion", a category that includes agnostics.
Between 64% and 65%
of Japanese and up to 81%
of Vietnamese are atheists, agnostics, or do not believe in a god. An official European Union survey reported that 3% of the EU population is unsure about their belief in a god or spirit.
Criticism.
Agnosticism is criticized from a variety of standpoints. Some religious thinkers see agnosticism as limiting the mind's capacity to know reality to materialism. Some atheists criticize the use of the term "agnosticism" as functionally indistinguishable from "atheism"; this results in frequent criticisms of those who adopt the term as avoiding the 'atheist label'.
Some thinkers and philosophers deny the validity of agnosticism, seeing it as a limitation of man's capacity to know the reality, by asserting that human intelligence has a non-material, spiritual element. They affirm that "not being able to see or hold some specific thing does not necessarily negate its existence," using gravity, entropy, reason and thought as examples.
Theistic.
Theistic critics claim that agnosticism is impossible in practice, since a person can live only either as if God did not exist ("etsi deus non-daretur"), or as if God did exist ("etsi deus daretur").
Religious scholars such as Laurence B. Brown criticize the misuse of the word Agnosticism, claiming that it has become one of the most misapplied terms in metaphysics. Brown raises the question, "You claim that nothing can be known with certainty ... how, then, can you be so sure?"
Christian.
According to Joseph Ratzinger, "strong agnosticism" in particular limits contradicts itself in affirming the power of reason to know scientific truth. He blames the exclusion of reasoning from religion and ethics for dangerous pathologies such as crimes against humanity and ecological disasters.
"Agnosticism", said Ratzinger, "is always the fruit of a refusal of that knowledge which is in fact offered to man ... The knowledge of God has always existed". He asserted that agnosticism is a choice of comfort, pride, dominion, and utility over truth, and is opposed by the following attitudes: the keenest self-criticism, humble listening to the whole of existence, the persistent patience and self-correction of the scientific method, a readiness to be purified by the truth.
The Catholic Church sees merit in examining what it calls Partial Agnosticism, specifically those systems that "do not aim at constructing a complete philosophy of the unknowable, but at excluding special kinds of truth, notably religious, from the domain of knowledge".
However, the Church is historically opposed to a full denial of the capacity of human reason to know God. The Council of the Vatican, relying on biblical scripture, declares, "God, the beginning and end of all, can, by the natural light of human reason, be known with certainty from the works of creation".
Christian Philosopher Blaise Pascal argued that even if there were truly no evidence for God, agnostics should consider what is now known as Pascal's Wager: the infinite expected value of acknowledging God is always greater than the finite expected value of not acknowledging his existence, and thus it is a safer "bet" to choose God.
Peter Kreeft and Ronald Tacelli cited 20 arguments for God's existence,
asserting that any 'demand' for evidence testable in a laboratory is in effect asking God, the supreme being, to become man's servant.
Atheistic.
According to Richard Dawkins, a distinction between agnosticism and atheism is unwieldy and depends on how close to zero a person is willing to rate the probability of existence for any given god-like entity. About himself, Dawkins continues, "I am agnostic only to the extent that I am agnostic about fairies at the bottom of the garden."
Dawkins also identifies two categories of agnostics: "Temporary Agnostics in Practice" (TAPs), and "Permanent Agnostics in Principle" (PAPs). Dawkins considers temporary agnosticism an entirely reasonable position, but views permanent agnosticism as "fence-sitting, intellectual cowardice".
Related concepts.
Ignosticism is the view that a coherent definition of a deity must be put forward before the question of the existence of a deity can be meaningfully discussed. If the chosen definition is not coherent, the ignostic holds the noncognitivist view that the existence of a deity is meaningless or empirically untestable.
A.J. Ayer, Theodore Drange, and other philosophers see both atheism and agnosticism as incompatible with ignosticism on the grounds that atheism and agnosticism accept "a deity exists" as a meaningful proposition that can be argued for or against.
References.
Notes
Bibliography

</doc>
<doc id="896" url="http://en.wikipedia.org/wiki?curid=896" title="Argon">
Argon

Argon is a chemical element with symbol Ar and atomic number 18. It is in group 18 of the periodic table and is a noble gas. Argon is the third most common gas in the Earth's atmosphere, at 0.93% (9,300 ppm), making it approximately 23.8 times as abundant as the next most common atmospheric gas, carbon dioxide (390 ppm), and more than 500 times as abundant as the next most common noble gas, neon (18 ppm). Nearly all of this argon is radiogenic argon-40 derived from the decay of potassium-40 in the Earth's crust. In the universe, argon-36 is by far the most common argon isotope, being the preferred argon isotope produced by stellar nucleosynthesis in supernovas.
The name "argon" is derived from the Greek word "αργον", neuter singular form of "αργος" meaning "lazy" or "inactive", as a reference to the fact that the element undergoes almost no chemical reactions. The complete octet (eight electrons) in the outer atomic shell makes argon stable and resistant to bonding with other elements. Its triple point temperature of 83.8058 K is a defining fixed point in the International Temperature Scale of 1990.
Argon is produced industrially by the fractional distillation of liquid air. Argon is mostly used as an inert shielding gas in welding and other high-temperature industrial processes where ordinarily non-reactive substances become reactive; for example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning. Argon gas also has uses in incandescent and fluorescent lighting, and other types of gas discharge tubes. Argon makes a distinctive blue-green gas laser.
Characteristics.
Argon has approximately the same solubility in water as oxygen, and is 2.5 times more soluble in water than nitrogen. Argon is colorless, odorless, nonflammable and nontoxic as a solid, liquid, and gas. Argon is chemically inert under most conditions and forms no confirmed stable compounds at room temperature.
Although argon is a noble gas, it has been found to have the capability of forming some compounds. For example, the creation of argon fluorohydride (HArF), a marginally stable compound of argon with fluorine and hydrogen, was reported by researchers at the University of Helsinki in 2000.
Although the neutral ground-state chemical compounds of argon are presently limited to HArF, argon can form clathrates with water when atoms of it are trapped in a lattice of the water molecules.
Argon-containing ions and excited state complexes, such as and ArF, respectively, are known to exist. Theoretical calculations have predicted several argon compounds that should be stable, but for which no synthesis routes are currently known.
History.
"Argon" (αργον, neuter singular form of αργος, Greek meaning "inactive", in reference to its chemical inactivity) was suspected to be present in air by Henry Cavendish in 1785 but was not isolated until 1894 by Lord Rayleigh and Sir William Ramsay at University College London in an experiment in which they removed all of the oxygen, carbon dioxide, water and nitrogen from a sample of clean air. They had determined that nitrogen produced from chemical compounds was one-half percent lighter than nitrogen from the atmosphere. The difference seemed insignificant, but it was important enough to attract their attention for many months. They concluded that there was another gas in the air mixed in with the nitrogen. Argon was also encountered in 1882 through independent research of H. F. Newall and W. N. Hartley. Each observed new lines in the color spectrum of air but were unable to identify the element responsible for the lines. Argon became the first member of the noble gases to be discovered. The symbol for argon is now "Ar", but up until 1957 it was "A".
Occurrence.
Argon constitutes 0.934% by volume and 1.288% by mass of the Earth's atmosphere, and air is the primary raw material used by industry to produce purified argon products. Argon is isolated from air by fractionation, most commonly by cryogenic fractional distillation, a process that also produces purified nitrogen, oxygen, neon, krypton and xenon. The Earth's crust and seawater contain 1.2 ppm and 0.45 ppm of argon, respectively.
Isotopes.
The main isotopes of argon found on Earth are (99.6%), (0.34%), and (0.06%). Naturally occurring , with a half-life of 1.25 years, decays to stable (11.2%) by electron capture or positron emission, and also to stable (88.8%) via beta decay. These properties and ratios are used to determine the age of rocks by the method of K-Ar dating.
In the Earth's atmosphere, is made by cosmic ray activity, primarily with . In the subsurface environment, it is also produced through neutron capture by or alpha emission by calcium. is created from the neutron spallation of as a result of subsurface nuclear explosions. It has a half-life of 35 days.
Argon is notable in that its isotopic composition varies greatly between different locations in the solar system. Where the major source of argon is the decay of in rocks, will be the dominant isotope, as it is on Earth. Argon produced directly by stellar nucleosynthesis, in contrast, is dominated by the alpha process nuclide, . Correspondingly, solar argon contains 84.6% based on solar wind measurements,
and the ratio of the three isotopes 36Ar : 38Ar : 40Ar in the atmospheres of the outer planets is measured to be 8400 : 1600 : 1. This contrasts with the abundance of primordial in Earth's atmosphere: only 31.5 ppmv (= 9340 ppmv × 0.337%), comparable to that of neon (18.18 ppmv); and with measurements by interplanetary probes.
The Martian atmosphere contains 1.6% of and 5 ppm of . The Mariner probe fly-by of the planet Mercury in 1973 found that Mercury has a very thin atmosphere with 70% argon, believed to result from releases of the gas as a decay product from radioactive materials on the planet. In 2005, the Huygens probe discovered the presence of exclusively on Titan, the largest moon of Saturn.
The predominance of radiogenic is responsible for the standard atomic weight of terrestrial argon being greater than that of the next element, potassium, which was puzzling at the time when argon was discovered. Mendeleev had placed the elements in his periodic table in order of atomic weight, but the inertness of argon suggested a placement "before" the reactive alkali metal. Henry Moseley later solved this problem by showing that the periodic table is actually arranged in order of atomic number. (See History of the periodic table).
Compounds.
Argon's complete octet of electrons indicates full s and p subshells. This full outer energy level makes argon very stable and extremely resistant to bonding with other elements. Before 1962, argon and the other noble gases were considered to be chemically inert and unable to form compounds; however, compounds of the heavier noble gases have since been synthesized. In August 2000, the first argon compound was formed by researchers at the University of Helsinki. By shining ultraviolet light onto frozen argon containing a small amount of hydrogen fluoride with caesium iodide, argon fluorohydride (HArF) was formed. It is stable up to 40 kelvin (−233 °C). The metastable dication, which is valence isoelectronic with carbonyl fluoride, was observed in 2010. Argon-36, in the form of argon hydride ions, has been detected in cosmic dust associated with the Crab Nebula supernova; this was the first noble-gas molecule detected in outer space.
Production.
Industrial.
Argon is produced industrially by the fractional distillation of liquid air in a cryogenic air separation unit; a process that separates liquid nitrogen, which boils at 77.3 K, from argon, which boils at 87.3 K, and liquid oxygen, which boils at 90.2 K. About 700,000 tonnes of argon are produced worldwide every year.
In radioactive decays.
40Ar, the most abundant isotope of argon, is produced by the decay of 40K with a half-life of 1.25 years by electron capture or positron emission. Because of this, it is used in potassium-argon dating to determine the age of rocks.
Applications.
There are several different reasons argon is used in particular applications:
Other noble gases would probably work as well in most of these applications, but argon is by far the cheapest. Argon is inexpensive since it occurs naturally in air, and is readily obtained as a byproduct of cryogenic air separation in the production of liquid oxygen and liquid nitrogen: the primary constituents of air are used on a large industrial scale. The other noble gases (except helium) are produced this way as well, but argon is the most plentiful by far. The bulk of argon applications arise simply because it is inert and relatively cheap.
Industrial processes.
Argon is used in some high-temperature industrial processes, where ordinarily non-reactive substances become reactive. For example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning.
For some of these processes, the presence of nitrogen or oxygen gases might cause defects within the material. Argon is used in various types of arc welding such as gas metal arc welding and gas tungsten arc welding, as well as in the processing of titanium and other reactive elements. An argon atmosphere is also used for growing crystals of silicon and germanium.
Argon is used in the poultry industry to asphyxiate birds, either for mass culling following disease outbreaks, or as a means of slaughter more humane than the electric bath. Argon's relatively high density causes it to remain close to the ground during gassing. Its non-reactive nature makes it suitable in a food product, and since it replaces oxygen within the dead bird, argon also enhances shelf life.
Argon is sometimes used for extinguishing fires where damage to equipment is to be avoided.
Scientific research.
Liquid argon is used as the target for neutrino experiments and direct dark matter searches. The interaction of a hypothetical WIMP particle with the argon nucleus produces scintillation light that is detected by photomultiplier tubes. Two-phase detectors also use argon gas to detect the ionized electrons produced during the WIMP-nucleus scattering. As with most other liquefied noble gases, argon has a high scintillation lightyield (~ 51 photons/keV), is transparent to its own scintillation light, and is relatively easy to purify. Compared to xenon, argon is cheaper and has a distinct scintillation time profile which allows the separation of electronic recoils from nuclear recoils. On the other hand, its intrinsic gamma-ray background is larger due to contamination, unless one uses underground argon sources with a low level of radioactivity. Dark matter detectors currently operating with liquid argon include WArP, ArDM, microCLEAN and DEAP-I. Neutrino experiments include Icarus and MicroBooNE both of which use high purity liquid argon in a time projection chamber for fine grained three-dimensional imaging of neutrino interactions.
Preservative.
Argon is used to displace oxygen- and moisture-containing air in packaging material to extend the shelf-lives of the contents (argon has the European food additive code of "E938"). Aerial oxidation, hydrolysis, and other chemical reactions which degrade the products are retarded or prevented entirely. Bottles of high-purity chemicals and certain pharmaceutical products are available in sealed bottles or ampoules packed in argon. In wine making, argon is used to top-off barrels to avoid the aerial oxidation of ethanol to acetic acid during the aging process.
Argon is also available in aerosol-type cans, which may be used to preserve compounds such as varnish, polyurethane, paint, etc. for storage after opening.
Since 2002, the American National Archives stores important national documents such as the Declaration of Independence and the Constitution within argon-filled cases to retard their degradation. Using argon reduces gas leakage, compared with the helium used in the preceding five decades.
Laboratory equipment.
Argon may be used as the inert gas within Schlenk lines and gloveboxes. The use of argon over comparatively less expensive nitrogen is preferred where nitrogen may react with the experimental reagents or apparatus.
Argon may be used as the carrier gas in gas chromatography and in electrospray ionization mass spectrometry; it is the gas of choice for the plasma used in ICP spectroscopy. Argon is preferred for the sputter coating of specimens for scanning electron microscopy. Argon gas is also commonly used for sputter deposition of thin films as in microelectronics and for wafer cleaning in microfabrication.
Medical use.
Cryosurgery procedures such as cryoablation use liquefied argon to destroy cancer cells. In surgery it is used in a procedure called "argon enhanced coagulation" which is a form of argon plasma beam electrosurgery. The procedure carries a risk of producing gas embolism in the patient and has resulted in the death of one person via this type of accident.
Blue argon lasers are used in surgery to weld arteries, destroy tumors, and to correct eye defects.
Argon has also been used experimentally to replace nitrogen in the breathing or decompression mix known as Argox, to speed the elimination of dissolved nitrogen from the blood.
Lighting.
Incandescent lights are filled with argon, to preserve the filaments at high temperature from oxidation. It is used for the specific way it ionizes and emits light, such as in plasma globes and calorimetry in experimental particle physics. Gas-discharge lamps filled with argon provide blue light. Argon is also used for the creation of blue and green laser light.
Miscellaneous uses.
Argon is used for thermal insulation in energy efficient windows. Argon is also used in technical scuba diving to inflate a dry suit, because it is inert and has low thermal conductivity.
Argon is being used as a propellant in the development of the Variable Specific Impulse Magnetoplasma Rocket (VASIMR).
Compressed argon gas is allowed to expand, to cool the seeker heads of the AIM-9 Sidewinder missile, and other missiles that use cooled thermal seeker heads. The gas is stored at high pressure.
Argon is used in High Frequency Facial Machines. Developed in the late 1800s by Nikola Tesla, a low level electric current is generated by the machine and passed through clear tempered glass electrodes filled with Argon gas. The electric current ignites the Argon gas and a healing electric light energy is produced. The glass electrode is passed over the skin to produce a multitude of effects. High Frequency treatments have been used to treat acne conditions, soften fine lines and wrinkles, tighten sagging skin, reduce puffiness, improve the appearance of cellulite, and to revitalize the scalp promoting hair growth.
Argon-39, with a half-life of 269 years, has been used for a number of applications, primarily ice core and ground water dating. Also, potassium-argon dating is used in dating igneous rocks.
Argon has been used by athletes as a doping agent to simulate hypoxic conditions. On August 31 2014 the World Anti Doping Agency (WADA) added Argon(and Xenon) to the list of prohibited substances and methods, although at this time there is no reliable test for abuse.
Safety.
Although argon is non-toxic, it is 38% denser than air and is therefore considered a dangerous asphyxiant in closed areas. It is also difficult to detect because it is colorless, odorless, and tasteless. A 1994 incident in which a man was asphyxiated after entering an argon filled section of oil pipe under construction in Alaska highlights the dangers of argon tank leakage in confined spaces, and emphasizes the need for proper use, storage and handling.

</doc>
<doc id="897" url="http://en.wikipedia.org/wiki?curid=897" title="Arsenic">
Arsenic

Arsenic is a chemical element with symbol As and atomic number 33. Arsenic occurs in many minerals, usually in conjunction with sulfur and metals, and also as a pure elemental crystal. Arsenic is a metalloid. It can exist in various allotropes, although only the gray form has important use in industry.
The main use of metallic arsenic is for strengthening alloys of copper and especially lead (for example, in car batteries). Arsenic is a common n-type dopant in semiconductor electronic devices, and the optoelectronic compound gallium arsenide is the most common semiconductor in use after doped silicon. Arsenic and its compounds, especially the trioxide, are used in the production of pesticides, treated wood products, herbicides, and insecticides. These applications are declining, however.
Arsenic is notoriously poisonous to multicellular life, although a few species of bacteria are able to use arsenic compounds as respiratory metabolites. Arsenic contamination of groundwater is a problem that affects millions of people across the world.
Characteristics.
Physical characteristics.
The three most common arsenic allotropes are "metallic gray", "yellow" and "black arsenic", with gray being the most common. "Gray arsenic" (α-As, space group Rm No. 166) adopts a double-layered structure consisting of many interlocked ruffled six-membered rings. Because of weak bonding between the layers, gray arsenic is brittle and has a relatively low Mohs hardness of 3.5. Nearest and next-nearest neighbors form a distorted octahedral complex, with the three atoms in the same double-layer being slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 5.73 g/cm3. Gray arsenic is a semimetal, but becomes a semiconductor with a bandgap of 1.2–1.4 eV if amorphized. Gray arsenic is also the most stable form. "Yellow arsenic" is soft and waxy, and somewhat similar to tetraphosphorus (). Both have four atoms arranged in a tetrahedral structure in which each atom is bound to each of the other three atoms by a single bond. This unstable allotrope, being molecular, is the most volatile, least dense and most toxic. Solid yellow arsenic is produced by rapid cooling of arsenic vapor, . It is rapidly transformed into the gray arsenic by light. The yellow form has a density of 1.97 g/cm3." Black arsenic" is similar in structure to red phosphorus.
Black arsenic can also be formed by cooling vapor at around 100–220 °C. It is glassy and brittle. It is also a poor electrical conductor.
Isotopes.
Naturally occurring arsenic is composed of one stable isotope, 75As. This makes it a monoisotopic element. As of 2003, at least 33 radioisotopes have also been synthesized, ranging in atomic mass from 60 to 92. The most stable of these is 73As with a half-life of 80.30 days. All other isotopes have half-lives of under one day, with the exception of 71As (t1/2=65.30 hours), 72As (t1/2=26.0 hours), 74As (t1/2=17.77 days), 76As (t1/2=1.0942 days), and 77As (t1/2=38.83 hours). Isotopes that are lighter than the stable 75As tend to decay by β+ decay, and those that are heavier tend to decay by β− decay, with some exceptions.
At least 10 nuclear isomers have been described, ranging in atomic mass from 66 to 84. The most stable of arsenic's isomers is 68mAs with a half-life of 111 seconds.
Chemistry.
When heated in air, arsenic oxidizes to arsenic trioxide; the fumes from this reaction have an odor resembling garlic. This odor can be detected on striking arsenide minerals such as arsenopyrite with a hammer. Arsenic (and some arsenic compounds) sublimes upon heating at atmospheric pressure, converting directly to a gaseous form without an intervening liquid state at . The triple point is 3.63 MPa and . Arsenic makes arsenic acid with concentrated nitric acid, arsenious acid with dilute nitric acid, and arsenic trioxide with concentrated sulfuric acid.
Compounds.
Arsenic compounds resemble in some respects those of phosphorus which occupies the same group (column) of the periodic table. Arsenic is less commonly observed in the pentavalent state, however. The most common oxidation states for arsenic are: −3 in the arsenides, such as alloy-like intermetallic compounds; and +3 in the arsenites, arsenates(III), and most organoarsenic compounds. Arsenic also bonds readily to itself as seen in the square As ions in the mineral skutterudite. In the +3 oxidation state, arsenic is typically pyramidal owing to the influence of the lone pair of electrons.
Inorganic compounds.
Arsenic forms colorless, odorless, crystalline oxides As2O3 ("white arsenic") and As2O5 which are hygroscopic and readily soluble in water to form acidic solutions. Arsenic(V) acid is a weak acid. Its salts are called arsenates which are the basis of arsenic contamination of groundwater, a problem that affects many people. Synthetic arsenates include Paris Green (copper(II) acetoarsenite), calcium arsenate, and lead hydrogen arsenate. These three have been used as agricultural insecticides and poisons.
The protonation steps between the arsenate and arsenic acid are similar to those between phosphate and phosphoric acid. Unlike phosphorous acid, arsenous acid is genuinely tribasic, with the formula As(OH)3.
A broad variety of sulfur compounds of arsenic are known. Orpiment (As2S3) and realgar (As4S4) are somewhat abundant and were formerly used as painting pigments. In As4S10, arsenic has a formal oxidation state of +2 in As4S4 which features As-As bonds so that the total covalency of As is still 3.
All trihalides of arsenic(III) are well known except the astatide which is unknown. Arsenic pentafluoride (AsF5) is the only important pentahalide, reflecting the lower stability of the 5+ oxidation state. (pentachloride is stable only below −50 °C.)
Alloys.
Arsenic is used as the group 5 element in the III-V semiconductors gallium arsenide, indium arsenide, and aluminium arsenide. The valence electron count of GaAs is the same as a pair of Si atoms, but the band structure is completely different which results distinct bulk properties. Other arsenic alloys include the II-V semiconductor cadmium arsenide.
Organoarsenic compounds.
A large variety of organoarsenic compounds are known. Several were developed as chemical warfare agents during World War I, including vesicants such as lewisite and vomiting agents such as adamsite. Cacodylic acid, which is of historic and practical interest, arises from the methylation of arsenic trioxide, a reaction that has no analogy in phosphorus chemistry.
Occurrence and production.
Arsenic makes up about 1.5 ppm (0.00015%) of the Earth's crust, making it the 53rd most abundant element. Soil contains 1–10 ppm of arsenic. Seawater has only 1.6 ppb arsenic.
Minerals with the formula MAsS and MAs2 (M = Fe, Ni, Co) are the dominant commercial sources of arsenic, together with realgar (an arsenic sulfide mineral) and native arsenic. An illustrative mineral is arsenopyrite (FeAsS), which is structurally related to iron pyrite. Many minor As-containing minerals are known. Arsenic also occurs in various organic forms in the environment.
In 2005, China was the top producer of white arsenic with almost 50% world share, followed by Chile, Peru, and Morocco, according to the British Geological Survey and the United States Geological Survey. Most operations in the US and Europe have closed for environmental reasons. The arsenic is recovered mainly as a side product from the purification of copper. Arsenic is part of the smelter dust from copper, gold, and lead smelters.
On roasting in air of arsenopyrite, arsenic sublimes as arsenic(III) oxide leaving iron oxides, while roasting without air results in the production of metallic arsenic. Further purification from sulfur and other chalcogens is achieved by sublimation in vacuum or in a hydrogen atmosphere or by distillation from molten lead-arsenic mixture.
History.
The word "arsenic" has its origin in the Syriac word ܠܐ ܙܐܦܢܝܐ "(al) zarniqa", from the Persian word "zarnikh", meaning "yellow" (literally "gold-colored") and hence "(yellow) orpiment". It was adopted into Greek as "arsenikon" (ἀρσενικόν), a form that is folk etymology, being the neuter form of the Greek word "arsenikos" (ἀρσενικός), meaning "male", "virile". The Greek word was adopted in Latin as "arsenicum", which in French became "arsenic", from which the English word "arsenic" is taken. Arsenic sulfides (orpiment, realgar) and oxides have been known and used since ancient times. Zosimos (circa 300 AD) describes roasting "sandarach" (realgar) to obtain "cloud of arsenic" (arsenious oxide), which he then reduces to metallic arsenic. As the symptoms of arsenic poisoning were somewhat ill-defined, it was frequently used for murder until the advent of the Marsh test, a sensitive chemical test for its presence. (Another less sensitive but more general test is the Reinsch test.) Owing to its use by the ruling class to murder one another and its potency and discreetness, arsenic has been called the "Poison of Kings" and the "King of Poisons".
During the Bronze Age, arsenic was often included in bronze, which made the alloy harder (so-called "arsenical bronze").
Albertus Magnus (Albert the Great, 1193–1280) is believed to have been the first to isolate the element from a compound in 1250, by heating soap together with arsenic trisulfide. In 1649, Johann Schröder published two ways of preparing arsenic. Crystals of elemental (native) arsenic are found in nature, although rare.
Cadet's fuming liquid (impure cacodyl), often claimed as the first synthetic organometallic compound, was synthesized in 1760 by Louis Claude Cadet de Gassicourt by the reaction of potassium acetate with arsenic trioxide.
In the Victorian era, "arsenic" ("white arsenic" or arsenic trioxide) was mixed with vinegar and chalk and eaten by women to improve the complexion of their faces, making their skin paler to show they did not work in the fields. Arsenic was also rubbed into the faces and arms of women to "improve their complexion". The accidental use of arsenic in the adulteration of foodstuffs led to the Bradford sweet poisoning in 1858, which resulted in approximately 20 deaths.
Two pigments based on arsenic have been widely used since their discovery – Paris Green and Scheele's Green. After arsenic's toxicity became widely known, they were less often used as pigments, so these compounds were more often used as insecticides. In the 1860s an arsenic by-product of dye production, London Purple – a solid consisting of a mixture of arsenic trioxide, aniline, lime and ferrous oxide, which is insoluble in water and very toxic by inhalation and ingestion – was widely used, but Paris Green, another arsenic based dye, was later substituted for it. With better understanding of the toxicology mechanism, two other compounds were used starting in the 1890s. Arsenite of lime and arsenate of lead were used widely as insecticides until the discovery of DDT in 1942.
Applications.
Agricultural.
The toxicity of arsenic to insects, bacteria and fungi led to its use as a wood preservative. In the 1950s a process of treating wood with chromated copper arsenate (also known as CCA or Tanalith) was invented, and for decades this treatment was the most extensive industrial use of arsenic. An increased appreciation of the toxicity of arsenic resulted in a ban for the use of CCA in consumer products; the European Union and United States initiated this process in 2004. CCA remains in heavy use in other countries however, e.g. Malaysian rubber plantations.
Arsenic was also used in various agricultural insecticides and poisons. For example, lead hydrogen arsenate was a common insecticide on fruit trees, but contact with the compound sometimes resulted in brain damage among those working the sprayers. In the second half of the 20th century, monosodium methyl arsenate (MSMA) and disodium methyl arsenate (DSMA) – less toxic organic forms of arsenic – have replaced lead arsenate in agriculture. With the exception of cotton farming the use of the organic arsenicals was phased out until 2013.
Arsenic is used as a feed additive in poultry and swine production, in particular in the U.S. to increase weight gain, improve feed efficiency, and to prevent disease. An example is roxarsone, which had been used as a broiler starter by about 70% of U.S. broiler growers. The Poison-Free Poultry Act of 2009 proposed to ban the use of roxarsone in industrial swine and poultry production. Alpharma, a subsidiary of Pfizer Inc., which produces Roxarsone, voluntarily suspended sales of the drug in response to studies showing elevated levels of inorganic arsenic, a carcinogen, in treated chickens. A successor to Alpharma, Zoetis, continues to sell nitarsone, primarily for use in turkeys.
Medical use.
During the 18th, 19th, and 20th centuries, a number of arsenic compounds were used as medicines, including arsphenamine (by Paul Ehrlich) and arsenic trioxide (by Thomas Fowler). Arsphenamine as well as neosalvarsan was indicated for syphilis and trypanosomiasis, but has been superseded by modern antibiotics. Arsenic trioxide has been used in a variety of ways over the past 500 years, but most commonly in the treatment of cancer. The US Food and Drug Administration in 2000 approved this compound for the treatment of patients with acute promyelocytic leukemia that is resistant to ATRA. It was also used as Fowler's solution in psoriasis. Recently new research has been done in locating tumors using arsenic-74 (a positron emitter). The advantages of using this isotope instead of the previously used iodine-124 is that the signal in the PET scan is clearer as the body tends to transport iodine to the thyroid gland producing a lot of noise.
In subtoxic doses, soluble arsenic compounds act as stimulants, and were once popular in small doses as medicine by people in the mid-18th century.
Alloys.
The main use of metallic arsenic is for alloying with lead. Lead components in car batteries are strengthened by the presence of a very small percentage of arsenic. Dezincification can be strongly reduced by adding arsenic to brass, a copper-zinc alloy. "Phosphorus Deoxidized Arsenical Copper" with an arsenic content of 0.3% has an increased corrosion stability in certain environments. Gallium arsenide is an important semiconductor material, used in integrated circuits. Circuits made from GaAs are much faster (but also much more expensive) than those made in silicon. Unlike silicon it has a direct bandgap, and so can be used in laser diodes and LEDs to directly convert electricity into light.
Military.
After World War I, the United States built up a stockpile of 20,000 tonnes of lewisite (ClCH=CHAsCl2), a chemical weapon that is a vesicant (blister agent) and lung irritant. The stockpile was neutralized with bleach and dumped into the Gulf of Mexico after the 1950s. During the Vietnam War the United States used Agent Blue, a mixture of sodium cacodylate and its acid form, as one of the rainbow herbicides to deprive invading North Vietnamese soldiers of foliage cover and rice.
Biological role.
Bacteria.
Some species of bacteria obtain their energy by oxidizing various fuels while reducing arsenate to arsenite. Under oxidative environmental conditions some bacteria use arsenite, which is oxidized to arsenate as fuel for their metabolism. The enzymes involved are known as arsenate reductases (Arr).
In 2008, bacteria were discovered that employ a version of photosynthesis in the absence of oxygen with arsenites as electron donors, producing arsenates (just as ordinary photosynthesis uses water as electron donor, producing molecular oxygen). Researchers conjecture that, over the course of history, these photosynthesizing organisms produced the arsenates that allowed the arsenate-reducing bacteria to thrive. One strain PHS-1 has been isolated and is related to the gammaproteobacterium "Ectothiorhodospira shaposhnikovii". The mechanism is unknown, but an encoded Arr enzyme may function in reverse to its known homologues.
Although the arsenate and phosphate anions are similar structurally, no evidence exists for the replacement of phosphate in ATP or nucleic acids by arsenic.
Heredity.
Arsenic has been linked to epigenetic changes, heritable changes in gene expression that occur without changes in DNA sequence. These include DNA methylation, histone modification, and RNA interference. Toxic levels of arsenic cause significant DNA hypermethylation of tumor suppressor genes p16 and p53, thus increasing risk of carcinogenesis. These epigenetic events have been studied "in vitro" using human kidney cells and "in vivo" using rat liver cells and peripheral blood leukocytes in humans. Inductively coupled plasma mass spectrometry (ICP-MS) is used to detect precise levels of intracellular arsenic and its other bases involved in epigenetic modification of DNA. Studies investigating arsenic as an epigenetic factor will help in developing precise biomarkers of exposure and susceptibility.
The Chinese brake fern ("Pteris vittata") hyperaccumulates arsenic present in the soil into its leaves and has a proposed use in phytoremediation.
Biomethylation.
Inorganic arsenic and its compounds, upon entering the food chain, are progressively metabolized through a process of methylation. For example, the mold "Scopulariopsis brevicaulis" produces significant amounts of trimethylarsine if inorganic arsenic is present. The organic compound arsenobetaine is found in some marine foods such as fish and algae, and also in mushrooms in larger concentrations. The average person's intake is about 10–50 µg/day. Values about 1000 µg are not unusual following consumption of fish or mushrooms, but there is little danger in eating fish because this arsenic compound is nearly non-toxic.
Environmental issues.
Exposure.
Other naturally occurring pathways of exposure include volcanic ash, weathering of arsenic-containing minerals and ores, and dissolved in groundwater. It is also found in food, water, soil, and air. Arsenic is absorbed by all plants, but is more concentrated in leafy vegetables, rice, apple and grape juice, and seafood. An additional route of exposure is through inhalation.
Occurrence in drinking water.
Widespread arsenic contamination of groundwater has led to a massive epidemic of arsenic poisoning in Bangladesh and neighboring countries. It is estimated that approximately 57 million people in the Bengal basin are drinking groundwater with arsenic concentrations elevated above the World Health Organization's standard of 10 parts per billion (ppb). However, a study of cancer rates in Taiwan suggested that significant increases in cancer mortality appear only at levels above 150 ppb. The arsenic in the groundwater is of natural origin, and is released from the sediment into the groundwater, owing to the anoxic conditions of the subsurface. This groundwater began to be used after local and western NGOs and the Bangladeshi government undertook a massive shallow tube well drinking-water program in the late twentieth century. This program was designed to prevent drinking of bacteria-contaminated surface waters, but failed to test for arsenic in the groundwater. Many other countries and districts in Southeast Asia, such as Vietnam and Cambodia have geological environments conducive to generation of high-arsenic groundwaters. was reported in Nakhon Si Thammarat, Thailand in 1987, and the Chao Phraya River is suspected of containing high levels of naturally occurring dissolved arsenic, but has not been a public health problem owing to the use of bottled water.
In the United States, arsenic is most commonly found in the ground waters of the southwest. Parts of New England, Michigan, Wisconsin, Minnesota and the Dakotas are also known to have significant concentrations of arsenic in ground water. Increased levels of skin cancer have been associated with arsenic exposure in Wisconsin, even at levels below the 10 part per billion drinking water standard. According to a recent film funded by the US Superfund, millions of private wells have unknown arsenic levels, and in some areas of the US, over 20% of wells may contain levels that exceed established limits.
Low-level exposure to arsenic at concentrations of 100 parts per billion (i.e., above the 10 parts per billion drinking water standard) compromises the initial immune response to H1N1 or swine flu infection according to NIEHS-supported scientists. The study, conducted in laboratory mice, suggests that people exposed to arsenic in their drinking water may be at increased risk for more serious illness or death in response to infection from the virus.
Some Canadians are drinking water that contains inorganic arsenic. Private dug well waters are most at risk for containing inorganic arsenic. Preliminary well water analyses typically does not test for arsenic. Researchers at the Geological Survey of Canada have modelled relative variation in natural arsenic hazard potential for the province of New Brunswick. This study has important implications for potable water and health concerns relating to inorganic arsenic.
Epidemiological evidence from Chile shows a dose-dependent connection between chronic arsenic exposure and various forms of cancer, in particular when other risk factors, such as cigarette smoking, are present. These effects have been demonstrated to persist below 50 ppb.
Analyzing multiple epidemiological studies on inorganic arsenic exposure suggests a small but measurable risk increase for bladder cancer at 10 ppb. According to Peter Ravenscroft of the Department of Geography at the University of Cambridge, roughly 80 million people worldwide consume between 10 and 50 ppb arsenic in their drinking water. If they all consumed exactly 10 ppb arsenic in their drinking water, the previously cited multiple epidemiological study analysis would predict an additional 2,000 cases of bladder cancer alone. This represents a clear underestimate of the overall impact, since it does not include lung or skin cancer, and explicitly underestimates the exposure. Those exposed to levels of arsenic above the current WHO standard should weigh the costs and benefits of arsenic remediation.
Early (1973) evaluations of the removal of dissolved arsenic by drinking water treatment processes demonstrated that arsenic is very effectively removed by co-precipitation with either iron or aluminum oxides. The use of iron as a coagulant, in particular, was found to remove arsenic with efficiencies exceeding 90%. Several adsorptive media systems have been approved for point-of-service use in a study funded by the United States Environmental Protection Agency (US EPA) and the National Science Foundation (NSF). A team of European and Indian scientists and engineers have set up six arsenic treatment plants in West Bengal based on in-situ remediation method (SAR Technology). This technology does not use any chemicals and arsenic is left as an insoluble form (+5 state) in the subterranean zone by recharging aerated water into the aquifer and thus developing an oxidation zone to support arsenic oxidizing micro-organisms. This process does not produce any waste stream or sludge and is relatively cheap.
Another effective and inexpensive method to remove arsenic from contaminated well water is to sink wells 500 feet or deeper to reach purer waters. A recent 2011 study funded by the US National Institute of Environmental Health Sciences' Superfund Research Program shows that deep sediments can remove arsenic and take it out of circulation. Through this process called adsorption in which arsenic sticks to the surfaces of deep sediment particles, arsenic can be naturally removed from well water.
Magnetic separations of arsenic at very low magnetic field gradients have been demonstrated in point-of-use water purification with high-surface-area and monodisperse magnetite (Fe3O4) nanocrystals. Using the high specific surface area of Fe3O4 nanocrystals the mass of waste associated with arsenic removal from water has been dramatically reduced.
Epidemiological studies have suggested a correlation between chronic consumption of drinking water contaminated with arsenic and the incidence of all leading causes of mortality. The literature provides reason to believe arsenic exposure is causative in the pathogenesis of diabetes.
Hungarian engineer László Schremmer has recently discovered that by the use of chaff-based filters it is possible to reduce the arsenic content of water to 3 µg/L. This is especially important in areas where the potable water is provided by filtering the water extracted from the underground aquifer.
San Pedro de Atacama.
For several centuries, the people of San Pedro de Atacama in Chile have been drinking water that is contaminated with arsenic, and it is believed that they may have developed some immunity to the ill effects of consuming it.
Wood preservation in the US.
As of 2002, US-based industries consumed 19,600 metric tons of arsenic. Ninety percent of this was used for treatment of wood with chromated copper arsenate (CCA). In 2007, 50% of the 5,280 metric tons of consumption was still used for this purpose. In the United States, the voluntary phasing-out of arsenic in production of consumer products and residential and general consumer construction products began on 31 December 2003, and alternative chemicals are now used, such as Alkaline Copper Quaternary, borates, copper azole, cyproconazole, and propiconazole.
Although discontinued, this application is also one of the most concern to the general public. The vast majority of older pressure-treated wood was treated with CCA. CCA lumber is still in widespread use in many countries, and was heavily used during the latter half of the 20th century as a structural and outdoor building material. Although the use of CCA lumber was banned in many areas after studies showed that arsenic could leach out of the wood into the surrounding soil (from playground equipment, for instance), a risk is also presented by the burning of older CCA timber. The direct or indirect ingestion of wood ash from burnt CCA lumber has caused fatalities in animals and serious poisonings in humans; the lethal human dose is approximately 20 grams of ash. Scrap CCA lumber from construction and demolition sites may be inadvertently used in commercial and domestic fires. Protocols for safe disposal of CCA lumber do not exist evenly throughout the world; there is also concern in some quarters about the widespread landfill disposal of such timber.
Mapping of industrial releases in the US.
One tool that maps releases of arsenic to particular locations in the United States and also provides additional information about such releases is TOXMAP. TOXMAP is a Geographic Information System (GIS) from the Division of Specialized Information Services of the United States National Library of Medicine (NLM) that uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs. TOXMAP is a resource funded by the US Federal Government. TOXMAP's chemical and environmental health information is taken from NLM's Toxicology Data Network (TOXNET) and PubMed, and from other authoritative sources.
Toxicity and precautions.
Arsenic and many of its compounds are especially potent poisons. Many water supplies close to mines are contaminated by these poisons.
Classification.
Elemental arsenic and arsenic compounds are classified as "toxic" and "dangerous for the environment" in the European Union under directive 67/548/EEC.
The International Agency for Research on Cancer (IARC) recognizes arsenic and arsenic compounds as group 1 carcinogens, and the EU lists arsenic trioxide, arsenic pentoxide and arsenate salts as category 1 carcinogens.
Arsenic is known to cause owing to its manifestation in drinking water, "the most common species being arsenate [; As(V)] and arsenite [H3AsO3; As(III)]".
Legal limits, food, and drink.
In the United States, since 2006, the maximum concentration in drinking water allowed by the Environmental Protection Agency (EPA) is 10 ppb and the FDA set the same standard in 2005 for bottled water. The Department of Environmental Protection for New Jersey set a drinking water limit of 5 ppb in 2006.
In 2008, based on its ongoing testing of a wide variety of American foods for toxic chemicals, the U.S. Food and Drug Administration set 23 ppb as the "level of concern" for inorganic arsenic apple and pear juices based on non-carcinogenic effects, and began refusing imports and demanding recalls for domestic products exceeding this level. In 2011, the national Dr. Oz television show broadcast a program highlighting tests performed by an independent lab hired by the producers. Though the methodology was disputed (it did not distinguish between organic and inorganic arsenic) the tests showed levels of arsenic up to 36 ppb. In response, FDA testing of the worst brand from the Oz show showed much lower levels, and its ongoing testing found 95% of apple juice samples were below the level of concern. Later testing by Consumer Reports showed inorganic arsenic at levels slightly above 10 ppb, with the organization urging parents to reduce consumption. In July 2013, after taking into account consumption by children, chronic exposure, and carcinogenic effect, the FDA established an "action level" of 10 ppb for apple juice, the same as the drinking water standard.
Concern about arsenic in rice in Bangladesh was raised in 2002, but at the time only Australia had a legal limit for the level found in food (one milligram per kilogram). The People's Republic of China has a food standard of 150 ppb for arsenic, as of 2011. Further concern was raised about people who were eating U.S. rice exceeding WHO standards for personal arsenic intake in 2005.
In the United States in 2012, testing by separate groups of researchers at the Children's Environmental Health and Disease Prevention Research Center at Dartmouth College (early in the year, focusing on urinary levels in children) and Consumer Reports (in November) found levels of arsenic in rice which resulted in calls for the FDA to set limits. The FDA released some testing results in September 2012, and as of July 2013 is still collecting data in support of a new potential regulation. It has not recommended any changes in consumer behavior. Consumer Reports recommended that the EPA and FDA eliminate arsenic-containing fertilizer, drugs, and pesticides in food production; that the FDA establish a legal limit for food; that industry change production practices to lower arsenic levels, especially in food for children; and that consumers test home water supplies, eat a varied diet, and cook rice with excess water which is drained off (reducing inorganic arsenic by about one third along with a slight reduction in vitamin content). Evidence-based public health advocates also recommend that, given the lack of regulation or labeling for arsenic in the U.S., children should eat no more than 1 to 1.5 servings per week of rice and should not drink rice milk as part of their daily diet before age 5. They also offer recommendations for adults and infants on how to limit arsenic exposure from rice, drinking water, and fruit juice.
A 2014 World Health Organization advisory conference will consider limits of 200–300 ppb for rice.
Biological mechanism.
The high affinity of arsenic(III) oxides for thiols is usually assigned as the cause of the high toxicity. Thiols, usually in the form of cysteine residues, but also in cofactors such as lipoic acid and coenzyme A, are situated at the active sites of many important enzymes.
Arsenic disrupts ATP production through several mechanisms. At the level of the citric acid cycle, arsenic inhibits lipoic acid, which is a cofactor for pyruvate dehydrogenase. In addition, by competing with phosphate, arsenate uncouples oxidative phosphorylation, thus inhibiting energy-linked reduction of NAD+, mitochondrial respiration and ATP synthesis. Hydrogen peroxide production is also increased, which, it is speculated, has potential to form reactive oxygen species and oxidative stress. These metabolic interferences lead to death from multi-system organ failure. The organ failure is presumed to be from necrotic cell death, not apoptosis, since energy reserves have been too depleted for apoptosis to occur.
Although arsenic causes toxicity, it can also play a protective role.
Exposure risks and remediation.
Occupational exposure and arsenic poisoning may occur in persons working in industries involving the use of inorganic arsenic and its compounds, such as wood preservation, glass production, nonferrous metal alloys, and electronic semiconductor manufacturing. Inorganic arsenic is also found in coke oven emissions associated with the smelter industry.
The ability of arsenic to undergo redox conversion between As(III) and As(V) makes its availability in the environment more abundant. According to Croal, Gralnick, Malasarn and Newman, "[the] understanding [of] what stimulates As(III) oxidation and/or limits As(V) reduction is relevant for bioremediation of contaminated sites (Croal). The study of chemolithoautotrophic As(III) oxidizers and the heterotrophic As(V) reducers can help the understanding of the oxidation and/or reduction of arsenic. It has been proposed that As (III) which is more toxic than Arsenic (V) can be removed from the ground water using baker's yeast "Saccharomyces cerevisiae".
Treatment.
Treatment of chronic arsenic poisoning is easily accomplished. British anti-lewisite (dimercaprol) is prescribed in doses of 5 mg/kg up to 300 mg every 4 hours for the first day, then every 6 hours for the second day, and finally every 8 hours for 8 additional days. However the USA's Agency for Toxic Substances and Disease Registry (ATSDR) states that the long-term effects of arsenic exposure cannot be predicted. Blood, urine, hair, and nails may be tested for arsenic; however, these tests cannot foresee possible health outcomes from the exposure. Excretion occurs in the urine and long-term exposure to arsenic has been linked to bladder and kidney cancer in addition to cancer of the liver, prostate, skin, lungs, and nasal cavity.

</doc>
<doc id="898" url="http://en.wikipedia.org/wiki?curid=898" title="Antimony">
Antimony

Antimony is a chemical element with symbol Sb (from ) and atomic number 51. A lustrous gray metalloid, it is found in nature mainly as the sulfide mineral stibnite (Sb2S3). Antimony compounds have been known since ancient times and were used for cosmetics; metallic antimony was also known, but it was erroneously identified as lead. It was first isolated by Vannoccio Biringuccio and described in 1540.
For some time, China has been the largest producer of antimony and its compounds, with most production coming from the Xikuangshan Mine in Hunan. The industrial methods to produce antimony are roasting and subsequent carbothermal reduction or direct reduction of stibnite with iron.
The largest applications for metallic antimony are as alloying material for lead and tin and for lead antimony plates in lead-acid batteries. Alloying lead and tin with antimony improves the properties of the alloys which are used in solders, bullets and plain bearings. Antimony compounds are prominent additives for chlorine- and bromine-containing fire retardants found in many commercial and domestic products. An emerging application is the use of antimony in microelectronics.
Characteristics.
Properties.
Antimony is in the nitrogen group (group 15) and has an electronegativity of 2.05. As expected from periodic trends, it is more electronegative than tin or bismuth, and less electronegative than tellurium or arsenic. Antimony is stable in air at room temperature, but reacts with oxygen if heated, to form antimony trioxide, Sb2O3.
Antimony is a silvery, lustrous gray metal that has a Mohs scale hardness of 3. Thus pure antimony is too soft to make hard objects; coins made of antimony were issued in China's Guizhou province in 1931, but because of their rapid wear, their minting was discontinued. Antimony is resistant to attack by acids.
Four allotropes of antimony are known: a stable metallic form and three metastable forms (explosive, black and yellow). Metallic antimony is a brittle, silver-white shiny metal. When slowly cooled, molten antimony crystallizes in a trigonal cell, isomorphic with the gray allotrope of arsenic. A rare explosive form of antimony can be formed from the electrolysis of antimony trichloride. When scratched with a sharp implement, an exothermic reaction occurs and white fumes are given off as metallic antimony is formed; when rubbed with a pestle in a mortar, a strong detonation occurs. Black antimony is formed upon rapid cooling of vapor derived from metallic antimony. It has the same crystal structure as red phosphorus and black arsenic, it oxidizes in air and may ignite spontaneously. At 100 °C, it gradually transforms into the stable form. The yellow allotrope of antimony is the most unstable. It has only been generated by oxidation of stibine (SbH3) at −90 °C. Above this temperature and in ambient light, this metastable allotrope transforms into the more stable black allotrope.
Metallic antimony adopts a layered structure (space group Rm No. 166) in which layers consist of fused ruffled six-membered rings. The nearest and next-nearest neighbors form an irregular octahedral complex, with the three atoms in the same double layer being slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 6.697 g/cm3, but the weak bonding between the layers leads to the low hardness and brittleness of antimony.
Isotopes.
Antimony has two stable isotopes: 121Sb with a natural abundance of 57.36% and 123Sb with a natural abundance of 42.64%. It also has 35 radioisotopes, of which the longest-lived is 125Sb with a half-life of 2.75 years. In addition, 29 metastable states have been characterized. The most stable of these is 120m1Sb with a half-life of 5.76 days. Isotopes that are lighter than the stable 123Sb tend to decay by β+ decay, and those that are heavier tend to decay by β- decay, with some exceptions.
Occurrence.
The abundance of antimony in the Earth's crust is estimated at 0.2 to 0.5 parts per million, comparable to thallium at 0.5 parts per million and silver at 0.07 ppm. Even though this element is not abundant, it is found in over 100 mineral species. Antimony is sometimes found natively, but more frequently it is found in the sulfide stibnite (Sb2S3) which is the predominant ore mineral.
Compounds.
Antimony compounds are often classified according to their oxidation state: Sb(III) and Sb(V). The +5 oxidation state is more stable.
Oxides and hydroxides.
Antimony trioxide () is formed when antimony is burnt in air. In the gas phase, this compound exists as , but it polymerizes upon condensing. Antimony pentoxide () can only be formed by oxidation by concentrated nitric acid. Antimony also forms a mixed-valence oxide, antimony tetroxide (), which features both Sb(III) and Sb(V). Unlike phosphorus and arsenic, these various oxides are amphoteric, do not form well-defined oxoacids and react with acids to form antimony salts.
Antimonous acid is unknown, but the conjugate base sodium antimonite () forms upon fusing sodium oxide and . Transition metal antimonites are also known. Antimonic acid exists only as the hydrate , forming salts containing the antimonate anion . Dehydrating metal salts containing this anion yields mixed oxides.
Many antimony ores are sulfides, including stibnite (), pyrargyrite (), zinkenite, jamesonite, and boulangerite. Antimony pentasulfide is non-stoichiometric and features antimony in the +3 oxidation state and S-S bonds. Several thioantimonides are known, such as and .
Halides.
Antimony forms two series of halides: and . The trihalides , , , and are all molecular compounds having trigonal pyramidal molecular geometry.
The trifluoride is prepared by the reaction of with HF:
It is Lewis acidic and readily accepts fluoride ions to form the complex anions and . Molten is a weak electrical conductor. The trichloride is prepared by dissolving in hydrochloric acid:
The pentahalides and have trigonal bipyramidal molecular geometry in the gas phase, but in the liquid phase, is polymeric, whereas is monomeric. is a powerful Lewis acid used to make the superacid fluoroantimonic acid ("H2SbF7").
Oxyhalides are more common for antimony than arsenic and phosphorus. Antimony trioxide dissolves in concentrated acid to form oxoantimonyl compounds such as SbOCl and .
Antimonides, hydrides, and organoantimony compounds.
Compounds in this class generally are described as derivatives of Sb3-. Antimony forms antimonides with metals, such as indium antimonide (InSb) and silver antimonide (). The alkali metal and zinc antimonides, such as Na3Sb and Zn3Sb2, are more reactive. Treating these antimonides with acid produces the unstable gas stibine, :
Stibine can also be produced by treating salts with hydride reagents such as sodium borohydride.Stibine decomposes spontaneously at room temperature. Because stibine has a positive heat of formation, it is thermodynamically unstable and thus antimony does not react with hydrogen directly.
Organoantimony compounds are typically prepared by alkylation of antimony halides with Grignard reagents. A large variety of compounds are known with both Sb(III) and Sb(V) centers, including mixed chloro-organic derivatives, anions, and cations. Examples include Sb(C6H5)3 (triphenylstibine), Sb2(C6H5)4 (with an Sb-Sb bond), and cyclic [Sb(C6H5)]n. Pentacoordinated organoantimony compounds are common, examples being Sb(C6H5)5 and several related halides.
History.
Antimony(III) sulfide, Sb2S3, was recognized in predynastic Egypt as an eye cosmetic (kohl) as early as about 3100 BC, when the cosmetic palette was invented.
An artifact, said to be part of a vase, made of antimony dating to about 3000 BC was found at Telloh, Chaldea (part of present-day Iraq), and a copper object plated with antimony dating between 2500 BC and 2200 BC has been found in Egypt. Austen, at a lecture by Herbert Gladstone in 1892 commented that "we only know of antimony at the present day as a highly brittle and crystalline metal, which could hardly be fashioned into a useful vase, and therefore this remarkable 'find' (artifact mentioned above) must represent the lost art of rendering antimony malleable."
Moorey was unconvinced the artifact was indeed a vase, mentioning that Selimkhanov, after his analysis of the Tello object (published in 1975), "attempted to relate the metal to Transcaucasian natural antimony" (i.e. native metal) and that "the antimony objects from Transcaucasia are all small personal ornaments." This weakens the evidence for a lost art "of rendering antimony malleable."
The Greek scholar Pliny the Elder described several ways of preparing antimony sulfide for medical purposes in his treatise Natural history. Pliny the Elder also made a distinction between 'male' and 'female' forms of antimony; the male form is probably the sulfide, while the female form, which is superior, heavier, and less friable, has been suspected to be native metallic antimony.
The Roman naturalist Pedanius Dioscorides mentioned that antimony sulfide could be roasted by heating by a current of air. It is thought that this produced metallic antimony.
The first description of a procedure for isolating antimony is in the book "De la pirotechnia" of 1540 by Vannoccio Biringuccio; this predates the more famous 1556 book by Agricola, "De re metallica". In this context Agricola has been often incorrectly credited with the discovery of metallic antimony. The book "Currus Triumphalis Antimonii" (The Triumphal Chariot of Antimony), describing the preparation of metallic antimony, was published in Germany in 1604. It was purported to have been written by a Benedictine monk, writing under the name Basilius Valentinus, in the 15th century; if it were authentic, which it is not, it would predate Biringuccio.
The metal antimony was known to German chemist Andreas Libavius in 1615 who obtained it by adding iron to a molten mixture of antimony sulfide, salt and potassium tartrate. This procedure produced antimony with a crystalline or starred surface.
With the advent of challenges to phlogiston theory it was recognized that antimony is an element forming sulfides, oxides etc as is the case of other metals.
The first natural occurrence of pure antimony in the Earth's crust was described by the Swedish scientist and local mine district engineer Anton von Swab in 1783; the type-sample was collected from the Sala Silver Mine in the Bergslagen mining district of Sala, Västmanland, Sweden.
Etymology.
The ancient words for antimony mostly have, as their chief meaning, kohl, the sulfide of antimony.
The Egyptians called antimony "mśdmt"; in hieroglyphs, the vowels are uncertain, but there is an Arabic tradition that the word is ميسديميت "mesdemet". The Greek word, στίμμι "stimmi", is probably a loan word from Arabic or Egyptian "sdm" O34:D46-G17-F21:D4 and is used by Attic tragic poets of the 5th century BC; later Greeks also used στἰβι "stibi", as did Celsus and Pliny, writing in Latin, in the first century AD. Pliny also gives the names "stimi" , "larbaris", alabaster, and the "very common" "platyophthalmos", "wide-eye" (from the effect of the cosmetic). Later Latin authors adapted the word to Latin as "stibium". The Arabic word for the substance, as opposed to the cosmetic, can appear as إثمد "ithmid, athmoud, othmod", or "uthmod". Littré suggests the first form, which is the earliest, derives from "stimmida", an accusative for "stimmi".
The use of Sb as the standard chemical symbol for antimony is due to Jöns Jakob Berzelius, who used this abbreviation of the name "stibium". The medieval Latin form, from which the modern languages and late Byzantine Greek take their names for antimony, is "antimonium". The origin of this is uncertain; all suggestions have some difficulty either of form or interpretation. The popular etymology, from ἀντίμοναχός "anti-monachos" or French "antimoine", still has adherents; this would mean "monk-killer", and is explained by many early alchemists being monks, and antimony being poisonous.
Another popular etymology is the hypothetical Greek word ἀντίμόνος "antimonos", "against aloneness", explained as "not found as metal", or "not found unalloyed". Lippmann conjectured a hypothetical Greek word ανθήμόνιον "anthemonion", which would mean "floret", and cites several examples of related Greek words (but not that one) which describe chemical or biological efflorescence.
The early uses of "antimonium" include the translations, in 1050–1100, by Constantine the African of Arabic medical treatises. Several authorities believe "antimonium" is a scribal corruption of some Arabic form; Meyerhof derives it from "ithmid"; other possibilities include "athimar", the Arabic name of the metalloid, and a hypothetical "as-stimmi", derived from or parallel to the Greek.
Production.
Top producers and production volumes.
The British Geological Survey reported that in 2005, the People's Republic of China was the top producer of antimony with an approximately 84% world share, followed at a distance by South Africa, Bolivia and Tajikistan. Xikuangshan Mine in Hunan province has the largest deposits in China with an estimated deposit of 2.1 million metric tons.
In 2010, according to the US Geological Survey, China accounted for 88.9% of total antimony production with South Africa, Bolivia and Russia sharing the second place.
However, Roskill Consulting estimates for primary production show that in 2010 China held a 76.75% share of world's supply with 120,462 tonnes (90,000 tonnes of reported and 30,464 tonnes of un-reported production), followed by Russia (4.14% share, 6,500 tonnes of production), Myanmar (3.76% share, 5,897 tonnes), Canada (3.61% share, 5,660 tonnes), Tajikistan (3.42% share, 5,370 tonnes) and Bolivia (3.17% share, 4,980 tonnes).
Roskill estimates that secondary production globally in 2010 was 39,540 tonnes.
Antimony was ranked first in a Risk List published by the British Geological Survey in the second half 2011. The list provides an indication of the relative risk to the supply of chemical elements or element groups required to maintain the current British economy and lifestyle.
Also, antimony was identified as one of 12 critical raw materials for the EU in a report published in 2011, primarily due to the lack of supply outside China.
Reported production of antimony in China fell in 2010 and is unlikely to increase in the coming years, according to the Roskill report. No significant antimony deposits in China have been developed for about ten years, and the remaining economic reserves are being rapidly depleted.
The world's largest antimony producers, according to Roskill, are listed below:
Reserves.
According to statistics from the US Geological Survey (USGS), current global reserves of antimony will be depleted in 13 years. However, the United States Geological Survey expects more resources will be found.
Production process.
The extraction of antimony from ores depends on the quality of the ore and composition of the ore. Most antimony is mined as the sulfide; lower grade ores are concentrated by froth flotation, while higher grade ores are heated to 500–600 °C, the temperature at which stibnite melts and is separated from the gangue minerals. Antimony can be isolated from the crude antimony sulfide by a reduction with scrap iron:
The sulfide is converted to an oxide and advantage is often taken of the volatility of antimony(III) oxide, which is recovered from roasting. This material is often used directly for the main applications, impurities being arsenic and sulfide.
Isolating antimony from its oxide is performed by a carbothermal reduction:
The lower grade ores are reduced in blast furnaces while the higher grade ores are reduced in reverberatory furnaces.
Applications.
About 60% of antimony is consumed in flame retardants, and 20% is used in alloys for batteries, plain bearings and solders.
Flame retardants.
Antimony is mainly used as its trioxide in making flame-proofing compounds. It is nearly always used in combination with halogenated flame retardants, with the only exception being in halogen-containing polymers. The formation of halogenated antimony compounds is the cause for the flame retarding effect of antimony trioxide, due to reaction of these compounds with hydrogen atoms and probably also with oxygen atoms and OH radicals, thus inhibiting fire. Markets for these flame-retardant applications include children's clothing, toys, aircraft and automobile seat covers. It is also used in the fiberglass composites industry as an additive to polyester resins for such items as light aircraft engine covers. The resin will burn while a flame is held to it but will extinguish itself as soon as the flame is removed.
Alloys.
Antimony forms a highly useful alloy with lead, increasing its hardness and mechanical strength. For most applications involving lead, varying amounts of antimony are used as alloying metal. In lead–acid batteries, this addition improves the charging characteristics and reduces generation of unwanted hydrogen during charging. It is used in antifriction alloys (such as Babbitt metal), in bullets and lead shot, cable sheathing, type metal (for example, for linotype printing machines), solder (some "lead-free" solders contain 5% Sb), in pewter, and in hardening alloys with low tin content in the manufacturing of organ pipes.
Other applications.
Three other applications make up nearly all the rest of the consumption. One of these uses is as a stabilizer and a catalyst for the production of polyethyleneterephthalate. Another application is to serve as a fining agent to remove microscopic bubbles in glass, mostly for TV screens; this is achieved by the interaction of antimony ions with oxygen, interfering the latter from forming bubbles. The third major application is the use as pigment.
Antimony is being increasingly used in the semiconductor industry as a dopant for heavily doped n-type silicon wafers in the production of diodes, infrared detectors, and Hall-effect devices. In the 1950s, tiny beads of a lead-antimony alloy were used to dope the emitters and collectors of n-p-n alloy junction transistors with antimony. Indium antimonide is used as a material for mid-infrared detectors.
Few biological or medical applications exist for antimony. Treatments principally containing antimony are known as antimonials and are used as emetics. Antimony compounds are used as antiprotozoan drugs. Potassium antimonyl tartrate, or tartar emetic, was once used as an anti-schistosomal drug from 1919 on. It was subsequently replaced by praziquantel. Antimony and its compounds are used in several veterinary preparations like anthiomaline or lithium antimony thiomalate, which is used as a skin conditioner in ruminants. Antimony has a nourishing or conditioning effect on keratinized tissues, at least in animals.
Antimony-based drugs, such as meglumine antimoniate, are also considered the drugs of choice for treatment of leishmaniasis in domestic animals. Unfortunately, as well as having low therapeutic indices, the drugs are poor at penetrating the bone marrow, where some of the "Leishmania" amastigotes reside, and so cure of the disease – especially the visceral form – is very difficult. Elemental antimony as an antimony pill was once used as a medicine. It could be reused by others after ingestion and elimination.
In the heads of some safety matches, antimony(III) sulfide is used. Antimony-124 is used together with beryllium in neutron sources; the gamma rays emitted by antimony-124 initiate the photodisintegration of beryllium. The emitted neutrons have an average energy of 24 keV. Antimony sulfides have been shown to help stabilize the friction coefficient in automotive brake pad materials.
Antimony also is used in the making of bullets and bullet tracers. This element is also used in paint and glass art crafts and as opacifier in enamel.
Precautions.
The effects of antimony and its compounds on human and environmental health differ widely. The massive antimony metal does not affect human and environmental health. Inhalation of antimony trioxide (and similar poorly soluble Sb(III) dust particles such as antimony dust) is considered harmful and suspected of causing cancer. However, these effects are only observed with female rats and after long-term exposure to high dust concentrations. The effects are hypothesized to be attributed to inhalation of poorly soluble Sb particles leading to impaired lung clearance, lung overload, inflammation and ultimately tumour formation, not to exposure to antimony ions (OECD, 2008). Antimony chlorides are corrosive to skin. The effects of antimony are not comparable to arsenic; this might be caused by the significant differences of uptake, metabolism and excretion between arsenic and antimony.
For oral absorption, ICRP (1994) recommended values of 10% for tartar emetic and 1% for all other antimony compounds. Dermal absorption for metals is estimates at most 1% (HERAG, 2007). Inhalation absorption of antimony trioxide and other poorly soluble Sb(III) substances (such as antimony dust) is estimated at 6.8% (OECD, 2008), whereas a value <1% is derived for Sb(V) substances. Antimony(V) is not quantitatively reduced to antimony(III) in the cell, and both species exist simultaneously.
Antimony is mainly excreted from the human body via urine. Antimony and its compounds are not causing acute human health effects. Exemption is antimony potassium tartrate (‘tartar emic’), a prodrug that is intentionally used to treat leishmania patients.
Prolonged skin contact with antimony dust may cause dermatitis However, it was agreed at EU-level that the skin rashes observed are not substance-specific, but most probably due to a physical blocking of sweat ducts (ECHA/PR/09/09, Helsinki, 6 July 2009).
Antimony is incompatible with strong acids/bases and reducing agents as it might form stibine (SbH3).
The 8-h TWA is set at 0.5 mg/m3 by e.g. US OSHA and ACGIH. Antimony compounds are used as catalyst for polyethylene terephthalate (PET) production. Some studies, report minor antimony leaching from PET bottles into liquids but levels are below drinking water guidelines. Antimony concentrations in fruit juice concentrates were somewhat higher (up to 44.7 µg/L of antimony), but juices do not fall under the drinking water regulations. The drinking water guidelines are:
The TDI proposed by WHO is 6 µg antimony per kg body weight.

</doc>
<doc id="899" url="http://en.wikipedia.org/wiki?curid=899" title="Actinium">
Actinium

Actinium is a radioactive chemical element with symbol Ac (not to be confused with the abbreviation for an acetyl group) and atomic number 89, which was discovered in 1899. It was the first non-primordial radioactive element to be isolated. Polonium, radium and radon were observed before actinium, but they were not isolated until 1902. Actinium gave the name to the actinide series, a group of 15 similar elements between actinium and lawrencium in the periodic table.
A soft, silvery-white radioactive metal, actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that prevents further oxidation. As with most lanthanides and actinides, actinium assumes oxidation state +3 in nearly all its chemical compounds. Actinium is found only in traces in uranium ores as the isotope 227Ac, which decays with a half-life of 21.772 years, predominantly emitting beta particles. One tonne of uranium ore contains about 0.2 milligrams of actinium. The close similarity of physical and chemical properties of actinium and lanthanum makes separation of actinium from the ore impractical. Instead, the element is prepared, in milligram amounts, by the neutron irradiation of 226 in a nuclear reactor. Owing to its scarcity, high price and radioactivity, actinium has no significant industrial use. Its current applications include a neutron source and an agent for radiation therapy targeting cancer cells in the body.
History.
André-Louis Debierne, a French chemist, announced the discovery of a new element in 1899. He separated it from pitchblende residues left by Marie and Pierre Curie after they had extracted radium. In 1899, Debierne described the substance as similar to titanium and (in 1900) as similar to thorium. Friedrich Oskar Giesel independently discovered actinium in 1902 as a substance being similar to lanthanum and called it "emanium" in 1904. After a comparison of the substances half-lives determined by Debierne, Hariett Brooks in 1904, and Otto Hahn and Otto Sackur in 1905, Debierne's chosen name for the new element was retained because it had seniority.
Articles published in the 1970s and later suggest that Debierne's results published in 1904 conflict with those reported in 1899 and 1900. This has led some authors to advocate that Giesel alone should be credited with the discovery. A less confrontational vision of scientific discovery is proposed by Adloff. He suggests that hindsight criticism of the early publications should be mitigated by the then nascent state of radiochemistry: highlighting the prudence of Debierne's claims in the original papers, he notes that nobody can contend that Debierne's substance did not contain actinium. Debierne, who is now considered by the vast majority of historians as the discoverer, lost interest in the element and left the topic. Giesel, on the other hand, can rightfully be credited with the first preparation of radiochemically pure actinium and with the identification of its atomic number 89.
The name actinium originates from the Ancient Greek "aktis, aktinos" (ακτίς, ακτίνος), meaning beam or ray. Its symbol Ac is also used in abbreviations of other compounds that have nothing to do with actinium, such as acetyl, acetate and sometimes acetaldehyde.
Properties.
Actinium is a soft, silvery-white, radioactive, metallic element. Its estimated shear modulus is similar to that of lead. Owing to its strong radioactivity, actinium glows in the dark with a pale blue light, which originates from the surrounding air ionized by the emitted energetic particles. Actinium has similar chemical properties as lanthanum and other lanthanides, and therefore these elements are difficult to separate when extracting from uranium ores. Solvent extraction and ion chromatography are commonly used for the separation.
The first element of the actinides, actinium gave the group its name, much as lanthanum had done for the lanthanides. The group of elements is more diverse than the lanthanides and therefore it was not until 1928 that Charles Janet proposed the most significant change to Dmitri Mendeleev's periodic table since the recognition of the lanthanides, by introducing the actinides, a move suggested again in 1945 by Glenn T. Seaborg.
Actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that impedes further oxidation. As with most lanthanides and actinides, actinium exists in the oxidation state +3, and the Ac3+ ions are colorless in solutions. The oxidation state +3 originates from the 6d17s2 electronic configuration of actinium, that is it easily donates 3 electrons assuming a stable closed-shell structure of the noble gas radon. The rare oxidation state +2 is only known for actinium dihydride (AcH2).
Chemical compounds.
Only a limited number of actinium compounds are known including AcF3, AcCl3, AcBr3, AcOF, AcOCl, AcOBr, Ac2S3, Ac2O3 and AcPO4. Except for AcPO4, they are all similar to the corresponding lanthanum compounds and contain actinium in the oxidation state +3. In particular, the lattice constants of the analogous lanthanum and actinium compounds differ by only a few percent.
Here "a", "b" and "c" are lattice constants, No is space group number and "Z" is the number of formula units per unit cell. Density was not measured directly but calculated from the lattice parameters.
Oxides.
Actinium oxide (Ac2O3) can be obtained by heating the hydroxide at 500 °C or the oxalate at 1100 °C, in vacuum. It crystal lattice is isotypic with the oxides of most trivalent rare-earth metals.
Halides.
Actinium trifluoride can be produced either in solution or in solid reaction. The former reaction is carried out at room temperature, by adding hydrofluoric acid to a solution containing actinium ions. In the latter method, actinium metal is treated with hydrogen fluoride vapors at 700 °C in an all-platinum setup. Treating actinium trifluoride with ammonium hydroxide at 900–1000 °C yields oxyfluoride AcOF. Whereas lanthanum oxyfluoride can be easily obtained by burning lanthanum trifluoride in air at 800 °C for an hour, similar treatment of actinium trifluoride yields no AcOF and only results in melting of the initial product.
Actinium trichloride is obtained by reacting actinium hydroxide or oxalate with carbon tetrachloride vapors at temperatures above 960 °C. Similar to oxyfluoride, actinium oxychloride can be prepared by hydrolyzing actinium trichloride with ammonium hydroxide at 1000 °C. However, in contrast to the oxyfluoride, the oxychloride could well be synthesized by igniting a solution of actinium trichloride in hydrochloric acid with ammonia.
Reaction of aluminium bromide and actinium oxide yields actinium tribromide:
and treating it with ammonium hydroxide at 500 °C results in the oxybromide AcOBr.
Other compounds.
Actinium hydride was obtained by reduction of actinium trichloride with potassium at 300 °C, and its structure was deduced by analogy with the corresponding LaH2 hydride. The source of hydrogen in the reaction was uncertain.
Mixing monosodium phosphate (NaH2PO4) with a solution of actinium in hydrochloric acid yields white-colored actinium phosphate hemihydrate (AcPO4·0.5H2O), and heating actinium oxalate with hydrogen sulfide vapors at 1400 °C for a few minutes results in a black actinium sulfide Ac2S3. It may possibly be produced by acting with a mixture of hydrogen sulfide and carbon disulfide on actinium oxide at 1000 °C.
Isotopes.
Naturally occurring actinium is composed of one radioactive isotope; . It is a very weak beta emitter, but can readily be identified through alpha spectrometry. Thirty-six radioisotopes have been identified, the most stable being with a half-life of 21.772 years, with a half-life of 10.0 days and with a half-life of 29.37 hours. All remaining radioactive isotopes have half-lives that are less than 10 hours and the majority of them have half-lives shorter than one minute. The shortest-lived known isotope of actinium is (half-life of 69 nanoseconds) which decays through alpha decay and electron capture. Actinium also has two meta states. The most significant isotopes for chemistry are 225Ac, 227Ac, and 228Ac.
Purified comes into equilibrium with its decay products at the end of 185 days. It decays according to its 21.772-year half-life emitting mostly beta (98.8%) and some alpha particles (1.2%); the successive decay products are part of the actinium series. Owing to the low available amounts, low energy of its beta particles (46 keV) and low intensity of alpha radiation, is difficult to detect directly by its emission and it is therefore traced via its decay products. The isotopes of actinium range in atomic weight from 206 u () to 236 u ().
Occurrence and synthesis.
Actinium is found only in traces in uranium ores as 227Ac – one tonne of ore contains about 0.2 milligrams of actinium. The actinium isotope 227Ac is a transient member of the actinium series decay chain, which begins with the parent isotope 235U (or 239Pu) and ends with the stable lead isotope 207Pb. Another actinium isotope (225Ac) is transiently present in the neptunium series decay chain, beginning with 237Np (or 233U) and ending with thallium (205Tl) and near-stable bismuth (209Bi).
The low natural concentration, and the close similarity of physical and chemical properties to those of lanthanum and other lanthanides, which are always abundant in actinium-bearing ores, render separation of actinium from the ore impractical, and complete separation was never achieved. Instead, actinium is prepared, in milligram amounts, by the neutron irradiation of 226 in a nuclear reactor.
The reaction yield is about 2% of the radium weight. 227Ac can further capture neutrons resulting in small amounts of 228Ac. After the synthesis, actinium is separated from radium and from the products of decay and nuclear fusion, such as thorium, polonium, lead and bismuth. The extraction can be performed with thenoyltrifluoroacetone-benzene solution from an aqueous solution of the radiation products, and the selectivity to a certain element is achieved by adjusting the pH (to about 6.0 for actinium). An alternative procedure is anion exchange with an appropriate resin in nitric acid, which can result in a separation factor of 1,000,000 for radium and actinium vs. thorium in a two-stage process. Actinium can then be separated from radium, with a ratio of about 100, using a low cross-linking cation exchange resin and nitric acid as eluant.
225Ac was first produced artificially at the Institute for Transuranium Elements (ITU) in Germany using a cyclotron and at St George Hospital in Sydney using a linac in 2000. This rare isotope has potential applications in radiation therapy and is most efficiently produced by bombarding a radium-226 target with 20–30 MeV deuterium ions. This reaction also yields 226Ac which however decays with a half-life of 29 hours and thus does not contaminate 225Ac.
Actinium metal has been prepared by the reduction of actinium fluoride with lithium vapor in vacuum at a temperature between 1100 and 1300 °C. Higher temperatures resulted in evaporation of the product and lower ones lead to an incomplete transformation. Lithium was chosen among other alkali metals because its fluoride is most volatile.
Applications.
Owing to its scarcity, high price and radioactivity, actinium currently has no significant industrial use.
227Ac is highly radioactive and was therefore studied for use as an active element of radioisotope thermoelectric generators, for example in spacecraft. The oxide of 227Ac pressed with beryllium is also an efficient neutron source with the activity exceeding that of the standard americium-beryllium and radium-beryllium pairs. In all those applications, 227Ac (a beta source) is merely a progenitor which generates alpha-emitting isotopes upon its decay. Beryllium captures alpha particles and emits neutrons owing to its large cross-section for the (α,n) nuclear reaction:
The 227AcBe neutron sources can be applied in a neutron probe – a standard device for measuring the quantity of water present in soil, as well as moisture/density for quality control in highway construction. Such probes are also used in well logging applications, in neutron radiography, tomography and other radiochemical investigations.
225Ac is applied in medicine to produce 213 in a reusable generator or can be used alone as an agent for radiation therapy, in particular targeted alpha therapy (TAT). This isotope has a half-life of 10 days that makes it much more suitable for radiation therapy than 213Bi (half-life 46 minutes). Not only 225Ac itself, but also its decay products emit alpha particles which kill cancer cells in the body. The major difficulty with application of 225Ac was that intravenous injection of simple actinium complexes resulted in their accumulation in the bones and liver for a period of tens of years. As a result, after the cancer cells were quickly killed by alpha particles from 225Ac, the radiation from the actinium and its decay products might induce new mutations. To solve this problem, 225Ac was bound to a chelating agent, such as citrate, ethylenediaminetetraacetic acid (EDTA) or diethylene triamine pentaacetic acid (DTPA). This reduced actinium accumulation in the bones, but the excretion from the body remained slow. Much better results were obtained with such chelating agents as HEHA(1,4,7,10,13,16-hexaazacyclohexadecane-N,N',N'`,N'``,N'``',N'``'`-hexaacetic acid) or DOTA (1,4,7,10-tetraazacyclododecane-1,4,7,10-tetraacetic acid) coupled to trastuzumab, a monoclonal antibody that interferes with the HER2/neu receptor. The latter delivery combination was tested on mice and proved to be effective against leukemia, lymphoma, breast, ovarian, neuroblastoma and prostate cancers.
The medium half-life of 227Ac (21.77 years) makes it very convenient radioactive isotope in modeling the slow vertical mixing of oceanic waters. The associated processes cannot be studied with the required accuracy by direct measurements of current velocities (of the order 50 meters per year). However, evaluation of the concentration depth-profiles for different isotopes allows estimating the mixing rates. The physics behind this method is as follows: oceanic waters contain homogeneously dispersed 235U. Its decay product, 231Pa, gradually precipitates to the bottom, so that its concentration first increases with depth and then stays nearly constant. 231Pa decays to 227Ac; however, the concentration of the latter isotope does not follow the 231Pa depth profile, but instead increases toward the sea bottom. This occurs because of the mixing processes which raise some additional 227Ac from the sea bottom. Thus analysis of both 231Pa and 227Ac depth profiles allows to model the mixing behavior.
Precautions.
227Ac is highly radioactive and experiments with it are carried out in a specially designed laboratory equipped with a glove box. When actinium trichloride is administered intravenously to rats, about 33% of actinium is deposited into the bones and 50% into the liver. Its toxicity is comparable to, but slightly lower than that of americium and plutonium.

</doc>
<doc id="900" url="http://en.wikipedia.org/wiki?curid=900" title="Americium">
Americium

Americium is a radioactive transuranic chemical element with symbol Am and atomic number 95. This member of the actinide series is located in the periodic table under the lanthanide element europium, and thus by analogy was named after another continent, America.
Americium was first produced in 1944 by the group of Glenn T. Seaborg at the University of California, Berkeley. Although it is the third element in the transuranic series, it was discovered fourth, after the heavier curium. The discovery was kept secret and only released to the public in November 1945. Most americium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 100 grams of americium. It is widely used in commercial ionization chamber smoke detectors, as well as in neutron sources and industrial gauges. Several unusual applications, such as a nuclear battery or fuel for space ships with nuclear propulsion, have been proposed for the isotope 242mAm, but they are as yet hindered by the scarcity and high price of this nuclear isomer.
Americium is a relatively soft radioactive metal with silvery appearance. Its most common isotopes are 241Am and 243Am. In chemical compounds, they usually assume the oxidation state +3, especially in solutions. Several other oxidation states are known, which range from +2 to +7 and can be identified by their characteristic optical absorption spectra. The crystal lattice of solid americium and its compounds contains intrinsic defects, which are induced by self-irradiation with alpha particles and accumulate with time; this results in a drift of some material properties.
History.
Although americium was likely produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in late autumn 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Leon O. Morgan, Ralph A. James, and Albert Ghiorso. They used a 60-inch cyclotron at the University of California, Berkeley. The element was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) of the University of Chicago. Following the lighter neptunium, plutonium, and heavier curium, americium was the fourth transuranium element to be discovered. At the time, the periodic table had been restructured by Seaborg to its present layout, containing the actinide row below the lanthanide one. This led to americium being located right below its twin lanthanide element europium; it was thus by analogy named after another continent, America: "The name americium (after the Americas) and the symbol Am are suggested for the element on the basis of its position as the sixth member of the actinide rare-earth series, analogous to europium, Eu, of the lanthanide series."
The new element was isolated from its oxides in a complex, multi-step process. First plutonium-239 nitrate (239PuNO3) solution was coated on a platinum foil of about 0.5 cm2 area, the solution was evaporated and the residue was converted into plutonium dioxide (PuO2) by annealing. After cyclotron irradiation, the coating was dissolved with nitric acid, and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid. Further separation was carried out by ion exchange, yielding a certain isotope of curium. The separation of curium and americium was so painstaking that those elements were initially called by the Berkeley group as "pandemonium" (from Greek for "all demons" or "hell") and "delirium" (from Latin for "madness").
Initial experiments yielded four americium isotopes: 241Am, 242Am, 239Am and 238Am. Americium-241 was directly obtained from plutonium upon absorption of one neutron. It decays by emission of a α-particle to 237Np; the half-life of this decay was first determined as 510 ± 20 years but then corrected to 432.2 years.
The second isotope 242Am was produced upon neutron bombardment of the already-created 241Am. Upon rapid β-decay, 242Am converts into the isotope of curium 242Cm (which had been discovered previously). The half-life of this decay was initially determined at 17 hours, which was close to the presently accepted value of 16.02 h.
The discovery of americium and curium in 1944 was closely related to the Manhattan Project; the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children "Quiz Kids" five days before the official presentation at an American Chemical Society meeting on 11 November 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. After the discovery of americium isotopes 241Am and 242Am, their production and compounds were patented listing only Seaborg as the inventor. The initial americium samples weighed a few micrograms; they were barely visible and were identified by their radioactivity. The first substantial amounts of metallic americium weighing 40–200 micrograms were not prepared until 1951 by reduction of americium(III) fluoride with barium metal in high vacuum at 1100 °C.
Occurrence.
The longest-lived and most common isotopes of americium, 241Am and 243Am, have half-lives of 432.2 and 7,370 years, respectively. Therefore, any primordial americium (americium that was present on Earth during its formation) should have decayed by now.
Existing americium is concentrated in the areas used for the atmospheric nuclear weapons tests conducted between 1945 and 1980, as well as at the sites of nuclear incidents, such as the Chernobyl disaster. For example, the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), revealed high concentrations of various actinides including americium; due to military secrecy, this result was published only in 1956. Trinitite, the glassy residue left on the desert floor near Alamogordo, New Mexico, after the plutonium-based Trinity nuclear bomb test on 16 July 1945, contains traces of americium-241. Elevated levels of americium were also detected at the crash site of a US B-52 bomber, which carried four hydrogen bombs, in 1968 in Greenland.
In other regions, the average radioactivity of surface soil due to residual americium is only about 0.01 picocuries/g (0.37 mBq/g). Atmospheric americium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 1,900 times higher concentration of americium inside sandy soil particles than in the water present in the soil pores; an even higher ratio was measured in loam soils.
Americium is produced mostly artificially in small quantities, for research purposes. A tonne of spent nuclear fuel contains about 100 grams of various americium isotopes, mostly 241Am and 243Am. Their prolonged radioactivity is undesirable for the disposal, and therefore americium, together with other long-lived actinides, have to be neutralized. The associated procedure may involve several steps, where americium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure is well known as nuclear transmutation, but it is still being developed for americium.
A few atoms of americium can be produced by neutron capture reactions and beta decay in very highly concentrated uranium-bearing deposits.
Synthesis and extraction.
Isotope nucleosyntheses.
Americium has been produced in small quantities in nuclear reactors for decades, and kilograms of its 241Am and 243Am isotopes have been accumulated by now. Nevertheless, since it was first offered for sale in 1962, its price, about 1,500 USD per gram of 241Am, remains almost unchanged owing to the very complex separation procedure. The heavier isotope 243Am is produced in much smaller amounts; it is thus more difficult to separate, resulting in a higher cost of the order 100,000–160,000 USD/g.
Americium is not synthesized directly from uranium – the most common reactor material – but from the plutonium isotope 239Pu. The latter needs to be produced first, according to the following nuclear process:
The capture of two neutrons by 239Pu (a so-called (n,γ) reaction), followed by a β-decay, results in 241Am:
The plutonium present in spent nuclear fuel contains about 12% of 241Pu. Because it spontaneously converts to 241Am, 241Pu can be extracted and may be used to generate further 241Am. However, this process is rather slow: half of the original amount of 241Pu decays to 241Am after about 15 years, and the 241Am amount reaches a maximum after 70 years.
The obtained 241Am can be used for generating heavier americium isotopes by further neutron capture inside a nuclear reactor. In a light water reactor (LWR), 79% of 241Am converts to 242Am and 10% to its nuclear isomer 242mAm:
Americium-242 has a half-life of only 16 hours, which makes its further up-conversion to 243Am, extremely inefficient. The latter isotope is produced instead in a process where 239Pu captures four neutrons under high neutron flux:
Metal generation.
Most synthesis routines yield a mixture of different actinide isotopes in oxide forms, from which isotopes of americium need to be separated. In a typical procedure, the spent reactor fuel (e.g. MOX fuel) is dissolved in nitric acid, and the bulk of uranium and plutonium is removed using a PUREX-type extraction (Plutonium –URanium EXtraction) with tributyl phosphate in a hydrocarbon. The lanthanides and remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction, to give, after stripping, a mixture of trivalent actinides and lanthanides. Americium compounds are then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. A large amount of work has been done on the solvent extraction of americium. For example, a recent EU funded project codenamed "EUROPART" studied triazines and other compounds as potential extraction agents. "Bis"-triazinyl bipyridine complex has been recently proposed as such reagent as highly selective to americium (and curium). Separation of americium from the highly similar curium can be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone, at elevated temperatures. Both Am and Cm are mostly present in solutions in the +3 valence state; whereas curium remains unchanged, americium oxidizes to soluble Am(IV) complexes which can be washed away.
Metallic americium is obtained by reduction from its compounds. Americium(III) fluoride was first used for this purpose. The reaction was conducted using elemental barium as reducing agent in a water- and oxygen-free environment inside an apparatus made of tantalum and tungsten.
An alternative is the reduction of americium dioxide by metallic lanthanum or thorium:
Physical properties.
In the periodic table, americium is located to the right of plutonium, to the left of curium, and below the lanthanide europium, with which it shares many similarities in physical and chemical properties. Americium is a highly radioactive element. When freshly prepared, it has a silvery-white metallic lustre, but then slowly tarnishes in air. With a density of 12 g/cm3, americium is less dense than both curium (13.52 g/cm3) and plutonium (19.8 g/cm3); but has a higher density than europium (5.264 g/cm3)—mostly because of its higher atomic mass. Americium is relatively soft and easily deformable and has a significantly lower bulk modulus than the actinides before it: Th, Pa, U, Np and Pu. Its melting point of 1173 °C is significantly higher than that of plutonium (639 °C) and europium (826 °C), but lower than for curium (1340 °C).
At ambient conditions, americium is present in its most stable α form which has a hexagonal crystal symmetry, and a space group P63/mmc with lattice parameters "a" = 346.8 pm and "c" = 1124 pm, and four atoms per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum and several actinides such as α-curium. The crystal structure of americium changes with pressure and temperature. When compressed at room temperature to 5 GPa, α-Am transforms to the β modification, which has a face-centered cubic ("fcc") symmetry, space group Fmm and lattice constant "a" = 489 pm. This "fcc" structure is equivalent to the closest packing with the sequence ABC. Upon further compression to 23 GPa, americium transforms to an orthorhombic γ-Am structure similar to that of α-uranium. There are no further transitions observed up to 52 GPa, except for an appearance of a monoclinic phase at pressures between 10 and 15 GPa. There is no consistency on the status of this phase in the literature, which also sometimes lists the α, β and γ phases as I, II and III. The β-γ transition is accompanied by a 6% decrease in the crystal volume; although theory also predicts a significant volume change for the α-β transition, it is not observed experimentally. The pressure of the α-β transition decreases with increasing temperature, and when α-americium is heated at ambient pressure, at 770 °C it changes into an "fcc" phase which is different from β-Am, and at 1075 °C it converts to a body-centered cubic structure. The pressure-temperature phase diagram of americium is thus rather similar to those of lanthanum, praseodymium and neodymium.
As with many other actinides, self-damage of the crystal lattice due to alpha-particle irradiation is intrinsic to americium. It is especially noticeable at low temperatures, where the mobility of the produced lattice defects is relatively low, by broadening of X-ray diffraction peaks. This effect makes somewhat uncertain the temperature of americium and some of its properties, such as electrical resistivity. So for americium-241, the resistivity at 4.2 K increases with time from about 2 µOhm·cm to 10 µOhm·cm after 40 hours, and saturates at about 16 µOhm·cm after 140 hours. This effect is less pronounced at room temperature, due to annihilation of radiation defects; also heating to room temperature the sample which was kept for hours at low temperatures restores its resistivity. In fresh samples, the resistivity gradually increases with temperature from about 2 µOhm·cm at liquid helium to 69 µOhm·cm at room temperature; this behavior is similar to that of neptunium, uranium, thorium and protactinium, but is different from plutonium and curium which show a rapid rise up to 60 K followed by saturation. The room temperature value for americium is lower than that of neptunium, plutonium and curium, but higher than for uranium, thorium and protactinium.
Americium is paramagnetic in a wide temperature range, from that of liquid helium, to room temperature, and above. This behavior is markedly different from that of its neighbor curium which exhibit antiferromagnetic transition at 52 K. The thermal expansion coefficient of americium is slightly anisotropic and amounts to (7.5 ± 0.2)/°C along the shorter "a" axis and (6.2 ± 0.4)/°C for the longer "c" hexagonal axis. The enthalpy of dissolution of americium metal in hydrochloric acid at standard conditions is −620.6 ± 1.3 kJ/mol, from which the standard enthalpy change of formation (Δf"H"°) of aqueous Am3+ ion is −621.2 ± 2.0 kJ/mol−1. The standard potential Am3+/Am0 is −2.08 ± 0.01 V.
Chemical properties.
Americium readily reacts with oxygen and dissolves well in acids. The most common oxidation state for americium is +3, in which americium compounds are rather stable against oxidation and reduction. In this sense, americium is chemically similar to most lanthanides. The trivalent americium forms insoluble fluoride, oxalate, iodate, hydroxide, phosphate and other salts. Other oxidation states have been observed between +2 and +7, which is the widest range among the actinide elements. Their color in aqueous solutions varies as follows: Am3+ (colorless to yellow-reddish), Am4+ (yellow-reddish), AmV; (yellow), AmVI (brown) and AmVII (dark green). All oxidation states have their characteristic optical absorption spectra, with a few sharp peaks in the visible and mid-infrared regions, and the position and intensity of these peaks can be converted into the concentrations of the corresponding oxidation states. For example, Am(III) has two sharp peaks at 504 and 811 nm, Am(V) at 514 and 715 nm, and Am(VI) at 666 and 992 nm.
Americium compounds with oxidation state +4 and higher are strong oxidizing agents, comparable in strength to the permanganate ion () in acidic solutions. Whereas the Am4+ ions are unstable in solutions and readily convert to Am3+, the +4 oxidation state occurs well in solids, such as americium dioxide (AmO2) and americium(IV) fluoride (AmF4).
All pentavalent and hexavalent americium compounds are complex salts such as KAmO2F2, Li3AmO4 and Li6AmO6, Ba3AmO6, AmO2F2. These high oxidation states Am(IV), Am(V) and Am(VI) can be prepared from Am(III) by oxidation with ammonium persulfate in dilute nitric acid, with silver(I) oxide in perchloric acid, or with ozone or sodium persulfate in sodium carbonate solutions. The pentavalent oxidation state of americium was first observed in 1951. It is present in aqueous solution in the form of ions (acidic) or ions (alkaline) which are however unstable and subject to several rapid disproportionation reactions:
Chemical compounds.
Oxygen compounds.
Three americium oxides are known, with the oxidation states +2 (AmO), +3 (Am2O3) and +4 (AmO2). Americium(II) oxide was prepared in minute amounts and has not been characterized in details. Americium(III) oxide is a red-brown solid with a melting point of 2205 °C. Americium(IV) oxide is the main form of solid americium which is used in nearly all its applications. As most other actinide dioxides, it is a black solid with a cubic (fluorite) crystal structure.
The oxalate of americium(III), vacuum dried at room temperature, has the chemical formula Am2(C2O4)3·7H2O. Upon heating in vacuum, it loses water at 240 °C and starts decomposing into AmO2 at 300 °C, the decomposition completes at about 470 °C. The initial oxalate dissolves in nitric acid with the maximum solubility of 0.25 g/L.
Halides.
Halides of americium are known for the oxidation states +2, +3 and +4, where the +3 is most stable, especially in solutions.
Reduction of Am(III) compounds with sodium amalgam yields Am(II) salts – the black halides AmCl2, AmBr2 and AmI2. They are very sensitive to oxygen and oxidize in water, releasing hydrogen and converting back to the Am(III) state. Specific lattice constants are:
They can also be prepared by reacting metallic americium with an appropriate mercury halide HgX2, where X = Cl, Br or I:
Americium(III) fluoride (AmF3) is poorly soluble and precipitates upon reaction of Am3+ and fluoride ions in weak acidic solutions:
The tetravalent americium(IV) fluoride (AmF4) is obtained by reacting solid americium(III) fluoride with molecular fluorine:
Another known form of solid tetravalent americium chloride is KAmF5. Tetravalent americium has also been observed in the aqueous phase. For this purpose, black Am(OH)4 was dissolved in 15-M NH3F with the americium concentration of 0.01 M. The resulting reddish solution had a characteristic optical absorption spectrum which is similar to that of AmF4 but differed from other oxidation states of americium. Heating the Am(IV) solution to 90 °C did not result in its disproportionation or reduction, however a slow reduction was observed to Am(III) and assigned to self-irradiation of americium by alpha particles.
Most americium(III) halides form hexagonal crystals with slight variation of the color and exact structure between the halogens. So, chloride (AmCl3) is reddish and has a structure isotypic to uranium(III) chloride (space group P63/m) and the melting point of 715 °C. The fluoride is isotypic to LaF3 (space group P63/mmc) and the iodide to BiI3 (space group R). The bromide is an exception with the orthorhombic PuBr3-type structure and space group Cmcm. Crystals of americium hexahydrate (AmCl3·6H2O) can be prepared by dissolving americium dioxide in hydrochloric acid and evaporating the liquid. Those crystals are hygroscopic and have yellow-reddish color and a monoclinic crystal structure.
Oxyhalides of americium in the form AmVIO2X2, AmVO2X, AmIVOX2 and AmIIIOX can be obtained by reacting the corresponding americium halide with oxygen or Sb2O3, and AmOCl can also be produced by vapor phase hydrolysis:
Chalcogenides and pnictides.
The known chalcogenides of americium include the sulfide AmS2, selenides AmSe2 and Am3Se4, and tellurides Am2Te3 and AmTe2. The pnictides of americium (243Am) of the AmX type are known for the elements phosphorus, arsenic, antimony and bismuth. They crystallize in the rock-salt lattice.
Silicides and borides.
Americium monosilicide (AmSi) and "disilicide" (nominally AmSix with: 1.87 < x < 2.0) were obtained by reduction of americium(III) fluoride with elementary silicon in vacuum at 1050 °C (AmSi) and 1150−1200 °C (AmSix). AmSi is a black solid isomorphic with LaSi, it has an orthorhombic crystal symmetry. AmSix has a bright silvery lustre and a tetragonal crystal lattice (space group "I"41/amd), it is isomorphic with PuSi2 and ThSi2. Borides of americium include AmB4 and AmB6. The tetraboride can be obtained by heating an oxide or halide of americium with magnesium diboride in vacuum or inert atmosphere.
Organoamericium compounds.
Analogous to uranocene, americium forms an organometallic compound with two cyclooctatetraene ligands, that is (η8-C8H8)2Am. It also makes trigonal (η5-C5H5)3Am complexes with three cyclopentadienyl rings.
Formation of the complexes of the type Am(n-C3H7-BTP)3, where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-C3H7-BTP and Am3+ ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with americium and therefore are useful in its selective separation from lanthanides and another actinides.
Biological aspects.
Americium is an artificial element of recent origin, and thus does not have a biological requirement. It has been proposed to use bacteria for removal of americium and other heavy metals from rivers and streams. Thus, Enterobacteriaceae of the genus "Citrobacter" precipitate americium ions from aqueous solutions, binding them into a metal-phosphate complex at their cell walls. Several studies have been reported on the biosorption and bioaccumulation of americium by bacteria and fungi.
Fission.
The isotope 242m1Am (half-life 141 years) has the largest cross sections for absorption of thermal neutrons (5,700 barns), that results in a small critical mass for a sustained nuclear chain reaction. The critical mass for a bare 242m1Am sphere is about 9–14 kg (the uncertainty results from insufficient knowledge of its material properties). It can be lowered to 3–5 kg with a metal reflector and should become even smaller with a water reflector. Such small critical mass is favorable for portable nuclear weapons, but those based on 242m1Am are not known yet, probably because of its scarcity and high price. The critical masses of two other readily available isotopes, 241Am and 243Am, are relatively high – 57.6 to 75.6 kg for 241Am and 209 kg for 243Am. Scarcity and high price yet hinder application of americium as a nuclear fuel in nuclear reactors.
There are proposals of very compact 10-kW high-flux reactors using as little as 20 grams of 242m1Am. Such low-power reactors would be relatively safe to use as neutron sources for radiation therapy in hospitals.
Isotopes.
About 19 isotopes and 8 nuclear isomers are known for americium. There are two long-lived alpha-emitters, 241Am and 243Am with half-lives of 432.2 and 7,370 years, respectively, and the nuclear isomer 242m1Am has a long half-life of 141 years. The half-lives of other isotopes and isomers range from 0.64 microseconds for 245m1Am to 50.8 hours for 240Am. As with most other actinides, the isotopes of americium with odd number of neutrons have relatively high rate of nuclear fission and low critical mass.
Americium-241 decays to 237Np emitting alpha particles of 5 different energies, mostly at 5.486 MeV (85.2%) and 5.443 MeV (12.8%). Because many of the resulting states are metastable, they also emit gamma rays with the discrete energies between 26.3 and 158.5 keV.
Americium-242 is a short-lived isotope with a half-life of 16.02 h. It mostly (82.7%) converts by β-decay to 242Cm, but also by electron capture to 242Pu (17.3%). Both 242Cm and 242Pu transform via nearly the same decay chain through 238Pu down to 234U.
Nearly all (99.541%) of 242m1Am decays by internal conversion to 242Am and the remaining 0.459% by α-decay to 238Np. The latter breaks down to 238Pu and then to 234U.
Americium-243 transforms by α-emission into 239Np, which converts by β-decay to 239Pu, and the 239Pu changes into 235U by emitting an α-particle.
Applications.
Ionization detectors.
Americium is the only synthetic element to have found its way into the household, where one common type of smoke detector uses 241Am in the form of americium dioxide as its source of ionizing radiation. This isotope is preferred over 226Ra because it emits 5 times more alpha particles and relatively little harmful gamma radiation. The amount of americium in a typical new smoke detector is 1 microcurie (37 kBq) or 0.28 microgram. This amount declines slowly as the americium decays into neptunium-237, a different transuranic element with a much longer half-life (about 2.14 million years). With its half-life of 432.2 years, the americium in a smoke detector includes about 3% neptunium after 19 years, and about 5% after 32 years. The radiation passes through an ionization chamber, an air-filled space between two electrodes, and permits a small, constant current between the electrodes. Any smoke that enters the chamber absorbs the alpha particles, which reduces the ionization and affects this current, triggering the alarm. Compared to the alternative optical smoke detector, the ionization smoke detector is cheaper and can detect particles which are too small to produce significant light scattering; however, it is more prone to false alarms.
Radionuclide.
As 241Am has a significantly longer half-life than 238Pu (432.2 years vs. 87 years), it has been proposed as an active element of radioisotope thermoelectric generators, for example in spacecraft. Although americium produces less heat and electricity – the power yield is 114.7 mW/g for 241Am and 6.31 mW/g for 243Am (cf. 390 mW/g for 238Pu) – and its radiation poses more threat to humans owing to neutron emission, the European Space Agency is planning to use americium for its space probes.
Another proposed space-related application of americium is a fuel for space ships with nuclear propulsion. It relies on the very high rate of nuclear fission of 242mAm, which can be maintained even in a micrometer-thick foil. Small thickness avoids the problem of self-absorption of emitted radiation. This problem is pertinent to uranium or plutonium rods, in which only surface layers provide alpha-particles. The fission products of 242mAm can either directly propel the spaceship or they can heat up a thrusting gas; they can also transfer their energy to a fluid and generate electricity through a magnetohydrodynamic generator.
One more proposal which utilizes the high nuclear fission rate of 242mAm is a nuclear battery. Its design relies not on the energy of the emitted by americium alpha particles, but on their charge, that is the americium acts as the self-sustaining "cathode". A single 3.2 kg 242mAm charge of such battery could provide about 140 kW of power over a period of 80 days. With all the potential benefits, the current applications of 242mAm are as yet hindered by the scarcity and high price of this nuclear isomer.
Neutron source.
The oxide of 241Am pressed with beryllium is an efficient neutron source. Here americium acts as the alpha source, and beryllium produces neutrons owing to its large cross-section for the (α,n) nuclear reaction:
The most widespread use of 241AmBe neutron sources is a neutron probe – a device used to measure the quantity of water present in soil, as well as moisture/density for quality control in highway construction. 241Am neutron sources are also used in well logging applications, as well as in neutron radiography, tomography and other radiochemical investigations.
Production of other elements.
Americium is a starting material for the production of other transuranic elements and transactinides – for example, 82.7% of 242Am decays to 242Cm and 17.3% to 242Pu. In the nuclear reactor, 242Am is also up-converted by neutron capture to 243Am and 244Am, which transforms by β-decay to 244Cm:
Irradiation of 241Am by 12C or 22Ne ions yields the isotopes 247Es (einsteinium) or 260Db (dubnium), respectively. Furthermore, the element berkelium (243Bk isotope) had been first intentionally produced and identified by bombarding 241Am with alpha particles, in 1949, by the same Berkeley group, using the same 60-inch cyclotron. Similarly, nobelium was produced at the Joint Institute for Nuclear Research, Dubna, Russia, in 1965 in several reactions, one of which included irradiation of 243Am with 15N ions. Besides, one of the synthesis reactions for lawrencium, discovered by scientists at Berkeley and Dubna, included bombardment of 243Am with 18O.
Spectrometer.
Americium-241 has been used as a portable source of both gamma rays and alpha particles for a number of medical and industrial uses. The 60-keV gamma ray emissions from 241Am in such sources can be used for indirect analysis of materials in radiography and X-ray fluorescence spectroscopy, as well as for quality control in fixed nuclear density gauges and nuclear densometers. For example, the element has been employed to gauge glass thickness to help create flat glass. Americium-241 is also suitable for calibration of gamma-ray spectrometers in the low-energy range, since its spectrum consists of nearly a single peak and negligible Compton continuum (at least three orders of magnitude lower intensity). Americium-241 gamma rays were also used to provide passive diagnosis of thyroid function. This medical application is however obsolete.
Health concerns.
As a highly radioactive element, americium and its compounds must be handled only in an appropriate laboratory under special arrangements. Although most americium isotopes predominantly emit alpha particles which can be blocked by thin layers of common materials, many of the daughter products emit gamma-rays and neutrons which have a long penetration depth.
If consumed, americium is excreted within a few days and only 0.05% is absorbed in the blood. From there, roughly 45% of it goes to the liver and 45% to the bones, and the remaining 10% is excreted. The uptake to the liver depends on the individual and increases with age. In the bones, americium is first deposited over cortical and trabecular surfaces and slowly redistributes over the bone with time. The biological half-life of 241Am is 50 years in the bones and 20 years in the liver, whereas in the gonads (testicles and ovaries) it remains permanently; in all these organs, americium promotes formation of cancer cells as a result of its radioactivity.
Americium often enters landfills from discarded smoke detectors. The rules associated with the disposal of smoke detectors are relaxed in most jurisdictions. In the U.S., the "Radioactive Boy Scout" David Hahn was able to concentrate americium from smoke detectors after managing to buy a hundred of them at remainder prices and also stealing a few. There have been cases of humans being contaminated with americium, the worst case being that of Harold McCluskey, who at the age of 64 was exposed to 500 times the occupational standard for americium-241 as a result of an explosion in his lab. McCluskey died at the age of 75, not as a result of exposure, but of a heart disease which he had before the accident.

</doc>
<doc id="901" url="http://en.wikipedia.org/wiki?curid=901" title="Astatine">
Astatine

Astatine is a radioactive chemical element with the chemical symbol At and atomic number 85. It occurs on Earth only as the result of the radioactive decay of certain heavier elements. All of its isotopes are short-lived; the most stable is astatine-210, with a half-life of 8.1 hours. Accordingly, much less is known about astatine than most other elements. The observed properties are consistent with it behaving as a heavier analog of iodine; many other properties have been estimated based on this resemblance.
Elemental astatine has never been viewed, because a mass large enough to be seen (by the naked human eye) would be immediately vaporized by the heat generated by its own radioactivity. Astatine may be dark, or it may have a metallic appearance and be a semiconductor, or it may even be a metal. It is likely to have a much higher melting point than iodine, on par with those of bismuth and polonium. Chemically, astatine behaves more or less as a halogen (the group including chlorine and fluorine), and would be expected to form ionic astatides with alkali or alkaline earth metals; it is known to form covalent compounds with nonmetals, including other halogens. It does, however, also have a notable cationic chemistry that distinguishes it from the lighter halogens. The second longest-lived isotope of astatine, astatine-211, is the only one currently having any commercial application, being employed in medicine to diagnose and treat some diseases via its emission of alpha particles (helium-4 nuclei). Only extremely small quantities are used, however, due to its intense radioactivity.
The element was first produced by Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè at the University of California, Berkeley in 1940. They named the element "astatine", a name coming from the great instability of the synthesized matter (the source Greek word αστατος ("astatos") means "unstable"). Three years later it was found in nature, although it is the least abundant element in the Earth's crust among the non-transuranic elements, with a total existing amount of much less than one gram at any given time. Six astatine isotopes, with mass numbers of 214 to 219, are present in nature as the products of various decay routes of heavier elements, but neither the most stable isotope of astatine (with mass number 210) nor astatine-211 (which is used in medicine) is produced naturally.
Characteristics.
Astatine is an extremely radioactive element; all its isotopes have half-lives of less than 12 hours, decaying into bismuth, polonium, radon, or other astatine isotopes. Among the first 101 elements in the periodic table, only francium is less stable.
The bulk properties of astatine are not known with any great degree of certainty. Research is limited by its short half-life, which prevents the creation of weighable quantities. A visible piece of astatine would be immediately and completely vaporized due to the heat generated by its intense radioactivity. Astatine is usually classified as either a nonmetal or a metalloid. However, metal formation for condensed-phase astatine has also been suggested.
Physical.
Most of the physical properties of astatine have been estimated (by interpolation or extrapolation), using various theoretically grounded or empirically derived methods. As an example, heavier halogens are darker than are halogens of lesser atomic weight – fluorine is nearly colorless, chlorine is bright green, bromine is brown, and iodine is dark gray/violet. Astatine is sometimes described as being a black solid (assuming that it follows this trend), or as having a metallic appearance (if it is a metalloid or a metal). The melting and boiling points of astatine are also expected to follow the trend seen in the halogen series, increasing with atomic number. On this basis, the melting and boiling points are estimated to be , respectively. However, some experimental evidence suggests astatine may have lower melting and boiling points than those implied by the halogen trend. Astatine sublimes less readily than does iodine, having a lower vapor pressure. Even so, half of a given amount of astatine will vaporize in an hour if put on a clean glass surface at room temperature.
The crystalline structure of solid astatine is unknown. Evidence for (or against) the existence of diatomic astatine (At2) is sparse and inconclusive. Some sources state that At2 does not exist, or at least has never been observed, while other sources assert or imply its existence. Despite this controversy, many properties of diatomic astatine have been predicted; for example, its density would be 6.2–6.5 g/cm3.
Chemical.
Many chemical properties of astatine have been determined using tracer studies on extremely dilute astatine solutions. Most known properties – such as anion formation – are in line with other halogens. However, astatine has a few metallic characteristics as well, such as plating out on a cathode, coprecipitating with metal sulfides in hydrochloric acid, and forming a cation in strong acidic solutions.
Astatine has an electronegativity of 2.2 on the revised Pauling scale. This is lower than that of iodine (2.66) and the same as that of hydrogen. However, in hydrogen astatide (HAt) the negative charge is predicted to be on the hydrogen atom, implying that this compound should instead be referred to as astatine hydride because the electronegativity of astatine on the Allred-Rochow scale (1.9) is less than that of hydrogen (2.2).
Compounds.
Astatine is the least reactive of the halogens, being less reactive than iodine; however, multiple compounds of astatine have been synthesized in microscopic amounts and studied as intensively as possible before their inevitable radioactive disintegration. The reactions involved are normally tested with dilute solutions of astatine mixed with larger amounts of iodine. The iodine acts as a carrier, ensuring that there is sufficient material for laboratory techniques (such as filtration and precipitation) to work.
The formation of an astatine compound with hydrogen – usually referred to as hydrogen astatide – was noted by the pioneers of astatine chemistry. As mentioned earlier, there are grounds for referring to this compound as astatine hydride instead – astatine is easily oxidized, acidification by (dilute) nitric acid gives the At0 or At+ forms, and the addition of silver(I) then precipitates astatine, only partially as silver(I) astatide (AgAt) (or not at all). Iodine, in contrast, is not oxidized, and precipitates readily as silver(I) iodide.
Only a few metal astatides have been reported, including those of sodium, palladium, silver, and lead. Some characteristic properties of silver astatide, and the known and hypothetical alkali and alkaline earth astatides, have been estimated by extrapolation from other silver or alkali or alkaline earth halides.
Astatine is known to react with its lighter homologues iodine, bromine, and chlorine in the vapor state; these reactions produce diatomic interhalogen compounds with formulas AtI, AtBr, and AtCl. The first two compounds may also be produced in water – astatine reacts with iodine/iodide solution to form AtI, whereas AtBr requires (aside from astatine) an iodine/iodine monobromide/bromide solution. The excess of iodides or bromides may lead to and ions, or in a chloride solution, they may produce species like or via equilibrium-balanced reactions with the chlorides. Oxidation of the element with dichromate (in nitric acid solution) showed that adding chloride turned the astatine into a molecule likely to be either AtCl or AtOCl. Similarly, or may be produced. In a plasma ion source mass spectrometer, the similar ions [AtI]+, [AtBr]+, and [AtCl]+ have been formed by introducing lighter halogen vapors into a helium-filled cell containing astatine, supporting the existence of stable neutral molecules in the plasma ion state. No astatine fluorides have been discovered as yet. Their absence has been speculatively attributed to the extreme reactivity of such compounds, including the reaction of an initially formed fluoride with the walls of the glass container to form a non-volatile product. Thus, although the synthesis of an astatine fluoride is thought to be possible, it may require a liquid halogen fluoride solvent, as has already been used for the characterization of radon fluorides.
With oxygen, there is evidence for the existence of the species AtO–, , and AtO+ in aqueous solution, formed by the reaction of astatine with an oxidant such as elemental bromine or (in the last case) by sodium persulfate in a solution of perchloric acid. The well characterized anion can be obtained by, for example, the oxidation of astatine with potassium hypochlorite in a solution of potassium hydroxide. Further oxidation, such as by xenon difluoride (in a hot alkaline solution) or periodate (in a neutral or alkaline solution), yields the perastatate ion ; however, this is only stable in neutral or alkaline solutions. Astatine is also thought to be capable of forming cationic salts with oxyanions such as iodate or dichromate; this is based on the observation that, in acidic solutions, monovalent or intermediate positive states of astatine coprecipitate with the insoluble salts of metal cations such as silver(I) iodate or thallium(I) dichromate.
Astatine may form bonds to the other chalcogens; these include S7At+ and with sulfur, a coordination selenourea compound with selenium, and an astatine–tellurium colloid with tellurium. Additionally, astatine is known to bind to nitrogen, lead, and boron under the proper conditions.
Carbon tetraastatide (CAt4) is known. Astatine can replace a hydrogen atom in benzene to form C6H5At; this may be oxidized to C6H5AtCl2 by chlorine. By treating this compound with an alkaline solution of hypochlorite, C6H5AtO2 can be produced.
History.
In 1869, when Dmitri Mendeleev published his periodic table, the space under iodine was empty; after Niels Bohr established the physical basis of the classification of chemical elements, it was suggested that the fifth halogen belonged there. Before its officially recognized discovery, it was called "eka-iodine" (from Sanskrit "eka" – "one") to imply it was one space under iodine (in the same manner as eka-silicon, eka-boron, and others). Scientists tried to find it in nature; given its rarity, these attempts resulted in a number of false discoveries.
The first claimed discovery of eka-iodine was made by Fred Allison and his associates at the Alabama Polytechnic Institute (now Auburn University) in 1931. The discoverers named element 85 "alabamine", and assigned it the symbol Ab, designations that were used for a few years afterward. In 1934, however, H. G. MacPherson of University of California, Berkeley disproved Allison's method and the validity of his discovery. This erroneous discovery was followed by another claim in 1937, by the chemist Rajendralal De. Working in Dacca in British India (now Dhaka in Bangladesh), he chose the name "dakin" for element 85, which he claimed to have isolated as the thorium series equivalent of Radium F (polonium-210) in the radium series. The properties he reported for dakin do not correspond to those of astatine, and the true identity of dakin is not known.
In 1940, the Swiss chemist Walter Minder announced the discovery of element 85 as the beta decay product of Radium A (polonium-218), choosing the name "helvetium" (from , "Switzerland"). However, Berta Karlik and Traude Bernert were unsuccessful in reproducing his experiments, and subsequently attributed Minder's results to contamination of his radon stream (radon-222 is the parent isotope of polonium-218). In 1942, Minder, in collaboration with the English scientist Alice Leigh-Smith, announced the discovery of another isotope of element 85, presumed to be the product of Thorium A (polonium-216) beta decay. They named this substance "anglo-helvetium", but Karlik and Bernert were again unable to reproduce these results.
In 1940, Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè finally isolated the element at the University of California, Berkeley. Instead of searching for the element in nature, the scientists created it by bombarding bismuth-209 with alpha particles in a cyclotron (particle accelerator) to produce, after emission of two neutrons, astatine-211. The name "astatine" comes from the Greek word ἄστατος ("ástatos", meaning "unstable"), due to its propensity for radioactive decay (later, all isotopes of the element were shown to be unstable), together with the ending "-ine", found in the names of the four previously discovered halogens. Three years later, astatine was found as a product of naturally occurring decay chains by Karlik and Bernert. Since then, astatine has been determined to be in three out of the four natural decay chains.
Isotopes.
There are 32 known isotopes of astatine, with atomic masses (mass numbers) of 191 and 193–223. No stable or even long-lived astatine isotope is known, and no such isotope is expected to exist.
Astatine has 23 nuclear isomers, which are nuclei with one or more nucleons (protons or neutrons) in an excited state. A nuclear isomer may also be called a "meta-state", meaning the system has more internal energy than the "ground state" (the state with the lowest possible internal energy), making the former likely to decay into the latter. There may be more than one isomer for each isotope. The most stable of these nuclear isomers is astatine-202m1, which has a half-life of about 3 minutes, longer than those of all the ground states except for those of isotopes 203–211 and 220. The least stable one is astatine-214m1; its half-life of 265 nanoseconds is shorter than those of all ground states except that of astatine-213.
Astatine's alpha decay energies follow the same trend as for other heavy elements. Lighter astatine isotopes have quite high energies of alpha decay, which become lower as the nuclei become heavier. Astatine-211, however, has a significantly higher energy than the previous isotope, because it has a nucleus with 126 neutrons, and 126 is a magic number corresponding to a filled neutron shell. Despite having a similar half-life to the previous isotope (8.1 hours for astatine-210 and 7.2 hours for astatine-211), the alpha decay probability is much higher for the latter: 41.81% against only 0.18%. The two following isotopes release even more energy, with astatine-213 releasing the highest amount of energy of all astatine isotopes. For this reason, it is the shortest-lived astatine isotope. Even though heavier astatine isotopes release less energy, no long-lived astatine isotope exists, due to the increasing role of beta decay (electron emission). This decay mode is especially important for astatine; as early as 1950 it was postulated that the element has no beta-stable isotopes (i.e., ones that do not beta decay at all). Beta decay modes have been found for all astatine isotopes except astatine-213, astatine-214, astatine-215, and astatine-216m. Astatine-210 and lighter isotopes exhibit beta plus decay (positron emission), astatine-216 and heavier isotopes exhibit beta (minus) decay, and astatine-212 decays via both modes, while astatine-211 undergoes electron capture instead.
The most stable isotope is astatine-210, which has a half-life of 8.1 hours. This isotope's primary decay mode is beta plus decay to the relatively long-lived (in comparison to astatine isotopes) alpha emitter polonium-210. In total, only five isotopes have half-lives exceeding one hour (those with mass numbers between 207 and 211). The least stable ground state isotope is astatine-213, with a half-life of 125 nanoseconds. It alpha decays to the extremely long-lived (in practice, stable) bismuth-209.
Natural occurrence.
Astatine is the rarest naturally occurring element that is not a transuranic element, with the total amount in Earth's crust estimated to be less than one gram at any given time. Any astatine that was present at the Earth's formation has long since decayed, and the minute amounts of astatine existing currently have formed through the decay of heavier elements. While it was previously thought to be the rarest element occurring on the Earth, astatine has lost this status to berkelium, atoms of which can be produced by neutron capture reactions and beta decay in very highly concentrated uranium-bearing deposits.
Six astatine isotopes occur naturally (astatine-214 to astatine-219). Because of their short half-lives, they are found only in trace amounts. There is no data indicating that astatine occurs in stars.
Four out of these isotopes (astatine-215, astatine-217, astatine-218, and astatine-219) are found due to their production in major natural decay chains. Francium-223, the father isotope of astatine-219, alpha decays with a probability of only 0.006%, making this astatine isotope extremely rare even compared to the other astatine isotopes; this is in spite of its half-life being the longest of the natural astatine isotopes (56 seconds). Astatine-219 decays to polonium-215, which itself beta decays to astatine-215 with an even smaller probability of 0.00023%. The entirety of North and South America combined, considered to a depth of 16 kilometers (10 miles), contain only about one trillion astatine-215 atoms at any given time. Astatine-218 is found in nature as a result of polonium-218 beta decay; as with francium-223 and polonium-215, decay to an astatine isotope is not the primary decay mode. However, the astatine-217 isotope has a straight chain leading directly to astatine; its father isotope (francium-221) decays exclusively to this nuclide. Given that its fathers, grandfathers, and so on each decay exclusively to only one nuclide, this gives only one possible way for the starting nuclide in the neptunium series (neptunium-237) to decay – via eventual production of astatine-217.
The isotopes with mass numbers 214 through 216 are found as the result of triple alpha decay of the naturally present protactinium isotopes protactinium-226, protactinium-227, and protactinium-228. However, these isotopes are extremely rare, so much so that they are often not cited as natural astatine isotopes.
Synthesis.
Formation.
Astatine was first produced by bombarding bismuth-209 with energetic alpha particles, and this is still the major route used to create the relatively long-lived isotopes astatine-209 through astatine-211. Astatine is only produced in microscopic quantities, with modern techniques allowing production runs of 2 terabecquerels (about 25 micrograms).
The most important isotope is astatine-211, the only one to currently have a commercial use. To produce the bismuth target, the metal is sputtered onto a gold, copper, or aluminium surface at 50 to 100 milligrams per square centimeter. The bismuth layer, or alternatively bismuth oxide, is forcibly fused with a copper plate. The target is kept under a chemically neutral nitrogen atmosphere, and is cooled with water to prevent premature astatine vaporization. In a particle accelerator, such as a cyclotron, alpha particles (helium-4 nuclei) are collided with the bismuth. Even though there is only one bismuth isotope used (bismuth-209), the reaction may occur in three possible ways, producing astatine-209, astatine-210, or astatine-211. In order to eliminate undesired nuclides, the maximum energy of the particle accelerator is set to any value (such as 30 MeV) above that for the reaction producing astatine-211 (to produce the desired isotope) and below the one producing astatine-210 (to avoid producing other astatine isotopes).
Separation.
Since the element is the main product of the synthesis, after its formation it must only be separated from the target and traces of other radioisotopes. The astatine-containing target is heated to to vaporize away volatile traces of radioisotopes, after which the temperature is raised to . Although 80% of astatine may vaporize at this temperature, bismuth begins to vaporize as well. Astatine's vaporization does not occur at an adequate rate at temperatures below , but at temperatures above , astatine's volatility from a bismuth surface increases significantly. The condensed vapor (distillate) is collected on a water-cooled platinum surface, which is later moved into a U-like quartz vessel. The quartz container vessel is heated to to remove further traces of impurities (typically polonium) and then to to remove astatine, which is collected on a cold finger. The purified element is then washed off the cold finger with a weak nitric acid solution. Using this technique, yields of astatine of up to 30% may be achieved.
Uses and precautions.
The newly formed astatine-211 is important in nuclear medicine. Once produced, astatine must be used quickly, as it decays with a half-life of 7.2 hours; this is, however, long enough to permit multi-step labeling strategies. Astatine-211 can be used for targeted alpha particle radiotherapy, since it decays either via emission of an alpha particle (to bismuth-207), or via electron capture (to an extremely short-lived nuclide of polonium-211, which itself undergoes further alpha decay).
In a manner similar to iodine, astatine is preferentially concentrated in the thyroid gland, although to a lesser extent. However, it tends to concentrate in the liver in the form of a radiocolloid if it is released into the systemic circulation. The principal medicinal difference between astatine-211 and iodine-131 (a radioactive iodine isotope also used in medicine) is that astatine does not emit high energy beta particles (electrons), as does iodine-131. Beta particles have considerably greater penetrating power through tissues than do the much heavier alpha particles. While an average energy alpha particle released by decay of astatine-211 can travel up to 70 µm through the surrounding tissues, an average energy beta particle emitted by iodine-131 can travel nearly 30 times as far, to about 2 mm. Thus, using astatine-211 instead of iodine-131 enables the thyroid to be dosed appropriately, while the neighboring parathyroid gland is spared. The short half-life and limited penetrating power of its radiation through tissues renders astatine generally preferable to iodine-131 when used in diagnosis as well.
Experiments in rats and monkeys, however, suggest that astatine causes much greater damage to the thyroid gland than does iodine-131, with repetitive injection of the nuclide resulting in necrosis and cell dysplasia within the gland. These experiments also suggest that astatine could cause damage to the thyroid of any organism. Early research suggested that injection of lethal quantities of astatine caused morphological changes in breast tissue (although not other tissues); however, this conclusion currently remains controversial.

</doc>
<doc id="902" url="http://en.wikipedia.org/wiki?curid=902" title="Atom">
Atom

The atom is the smallest unit that defines the chemical elements and their isotopes. Every material object, or substance that can be touched and felt, is made up of atoms. Everything that is solid, liquid, or gas is made up of atoms. Atoms are tiny; their size is typically measured in picometers (trillionths of a meter). A single strand of human hair is about one million carbon atoms wide.
Every atom is composed of a nucleus made of protons and neutrons (hydrogen-1 has no neutrons). The nucleus is in turn surrounded by a cloud of electrons. The electrons in an atom are bound to the atom by the electromagnetic force, and the protons and neutrons in the nucleus are bound to each other by the nuclear force. Over 99% of the atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. Normally, an atom's electrons balance out the positive charge of its protons to make it electrically neutral. If an atom has a surplus or deficit of electrons, then it will have an overall charge, and is called an ion.
The number of protons in the nucleus determines what chemical element the atom belongs to (e.g. all copper atoms contain 29 protons). The number of neutrons determines what isotope of the element it is. The electron cloud of the atom determines the atom's chemical properties and strongly influences its magnetic properties.
Atoms can attach themselves to each other by chemical bonds to form molecules, network solids, metal alloys, crystals, and other solid solutions. The tendency for atoms to bond and break apart is responsible for most of the physical changes we observe in nature, and this is studied by the science of chemistry.
Atoms and sub-atomic particles behave in peculiar ways that cannot be explained through the classical laws of physics. The field of quantum mechanics was developed to explain the structure and behavior of atoms.
Atoms are not the only basic structure in the Universe, but they do comprise all the types of matter than can be seen and touched. In addition to atomic matter that is made of atoms, and also certain free subatomic particles, it is believed that the Universe contains an even larger amount of dark matter, which is not made of atoms, and is composed of particles of yet unknown type.
Etymology.
The name atom comes from the Greek ἄτομος ("atomos", "indivisible") from "ἀ-" ("a-", "not") and "τέμνω" ("temnō", "I cut"), which means uncuttable, or indivisible, something that cannot be divided further.
History of atomic theory.
Atoms in philosophy.
The idea that matter is made up of discrete units is a very old one, appearing in many ancient cultures such as Greece and India. The word "atom", in fact, was coined by ancient Greek philosophers. However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation. As a result, their views on what atoms look like and how they behave were very incorrect. They also couldn't convince everybody, so atomism was but one of a number of competing theories on the nature of matter. It wasn't until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.
First evidence-based theory.
In the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers (the law of multiple proportions). For instance, there are two types of tin oxide: one is 88.1% tin and 11.9% oxygen and the other is 78.7% tin and 21.3% oxygen (tin(II) oxide and tin dioxide respectively). This means that 100g of tin will combine either with 13.5g or 27g of oxygen. 13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers. This common pattern in chemistry suggested to Dalton that elements react in whole number multiples of discrete units—in other words, atoms. In the case of tin oxides, one tin atom will combine with either one or two oxygen atoms.
Dalton also believed atomic theory could explain why water absorbs different gases in different proportions. For example, he found that water absorbs carbon dioxide far better than it absorbs nitrogen. Dalton hypothesized this was due to the differences in mass and complexity of the gases' respective particles. Indeed, carbon dioxide molecules (CO2) are heavier and larger than nitrogen molecules (N2).
In 1827, botanist Robert Brown used a microscope to look at dust grains floating in water and discovered that they moved about erratically, a phenomenon that became known as "Brownian motion". This was thought to be caused by water molecules knocking the grains about. In 1905 Albert Einstein produced the first mathematical analysis of the motion. French physicist Jean Perrin used Einstein's work to experimentally determine the mass and dimensions of atoms, thereby conclusively verifying Dalton's atomic theory.
The structure of atoms.
The physicist J. J. Thomson, through his work on cathode rays in 1897, discovered the electron, and concluded that they were a component of every atom. Thus he overturned the belief that atoms are the indivisible, ultimate particles of matter. Thomson postulated that the low mass, negatively charged electrons were distributed throughout the atom in a uniform sea of positive charge. This became known as the plum pudding model.
In 1909, Hans Geiger and Ernest Marsden, under the direction of Ernest Rutherford, bombarded a metal foil with alpha particles to observe how they scattered. They expected all the alpha particles to pass straight through with little deflection, because Thomson's model said that the charges in the atom are so diffuse that their electric fields could not affect the alpha particles much. However, Geiger and Marsden spotted alpha particles being deflected by angles greater than 90°, which was supposed to be impossible according to Thomson. To explain this, Rutherford proposed that the positive charge of the atom is concentrated in a tiny nucleus at the center of the atom.
While experimenting with the products of radioactive decay, in 1913 radiochemist Frederick Soddy discovered that there appeared to be more than one type of atom at each position on the periodic table. The term isotope was coined by Margaret Todd as a suitable name for different atoms that belong to the same element. J.J. Thomson created a technique for separating atom types through his work on ionized gases, which subsequently led to the discovery of stable isotopes.
Meanwhile, in 1913, physicist Niels Bohr suggested that the electrons were confined into clearly defined, quantized orbits, and could jump between these, but could not freely spiral inward or outward in intermediate states like satellites orbiting a planet. An electron must absorb or emit specific amounts of energy to transition between these fixed orbits. This explained why the electrons don't spiral down into the nucleus, and why elements absorb and emit light in discrete spectra.
Later in the same year Henry Moseley provided additional experimental evidence in favor of Niels Bohr's theory. These results refined Ernest Rutherford's and Antonius Van den Broek's model, which proposed that the atom contains in its nucleus a number of positive nuclear charges that is equal to its (atomic) number in the periodic table. Until these experiments, atomic number was not known to be a physical and experimental quantity. That it is equal to the atomic nuclear charge remains the accepted atomic model today.
Chemical bonds between atoms were now explained, by Gilbert Newton Lewis in 1916, as the interactions between their constituent electrons. As the chemical properties of the elements were known to largely repeat themselves according to the periodic law, in 1919 the American chemist Irving Langmuir suggested that this could be explained if the electrons in an atom were connected or clustered in some manner. Groups of electrons were thought to occupy a set of electron shells about the nucleus.
The Stern–Gerlach experiment of 1922 provided further evidence of the quantum nature of the atom. When a beam of silver atoms was passed through a specially shaped magnetic field, the beam was split based on the direction of an atom's angular momentum, or spin. As this direction is random, the beam could be expected to spread into a line. Instead, the beam was split into two parts, depending on whether the atomic spin was oriented up or down.
In 1924, Louis de Broglie proposed that all particles behave to an extent like waves. In 1926, Erwin Schrödinger used this idea to develop a mathematical model of the atom that described the electrons as three-dimensional waveforms rather than point particles. A consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both the position and momentum of a particle at the same time; this became known as the uncertainty principle, formulated by Werner Heisenberg in 1926. In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa. This model was able to explain observations of atomic behavior that previous models could not, such as certain structural and spectral patterns of atoms larger than hydrogen. Thus, the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be observed.
The development of the mass spectrometer allowed the exact mass of atoms to be measured. The device uses a magnet to bend the trajectory of a beam of ions, and the amount of deflection is determined by the ratio of an atom's mass to its charge. The chemist Francis William Aston used this instrument to show that isotopes had different masses. The atomic mass of these isotopes varied by integer amounts, called the whole number rule. The explanation for these different isotopes awaited the discovery of the neutron, a neutral-charged particle with a mass similar to the proton, by the physicist James Chadwick in 1932. Isotopes were then explained as elements with the same number of protons, but different numbers of neutrons within the nucleus.
Fission, high-energy physics and condensed matter.
In 1938, the German chemist Otto Hahn, a student of Rutherford, directed neutrons onto uranium atoms expecting to get transuranium elements. Instead, his chemical experiments showed barium as a product. A year later, Lise Meitner and her nephew Otto Frisch verified that Hahn's result were the first experimental "nuclear fission". In 1944, Hahn received the Nobel prize in chemistry. Despite Hahn's efforts, the contributions of Meitner and Frisch were not recognized.
In the 1950s, the development of improved particle accelerators and particle detectors allowed scientists to study the impacts of atoms moving at high energies. Neutrons and protons were found to be hadrons, or composites of smaller particles called quarks. The standard model of particle physics was developed that so far has successfully explained the properties of the nucleus in terms of these sub-atomic particles and the forces that govern their interactions.
Structure.
Subatomic particles.
Though the word "atom" originally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of various subatomic particles. The constituent particles of an atom are the electron, the proton and the neutron; all three are fermions. However, the hydrogen-1 atom has no neutrons and the hydron ion has no electrons.
The electron is by far the least massive of these particles at , with a negative electrical charge and a size that is too small to be measured using available techniques. It is the lightest particle with a positive rest mass measured. Under ordinary conditions, electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges. If an atom have more or fewer electrons than its atomic number, then is become respectively negatively or positively charged as a whole; a charged atom is called ion. Electrons are known since the late 19th century, mostly thanks to J.J. Thomson; see history of subatomic physics for details.
Protons have a positive charge and a mass 1,836 times that of the electron, at . The number of protons in an atom is called its atomic number. Ernest Rutherford (1919) observed that nitrogen under alpha-particle bombardment ejects what appeared to be hydrogen nuclei. By 1920 he had accepted that the hydrogen nucleus is a distinct particle within the atom and named it proton.
Neutrons have no electrical charge and have a free mass of 1,839 times the mass of the electron, or , the heaviest of the three constituent particles, but it can be reduced by the nuclear binding energy. Neutrons and protons (collectively known as nucleons) have comparable dimensions—on the order of —although the 'surface' of these particles is not sharply defined. The neutron was discovered in 1932 by the English physicist James Chadwick.
In the Standard Model of physics, electrons are truly elementary particles with no internal structure. However, both protons and neutrons are composite particles composed of elementary particles called quarks. There are two types of quarks in atoms, each having a fractional electric charge. Protons are composed of two up quarks (each with charge +) and one down quark (with a charge of −). Neutrons consist of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles.
The quarks are held together by the strong interaction (or strong force), which is mediated by gluons. The protons and neutrons, in turn, are held to each other in the nucleus by the nuclear force, which is a residuum of the strong force that has somewhat different range-properties (see the article on the nuclear force for more). The gluon is a member of the family of gauge bosons, which are elementary particles that mediate physical forces.
Nucleus.
All the bound protons and neutrons in an atom make up a tiny atomic nucleus, and are collectively called nucleons. The radius of a nucleus is approximately equal to 1.07  fm, where "A" is the total number of nucleons. This is much smaller than the radius of the atom, which is on the order of 105 fm. The nucleons are bound together by a short-ranged attractive potential called the residual strong force. At distances smaller than 2.5 fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other.
Atoms of the same element have the same number of protons, called the atomic number. Within a single element, the number of neutrons may vary, determining the isotope of that element. The total number of protons and neutrons determine the nuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoing radioactive decay.
The proton, the electron, and the neutron are classified as fermions. Fermions obey the Pauli exclusion principle which prohibits "identical" fermions, such as multiple protons, from occupying the same quantum state at the same time. Thus, every proton in the nucleus must occupy a quantum state different from all other protons, and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud. However, a proton and a neutron are allowed to occupy the same quantum state.
For atoms with low atomic numbers, a nucleus that has more neutrons than protons tends to drop to a lower energy state through radioactive decay so that the neutron-proton ratio is closer to one. However, as the atomic number increases, a higher proportion of neutrons is required to offset the mutual repulsion of the protons. Thus, there are no stable nuclei with equal proton and neutron numbers above atomic number "Z" = 20 (calcium) and as "Z" increases, the neutron-proton ratio of stable isotopes increases. The stable isotope with the highest proton-neutron ration is lead-208 (about 1.5).
The number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force. Nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3–10 keV to overcome their mutual repulsion—the coulomb barrier—and fuse together into a single nucleus. Nuclear fission is the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element.
If the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values can be emitted as a type of usable energy (such as a gamma ray, or the kinetic energy of a beta particle), as described by Albert Einstein's mass–energy equivalence formula, "E" = "mc"2, where "m" is the mass loss and "c" is the speed of light. This deficit is part of the binding energy of the new nucleus, and it is the non-recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate.
The fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel—a total nucleon number of about 60—is usually an exothermic process that releases more energy than is required to bring them together. It is this energy-releasing process that makes nuclear fusion in stars a self-sustaining reaction. For heavier nuclei, the binding energy per nucleon in the nucleus begins to decrease. That means fusion processes producing nuclei that have atomic numbers higher than about 26, and atomic masses higher than about 60, is an endothermic process. These more massive nuclei can not undergo an energy-producing fusion reaction that can sustain the hydrostatic equilibrium of a star.
Electron cloud.
The electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force. This force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus, which means that an external source of energy is needed for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.
Electrons, like other particles, have properties of both a particle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensional standing wave—a wave form that does not move relative to the nucleus. This behavior is defined by an atomic orbital, a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured. Only a discrete (or quantized) set of these orbitals exist around the nucleus, as other possible wave patterns rapidly decay into a more stable form. Orbitals can have one or more ring or node structures, and they differ from each other in size, shape and orientation.
Each atomic orbital corresponds to a particular energy level of the electron. The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. Likewise, through spontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible for atomic spectral lines.
The amount of energy needed to remove or add an electron—the electron binding energy—is far less than the binding energy of nucleons. For example, it requires only 13.6 eV to strip a ground-state electron from a hydrogen atom, compared to 2.23 "million" eV for splitting a deuterium nucleus. Atoms are electrically neutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are called ions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals.
Properties.
Nuclear properties.
By definition, any two atoms with an identical number of "protons" in their nuclei belong to the same chemical element. Atoms with equal numbers of protons but a different number of "neutrons" are different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons (hydrogen-1, by far the most common form, also called protium), one neutron (deuterium), two neutrons (tritium) and more than two neutrons. The known elements form a set of atomic numbers, from the single proton element hydrogen up to the 118-proton element ununoctium. All known isotopes of elements with atomic numbers greater than 82 are radioactive.
About 339 nuclides occur naturally on Earth, of which 254 (about 75%) have not been observed to decay, and are referred to as "stable isotopes". However, only 90 of these nuclides are stable to all decay, even in theory. Another 164 (bringing the total to 254) have not been observed to decay, even though in theory it is energetically possible. These are also formally classified as "stable". An additional 34 radioactive nuclides have half-lives longer than 80 million years, and are long-lived enough to be present from the birth of the solar system. This collection of 288 nuclides are known as primordial nuclides. Finally, an additional 51 short-lived nuclides are known to occur naturally, as daughter products of primordial nuclide decay (such as radium from uranium), or else as products of natural energetic processes on Earth, such as cosmic ray bombardment (for example, carbon-14).
For 80 of the chemical elements, at least one stable isotope exists. As a rule, there is only a handful of stable isotopes for each of these elements, the average being 3.2 stable isotopes per element. Twenty-six elements have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the element tin. Elements 43, 61, and all elements numbered 83 or higher have no stable isotopes.
Stability of isotopes is affected by the ratio of protons to neutrons, and also by the presence of certain "magic numbers" of neutrons or protons that represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 254 known stable nuclides, only four have both an odd number of protons "and" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10 and nitrogen-14. Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138 and tantalum-180m. Most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.
Mass.
The large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called "nucleons") in a given atom is called the mass number. The mass number is a simple whole number, and has units of "nucleons." An example of use of a mass number is "carbon-12," which has 12 nucleons (six protons and six neutrons).
The actual mass of an atom at rest is often expressed using the unified atomic mass unit (u), which is also called a dalton (Da). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately . Hydrogen-1, the lightest isotope of hydrogen and the atom with the lowest mass, has an atomic weight of 1.007825 u. The value of this number is called the atomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the mass of the atomic mass unit. However, this number will not be an exact whole number except in the case of carbon-12 (see below). The heaviest stable atom is lead-208, with a mass of .
As even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. One mole of atoms of any element always has the same number of atoms (about ). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of the unified atomic mass unit, each carbon-12 atom has an atomic mass of exactly 12 u, and so a mole of carbon-12 atoms weighs exactly 0.012 kg.
Shape and size.
Atoms lack a well-defined outer boundary, so their dimensions are usually described in terms of an atomic radius. This is a measure of the distance out to which the electron cloud extends from the nucleus. However, this assumes the atom to exhibit a spherical shape, which is only obeyed for atoms in vacuum or free space. Atomic radii may be derived from the distances between two nuclei when the two atoms are joined in a chemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and a quantum mechanical property known as spin. On the periodic table of the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right). Consequently, the smallest atom is helium with a radius of 32 pm, while one of the largest is caesium at 225 pm.
When subjected to external forces, like electrical fields, the shape of an atom may deviate from spherical symmetry. The deformation depends on the field magnitude and the orbital type of outer shell electrons, as shown by group-theoretical considerations. Aspherical deviations might be elicited for instance in crystals, where large crystal-electrical fields may occur at low-symmetry lattice sites. Significant ellipsoidal deformations have recently been shown to occur for sulfur ions and chalcogen ions in pyrite-type compounds.
Atomic dimensions are thousands of times smaller than the wavelengths of light (400–700 nm) so they cannot be viewed using an optical microscope. However, individual atoms can be observed using a scanning tunneling microscope. To visualize the minuteness of the atom, consider that a typical human hair is about 1 million carbon atoms in width. A single drop of water contains about 2 sextillion () atoms of oxygen, and twice the number of hydrogen atoms. A single carat diamond with a mass of contains about 10 sextillion (1022) atoms of carbon. If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.
Radioactive decay.
Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.
The most common forms of radioactive decay are:
Other more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, is internal conversion— a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay called spontaneous nuclear fission.
Each radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth.
Magnetic moment.
Elementary particles possess an intrinsic quantum mechanical property known as spin. This is analogous to the angular momentum of an object that is spinning around its center of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reduced Planck constant (ħ), with electrons, protons and neutrons all having spin ½ ħ, or "spin-½". In an atom, electrons in motion around the nucleus possess orbital angular momentum in addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin.
The magnetic field produced by an atom—its magnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field. However, the most dominant contribution comes from spin. Due to the nature of electrons to obey the Pauli exclusion principle, in which no two electrons may be found in the same quantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.
In ferromagnetic elements such as iron, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a process known as an exchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field. Paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field.
The nucleus of an atom can also have a net spin. Normally these nuclei are aligned in random directions because of thermal equilibrium. However, for certain elements (such as xenon-129) it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition called hyperpolarization. This has important applications in magnetic resonance imaging.
Energy levels.
The potential energy of an electron in an atom is negative, its dependence of its position reaches the minimum (the most absolute value) inside the nucleus, and vanishes when the distance from the nucleus goes to infinity, roughly in an inverse proportion to the distance. In the quantum-mechanical model, a bound electron can only occupy a set of states centered on the nucleus, and each state corresponds to a specific energy level; see time-independent Schrödinger equation for theoretical explanation. An energy level can be measured by the amount of energy needed to unbind the electron from the atom, and is usually given in units of electronvolts (eV). The lowest energy state of a bound electron is called the ground state, i.e. stationary state, while an electron transition to a higher level results in an excited state. The electron's energy raises when "n" increases because the (average) distance to the nucleus increases. Dependence of the energy on is caused not by electrostatic potential of the nucleus, but by interaction between electrons.
For an electron to transition between two different states, e.g. grounded state to first excited level (ionization), it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels, according to Niels Bohr model, what can be precisely calculated by the Schrödinger equation.
Electrons jump between orbitals in a particle-like fashion. For example, if a single photon strikes the electrons, only a single electron changes states in response to the photon; see Electron properties.
The energy of an emitted photon is proportional to its frequency, so these specific energy levels appear as distinct bands in the electromagnetic spectrum. Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors.
When a continuous spectrum of energy is passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output. (An observer viewing the atoms from a view that does not include the continuous spectrum in the background, instead sees a series of emission lines from the photons emitted by the atoms.) Spectroscopic measurements of the strength and width of atomic spectral lines allow the composition and physical properties of a substance to be determined.
Close examination of the spectral lines reveals that some display a fine structure splitting. This occurs because of spin–orbit coupling, which is an interaction between the spin and motion of the outermost electron. When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multiple electron configurations with the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines. The presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called the Stark effect.
If a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band.
Valence and bonding behavior.
The outermost electron shell of an atom in its uncombined state is known as the valence shell, and the electrons in
that shell are called valence electrons. The number of valence electrons determines the bonding
behavior with other atoms. Atoms tend to chemically react with each other in a manner that fills (or empties) their outer valence shells. For example, a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one-electron more than a filled shell, and others that are one-electron short of a full shell, such as occurs in the compound sodium chloride and other chemical ionic salts. However, many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus, chemical bonding between these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and the organic compounds.
The chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as the noble gases.
States.
Quantities of atoms are found in different states of matter that depend on the physical conditions, such as temperature and pressure. By varying the conditions, materials can transition between solids, liquids, gases and plasmas.
 Within a state, a material can also exist in different allotropes. An example of this is solid carbon, which can exist as graphite or diamond. Gaseous allotropes exist as well, such as dioxygen and ozone.
At temperatures close to absolute zero, atoms can form a Bose–Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale. This super-cooled collection of atoms
then behaves as a single super atom, which may allow fundamental checks of quantum mechanical behavior.
Identification.
The scanning tunneling microscope is a device for viewing surfaces at the atomic level. It uses the quantum tunneling phenomenon, which allows particles to pass through a barrier that would normally be insurmountable. Electrons tunnel through the vacuum between two planar metal electrodes, on each of which is an adsorbed atom, providing a tunneling-current density that can be measured. Scanning one atom (taken as the tip) as it moves past the other (the sample) permits plotting of tip displacement versus lateral separation for a constant current. The calculation shows the extent to which scanning-tunneling-microscope images of an individual atom are visible. It confirms that for low bias, the microscope images the space-averaged dimensions of the electron orbitals across closely packed energy levels—the Fermi level local density of states.
An atom can be ionized by removing one of its electrons. The electric charge causes the trajectory of an atom to bend when it passes through a magnetic field. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. The mass spectrometer uses this principle to measure the mass-to-charge ratio of ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis.
A more area-selective method is electron energy loss spectroscopy, which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample. The atom-probe tomograph has sub-nanometer resolution in 3-D and can chemically identify individual atoms using time-of-flight mass spectrometry.
Spectra of excited states can be used to analyze the atomic composition of distant stars. Specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using a gas-discharge lamp containing the same element. Helium was discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.
Origin and current state.
Atoms form about 4% of the total energy density of the observable Universe, with an average density of about 0.25 atoms/m3. Within a galaxy such as the Milky Way, atoms have a much higher concentration, with the density of matter in the interstellar medium (ISM) ranging from 105 to 109 atoms/m3. The Sun is believed to be inside the Local Bubble, a region of highly ionized gas, so the density in the solar neighborhood is only about 103 atoms/m3. Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium. Up to 95% of the Milky Way's atoms are concentrated inside stars and the total mass of atoms forms about 10% of the mass of the galaxy. (The remainder of the mass is an unknown dark matter.)
Formation.
Electrons are thought to exist in the Universe since early stages of the Big Bang. Atomic nuclei forms in nucleosynthesis reactions. In about three minutes Big Bang nucleosynthesis produced most of the helium, lithium, and deuterium in the Universe, and perhaps some of the beryllium and boron.
Ubiquitousness and stability of atoms relies on their binding energy, which means that an atom has a lower energy than an unbound system of the nucleus and electrons. Where the temperature is much higher than ionization potential, the matter exists in the form of plasma – a gas of positively-charged ions (possibly, bare nuclei) and electrons. When the temperature drops below the ionization potential, atoms become statistically favorable. Atoms (complete with bound electrons) became to dominate over charged particles 380,000 years after the Big Bang—an epoch called recombination, when the expanding Universe cooled enough to allow electrons to become attached to nuclei.
Since the Big Bang, which produced no carbon or heavier elements, atomic nuclei have been combined in stars through the process of nuclear fusion to produce more of the element helium, and (via the triple alpha process) the sequence of elements from carbon up to iron; see stellar nucleosynthesis for details.
Isotopes such as lithium-6, as well as some beryllium and boron are generated in space through cosmic ray spallation. This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected.
Elements heavier than iron were produced in supernovae through the r-process and in AGB stars through the s-process, both of which involve the capture of neutrons by atomic nuclei. Elements such as lead formed largely through the radioactive decay of heavier elements.
Earth.
Most of the atoms that make up the Earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the Solar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine the age of the Earth through radiometric dating. Most of the helium in the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance of helium-3) is a product of alpha decay.
There are a few trace atoms on Earth that were not present at the beginning (i.e., not "primordial"), nor are results of radioactive decay. Carbon-14 is continuously generated by cosmic rays in the atmosphere. Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions. Of the transuranic elements—those with atomic numbers greater than 92—only plutonium and neptunium occur naturally on Earth. Transuranic elements have radioactive lifetimes shorter than the current age of the Earth and thus identifiable quantities of these elements have long since decayed, with the exception of traces of plutonium-244 possibly deposited by cosmic dust. Natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore.
The Earth contains approximately atoms. Although small numbers of independent atoms of noble gases exist, such as argon, neon, and helium, 99% of the atmosphere is bound in the form of molecules, including carbon dioxide and diatomic oxygen and nitrogen. At the surface of the Earth, an overwhelming majority of atoms combine to form various compounds, including water, salt, silicates and oxides. Atoms can also combine to create materials that do not consist of discrete molecules, including crystals and liquid or solid metals. This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.
Rare and theoretical forms.
Superheavy elements.
While isotopes with atomic numbers higher than lead (82) are known to be radioactive, an "island of stability" has been proposed for some elements with atomic numbers above 103. These superheavy elements may have a nucleus that is relatively stable against radioactive decay. The most likely candidate for a stable superheavy atom, unbihexium, has 126 protons and 184 neutrons.
Exotic matter.
Each particle of matter has a corresponding antimatter particle with the opposite electrical charge. Thus, the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. The first causes of this imbalance are not yet fully understood, although theories of baryogenesis may offer an explanation. As a result, no antimatter atoms have been discovered in nature. However, in 1996 the antimatter counterpart of the hydrogen atom (antihydrogen) was synthesized at the CERN laboratory in Geneva.
Other exotic atoms have been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massive muon, forming a muonic atom. These types of atoms can be used to test the fundamental predictions of physics.
References.
Encyclopædia Britannica

</doc>
<doc id="903" url="http://en.wikipedia.org/wiki?curid=903" title="Arable land">
Arable land

Arable land (from Latin "arabilis", "able to be plowed") is land "capable" of being plowed and used to grow crops. In Britain, it was traditionally contrasted with pasturable lands such as heaths which could be used for sheep-rearing but not farmland.
In modern agriculture, however, as at the Food and Agriculture Organization, Eurostat, and the World Bank, "arable land" is a term of art meaning land that is "actually" being farmed (at minimum every five years) with crops that are sown and harvested within the same agricultural year. Arable land actually under crops in the present year is known as or . The emended definition is preferred by the agencies because it distinguishes that "could" be used to raise such annual crops but is instead devoted to "permanent cropland": for example, vinyards, orchards, and farms and plantations growing coffee, rubber, or nuts.
Arable land area.
Although constrained by land mass and topography, the amount of arable land, both regionally and globally, fluctuates due to human and climatic factors such as irrigation, deforestation, desertification, terracing, land reclamation and urban sprawl. In 2008, the world's arable land amounted to 1,386 M ha, out of a total 4,883 M ha land used for agriculture.
Non-arable land.
Land which is unsuitable for arable farming usually has at least one of the following deficiencies: no source of fresh water; too hot (desert); too cold (Arctic); too rocky; too mountainous; too salty; too rainy; too snowy; too polluted; or too nutrient poor. Clouds may block the sunlight plants need for photosynthesis, reducing productivity. Starvation and nomadism often exists on marginally arable land. Non-arable land is sometimes called "wasteland", "badlands", "worthless" or "no man's land".
However, non-arable land can sometimes be converted into arable land. New arable land makes more food, and can reduce starvation. This outcome also makes a country more self-sufficient and politically independent, because food importation is reduced. Making non-arable land arable often involves digging new irrigation canals and new wells, aqueducts, desalination plants, planting trees for shade in the desert, hydroponics, fertilizer, nitrogen fertilizer, pesticides, reverse osmosis water processors, PET film insulation or other insulation against heat and cold, digging ditches and hills for protection against the wind, and greenhouses with internal light and heat for protection against the cold outside and to provide light in cloudy areas. This process is often extremely expensive. An alternative is the Seawater Greenhouse which desalinates water through evaporation and condensation using solar energy as the only energy input. This technology is optimized to grow crops on desert land close to the sea.
Some examples of infertile non-arable land being turned into fertile arable land are:
Some examples of fertile arable land being turned into infertile land are:

</doc>
<doc id="904" url="http://en.wikipedia.org/wiki?curid=904" title="Aluminium">
Aluminium

Aluminium (or aluminum; see spelling differences) is a chemical element in the boron group with symbol Al and atomic number 13. It is a silvery white, soft, ductile metal. Aluminium is the third most abundant element (after oxygen and silicon), and the most abundant metal in the Earth's crust. It makes up about 8% by weight of the Earth's solid surface. Aluminium metal is so chemically reactive that native specimens are rare and limited to extreme reducing environments. Instead, it is found combined in over 270 different minerals. The chief ore of aluminium is bauxite.
Aluminium is remarkable for the metal's low density and for its ability to resist corrosion due to the phenomenon of passivation. Structural components made from aluminium and its alloys are vital to the aerospace industry and are important in other areas of transportation and structural materials. The most useful compounds of aluminium, at least on a weight basis, are the oxides and sulfates.
Despite its prevalence in the environment, no known form of life uses aluminium salts metabolically. In keeping with its pervasiveness, aluminium is well tolerated by plants and animals. Owing to their prevalence, potential beneficial (or otherwise) biological roles of aluminium compounds are of continuing interest.
Characteristics.
Physical.
Aluminium is a relatively soft, durable, lightweight, ductile and malleable metal with appearance ranging from silvery to dull gray, depending on the surface roughness. It is nonmagnetic and does not easily ignite. A fresh film of aluminium serves as a good reflector (approximately 92%) of visible light and an excellent reflector (as much as 98%) of medium and far infrared radiation. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium has about one-third the density and stiffness of steel. It is easily machined, cast, drawn and extruded.
Aluminium atoms are arranged in a face-centered cubic (fcc) structure. Aluminium has a stacking-fault energy of approximately 200 mJ/m2.
Aluminium is a good thermal and electrical conductor, having 59% the conductivity of copper, both thermal and electrical, while having only 30% of copper's density. Aluminium is capable of being a superconductor, with a superconducting critical temperature of 1.2 Kelvin and a critical magnetic field of about 100 gauss (10 milliteslas).
Chemical.
Corrosion resistance can be excellent due to a thin surface layer of aluminium oxide that forms when the metal is exposed to air, effectively preventing further oxidation. The strongest aluminium alloys are less corrosion resistant due to galvanic reactions with alloyed copper. This corrosion resistance is also often greatly reduced by aqueous salts, particularly in the presence of dissimilar metals.
Owing to its resistance to corrosion, aluminium is one of the few metals that retain silvery reflectance in finely powdered form, making it an important component of silver-colored paints. Aluminium mirror finish has the highest reflectance of any metal in the 200–400 nm (UV) and the 3,000–10,000 nm (far IR) regions; in the 400–700 nm visible range it is slightly outperformed by tin and silver and in the 700–3000 (near IR) by silver, gold, and copper.
Aluminium is oxidized by water to produce hydrogen and heat:
This conversion is of interest for the production of hydrogen. Challenges include circumventing the formed oxide layer, which inhibits the reaction and the expenses associated with the storage of energy by regeneration of the Al metal.
Isotopes.
Aluminium has many known isotopes, whose mass numbers range from 21 to 42; however, only 27Al (stable isotope) and 26Al (radioactive isotope, t1⁄2 = 7.2×105 y) occur naturally. 27Al has a natural abundance above 99.9%. 26Al is produced from argon in the atmosphere by spallation caused by cosmic-ray protons. Aluminium isotopes have found practical application in dating marine sediments, manganese nodules, glacial ice, quartz in rock exposures, and meteorites. The ratio of 26Al to 10Be has been used to study the role of transport, deposition, sediment storage, burial times, and erosion on 105 to 106 year time scales. Cosmogenic 26Al was first applied in studies of the Moon and meteorites. Meteoroid fragments, after departure from their parent bodies, are exposed to intense cosmic-ray bombardment during their travel through space, causing substantial 26Al production. After falling to Earth, atmospheric shielding drastically reduces 26Al production, and its decay can then be used to determine the meteorite's terrestrial age. Meteorite research has also shown that 26Al was relatively abundant at the time of formation of our planetary system. Most meteorite scientists believe that the energy released by the decay of 26Al was responsible for the melting and differentiation of some asteroids after their formation 4.55 billion years ago.
Natural occurrence.
Stable aluminium is created when hydrogen fuses with magnesium either in large stars or in supernovae.
In the Earth's crust, aluminium is the most abundant (8.3% by weight) metallic element and the third most abundant of all elements (after oxygen and silicon). Because of its strong affinity to oxygen, it is almost never found in the elemental state; instead it is found in oxides or silicates. Feldspars, the most common group of minerals in the Earth's crust, are aluminosilicates. Native aluminium metal can only be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. Native aluminium has been reported in cold seeps in the northeastern continental slope of the South China Sea and Chen "et al." (2011) have proposed a theory of its origin as resulting by reduction from tetrahydroxoaluminate Al(OH)4– to metallic aluminium by bacteria.
It also occurs in the minerals beryl, cryolite, garnet, spinel and turquoise. Impurities in Al2O3, such as chromium or iron yield the gemstones ruby and sapphire, respectively.
Although aluminium is an extremely common and widespread element, the common aluminium minerals are not economic sources of the metal. Almost all metallic aluminium is produced from the ore bauxite (AlO"x"(OH)3–2"x"). Bauxite occurs as a weathering product of low iron and silica bedrock in tropical climatic conditions. Large deposits of bauxite occur in Australia, Brazil, Guinea and Jamaica and the primary mining areas for the ore are in Australia, Brazil, China, India, Guinea, Indonesia, Jamaica, Russia and Suriname.
Production and refinement.
Bauxite is converted to aluminium oxide (Al2O3) via the Bayer process. Relevant chemical equations are:
The intermediate sodium aluminate, given the simplified formula NaAlO2, is soluble in strongly alkaline water, and the other components of the ore are not. Depending on the quality of the bauxite ore, twice as much waste ("red mud") is generated compared to the amount of alumina. 
The conversion of alumina to aluminum metal is achieved by the Hall-Héroult process. In this energy-intensive process, a solution of alumina in a molten () mixture of cryolite (Na3AlF6) with calcium fluoride is electrolyzed to give the metal:
At the anode, oxygen is formed:
The aluminium metal then sinks to the bottom of the solution and is tapped off, usually cast into large blocks called aluminium billets for further processing. To some extent, the carbon anode is consumed by subsequent reaction with oxygen to form carbon dioxide. The anodes in a reduction cell must therefore be replaced regularly, since they are consumed in the process. The cathodes do erode, mainly due to electrochemical processes and metal movement. After five to ten years, depending on the current used in the electrolysis, a cell must be rebuilt because of cathode wear.
Aluminium electrolysis with the Hall-Héroult process consumes a lot of energy. The worldwide average specific energy consumption is approximately 15±0.5 kilowatt-hours per kilogram of aluminium produced (52 to 56 MJ/kg). The most modern smelters achieve approximately 12.8 kW·h/kg (46.1 MJ/kg). (Compare this to the heat of reaction, 31 MJ/kg, and the Gibbs free energy of reaction, 29 MJ/kg.) Reduction line currents for older technologies are typically 100 to 200 kiloamperes; state-of-the-art smelters operate at about 350 kA. Trials have been reported with 500 kA cells.
The Hall-Heroult process produces aluminium with a purity of above 99%. Further purification can be done by the Hoopes process. The process involves the electrolysis of molten aluminium with a sodium, barium and aluminium fluoride electrolyte. The resulting aluminium has a purity of 99.99%.
Electric power represents about 20% to 40% of the cost of producing aluminium, depending on the location of the smelter. Aluminium production consumes roughly 5% of electricity generated in the U.S. Aluminium producers tend to locate smelters in places where electric power is both plentiful and inexpensive—such as the United Arab Emirates with its large natural gas supplies, and Iceland and Norway with energy generated from renewable sources. The world's largest smelters of alumina are People's Republic of China, Russia, and Quebec and British Columbia in Canada.
In 2005, the People's Republic of China was the top producer of aluminium with almost a one-fifth world share, followed by Russia, Canada, and the USA, reports the British Geological Survey.
Over the last 50 years, Australia has become the world's top producer of bauxite ore and a major producer and exporter of alumina (before being overtaken by China in 2007). Australia produced 77 million tonnes of bauxite in 2013. The Australian deposits have some refining problems, some being high in silica, but have the advantage of being shallow and relatively easy to mine.
Recycling.
Aluminium is theoretically 100% recyclable without any loss of its natural qualities. According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of aluminium in use in society (i.e. in cars, buildings, electronics etc.) is 80 kg. Much of this is in more-developed countries (350–500 kg per capita) rather than less-developed countries (35 kg per capita). Knowing the per capita stocks and their approximate lifespans is important for planning recycling.
Recovery of the metal via recycling has become an important use of the aluminium industry. Recycling was a low-profile activity until the late 1960s, when the growing use of aluminium beverage cans brought it to the public awareness.
Recycling involves melting the scrap, a process that requires only 5% of the energy used to produce aluminium from ore, though a significant part (up to 15% of the input material) is lost as dross (ash-like oxide). An aluminium stack melter produces significantly less dross, with values reported below 1%. The dross can undergo a further process to extract aluminium.
In Europe aluminium experiences high rates of recycling, ranging from 42% of beverage cans, 85% of construction materials and 95% of transport vehicles.
Recycled aluminium is known as secondary aluminium, but maintains the same physical properties as primary aluminium. Secondary aluminium is produced in a wide range of formats and is employed in 80% of alloy injections. Another important use is for extrusion.
White dross from primary aluminium production and from secondary recycling operations still contains useful quantities of aluminium that can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia), which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, the waste has found use as a filler in asphalt and concrete.
Compounds.
Oxidation state +3.
The vast majority of compounds, including all Al-containing minerals and all commercially significant aluminium compounds, feature aluminium in the oxidation state 3+. The coordination number of such compounds varies, but generally Al3+ is six-coordinate or tetracoordinate. Almost all compounds of aluminium(III) are colorless.
Halides.
All four trihalides are well known. Unlike the structures of the three heavier trihalides, aluminium fluoride (AlF3) features six-coordinate Al. The octahedral coordination environment for AlF3 is related to the compactness of fluoride ion, six of which can fit around the small Al3+ centre. AlF3 sublimes (with cracking) at . With heavier halides, the coordination numbers are lower. The other trihalides are dimeric or polymeric with tetrahedral Al centers. These materials are prepared by treating aluminium metal with the halogen, although other methods exist. Acidification of the oxides or hydroxides affords hydrates. In aqueous solution, the halides often form mixtures, generally containing six-coordinate Al centres, which are feature both halide and aquo ligands. When aluminium and fluoride are together in aqueous solution, they readily form complex ions such as , , and . In the case of chloride, polyaluminium clusters are formed such as [Al13O4(OH)24(H2O)12]7+.
Oxide and hydroxides.
Aluminium forms one stable oxide, known by its mineral name corundum. Sapphire and ruby are impure corundum contaminated with trace amounts of other metals. The two oxide-hydroxides, AlO(OH), are boehmite and diaspore. There are three trihydroxides: bayerite, gibbsite, and nordstrandite, which differ in their crystalline structure (polymorphs). Most are produced from ores by a variety of wet processes using acid and base. Heating the hydroxides leads to formation of corundum. These materials are of central importance to the production of aluminium and are themselves extremely useful.
Carbide, nitride, and related materials.
Aluminium carbide (Al4C3) is made by heating a mixture of the elements above . The pale yellow crystals consist of tetrahedral aluminium centres. It reacts with water or dilute acids to give methane. The acetylide, Al2(C2)3, is made by passing acetylene over heated aluminium.
Aluminium nitride (AlN) is the only nitride known for aluminium. Unlike the oxides it features tetrahedral Al centres. It can be made from the elements at . It is air-stable material with a usefully high thermal conductivity. Aluminium phosphide (AlP) is made similarly, and hydrolyses to give phosphine:
Organoaluminium compounds and related hydrides.
A variety of compounds of empirical formula AlR3 and AlR1.5Cl1.5 exist. These species usually feature tetrahedral Al centers, e.g. "trimethylaluminium" has the formula Al2(CH3)6 (see figure). With large organic groups, triorganoaluminium exist as three-coordinate monomers, such as triisobutylaluminium. Such compounds are widely used in industrial chemistry, despite the fact that they are often highly pyrophoric. Few analogues exist between organoaluminium and organoboron compounds except for large organic groups.
The important aluminium hydride is lithium aluminium hydride (LiAlH4), which is used in as a reducing agent in organic chemistry. It can be produced from lithium hydride and aluminium trichloride:
Several useful derivatives of LiAlH4 are known, e.g. sodium bis(2-methoxyethoxy)dihydridoaluminate. The simplest hydride, aluminium hydride or alane, remains a laboratory curiosity. It is a polymer with the formula (AlH3)"n", in contrast to the corresponding boron hydride with the formula (BH3)2.
Oxidation states +1 and +2.
Although the great majority of aluminium compounds feature Al3+ centers, compounds with lower oxidation states are known and sometime of significance as precursors to the Al3+ species.
Aluminium(I).
AlF, AlCl and AlBr exist in the gaseous phase when the trihalide is heated with aluminium. The composition AlI is unstable at room temperature with respect to the triiodide:
A stable derivative of aluminium monoiodide is the cyclic adduct formed with triethylamine, Al4I4(NEt3)4. Also of theoretical interest but only of fleeting existence are Al2O and Al2S. Al2O is made by heating the normal oxide, Al2O3, with silicon at in a vacuum. Such materials quickly disproportionates to the starting materials.
Aluminium(II).
Very simple Al(II) compounds are invoked or observed in the reactions of Al metal with oxidants. For example, aluminium monoxide, AlO, has been detected in the gas phase after explosion and in stellar absorption spectra. More thoroughly investigated are compounds of the formula R4Al2 where R is a large organic ligand.
Analysis.
The presence of aluminium can be detected in qualitative analysis using aluminon.
Applications.
General use.
Aluminium is the most widely used non-ferrous metal. Global production of aluminium in 2005 was 31.9 million tonnes. It exceeded that of any other metal except iron (837.5 million tonnes). Forecast for 2012 is 42–45 million tonnes, driven by rising Chinese output.
Aluminium is almost always alloyed, which markedly improves its mechanical properties, especially when tempered. For example, the common aluminium foils and beverage cans are alloys of 92% to 99% aluminium. The main alloying agents are copper, zinc, magnesium, manganese, and silicon (e.g., duralumin) and the levels of these other metals are in the range of a few percent by weight.
Some of the many uses for aluminium metal are in:
Aluminium is usually alloyed – it is used as pure metal only when corrosion resistance and/or workability is more important than strength or hardness. A thin layer of aluminium can be deposited onto a flat surface by physical vapor deposition or (very infrequently) chemical vapor deposition or other chemical means to form optical coatings and mirrors.
Aluminium compounds.
Because aluminium is abundant and most of its derivatives exhibit low toxicity, the compounds of aluminium enjoy wide and sometimes large-scale applications.
Alumina.
Aluminium oxide (Al2O3) and the associated oxy-hydroxides and trihydroxides are produced or extracted from minerals on a large scale. The great majority of this material is converted to metallic aluminium. In 2013 about 10% of the domestic shipments in the Unitated States were used for other applications. A major use is as an absorbent. For example, alumina removes water from hydrocarbons, which enables subsequent processes that are poisoned by moisture. Aluminium oxides are common catalysts for industrial processes, e.g. the Claus process for converting hydrogen sulfide to sulfur in refineries and for the alkylation of amines. Many industrial catalysts are "supported", meaning generally that an expensive catalyst (e.g., platinum) is dispersed over a high surface area material such as alumina. Being a very hard material (Mohs hardness 9), alumina is widely used as an abrasive and the production of applications that exploit its inertness, e.g., in high pressure sodium lamps.
Sulfates.
Several sulfates of aluminium find applications. Aluminium sulfate (Al2(SO4)3·(H2O)18) is produced on the annual scale of several billions of kilograms. About half of the production is consumed in water treatment. The next major application is in the manufacture of paper. It is also used as a mordant, in fire extinguisher, as a food additive, in fireproofing, and in leather tanning. Aluminium ammonium sulfate, which is also called ammonium alum, (NH4)Al(SO4)2·12H2O, is used as a mordant and in leather tanning. Aluminium potassium sulfate ([Al(K)](SO4)2)·(H2O)12 is used similarly. The consumption of both alums is declining.
Chlorides.
Aluminium chloride (AlCl3) is used in petroleum refining and in the production of synthetic rubber and polymers. Although it has a similar name, aluminium chlorohydrate has fewer and very different applications, e.g. as a hardening agent and an antiperspirant. It is an intermediate in the production of aluminium metal.
Niche compounds.
Given the scale of aluminium compounds, a small scale application could still involve thousands of tonnes. One of the many compounds used at this intermediate level include aluminium acetate, a salt used in solution as an astringent. Aluminium borate (Al2O3·B2O3) is used in the production of glass and ceramics. Aluminium fluorosilicate (Al2(SiF6)3) is used in the production of synthetic gemstones, glass and ceramic. Aluminium phosphate (AlPO4) is used in the manufacture: of glass and ceramic, pulp and paper products, cosmetics, paints and varnishes and in making dental cement. Aluminium hydroxide (Al(OH)3) is used as an antacid, as a mordant, in water purification, in the manufacture of glass and ceramic and in the waterproofing of fabrics. Lithium aluminium hydride is a powerful reducing agent used in organic chemistry. Organoaluminiums are used as Lewis acids and cocatalysts. For example, methylaluminoxane is a cocatalyst for Ziegler-Natta olefin polymerization to produce vinyl polymers such as polyethene.
Aluminium alloys in structural applications.
Aluminium alloys with a wide range of properties are used in engineering structures. Alloy systems are classified by a number system (ANSI) or by names indicating their main alloying constituents (DIN and ISO).
The strength and durability of aluminium alloys vary widely, not only as a result of the components of the specific alloy, but also as a result of heat treatments and manufacturing processes. A lack of knowledge of these aspects has from time to time led to improperly designed structures and gained aluminium a bad reputation.
One important structural limitation of aluminium alloys is their fatigue strength. Unlike steels, aluminium alloys have no well-defined fatigue limit, meaning that fatigue failure eventually occurs, under even very small cyclic loadings. This implies that engineers must assess these loads and design for a fixed life rather than an infinite life.
Another important property of aluminium alloys is their sensitivity to heat. Workshop procedures involving heating are complicated by the fact that aluminium, unlike steel, melts without first glowing red. Forming operations where a blow torch is used therefore require some expertise, since no visual signs reveal how close the material is to melting. Aluminium alloys, like all structural alloys, also are subject to internal stresses following heating operations such as welding and casting. The problem with aluminium alloys in this regard is their low melting point, which make them more susceptible to distortions from thermally induced stress relief. Controlled stress relief can be done during manufacturing by heat-treating the parts in an oven, followed by gradual cooling—in effect annealing the stresses.
The low melting point of aluminium alloys has not precluded their use in rocketry; even for use in constructing combustion chambers where gases can reach 3500 K. The Agena upper stage engine used a regeneratively cooled aluminium design for some parts of the nozzle, including the thermally critical throat region.
Another alloy of some value is aluminium bronze (Cu-Al alloy).
History.
Ancient Greeks and Romans used aluminium salts as dyeing mordants and as astringents for dressing wounds; alum is still used as a styptic.
In 1761, Guyton de Morveau suggested calling the base alum "alumine." In 1808, Humphry Davy identified the existence of a metal base of alum, which he at first termed "alumium" and later "aluminum" (see etymology section, below).
The metal was first produced in 1825 in an impure form by Danish physicist and chemist Hans Christian Ørsted. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal looking similar to tin. Friedrich Wöhler was aware of these experiments and cited them, but after redoing the experiments of Ørsted he concluded that this metal was pure potassium. He conducted a similar experiment in 1827 by mixing anhydrous aluminium chloride with potassium and yielded aluminium. Wöhler is generally credited with isolating aluminium (Latin "alumen", alum). Further, Pierre Berthier discovered aluminium in bauxite ore. Henri Etienne Sainte-Claire Deville improved Wöhler's method in 1846. As described in his 1859 book, aluminium trichloride could be reduced by sodium, which was more convenient and less expensive than potassium used by Wöhler. 
In the mid 1880s, aluminium metal was exceedingly difficult to produce, which made pure aluminium more valuable than gold. So celebrated was the metal that bars of aluminium were exhibited at the Exposition Universelle of 1855. Napoleon III of France is reputed to held a banquet where the most honored guests were given aluminium utensils, while the others made do with gold.
Aluminium was selected as the material to use for the capstone of the Washington Monument in 1884, a time when one ounce (30 grams) cost the daily wage of a common worker on the project. The capstone, which was set in place on 6 December 1884, in an elaborate dedication ceremony, was the largest single piece of aluminium cast at the time, when aluminium was as expensive as silver.
The Cowles companies supplied aluminium alloy in quantity in the United States and England using smelters like the furnace of Carl Wilhelm Siemens by 1886. 
Hall-Heroult process: availability of cheap aluminium metal.
Charles Martin Hall of Ohio in the U.S. and Paul Héroult of France independently developed the Hall-Héroult electrolytic process that facilitated large-scale production of metallic aluminium. This process remains in use today. In 1888 with the financial backing of Alfred E. Hunt, the Pittsburgh Reduction Company started, today it is known as Alcoa. Héroult's process was in production by 1889 in Switzerland at Aluminium Industrie, now Alcan, and at British Aluminium, now Luxfer Group and Alcoa, by 1896 in Scotland.
By 1895, the metal was being used as a building material as far away as Sydney, Australia in the dome of the Chief Secretary's Building.
With the explosive expansion of the airplane industry during World War I (1914-1917), major governments demanded large shipments of aluminium for light, strong airframes. They often subsidized factories and the necessary electrical supply systems.
Many navies have used an aluminium superstructure for their vessels; the 1975 fire aboard USS "Belknap" that gutted her aluminium superstructure, as well as observation of battle damage to British ships during the Falklands War, led to many navies switching to all steel superstructures.
Aluminium wire was once widely used for domestic electrical wiring. Owing to corrosion-induced failures, a number of fires resulted.
Etymology.
Two variants of the metal's name are in current use, "aluminium" () and "aluminum" ()—besides the obsolete "alumium". The International Union of Pure and Applied Chemistry (IUPAC) adopted "aluminium" as the standard international name for the element in 1990 but, three years later, recognized "aluminum" as an acceptable variant. Hence their periodic table includes both. IUPAC internal publications use either spelling in nearly the same number.
Most countries use the spelling "aluminium". In the United States and Canada, the spelling "aluminum" predominates. The Canadian Oxford Dictionary prefers "aluminum", whereas the Australian Macquarie Dictionary prefers "aluminium". In 1926, the American Chemical Society officially decided to use "aluminum" in its publications; American dictionaries typically label the spelling "aluminium" as "chiefly British".
The various names all derive from its status as a base of alum. It is borrowed from Old French; its ultimate source, "alumen", in turn is a Latin word that literally means "bitter salt".
The earliest citation given in the Oxford English Dictionary for any word used as a name for this element is "alumium", which British chemist and inventor Humphry Davy employed in 1808 for the metal he was trying to isolate electrolytically from the mineral "alumina". The citation is from the journal "Philosophical Transactions of the Royal Society of London": "Had I been so fortunate as to have obtained more certain evidences on this subject, and to have procured the metallic substances I was in search of, I should have proposed for them the names of silicium, alumium, zirconium, and glucium."
Davy settled on "aluminum" by the time he published his 1812 book "Chemical Philosophy": "This substance appears to contain a peculiar metal, but as yet Aluminum has not been obtained in a perfectly free state, though alloys of it with other metalline substances have been procured sufficiently distinct to indicate the probable nature of alumina." But the same year, an anonymous contributor to the "Quarterly Review," a British political-literary journal, in a review of Davy's book, objected to "aluminum" and proposed the name "aluminium", "for so we shall take the liberty of writing the word, in preference to aluminum, which has a less classical sound."
The "-ium" suffix conformed to the precedent set in other newly discovered elements of the time: potassium, sodium, magnesium, calcium, and strontium (all of which Davy isolated himself). Nevertheless, "-um" spellings for elements were not unknown at the time, as for example platinum, known to Europeans since the 16th century, molybdenum, discovered in 1778, and tantalum, discovered in 1802. The "-um" suffix is consistent with the universal spelling alumina for the oxide (as opposed to aluminia), as lanthana is the oxide of lanthanum, and magnesia, ceria, and thoria are the oxides of magnesium, cerium, and thorium respectively.
The "aluminum" spelling is used in the Webster's Dictionary of 1828. In his advertising handbill for his new electrolytic method of producing the metal in 1892, Charles Martin Hall used the "-um" spelling, despite his constant use of the "-ium" spelling in all the patents he filed between 1886 and 1903. It has consequently been suggested that the spelling reflects an easier-to-pronounce word with one fewer syllable, or that the spelling on the flyer was a mistake. Hall's domination of production of the metal ensured that "aluminum" became the standard English spelling in North America.
Health concerns.
Despite its widespread occurrence in nature, aluminium has no known function in biology. Aluminium salts are remarkably nontoxic, aluminium sulfate having an LD50 of 6207 µg/kg (oral, mouse), which corresponds to 500 grams for an 80 kg person. The extremely low acute toxicity notwithstanding, the health effects of aluminium are of interest in view of the widespread occurrence of the element in the environment and in commerce.
Some toxicity can be traced to deposition in bone and the central nervous system, which is particularly increased in patients with reduced renal function. Because aluminium competes with calcium for absorption, increased amounts of dietary aluminium may contribute to the reduced skeletal mineralization (osteopenia) observed in preterm infants and infants with growth retardation. In very high doses, aluminium can cause neurotoxicity, and is associated with altered function of the blood–brain barrier. A small percentage of people are allergic to aluminium and experience contact dermatitis, digestive disorders, vomiting or other symptoms upon contact or ingestion of products containing aluminium, such as antiperspirants or antacids. In those without allergies, aluminium is not as toxic as heavy metals, but there is evidence of some toxicity if it is consumed in amounts greater than 40 mg/kg/day. Although the use of aluminium cookware has not been shown to lead to aluminium toxicity in general, excessive consumption of antacids containing aluminium compounds and excessive use of aluminium-containing antiperspirants provide more significant exposure levels. Studies have shown that consumption of acidic foods or liquids with aluminium significantly increases aluminium absorption, and maltol has been shown to increase the accumulation of aluminium in nervous and osseus tissue. Furthermore, aluminium increases estrogen-related gene expression in human breast cancer cells cultured in the laboratory. The estrogen-like effects of these salts have led to their classification as a metalloestrogen.
The effects of aluminium in antiperspirants have been examined over the course of decades with little evidence of skin irritation. Nonetheless, its occurrence in antiperspirants, dyes (such as aluminium lake), and food additives has caused concern. Although there is little evidence that normal exposure to aluminium presents a risk to healthy adults, some studies point to risks associated with increased exposure to the metal. Aluminium in food may be absorbed more than aluminium from water. It is classified as a non-carcinogen by the US Department of Health and Human Services.
In case of suspected sudden intake of a large amount of aluminium, deferoxamine mesylate may be given to help eliminate it from the body by chelation.
Alzheimer's disease.
Aluminium has controversially been implicated as a factor in Alzheimer's disease. The Camelford water pollution incident involved a number of people consuming aluminium sulfate. Investigations of the long-term health effects are still ongoing, but elevated brain aluminium concentrations have been found in post-mortem examinations of victims, and further research to determine if there is a link with cerebral amyloid angiopathy has been commissioned.
According to the Alzheimer's Society, the medical and scientific opinion is that studies have not convincingly demonstrated a causal relationship between aluminium and Alzheimer's disease. Nevertheless, some studies, such as those on the PAQUID cohort, cite aluminium exposure as a risk factor for Alzheimer's disease. Some brain plaques have been found to contain increased levels of the metal. Research in this area has been inconclusive; aluminium accumulation may be a consequence of the disease rather than a causal agent. In any event, if there is any toxicity of aluminium, it must be via a very specific mechanism, since total human exposure to the element in the form of naturally occurring clay in soil and dust is enormously large over a lifetime. Scientific consensus does not yet exist about whether aluminium exposure could directly increase the risk of Alzheimer's disease.
Effect on plants.
Aluminium is primary among the factors that reduce plant growth on acid soils. Although it is generally harmless to plant growth in pH-neutral soils, the concentration in acid soils of toxic Al3+ cations increases and disturbs root growth and function.
Most acid soils are saturated with aluminium rather than hydrogen ions. The acidity of the soil is therefore a result of hydrolysis of aluminium compounds. This concept of "corrected lime potential" to define the degree of base saturation in soils became the basis for procedures now used in soil testing laboratories to determine the "lime requirement" of soils.
Wheat's adaptation to allow aluminium tolerance is such that the aluminium induces a release of organic compounds that bind to the harmful aluminium cations. Sorghum is believed to have the same tolerance mechanism. The first gene for aluminium tolerance has been identified in wheat. It was shown that sorghum's aluminium tolerance is controlled by a single gene, as for wheat. This is not the case in all plants.
Biodegradation.
The fungus "Geotrichum candidum" has been found to consume the aluminium in compact discs. The bacterium Pseudomonas aeruginosa and the fungus Cladosporium resinae are commonly detected in aircraft engines, and can degrade aluminium in cultures.

</doc>
<doc id="905" url="http://en.wikipedia.org/wiki?curid=905" title="Advanced Chemistry">
Advanced Chemistry

Advanced Chemistry is a German hip hop group from Heidelberg, a scenic city in Baden-Württemberg, South Germany. Advanced Chemistry was founded in 1987 by Toni L, Linguist, Gee-One, DJ Mike MD (Mike Dippon) and MC Torch. Each member of the group holds German citizenshhip, and Toni L, Linguist, and Torch are of Italian, Ghanaian, and Haitian backgrounds, respectively.
Influenced by North American socially conscious rap and the Native tongues movement, Advanced Chemistry is regarded as one of the main pioneers in German hip hop. They were one of the first groups to rap in German (although their name is in English). Furthermore, their songs tackled controversial social and political issues, distinguishing them from early German hip hop group "Die Fantastischen Vier" (The Fantastic Four), which had a more light-hearted, playful, party image. 
The rivalry between Advanced Chemistry and Die Fantastischen Vier has served to highlight a dichotomy in the routes that hip hop has taken in becoming a part of the German soundscape. While Die Fantastischen Vier may be said to view hip hop primarily as an aesthetic art form, Advanced Chemistry understand hip hop as being inextricably linked to the social and political circumstances under which it is created. For Advanced Chemistry, hip hop is a “vehicle of general human emancipation,”. In their undertaking of social and political issues, the band introduced the term "Afro-German" into the context of German hip hop, and the theme of race is highlighted in much of their music.
With the release of the single “Fremd im eigenen Land”, Advanced Chemistry separated itself from the rest of the rap being produced in Germany. This single was the first of its kind to go beyond simply imitating US rap and addressed the current issues of the time. Fremd im eigenen Land which translates to “foreign in my own country” dealt with the widespread racism that non-white German citizens faced. This change from simple imitation to political commentary was the start of German identification with rap. The sound of “Fremd im eigenen Land” was influenced by the 'wall of noise' created by Public Enemy's producers, The Bomb Squad.
After the reunification of Germany, an abundance of anti-immigrant sentiment emerged, as well as attacks on the homes of refugees in the early 90's. Advanced Chemistry came to prominence in the wake of these actions because of their pro-multicultural society stance in their music. Advanced Chemistry's attitudes revolve around their attempts to create a distinct "Germanness" in hip hop, as opposed to imitating American hip hop as other groups had done. Torch has said, "What the Americans do is exotic for us because we don't live like they do. What they do seems to be more interesting and newer. But not for me. For me it's more exciting to experience my fellow Germans in new contexts...For me, it's interesting to see what the kids try to do that's different from what I know." Advanced Chemistry were the first to use the term "Afro-German" in a hip hop context. This was part of the pro-immigrant political message they sent via their music.
While Advanced Chemistry's use of the German language in their rap allows them to make claims to authenticity and true German heritage, bolstering pro-immigration sentiment, their style can also be problematic for immigrant notions of any real ethnic roots. Indeed, part of the Turkish ethnic minority of Frankfurt views Advanced Chemistry's appeal to the German image as a "symbolic betrayal of the right of ethnic minorities to 'roots' or to any expression of cultural heritage." In this sense, their rap represents a complex social discourse internal to the German soundscape in which they attempt to negotiate immigrant assimilation into a xenophobic German culture with the maintenance of their own separate cultural traditions. It is quite possibly the feelings of alienation from the pure-blooded German demographic that drive Advanced Chemistry to attack nationalistic ideologies by asserting their "Germanness" as a group composed primarily of ethnic others. The response to this pseudo-German authenticity can be seen in what Andy Bennett refers to as "alternative forms of local hip hop culture which actively seek to rediscover and, in many cases, reconstruct notions of identity tied to cultural roots." These alternative local hip hop cultures include Oriental hip hop, the members of which cling to their Turkish heritage and are confused by Advanced Chemistry's elicitation of a German identity politics to which they technically do not belong. This cultural binary illustrates that rap has taken different routes in Germany and that, even among an already isolated immigrant population, there is still disunity and, especially, disagreement on the relative importance of assimilation versus cultural defiance. According to German hip hop enthusiast 9@home, Advanced Chemistry is part of a "hip-hop movement [which] took a clear stance for the minorities and against the [marginalization] of immigrants who...might be German on paper, but not in real life," which speaks to the group's hope of actually being recognized as German citizens and not foreigners, despite their various other ethnic and cultural ties.
Market conditions for rap.
One of the first issues that confronts us when we move outside the English-speaking market for recorded music is to establish whether or not the discrete musical genres we know from that market are fully congruent with similar divisions in other pop worlds. This is important in two ways. First, although no single country comes close to matching the amounts spent on recorded music in the United States, these markets are nonetheless economically significant. Germany, for instance, is the largest single market in western Europe, with estimated annual sales of U.S. $3.74 billion in 1996. This represents around 30 percent of reported U.S. sales and makes Germany the third biggest music market in the world. 
Advanced Chemistry frequently rapped about their lives and experiences as children of immigrants, exposing the marginalization experienced by most ethnic minorities in Germany, and the feelings of frustration and resentment that being denied a German identity can cause. The song "Fremd im eigenem Land" (Foreign in your own nation) was released by Advanced Chemistry in November 1992. The single became a staple in the German hip hop scene. It made a strong statement about the status of immigrants throughout Germany, as the group was composed of multi-national and multi-racial members. The video shows several members brandishing their German passports as a demonstration of their German citizenship to skeptical and unaccepting 'ethnic' Germans. 
This idea of national identity is important, as many rap artists in Germany have been of foreign origin. These so-called "Gastarbeiter" (guest workers) children saw breakdance, graffiti, rap music, and hip hop culture as a means of expressing themselves. Since the release of "Fremd im eigenen Land", many other German-language rappers have also tried to confront anti-immigrant ideas and develop themes of citizenship. However, though many ethnic minority youth in Germany find the these German identity themes appealing, others view the desire of immigrants to be seen as German negatively, and they have actively sought to revive and recreate concepts of identity in connection to traditional ethnic origins. 
Advanced Chemistry helped to found the German chapter of the Zulu nation.
Bibliography.
El-Tayeb, Fatima “‘If You Cannot Pronounce My Name, You Can Just Call Me 
Pride.’ Afro-German Activism, Gender, and Hip Hop,” "Gender & History"15/3(2003):459-485.
Felbert, Oliver von. “Die Unbestechlichen.” "Spex" (March 1993): 50-53.
Weheliye, Alexander G. "Phonographies:Grooves in Sonic Afro-Modernity", Duke University Press, 2005.

</doc>
