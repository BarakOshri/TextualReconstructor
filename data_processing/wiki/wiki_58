<doc id="8467" url="http://en.wikipedia.org/wiki?curid=8467" title="Draco (lawgiver)">
Draco (lawgiver)

Draco (; , "Drakōn") ("circa" 7th century BC) was the first legislator of Athens in Ancient Greece. He replaced the prevailing system of oral law and blood feud by a written code to be enforced only by a court. Draco's written law became known for its harshness, with the adjective draconian referring to similarly unforgiving rules or laws.
Life.
During the 39th Olympiad, in 622 or 621 BC, Draco established the legal code with which he is identified.
Little is known about his life. He may have belonged to the Greek nobility of the Attica, with which the 10th-century Suda text records him as contemporaneous, prior to the period of the Seven Sages of Greece. It also relates a folkloric story of his death in the Aeginetan theatre. In a traditional ancient Greek show of approval, his supporters "threw so many hats and shirts and cloaks on his head that he suffocated, and was buried in that same theatre".
Draconian constitution --.
The laws ( - "thesmoi") he laid down were the first written constitution of Athens. So that no one would be unaware of them, they were posted on wooden tablets ( - "axones"), where they were preserved for almost two centuries, on steles of the shape of three-sided pyramids ( - "kyrbeis"). The tablets were called "axones", perhaps because they could be pivoted along the pyramid's axis, to read any side.
The constitution featured several major innovations:
The laws, however, were particularly harsh. For example, any debtor whose status was lower than that of his creditor was forced into slavery. The punishment was more lenient for those owing debt to a member of a lower class. The death penalty was the punishment for even minor offences, such as "stealing a cabbage". Concerning the liberal use of the death penalty in the Draconic code, Plutarch states: "It was a lot for himself, when asked why he had fixed the punishment of death for most offences, answered that he considered these lesser crimes to deserve it, and he had no greater punishment for more important ones."
All his laws were repealed by Solon in the early 6th century BC, with the exception of the homicide law.
Law of Homicide.
After much debate from the Athenians, it was decided to revise the laws, including the homicide law, in 409 B.C. The homicide law is a highly fragmented inscription, but it does state that it is up to the victim’s relatives to prosecute a killer. According to the preserved part of the inscription, unintentional homicides receive a sentence of exile, while intentional murders are punishable by death. Apart from the inscriptions very little is known about Draco’s background or the nature of most of his laws. However, the significance of his work was prevalent when most of his laws were successfully abolished by Solon.
Council of Four Hundred.
Draco introduced the lot-chosen Council of Four Hundred —distinct from the Areopagus—which evolved in later constitutions to play a large role in Athenian democracy. Aristotle notes that Draco, while having the laws written, merely legislated for an existing unwritten Athenian constitution, such as setting exact qualifications for eligibility for office.
Draco extended the franchise to all free men who could furnish themselves with a set of military equipment. They elected the Council of Four Hundred from among their number; nine Archons and the Treasurers were drawn from persons possessing an unencumbered property of not less than ten "minas", the generals ("strategoi") and commanders of cavalry ("hipparchoi") from those who could show an unencumbered property of not less than a hundred "minas", and had children born in lawful wedlock over ten years of age. Thus, in the event of their death, their estate could pass to a competent heir. These officers were required to hold to account the "prytanes" (councillors), "strategoi" (generals) and "hipparchoi" (cavalry officers) of the preceding year until their accounts had been audited. "The Council of Areopagus was guardian of the laws, and kept watch over the magistrates to see that they executed their offices in accordance with the laws. Any person who felt himself wronged might lay an information before the Council of Areopagus, on declaring what law was broken by the wrong done to him. But, as has been said before, loans were secured upon the persons of the debtors, and the land was in the hands of a few."

</doc>
<doc id="8468" url="http://en.wikipedia.org/wiki?curid=8468" title="Determinant">
Determinant

In linear algebra, the determinant is a value associated with a square matrix. It can be computed from the entries of the matrix by a specific arithmetic expression, while other ways to determine its value exist as well. The determinant provides important information about a matrix of coefficients of a system of linear equations, or about a matrix that corresponds to a linear transformation of a vector space. In the first case the system has a unique solution exactly when the determinant is nonzero; when the determinant is zero there are either no solutions or many solutions. In the second case the transformation has an inverse operation exactly when the determinant is nonzero. A geometric interpretation can be given to the value of the determinant of a square matrix with real entries: the absolute value of the determinant gives the scale factor by which area or volume (or a higher-dimensional analogue) is multiplied under the associated linear transformation, while its sign indicates whether the transformation preserves orientation. Thus a matrix with determinant −2, when applied to a region of the plane with finite area, will transform that region into one with twice the area, while reversing its orientation.
Determinants occur throughout mathematics. The use of determinants in calculus includes the Jacobian determinant in the substitution rule for integrals of functions of several variables. They are used to define the characteristic polynomial of a matrix that is an essential tool in eigenvalue problems in linear algebra. In some cases they are used just as a compact notation for expressions that would otherwise be unwieldy to write down.
The determinant of a matrix "A" is denoted det("A"), det "A", or |"A"|. In the case where the matrix entries are written out in full, the determinant is denoted by surrounding the matrix entries by vertical bars instead of the brackets or parentheses of the matrix. For instance, the determinant of the matrix
is written
and has the value
Although most often used for matrices whose entries are real or complex numbers, the definition of the determinant only involves addition, subtraction and multiplication, and so it can be defined for square matrices with entries taken from any commutative ring. Thus for instance the determinant of a matrix with integer coefficients will be an integer, and the matrix has an inverse with integer coefficients if and only if this determinant is 1 or −1 (these being the only invertible elements of the integers). For square matrices with entries in a non-commutative ring, for instance the quaternions, there is no unique definition for the determinant, and no definition that has all the usual properties of determinants over commutative rings.
Definition.
There are various ways to define the determinant of a square matrix "A", i.e. one with the same number of rows and columns. Perhaps the most natural way is expressed in terms of the columns of the matrix. If we write an matrix in terms of its column vectors
where the formula_5 are vectors of size "n", then the determinant of "A" is defined so that
where "b" and "c" are scalars, "v" is any vector of size "n" and "I" is the identity matrix of size "n". These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.
Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has "n" terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an matrix contributes "n"! terms), so it will first be given explicitly for the case of matrices and matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.
Assume "A" is a square matrix with "n" rows and "n" columns, so that it can be written as
The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.
The determinant of "A" is denoted as det("A"), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:
2 × 2 matrices.
The determinant of a matrix is defined by
If the matrix entries are real numbers, the matrix "A" can be used to represent two linear maps: one that maps the standard basis vectors to the rows of "A", and one that maps them to the columns of "A". In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at , , , and , as shown in the accompanying diagram. The absolute value of is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by "A". (The parallelogram formed by the columns of "A" is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)
The absolute value of the determinant together with the sign becomes the "oriented area" of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).
Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by "A". When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.
The object known as the "bivector" is related to these ideas. In 2D, it can be interpreted as an "oriented plane segment" formed by imagining two vectors each with origin , and coordinates and . The bivector magnitude (denoted ) is the "signed area", which is also the determinant .
3 × 3 matrices.
The determinant of a matrix is defined by
The rule of Sarrus is a mnemonic for the matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a matrix does not carry over into higher dimensions.
"n" × "n" matrices.
The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.
The Leibniz formula for the determinant of an matrix "A" is
Here the sum is computed over all permutations σ of the set A permutation is a function that reorders this set of integers. The value in the "i"th position after the reordering σ is denoted σ"i". For example, for , the original sequence 1, 2, 3 might be reordered to , with , , and . The set of all such permutations (also known as the symmetric group on "n" elements) is denoted S"n". For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.
In any of the formula_12 summands, the term
is notation for the product of the entries at positions , where "i" ranges from 1 to "n":
For example, the determinant of a matrix "A" () is
Levi-Civita symbol.
It is sometimes useful to extend the Leibniz formula to a summation in which not only permutations, but all sequences of "n" indices in the range occur, ensuring that the contribution of a sequence will be zero unless it denotes a permutation. Thus the totally antisymmetric Levi-Civita symbol formula_16 extends the signature of a permutation, by setting formula_17 for any permutation σ of "n", and formula_18 when no permutation σ exists such that formula_19 for formula_20 (or equivalently, whenever some pair of indices are equal). The determinant for an matrix can then be expressed using an "n"-fold summation as
or using two epsilon symbols as
where now each "ir" and each "jr" should be summed over .
Properties of the determinant.
The determinant has many properties. Some basic properties of determinants are:
These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 11 and 12 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.
For example, the determinant of
can be computed using the following matrices:
Here, "B" is obtained from "A" by adding −1/2×the first row to the second, so that . "C" is obtained from "B" by adding the first to the third row, so that . Finally, "D" is obtained from "C" by exchanging the second and third row, so that . The determinant of the (upper) triangular matrix "D" is the product of its entries on the main diagonal: . Therefore, .
Multiplicativity and matrix groups.
The determinant of a matrix product of square matrices equals the product of their determinants:
Thus the determinant is a "multiplicative map". This property is a consequence of the characterization given above of the determinant as the unique "n"-linear alternating function of the columns with value 1 on the identity matrix, since the function that maps can easily be seen to be "n"-linear and alternating in the columns of "M", and takes the value det("A") at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.
The determinant det("A") of a matrix "A" is non-zero if and only if "A" is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by
In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size "n") form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if "n" is 2 or 3 consists of all rotation matrices), and the special unitary group.
Laplace's formula and the adjugate matrix.
Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor "M""i","j" is defined to be the determinant of the -matrix that results from "A" by removing the "i"th row and the "j"th column. The expression is known as cofactor. The determinant of "A" is given by
Calculating det("A") by means of that formula is referred to as expanding the determinant along a row or column. For the example matrix
Laplace expansion along the second column (, the sum runs over "i") yields:
However, Laplace expansion is efficient for small matrices only.
The adjugate matrix adj("A") is the transpose of the matrix consisting of the cofactors, i.e.,
Sylvester's determinant theorem.
Sylvester's determinant theorem states that for "A", an matrix, and "B", an matrix (so that "A" and "B" have dimensions allowing them to be multiplied in either order):
where "I""m" and "I""n" are the and identity matrices, respectively.
From this general result several consequences follow.
Properties of the determinant in relation to other notions.
Relation to eigenvalues and trace.
Determinants can be used to find the eigenvalues of the matrix "A": they are the solutions of the characteristic equation
where "I" is the identity matrix of the same dimension as "A". Conversely, det("A") is the product of the eigenvalues of "A", counted with their algebraic multiplicities. The product of all non-zero eigenvalues is referred to as pseudo-determinant.
An Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices
being positive, for all "k" between 1 and "n".
The trace tr("A") is by definition the sum of the diagonal entries of "A" and also equals the sum of the eigenvalues. Thus, for complex matrices "A",
or, for real matrices "A",
Here exp("A") denotes the matrix exponential of "A", because every eigenvalue λ of "A" corresponds to the eigenvalue exp(λ) of exp("A"). In particular, given any logarithm of "A", that is, any matrix "L" satisfying
the determinant of "A" is given by
For example, for , , and , respectively,
cf. Cayley-Hamilton theorem. Such expressions are deducible from Newton's identities.
In the general case,
where the sum is taken over the set of all integers "kl" ≥ 0 satisfying the equation
This formula can also be used to find the determinant of a matrix "AIJ" with multidimensional indices and . The product and trace of such matrices are defined in a natural way as
An arbitrary dimension "n" identity can be obtained from the Mercator series expansion of the logarithm,
where "I" is the identity matrix. The sum and the expansion of the exponential only need to go up to "n" instead of ∞, since the determinant cannot exceed "O"("An").
Cramer's rule.
For a matrix equation
the solution is given by Cramer's rule:
where "A""i" is the matrix formed by replacing the "i"th column of "A" by the column vector "b". This follows immediately by column expansion of the determinant, i.e.
where the vectors formula_5 are the columns of "A". The rule is also implied by the identity
It has recently been shown that Cramer's rule can be implemented in O("n"3) time, which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.
Block matrices.
Suppose "A", "B", "C", and "D" are matrices of dimension , , , and , respectively. Then
This can be seen from the Leibniz formula or by induction on "n". When "A" is invertible, employing the following identity
leads to
When "D" is invertible, a similar identity with formula_59 factored out can be derived analogously, that is,
When the blocks are square matrices of the same order further formulas hold. For example, if "C" and "D" commute (i.e., ), then the following formula comparable to the determinant of a matrix holds:
When "A" = "D" and "B" = "C", the blocks are square matrices of the same order and the following formula holds (even if "A" and "B" do not commute)
When "D" is a 1×1 matrix, "B" is a column vector, and "C" is a row vector then
Derivative.
By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:
where adj("A") denotes the adjugate of "A". In particular, if "A" is invertible, we have
Expressed in terms of the entries of "A", these are
Yet another equivalent formulation is
using big O notation. The special case where formula_68, the identity matrix, yields
This identity is used in describing the tangent space of certain matrix Lie groups.
If the matrix A is written as formula_70 where a, b, c are vectors, then the gradient over one of the three vectors may be written as the cross product of the other two:
Abstract algebraic aspects.
Determinant of an endomorphism.
The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices "A" and "B" are similar, if there exists an invertible matrix "X" such that . Indeed, repeatedly applying the above identities yields
The determinant is therefore also called a similarity invariant. The determinant of a linear transformation
for some finite-dimensional vector space "V" is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in "V". By the similarity invariance, this determinant is independent of the choice of the basis for "V" and therefore only depends on the endomorphism "T".
Transformation on alternating multilinear "n"-forms.
The vector space "W" of all alternating multilinear "n"-forms on an "n"-dimensional vector space "V" has dimension one. To each linear transformation "T" on "V" we associate a linear transformation "T"′ on "W", where for each "w" in "W" we define . As a linear transformation on a one-dimensional space, "T"′ is equivalent to a scalar multiple. We call this scalar the determinant of "T".
Exterior algebra.
The determinant can also be characterized as the unique function
from the set of all matrices with entries in a field "K" to this field satisfying the following three properties: first, "D" is an "n"-linear function: considering all but one column of "A" fixed, the determinant is linear in the remaining column, that is
for any column vectors "v"1, ..., "v""n", and "w" and any scalars (elements of "K") "a" and "b". Second, "D" is an alternating function: for any matrix "A" with two identical columns . Finally, "D"("I""n") = 1. Here "I""n" is the identity matrix.
This fact also implies that every other "n"-linear alternating function satisfies
The last part in fact follows from the preceding statement: one easily sees that if "F" is nonzero it satisfies , and function that associates "F"("M")/"F"("I") to "M" satisfies all conditions of the theorem. The importance of stating this part is mainly that it remains valid if "K" is any commutative ring rather than a field, in which case the given argument does not apply.
The determinant of a linear transformation of an "n"-dimensional vector space "V" can be formulated in a coordinate-free manner by considering the "n"th exterior power Λ"n""V" of "V". "A" induces a linear map
As Λ"n""V" is one-dimensional, the map Λ"n"A is given by multiplying with some scalar. This scalar coincides with the determinant of "A", that is to say
This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the parity of the determinant; likewise, permuting the vectors in the exterior product to , say, also alters the parity.
For this reason, the highest non-zero exterior power Λ"n"("V") is sometimes also called the determinant of "V" and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms Λ"k""V" with .
Square matrices over commutative rings and abstract properties.
The determinant of a matrix can be defined, for example using the Leibniz formula, for matrices with entries in any commutative ring. Briefly, a commutative ring is a structure where addition, subtraction and multiplication are defined, and multiplication is associative, commutative and distributive over addition. For example, the integers form a commutative ring.
Several of the properties of the determinant of real matrices carry over unchanged to determinants of these more general matrices: the determinant is multiplicative in this more general situation, and Cramer's rule also holds. A square matrix over a commutative ring "R" is invertible if and only if its determinant is a invertible element in "R". If "R" is a field, this latter condition is equivalent to the determinant being nonzero. For example, a matrix "A" with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.
The determinant defines a mapping
between the group of invertible matrices with entries in "R" and the multiplicative group of units in "R". Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism , there is a map given by replacing all entries in "R" by their images under "f". The determinant respects these maps, i.e., given a matrix with entries in "R", the identity
holds. For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo "m" of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo "m" (the latter determinant being computed using modular arithmetic). In the more high-brow parlance of category theory, the determinant is a natural transformation between the two functors GL"n" and (⋅)×. Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,
Generalizations and related notions.
Infinite matrices.
For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in Leibniz' formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.
The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula
Another infinite-dimensional notion of determinant is the functional determinant.
Related notions for non-commutative rings.
For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear with a regular element of "R" as value on some pair of arguments implies that "R" is commutative). Nevertheless various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and "q"-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.
Further variants.
Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.
The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz' rule are omitted. The immanant generalizes both by introducing a character of the symmetric group S"n" in Leibniz' rule.
Calculation.
Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques. Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.
Naive methods of implementing an algorithm to compute the determinant include using Leibniz' formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order "n"! ("n" factorial) for an matrix "M". For example, Leibniz' formula requires to calculate "n"! products. Therefore, more involved techniques have been developed for calculating determinants.
Decomposition methods.
Given a matrix "A", some methods compute its determinant by writing "A" as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O("n"3), which is a significant improvement over O("n"!)
The LU decomposition expresses "A" in terms of a lower triangular matrix "L", an upper triangular matrix "U" and a permutation matrix "P":
The determinants of "L" and "U" can be quickly calculated, since they are the products of the respective diagonal entries. The determinant of "P" is just the sign formula_85 of the corresponding permutation (which is +1 for an even number of permutations and is −1 for an uneven number of permutations). The determinant of "A" is then
Moreover, the decomposition can be chosen such that "L" is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies to
Further methods.
If the determinant of "A" and the inverse of "A" have already been computed, the matrix determinant lemma allows to quickly calculate the determinant of , where "u" and "v" are column vectors.
Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed algorithms with run-time proportional to "n"4 exist. An algorithm of Mahajan and Vinay, and Berkowitz is based on closed ordered walks (short "clow"). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.
If two matrices of order "n" can be multiplied in time "M"("n"), where for some , then the determinant can be computed in time O("M"("n")). This means, for example, that an O("n"2.376) algorithm exists based on the Coppersmith–Winograd algorithm.
Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) methods is of order O("n"3), but the bit length of intermediate values can become exponentially long. The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order "n"3, but the bit complexity is roughly the bit size of the original entries in the matrix times "n".
History.
Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook "The Nine Chapters on the Mathematical Art" (九章算術, Chinese scholars, around the 3rd century BC). In Europe, determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.
In Japan, Seki Takakazu (関 孝和) is credited with the discovery with the resultant and determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).
It was Vandermonde (1771) who first recognized determinants as independent functions. Laplace (1772) gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order. Lagrange was the first to apply determinants to questions of elimination theory; he proved many special cases of general identities.
Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used "resultant"), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.
The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of "m" columns and "n" rows, which for the special case of reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense, summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's. With him begins the theory in its generality.
The next important figure was Jacobi (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in "Crelle" for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called "alternants". About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.
The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.
Applications.
Linear independence.
As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors "v"1, "v"2 in R3, a third vector "v"3 lies in the plane spanned by the former two vectors exactly if the determinant of the matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given "n" functions "f"1("x"), ..., "f""n"("x") (supposed to be times differentiable), the Wronskian is defined to be
It is non-zero (for some "x") in a specified interval if and only if the given functions and all their derivatives up to order "n"−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.
Orientation of a basis.
The determinant can be thought of as assigning a number to every sequence of "n" vectors in R"n", by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in R"n" represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.
More generally, if the determinant of "A" is positive, "A" represents an orientation-preserving linear transformation (if "A" is an orthogonal or matrix, this is a rotation), while if it is negative, "A" switches the orientation of the basis.
Volume and Jacobian determinant.
As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if is the linear map represented by the matrix "A", and "S" is any measurable subset of R"n", then the volume of "f"("S") is given by |det("A")| times the volume of "S". More generally, if the linear map is represented by the matrix "A", then the "n"-dimensional volume of "f"("S") is given by:
By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is , or any other combination of pairs of vertices that would form a spanning tree over the vertices.
For a general differentiable function, much of the above carries over by considering the Jacobian matrix of "f". For
the Jacobian is the matrix whose entries are given by
Its determinant, the Jacobian determinant appears in the higher-dimensional version of integration by substitution: for suitable functions "f" and an open subset "U" of R'"n" (the domain of "f"), the integral over "f"("U") of some other function is given by
The Jacobian also occurs in the inverse function theorem.
Vandermonde determinant (alternant).
Third order
In general, the "n"th-order Vandermonde determinant is 
where the right-hand side is the continued product of all the differences that can be formed from the "n"("n"−1)/2 pairs of numbers taken from "x"1, "x"2, ..., "x""n", with the order of the differences taken in the reversed order of the suffixes that are involved.
Circulants.
Second order
Third order
where ω and ω2 are the complex cube roots of 1. In general, the "n"th-order circulant determinant is
where ω"j" is an "n"th root of 1.

</doc>
<doc id="8470" url="http://en.wikipedia.org/wiki?curid=8470" title="David Ricardo">
David Ricardo

David Ricardo (18 April 1772 – 11 September 1823) was a British political economist. He was one of the most influential of the classical economists, along with Thomas Malthus, Adam Smith, and James Mill. He began his professional life as a broker and financial market speculator. He amassed a considerable personal fortune, largely from financial market speculation and, having retired, bought a seat in the U.K. Parliament. He held his parliamentary seat for the last four years of his life. Perhaps his most important legacy is his theory of comparative advantage, which suggests that a nation should concentrate its resources solely in industries where it is "most" internationally competitive and trade with other countries to obtain products not produced nationally. In essence, Ricardo promoted the idea of extreme industry specialisation by nations, to the point of dismantling internationally competitive and otherwise profitable industries. In this thinking Ricardo assumed the existence of a national industry policy aimed at promoting some industries to the detriment of others. For Ricardo some form of Central Economic Planning was a given. Ricardo's theory of comparative advantage has been challenged by, among others, Joan Robinson and Piero Sraffa, but remains the cornerstone of the argument in favour of international free trade as a means of increasing economic prosperity. The theory of comparative advantage was the forerunner of the push towards globalisation via increased international trade, the guiding theme in economic policy currently promoted by the OECD and the World Trade Organisation.
Personal life.
Born in London, England, Ricardo was the third of 17 children of a Sephardic Jewish family of Portuguese origin who had recently relocated from the Dutch Republic. His father, Abraham Ricardo, was a successful stockbroker. He began working with his father at the age of 14. At age 21, Ricardo eloped with a Quaker, Priscilla Anne Wilkinson, and became a Unitarian, leading to estrangement from his family. His father disowned him and his mother apparently never spoke to him again.
Following his estrangement from his father he started a successful business as a broker with the support of Lubbocks and Forster, an eminent banking house. He made the bulk of his fortune as a result of speculation on the outcome of the Battle of Waterloo, using methods which today would result in prosecution for . Prior to the battle, Ricardo to convey early results of the outcome, he then deliberately created the impression the French had won by openly selling British securities. A market panic ensued. Following this panic he moved to buy British securities at a steep discount. The Sunday Times reported in Ricardo’s obituary, published on 14 September 1823, that during the Battle of Waterloo Ricardo "netted upwards of a million sterling", a huge sum at the time. Following this trading coup, he retired. He was 41. He purchased Gatcombe Park, an estate in Gloucestershire, now owned by Princess Anne, the Princess Royal. He was appointed High Sheriff of Gloucestershire for 1818–19.
Some years into retirement Ricardo became keen to enter Parliament and in August 1818 he secured Lord Portarlington’s borough for £4,000, as part of the terms of a loan of £25,000. As a result Ricardo entered the House of Commons, representing Portarlington, an Irish rotten borough. He was 47 years of age. He held the seat until his death four years later.
Ricardo was a close friend of James Mill. Other notable friends included Jeremy Bentham and Thomas Malthus, with whom Ricardo had a considerable debate (in correspondence) over such things as the role of landowners in a society. He also was a member of Malthus' Political Economy Club, and a member of the King of Clubs. He was one of the original members of The Geological Society. His sister was Sarah Ricardo-Porter, and was an author in her own right (e.g., "Conversations in Arithmetic").
Death and legacy.
Ten years after retiring and four years after entering Parliament Ricardo died from an infection of the middle ear that spread into the brain and induced septicaemia. He was 51.
He had eight children, including three sons, of whom Osman Ricardo (1795–1881; MP for Worcester 1847–1865) and another David Ricardo (1803–1864, MP for Stroud 1832–1833), became Members of Parliament, while the third, Mortimer Ricardo, served as an officer in the Life Guards and was a deputy lieutenant for Oxfordshire.
Ricardo is buried in an ornate grave in the churchyard of Saint Nicholas in Hardenhuish, now a suburb of Chippenham, Wiltshire. The inscription on his grave reads: "A Jew, born in Holland, he was one of the first free traders and a famous Radical in his day." At the time of his death his fortune was estimated at about £600,000.
Ideas.
Ricardo became interested in economics after reading Adam Smith's "The Wealth of Nations" in 1799. He wrote his first economics article at age 37. Ricardo's idea became accepted in England and have become orthodox economic ideas in the modern western world where the government is seen as having a determining role in economic development. 
He was also an abolitionist, speaking at a meeting of the Court of the East India Company in March 1823, where he said he regarded slavery as stain on the character of the nation. His sister, Hanna, had married David Samuda who owned a substantial number of slaves in Jamaica.
Comparative advantage.
Between 1500 and 1750 most economists advocated Mercantilism which promoted the idea of international trade for the purpose of gaining bullion by running a trade surplus with other countries. Ricardo challenged the idea that the purpose of trade was merely to accumulate gold or silver. With "comparative advantage" Ricardo argued in favour of industry specialisation and free trade. He attempted to prove, using simple mathematics, that industry specialization combined with free international trade always produces positive results. This theory expanded on the concept of absolute advantage. 
Ricardo argued that there is mutual national benefit from trade even if one country is more competitive in every area than its trading counterpart and that a nation should concentrate resources only on industries where it had a comparative advantage, that is in those industries in which it has the greatest competitive edge. Ricardo suggested that national industries which were, in fact, profitable and internationally competitive should be jettisoned in favour of the most competitive industries. Ricardo's theory of comparative advantage assumes the existence of an industry and trade policy at a national level. It does not presume that business decisions are or should be made independently by entrepreneurs on the basis of viability or profit.
Ricardo attempted to prove, using a simple numerical example, that international trade is always beneficial. Paul Samuelson called the numbers used in Ricardo's numerical example dealing with trade between England and Portugal the "four magic numbers". "In spite of the fact that the Portuguese could produce both cloth and wine with less amount of labor, Ricardo suggested that "theoretically" both countries benefit from trade with each other."
As Joan Robinson subsequently pointed out in reality following an opening of free trade with England, Portugal endured centuries of economic underdevelopment: "the imposition of free trade on Portugal killed off a promising textile industry and left her with a slow-growing export market for wine, while for England, exports of cotton cloth led to accumulation, mechanisation and the whole spiralling growth of the industrial revolution". Robinson argued that Ricardo's example required that economies were in static equilibrium positions with full employment and that there could not be a trade deficit or a trade surplus. These conditions, she wrote, were not relevant to the real world. She also argued that Ricardo's theory did not take into account that some countries may be at different levels of development and that this raised the prospect of 'unequal exchange' which might hamper a country's development, as we saw in the case of Portugal.
Protectionism.
Like Adam Smith, Ricardo was an opponent of protectionism for national economies, especially for agriculture. He believed that the British "Corn Laws"—tariffs on agricultural products—ensured that less-productive domestic land would be harvested and rents would be driven up . Thus, profits would be directed toward landlords and away from the emerging industrial capitalists. Since Ricardo believed landlords tended to squander their wealth on luxuries, rather than invest, he believed that the Corn Laws were leading to the stagnation of the British economy. In 1846, his nephew John Lewis Ricardo, MP for Stoke-on-Trent, advocated free trade and the repeal of the Corn Laws.
Modern empirical analysis of the Corn Laws yield mixed results. Parliament repealed the Corn Laws in 1846.
Criticism of the Ricardian theory of trade.
Ricardo's argument in favour of free trade has been attacked by those who believe trade restriction can be necessary. Utsa Patnaik claims that Ricardian theory of international trade contains a logical fallacy. Ricardo assumed that in both countries two goods are producible and actually are produced, but developed and underdeveloped countries often trade those goods which are not producible in their own country. For example, many Northern countries do not produce tropical fruits. In these cases, one cannot define which country has comparative advantage.
Critics also argue that Ricardo's theory of comparative advantage is flawed in that it assumes production is continuous and absolute. In the real world, events outside the realm of human control (e.g. natural disasters) can disrupt production. In this case, specialisation could cripple a country that depends on imports from foreign, naturally disrupted countries. For example, if an industrially based country trades its manufactured goods with an agrarian country in exchange for agricultural products, a natural disaster in the agricultural country (e.g. drought) may cause an industrially based country to starve.
The development economist Ha-Joon Chang challenges the argument that free trade benefits every country:
Ricardo’s theory is absolutely right—within its narrow confines. His theory correctly says that, "accepting their current levels of technology as given", it is better for countries to specialize in things that they are relatively better at. One cannot argue with that. His theory fails when a country wants to acquire more advanced technologies—that is, when it wants to develop its economy. It takes time and experience to absorb new technologies, so technologically backward producers need a period of protection from international competition during this period of learning. Such protection is costly, because the country is giving up the chance to import better and cheaper products. However, it is a price that has to be paid if it wants to develop advanced industries. Ricardo’s theory is, thus seen, for those who accept the "status quo" but not for those who want to change it.
Value theory.
Ricardo's most famous work is his "Principles of Political Economy and Taxation" (1817). Ricardo opens the first chapter with a statement of the labour theory of value. His labour theory of value required several assumptions:
Ricardo himself realized that the second and third assumptions were quite unrealistic and hence admitted two exceptions to his labour theory of value:
Ricardo continued to work on his value theory to the end of his life.
Rent.
Ricardo is responsible for developing theories of rent, wages, and profits. He defined rent as "the difference between the produce obtained by the employment of two equal quantities of capital and labor." Ricardo believed that the process of economic development, which increased land utilization and eventually led to the cultivation of poorer land, principally benefited landowners. According to Ricardo, such premium over "real social value" that is reaped due to ownership constitutes value to an individual but is at best a paper monetary return to "society". The portion of such purely individual benefit, and exclusively that portion, that accrues to scarce resources such as land or gold, over and above any socially beneficial exchange, Ricardo labels "rent".
Ricardo concluded that a tax on land value, equivalent to a tax on the land rent, minus the improvements, was the only form of taxation that would not lead to price increases. Land itself has no cost of production, because it is not produced by humans. Thus, the price is not determined by the cost, but only by the best available rent-free alternative, not by the tax burdens of the person claiming exclusive use.
Malthus's criticism and Extrapolation of the problem of Ricardian Rent.
In attempting to demonstrate that Ricardian Rent constitutes value for nothing Ricardo was neglecting Say's Law that all savings by-definition-equals investment. Malthus suggested that rent, however misplaced, constitutes a prime source of savings and investment for the future.
Ricardian equivalence.
Another idea associated with Ricardo is Ricardian equivalence, an argument suggesting that in some circumstances a government's choice of how to pay for its spending ("i.e.," whether to use tax revenue or issue debt and run a deficit) might have no effect on the economy. Ricardo notes that the proposition is theoretically implied in the presence of intertemporal optimisation by rational tax-payers: but that since tax-payers do not act so rationally, the proposition fails to be true in practice. Thus, while the proposition bears his name, he does not seem to have believed it. Economist Robert Barro is responsible for its modern prominence.
Ricardo's theories of wages and profits.
Several authorities consider that Ricardo is the source of the concepts behind the so-called Iron Law of Wages, according to which wages naturally tend to a subsistence level. Others dispute the assignment to Ricardo of this idea.
In his "Theory of Profit", Ricardo stated that as real wages increase, real profits decrease because the revenue from the sale of manufactured goods is split between profits and wages. He said in his "Essay on Profits", "Profits depend on high or low wages, wages on the price of necessaries, and the price of necessaries chiefly on the price of food."
His influence and intellectual legacy.
David Ricardo's ideas had a tremendous influence on later developments in economics. US economists rank Ricardo as the second most influential economic thinker, behind Adam Smith, prior to the twentieth century.
Ricardo became the theoretical father of classical political economy. However, Schumpeter coined an expression "Ricardian vice", which indicates that rigorous logic does not provide a good economic theory. This criticism applies also to most neoclassical theories, which make heavy use of mathematics, but are, according to him, theoretically unsound, because the conclusion being drawn does not logically follow from the theories used to defend it.
Ricardian socialists.
Ricardo's writings fascinated a number of early socialists in the 1820s, who thought his value theory had radical implications. They argued that, in view of labor theory of value, labor produces the entire product, and the profits capitalists get are a result of exploitations of workers. These include Thomas Hodgskin, William Thompson, John Francis Bray, and Percy Ravenstone.
Georgists.
Georgists believe that rent, in the sense that Ricardo used, belongs to the community as a whole. Henry George was greatly influenced by Ricardo, and often cited him, including in his most famous work, Progress and Poverty from 1879. In the preface to the fourth edition, he wrote: "What I have done in this book, if I have correctly solved the great problem I have sought to investigate, is, to unite the truth perceived by the school of Smith and Ricardo to the truth perceived by the school of Proudhon and Lasalle; to show that laissez faire (in its full true meaning) opens the way to a realization of the noble dreams of socialism; to identify social law with moral law, and to disprove ideas which in the minds of many cloud grand and elevating perceptions."
Neo-Ricardians.
After the rise of the 'neoclassical' school, Ricardo's influence declined temporarily. It was Piero Sraffa, the editor of the Collected Works of David Ricardo and the author of seminal "Production of Commodities by Means of Commodities", who resurrected Ricardo as the originator of another strand of economics thought, which was effaced with the arrival of the neoclassical school. The new interpretation of Ricardo and Sraffa's criticism against the marginal theory of value gave rise to a new school, now named neo-Ricardian or Sraffian school. Major contributors to this school includes Luigi Pasinetti (1930–), Pierangelo Garegnani (1930–2011), Ian Steedman (1941–), Geoffrey Harcourt (1931–), Heinz Kurz (1946–), Neri Salvadori (1951–), Pier Paolo Saviotti (–) among others. See also Neo-Ricardianism. The Neo-Ricardian school is sometimes seen to be a component of Post-Keynesian economics.
Neo-Ricardian trade theory.
Inspired by Piero Sraffa, a new strand of trade theory emerged and was named neo-Ricardian trade theory. The main contributors include Ian Steedman and Stanley Metcalfe. They have criticised neoclassical international trade theory, namely the Heckscher–Ohlin model on the basis that the notion of capital as primary factor has no method of measuring it before the determination of profit rate (thus trapped in a logical vicious circle). This was a second round of the Cambridge capital controversy, this time in the field of international trade.
Ricardian trade theory ordinarily assumes that the labour is the unique input. This is a deficiency as intermediate goods are a great part of international trade. The situation changed greatly after the appearance of Yoshinori Shiozawa's seminal work of 2007.
Yeats found that 30% of world trade in manufacturing is intermediate inputs. Bardhan and Jafee found that intermediate inputs occupy 37 to 38% in the imports to the US for the years from 1992 to 1997, whereas the percentage of intrafirm trade grew from 43% in 1992 to 52% in 1997.
Evolutionary growth theory.
Several distinctive groups have sprung out of the neo-Ricardian school. One is the evolutionary growth theory, developed notably by Luigi Pasinetti, J.S. Metcalfe, Pier Paolo Saviotti, and Koen Frenken and others.
Pasinetti argued that the demand for any commodity came to stagnate and frequently decline, demand saturation occurs. Introduction of new commodities (goods and services) is necessary to avoid economic stagnation.
Contemporary theories.
Ricardo's idea was even expanded to the case of continuum of goods by Dornbusch, Fischer, and Samuelson This formulation is employed for example by Matsuyama and others.
Unequal Exchange.
Chris Edward includes Emmanuel's Unequal Exchange theory among variations of neo-Ricardian trade theory. Arghiri Emmanuel argued that the Third World is poor because of the international exploitation of labour.
The unequal exchange theory of trade has been influential to the (new) dependency theory.
Publications.
Ricardo's publications included:
His works and writings were collected in:

</doc>
<doc id="8471" url="http://en.wikipedia.org/wiki?curid=8471" title="Delphinus">
Delphinus

Delphinus is a constellation in the northern sky, close to the celestial equator. Its name is Latin for dolphin. Delphinus was one of the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains among the 88 modern constellations recognized by the International Astronomical Union. It is one of the smaller constellations, ranked 69th in size.
Delphinus' brightest stars form a distinctive asterism that can easily be recognized. It is bordered (clockwise from north) by Vulpecula the fox, Sagitta the arrow, Aquila the eagle, Aquarius the water-carrier, Equuleus the foal and Pegasus the flying horse.
Notable features.
Stars.
Delphinus does not have any bright stars; its brightest star is of magnitude 3.8. The main asterism in Delphinus is Job's Coffin, formed from the four brightest stars: Alpha, Beta, Gamma, and Delta Delphini. Alpha and Beta Delphini are named Sualocin and Rotanev, respectively. When read backwards, they read as Nicolaus Venator, the Latinized name of Palermo Observatory's former director, Niccolò Cacciatore. However, Delphinus is in a rich Milky Way star field.
Alpha Delphini, called Sualocin, is a blue-white hued main sequence star of magnitude 3.8, 241 light-years from Earth. Beta Delphini, called Rotanev, is a close binary star and the brightest in Delphinus, divisible in only large amateur telescopes. To the unaided eye, it appears to be a white star of magnitude 3.6. It has a period of 27 years and is 97 light-years from Earth. Gamma Delphini is a celebrated binary star among amateur astronomers. The primary is a gold-colored star of magnitude 4.3 and the secondary is a yellow-tinged star of magnitude 5.1. 102 light-years away, the components of Gamma Delphini are divisible in a small amateur telescope. The secondary, also described as green, is 10 arcseconds from the primary. Struve 2725, called the "Ghost Double", is a pair that appears similar to a dimmer Gamma Delphini. Its components of magnitudes 7.6 and 8.4 are separated by 6 arcseconds and are 15 arcminutes from Gamma Delphini itself.
There are several dimmer stars in Delphinus. Delta Delphini is a type A7 IIIp star of magnitude 4.43. Epsilon Delphini, called Deneb Dulfim, meaning "tail of the Dolphin", is a star of spectral class B6 III and magnitude 4.
Delphinus is also home to several variable stars. R Delphini is a Mira-type variable star with a period of 285.5 days. Its magnitude ranges between a maximum of 7.6 and a minimum of 13.8.
Rho Aquilae moved across the border into Delphinus in 1992.
HR Delphini was a nova that brightened to magnitude 3.5 in December 1967. On 14 August 2013, a possible nova was discovered by amateur astronomer Koichi Itagaki, initially labelled PNV J20233073+2046041, now labelled Nova Delphini 2013.
Deep-sky objects.
Because it is in a rich Milky Way star field, Delphinus has several deep-sky objects. NGC 6891 is a planetary nebula of magnitude 10.5. NGC 6934 is a globular cluster of magnitude 9.75. At a distance of about 185,000 light-years, the globular cluster NGC 7006 is extremely remote. It is also fairly dim at magnitude 11.5.
Mythology.
Delphinus is associated with two stories from Greek mythology.
According to the first Greek god Poseidon wanted to marry Amphitrite, a beautiful nereid. She, however, wanting to protect her virginity, fled to the Atlas mountains. Her suitor then sent out several searchers, among them a certain Delphinus. Delphinus accidentally stumbled upon her and was able to persuade Amphitrite to accept Poseidon's wooing. Out of gratitude the god placed the image of a dolphin among the stars.
The second story tells of the Greek poet Arion of Lesbos (7th century BC), who was saved by a dolphin. He was a court musician at the palace of Periander, ruler of Corinth. Arion had amassed a fortune during his travels to Sicily and Italy. On his way home from Tarentum his wealth caused the crew of his ship to conspire against him. Threatened with death, Arion asked to be granted a last wish which the crew granted: he wanted to sing a dirge. This he did, and while doing so, flung himself into the sea. There, he was rescued by a dolphin which had been charmed by Arion's music. The dolphin carried Arion to the coast of Greece and left.
Equivalents.
In Chinese astronomy, the stars of Delphinus are located within "the Black Tortoise of the North" (北方玄武, "Běi Fāng Xuán Wǔ").
In Polynesia, two cultures recognized Delphinus as a constellation. On Pukapuka, it was called "Te Toloa" and in the Tuamotus, it was called "Te Uru-o-tiki".
Namesakes.
USS Delphinus (AF-24) and USS Delphinus (PHM-1), two United States Navy ships, are named after the constellation.
See also.
Delphinus (Chinese astronomy)

</doc>
<doc id="8472" url="http://en.wikipedia.org/wiki?curid=8472" title="Disk storage">
Disk storage

Disk storage is a general category of storage mechanisms where data are recorded by various electronic, magnetic, optical, or mechanical changes to a surface layer of one or more rotating disks. A disk drive is a device implementing such a storage mechanism and is usually distinguished from the disk medium. Notable types are the hard disk drive (HDD) containing a non-removable disk, the floppy disk drive (FDD) and its removable floppy disk, and various optical disc drives and associated optical disc media.
Disk and disc are used interchangeably except where trademarks preclude one usage, e.g. the Compact Disc logo. The choice of a particular form is frequently historical, as in IBM's usage of the disk form beginning in 1956 with the "IBM 350 disk storage unit".
Background.
Audio information was originally recorded by analog methods (see Sound recording and reproduction). Similarly the first video disc used an analog recording method. In the music industry, analog recording has been mostly replaced by digital optical technology where the data are recorded in a digital format with optical information.
The first commercial digital disk storage device was the IBM 350 which shipped in 1956 as a part of the IBM 305 RAMAC computing system. The random-access, low-density storage of disks was developed to complement the already used sequential-access, high-density storage provided by tape drives using magnetic tape. Vigorous innovation in disk storage technology, coupled with less vigorous innovation in tape storage, has reduced the difference in acquisition cost per terabyte between disk storage and tape storage; however, the total cost of ownership of data on disk including power and management remains larger than that of tape.
Disk storage is now used in both computer storage and consumer electronic storage, e.g., audio CDs and video discs (standard DVD and Blu-ray).
Access methods.
Digital disk drives are block storage devices. Each disk is divided into logical blocks (collection of sectors). Blocks are addressed using their logical block addresses (LBA). Read from or writing to disk happens at the granularity of blocks.
Originally the disk capacity was quite low and has been improved in one of several ways. Improvements in mechanical design and manufacture allowed smaller and more precise heads, meaning that more tracks could be stored on each of the disks. Advancements in data compression methods permitted more information to be stored in each of the individual sectors.
The drive stores data onto cylinders, heads, and sectors. The sectors unit is the smallest size of data to be stored in a hard disk drive and each file will have many sectors units assigned to it. The smallest entity in a CD is called a frame, which consists of 33 bytes and contains six complete 16-bit stereo samples (two bytes × two channels × six samples = 24 bytes). The other nine bytes consist of eight CIRC error-correction bytes and one subcode byte used for control and display.
The information is sent from the computer processor to the BIOS into a chip controlling the data transfer. This is then sent out to the hard drive via a multi-wire connector. Once the data are received onto the circuit board of the drive, they are translated and compressed into a format that the individual drive can use to store onto the disk itself. The data are then passed to a chip on the circuit board that controls the access to the drive. The drive is divided into sectors of data stored onto one of the sides of one of the internal disks. An HDD with two disks internally will typically store data on all four surfaces.
The hardware on the drive tells the actuator arm where it is to go for the relevant track and the compressed information is then sent down to the head which changes the physical properties, optically or magnetically for example, of each byte on the drive, thus storing the information. A file is not stored in a linear manner, rather, it is held in the best way for quickest retrieval.
Rotation speed and track layout.
Mechanically there are two different motions occurring inside the drive. One is the rotation of the disks inside the device. The other is the side-to-side motion of the head across the disk as it moves between tracks.
There are two types of disk rotation methods:
Track positioning also follows two different methods across disk storage devices. Storage devices focused on holding computer data, e.g., HDDs, FDDs, Iomega zip drives, use concentric tracks to store data. During a sequential read or write operation, after the drive accesses all the sectors in a track it repositions the head(s) to the next track. This will cause a momentary delay in the flow of data between the device and the computer. In contrast, optical audio and video discs use a single spiral track that starts at the inner most point on the disc and flows continuously to the outer edge. When reading or writing data there is no need to stop the flow of data to switch tracks. This is similar to vinyl records except vinyl records started at the outer edge and spiraled in toward the center.
Interfaces.
The disk drive interface is the mechanism/protocol of communication between the rest of the system and the disk drive itself. Storage devices intended for desktop and mobile computers typically use ATA (PATA) and SATA interfaces. Enterprise systems and high-end storage devices will typically use SCSI, SAS, and FC interfaces in addition to some use of SATA.

</doc>
<doc id="8474" url="http://en.wikipedia.org/wiki?curid=8474" title="Arthur Wellesley, 1st Duke of Wellington">
Arthur Wellesley, 1st Duke of Wellington

Field Marshal Arthur Wellesley, 1st Duke of Wellington, (1 May 1769 – 14 September 1852), was a British soldier and statesman, a native of Ireland from the Anglo-Irish Ascendancy, and one of the leading military and political figures of the 19th century. His importance in national history is such that he is often referred to as "the Duke of Wellington" instead of "the 1st Duke of Wellington" (overshadowing the heirs to his dukedom including the current duke — see Dukes of Wellington).
Wellesley was commissioned as an ensign in the British Army in 1787. Serving in Ireland as aide-de-camp to two successive Lords Lieutenant of Ireland he was also elected as a Member of Parliament in the Irish House of Commons. A colonel by 1796, Wellesley saw action in the Netherlands and in India, where he fought in the Fourth Anglo-Mysore War at the Battle of Seringapatam. He was appointed governor of Seringapatam and Mysore in 1799 and as a newly appointed major-general won a decisive victory over the Maratha Confederacy at the Battle of Assaye in 1803.
Wellesley rose to prominence as a general during the Peninsular campaign of the Napoleonic Wars, and was promoted to the rank of field marshal after leading the allied forces to victory against the French at the Battle of Vitoria in 1813. Following Napoleon's exile in 1814, he served as the ambassador to France and was granted a dukedom. During the Hundred Days in 1815, he commanded the allied army which, together with a Prussian army under Blücher, defeated Napoleon at the Battle of Waterloo. Wellesley's battle record is exemplary, ultimately participating in some 60 battles during the course of his military career.
Wellesley is famous for his adaptive defensive style of warfare, resulting in several victories against a numerically superior force while minimising his own losses. He is regarded as one of the greatest defensive commanders of all time, and many of his tactics and battle plans are still studied in military academies around the world. Regarded as one of Britain's most significant military figures, in 2002, he was placed at number 15 in the BBC's poll of the 100 Greatest Britons.
He was twice British prime minister under the Tory party and oversaw the passage of the Catholic Relief Act 1829. He was prime minister from 1828–30 and served briefly in 1834. He was unable to prevent the passage of the Reform Act 1832 and continued as one of the leading figures in the House of Lords until his retirement. He remained Commander-in-Chief of the British Army until his death.
Early life and education.
Wellesley was born into a wealthy Anglo-Irish aristocratic family in the Kingdom of Ireland as Hon. Arthur Wesley, the third of five surviving sons (fourth otherwise) to The 1st Earl of Mornington and his wife Anne, the eldest daughter of The 1st Viscount Dungannon. He was most likely born at their townhouse, 24 Upper Merrion Street, Dublin, now The Merrion Hotel. His biographers mostly follow the contemporary newspaper evidence in saying he was born 1 May 1769, the day he was baptised. His mother, Anne, Countess of Mornington, recalled in 1815 that he had been born at 6 Merrion Street, Dublin. Other places which have been put forward as the location of his birth include Mornington House (the house which used to be next door) - as his father had asserted, the Dublin packet boat and the mansion in the family estate of Athy (consumed in the fires of 1916) - as the Duke apparently put on his 1851 census return.
He spent most of his childhood at his family's two homes, the first a large house in Dublin and the second, Dangan Castle, north of Summerhill on the Trim Road in County Meath. In 1781, Arthur's father died and his eldest brother Richard inherited his father's earldom.
He went to the diocesan school in Trim when at Dangan, Mr. Whyte's Academy when in Dublin, and Brown's School in Chelsea when in London. He then enrolled at Eton, where he studied from 1781 to 1784. His loneliness there caused him to hate it, and makes it highly unlikely that he actually said, "The Battle of Waterloo was won on the playing fields of Eton". Moreover, Eton had no playing fields at the time. In 1785, a lack of success at Eton, combined with a shortage of family funds due to his father's death, forced the young Wellesley and his mother to move to Brussels. Until his early twenties, Arthur continued to show little sign of distinction and his mother grew increasingly concerned at his idleness, stating, "I don't know what I shall do with my awkward son Arthur".
A year later, Arthur enrolled in the French Royal Academy of Equitation in Angers, where he progressed significantly, becoming a good horseman and learning French, which was later to prove very useful. Upon returning to England in late 1786, he astonished his mother with his improvement.
Military career.
Early career.
Despite his new promise he had yet to find a job and his family was still short of money, so upon the advice of his mother, his brother Richard asked his friend The 4th Duke of Rutland (then Lord Lieutenant of Ireland) to consider Arthur for a commission in the army. Soon after, on 7 March 1787 he was gazetted ensign in the 73rd Regiment of Foot. In October, with the assistance of his brother, he was assigned as aide-de-camp, on ten shillings a day (twice his pay as an ensign), to the new Lord Lieutenant of Ireland Lord Buckingham. He was also transferred to the new 76th Regiment forming in Ireland and on Christmas Day, 1787, was promoted to lieutenant. During his time in Dublin his duties were mainly social; attending balls, entertaining guests and providing advice to Buckingham. While in Ireland, he over extended himself in borrowing due to his occasional gambling, but in his defence stated that "I have often known what it was to be in want of money, but I have never got helplessly into debt".
On 23 January 1788, he transferred into the 41st Regiment of Foot, then again on 25 June 1789, still a lieutenant, he transferred to the 12th (Prince of Wales's) Regiment of (Light) Dragoons and, according to military historian Richard Holmes, he also dipped a reluctant toe into politics. Shortly before the general election of 1789, he went to the "rotten borough" of Trim to speak against the granting of the title "Freeman" of Dublin to the parliamentary leader of the Irish Patriot Party, Henry Grattan. Succeeding, he was later nominated and duly elected as a Member of Parliament for Trim in the Irish House of Commons. Because of the limited suffrage at the time, he sat in a parliament where at least two-thirds of the members owed their election to the landowners of fewer than a hundred boroughs. Wellesley continued to serve at Dublin Castle, voting with the government in the Irish parliament over the next two years. On 30 January 1791 he became a captain and was transferred to the 58th Regiment of Foot.
On 31 October, he transferred to the 18th Light Dragoons and it was during this period that he grew increasingly attracted to Kitty Pakenham, the daughter of Edward Pakenham, 2nd Baron Longford. She was described as being full of 'gaiety and charm'. In 1793, he sought her hand, but was turned down by her brother Thomas, Earl of Longford, who considered Wellesley to be a young man, in debt, with very poor prospects. An aspiring amateur musician, Wellesley, devastated by the rejection, burnt his violins in anger, and resolved to pursue a military career in earnest. Gaining further promotion (largely by purchasing his rank, which was common in the British Army at the time), he became a major in the 33rd Regiment in 1793. A few months later, in September, his brother lent him more money and with it he purchased a lieutenant-colonelcy in the 33rd.
Netherlands.
In 1793, the Duke of York was sent to Flanders in command of the British contingent of an allied force destined for the invasion of France. In 1794, the 33rd regiment was sent to join the force and Wellesley, having just purchased his majority on 30 April 1793, set sail from Cork for Flanders in June, destined for his first real battle experience. Three months later on 30 September 1793 he purchased the lieutenant colonelcy of his regiment. During the campaign he rose to command a brigade and in September Wellesley's unit came under fire just east of Breda, just before the Battle of Boxtel. For the latter part of the campaign, during the winter, his unit defended the line of the Waal River, during which time he became ill for a while, owing to the damp environment. Though the campaign was to prove unsuccessful, with the Duke of York's force returning in 1795, Wellesley was to learn several valuable lessons, including the use of steady fire lines against advancing columns and of the merits of supporting sea-power. He concluded that many of the campaign's blunders were due to the faults of the leaders and the poor organisation at headquarters. He remarked later of his time in the Netherlands that "At least I learned what not to do, and that is always a valuable lesson".
Returning to England in March 1795, he was returned as a Member of Parliament for Trim for a second time. He hoped to be given the position of secretary of war in the new Irish government but the new lord-lieutenant, Lord Camden, was only able to offer him the post of Surveyor-General of the Ordnance. Declining the post, he returned to his regiment, now at Southampton preparing to set sail for the West Indies. After seven weeks at sea, a storm forced the fleet back to Poole, England. The 33rd was given time to convalesce and a few months later, Whitehall decided to send the regiment to India. Wellesley was promoted full colonel by seniority on 3 May 1796 and a few weeks later set sail for Calcutta with his regiment.
India.
Arriving in Calcutta in February 1797 he spent several months there, before being sent on a brief expedition to the Philippines, where he established a list of new hygiene precautions for his men to deal with the unfamiliar climate. Returning in November to India, he learnt that his elder brother Richard, now known as Lord Mornington, had been appointed as the new Governor-General of India.
In 1798, he changed the spelling of his surname to "Wellesley"; up to this time he was still known as Wesley, which his oldest brother considered the ancient and proper spelling.
Fourth Anglo-Mysore War.
As part of the campaign to extend the rule of the British East India Company, the Fourth Anglo-Mysore War broke out in 1798 against the Sultan of Mysore, Tipu Sultan. Arthur's brother Richard ordered that an armed force be sent to capture Seringapatam and defeat Tipu. Under the command of General Harris, some 24,000 troops were dispatched to Madras (to join an equal force being sent from Bombay in the west). Arthur and the 33rd sailed to join them in August.
After extensive and careful logistic preparation (which would become one of Wellesley's main attributes) the 33rd left with the main force in December and travelled across of jungle from Madras to Mysore. On account of his brother, during the journey, Wellesley was given an additional command, that of chief advisor to the Nizam of Hyderabad's army (sent to accompany the British force). This position was to cause friction among many of the senior officers (some of whom were senior to Wellesley). Much of this friction was put to rest after the Battle of Mallavelly, some from Seringapatam, in which Harris's army attacked a large part of the sultan's army. During the battle, Wellesley led his men, in a line of battle of two ranks, against the enemy to a gentle ridge and gave the order to fire. After an extensive repetition of volleys, followed by a bayonet charge, the 33rd, in conjunction with the rest of Harris's force, forced Tipu's infantry to retreat.
Seringapatam.
Immediately after their arrival at Seringapatam on 5 April 1799, the Battle of Seringapatam began and Wellesley was ordered to lead a night attack on the village of Sultanpettah, adjacent to the fortress to clear the way for the artillery. Because of the enemy's strong defensive preparations, and the darkness, with the resulting confusion, the attack failed with 25 casualties. Wellesley suffered a minor injury to his knee from a spent musket-ball. Although they would re-attack successfully the next day, after time to scout ahead the enemy's positions, the affair had an impact on Wellesley. He resolved "never to attack an enemy who is preparing and strongly posted, and whose posts have not been reconnoitered by daylight".
Lewin Bentham Bowring gives this alternative account:
A few weeks later, after extensive artillery bombardment, a breach was opened in the main walls of the fortress of Seringapatam. An attack led by Major-General Baird secured the fortress. Wellesley secured the rear of the advance, posting guards at the breach and then stationed his regiment at the main palace. After hearing news of the death of the Tipu Sultan, Wellesley was the first at the scene to confirm his death, checking his pulse. Over the coming day, Wellesley grew increasingly concerned over the lack of discipline among his men, who drank and pillaged the fortress and city. To restore order, several soldiers were flogged and four hanged.
After battle and the resulting end of the war, the main force under General Harris left Seringapatam and Wellesley, aged 30, stayed behind to command the area as the new Governor of Seringapatam and Mysore. He was promoted to brigadier-general on 17 July 1801. He took residence within the Sultan's summer palace and reformed the tax and justice systems in his province to maintain order and prevent bribery. He also hunted down the mercenary 'King' Dhoondiah Waugh, who had escaped from prison in Seringapatam during the battle. Wellesley, with command of four regiments, defeated Dhoondiah's larger rebel force, along with Dhoondiah himself who was killed in the battle. He paid for the future upkeep of Dhoondiah's orphaned son.
While in India, Wellesley was ill for a considerable time, first with severe diarrhoea from the water and then with fever, followed by a serious skin infection caused by trichophyton. He received good news when in September 1802 he learnt that he had been promoted to the rank of major-general. Wellesley had been gazetted on 29 April 1802, but the news took several months to reach him by sea. He remained at Mysore until November when he was sent to command an army in the Second Anglo-Maratha War.
Second Anglo-Maratha War.
When he determined that a long defensive war would ruin his army, Wellesley decided to act boldly to defeat the numerically larger force of the Maratha Empire. With the logistic assembly of his army complete (24,000 men in total) he gave the order to break camp and attack the nearest Maratha fort on 8 August 1803. The fort surrendered on 12 August after an infantry attack had exploited an artillery-made breach in the wall. With the fort now in British control Wellesley was able to extend control southwards to the river Godavari.
Assaye.
Splitting his army into two forces, to pursue and locate the main Marathas army, (the second force, commanded by Colonel Stevenson was far smaller) Wellesley was preparing to rejoin his forces on 24 September. His intelligence, however, reported the location of the Marathas' main army, between two rivers near Assaye. If he waited for the arrival of his second force, the Marathas would be able to mount a retreat, so Wellesley decided to launch an attack immediately.
On 23 September, Wellesley led his forces over a ford in the river Kaitna and the Battle of Assaye commenced. After crossing the ford the infantry was reorganised into several lines and advanced against the Maratha infantry. Wellesley ordered his cavalry to exploit the flank of the Maratha army just near the village. During the battle Wellesley himself came under fire; two of his horses were shot from under him and he had to mount a third. At a crucial moment, Wellesley regrouped his forces and ordered Colonel Maxwell (later killed in the attack) to attack the eastern end of the Maratha position while Wellesley himself directed a renewed infantry attack against the centre.
An officer in the attack wrote of the importance of Wellesley's personal leadership: "The General was in the thick of the action the whole time ... I never saw a man so cool and collected as he was ... though I can assure you, 'til our troops got the order to advance the fate of the day seemed doubtful ..." With some 6,000 Marathas killed or wounded, the enemy was routed, though Wellesley's force was in no condition to pursue. British casualties were heavy: the British losses were counted as 409 soldiers being killed out of which 164 were Europeans and the remaining 245 were Indian; a further 1,622 British soldiers were wounded and 26 soldiers were reported missing (the British casualty figures were taken from Wellesley's own despatch). Wellesley was troubled by the loss of men and remarked that he hoped "I should not like to see again such loss as I sustained on 23 September, even if attended by such gain". Years later, however, he remarked that Assaye was the best battle he ever fought.
Argaum and Gawilghur.
Despite the damage done to the Maratha army, the battle did not end the war. A few months later in November, Wellesley attacked a larger force near Argaum, leading his army to victory again, with an astonishing 5,000 enemy dead at the cost of only 361 British casualties. A further successful attack at the fortress at Gawilghur, combined with the victory of General Lake at Delhi forced the Maratha to sign a peace settlement at Anjangaon (not concluded until a year later) called as the Treaty of Surji-Anjangaon.
Military historian Richard Holmes remarked that Wellesley's experiences in India had an important influence on his personality and military tactics, teaching him much about military matters that would prove vital to his success in the Peninsular War. These included a strong sense of discipline through drill and order, the use of diplomacy to gain allies, and the vital necessity for a secure supply line. He also established a high regard for the acquisition of intelligence through scouts and spies. His personal tastes also developed, including dressing himself in white trousers, a dark tunic, with Hessian boots and black cocked hat (that later became synonymous as his style).
Leaving India.
Wellesley had grown tired of his time in India, remarking "I have served as long in India as any man ought who can serve anywhere else". In June 1804 he applied for permission to return home and as a reward for his service in India he was made a Knight of the Bath in September. While in India, Wellesley had amassed a fortune of £42,000 (considerable at the time), consisting mainly of prize money from his campaign. When his brother's term as Governor-General of India ended in March 1805, the brothers returned together to England on HMS "Howe". Arthur, coincidentally, stopped on his voyage at the little island of Saint Helena and stayed in the same building to which Napoleon I would later be exiled.
Back in Britain.
Wellesley then served in the abortive Anglo-Russian expedition to north Germany in 1805, taking a brigade to Elbe. Upon this return from the campaign, Wellesley received good news; owing to his new title and status, Kitty Pakenham's family had consented to his marrying her. Wellesley and Kitty were married in Dublin on 10 April 1806. The marriage would later prove to be unsatisfactory and the two would spend years apart while Wellesley was campaigning. He then took a period of extended leave from the army and was elected Tory member of Parliament for Rye in January 1806. A year later, he was elected MP for Newport on the Isle of Wight and was then appointed to serve as Chief Secretary for Ireland, under the Duke of Richmond. At the same time, he was made a privy counsellor. While in Ireland, he gave a verbal promise that the remaining Penal Laws would be enforced with great moderation, perhaps an indication of his later willingness to support Catholic Emancipation.
War on Denmark.
Wellesley was in Ireland in May 1807 when he heard of the British expedition to Denmark. He decided to go, stepping down from his political appointments and was appointed to command an infantry brigade in the Second Battle of Copenhagen which took place in August. He fought at the Køge, during which the men under his command took 1,500 prisoners, with Wellesley later present during the surrender.
By 30 September, he had returned to England and was raised to the rank of lieutenant general on 25 April 1808. In June 1808 he accepted the command of an expedition of 9,000 men. Preparing to sail for an attack on the Spanish colonies in South America (to assist the Latin American patriot Francisco de Miranda) his force was instead ordered to sail for Portugal, to take part in the Peninsular Campaign and rendezvous with 5,000 troops from Gibraltar.
To the Peninsula.
Ready for battle, he left Cork on 12 July 1808 to participate in the war against French forces in the Iberian Peninsula, with his skills as a commander tested and developed. According to the historian Robin Neillands, "Wellesley had by now acquired the experience on which his later successes were founded. He knew about command from the ground up, about the importance of logistics, about campaigning in a hostile environment. He enjoyed political influence and realised the need to maintain support at home. Above all, he had gained a clear idea of how, by setting attainable objectives and relying on his own force and abilities, a campaign could be fought and won."
Peninsular War.
1808.
Wellesley defeated the French at the Battle of Roliça and the Battle of Vimeiro in 1808 but was superseded in command immediately after the latter battle. General Dalrymple then signed the controversial Convention of Sintra, which stipulated that the British Royal Navy transport the French army out of Lisbon with all their loot, and insisted on the association of the only available government minister, Wellesley.
Dalrymple and Wellesley were recalled to Britain to face a Court of Enquiry. Wellesley had agreed to sign the preliminary armistice, but had not signed the convention, and was cleared.
Meanwhile, Napoleon himself entered Spain with his veteran troops to put down the revolt; the new commander of the British forces in the Peninsula, Sir John Moore, died during the Battle of Corunna in January 1809.
Although overall the land war with France was not going well from a British perspective, the Peninsula was the one theatre where they, with the Portuguese, had provided strong resistance against France and her allies. This contrasted with the disastrous Walcheren expedition, which was typical of the mismanaged British operations of the time. Wellesley submitted a memorandum to Lord Castlereagh on the defence of Portugal. He stressed its mountainous frontiers and advocated Lisbon as the main base because the Royal Navy could help to defend it. Castlereagh and the cabinet approved the memo, appointed him head of all British forces in Portugal.
1809.
Wellesley arrived in Lisbon on 22 April 1809 onboard HMS "Surveillante", after narrowly escaping shipwreck. Reinforced, he took to the offensive. In the Second Battle of Porto he crossed the Douro river in a daylight "coup de main", and routed Marshal Soult's French troops in Porto.
With Portugal secured, Wellesley advanced into Spain to unite with General Cuesta's forces. The combined allied force prepared for an assault on Victor's I Corps at Talavera, 23 July. Cuesta, however, was reluctant to agree, and was only persuaded to advance on the following day. The delay allowed the French to withdraw, but Cuesta sent his army headlong after Victor, and found himself faced by almost the entire French army in New Castile—Victor had been reinforced by the Toledo and Madrid garrisons. The Spanish retreated precipitously, necessitating the advance of two British divisions to cover their retreat.
The next day, 27 July, at the Battle of Talavera the French advanced in three columns and were repulsed several times throughout the day by Wellesley, but at a heavy cost to the British force. In the aftermath Marshal Soult's army was discovered to be advancing south, threatening to cut Wellesley off from Portugal. Wellesley moved east on 3 August to block it, leaving 1,500 wounded in the care of the Spanish, intending to confront Soult before finding out that the French were in fact 30,000 strong. The British commander sent the Light Brigade on a dash to hold the bridge over the Tagus River at Almaraz. With communications and supply from Lisbon secured for now, Wellesley considered joining with Cuesta again but found out that his Spanish ally had abandoned the British wounded to the French and was thoroughly uncooperative, promising and then refusing to supply the British forces, aggravating Wellesley and causing considerable friction between the British and their Spanish allies. The lack of supplies, coupled with the threat of French reinforcement (including the possible inclusion of Napoleon himself) in the spring, led to the British deciding to retreat into Portugal.
1810.
In 1810, a newly enlarged French army under Marshal André Masséna invaded Portugal. British opinion both at home and in the army was negative and there were suggestions that they must evacuate Portugal. Instead, Wellington first slowed the French down at Buçaco; he then prevented them from taking the Lisbon Peninsula by the construction of his massive earthworks, the Lines of Torres Vedras, which had been assembled in complete secrecy and had flanks guarded by the Royal Navy. The baffled and starving French invasion forces retreated after six months. Wellington's pursuit was frustrated by a series of reverses inflicted by Marshal Ney in a much-lauded rear guard campaign.
1811.
In 1811, Masséna returned toward Portugal to relieve Almeida; Wellington narrowly checked the French at the Battle of Fuentes de Onoro. Simultaneously, his subordinate, Viscount Beresford, fought Soult's 'Army of the South' to a mutual bloody standstill at the Battle of Albuera in May. Wellington was promoted to full General on 31 July for his services. The French abandoned Almeida, slipping away from British pursuit, but retained the twin Spanish fortresses of Ciudad Rodrigo and Badajoz, the 'Keys' guarding the roads through the mountain passes into Portugal. For his actions for the Portuguese cause, Wellesley was conferred the title of Count of Vimeiro, in the Peerage of Portugal.
1812.
In 1812, Wellington finally captured Ciudad Rodrigo by a rapid movement as the French went into winter quarters, storming it before they could react. He then moved south quickly, besieged the fortress of Badajoz for a month and captured it during one bloody night. On viewing the aftermath of the Storming of Badajoz, Wellington lost his composure and cried at the sight of the bloody carnage in the breaches.
His army now was a veteran British force reinforced by units of the retrained Portuguese army. Campaigning in Spain, he routed the French at the Battle of Salamanca, taking advantage of a minor French mispositioning. The victory liberated the Spanish capital of Madrid. As reward, he was created "Earl" and then "Marquess of Wellington" and given command of all Allied armies in Spain. Wellington attempted to take the vital fortress of Burgos, which linked Madrid to France. But failure, due in part to a lack of siege guns, forced him into a headlong retreat with the loss of over 2,000 casualties.
The French abandoned Andalusia, and combined the troops of Soult and Marmont. Thus combined, the French outnumbered the British, putting the British forces in a precarious position. Wellington withdrew his army and, joined with the smaller corps commanded by Rowland Hill, began to retreat to Portugal. Marshal Soult declined to attack.
In 1812, Wellesley was granted the titles of Marquis of Torres Vedras and Duke of Vitória, both in Portuguese nobility, by decree of Queen Maria I of Portugal, for his actions in the name of the Portuguese nation.
1813.
In 1813, Wellington led a new offensive, this time against the French line of communications. He struck through the hills north of Burgos, the Tras os Montes, and switched his supply line from Portugal to Santander on Spain's north coast; this led to the French abandoning Madrid and Burgos. Continuing to outflank the French lines, Wellington caught up with and smashed the army of King Joseph Bonaparte in the Battle of Vitoria, for which he was promoted to field marshal on 21 June. He personally led a column against the French centre, while other columns commanded by Sir Thomas Graham, Rowland Hill and the Earl of Dalhousie looped around the French right and left (this battle became the subject of Beethoven's opus 91, "Wellington's Victory"). The British troops broke ranks to loot the abandoned French wagons instead of pursuing the beaten foe. This gross abandonment of discipline caused an enraged Wellington to write in a famous dispatch to Earl Bathurst, "We have in the service the scum of the earth as common soldiers".
Although later, when his temper had cooled, he extended his comment to praise the men under his command saying that though many of the men were, "the scum of the earth; it is really wonderful that we should have made them to the fine fellows they are".
After taking the small fortresses of Pamplona, Wellington invested San Sebastián but was frustrated by the obstinate French garrison, losing 693 dead and 316 captured in a failed assault and suspending the siege at the end of July. Soult's relief attempt was blocked by the Spanish Army of Galicia at San Marcial, allowing the Allies to consolidate their position and tighten the ring around the city, which fell in September after a second spirited defence. Wellington then forced Soult's demoralised and battered army into a fighting retreat into France, punctuated by battles at the Pyrenees, Bidassoa and Nivelle. Wellington invaded southern France, winning at the Nive and Orthez. Wellington's final battle against his rival Soult occurred at Toulouse, where the Allied divisions were badly mauled storming the French redoubts, losing some 4,600 men. Despite this momentary victory, news arrived of Napoleon's defeat and abdication and Soult, seeing no reason to continue the fighting, agreed on a ceasefire with Wellington, allowing Soult to evacuate the city.
Aftermath.
Hailed as the conquering hero by the British, Wellington was created "Duke of Wellington", a title still held by his descendants (as he did not return to England until the Peninsular War was over, he was awarded all his patents of nobility in a unique ceremony lasting a full day). Although Wellesley spent nearly six years driving the French Army from Spain and removing Joseph Bonaparte from the Spanish throne, he has received little recognition in Spain: history, as taught in Spanish schools, minimises his contribution and those of the British and Portuguese soldiers that fought with him. He received some recognition during his lifetime (the title of "Duque de Ciudad Rodrigo") and the Spanish King Ferdinand VII allowed him to keep part of the works of art from the Royal Collection which he had recovered from the French. His equestrian portrait features prominently in the Monument to the Battle of Vitoria, in present-day Vitoria-Gasteiz.
His popularity in Britain was due to his image and his appearance as well as to his military triumphs. His victory fit well with the passion and intensity of the Romantic movement, with its emphasis on individuality. His personal style had an impact on the fashions on Britain at the time: his tall, lean figure and his plumed black hat and grand yet classic uniform and white trousers became very popular.
In late 1814 The Prime Minister wanted him to take command in Canada and with the assignment of winning the War of 1812 against the United States. Wellington replied that he would go to America, but he believed that he was needed more in Europe. He stated:
The Prime Minister agreed with Wellington and speeded up the negotiations that ended the war with no boundary changes through the Treaty of Ghent.
He was appointed ambassador to France, then took Lord Castlereagh's place as first plenipotentiary to the Congress of Vienna, where he strongly advocated allowing France to keep its place in the European balance of power. On 2 January 1815 the title of his Knighthood of the Bath was converted to Knight Grand Cross upon the expansion of that order.
Waterloo campaign.
On 26 February 1815, Napoleon escaped from Elba and returned to France. He regained control of the country by May and faced a renewed alliance against him. Wellington left Vienna for what became known as the Waterloo Campaign. He arrived in Belgium to take command of the British-German army and their allied Dutch-Belgians, all stationed alongside the Prussian forces of Gebhard Leberecht von Blücher.
Napoleon's strategy was to isolate the Allied and Prussian armies, and annihilate each one separately before the Austrians and Russians arrived. In doing so the vast superiority in numbers of the Coalition would be greatly diminished. He would then seek the possibility of a peace with Austria and Russia.
The French invaded Belgium, defeated the Prussians at Ligny, and fought an indecisive battle with Wellington at the Battle of Quatre Bras. These events compelled the Anglo-Allied army to retreat to a ridge on the Brussels road, just south of the small town of Waterloo. On 17 June, a torrential rain soaked in, hampering movement. The next day, on 18 June, the Battle of Waterloo was fought. This was the first time Wellington had encountered Napoleon, and he commanded an Anglo-Dutch-German army that consisted of approximately 73,000 troops, 26,000 (36 percent) of whom were British.
Battle.
The Battle of Waterloo commenced with a diversionary attack on Hougoumont by a division of French soldiers. After a barrage of 80 cannons the first French infantry attack was launched by Comte D'Erlon's I Corps. D'Erlon's troops advanced through the Allied centre, resulting in Allied troops in front of the ridge retreating in disorder through the main position. D'Erlon's corps stormed the most fortified Allied position, La Haye Sainte, but failed to take it. An Allied division under Thomas Picton met the remainder of D'Erlon's corps head to head, engaging them in an infantry duel in which Picton fell. During this struggle Lord Uxbridge launched two of his cavalry brigades at the enemy, catching the French infantry off guard, driving them to the bottom of the slope, and capturing two French Imperial Eagles. The charge, however, over-reached itself, and the British cavalry, crushed by fresh French horsemen hurled at them by Napoleon, were driven back, suffering tremendous losses.
A little before 16:00, Marshal Ney noted an apparent exodus from Wellington's centre. He mistook the movement of casualties to the rear for the beginnings of a retreat, and sought to exploit it. Ney at this time had few infantry reserves left, as most of the infantry had been committed either to the futile Hougoumont attack or to the defence of the French right. Ney therefore tried to break Wellington's centre with a cavalry charge alone.
At about 16:30, the first Prussian corps arrived. Commanded by Freiherr von Bülow, IV Corps arrived as the French cavalry attack was in full spate. Bülow sent the 15th Brigade to link up with Wellington's left flank in the Frichermont-La Haie area while the brigade's horse artillery battery and additional brigade artillery deployed to its left in support. Napoleon sent Lobau's corps to intercept the rest of Bülow's IV Corps proceeding to Plancenoit. The 15th Brigade sent Lobau's corps into retreat to the Plancenoit area. Von Hiller's 16th Brigade also pushed forward with six battalions against Plancenoit. Napoleon had dispatched all eight battalions of the Young Guard to reinforce Lobau, who was now seriously pressed by the enemy. Napoleon's Young Guard counter-attacked and, after very hard fighting, secured Plancenoit, but were themselves counter-attacked and driven out. Napoleon then resorted to sending two battalions of the Middle/Old Guard into Plancenoit and after ferocious fighting they recaptured the village.
The French cavalry attacked the British infantry squares many times, each at heavy cost to the French but with few British casualties. Ney himself was displaced from his horse four times. Eventually it became obvious, even to Ney, that cavalry alone were achieving little. Belatedly, he organised a combined-arms attack, using Bachelu's division and Tissot's regiment of Foy's division from Reille's II Corps plus those French cavalry that remained in a fit state to fight. This assault was directed along much the same route as the previous heavy cavalry attacks.
Meanwhile at approximately the same time as Ney's combined-arms assault on the centre-right of Wellington's line, Napoleon ordered Ney to capture La Haye Sainte at whatever the cost. Ney accomplished this with what was left of D'Erlon's corps soon after 18:00. Ney then moved horse artillery up towards Wellington's centre and began to destroy the infantry squares at short-range with canister. This all but destroyed the 27th (Inniskilling) Regiment, and the 30th and 73rd Regiments suffered such heavy losses that they had to combine to form a viable square. Wellington's centre was now on the verge of collapse and wide open to an attack from the French. Luckily for Wellington, Pirch I's and Zieten's corps of the Prussian Army were now at hand. Zieten's corps permitted the two fresh cavalry brigades of Vivian and Vandeleur on Wellington's extreme left to be moved and posted behind the depleted centre. Pirch I Corps then proceeded to support Bülow and together they regained possession of Plancenoit, and once more the Charleroi road was swept by Prussian round shot. The value of this reinforcement at this particular moment can hardly be overestimated.
The French army now fiercely attacked the Coalition all along the line with the culminating point being reached when Napoleon sent forward the Imperial Guard at 19:30. The attack of the Imperial Guards was mounted by five battalions of the Middle Guard, and not by the Grenadiers or Chasseurs of the Old Guard. Marching through a hail of canister and skirmisher fire and severely outnumbered, the 3,000 or so Middle Guardsmen advanced to the west of La Haye Sainte and proceeded to separate into three distinct attack forces. One, consisting of two battalions of Grenadiers, defeated the Coalition's first line and marched on. Chassé's relatively fresh Dutch division was sent against them and Allied artillery fired into the victorious Grenadiers' flank. This still could not stop the Guard's advance, so Chassé ordered his first brigade to charge the outnumbered French, who faltered and broke.
Further to the west, 1,500 British Foot Guards under Maitland were lying down to protect themselves from the French artillery. As two battalions of Chasseurs approached, the second prong of the Imperial Guard's attack, Maitland's guardsmen rose and devastated them with point-blank volleys. The Chasseurs deployed to counter-attack, but began to waver. A bayonet charge by the Foot Guards then broke them. The third prong, a fresh Chasseur battalion, now came up in support. The British guardsmen retreated with these Chasseurs in pursuit, but the latter were halted as the 52nd Light Infantry wheeled in line onto their flank and poured a devastating fire into them and then charged. Under this onslaught they too broke.
The last of the Guard retreated headlong. A ripple of panic passed through the French lines as the astounding news spread: ""La Garde recule. Sauve qui peut"!" ("The Guard retreats. Save yourself if you can!"). Wellington then stood up in Copenhagen's stirrups, and waved his hat in the air to signal an advance of the Allied line just as the Prussians were overrunning the French positions to the east. What remained of the French army then abandoned the field in disorder. Wellington and Blücher met at the inn of La Belle Alliance, on the north-south road which bisected the battlefield, and it was agreed that the Prussians should pursue the retreating French army back to France. The Treaty of Paris was signed on 20 November 1815.
Controversy.
Much historical discussion has been made about Napoleon's decision to send 33,000 troops under Marshal Grouchy to intercept the Prussians, but—having defeated Blücher at Ligny on 16 June and forced the Allies to retreat in divergent directions—Napoleon may have been strategically astute in a judgement that he would have been unable to beat the combined Allied forces on one battlefield. Wellington's comparable strategic gamble was to leave 17,000 troops and artillery, mostly Dutch and Belgian, away at Halle, north-west of Mont-Saint-Jean, in case of a French advance up the Mons-Hal-Brussels road.
Political career.
Wellington entered politics again, when he was appointed Master-General of the Ordnance in the Tory government of Lord Liverpool on 26 December 1818. He also became Governor of Plymouth on 9 October 1819. He was appointed Commander-in-Chief of the British Army on 22 January 1827 and Constable of the Tower of London on 5 February 1827.
Prime Minister.
Along with Robert Peel, Wellington became an increasingly influential member of the Tory party, and in 1828 he resigned as Commander-in-Chief and became Prime Minister of the United Kingdom.
During his first seven months as prime minister he chose not to live in the official residence at 10 Downing Street, finding it too small. He moved in only because his own home, Apsley House, required extensive renovations. During this time he was largely instrumental in the foundation of King's College London. On 20 January 1829 Wellington was appointed Lord Warden of the Cinque Ports.
As prime minister, Wellington was conservative, fearing the anarchy of the French Revolution would spread to England.
Catholic Emancipation.
The highlight of his term was Catholic Emancipation; the granting of almost full civil rights to Catholics in the United Kingdom. The change was prompted by the landslide by-election win of Daniel O'Connell, an Irish Catholic proponent of emancipation, who was elected despite not being legally allowed to sit in Parliament.
In the House of Lords, facing stiff opposition, Wellington spoke for Catholic Emancipation, giving one of the best speeches of his career. He was Irish, and later governed the country, so had some understanding of the grievances of the Catholic communities there; as Chief Secretary, he had given an undertaking that the remaining Penal Laws would only be enforced as "mildly" as possible. The Catholic Relief Act 1829 was passed with a majority of 105. Many Tories voted against the Act, and it passed only with the help of the Whigs. Wellington had threatened to resign as Prime Minister if the King (George IV) did not give his Royal Assent.
The Earl of Winchilsea accused the Duke of, "an insidious design for the infringement of our liberties and the introduction of Popery into every department of the State". Wellington responded by immediately challenging Winchilsea to a duel. On 21 March 1829, Wellington and Winchilsea met on Battersea fields. When it came time to fire, the Duke took aim and Winchilsea kept his arm down. The Duke fired wide to the right. Accounts differ as to whether he missed on purpose, an act known in dueling as a "delope". Wellington claimed he did. However, he was noted for his poor aim and reports more sympathetic to Winchilsea claimed he had aimed to kill. Winchilsea did not fire, a plan he and his second almost certainly decided upon before the duel. Honour was saved and Winchilsea wrote Wellington an apology.
The nickname "Iron Duke" originates from this period, when he experienced a high degree of personal and political unpopularity. Its repeated use in "Freeman's Journal" throughout June 1830 appears to bear reference to his resolute political will, with taints of disapproval from its Irish editors. His residence at Apsley House was targeted by a mob of demonstrators on 27 April 1831 and again on 12 October, leaving his windows smashed. Iron shutters were installed in June 1832 to prevent further damage by crowds angry over rejection of the Reform Bill, which he strongly opposed.
Wellington's government fell in 1830. In the summer and autumn of that year, a wave of riots swept the country. The Whigs had been out of power for most years since the 1770s, and saw political reform in response to the unrest as the key to their return. Wellington stuck to the Tory policy of no reform and no expansion of suffrage, and as a result lost a vote of no confidence on 15 November 1830.
The Reform Act.
The Whigs introduced the first Reform Bill while Wellington and the Tories worked to prevent its passage. The bill passed in the British House of Commons, but was defeated in the House of Lords. An election followed in direct response, and the Whigs were returned with an even larger majority. A second Reform Act was introduced, and defeated in the same way, and another wave of near insurrection swept the country. During this time, Wellington was greeted by a hostile reaction from the crowds at the opening of the Liverpool and Manchester Railway. The Whig Government fell in 1832 and Wellington was unable to form a Tory Government partly because of a run on the Bank of England. This left King William IV no choice but to restore Earl Grey to the premiership. Eventually the bill passed the House of Lords after the King threatened to fill that House with newly created Whig peers if it were not. Wellington was never reconciled to the change; when Parliament first met after the first election under the widened franchise, Wellington is reported to have said "I never saw so many shocking bad hats in my life".
Jewish Emancipation.
During debate on the Jewish Civil Disabilities Repeal Bill, Wellington, who opposed the Bill, stated in Parliament on 1 August 1833: "... this is a Christian country and a Christian legislature, and that the effect of this measure would be to remove that peculiar character." And "I see no ground whatever for passing the Bill; and shall, therefore, vote against it." The Bill was defeated, 104 votes against, and 54 for.
Conservative Government.
Wellington was gradually superseded as leader of the Tories by Robert Peel, while the party evolved into the Conservatives. When the Tories were returned to power in 1834, Wellington declined to become Prime Minister and Peel was selected instead. However, Peel was in Italy at that time and for three weeks in November and December 1834, Wellington acted as interim leader, taking the responsibilities of Prime Minister and most of the other ministries. In Peel's first cabinet (1834–1835), Wellington became Foreign Secretary, while in the second (1841–1846) he was a Minister without Portfolio and Leader of the House of Lords. Wellington was also re-appointed Commander-in-Chief of the British Army on 15 August 1842 following the resignation of Lord Hill.
Retirement.
Wellington retired from political life in 1846, although he remained Commander-in-Chief, and returned briefly to the spotlight in 1848 when he helped organise a military force to protect London during that year of European revolution.
The Conservative Party had split over the Repeal of the Corn Laws in 1846, with Wellington and most of the former Cabinet still supporting Robert Peel, but most of the MPs led by Lord Derby supporting a protectionist stance. Early in 1852 Wellington, by then very deaf, gave Derby's first government its nickname by shouting "Who? Who?" as the list of inexperienced Cabinet Ministers was read out in the House of Lords.
He became Chief Ranger and Keeper of Hyde Park and St. James's Park on 31 August 1850. He was also colonel of the 33rd Regiment of Foot from 1 February 1806 and colonel of the Grenadier Guards from 22 January 1827.
Kitty died of cancer in 1831; despite their generally unhappy relations Wellington was said to have been saddened by her death. He had found consolation for his unhappy marriage in his warm friendship with the diarist Harriet Arbuthnot, wife of his colleague Charles Arbuthnot. Harriet's death in the cholera epidemic of 1834 was almost as great a blow to Wellington as it was to her husband. The two widowers spent their last years together at Aspley House.
Death and funeral.
Wellington died on 14 September 1852, aged 83, of the after effects of a stroke culminating in a series of epileptic seizures.
Although in life he hated travelling by rail (after witnessing the death of William Huskisson, one of the first railway accident casualties), his body was then taken by train to London, where he was given a state funeral—one of only a handful of British subjects to be honoured in that way (other examples are Lord Nelson and Winston Churchill)—and the last heraldic state funeral to be held in Britain. The funeral took place on 18 November 1852. At his funeral there was hardly any space to stand because of the number of people attending, and the effusive praise given him in Tennyson's "Ode on the Death of the Duke of Wellington" attests to his stature at the time of his death. He was buried in a sarcophagus of luxulyanite in St Paul's Cathedral next to Lord Nelson. A bronze memorial was sculpted by Alfred Stevens, and features two intricate supports: "Truth tearing the tongue out of the mouth of False-hood", and "Valour trampling Cowardice underfoot". Stevens did not live to see it placed in its home under one of the great arches of the Cathedral.
Wellington's casket was decorated with banners which were made for his funeral procession. Originally, there was one from Prussia, which was removed during World War I and never reinstated.
Most of the book "A Biographical Sketch of the Military and Political Career of the Late Duke of Wellington" by Weymouth newspaper proprietor Joseph Drew is a detailed contemporary account of his death, lying in state and funeral.
After his death Irish and English newspapers disputed whether Wellington had been born an Irishman or Englishman. During his life he had openly disliked being referred to as an "Irishman".
Owing to its links with Wellington, as the former commanding officer and colonel of the regiment, the title "33rd (The Duke of Wellington's) Regiment" was granted to the 33rd Regiment of Foot, on 18 June 1853 (the 38th anniversary of the Battle of Waterloo) by Queen Victoria.
Personality.
Wellington always rose early, he "couldn't bear to lie awake in bed," even if the army was not on the march. Even when he returned to civilian life after 1815, he slept in a camp bed, reflecting his lack of regard for creature comforts—it remains on display in Walmer Castle. General Miguel de Álava complained that Wellington said so often that the army would march "at daybreak" and dine on "cold meat", that he began to dread those two phrases. While on campaign, he seldom ate anything between breakfast and dinner. During the retreat to Portugal in 1811, he subsisted, to the despair of his staff who dined with him, on "cold meat and bread". He was, however, renowned for the quality of the wine he drank and served, often drinking a bottle with his dinner—not a great quantity by the standards of his day.
He rarely showed emotion in public, and often appeared condescending to those less competent or less well-born than himself (which was nearly everyone). However, Álava was a witness to an incident just before the Battle of Salamanca. Wellington was eating a chicken leg while observing the manoeuvres of the French army through a spyglass. He spotted an overextension in the French left flank, and realised he could launch a successful attack there. He threw the drumstick in the air and shouted "Les français sont perdus!" ("The French are lost!"). Another time, after the Battle of Toulouse, when an aide brought him the news of Napoleon's abdication, he broke into an impromptu flamenco dance, spinning around on his heels and clicking his fingers.
Despite his famous stern countenance and iron-handed discipline (he was said to disapprove of soldiers cheering as "too nearly an expression of opinion"), Wellesley cared for his men; he refused to pursue the French after the battles of Porto and Salamanca, foreseeing an inevitable cost to his army in chasing a diminished enemy through rough terrain. The only time he ever showed grief in public was after the storming of Badajoz; he cried at the sight of the British dead in the breaches. In this context, his famous dispatch after the Battle of Vitoria calling them the "scum of the earth" can be seen to be fuelled as much by disappointment at their breaking ranks as by anger. He expressed his grief openly the night after Waterloo before his personal physician, and later with his family; unwilling to be congratulated for his victory he broke down in tears, his fighting spirit diminished by the high cost of the battle and great personal loss.
Viva Montgomerie, niece to the third Duke of Wellington, relates an anecdote that Holman, valet to the duke, often recalled how his master never spoke to servants unless he was obliged to, preferring instead to write his orders on a note pad on his dressing-table. Holman, incidentally, was said to greatly resemble Napoleon.
In 1822 he had had an operation to improve the hearing of the left ear. The result, however, was that he became permanently deaf on that side. It is claimed that he was "... never quite well afterwards".
In 1824 Wellington received a letter from a publisher offering to refrain from issuing an edition of the rather racy memoirs of one of his mistresses, Harriette Wilson, in exchange for financial consideration. It is said that the Duke promptly returned the missive, after scrawling across it, "Publish and be damned". However, Hibbert notes in his biography that the letter can be found among the Duke's papers, with nothing written on it. That Wellington "did" reply is certain, and the tone of a further letter from the publisher, quoted by Longford, suggests that he had refused, in the strongest language, to submit to blackmail.
He was also a remarkably practical man, who spoke concisely. In 1851, when it was discovered that there were a great many sparrows flying about in the Crystal Palace just before the Great Exhibition was to open, his advice to Queen Victoria was "Sparrowhawks, ma'am".
Wellington has often been portrayed as a defensive general, even though many, perhaps most, of his battles were offensive (Argaum, Assaye, Oporto, Salamanca, Vitoria, Toulouse). But for most of the Peninsular War, where he earned his fame, his troops lacked the numbers for an attack.
Meeting Lord Nelson.
In September 1805, the then Major-General Wellesley, newly returned from his campaigns in India and not yet particularly well-known to the public, reported to the office of the Secretary for War to request a new assignment. In the waiting room, he met Vice-Admiral Horatio Nelson, already a legendary figure after his victories at the Nile and Copenhagen, and who was briefly in England after months chasing the French Toulon fleet to the West Indies and back. Some 30 years later, Wellington recalled a conversation that Nelson began with him which Wellesley found "almost all on his side in a style so vain and silly as to surprise and almost disgust me". Nelson left the room to inquire who the young general was and on his return switched to a very different tone, discussing the war, the state of the colonies and the geopolitical situation as between equals. On this second discussion Wellington recalled, "I don't know that I ever had a conversation that interested me more". This was the only time that the two men met; Nelson was killed at his great victory at Trafalgar just seven weeks later.<br clear=all>
Wellesley and Colley heritage.
The earliest mention of the "Welles-lieghs" dates from 1180, around a settlement still known as Wellesley Farm. The family had been granted lands to the south of Wells, Somerset for their 'Passive acceptance of the Norman conquest of England of 1066. An early member of the family relocated to Ireland during 1171, in the role of a Standard Bearer to Henry II. The surname "Wesley" was adopted from a childless wealthy cousin, Garret Wesley. In 1728, Wellington's paternal grandfather Richard Colley, a landlord who lived at Rahin near Carbury, County Kildare, changed his surname to Wesley.
The Colley or Cowley family had lived in that part of Kildare since the time of Wellington's ancestor, Sir Henry Colley or Cowley, who died before 2 October 1584. Sir Henry in his lifetime possessed Carbury Castle, in north-west Kildare, starting with a 21-year lease in 1554.
Colley is a surname of English origin. However, Colley or Cowley is also an Anglicised form of Mac Amhalghaidh, a family who were lords of Cálraighe in Chalaid in what is now County Westmeath. This family were claimed descent from the 5th-century Irish king, Niall of the Nine Hostages, and had the following genealogy ("m" indicates "son of"):
The weight of evidence is that Wellington's family originated in Rutland and came to Ireland about 1500. Robert Cowley who became Master of the Rolls in Ireland died in 1546 leaving a son, Walter Cowley, Principal Solicitor for Ireland, who appears to have been father to Sir Henry. Henry Colley married Catherine Cusack, daughter of Sir Thomas Cusack, Lord Chancellor of Ireland which began the Colley-Wellesley connection, since Sir Thomas was the son of Alison de Wellesley.
Titles and tributes.
Wellington received numerous awards and honours during and after his lifetime, including statues and monuments raised in his honour. He held a wide range of titles, and had various buildings and places named after him around the world which still stand today.
Nicknames.
The Iron Duke.
This commonly used nickname originally related to his consistent political resolve rather than to any particular incident. In various cases its editorial use appears to be disparaging. It is likely that its use became more widespread after an incident in 1832 in which he installed metal shutters to prevent rioters breaking windows at Apsley House. The term may have been made increasingly popular by "Punch" cartoons published in 1844–45.
Wellington had various other nicknames:
In addition:

</doc>
<doc id="8476" url="http://en.wikipedia.org/wiki?curid=8476" title="Disk operating system">
Disk operating system

Disk Operating System (specifically) and disk operating system (generically), most often reveal themselves in abbreviated form as DOS, refer to an operating system software used in most computers that provides the abstraction and management of secondary storage devices and the information on them (e.g., file systems for organizing files of all sorts). Such software is referred to as a "disk" operating system when the storage devices it manages are made of rotating platters, such as floppy disks or hard disks.
In the early days of microcomputers, computer memory space was often limited, so the disk operating system was an extension of the operating system. This component was only loaded if needed. Otherwise, disk access would be limited to low-level operations such as reading and writing disks at the sector-level.
In some cases, the "disk operating system" component (or even the operating system) was known as "DOS".
Sometimes, a "disk operating system" can refer to the entire operating system if it is loaded off a disk and supports the abstraction and management of disk devices. Examples include DOS/360. On the PC compatible platform, an entire family of operating systems was called "DOS".
History.
In the early days of computers, there were no disk drives, floppies or modern flash storage devices. Early storage devices such as delay lines, punched cards, paper tape, magnetic tape, and magnetic drums were used instead. And in the early days of microcomputers, paper tape or audio cassette tape (see Kansas City standard) or nothing were used instead. In the latter case, program and data entry was done at front panel switches directly into memory or through a computer terminal / keyboard, sometimes controlled by a read-only memory (ROM) BASIC interpreter; when power was turned off after running the program, the information so entered vanished.
Both hard disks and floppy disk drives require software to manage rapid access to block storage of sequential and other data. When microcomputers rarely had expensive disk drives of any kind, the need to have software to manage such devices (the disks) carried much status. To have one or the other was a mark of distinction and prestige, and so was having the Disk sort of an Operating System. As prices for both disk hardware and operating system software decreased, there were many such microcomputer systems.
Mature versions of the Commodore, SWTPC, Atari and Apple home computer systems all featured a disk operating system (actually called 'DOS' in the case of the Commodore 64 ("CBM DOS"), Atari 800 ("Atari DOS"), and Apple II machines ("Apple DOS")), as did (at the other end of the hardware spectrum, and much earlier) IBM's System/360, 370 and (later) 390 series of mainframes (e.g., DOS/360: Disk Operating System / 360" and DOS/VSE: Disk Operating System / Virtual Storage Extended"). Most home computer DOS'es were stored on a floppy disk always to be booted at start-up, with the notable exception of Commodore, whose DOS resided on ROM chips in the disk drives themselves (the computer itself had no DOS, just a form of a BIOS for communicating with peripherals). The Lt. Kernal hard disk subsystem for the Commodore 64 and Commodore 128 models stored its DOS on the disk, as is the case with modern systems, and loaded the DOS into RAM at boot time.
In large machines there were other disk operating systems, such as IBM's VM, DEC's RSTS / RT-11 / VMS / TOPS-10 / TWENEX, MIT's ITS / CTSS, Control Data's assorted NOS variants, Harris's Vulcan, Bell Labs' Unix, and so on. In microcomputers, SWTPC's 6800 and 6809 machines used TSC's FLEX disk operating system, Radio Shack's TRS-80 machines used TRS-DOS, their Color Computer used OS-9, and most of the Intel 8080 based machines from IMSAI, MITS (makers of the legendary Altair 8800), Cromemco, North Star, etc., used the CP/M-80 disk operating system. See list of operating systems.
Usually, a disk operating system was loaded from a disk. Only a very few comparable DOSes were stored elsewhere than floppy disks; among these exceptions were the British BBC Micro's optional Disc Filing System, DFS, offered as a kit with a disk controller chip, a ROM chip, and a handful of logic chips, to be installed inside the computer; and Commodore's CBM DOS, located in a ROM chip in each disk drive.
Disk operating systems that were the main OS.
Some disk operating systems were the operating system for the entire computer system.

</doc>
<doc id="8477" url="http://en.wikipedia.org/wiki?curid=8477" title="Dual">
Dual

Dual may refer to:

</doc>
<doc id="8478" url="http://en.wikipedia.org/wiki?curid=8478" title="Doublespeak">
Doublespeak

Doublespeak is language that deliberately disguises, distorts, or reverses the meaning of words. Doublespeak may take the form of euphemisms (e.g., "downsizing" for layoffs, "servicing the target" for bombing), in which case it is primarily meant to make the truth sound more palatable. It may also refer to intentional ambiguity in language or to actual inversions of meaning (for example, naming a state of war "peace"). In such cases, doublespeak disguises the nature of the truth. Doublespeak is most closely associated with political language.
Origins and concepts.
The term "doublespeak" probably has its roots in George Orwell's book, "Nineteen Eighty-Four". Although the term is not used in the book, it is a close relative of one of the book's central concepts, "doublethink". Another variant, "doubletalk," also referring to deliberately ambiguous speech, did exist at the time Orwell wrote his book, but the usage of "doublespeak" as well as of "doubletalk" in the sense emphasizing ambiguity clearly postdates the publication of "Nineteen Eighty-Four". Parallels have also been drawn between Doublespeak and Orwell's classic essay "Politics and the English Language", which discusses the distortion of language for political purposes.
Edward S. Herman, political economist and media analyst, has highlighted some examples of doublespeak and doublethink in modern society. Herman describes in his book, "Beyond Hypocrisy" the principal characteristics of doublespeak:
In his essay "Politics and the English Language", George Orwell observes that political language serves to distort and obfuscate reality. Orwell’s description of political speech is extremely similar to the contemporary definition of doublespeak;
Theoretical approaches.
Although the theories that premise doublespeak are still indefinite, there are some theories that have parallels with the theory of doublespeak and Orwell's ideology in "Nineteen Eighty-Four" and might possibly provide a better understanding of where doublespeak's theories could have come from.
Conflict theories.
Due to the inherently deceptive nature of doublespeak as well as its prominent use in politics, doublespeak has been linked to the sociological perspective known as conflict theories. Conflict theories detract from ideas of society being naturally in harmony, instead placing emphasis on political and material inequality as its structural features. Antonio Gramsci's concepts on cultural hegemony, in particular, suggest that the culture and values of the economic elite – the bourgeoisie – become indoctrinated as ‘common sense’ to the working-class, allowing for the maintenance of the status quo through misplaced belief. Being himself one of the leaders of the Communist Party of Italy, (CPI), his theories had, in turn, been strongly influenced by the German social thinker Karl Marx, and have their ideological roots grounded in Marxist theory of false consciousness and capitalist exploitation. While Gramsci's views argue that culture (beliefs, perceptions and values) allows the ruling class to maintain domination, Marx's explanation is along more economic lines, with concepts such as commodity fetishism demonstrating how the ideology of the bourgeoisie (in this case, the existence of property as a social creation rather than an 'eternal entity') dominate over that of the working classes. 
In both cases, both philosophers argue that one view - that of the bourgeoisie - dominates over others, hence the term conflict theories.
On the other hand, Terrence P. Moran of the NCTE has compared the use of doublespeak in the mass media to laboratory experiments conducted on rats, where a batch of rats were deprived of food, before one half was fed sugar and water and the other half a saccharine solution. Both groups exhibited behavior indicating that their hunger was satisfied, but rats in the second group (which were fed saccharine solution) died from malnutrition. Moran highlights the structural nature of doublespeak, and notes that social institutions such as the mass media adopt an active, top-down approach in managing opinion. Therefore, Moran parallels doublespeak to producing an illusionary effect;
Contemporary writings.
Doublespeak might also have some connections with contemporary theories as well. 
Edward S. Herman and Noam Chomsky note in their book that Orwellian Doublespeak is an important component of the manipulation of the English language in American media, through a process called ‘dichotomization’; a component of media propaganda involving ‘deeply embedded double standards in the reporting of news’. For example, the use of state funds by the poor and financially needy is commonly referred to as 'social welfare' or 'handouts', which the 'coddled' poor 'take advantage of'. These terms, however, do not apply to other beneficiaries of government spending such as tax incentives and military spending. 
Examples of the structural nature of the use of Doublespeak have been made by modern scholars. Noam Chomsky argues in "" that people in modern society consist of decision-makers and social participants who have to be made to agree. According to Chomsky, the media and public relations industry actively shape public opinion, working to present messages in line with their economic agenda for the purposes of controlling of the 'public mind'. Contrary to the popular belief that indoctrination is inconsistent with democracy, Chomsky goes so far as to argue that 'it's the essence of democracy.'
Edward Herman's book "Beyond Hypocrisy" also includes a doublespeak dictionary of commonly employed media terms and phrases into plain English.
Henceforth, conflict theories demonstrates the dominating ideology of the bourgeoisie and Moran's theory highlights that doublespeak produces an illusionary effect; both theories having parallels to Orwell's ideology in "Nineteen Eighty-Four". Similarly, Herman's theory of doublespeak having an inherent nature to be manipulative and Chomsky's theory of 'dichotomization' relates directly to the practice of doublespeak and how doublespeak is deliberately deceptive in nature.
Main contributors to Doublespeak.
William Lutz.
William D. Lutz, serves as the third chairman of the Doublespeak Committee since 1975 to the present. In 1989, both his own book "Doublespeak" and, under his editorship, the committee's third book, "Beyond Nineteen Eighty-Four", were published. Lutz was also the former editor of the now defunct "Quarterly Review of Doublespeak", which examines ways that jargon has polluted the public vocabulary with phrases, words and usages of words designed to obscure the meaning of plain English.
His book, "Beyond Nineteen Eighty-Four", consists of 220 pages and eighteen articles contributed by long-time Committee members and others whose body of work has made important contributions to understandings about language, as well as a bibliography of 103 sources on doublespeak. 
Lutz is one of the main contributors to the committee as well as promoting the term "doublespeak" to a mass audience so as to inform them of the deceptive qualities that doublespeak contains. He mentions: 
He also mentions that the NCTE Committee on Public Doublespeak and their works with regards to educating the public on doublespeak is responsible for "the rather awesome task of combating the advertisers, the politicians, and the major manipulators of public language in our society." 
Lutz states that it is important to highlight doublespeak to the public because "language isn't the invention of human beings to lie, deceive, mislead, and manipulate" and the "purpose of language is to communicate the truth and to facilitate social groups getting together". Thus, according to Lutz, doublespeak is a form of language that defeats the purpose of inventing language because doublespeak does not communicate the truth but seeks to do the opposite and the doublespeak committee is tasked with correcting this problem that doublespeak has created in the world of language.
The NCTE Committee on Public Doublespeak.
The National Council of Teachers of English (NCTE) Committee on Public Doublespeak was formed in 1971, in the midst of the Watergate scandal, at a point when there was widespread skepticism about the degree of truth which characterized relationships between the public and the worlds of politics, the military, and business. NCTE passed two resolutions. One called for the Council to find means to study dishonest and inhumane uses of language and literature by advertisers, to bring offenses to public attention, and to propose classroom techniques for preparing children to cope with commercial propaganda. The other called for the Council to find means to study the relations of language to public policy, to keep track of, publicize, and combat semantic distortion by public officials, candidates for office, political commentators, and all those who transmit through the mass media. Bringing the charges of the two resolutions to life was accomplished by forming NCTE's Committee on Public Doublespeak, a body which has acquitted itself with notable achievements since its inception. The National Council's publications on doublespeak have made significant contributions in describing the need for reform where clarity in communication has been deliberately distorted. Such structures can be applied to the field of education, where they could conceivably initiate an anti-pollution bandwagon in educational communication and educate people on how to counter doublespeak. 
William Lutz stated that "the doublespeak committee was formed to combat the use of public language by increasing people's awareness of what is good, clear, solid use of language and what is not."
"The committee does more than help students and the general public recognize what doublespeak is; it dramatizes that clarity of expression reflects clarity of thought."
Hugh Rank.
Hugh Rank formed the Doublespeak committee and was the first chairman of this committee. Under his editorship, the committee produced a book called "Language and Public Policy" (1974), with the aim of informing readers of the extensive scope of doublespeak being used to deliberately mislead and deceive the audience. He highlighted the deliberate public misuses of language and provided strategies for countering doublespeak by focusing on educating people in the English language so as to help them identify when doublespeak is being put into play. He was also the founder of the Intensify/Downplay pattern that has been widely used to identify instances of Doublespeak being used.
Daniel Dieterich.
Daniel Dieterich served as the second chairman of the Doublespeak committee after Hugh Rank in 1975. He served as editor of its second publication, "Teaching about Doublespeak (1976)",which carried forward the Committee's charge to inform teachers of ways of teaching students how to recognize and combat language designed to mislead and misinform.
Critique of NCTE.
A. M. Tibbetts is one of the main critics of the NCTE, claiming that 'the Committee's very approach to the misuse of language and what it calls "doublespeak" may in the long run limit its usefulness'. According to him, the 'Committee's use of Orwell is both confused and confusing'. The NCTE's publications resonate with George Orwell's name, and allusions to him abound in statements on doublespeak; for example, the committee quoted Orwell's remark that "language is often used as an instrument of social control" in "Language and Public Policy". Tibbetts argues that such a relation between NCTE and Orwell's work is contradicting because 'the Committee's attitude towards language is liberal, even radical' while 'Orwell's attitude was conservative, even reactionary'. He also criticizes on the Committee's 'continual attack' against linguistic 'purism'.
Modern uses of Doublespeak.
Whereas in the early days of the practice it was considered wrong to construct words to disguise meaning, this is now an accepted and established practice. There is a thriving industry in constructing words without explicit meaning but with particular connotations for new products or companies. Doublespeak is also employed in the field of politics. Hence, education is necessary to recognize and combat against doublespeak-use effectively.
Doublespeak in advertising.
Advertisers can use doublespeak to mask their commercial intent from users, as users' defenses against advertising become more well entrenched. Some are attempting to counter this technique, however, with a number of systems which offer diverse views and information which highlights the manipulative and dishonest methods that advertisers employ. 
According to Jacques Ellul, “the aim is not to even modify people’s ideas on a given subject, rather, it is to achieve conformity in the way that people act." He demonstrates this view by offering an example from drug advertising. By using doublespeak in advertisements, aspirin production rose by almost 50 percent from over 23 million pounds in 1960 to over 35 million pounds in 1970.
The Rule of Parity.
William Lutz's book on "The Rule of Parity" illustrates how doublespeak is being employed in the advertising industry.
Lutz uses the example of parity products: products in which most, if not all, brands in a class or category are of similar quality. To highlight the uniqueness of their product, advertisers may choose to market it differently from their competitors. Advertising is used to create the impression of superiority. This is shown in the first rule of parity, which involves the use of the words "better" and "best". In parity claims, "better" means "best", and "best" means "equal to". 
Lutz goes on to say that when advertisers state that their product is “good", it is equivalent in meaning to saying that their product is the best. If all the brands are similar, they must all be similarly good. When they claim that their product is the "best", they mean that the product is as good as the other superior products in its category. Using the toothpaste industry as an example, Lutz says that, because there is no dramatic difference among the products of the major toothpaste companies today, they are equal. However, if all of the different toothpastes are good and equal, there is no need to prove their claim. On the contrary, advertisers cannot market their products as “better” as it is a comparative term, and a claim of superiority.
Education to combat Doublespeak.
Educating students has been suggested by experts to be one of the ways to counter doublespeak. Educating students in the English language is important to help them identify how doublespeak is being used to mislead and conceal information. 
Charles Weingartner, one of the founding members of the NCTE committee on Public Doublespeak mentioned: “people do not know enough about the subject (the reality) to recognize that the language being used conceals, distorts, misleads”. There is a crucial need for English language teachers to educate and become experts in teaching about linguistic vulnerability. “Teachers of English should teach our students that words are not things, but verbal tokens or signs of things that should finally be carried back to the things that they stand for to be verified. Students should be taught a healthy skepticism about the potential abuse of language but duly warned about the dangers of an unhealthy cynicism.
According to William Lutz: "Only by teaching respect and love for the language can teachers of English instill in students the sense of outrage they should experience when they encounter doublespeak." "Students must first learn to use the language effectively, to understand its beauty and power." "Only by using language well will we come to appreciate the perversion inherent in doublespeak."
Intensify/Downplay pattern.
This pattern was formulated by Hugh Rank and is a simple tool designed to teach some basic patterns of persuasion used in political propaganda and commercial advertising. As it was formulated to educate the public on how to counter doublespeak via education, its aim was to reach the widest possible audience of citizens. It was prepared to be incorporated within a wide variety of existing programs and textbooks in English, speech, media, communications, journalism, social studies. The NCTE has endorsed this pattern as a useful way of teaching students to cope with propaganda from any source.
The function of the Intensify/Downplay pattern is not to dictate what should be discussed but to encourage coherent thought and systematic organization. The pattern works in two ways: intensifying and downplaying. All people intensify and this is done via repetition, association and composition. Downplaying is commonly done via omission, diversion and confusion as they communicate in words, gestures, numbers, et cetera. Individuals can better cope with organized persuasion by recognizing the common ways whereby communication is intensified or downplayed, so as to counter doublespeak.
Doublespeak in politics.
Doublespeak is often used to avoid answering questions or to avoid the public's questions without directly stating that the specific politician is ignoring or rephrasing the question.
The Doublespeak Award.
Doublespeak is often used by politicians for the advancement of their agenda. The Doublespeak Award is an "ironic tribute to public speakers who have perpetuated language that is grossly deceptive, evasive, euphemistic, confusing, or self-centered." It has been issued by the National Council of Teachers of English (NCTE) since 1974. The recipients of the Doublespeak Award are usually politicians, national administration or departments. An example of this is the United States Department of Defense, which won the award three times in 1991, 1993, and 2001 respectively. For the 1991 award, the United States Department of Defense 'swept the first six places in the Doublespeak top ten' for using euphemisms like ""servicing the target" (bombing) and "force packages" (warplanes). Among the other phrases in contention were "difficult exercise in labor relations", meaning a strike, and "meaningful downturn in aggregate output"," an attempt to avoid saying the word "recession".

</doc>
<doc id="8481" url="http://en.wikipedia.org/wiki?curid=8481" title="Dressed to Kill (1980 film)">
Dressed to Kill (1980 film)

Dressed to Kill is a 1980 erotic crime thriller film written and directed by Brian De Palma and starring Michael Caine, Angie Dickinson, Nancy Allen and Keith Gordon. It centers on the murder of a housewife and an investigation involving a young prostitute who witnessed the murder, the victim’s teenaged son and her psychiatrist. The original music score is composed by Pino Donaggio.
Brian De Palma originally wanted the Norwegian actress Liv Ullmann to play Kate Miller, but she declined because of the violence. The role then went on to Angie Dickinson. Sean Connery was offered the role of Robert Elliot and was enthusiastic about it, but declined on account of previously acquired commitments. Seven years after the film's release, Connery would finally have his chance with De Palma in his Oscar winning role in "The Untouchables" (1987).
Plot.
Kate Miller (Angie Dickinson) is a sexually frustrated housewife who is in therapy with New York City psychiatrist Dr. Robert Elliott (Michael Caine). During an appointment, Kate attempts to seduce him, but Elliott rejects her advances.
Kate goes to the Metropolitan Museum. In a ten-minute sequence entirely without dialogue, she has an unexpected flirtation with a mysterious stranger. Kate and the stranger "stalk" each other through the museum until they finally wind up outside, where Kate joins him in a taxi. They begin to have sex and continue at his apartment, unaware that Kate has left her underwear on the floor of the cab.
Hours later, Kate awakens and, thoroughly satisfied with her evening, decides to discreetly leave while the man, Warren Lockman, is asleep. Kate sits at his desk to leave Warren a note and finds a document indicating that he has contracted a sexually transmitted disease. Mortified, she leaves the apartment. But in her haste, she has left her wedding ring on the stranger's nightstand, so she returns to retrieve it.
The elevator doors open on the figure of a tall, blonde woman in dark sunglasses wielding a straight razor, and Kate is slashed to death in the elevator (a murder scene DePalma has called the best he has ever done). A high-priced call girl, Liz Blake (Nancy Allen), happens upon the body. She catches a glimpse of the killer, therefore becoming both the prime suspect and the killer's next target.
Dr. Elliott receives a bizarre answering machine message from "Bobbi," a transgender patient. Bobbi taunts the psychiatrist for breaking off their therapy sessions, apparently because Elliott refuses to sign the necessary papers for Bobbi to get a sex change operation. Elliott tries to convince Dr. Levy, the patient's new doctor, that Bobbi is a danger to herself and others.
A police detective, Marino (Dennis Franz), is skeptical about Liz's story, partly due to her profession, so Liz joins forces with Kate's revenge-minded son Peter to find the killer. Peter, an inventor, uses a series of homemade listening devices and time-lapse cameras to track patients leaving Elliott's office. They catch Bobbi on camera, and soon Liz is being stalked by a tall blonde figure in sunglasses. Several attempts are subsequently made on Liz's life. One, in the New York City Subway, is thwarted by Peter, who sprays Bobbi with homemade mace.
Liz and Peter scheme to learn Bobbi's real name by getting inside Dr. Elliott's office to look at his appointment book. Liz baits the therapist by stripping to lingerie and coming on to him, distracting him long enough to make a brief exit and leaf through his appointment book. Peter is watching through the window when a Blonde lady pulls him away. When Liz returns, the tall blonde figure with the razor confronts her; Peter shouts to Liz when the blonde figure is about to attack Liz. However, the blonde lady outside shoots and wounds the blonde figure, the wig falls off, it is Doctor Elliott revealing that he is Bobbi. The blonde lady who shot Bobbi is actually a female police officer who looks like Bobbi, revealing herself to be the tall blonde figure who has been trailing Liz.
Elliott is arrested by the police and placed in an insane asylum. It is explained by Dr. Levy that Elliott wanted to be a woman, but his "male" side would not allow him to go through with the operation. Whenever a woman sexually aroused Elliott, it was "Bobbi," representing the female side of the doctor's personality, who became threatened.
In a final sequence, Elliott escapes from the asylum and slashes Liz's throat in a bloody act of vengeance. She wakes up screaming, Peter by her side, realizing that it was just a dream.
Cast.
The nude body in the opening scene, taking place in a shower, was not that of Angie Dickinson but of 1977 "Penthouse" Pet of the Year model Victoria Lynn Johnson.
Awards and honors.
American Film Institute
Reception.
"Dressed to Kill" currently holds an 84% "fresh" rating on Rotten Tomatoes. Roger Ebert awarded the film 3 stars out of 4, stating ""Dressed to Kill" is an exercise in style, not narrative; it would rather look and feel like a thriller than make sense, but DePalma has so much fun with the conventions of the thriller that we forgive him and go along." In his movie guide, Leonard Maltin gave the film 3 1/2 stars out of 4, calling it a "High-tension melodrama", and stating "De Palma works on viewers' emotions, not logic, and maintains a fever pitch from start to finish." He also praised Pino Donaggio's "chilling" music score.
Two versions of the film exist in North America, an R-rated version and an unrated version. The unrated version is around 30 seconds longer and shows more pubic hair in the shower scene, more blood in the elevator scene (including a close-up shot of the killer slitting Kate's throat), and some sexier dialogue from Liz during the scene in Elliott's office. These scenes were trimmed when the MPAA originally gave the film an "X" rating.
Allen earned a Golden Globe nomination for Best New Star, but a Razzie nomination as well. Filmmaker Quentin Tarantino, a fan of De Palma, was influenced to write "True Romance" because of this film, pointing to Nancy Allen's performance as the inspiration for the film's leading woman.

</doc>
<doc id="8483" url="http://en.wikipedia.org/wiki?curid=8483" title="Diesel cycle">
Diesel cycle

The Diesel cycle is the thermodynamic cycle which approximates the pressure and volume of the combustion chamber of the diesel engine, invented by Rudolph Diesel in 1897. It is assumed to have constant pressure during the first part of the "combustion" phase (formula_1 to formula_2 in the diagram, below). This is an idealized mathematical model: real physical diesels do have an increase in pressure during this period, but it is less pronounced than in the Otto cycle. The idealized Otto cycle of a gasoline engine approximates constant volume during that phase, generating more of a spike in a p-V diagram.
The Idealized Diesel Cycle.
The image on the left shows a p-V diagram for the ideal Diesel cycle; where formula_3 is pressure and formula_4 is specific volume. The ideal Diesel cycle follows the following four distinct processes (The color references refer to the color of the line on the diagram.):
The Diesel engine is a heat engine: it converts heat into work. The isentropic processes are impermeable to heat: heat flows into the loop through the left expanding isobaric process and some of it flows back out through the right depressurizing process, and the heat that remains does the work.
Maximum thermal efficiency.
The maximum thermal efficiency of a Diesel cycle is dependent on the compression ratio and the cut-off ratio. It has the following formula under cold air standard analysis: 
formula_9
where
The cut-off ratio can be expressed in terms of temperature as shown below:
formula_19 can be approximated to the flame temperature of the fuel used. The flame temperature can be approximated to the adiabatic flame temperature of the fuel with corresponding air-to-fuel ratio and compression pressure, formula_20.
formula_21 can be approximated to the inlet air temperature.
This formula only gives the ideal thermal efficiency. The actual thermal efficiency will be significantly lower due to heat and friction losses. The formula is more complex than the Otto cycle (petrol/gasoline engine) relation that has the following formula;
formula_22
The additional complexity for the Diesel formula comes around since the heat addition is at constant pressure and the heat rejection is at constant volume. The Otto cycle by comparison has both the heat addition and rejection at constant volume.
Comparing the two formulae it can be seen that for a given compression ratio (), the ideal Otto cycle will be more efficient. However, a diesel engine will be more efficient overall since it will have the ability to operate at higher compression ratios. If a petrol engine were to have the same compression ratio, then knocking (self-ignition) would occur and this would severely reduce the efficiency, whereas in a diesel engine, the self ignition is the desired behavior. Additionally, both of these cycles are only idealizations, and the actual behavior does not divide as clearly or sharply. And the ideal Otto cycle formula stated above does not include throttling losses, which do not apply to diesel engines.
The Diesel cycle is a combustion process of a reciprocating internal combustion engine. In it, fuel is ignited by heat generated by compressing air in the combustion chamber, into which fuel is injected. This is in contrast to igniting it with a spark plug as in the Otto cycle (four-stroke/petrol) engine. Diesel engines (heat engines using the Diesel cycle) are used in automobiles, power generation, diesel-electric locomotives, and submarines.
Applications.
Diesel engines.
Diesel engines have the lowest specific fuel consumption of any large internal combustion engine employing a single cycle, 0.26 lb/hp·h (0.16 kg/kWh) for very large marine engines (combined cycle power plants are more efficient, but employ two engines rather than one). Two-stroke diesels with high pressure forced induction, particularly turbocharging, make up a large percentage of the very largest diesel engines.
In North America, diesel engines are primarily used in large trucks, where the low-stress, high-efficiency cycle leads to much longer engine life and lower operational costs. These advantages also make the diesel engine ideal for use in the heavy-haul railroad environment.
Other internal combustion engines without spark plugs.
Many model airplanes use very simple "glow" and "diesel" engines. Glow engines use glow plugs. "Diesel" model airplane engines have variable compression ratios. Both types depend on special fuels (easily obtainable in such limited quantities) for their ignition timing.
Some 19th-century or earlier experimental engines used external flames, exposed by valves, for ignition, but this becomes less attractive with increasing compression. (It was the research of Nicolas Léonard Sadi Carnot that established the thermodynamic value of compression.) A historical implication of this is that the diesel engine would eventually have been invented without the aid of electricity.
 See the development of the hot bulb engine and indirect injection for historical significance.

</doc>
<doc id="8484" url="http://en.wikipedia.org/wiki?curid=8484" title="Deus Ex">
Deus Ex

Deus Ex ( ) is a cyberpunk-themed action-role playing video game—combining first-person shooter, stealth and role-playing elements—developed by Ion Storm and published by Eidos Interactive in 2000. First published for personal computers running Windows, "Deus Ex" was later ported to Macintosh systems, as well as the PlayStation 2 game console. Set in a dystopian world during the year 2052, the central plot follows rookie United Nations Anti-Terrorist Coalition agent JC Denton, as he sets out to combat terrorist forces, which have become increasingly prevalent in a world slipping ever further into chaos. As the plot unfolds, Denton becomes entangled in a deep and ancient conspiracy, encountering organizations such as Majestic 12, the Illuminati, and the Hong Kong Triads through his journey.
The game received universal critical and industry acclaim, including repeatedly being named "Best PC Game of All Time" in PC Gamer's "Top 100 PC Games" (last in 2011) and in a poll carried out by UK gaming magazine "PC Zone". It was a frequent candidate for and winner of Game of the Year awards, drawing praise for its pioneering designs in player choice and multiple narrative paths. It has sold more than 1 million copies, as of April 23, 2009. The game has spawned both a sequel, ', released in 2003, and a prequel, ', released in 2011. Additionally, "" was released for tablets and mobile devices (released for iOS on July 11, 2013 and for the Android on January 22, 2014), and has made its way to the PC via Steam (released for Microsoft Windows on March 18, 2014).
Gameplay.
"Deus Ex" incorporates elements from four video game genres: role-playing, first-person shooter, adventure, and "immersive simulation", the last of which being a game where "nothing reminds you that you're just playing a game". For example, the game uses a first-person camera during gameplay and includes exploration and character interaction as primary features.
The player assumes the role of JC Denton, a nanotech-augmented operative of the United Nations Anti-Terrorist Coalition (UNATCO). This nanotechnology is a central gameplay mechanism, and allows players to perform superhuman feats.
Role-playing elements.
As the player accomplishes objectives, the player character is rewarded with "skill points". Skill points are used to enhance a character's abilities in eleven different areas, and were designed to provide players with a way to customize their characters; a player might create a combat-focused character by increasing proficiency with pistols or rifles, while a more furtive character can be created by focusing on lock picking and computer hacking abilities. There are four different levels of proficiency in each skill, with the skill point cost increasing for each successive level.
Weapons may be customized through "weapon modifications", which can be found or purchased throughout the game. The player might add scopes, silencers, or laser sights; increase the weapon's range, accuracy, or magazine size; or decrease its recoil and reload time. Not all modifications are available to all weapons; for example, a rocket launcher cannot be silenced, and recoil cannot be reduced on a flamethrower.
Players are further encouraged to customize their characters through nano-augmentations—cybernetic devices that grant characters superhuman powers. While the game contains eighteen different nano-augmentations, the player can install a maximum of nine, as each must be used on a certain part of the body: one in the arms, legs, eyes, and head; two underneath the skin; and three in the torso. This forces the player to choose carefully between the benefits offered by each augmentation. For example, the arm augmentation requires the player to decide between boosting their character's skill in hand-to-hand combat or his ability to lift heavy objects.
Interaction with non-player characters (NPCs) was a large design focus. When the player interacts with a non-player character, the game will enter a cutscene-like conversation mode where the player advances the conversation by selecting from a list of dialogue options. The player's choices often have a substantial effect on both gameplay and plot, as non-player characters will react in different ways depending on the selected answer (e.g. rudeness makes them less likely to provide assistance).
Combat elements.
"Deus Ex" features combat similar to first-person shooters, with real-time action, a first-person perspective, and reflex-based gameplay. As the player will often encounter enemies in groups, combat often tends toward a tactical approach, including the use of cover, strafing, and "hit-and-run". A USA Today reviewer found "At the easiest difficulty setting, your character is puréed again and again by an onslaught of human and robotic terrorists until you learn the value of stealth." However, through the game's role-playing systems (see above), it is possible to develop a character's skills and augmentations to create a tank-like combat specialist with the ability to deal and absorb large amounts of damage. Non-player characters will praise or criticize the main character depending on his use of force, incorporating a moral element into the gameplay.
"Deus Ex" features a head-up display crosshair, whose size dynamically shows where shots will fall based on movement, aim, and the weapon in use; the reticle expands while the player is moving or shifting his or her aim, and slowly shrinks to its original size while no actions are taken. How quickly the reticle shrinks depends on the character's proficiency with the equipped weapon, the number of accuracy modifications added to the weapon, and the level of the "targeting" nano-augmentation.
"Deus Ex" features twenty-four weapons, ranging from crowbars, electroshock weapons, and riot baton, to laser guided anti-tank rockets and assault rifles; both lethal and non-lethal weapons are available. The player can also make use of several weapons of opportunity, such as fire extinguishers.
Player choice.
Gameplay in "Deus Ex" emphasizes player choice. Objectives can be completed in numerous ways, including stealth, sniping, heavy frontal assault, dialogue, or engineering and computer hacking. This level of freedom requires that levels, characters, and puzzles be designed with significant redundancy, as a single play-through of the game will miss large sections of dialogue, areas, and other content. In some missions the player is encouraged to avoid using deadly force, and certain aspects of the story may change depending on how violent or non-violent the player chooses to be. The game is also unusual in that two of its boss villains can be killed off early in the game, or left alive to be defeated later, and this too affects how other characters interact with the player.
Because of its design focus on player choice, "Deus Ex" has been compared with "System Shock", a game that inspired its design. Together, these factors give the game a great degree of replayability, as the player will have vastly different experiences, depending on which methods he or she uses to accomplish objectives.
Multiplayer.
"Deus Ex" was designed as a single player game, and the initial releases of the Windows and Macintosh versions of the game did not include multiplayer functionality. Support for multiplayer modes was later incorporated through patches. The component includes three game modes: deathmatch, basic team deathmatch, and advanced team deathmatch. Only five maps, based on levels from the single-player portion of the game, were included with the original multiplayer patch, but many user-created maps now exist. The PlayStation 2 release of "Deus Ex" does not offer a multiplayer mode. In April 2014 it was announced that Gamespy would cease their masterserver services, also affecting Deus Ex. A community-made patch for the multiplayer mode has been created as a response to this.
Synopsis.
Setting and characters.
"Deus Ex" takes place in the year 2052 in a world that draws heavily upon popular real world conspiracy theories for many of its plot elements. These include speculations regarding black helicopters, vaccinations, and FEMA, as well as Area 51, the ECHELON network, Men in Black, cow mutilations, chupacabras (in the form of "greasels"), and grey aliens. Mysterious groups such as Majestic 12, the Illuminati, the Knights Templar, the Bilderberg Group, and the Trilateral Commission also either play a central part in the plot, or are alluded to during the course of the game. This dark setting is enhanced by the fact that the entire game takes place at night, a backdrop that adds to the atmosphere of conspiracies and stealth.
The game contradicts itself in several instances regarding the exact year in which the events of the story take place, but information in the sequel "" reconciles this inconsistency via retroactive continuity, placing the events of "Deus Ex" in the year 2052. Most of the game takes place in fictionalized versions of real-world locations, including New York City, Hong Kong, Paris, Vandenberg Air Force Base, and Area 51.
The plot of "Deus Ex" depicts a society on a slow spiral into chaos. There is a massive division between the rich and the poor, not only socially, but in some cities physically. A lethal pandemic known as the "Gray Death" ravages the world's population, especially within the United States, and has no cure. A synthetic vaccine, "Ambrosia", manufactured by the company VersaLife, nullifies the effects of the virus, but is in critically short supply. Because of its scarcity, Ambrosia is available only to those deemed "vital to the social order", and finds its way primarily to government officials, military personnel, the rich and influential, scientists, and the intellectual elite. With no hope for the common people of the world, riots occur worldwide, and a number of terrorist organizations have formed with the professed intent of assisting the downtrodden, among them the National Secessionist Forces of the U.S. and a French group known as Silhouette.
In order to combat these threats to the world order, the United Nations has greatly expanded its governmental influence around the globe. The United Nations Anti-Terrorist Coalition (UNATCO) is formed, with the intent of maintaining peace internationally and combating the world's ever-growing number of terrorist groups. It is headquartered near New York City in a bunker beneath Liberty Island, placed there after a terrorist strike on the Statue of Liberty. Alex Jacobson's character model and name are based on Warren Spector's own nephew, Alec Jacobson.
Plot.
The player assumes the identity of JC Denton, a nanotechnologically-augmented ("nano-aug") UNATCO agent. After completing his training, JC takes several missions given by Director Joseph Manderley to track down members of the National Secessionist Forces (NSF) and their stolen shipments of the "Ambrosia" vaccine, the treatment for the "Gray Death" virus. Through these missions, JC is reunited with his brother, Paul, who is also nano-augmented. JC tracks the Ambrosia shipment to a private terminal at LaGuardia Airport. Paul meets JC outside the plane, and explains that he has defected from UNATCO and is now working with the NSF after learning that the Gray Death is a man-made virus, with UNATCO using its power to make sure only the elite receive the vaccine.
JC returns to UNATCO headquarters and is told by Manderley that both he and Paul have been outfitted with a 24-hour kill switch, and that Paul's has been activated due to his betrayal. Manderley orders JC to fly to Hong Kong to eliminate Tracer Tong, a hacker whom Paul has contact with, and who can disable the kill switches. Instead, JC returns to Paul's apartment to find Paul hiding inside. Paul further explains his defection and encourages JC to also defect by sending out a distress call to alert the NSF's allies. Upon doing so, JC becomes a wanted man by UNATCO, and his own kill switch is activated by Federal Emergency Management Agency (FEMA) Director Walton Simons. JC is unable to escape UNATCO forces, and both he and Paul (provided he survived the raid on the apartment) are taken to a secret prison below UNATCO headquarters. An entity named "Daedalus" contacts JC and informs him that the prison is part of Majestic 12, and arranges for him and Paul to escape. The two flee to Hong Kong to meet with Tong, who deactivates their kill switches. Tong requests JC infiltrate the VersaLife building. Doing so, JC discovers that the corporation is the source for the Gray Death, and he is able to steal the plans for the virus and destroy the "universal constructor" (UC) that produces it.
Analysis of the virus shows it was manufactured by the Illuminati, prompting Tong to send JC to Paris to try to make contact with the organization and obtain their help fighting Majestic 12. JC eventually meets with Illuminati leader Morgan Everett, and learns that the Gray Death virus was intended to be used for augmentation technology, but Majestic 12, led by trillionaire businessman and former Illuminatus Bob Page, was able to steal and repurpose it into its current viral form. Everett recognizes that without VersaLife's universal constructor, Majestic 12 can no longer create the virus, and will likely target Vandenberg Air Force Base, where X-51, a group of former Area 51 scientists, has built another one. After aiding the base personnel in repelling a Majestic 12 attack, JC meets X-51 leader Gary Savage, who reveals that Daedalus is an artificial intelligence (AI) borne out of the ECHELON program. Everett attempts to gain control over Majestic 12's communications network by releasing Daedalus onto the U.S. military networks, but Page counters by releasing his own AI, Icarus, which merges with Daedalus to form a new AI, Helios, with the ability to control all global communications. After this, Savage enlists JC's help in procuring schematics for reconstructing components for the UC that were damaged during Majestic 12's raid of Vandenberg. JC finds the schematics and electronically transmits them to Savage. Page intercepts the transmission and launches a nuclear missile at Vandenberg to ensure that Area 51 (now Majestic 12's headquarters), will be the only location in the world with an operational UC. However, JC is able to reprogram the missile to strike Area 51. JC then travels there himself to confront Page.
When JC locates him, Page reveals that he seeks to merge with Helios and gain full control over all nanotechnology, essentially becoming a god. JC is contacted by Tong, Everett, and the Helios AI simultaneously. All three factions ask for his help in defeating Page, while furthering their own objectives, and JC is forced to choose between them. Tong seeks to plunge the world into a second Dark Age by destroying the global communications hub and preventing anyone from taking control of the world. Everett offers Denton the chance to bring the Illuminati back to power by killing Bob Page and using the technology of Area 51 to rule the world with an invisible hand. Helios wishes to merge with Denton and rule the world as a benevolent dictator with infinite knowledge and reason. The player's decision determines the course of the future, and brings the game to a close.
Development.
After Looking Glass Technologies and Origin Systems released "" in January 1993, producer Warren Spector began to plan "Troubleshooter", the game that would become "Deus Ex". Spector found himself burnt out on fantasy and science fiction settings, and hoped to make a game set in the real world. In his 1994 proposal, he described the concept as ""Underworld"-style first-person action" in a real world setting with "big-budget, nonstop action". Spector later commented that Origin did not have the interest, nor Looking Glass the funding, to produce the game. He eventually left Origin for Looking Glass and continued to develop the game's concept, but his project "Junction Point", which was inspired by ideas from "Troubleshooter", was cancelled. After Spector and his team were laid off from Looking Glass, John Romero of Ion Storm offered him the chance to make his "dream game" without any restrictions. Spector quickly joined the company.
Preproduction for "Deus Ex" began around August 1997 and lasted roughly six months. The six-person team came from Looking Glass's Austin studio. Spector, the team's director and producer, saw their work as improving upon the game design ideas of Origin, Looking Glass, and Valve by doing what those companies did not. The game's "ironic" working title was "Shooter: Majestic Revelations", and it was scheduled for release on Christmas 1998. The team developed the setting before the game mechanics. Noticing his wife's fascination with "The X-Files", Spector connected the "real world, millennial weirdness, [and] conspiracy" topics on his mind and decided to make a game about them that would appeal to a wide audience. "Shooter"s fiction was based in part on conspiracy theories related to Area 51, CIA drug trafficking, the John F. Kennedy assassination, the Majestic 12, and a Masonic bunker beneath Denver International Airport. The team designed over 200 characters without associated in-game roles, which was both helpful when designing missions and unhelpful as they attempted to reduce their scope. Later in 1997, Spector wrote a "manifesto" on his ideal game and the structure of role-playing video games. His principles included "problems, not puzzles", "no forced failure", "places do; NPCs watch", and "areas with multiple entrance and exit points". In retrospect, Spector believed that "Deus Ex" accomplished the intent of his manifesto.
The "Shooter" design document cast the player as an augmented agent working against an elite cabal in the "dangerous and chaotic" 2050s. It cited "Half-Life", "Fallout", ', and "GoldenEye 007" as game design influences, and used the stories and settings of ', "The Manchurian Candidate", "Robocop", "The X-Files" and "Men in Black" as reference points. According to the document, the game would engage with "the millennial madness that's gripping the world ... and a general fascination with conspiracy theories and the desire to play with high-tech espionage toys". The team designed a skill system that featured "special powers" derived from nanotechnological augmentation, and avoided the inclusion of die rolling and skills that required micromanagement. Augmentations were unique to the player character. By March 1998, preproduction had generated 300 pages of documentation. The document grew to 500 pages, with "radically different" content, by the game's April 1999 Alpha 1 deadline. Of Spector's original design document, the marketing section was the only part left unedited.
In early 1998, the "Deus Ex" team grew to 20 people and the game entered a 28-month production phase. Spector hired new staff for his Austin studio, and was assigned an art team from Ion Storm's Dallas branch. The development team consisted of three programmers, six designers, seven artists, a writer, an associate producer, a "tech", and Warren Spector, the producer and director. Two writers and four testers were hired as contractors. Chris Norden was the lead programmer and assistant director, Harvey Smith the lead designer, Jay Lee the lead artist, and Sheldon Pacotti the lead writer. However, Spector's initial management structure, which involved two competing design teams and the matrix management of the Dallas art team, was a failure. According to Spector, the team was interested in multiple video game genres, and it contained both design maximalists who wanted to "do everything" and design minimalists who wanted to do a few things well. Close friends of the team who understood the intentions behind the game were invited to playtest and give feedback. The wide range of input led to debates in the office and changes to the game. Spector later concluded that the team was "blinded by promises of complete creative freedom", and by their belief that the game would have no budget, marketing or time restraints. By mid-1998, the game's title had become "Deus Ex", derived from the Latin literary device "deus ex machina" ("god from the machine") in which a plot is resolved by an unpredictable intervention. Spector acknowledged its grammatical faults as a title, but he liked it because of its relevance to the in-game struggle for power, to the medium's storytelling difficulties, to the game being played on a computer, and to the "self-referential" acceptance of trying one's best to resolve affairs.
Spector felt that the best aspects of "Deus Ex"s development were the "high-level vision" and length of preproduction, flexibility within the project, testable "proto-missions", and Unreal Engine license. The team's pitfalls included the management structure, unrealistic goals, underestimating risks with artificial intelligence, their handling of proto-missions, and weakened morale from bad press. He referred to that period of Ion Storm as "Sturm und Drang", because of the degree of hype and the vitriol following "Daikatana" trash talk marketing, alongside negative press in 1998 and 1999. He said that his Austin team had "frequent" slumps in morale from taking the company's coverage personally and seeing their private emails posted online. Eventually, the "Deus Ex" Austin team developed a we'll show them' mentality" to distinguish their work and reputation from those of the Dallas branch. "Deus Ex" was published by Eidos Interactive and released on June 23, 2000 for Microsoft Windows. The team planned third-party ports for Mac OS 9 and Linux.
Design.
The original 1997 design document for "Deus Ex" privileges character development over all other features, including experimental sequences and technology demos. The game was designed to be "genre-busting": in parts simulation, role-playing game, first-person shooter, and adventure. The team wanted players to consider "who they wanted to be" in the game, and for that to alter how they behaved in the game. In this way, the game world was "deeply simulated", or realistic and believable enough that the player would solve problems in creative, emergent ways without noticing distinct puzzles. The developers also wanted to include "choice" and "consequence", which Spector called the team's "two most frequently uttered words". However, the team's simulation ultimately failed to maintain the desired level of openness, and they had to brute force "skill", "action", and "character interaction" paths through each level. Playtesting also revealed that their idea of a role-playing game based in the real world was more interesting in theory than in reality. The team chose two real-world bases for levels: "highly interconnected, multi-level" spaces, and places that most cannot visit (e.g., the White House). In practice, the team found that certain aspects of the real world, such as hotels and office buildings, were not compelling in a game. Ion Storm saw "Deus Ex" as being about "player expression" rather than making the developers appear "clever". They treated the player as a "collaborator", who they sought to empower to "make choices and ... deal with the consequences".
 The game's story changed greatly during production, but the idea of an augmented counterterrorist protagonist named JC Denton remained throughout. Though Spector originally pictured "Deus Ex" as akin to "The X-Files", lead writer Sheldon Pacotti felt that it ended up more like James Bond. Spector wrote that the team overextended itself by planning highly elaborate scenes, including a replica of downtown Austin, a reconstruction of Area 51 from satellite data, a sunken post-earthquake Los Angeles, a raid to free thousands of prisoners of war from a Federal Emergency Management Agency-controlled United Nations concentration camp, and over 25 missions throughout Siberia, western Europe, and the United States. Designer Harvey Smith pushed for the removal of a subplot in which Mexico invaded Texas, in order to make development easier and the narrative more personal. He also removed a largely complete White House level due to its complexity and production needs. Finished digital assets were repurposed or, in the cases of Texas and the Denver airport, abandoned by the team. Pete Davison of USgamer referred to the White House and presidential bunker as "the truly deleted scenes of "Deus Ex" lost levels".
Once coded, the team's game systems did not work as intended. Prototypes of the systems and of certain missions were built near the beginning of development, which revealed some of the team's planning mistakes. For example, the early tests of the conversation system and user interface were flawed, but the team had time to revise them before the game's release. The team also found augmentations and skills to be less interesting than they had seemed in the design document. Colleagues from other companies—such as Doug Church, Rob Fermier, Marc LeBlanc, and Gabe Newell—noticed and pointed out these deficiencies in game "tension" when they played the prototype. In response, Harvey Smith substantially revised the augmentations and skills. Production milestones served as wake-up calls for the game's direction. A May 1998 milestone that called for a functional demo revealed that the size of the game's maps caused frame rate issues, which was one of the first signs that maps needed to be cut. A year later, the team reached a milestone for finished game systems that Spector nicknamed the "Wow, these missions suck" milestone, which led to better estimates for their future mission work and to the reduction of the 500-page design document to 270 pages. Spector recalled Smith's mantra on this point: "less is more". 
One of the team's biggest blind spots was the AI programming for NPCs. Spector wrote that they considered it in preproduction, but that they did not figure out how to handle it until "relatively late in development". This led to wasted time when the team had to discard their early AI code. The team built atop their game engine's shooter-based AI instead of writing new code that would allow characters to exhibit convincing emotions. As a result, NPC behavior was variable until the very end of development. Spector felt that the team's "sin" was their inconsistent display of a trustable "human AI".
Technology.
 The game was developed on systems including dual-processor Pentium Pro 200s and Athlon 800s with eight and nine gigabyte hard drives, some using SCSI. The team used "more than 100 video cards" throughout development. "Deus Ex" was built using Visual Studio, Lightwave, and Lotus Notes. They also built a custom dialogue editor, ConEdit. The team used UnrealEd atop the Unreal game engine for map design, which Spector wrote was "superior to anything else available". Their trust in UnrealScript led them to code "special-cases" for their immediate mission needs instead of more generalized multi-case code. Even as concerned team members expressed misgivings, the team only addressed this later in the project. To Spector, this was a lesson to always prefer "general solutions" over "special casing", such that the tool set works predictably.
They waited to license a game engine until after preproduction, expecting the benefits of licensing to be more time for the content and gameplay, which Spector reported to be the case. They chose the Unreal engine as it did 80% of what they needed from an engine and was more economical than building from scratch. Their small programming team allowed for a larger design group. The programmers also found the engine accommodating, though it took about nine months to acclimate to the software. Spector felt that they would have understood the code better had they built it themselves, instead of "treating the engine as a black box" and coding conservatively. He acknowledged that this precipitated into the Direct3D issues in their final release, which slipped through their quality assurance testing. Spector also noted that the artificial intelligence, pathfinding, and sound propagation were designed for shooters and should have been rewritten from scratch instead of relying on the engine. He thought the licensed engine worked well enough that he expected to use the same for and "Thief 3". He added that developers should not attempt to force their technology to perform in ways it was not intended, and should find a balance between perfection and pragmatism.
Music.
The soundtrack of "Deus Ex", composed by Alexander Brandon (primary contributor, including main theme), Dan Gardopée ("Naval Base" and "Vandenberg"), Michiel van den Bos ("UNATCO", "Lebedev's Airfield", "Airfield Action", "DuClare Chateau" plus minor contribution to some of Brandon's tracks), and Reeves Gabrels ("NYC Bar"), was praised by critics for complementing the gritty atmosphere predominant throughout the game with melodious and ambient music incorporated from a number of genres, including techno, jazz, and classical. The music sports a basic dynamic element, similar to the iMUSE system used in early 1990s LucasArts games; during play, the music will change to a different iteration of the currently playing song based on the player's actions, such as when the player starts a conversation, engages in combat, or transitions to the next level. All the music in the game is tracked - Gabrels' contribution, "NYC Bar", was converted to a module by Brandon.
A compact disc of the "Deus Ex" soundtrack was included in the "Game of the Year" edition and is not available for separate purchase. Notably, the soundtrack is not a direct audio rip from the game itself, however; it is a "remastering" of the soundtrack with added instruments and audio production. Originally only thirty tracks were included with the re-release, with tracks thirty-one through forty-one considered as extras. The PlayStation 2 port featured live, orchestral renditions of some tracks. Also OCRemix made an official remix called 
Release.
"Deus Ex" has been re-released in several iterations since its original publication, and has also been the basis of a number of mods developed by its fan community.
The "Deus Ex: Game of the Year Edition", which was released on May 8, 2001, contains the latest game updates and a software development kit, a separate soundtrack CD, and a page from a fictional newspaper featured prominently in "Deus Ex" titled "The Midnight Sun", which recounts recent events in the game's world. However, later releases of said version do not include the soundtrack CD, and contain a PDF version of the newspaper on the game's disc.
The Macintosh version of the game, released shortly after the PC version, was shipped with the same capabilities and can also be patched to enable multiplayer support. However, publisher Aspyr Media did not release any subsequent editions of the game or any additional patches. As such, the game is only supported in Mac OS 9 and the "Classic" environment in Mac OS X, neither of which are compatible with Intel-based Macs. The PC version will run on Intel-based Macs using Crossover, Boot Camp, or other software to enable a compatible version of Microsoft Windows to run on a Mac.
A PlayStation 2 port of the game, renamed "Deus Ex: The Conspiracy" (although kept as "Deus Ex" in Europe) was released on March 26, 2002. Along with pre-rendered introductory and ending cinematics that replaced the original versions, it features a simplified interface with optional auto aim and motion captured character models. There are many minor changes in level design, some for the purpose of balancing gameplay, but most to accommodate loading transition areas, due to the memory limitations of the PlayStation 2.
Loki Games worked on a Linux version of the game, but the company went out of business before releasing it. The OpenGL layer they wrote for the port however was sent out to Windows gamers through an online patch, which also makes the game far more compatible with Wine on Linux than it would have been with only Direct3D.
Though their quality assurance did not see major Direct3D issues, players noted "dramatic slowdowns" immediately following launch, and the team did not understand the "black box" of the Unreal engine well enough to make it do exactly what they needed. Spector characterized "Deus Ex" reviews into two categories based on how they begin with either how "Warren Spector makes games all by himself" or that ""Deus Ex" couldn't possibly have been made by Ion Storm". He has said that the game won over 30 "best of" awards in 2001, and concluded that their final game was not perfect, but that they were much closer for having tried to "do things right or not at all".
Mods.
"Deus Ex" is built on Unreal Engine, previous games of which saw active community involvement in modding. On September 20, 2000, Eidos Interactive and Ion Storm announced in a press release that they would be releasing the software development kit (SDK). According to the announcement, the SDK includes all the tools used to create the original game. Several team members as well as project director Warren Spector said that they were "really looking forward to seeing what [the community] does with our tools". The kit was released on September 22, 2000, and soon gathered community interest, followed by release of tutorials, small mods, up to announcements of large mods and conversions. While ION Storm did not hugely alter the engine's rendering and core functionality, they introduced role-playing elements.
In 2009, a fan-made mod called "The Nameless Mod" ("TNM") was released by Off Topic Productions. The game's protagonist is a user of an Internet forum, with digital places represented as physical locations. The mod offers roughly the same amount of gameplay as Deus Ex and adds several new features to the game, with a more open world structure than "Deus Ex" and new weapons such as the player character's fists. The mod was developed over 7 years and has thousands of lines of recorded dialogue and two different parallel story arcs. Upon its release, "TNM" earned a 9/10 overall from Australia's "PCPowerPlay" magazine. In ModDB's 2009 Mod of the Year awards, "The Nameless Mod" won the Editor's Choice award for Best Singleplayer Mod.
Reception.
Critical response.
"Deus Ex" received positive reviews, attaining a score of 90 out of 100 from 28 critics on Metacritic. Thierry Nguyen from Computer Gaming World said that the game "delivers moments of brilliance, idiocy, ingenuity, and frustration." Computer Games Magazine praised the title for its deep gameplay and its use of multiple solutions to situations in the game.
Former GameSpot reviewer Greg Kasavin, though awarding the game a score of 8.2 of 10, was disappointed by the security and lockpicking mechanics. "Such instances are essentially noninteractive", he wrote. "You simply stand there and spend a particular quantity of electronic picks or modules until the door opens or the security goes down." Kasavin made similar complaints about the hacking interface, noting that, "Even with basic hacking skills, you'll still be able to bypass the encryption and password protection ... by pressing the 'hack' button and waiting a few seconds."
The game's graphics and sounds were also met with muted enthusiasm. Kasavin complained of "Deus Ex"s relatively sub-par graphics, blaming them on the game's "incessantly dark industrial environments." GamePro reviewer Chris Patterson took time to note that despite being "solid acoustically," "Deus Ex" had moments of weakness. He poked fun at JC's "Joe Friday, 'just the facts, deadpan," and the "truly cheesy accents" of minor characters in Hong Kong and New York City. IGN called the graphics "blocky", adding that "the animation is stiff, and the dithering is just plain awful in some spots," referring to the limited capabilities of the Unreal Engine used to design the game. the website later on stated that "overall Deus Ex certainly looks better than your average game."
Reviewers and players also complained about the size of "Deus Ex"s save files. An Adrenaline Vault reviewer noted that, "Playing through the entire adventure, [he] accumulated over 250MB of save game data, with the average file coming in at over 15MB."
Awards and accolades.
"Deus Ex" received over 30 "best of" awards in 2001, from outlets such as IGN, GameSpy, "PC Gamer", "Computer Gaming World", and The Adrenaline Vault. It won "Excellence in Game Design" and "Game Innovation Spotlight" at the 2001 Game Developers Choice Awards, and it was nominated for "Game of the Year". At the Interactive Achievement Awards, it won in the "Computer Innovation" and "Computer Action/Adventure" categories and received nominations for "Sound Design", "PC Role-Playing", and "Game of the Year" in both the PC and overall categories. The British Academy of Film and Television Arts named it "PC Game of the Year". The game also collected several "Best Story" accolades, including first prize in Gamasutra's 2006 "Quantum Leap" awards for storytelling in a video game.
Since its release, "Deus Ex" has appeared in a number of "Greatest Games of All Time" lists and Hall of Fame features. It was included in IGN's "100 Greatest Games of All Time" (#40, #21 and #34 in 2003, 2005 and 2007, respectively), "Top 25 Modern PC Games" (4th place in 2010) and "Top 25 PC Games of All Time" (#20 and #21 in 2007 and 2009 respectively) lists. GameSpy featured the game in its "Top 50 Games of All Time" (18th place in 2001) and "25 Most Memorable Games of the Past 5 Years" (15th place in 2004) lists, and in the site's "Hall of Fame". "PC Gamer" placed "Deus Ex" on its "Top 100 PC Games of All Time" (#2, #2, #1 by staff and #4 by readers in 2007, 2008, 2010 and 2010 respectively) and "50 Best Games of All Time" (#10 and #27 in 2001 and 2005) lists, and it was awarded 1st place in "PC Zone"s "101 Best PC Games Ever" feature. It was also included in Yahoo! UK Video Games' "100 Greatest Computer Games of All Time" (28th place) list, and in "Edge"s "The 100 Best Videogames" (29th place in 2007) and "100 Best Games to Play Today" (57th place in 2009) lists. "Deus Ex" was named the second-best game of the 2000s by Gamasutra. In 2012, "Time" named it one of the 100 greatest video games of all time, and G4tv ranked it as the 53rd best game of all time for its "complex and well-crafted story that was really the start of players making choices that genuinely affect the outcome." 1UP.com listed it as one of the most important games of all time, calling its influence "too massive to properly gauge."
Legacy.
A film adaptation based on the game was originally announced in May 2002 by Columbia Pictures. The movie was being produced by Laura Ziskin, along with Greg Pruss attached with writing the screenplay. Peter Schlessel, president of production for Columbia Pictures, and Paul Baldwin, president of marketing for Eidos Interactive, stated that they were confident in that the adaptation would be a successful development for both the studios and the franchise. In March 2003, during an interview with Greg Pruss, he informed IGN that the character of JC Denton will be "a little bit filthier than he was in the game." He further stated that the script was shaping up to be darker in tone than the original game. Although a release date was scheduled for 2006, the film never got past the scripting stage.<br>In 2012, CBS films revived the project, buying the rights and commissioning a film inspired by the "Deux Ex" series; its direct inspiration will be the 2011 game "". C. Robert Cargill and Scott Derrickson are writing the screenplay, and Derrickson will direct the film.
A sequel to the game, entitled "", was released in the United States on December 2, 2003, and then in Europe in early 2004 for both the PC and the Xbox game console. A second sequel, entitled "Deus Ex: Clan Wars", was originally conceived as a multiplayer-focused third game for the series. After the commercial performance and public reception of "Deus Ex: Invisible War" failed to meet expectations, the decision was made to set the game in its own universe, and "Deus Ex: Clan Wars" was eventually published under the title "Project: Snowblind".
On March 29, 2007, Valve announced "Deus Ex" and its sequel would be available for purchase from their Steam service. Among the games announced are several other Eidos franchise titles, including "" and "Tomb Raider".
Eidos Montreal produced a prequel to "Deus Ex" called "". This was confirmed on November 26, 2007 when Eidos Montreal posted a teaser trailer for the title on their website. The game was released on August 23, 2011 for the PC, PlayStation 3, and Xbox 360 platforms.

</doc>
<doc id="8485" url="http://en.wikipedia.org/wiki?curid=8485" title="Diego Maradona">
Diego Maradona

Diego Armando Maradona Franco (, born 30 October 1960) is a former Argentine footballer. He has served as a manager and coach at other clubs as well as for the national team of Argentina. Many experts, football critics, former players, current players and football fans regard Maradona as the best football player of all time. He was joint FIFA Player of the 20th Century with Pelé.
A playmaker who operated in the classic number 10 position, Maradona is the only player in football history to set the world record transfer fee twice, first when he transferred to Barcelona for a then world record £5m, and second, when he transferred to Napoli for another record fee £6.9m. He played for Argentinos Juniors, Boca Juniors, Barcelona, Napoli, Sevilla and Newell's Old Boys during his club career, and is most famous for his time at Napoli where he won numerous accolades. In his international career, playing for Argentina, he earned 91 caps and scored 34 goals.
Maradona played in four FIFA World Cups, including the 1986 World Cup where he captained Argentina and led them to victory over West Germany in the final, and won the Golden Ball award as the tournament's best player. In the 1986 World Cup quarter final, he scored both goals in a 2–1 victory over England that entered football history, though for two different reasons. The first goal was an unpenalized handling foul known as the "Hand of God", while the second goal followed a dribble past five England players, voted "The Goal of the Century" by FIFA.com voters in 2002.
Maradona is considered one of the sport's most controversial and newsworthy figures. He was suspended from football for 15 months in 1991 after failing a drug test, for cocaine, in Italy, and he was sent home from the 1994 World Cup in the U.S. after testing positive for ephedrine. In 2005, he lost a considerable amount of extra weight and overcame his cocaine addiction. His outspoken views have sometimes put him in conflict with journalists and sport executives. Although he had little managerial experience, he became head coach of the Argentina national team in November 2008, and held the job for eighteen months, until his contract expired after the 2010 World Cup.
He coached Dubai based club Al Wasl in the UAE Pro-League for the 2011–12 season. In August 2013, Maradona joined Argentine Primera D club Deportivo Riestra's staff as "spiritual coach".
Early years.
Diego Maradona was born on 30 October 1960, at the Policlínico (Polyclinic) Evita Hospital in Lanús, Buenos Aires Province, but raised in Villa Fiorito, a shantytown on the southern outskirts of Buenos Aires, Argentina, to a poor family that had moved from Corrientes Province. He was the first son after three daughters. He has two younger brothers, Hugo ("el Turco") and Raúl (Lalo), both of whom were also professional football players. Maradona is of Italian, Spanish, Croatian, Indigenous-Argentinian ancestry. His surname originates from the Spanish region Galicia.
He was the fifth child and first son of Diego Maradona 'Chitoro' and Dalma Salvadora Franco 'Doña Tota' (1930–2011). Both his parents were illegitimate children. His father took the family name of his mother because his father did not recognise him as his own. Maradona's mother was not recognised by her father, Atanancio Ramón Edisto Franco, until she was eighteen years old. Maradona's parents were both born and brought up in the town of Esquina in the north-east province of Corrientes Province, living only two hundred yards from each other on the banks of the Corriente River. In 1950, they left Esquina and settled in Buenos Aires.
At age eight, Maradona was spotted by a talent scout while he was playing in his neighborhood club "Estrella Roja". He became a staple of "Los Cebollitas" (The Little Onions), the junior team of Buenos Aires's Argentinos Juniors. As a 12-year-old ball boy, he amused spectators by showing his wizardry with the ball during the halftime intermissions of first division games. He named Manchester United winger George Best as one of the players who inspired him when he was growing up.
Club career.
Argentinos Juniors and Boca Juniors.
On 20 October 1976, Maradona made his professional debut with Argentinos Juniors, ten days before his sixteenth birthday. He played there between 1976 and 1981, scoring 115 goals in 167 appearances before his £1m transfer to Boca Juniors. Boca was the team Maradona always wanted to play for. Having joined the Boca squad midway through the 1981 season, Maradona played through 1982 earning his first league championship medal.
Barcelona.
After the 1982 World Cup, in June, Maradona was transferred to FC Barcelona in Spain for a then world record fee of £5m ($7.6m). In 1983, under coach César Luis Menotti, Barcelona and Maradona won the Copa del Rey (Spain's annual national cup competition), beating Real Madrid, and the Spanish Super Cup, beating Athletic Bilbao. 
On June 26, 1983, Barcelona defeated Real Madrid on the road in the world's biggest club game, "El Clásico", a match where Maradona scored and became the first Barcelona player to be applauded by arch rival Real Madrid fans. Maradona dribbled past Madrid goalkeeper Agustín, and as he approached the empty goal he stopped just as Madrid defender Juan José came sliding in a desperate attempt to block the shot and ended up crashing into the post, before Maradona slotted the ball into the net. Many inside the stadium were stunned at the manner of the goal and started applauding: only Ronaldinho, in November 2005, has since been granted such an ovation as a Barcelona player from Madrid fans at the Santiago Bernabéu. 
Due to illness and injury, Maradona had a difficult tenure in Barcelona. First a bout of hepatitis, then a broken ankle caused by an ill-timed tackle by Athletic's Andoni Goikoetxea threatened to jeopardize Maradona's career, but after treatment and therapy it was possible for him to soon be back on the pitch.
During his two injury-hit seasons at Barcelona, Maradona scored 38 goals in 58 games. Maradona got into frequent disputes with the Barcelona directors, especially club president Josep Lluís Núñez, culminating with a demand to be transferred out of Camp Nou in 1984. He was transferred to Napoli in Italy's Serie A for another world record fee, £6.9m ($10.48m).
Napoli.
Maradona arrived in Naples and was presented to the world media as a Napoli player on 5 July 1984, where he was welcomed by 75,000 fans at his presentation at the Stadio San Paolo. Sports writer David Goldblatt commented; "They [the fans] were convinced that the saviour had arrived." A local newspaper stated that despite the lack of a "mayor, houses, schools, buses, employment and sanitation, none of this matters because we have Maradona." Prior to Maradona's arrival, Italian football was dominated from teams north of Naples, such as A.C. Milan, Juventus, Inter Milan and A.S. Roma, and no team from southern Italy had ever won the league title.
At Napoli, Maradona reached the peak of his professional career. He quickly became an adored star among the club's fans, and in his time there he elevated the team to the most successful era in its history. Maradona played for Napoli at a period when North-South tensions in Italy were at a peak due to a variety of issues, notably the economic differences between the two. Led by Maradona, Napoli won their first ever Serie A Italian Championship in 1986/87. Goldblatt wrote; "The celebrations were tumultuous. A rolling series of impromptu street parties and festivities broke out contagiously across the city in a round-the-clock carnival which ran for over a week. The world was turned upside down. The Neapolitans held mock funerals for Juventus and Milan, burning their coffins, their death notices announcing 'May 1987, the other Italy has been defeated. A new empire is born.' Murals of Maradona were painted on the cities ancient buildings, and new born children were named in his honor. 
Napoli would win their second league title in 1989/1990, and finish runners up in the league twice, in 1987/88 and 1988/89. Other honors during the Maradona era at Napoli included the Coppa Italia in 1987, (second place in the Coppa Italia in 1989), the UEFA Cup in 1989 and the Italian Supercup in 1990. Maradona was the top scorer in Serie A in 1987/88, and is the all time leading goalscorer for Napoli with 115 goals.
While he was successful on the field, during his time in Italy Maradona's personal problems increased. His cocaine use continued, and he received US $70,000 in fines from his club for missing games and practices, ostensibly because of 'stress'. He faced a scandal there regarding an illegitimate son; and he was also the object of some suspicion over an alleged friendship with the Camorra. Later on, in honor of Maradona and his achievements during his career at Napoli, the No. 10 jersey of Napoli was officially retired.
Sevilla, Newell's Old Boys and Boca Juniors.
After serving a 15-month ban for failing a drug test for cocaine, Maradona left Napoli in disgrace in 1992. Despite interest from Real Madrid of Spain and Olympique Marseille of France, he signed for Sevilla of Spain, where he stayed for one year. In 1993 he played for Newell's Old Boys and in 1995 he returned to Boca Juniors for two years. Maradona also appeared for Tottenham Hotspur in a friendly match against Internazionale, shortly before the 1986 World Cup. The match was a testimonial for Osvaldo Ardiles, who insisted that his friend Maradona play.
International career.
During his time with the Argentine national team, Maradona scored 34 goals in 91 appearances. He made his full international debut at age 16, against Hungary on 27 February 1977. At age 18, he played the World Youth Championship for Argentina, and was the star of the tournament, shining in their 3–1 final win over the Soviet Union. On 2 June 1979, Maradona scored his first senior international goal in a 3–1 win against Scotland at Hampden Park. He and his compatriot and heir apparent, Lionel Messi, are the only players to win the Golden Ball at both the FIFA U-20 World Cup and FIFA World Cup. Maradona did so in 1979 and 1986, while Messi managed in 2005 and 2014.
1982 World Cup.
Maradona played his first World Cup tournament in 1982. Argentina played Belgium in the opening game of the 1982 Cup in Barcelona. The Catalan crowd was eager to see their new world-record signing Diego Maradona in action, but he did not perform to expectations. Argentina, the defending champions, lost to Belgium 1–0. Although the team convincingly beat Hungary and El Salvador to progress to the second round, they were defeated in the second round by Brazil and by eventual winners Italy. The Italian match is renowned for Maradona being aggressively man-marked by Claudio Gentile, as Italy beat Argentina in Maradona's new home city of Barcelona. Maradona played in all five matches without being substituted, scoring twice against Hungary. After being fouled repeatedly in all games and particularly in the last one against Brazil, Maradona's temper eventually got the better of him and he was sent off with 5 minutes remaining for a serious retaliatory foul against Joao Batista da Silva.
1986 World Cup.
Maradona captained the Argentine national team to victory in the 1986 FIFA World Cup, winning the final in Mexico against West Germany. Throughout the 1986 World Cup Maradona asserted his dominance and was the most dynamic player of the tournament. He played every minute of every Argentina game, scored 5 goals and made 5 assists, three of those in the opening match against South Korea. His first goal of the tournament came against Italy in the second group game. After scoring two contrasting goals in the 2–1 quarter-final win against England his legend was cemented. The majesty of his second goal and the notoriety of his first led to the French newspaper "L'Equipe" describing Maradona as "half-angel, half-devil".
This match was played with the background of the Falklands War between Argentina and the United Kingdom. Replays showed that the first goal was scored by striking the ball with his hand. Maradona was coyly evasive, describing it as "a little with the head of Maradona and a little with the hand of God." It became known as the "Hand of God". Ultimately, on 22 August 2005 Maradona acknowledged on his television show that he had hit the ball with his hand purposely, and no contact with his head was made, and that he immediately knew the goal was illegitimate. This became known as an international fiasco in World Cup history. The goal stood, much to the wrath of the English players.
Maradona's second goal, just four minutes after the hotly disputed hand-goal, was later voted by FIFA as the greatest goal in the history of the World Cup. He received the ball in his own half, swivelled around, and with 11 touches ran more than half the length of the field, dribbling past five English outfield players (Peter Beardsley, Steve Hodge, Peter Reid, Terry Butcher, and Terry Fenwick) before he left goalkeeper Peter Shilton on his backside with a feint, and slotted the ball into the net. This goal was voted "Goal of the Century" in a 2002 online poll conducted by FIFA.
Maradona followed this with two more goals in the semi-final against Belgium, including another virtuoso dribbling display for the second goal. In the final, the opposing West German side attempted to contain him by double-marking, but he nevertheless found the space to give the final pass to Jorge Burruchaga for the winning goal. Argentina beat West Germany 3–2 in front of 115,000 spectators at the Azteca Stadium in Mexico City.
During the course of the tournament, Maradona attempted or created more than half of Argentina's shots, embarked on 90 dribbles some three times more than any other player and was fouled 53 times winning his team twice as many free kicks as any player. Maradona scored or assisted 10 of Argentina's 14 goals including the assist for the winning goal in the final, ensuring that he would be remembered as one of the greatest names in football history.
By the end of the World Cup, Maradona went on to win the Golden Ball as the best player of the tournament by unanimous vote and was widely regarded to have won the World Cup virtually single-handedly. In a tribute to him, Azteca Stadium authorities built a statue of him scoring the "Goal of the Century" and placed it at the entrance of the stadium.
1990 World Cup.
Maradona captained Argentina again in the 1990 FIFA World Cup to yet another World Cup Final. An ankle injury affected his overall performance, and he was much less dominant than four years earlier. Argentina were almost eliminated in the first round, only qualifying in third position from their group. In the round of 16 match against Brazil, Claudio Caniggia scored the only goal after being set up by Maradona.
In the quarter final, Argentina faced Yugoslavia, the match ending 0–0 after 120 minutes, and Argentina advancing on penalty kicks, despite Maradona missing one of the penalties in the shootout with a weak shot to the goalkeeper's right. The semifinal against the host nation Italy was also resolved on penalties after a 1–1 draw; this time, Maradona was successful with his effort, daringly rolling the ball into the net with an almost exact replica of his missed shot in the previous round. In the final, Argentina lost 1–0 to West Germany, the only goal being a penalty by Andreas Brehme in the 85th minute after a controversial foul on Rudi Völler.
1994 World Cup.
At the 1994 FIFA World Cup Maradona played in only two games, scoring one goal against Greece, before being sent home after failing a drug test for ephedrine doping. In his autobiography, Maradona argued that the test result was due to his personal trainer giving him the power drink Rip Fuel. His claim was that the U.S. version, unlike the Argentine one, contained the chemical and that, having run out of his Argentine dosage, his trainer unwittingly bought the U.S. formula. FIFA expelled him from USA '94 and Argentina were subsequently eliminated in the second round. Maradona has also separately claimed that he had an agreement with FIFA, on which the organization reneged, to allow him to use the drug for weight loss before the competition in order to be able to play. His failed drugs test at the 1994 World Cup signaled the end of his international career, which had lasted 17 years and yielded 34 goals from 91 games.
Playing style.
A classic number 10, Maradona was renowned for his dribbling ability, vision, close ball control, passing and creativity, and is considered one of the most skillful players ever. He had a compact physique, and with his strong legs and low center of gravity he could withstand physical pressure well while running with the ball. Dutch legend Johan Cruyff saw similarities between Maradona and Lionel Messi with the ball seemingly attached to their body when dribbling. His physical strengths were illustrated by his two goals against Belgium in the 1986 World Cup. He was a strategist and a team player, as well as highly technical with the ball. He could manage himself effectively in limited spaces, and would attract defenders only to quickly dash out of the melee (as in the second 1986 goal against England), or give an assist to a free teammate. Being short, but strong, he could hold the ball long enough with a defender on his back to wait for a teammate making a run or to find a gap for a quick shot. He showed leadership qualities on the field and captained Argentina in their World Cup campaigns of 1986, 1990 and 1994.
One of Maradona's trademark moves was dribbling full-speed on the right wing, and on reaching the opponent's goal line, delivering accurate passes to his teammates. Another trademark was the "Rabona," a reverse-cross pass shot behind the leg that holds all the weight. This maneuver led to several assists, such as the powerful cross for Ramón Díaz's header in the 1980 friendly against Switzerland. He was also a dangerous free kick taker.
Maradona was famous for his cunning and clever personality. Some critics view his controversial "Hand of God" goal as a very clever move, with one of the opposition players, Glenn Hoddle, admitting that Maradona had disguised it cunningly in flicking his head at the same time as palming the ball. The goal itself has been viewed as being an embodiment of the Buenos Aires shantytown Maradona was brought up in and its concept of "viveza criolla" — native cunning.
Maradona was dominantly left-footed, often using his left foot even when the ball was positioned more suitably for a right-footed connection. His first goal against Belgium in the 1986 World Cup semi-final is a worthy indicator of such; he had run into the inside right channel to receive a pass but let the ball travel across to his left foot, requiring more technical ability. During his run past several England players in the previous round for the "Goal of the Century" he did not use his right foot once, despite spending the whole movement on the right-hand side of the pitch. In the 1990 World Cup second round tie against Brazil, he did use his right foot to set up the winning goal for Caniggia due to two Brazilian markers forcing him into a position that made use of his left foot less practical.
Retirement and honours.
Hounded for years by the press, Maradona once fired a compressed-air rifle at reporters who he claimed were invading his privacy. This quote from former teammate Jorge Valdano summarizes the feelings of many:
In 1999 Konex Foundation from Argentina granted him the Diamond Konex Award, one of the most prestigious culture awards in Argentina, as the most important personality in Sports in the last decade in his country.
In 2000, Maradona published his autobiography "Yo Soy El Diego" ("I am "The Diego""), which became an instant bestseller in his home country. Two years later, Maradona donated the Cuban royalties of his book to "the Cuban people and Fidel."
In 2000, he won FIFA Player of the Century award which was to be decided by votes on their official website, their official magazine and a grand jury. Maradona won the Internet based poll, garnering 53.6% of the votes against 18.53% for Pelé. In spite of this, and shortly before the ceremony, FIFA added a second award and appointed a "Football Family" committee composed of football journalists that also gave to Pelé the title of best player of the century to make it a draw. Maradona also came fifth in the vote of the IFFHS (International Federation of Football History and Statistics).
In 2001, the Argentine Football Association (AFA) asked FIFA for authorization to retire the jersey number 10 for Maradona. FIFA did not grant the request, even though Argentine officials have maintained that FIFA hinted that it would.
Maradona has topped a number of fan polls, including a 2002 FIFA poll in which his second goal against England was chosen as the best goal ever scored in a World Cup; he also won the most votes in a poll to determine the All-Time Ultimate World Cup Team. On 22 March 2010, Maradona was chosen number 1 in The Greatest 10 World Cup players of all time by the London based newspaper "The Times". Argentinos Juniors named its stadium after Maradona on 26 December 2003.
In 2003, Maradona was employed by the Libyan footballer Al-Saadi Gaddafi, the third son of Colonel Muammar Gaddafi, as a "technical consultant", while Al-Saadi was playing for the Italian club, Perugia Calcio, which was in Serie A at the time.
On 22 June 2005, it was announced that Maradona would return to Boca Juniors as a sports vice president in charge of managing the First Division roster (after a disappointing 2004–05 season, which coincided with Boca's centenary). His contract began 1 August 2005, and one of his first recommendations proved to be very effective: he was the one who decided to hire Alfio Basile as the new coach. With Maradona fostering a close relationship with the players, Boca went on to win the 2005 Apertura title, the 2006 Clausura title, the 2005 Copa Sudamericana and trhe 2005 Recopa Sudamericana.
On 15 August 2005, Maradona made his debut as host of a talk-variety show on Argentine television, La Noche del 10 ("The Night of the no. 10"). His main guest on opening night was Pelé; the two had a friendly chat, showing no signs of past differences. However, the show also included a cartoon villain with a clear physical resemblance to Pelé. In subsequent evenings, he led the ratings on all occasions but one. Most guests were drawn from the worlds of football and show business, including Ronaldo and Zinedine Zidane, but also included interviews with other notable friends and personalities such as Cuban leader Fidel Castro and boxers Roberto Durán and Mike Tyson. Maradona gave each of his guests a signed Argentina jersey, which Tyson wore when he arrived in Brazil, Argentina's deadliest rivals.
In May 2006, Maradona agreed to take part in UK's Soccer Aid (a program to raise money for Unicef). In September 2006, Maradona, in his famous blue and white number 10, was the captain for Argentina in a three-day World Cup of Indoor Football tournament in Spain. On 26 August 2006, it was announced that Maradona was quitting his position in the club Boca Juniors because of disagreements with the AFA, who selected Basile to be the new coach of the Argentina national football team. In 2008, award-winning Serbian filmmaker Emir Kusturica made a documentary about Maradona's life, entitled "Maradona".
On 1 September 2014, Maradona, along with many current and former footballing stars, took part in the "Match for Peace", which was played at the Stadio Olimpico, in Rome, with the proceeds being donated entirely to charity. Maradona set up a goal for Roberto Baggio during the first half of the match, with a chipped through-ball over the defence with the outside of his left foot. Unusually, both Baggio and Maradona wore the number 10 shirt, despite playing on the same team.
Managerial career.
Club management.
He attempted to work as a coach alongside former Argentinos Juniors midfield team mate Carlos Fren. The pair led Mandiyú of Corrientes (1994) and Racing Club (1995), but with little success. In May 2011 he became manager of Dubai club Al Wasl FC in the United Arab Emirates. Maradona was sacked in 10 July 2012.
International management.
After the resignation of Argentina national football team coach Alfio Basile in 2008, Diego Maradona immediately proposed his candidacy for the vacant role. According to several press sources, his major challengers included Diego Simeone, Carlos Bianchi, Miguel Ángel Russo and Sergio Batista.
On 29 October 2008, AFA chairman Julio Grondona confirmed that Maradona would be the head coach of the national side from December 2008. On 19 November 2008, Diego Maradona managed Argentina for the first time when Argentina played against Scotland at Hampden Park in Glasgow which Argentina won 1–0.
After winning his first three matches in charge of the national team, he oversaw a 6–1 defeat to Bolivia, equalling the team's worst ever margin of defeat. With two matches remaining in the qualification tournament for the 2010 World Cup, Argentina was in fifth place and faced the possibility of failing to qualify, but victory in the last two matches secured qualification for the finals.
After Argentina's qualification, Maradona used abusive language at the live post-game press conference, telling members of the media to "suck it and keep on sucking it". FIFA responded with a two-month ban on all footballing activity, which expired on 15 January 2010, and a CHF 25,000 fine, with a warning as to his future conduct. The friendly match scheduled to take place at home to the Czech Republic on 15 December, during the period of the ban, was cancelled. The only match Argentina played during Maradona's ban was a friendly away to Catalonia, which Argentina lost 4–2.
At the World Cup finals in June 2010, Argentina started by winning 1–0 against Nigeria, and then defeated South Korea by 4–1, with a hat-trick from Gonzalo Higuain. In the final match of the group stage Argentina won 2–0 against Greece to win their the group and advance to a second round meeting with Mexico. After defeating Mexico 3–1, Argentina was in turn routed by Germany, 4–0 in the quarter finals to go out of the competition. Argentina was ranked 5th in the tournament. After the defeat to Germany Maradona admitted that he was considering his future as Argentina coach, "I may leave tomorrow," he said. On 15 July 2010, the Argentine Football Association said that he would be offered a new 4-year deal that would keep him in charge through to the summer of 2014 when Brazil stages the World Cup, however on 27 July the AFA announced that its board had unanimously decided not to renew his contract. Afterwards on 29 July 2010, Maradona claimed that AFA president Julio Grondona and director of national teams Carlos Bilardo had "lied to" and "betrayed" and effectively sacked him from the role. Saying "they wanted me to continue, but seven of my staff should not go on, if he told me that, it meant he did not want me to keep working".
Personal life.
Family.
His parents are Diego Maradona Senior and Dalma Salvadora Franco. His father is of Italian and Native Amerindian origin and his mother is of Croatian origin. Maradona married long-time fiancée Claudia Villafañe on 7 November 1984 in Buenos Aires, and they had two daughters, Dalma Nerea (born 2 April 1987) and Giannina Dinorah (born 16 May 1989), by whom he became a grandfather in 2009. In his autobiography, Maradona admits he was not always faithful to Claudia, even though he refers to her as the love of his life.
Maradona and Villafañe divorced in 2004. Daughter Dalma has since asserted that the divorce was the best solution for all, as her parents remained on friendly terms. They travelled together to Napoli for a series of homages in June 2005 and were seen together on many other occasions, including the Argentina matches during 2006 FIFA World Cup.
During the divorce proceedings, Maradona admitted he was the father of Diego Sinagra . The Italian courts had already ruled so in 1993, after Maradona refused to undergo DNA tests for proving or disproving his paternity. met Maradona for the first time in May 2003 after tricking his way onto a golf course in Italy where Maradona was playing. Diego Sinagra is now a footballer playing in Italy. After the divorce, Claudia embarked on a career as a theatre producer, and Dalma was seeking an acting career; she had expressed her desire to attend the Actor's Studio in Los Angeles.
His younger daughter, Giannina, married Manchester City striker Sergio Agüero, with whom she has a son, Benjamin, born in Madrid on 19 February 2009. In January 2013, Agüero and Giannina separated.
His mother, Dalma, died on 19 November 2011. Diego was in Dubai at the time, and desperately tried to fly back in time to see her, but was too late. She was 81 years old. His son Diego Fernando, whom he had with his ex long term partner Veronica Ojeda, was born 13 February 2013.
Drug abuse and health issues.
From the mid-1980s until 2004 Diego Maradona was addicted to cocaine. He allegedly began using the drug in Barcelona in 1983. By the time he was playing for Napoli he had a regular addiction, which began to interfere with his ability to play football.
Over the years following his retirement his health seriously deteriorated. On 4 January 2000, while vacationing in Punta del Este, Uruguay, Maradona had to be rushed to the emergency room of a local clinic. In a press conference, doctors stated that it was detected heart muscle damage due to "an underlying health issue". It was later known that traces of cocaine were found in his blood and Maradona had to explain the circumstances to the police. After this he left Argentina and went to Cuba in order to follow a drug rehab plan.
On 18 April 2004, doctors reported that Maradona had suffered a major myocardial infarction following a cocaine overdose; he was admitted to intensive care in a Buenos Aires hospital. Scores of fans gathered around the clinic. He was taken off the respirator on 23 April and remained in intensive care for several days before being discharged on 29 April. He tried to return to Cuba, where he had spent most of his time in the years leading up to the heart attack, but his family opposed, having filed a judicial petition to exercise his legal guardianship.
Maradona had a tendency to put on weight, and suffered increasingly from obesity from the end of his playing career until undergoing gastric bypass surgery in a clinic in Cartagena de Indias, Colombia on 6 March 2005. His surgeon said that Maradona would follow a liquid diet for three months in order to return his normal weight. When Maradona resumed public appearances shortly thereafter, he displayed a notably thinner figure.
On 29 March 2007, Maradona was readmitted to a hospital in Buenos Aires. He was treated for hepatitis and effects of alcohol abuse, and was released on 11 April, but re-admitted two days later. In the following days there were constant rumors about his health, including three false claims of his death within a month. After transfer to a psychiatric clinic specialising in alcohol-related problems, he was discharged on 7 May. On 8 May 2007, Maradona appeared on Argentine television and stated that he had quit drinking and had not used drugs in two and a half years.
Political views.
Only in recent years, Maradona has shown sympathy to left-wing ideologies. Before that he had been vocal in his support of neoliberal Argentina President Carlos Menem, and his Harvard University-educated economist Domingo Cavallo. He became friends with Cuban leader Fidel Castro while receiving treatment on the island, with Castro stating; "Diego is a great friend and very noble too. There’s also no question he’s a wonderful athlete and has maintained a friendship with Cuba to no material gain of his own.” He has a portrait of Castro tattooed on his left leg and one of Fidel's second in command, fellow Argentine Che Guevara on his right arm. In his autobiography, "El Diego", he dedicated the book to various people, including Castro, he wrote "To Fidel Castro and, through him, all the Cuban people".
Maradona was also a supporter of former Venezuelan President Hugo Chávez. In 2005 he visited Venezuela with the specific aim of meeting Chávez, who received him in Miraflores. After this meeting Maradona claimed that he had come with the aim of meeting a "great man" ("un grande" in Spanish) but he had met instead a gigantic man ("un gigante" in Spanish, meaning he was more than great). "I believe in Chávez, I am Chavista. Everything Fidel does, everything Chávez does, for me is the best." Maradona was the guest of honor of Chávez at the opening game of the 2007 Copa América held in Venezuela.
He has declared his opposition to what he identifies as imperialism, notably during the 2005 Summit of the Americas in Mar del Plata, Argentina. There he protested George W. Bush's presence in Argentina, wearing a T-shirt labeled "STOP BUSH" (with the "s" in "Bush" being replaced with a swastika) and referring to Bush as "human garbage". In August 2007, Maradona went further, making an appearance on Chávez's weekly television show "Alo Presidente" and saying: "I hate everything that comes from the United States. I hate it with all my strength." In December 2008, Maradona expressed admiration for Bush's successor, President-elect Barack Obama, and held great expectations for him.
With his poor shantytown upbringing, Maradona has cultivated a man of the people persona. During a meeting with Pope John Paul II at the Vatican in 1987 they clashed on the issue of wealth disparity, with Maradona stating: “I argued with him because I was in the Vatican and I saw all these golden ceilings and afterwards I heard the Pope say the Church was worried about the welfare of poor kids. Sell your ceiling then amigo, do something!”. In September 2014 Maradona met with Pope Francis in Rome, crediting Francis for inspiring him to return to religion after many years, and stated: "We should all imitate Pope Francis. If each one of us gives something to someone else, no one in the world would be starving".
In December 2007, Maradona presented a signed shirt with a message of support to the people of Iran: it is displayed in the Iranian Ministry of Foreign Affairs' museum. In April 2013, Maradona visited the tomb of Hugo Chávez and urged Venezuelans to elect the late leader's designated successor, Nicolás Maduro, to continue the socialist leader's legacy; "Continue the struggle," Maradona said on television. Maradona attended Maduro's final campaign rally in Caracas, signing footballs and kicking them to the crowd, and presented Maduro with an Argentina jersey. Having visited Chávez's tomb with Maradona, Maduro said; "Speaking with Diego was very emotional because comandante Chávez also loved him very much".
Financial problems.
In March 2009 Italian officials announced that Maradona still owed the Italian government €37 million in taxes; €23.5 million of which was accrued interest on his original debt. They reported that thus far, Maradona has paid only €42,000, two luxury watches and a set of earrings.
In popular culture.
The American newspaper "The Houston Chronicle" wrote about Maradona:
In Argentina, Maradona is considered a sports hero to many. He is idolized, receiving the name of "God". On the idolatry that exists in Argentina, former teammate Jorge Valdano said: "At the time that Maradona retired from active football, left traumatized Argentina. Maradona was more than just a great footballer. It was a special compensation factor for a country that in a few years lived several military dictatorships and social frustrations of all kinds". Valdano added that "Maradona offered to the Argentines a way out of their collective frustration, and that's why people love him. There is a divine figure."
Ever since 1986, it is common for Argentines abroad to hear Maradona's name as a token of recognition, even in remote places. The Tartan Army sing a version of the Hokey Cokey in honour of the Hand of God goal against England. In Argentina, Maradona is often talked about in terms reserved for legends. In the Argentine film "El Hijo de la Novia" ("Son of the Bride"), somebody who impersonates a Catholic priest says to a bar patron: "they idolized him and then crucified him". When a friend scolds him for taking the prank too far, the fake priest retorts: "But I was talking about Maradona". He's the subject of the film "El Camino de San Diego", though he himself only appears in archive footage.
Maradona was included in many cameos in the Argentine comic book El Cazador de Aventuras. After the closing of it, the authors started a new short-lived comic book titled "El Die", using Maradona as the main character. Maradona has had several online flash games that are entirely dedicated to his legacy. In Rosario, Argentina, locals organized the parody religion of the "Church of Maradona". The organization reformulates many elements from Christian tradition, such as Christmas or prayers, reflecting instead details from Maradona. It had 200 founding members, tens of thousands more have become members via the church's official web site.
Many Argentine artists performed songs in tribute to Diego, like: "Maradó" by El Potro Rodrigo, "Maradona" by Andrés Calamaro, "Para siempre Diego" (Diego forever) by Los Ratones Paranoicos, "Para verte gambetear" (For seeing you dribble) by La Guardia Hereje, "Francotirador" (Sniper) by Attaque 77, "Dale Diez" (C'mon Diez) by Julio Lacarra, "Maradona blues" by Charly García, "Santa Maradona" (Saint Maradona) by Mano Negra, "La Vida Tombola" by Manu Chao, "¿Qué es Dios?" (What is God?) by Las Pastillas del Abuelo, "Pelusa"(Fluff) by Los Cafres, among others. And many films, like: "Maradona, La Mano de Dios" (Maradona, the Hand of God), "El Camino de San Diego" (Saint Diego's Road), "Amando a Maradona" (Loving Maradona), "Maradona by Kusturica". Maradona features in the music video to the 2010 World Cup song "Waka Waka" by Shakira, with footage shown of him celebrating Argentina winning the 1986 World Cup.
A 2006 television commercial for Brazilian soft drink Guaraná Antarctica portrayed Maradona as a member of the Brazilian national football team, including wearing the yellow jersey and singing the Brazilian national anthem with Brazilian caps Kaká and Ronaldo. Later on in the commercial he wakes up realizing it was a nightmare after having drunk too much of the drink. This generated some controversy in the Argentine media after its release (although the commercial was not supposed to air on the Argentine market, fans could see it online). Maradona replied that he has no problem in wearing the Brazilian national squad jersey despite Argentina and Brazil having a tense rivalry in football, but that he would refuse to wear the shirt of River Plate, Boca Juniors' traditional rival. In 2010, Maradona appeared in a commercial for the French fashion house Louis Vuitton, indulging in a game of table football with fellow legends Pelé and Zinedine Zidane.
Honours.
Club.
 Argentinos Juniors
Runner-up
 Boca Juniors
Runner-up
 Barcelona
 Napoli
Runner-up
Country.
 Argentina
Runner-up
Club.
 Al-Wasl
Runner-up

</doc>
<doc id="8487" url="http://en.wikipedia.org/wiki?curid=8487" title="David Brewster">
David Brewster

Sir David Brewster (11 December 1781 – 10 February 1868) was a Scottish physicist, mathematician, astronomer, inventor, writer, historian of science and university principal.
Most noted for his contributions to the field of optics, he studied the double refraction by compression and discovered the photoelastic effect, which gave birth to the field of optical mineralogy. For his work, William Whewell dubbed him the "Father of modern experimental optics" and "the Johannes Kepler of Optics."
He is well-recognized for being the inventor of the kaleidoscope and an improved version of the stereoscope applied to photography. He called it the "lenticular stereoscope", which was the first portable, 3D viewing device. He also invented the binocular camera, two types of polarimeters, the polyzonal lens and the lighthouse illuminator.
A prominent figure in the popularization of science, he is considered one of the founders of the "British Association", of which he would be elected President in 1849. In addition, he was the editor of the 18-volume "Edinburgh Encyclopædia".
Early life.
David Brewster was born at the Canongate in Jedburgh, Roxburghshire, to Margaret Key (1753–1790) and James Brewster (c. 1735–1815), the rector of Jedburgh Grammar School and a teacher of high reputation. David was the third of six children, two daughters and four sons: James (1777–1847), minister at Craig, Ferryden; David; David; George (1784–1855), minister at Scoonie, Fife; and Patrick (1788–1859), minister at the abbey church, Paisley.
At the age of 12, David was sent to the University of Edinburgh (graduating MA in 1800), being intended for the clergy. He was licensed a minister of the Church of Scotland, but only preached from the pulpit on one occasion. He had already shown a strong inclination for natural science, and this had been fostered by his intimacy with a "self-taught philosopher, astronomer and mathematician", as Sir Walter Scott called him, of great local fame—James Veitch of Inchbonny—a man who was particularly skilful in making telescopes.
Career.
Work on optics.
Though Brewster duly finished his theological studies and was licensed to preach, his other interests distracted him from the duties of his profession. In 1799 fellow-student Henry Brougham persuaded him to study the diffraction of light. The results of his investigations were communicated from time to time in papers to the "Philosophical Transactions" of London and other scientific journals. The fact that other scientists – notably Étienne-Louis Malus and Augustin Fresnel – were pursuing the same investigations contemporaneously in France does not invalidate Brewster's claim to independent discovery, even though in one or two cases the priority must be assigned to others. A lesser-known classmate of his, Thomas Dick, also went on to become a popular astronomical writer.
The most important subjects of his inquiries can be enumerated under the following five headings:
In this line of investigation, the prime importance belongs to the discovery of
These discoveries were promptly recognised. As early as 1807 the degree of LL.D. was conferred upon Brewster by Marischal College, Aberdeen; in 1815 he was elected a Fellow of the Royal Society of London, and received the Copley Medal; in 1818 he received the Rumford Medal of the society; and in 1816 the French Institute awarded him one-half of the prize of three thousand francs for the two most important discoveries in physical science made in Europe during the two preceding years. In 1821, he was made a foreign member of the Royal Swedish Academy of Sciences.
Among the non-scientific public, his fame spread more effectually by his invention in about 1815 of the kaleidoscope, for which there was a great demand in both the United Kingdom, France, and the United States. As a reflection of this fame, Brewster portrait was later printed in some cigar boxes. Brewster chose renowned achromatic lens developer Philip Carpenter as the sole manufacturer of the kaleidoscope in 1817. Although Brewster patented the kaleidoscope in 1817 (GB 4136), a copy of the prototype was shown to London opticians and copied before the patent was granted. As a consequence, the kaleidoscope became produced in large numbers, but yielded no direct financial benefits to Brewster. It proved to be a massive success with two hundred thousand kaleidoscopes sold in London and Paris in just three months.
An instrument of more significance, the stereoscope, which – though of much later date (1849) – along with the kaleidoscope did more than anything else to popularise his name, was not as has often been asserted the invention of Brewster. Sir Charles Wheatstone discovered its principle and applied it as early as 1838 to the construction of a cumbersome but effective instrument, in which the binocular pictures were made to combine by means of mirrors. A dogged rival of Wheatstone's, Brewster was unwilling to credit him with the invention, however, and proposed that the true author of the stereoscope was a Mr. Elliot, a "Teacher of Mathematics" from Edinburgh, who, according to Brewster, had conceived of the principles as early as 1823 and had constructed a lensless and mirrorless prototype in 1839, through which one could view drawn landscape transparencies, since photography had yet to be invented. Brewster's personal contribution was the suggestion to use prisms for uniting the dissimilar pictures; and accordingly the lenticular stereoscope may fairly be said to be his invention.
A much more valuable and practical result of Brewster's optical researches was the improvement of the British lighthouse system. Although Fresnel, who had also the satisfaction of being the first to put it into operation, perfected the dioptric apparatus independently, Brewster was active earlier in the field than Fresnel, describing the dioptric apparatus in 1812. Brewster pressed its adoption on those in authority at least as early as 1820, two years before Fresnel suggested it, and it was finally introduced into lighthouses mainly through Brewster's persistent efforts.
Other work.
Although Brewster's own discoveries were important, they were not his only service to science. He began writing in 1799 as a regular contributor to the "Edinburgh Magazine", of which he acted as editor at the age of twenty. In 1807, he undertook the editorship of the newly projected "Edinburgh Encyclopædia", of which the first part appeared in 1808, and the last not until 1830. The work was strongest in the scientific department, and many of its most valuable articles were from the pen of the editor. At a later period he was one of the leading contributors to the "Encyclopædia Britannica" (seventh and eighth editions) writing, among others, the articles on electricity, hydrodynamics, magnetism, microscope, optics, stereoscope, and voltaic electricity.
In 1819 Brewster undertook further editorial work by establishing, in conjunction with Robert Jameson (1774–1854), the "Edinburgh Philosophical Journal", which took the place of the "Edinburgh Magazine". The first ten volumes (1819–1824) were published under the joint editorship of Brewster and Jameson, the remaining four volumes (1825–1826) being edited by Jameson alone. After parting company with Jameson, Brewster started the "Edinburgh Journal of Science" in 1824, 16 volumes of which appeared under his editorship during the years 1824–1832, with very many articles from his own pen.
He contributed around three hundred papers to the transactions of various learned societies, and few of his contemporaries wrote as much for the various reviews. In the "North British Review" alone, seventy-five articles of his appeared. A list of his larger separate works will be found below. Special mention, however, must be made of the most important of them all: his biography of Sir Isaac Newton. In 1831 he published a short popular account of the philosopher's life in "Murray's Family Library"; but it was not until 1855 that he was able to issue the much fuller "Memoirs of the Life, Writings and Discoveries of Sir Isaac Newton", a work which embodied the results of more than 20 years' investigation of original manuscripts and other available sources.
Brewster's position as editor brought him into frequent contact with the most eminent scientific men, and he was naturally among the first to recognise the benefit that would accrue from regular communication among those in the field of science. In a review of Charles Babbage's book "Decline of Science in England" in "John Murray's Quarterly Review", he suggested the creation of "an association of our nobility, clergy, gentry and philosophers". This was taken up by various "Declinarians" and found speedy realisation in the British Association for the Advancement of Science. Its first meeting was held at York in 1831; and Brewster, along with Babbage and Sir John Herschel, had the chief part in shaping its constitution.
In the same year in which the British Association held its first meeting, Brewster received the honour of knighthood and the decoration of the Royal Guelphic Order. In 1838, he was appointed principal of the united colleges of St Salvator and St Leonard, University of St Andrews. In 1849, he acted as president of the British Association and was elected one of the eight foreign associates of the Institute of France in succession to J. J. Berzelius; and ten years later, he accepted the office of principal of the University of Edinburgh, the duties of which he discharged until within a few months of his death. In 1855, the government of France made him an Officier de la Légion d'honneur.
He was a close friend of William Henry Fox Talbot, inventor of the calotype process, who sent Brewster early examples of his work. It was Brewster who suggested Talbot only patent his process in England, initiating the development of early photography in Scotland and eventually allowing for the formation of the first photographic society in the world, the Edinburgh Calotype Club, in 1843. Brewster was a prominent member of the club until its dissolution sometime in the mid-1850s; however, his interest in photography continued, and he was elected the first President of the Photographic Society of Scotland when it was founded in 1856.
Of a high-strung and nervous temperament, Brewster was somewhat irritable in matters of controversy; but he was repeatedly subjected to serious provocation. He was a man of highly honourable and fervently religious character. In estimating his place among scientific discoverers, the chief thing to be borne in mind is that his genius was not characteristically mathematical. His method was empirical, and the laws that he established were generally the result of repeated experiment. To the ultimate explanation of the phenomena with which he dealt he contributed nothing, and it is noteworthy although he did not maintain to the end of his life the corpuscular theory he never explicitly adopted the wave theory of light. Few would dispute the verdict of James David Forbes, an editor of the eighth edition of the "Encyclopædia Britannica": "His scientific glory is different in kind from that of Young and Fresnel; but the discoverer of the law of polarization of biaxial crystals, of optical mineralogy, and of double refraction by compression, will always occupy a foremost rank in the intellectual history of the age." In addition to the various works of Brewster already mentioned, the following may be added: "Notes and Introduction to Carlyle's translation of Legendre's Elements of Geometry" (1824); "Treatise on Optics" (1831); "The Martyrs of Science, or the Lives of Galileo, Tycho Brahe, and Kepler" (1841); "More Worlds than One" (1854).
In his "Treatise" he demonstrated that vegetal colors were related with the absorption spectra and he described for the first time the red fluorescence of chlorophyll.
Opposition to evolution.
Brewster's Christian beliefs stirred him to respond against the idea of the transmutation of species and the theory of evolution. In 1845 he wrote a highly critical review of the evolutionist work "Vestiges of the Natural History of Creation", in the "North British Review"., which he considered to be an insult to Christian revelation and a dangerous example of materialism.
In 1862, he again responded to Darwin's "On the Origin of Species" and published the article "" in "Good Words". He stated that Darwin's book combined both "interesting facts and idle fancies" which made up a "dangerous and degrading speculation". He accepted adaptive changes, but he strongly opposed Darwin's statement about the "primordial form", which he considered an offensive idea to "both the naturalist and the Christian."
Family.
Brewster married twice. His first wife, Juliet Macpherson (c. 1776–1850), was a daughter of James Macpherson (1736–1796), a probable translator of Ossian poems. They married on 31 July 1810 in Edinburgh and had four sons and a daughter:
Brewster married a second time in Nice, on 26 (or 27) March 1857, to Jane Kirk Purnell (b. 1827), the second daughter of Thomas Purnell of Scarborough. Brewster died in 1868, and was buried at Melrose Abbey, next to his first wife and second son. The physics building at Heriot-Watt University is named in his honour.
External links.
 

</doc>
<doc id="8488" url="http://en.wikipedia.org/wiki?curid=8488" title="Dual-tone multi-frequency signaling">
Dual-tone multi-frequency signaling

Dual-tone multi-frequency signaling (DTMF) is used for telecommunication signaling over analog telephone lines in the voice-frequency band between telephone handsets and other communications devices and the switching center. The version of DTMF that is used in push-button telephones for tone dialing is known as Touch-Tone. It was developed by Western Electric and first used by the Bell System in commerce, using that name as a registered trademark. DTMF is standardized by ITU-T Recommendation . It is also known in the UK as "MF4".
Other multi-frequency systems are used for internal signaling within the telephone network.
Introduced by AT&T in 1963, the Touch-Tone system using the telephone keypad gradually replaced the use of rotary dial and has become the industry standard for landline service.
Multifrequency signaling.
Prior to the development of DTMF, numbers were dialed on automated telephone systems by means of pulse dialing ("dial pulse," DP, in the U.S.) or loop disconnect (LD) signaling, which functions by rapidly disconnecting and re-connecting the calling party's telephone line, similar to flicking a light switch on and off. The repeated interruptions of the line, as the dial spins, sounds like a series of clicks. The exchange equipment interprets these dial pulses to determine the dialed number. Loop disconnect range was restricted by telegraphic distortion and other technical problems, and placing calls over longer distances required either operator assistance (operators used an earlier kind of multi-frequency dial) or the provision of subscriber trunk dialing equipment.
Other vendors of compatible telephone equipment called the Touch-Tone feature "Tone dialing" or "DTMF", or used their own registered trade names such as the "Digitone" of Northern Electric (later known as Nortel Networks).
The DTMF system uses eight different frequency signals transmitted in pairs to represent 16 different numbers, symbols and letters - as detailed below.
As a method of in-band signaling, DTMF tones were also used by cable television broadcasters to indicate the start and stop times of local commercial insertion points during station breaks for the benefit of cable companies. Until better out-of-band signaling equipment was developed in the 1990s, fast, unacknowledged, and loud DTMF tone sequences could be heard during the commercial breaks of cable channels in the United States and elsewhere. Previously, terrestrial television stations also used DTMF tones to shut off and turn on remote transmitters.
Multi-frequency signaling (see also MF) is a group of signaling methods that use a mixture of two pure tone (pure sine wave) sounds. Various MF signaling protocols were devised by the Bell System and CCITT. The earliest of these were for in-band signaling between switching centers, where long-distance telephone operators used a 16-digit keypad to input the next portion of the destination telephone number in order to contact the next downstream long-distance telephone operator. This semi-automated signaling and switching proved successful in both speed and cost effectiveness. Based on this prior success with using MF by specialists to establish long-distance telephone calls, Dual-tone multi-frequency (DTMF) signaling was developed for the consumer to signal their own telephone-call's destination telephone number instead of talking to a telephone operator.
AT&Ts Compatibility Bulletin No. 105 described the product as "a method for pushbutton signaling from customer stations using the voice transmission path." In order to prevent consumer telephones from interfering with the MF-based routing and switching between telephone switching centers, DTMF's frequencies differ from all of the pre-existing MF signaling protocols between switching centers: MF/R1, R2, CCS4, CCS5, and others that were later replaced by SS7 digital signaling. DTMF, as used in push-button telephone tone dialing, was known throughout the Bell System by the trademark Touch-Tone. This term was first used by AT&T in commerce on July 5, 1960 and then was introduced to the public on November 18, 1963, when the first push-button telephone was made available to the public. It was AT&T's registered trademark from September 4, 1962 to March 13, 1984, and is standardized by ITU-T Recommendation Q.23. It is also known in the UK as MF4.
#, *, A, B, C, and D.
The engineers had envisioned phones being used to access computers, and surveyed a number of companies to see what they would need for this role. This led to the addition of the number sign (#, "pound" or "diamond" in this context, "hash", "square" or "gate" in the UK, and "octothorpe" by the original engineers) and asterisk or "star" (*) keys as well as a group of keys for menu selection: A, B, C and D. In the end, the lettered keys were dropped from most phones, and it was many years before the two symbol keys became widely used for vertical service codes such as *67 in the United States of America and Canada to suppress caller ID.
Public payphones that accept credit cards use these additional codes to send the information from the magnetic strip.
The United States Armed Forces also used the letters, relabeled, in their now-defunct AUTOVON telephone system to indicate the priority of a call. Precedence dialing is still done on the military phone networks, but using number combinations (Example: Entering 93 before a number is a priority call) rather than the separate tones and the Government Emergency Telecommunications Service has superseded AUTOVON for any civilian priority telephone company access.
Present-day uses of the A, B, C and D keys on telephone networks are few, and exclusive to network control. For example, the A key is used on some networks to cycle through different carriers at will (thereby listening in on calls). Their use is probably prohibited by most carriers. The A, B, C and D tones are used in radio phone patch and repeater operations to allow, among other uses, control of the repeater while connected to an active phone line.
The *, #, A, B, C and D keys are still widely used worldwide by amateur radio operators for repeater control, remote-base operations and some telephone communications systems.
DTMF signaling tones can also be heard at the start or end of some VHS (Video Home System) cassette tapes. Information on the master version of the video tape is encoded in the DTMF tone. The encoded tone provides information to automatic duplication machines, such as format, duration and volume levels, in order to replicate the original video as closely as possible.
DTMF tones are sometimes used in caller ID systems to transfer the caller ID information, but in the United States only Bell 202 modulated FSK signaling is used to transfer the data.
Keypad.
The DTMF keypad is laid out in a 4×4 matrix in which each row represents a "low" frequency and each column represents a "high" frequency. Pressing a single key sends a sinusoidal tone for each of the two frequencies. For example, the key 1 produces a superimposition of tones of 697 and 1209 hertz (Hz). Initial pushbutton designs employed levers, so that each button activated two contacts. The tones are decoded by the switching center to determine the keys pressed by the user.
Special tone frequencies.
National telephone systems define additional tones to indicate the status of lines, equipment, or the result of calls with special tones. Such tones are standardized in each country and may consist of single or multiple frequencies. Most European countries use a single precise frequency of 425 Hz, where the United States uses a dual frequency system.
The tone frequencies, as defined by the Precise Tone Plan, are selected such that harmonics and intermodulation products will not cause an unreliable signal. No frequency is a multiple of another, the difference between any two frequencies does not equal any of the frequencies, and the sum of any two frequencies does not equal any of the frequencies. The frequencies were initially designed with a ratio of 21/19, which is slightly less than a whole tone. The frequencies may not vary more than ±1.8% from their nominal frequency, or the switching center will ignore the signal. The high frequencies may be the same volume as – or louder than – the low frequencies when sent across the line. The loudness difference between the high and low frequencies can be as large as 3 decibels (dB) and is referred to as "twist." The duration of the tone should be at least 537 ms.
European Tones:
As with other multi-frequency receivers, DTMF was originally decoded by tuned filter banks. Late in the 20th century most were replaced with digital signal processors. Although DTMF can be decoded using any frequency domain transform (such as the popular Fast Fourier transform), the Goertzel algorithm is a common algorithm to consider due to its high performance for DTMF.

</doc>
<doc id="8489" url="http://en.wikipedia.org/wiki?curid=8489" title="Deuterocanonical books">
Deuterocanonical books

Deuterocanonical books is a term used since the 16th century in the Catholic Church and Eastern Christianity to describe certain books and passages of the Christian Old Testament that are not part of the Hebrew Bible. The term is used in contrast to the protocanonical books, which are contained in the Hebrew Bible. This distinction had previously contributed to debate in the early Church about whether they should be classified as canonical texts. The term is used as a matter of convenience by the Ethiopian Orthodox Tewahedo Church and other Churches to refer to books of their Old Testament which are not part of the Masoretic Text.
The deuterocanonical books are considered canonical by Catholics, Eastern Orthodox, Oriental Orthodox, and the Church of the East, but are considered non-canonical by most Protestants. The word "deuterocanonical" comes from the Greek meaning 'belonging to the second canon'.
The original usage of the term distinguished these scriptures both from those considered "non-canonical" and from those considered "protocanonical". However, some editions of the Bible include text from both deuterocanonical and non-canonical scriptures in a single section designated "Apocrypha". This arrangement can lead to conflation between the otherwise distinct terms "deuterocanonical" and "apocryphal".
History.
Deuterocanonical is a term coined in 1566 by the theologian Sixtus of Siena, who had converted to Catholicism from Judaism, to describe scriptural texts of the Old Testament considered canonical by the Catholic Church, but which are not present in the Hebrew Bible, and which had been omitted by some early canon lists, especially in the East.
Their acceptance among early Christians was widespread, though not universal, and the Bible of the early Church always included, with varying degrees of recognition, books now called "deuterocanonical". Some say that their canonicity seems not to have been doubted in the Church until it was challenged by Jews after AD 100, sometimes postulating a hypothetical Council of Jamnia. Regional councils in the West published official canons that included these books as early as the 4th and 5th centuries.
Dead Sea scrolls.
Fragments of three deuterocanonical books have been found among the Dead Sea scrolls found at Qumran, in addition to several partial copies of "I Enoch" and "Jubilees" from the Ethiopic deuterocanon. "Sirach", whose Hebrew text was already known from the Cairo Geniza, has been found in two scrolls (2QSir or 2Q18, 11QPs_a or 11Q5) in Hebrew. Another Hebrew scroll of "Sirach" has been found in Masada (MasSir). The "Book of Tobit" has been found in Qumran in four scrolls written in Aramaic and in one written in Hebrew. The "Letter of Jeremiah" (or "Baruch" chapter 6) has been found in cave 7 (7Q5) in Greek. It has been theorized by recent scholars that the Qumran library was not entirely produced at Qumran, but may have included part of the library of the Jerusalem Temple, that may have been hidden in the caves for safekeeping at the time the Temple was destroyed by Romans in 70 AD.
Influence of the Septuagint.
The large majority of Old Testament references in the New Testament are taken from the Greek Septuagint (LXX)—which includes the deuterocanonical books, as well as apocrypha —both of which are called collectively ἀναγιγνωσκόμενα "anagignoskomena" (things that are read or "profitable reading"). Several appear to have been written originally in Hebrew, but the original text has long been lost. Archaeological finds, however, discovered some original texts among the Dead Sea scrolls. The Septuagint was widely accepted and used by Greek-speaking Jews in the 1st century, even in the region of Roman Judea, and therefore naturally became the text most widely used by early Christians, who were predominantly Greek speaking.
In the New Testament, Hebrews 11:35 refers to an event that was
recorded in one of the deuterocanonical books 2 Maccabees.
Other New Testament authors also quote period literature which was familiar to the audience but that was not included in the Old Testament or the deuterocanonical books. For instance, Paul cites Greek writers and philosophers, and the author of Hebrews references oral tradition which spoke of an Old Testament prophet who was sawn in half in Hebrews 11:37, two verses after the 2nd Maccabees reference.
The Jewish historian Josephus speaks of there being 22 books in the canon of the Hebrew Bible, a Jewish tradition reported also by the Christian bishop Athanasius. However, included in Athanasius's list of 22 Old Testament books are Baruch and the Letter of Jeremiah. At the same time, he mentioned that certain other books, including five deuterocanonical books but also the Didache and the Shepherd of Hermas, while not being part of the canon, "were appointed by the Fathers to be read". He excluded what he called "apocryphal writings" entirely.
In the Catholic Church.
In the Catholic Church, "the first infallible and effectually promulgated pronouncement on the Canon" was that defined by the Council of Trent. Among the minority, in Trent, that showed opposition to these books' inclusion were Cardinals Seripando and Cajetan, the latter an opponent of Luther at Augsburg. However, Trent confirmed the statements of earlier and less authoritative regional councils which also included the deuterocanonical books, such as the Synod of Hippo (393), and the Councils of Carthage of 397. Much later (15th century), the Council of Florence taught the divine inspiration of these books, but "did not formally pass on their canonicity."
In the canonical debate between Catholics and Protestants controversy remains as to the significance of Trent's omission of the Septuagint version of 1 Esdras which Carthage may have ratified. However, there is ambiguity over the naming of the books of Esdras. The "Canon of Carthage" lists two books of Esdras. This could mean 1 Esdras and Ezra-Nehemiah as in the Septuagint or Ezra and Nehemiah as in the Vulgate.
The Catholic deuterocanonical scriptural texts are:
Influence of the Vulgate.
Jerome in the Vulgate's prologues describes a canon which excludes the deuterocanonical books, possibly excepting Baruch. In his "Prologues", Jerome mentions all of the deuterocanonical and apocryphal works by name as being apocryphal or "not in the canon" except for "Prayer of Manasses" and "Baruch". He mentions "Baruch" by name in his and notes that it is neither read nor held among the Hebrews, but does not explicitly call it apocryphal or "not in the canon". The inferior status to which the deuterocanonical books were relegated by authorities like Jerome is seen by some as being due to a rigid conception of canonicity, one demanding that a book, to be entitled to this supreme dignity, must be received by all, must have the sanction of Jewish antiquity, and must moreover be adapted not only to edification, but also to the "confirmation of the doctrine of the Church". Eventually however, Jerome's Vulgate did include the deuterocanonical books as well as apocrypha. Jerome referenced and quoted from some as scripture despite describing them as "not in the canon". In his prologue to Judith, without using the word canon, he mentioned that Judith was held to be scriptural by the First Council of Nicaea. In his reply to Rufinus, he affirmed that he was consistent with the choice of the church regarding which version of the deuterocanonical portions of Daniel to use, which the Jews of his day did not include:
Thus Jerome acknowledged the principle by which the canon would be settled —the judgment of the Church, rather than his own judgment or the judgment of Jews, though he wondered why one would sanction the version of a heretic and judaizer.
The Vulgate is also important as the touchstone of the canon concerning which parts of books are canonical. When the Council of Trent listed the books included in the canon, it qualified the books as being "entire with all their parts, as they have been used to be read in the Catholic Church, and as they are contained in the old Latin vulgate edition". This decree was clarified somewhat by Pope Pius XI on June 2, 1927, who allowed that the Comma Johanneum was open to dispute, and it was further explicated by Pope Pius XII's Divino Afflante Spiritu.
Reception in Orthodox Christianity and other churches.
Outside the Roman Catholic Church, the term deuterocanonical is sometimes used, by way of analogy, to describe books that Eastern Orthodoxy, and Oriental Orthodoxy included in the Old Testament that are not part of the Jewish Tanakh, nor the Protestant Old Testament. Among Orthodox, the term is understood to mean that they were compiled separately from the primary canon, as explained in 2 Esdras, where Esdras is instructed to keep certain books separate and hidden.
Eastern Orthodoxy.
The Eastern Orthodox Churches have traditionally included all the books of the Septuagint in their Old Testaments. The Greeks use the word "Anagignoskomena" (Ἀναγιγνωσκόμενα "readable, worthy to be read") to describe the books of the Greek Septuagint that are not present in the Hebrew Tanakh. When Orthodox theologians use the term "deuterocanonical," it is important to note that the meaning is not identical to the Roman Catholic usage. In Orthodox Christianity, deuterocanonical means that a book is part of the corpus of the Old Testament (i.e. is read during the services) but has secondary authority. In other words, deutero (second) applies to authority or witnessing power, whereas in Roman Catholicism, deutero applies to chronology (the fact that these books were confirmed later), not to authority.
The Eastern Orthodox books included in the Old Testament are the seven deuterocanonical books listed above, plus 3 Maccabees and 1 Esdras (also included in the Clementine Vulgate), while Baruch is divided from the Epistle of Jeremiah, making a total of 49 Old Testament books in contrast with the Protestant 39-book canon.
Like the Roman Catholic deuterocanonical books, these texts are integrated with the rest of the Old Testament, not printed in a separate section.
Other texts printed in Orthodox Bibles are considered of some value (like the additional Psalm 151, and the Prayer of Manasseh) or are included as an appendix (like the Greek 4 Maccabees, and the Slavonic 2 Esdras).
Ethiopian Orthodoxy.
In the Amharic Bible used by the Ethiopian Orthodox Church (an Oriental Orthodox Church), those books of the Old Testament that are still counted as canonical, but not by all other Churches, are often set in a separate section titled "Deeyutrokanoneekal" (ዲዩትሮካኖኒካል), which is the same word. The Ethiopian Orthodox Deuterocanon, in addition to the standard set listed above, along with the books of Esdras and "Prayer of Minasse", also includes some books that are still held canonical by only the Ethiopian Church, including Enoch or "Henok" (I Enoch), "Kufale" (Jubilees) and 1, 2 and 3 Meqabyan (which are sometimes wrongly confused with the "Books of Maccabees").
Jewish position.
Judaism and most Protestant versions of the Bible exclude these books. It is commonly said that Judaism officially excluded the deuterocanonicals and the additional Greek texts listed here from their Scripture in the Council of Jamnia (c.70-90 AD), but this claim is also disputed.
Reception in Christian churches having their origins in the Reformation.
Anglicanism.
There is a great deal of overlap between the Apocrypha section of the original 1611 King James Bible and the Catholic deuterocanon, but the two are distinct. The Apocrypha section of the original 1611 King James Bible includes, in addition to the deuterocanonical books, the following three books, which were not included in the list of the canonical books by the Council of Trent:
These books make up the Apocrypha section of the Clementine Vulgate: 3 Esdras (1 Esdras); 4 Esdras (2 Esdras); and The Prayer of Manasseh, where they are specifically described as "outside of the series of the canon". The 1609 Douai Bible includes them in an appendix, but they have been dropped from recent Catholic translations into English. They are found, along with the deuterocanonical books, in the Apocrypha section of Protestant bibles.
Using the word apocrypha (Greek: hidden away) to describe texts, although not necessarily pejorative, implies to some people that the writings in question should not be included in the canon of the Bible. This classification commingles them with certain non-canonical gospels and New Testament Apocrypha. The "Style Manual for the Society of Biblical Literature" recommends the use of the term "deuterocanonical literature" instead of "Apocrypha" in academic writing.
The Thirty-Nine Articles of Religion of the Church of England lists the deuterocanonical books as suitable to be read for "example of life and instruction of manners, but yet doth not apply them to establish any doctrine." The early lectionaries of the Anglican Church (as included in the Book of Common Prayer of 1662) included the deuterocanonical books amongst the cycle of readings, and passages from them were used in the services (such as the Benedicite)
Readings from the deuterocanonical books are now included in most, if not all, of the modern lectionaries in the Anglican Communion, based on the Revised Common Lectionary (in turn based on the post-conciliar Roman Catholic lectionary).
Presbyterianism.
The Westminster Confession of Faith, a Calvinist document that serves as a systematic summary of doctrine for the Church of Scotland and Presbyterian churches worldwide, recognizes only the sixty-six books of the Protestant canon as authentic Scripture. Chapter 1, Article 3 of the Confession reads: "The books commonly called Apocrypha, not being of divine inspiration, are no part of the Canon of Scripture; and therefore are of no authority in the Church of God, nor to be any otherwise approved, or made use of, than other human writings."
Reformed churches.
The Belgic Confession, used in Reformed churches, devotes a section (Article 6) to "The difference between the canonical and apocryphal books" and asserts that "All which the Church may read and take instruction from, so far as they agree with the canonical books; but they are far from having such power and efficacy as that we may from their testimony confirm any point of faith or of the Christian religion; much less to detract from the authority of the other sacred books."
New Testament apocrypha and deuterocanonicals.
The term "deuterocanonical" is sometimes used to describe the canonical antilegomena, those books of the New Testament which, like the deuterocanonicals of the Old Testament, were not universally accepted by the early Church, but which are now included in the 27 books of the New Testament recognized by almost all Christians. The deuterocanonicals of the New Testament are as follows:

</doc>
<doc id="8490" url="http://en.wikipedia.org/wiki?curid=8490" title="Discus throw">
Discus throw

The discus throw () is a track and field event in which an athlete throws a heavy disc—called a discus—in an attempt to mark a farther distance than his or her competitors. It is an ancient sport, as evidenced by the fifth-century-B.C. Myron statue, "Discobolus". Although not part of the modern pentathlon, it was one of the events of the ancient Greek pentathlon, which can be dated at least to 708 BC.
History.
The discus throw is a routine part of most modern track-and-field meets at all levels and is a sport which is particularly iconic of the Olympic Games. The men's competition has been a part of the modern Summer Olympic Games since the first Olympiad in 1896. Images of discus throwers figured prominently in advertising for early modern Games, such as fundraising stamps for the 1896 games and for the 1920 and 1948 Summer Olympics.
The women's competition was added to the Olympic program in the 1928 games, although they had been competing at some national and regional levels previously.
Description.
The discus, the object to be thrown, is a heavy lenticular disc with a weight of and diameter of for the men's event, and a weight of and diameter of for the women's program.
Under IAAF (international) rules, Youth boys (16–17 years) throw the discus, the Junior men (18–19 years) throw the unique discus, and the girls/women of those ages throw the 1 kg discus.
In international competition, men throw the 2 kg discus through age 49. The discus is thrown by ages 50–59, and men age 60 and beyond throw the discus. Women throw the discus through age 74. Starting with age 75, women throw the discus.
The typical discus has sides made of plastic, wood, fiberglass, carbon fiber or metal with a metal rim and a metal core to attain the weight. The rim must be smooth, with no roughness or finger holds. A discus with more weight in the rim produces greater angular momentum for any given spin rate, and thus more stability, although it is more difficult to throw. However, a higher rim weight, if thrown correctly, can lead to a farther throw. a solid rubber discus is sometimes used (see in the United States).
To make a throw, the competitor starts in a circle of diameter, which is recessed in a concrete pad by 20 mm. The thrower typically takes an initial stance facing away from the direction of the throw. He then spins counter-clockwise (for right-handers) around one and a half times through the circle to build momentum, then releases his throw. The discus must land within a 34.92-degree sector. The rules of competition for discus are virtually identical to those of shot put, except that the circle is larger, a stop board is not used and there are no form rules concerning how the discus is to be thrown.
The distance from the front edge of the circle to where the discus has landed is measured, and distances are rounded down to the nearest centimetre. The competitor's best throw from the allocated number of throws, typically three to six, is recorded, and the competitor who legally throws the discus the farthest is declared the winner. Ties are broken by determining which thrower has the longer second-best throw.
The basic motion is a forehanded sidearm movement. The discus is spun off the index finger or the middle finger of the throwing hand. In flight the disc spins clockwise when viewed from above for a right-handed thrower, and counter-clockwise for a lefty. As well as achieving maximum momentum in the discus on throwing, the discus' distance is also determined by the trajectory the thrower imparts, as well as the aerodynamic behavior of the discus. Generally, throws into a moderate headwind achieve the maximum distance. Also, a faster-spinning discus imparts greater gyroscopic stability. The technique of discus throwing is quite difficult to master and needs lots of experience to get right, thus most top throwers are 30 years old or more.
Phases.
There are six key movements of the discus throw: wind up, move in rhythm, balance, right leg engine, orbit, and delivery.
The wind up is one of the most important aspects of the throw because it sets the tone for the entire throw. The wind up is both mental and technical. It is mental because the wind up sets the thrower up for the rest of the throw.
The following are the technical aspects: flat right foot, on the ball of your left foot, keep your weight evenly distributed between your feet, and not being overly active, which results in the waste of energy. Although the wind up sets the tone for the entire throw, the rhythm of the throw is the most important aspect. It is necessary to move in rhythm throughout the entire throw.
The best throwers contain the same amount of time in each phase while completing a great throw. Focusing on rhythm can bring about the consistency to get in the right positions that many throwers lack. Executing a sound discus throw with solid technique requires perfect balance. This is due to the throw being a linear movement combined with a one and a half rotation and an implement at the end of one arm. Thus, a good discus thrower needs to maintain balance within the circle.
It is also important that the discus thrower keeps their shoulders at the same level during the throw until the end, where the thrower must extend their shoulders upward to get good lift under the discus. If extension is executed properly the discus will be at the right angle to ride on the air current and thus be taken a farther distance.
Culture.
The discus throw is the subject of a number of well-known ancient Greek statues and Roman copies such as the Discobolus and Discophoros.
Discus throwers have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Discus commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the obverse of the coin a modern athlete is seen in the foreground in a half-turned position, while in the background an ancient discus thrower has been captured in a lively bending motion, with the discus high above his head, creating a vivid representation of the sport.
United States.
In U.S. high school track and field, boys typically throw a discus weighing 1.6 kg (3 lb 9 oz) and the girls throw the 1 kg (2.2 lb) women's discus. Under USATF Youth rules, boys throw the 1 kg discus between the ages of 11-14, and transition to the 1.6 kg discus as 15-18 year olds. Girls throw the 1 kg discus as 11-18 year olds.
Under US high school rules, if a discus hits the surrounding safety cage and is deflected into the sector, it is ruled a foul. In contrast, under IAAF, WMA, NCAA and USATF rules, it is ruled a legal throw. Additionally, under US high school rules, distances thrown are rounded down to the nearest whole inch, rather than the nearest centimetre.
US high school rules allow the use of a solid rubber discus; it is cheaper and easier to learn to throw (due to its more equal distribution of weight, as opposed to the heavy rim weight of the metal rim/core discus), but less durable.
Top ten performers.
"Accurate as of June 2013".

</doc>
<doc id="8492" url="http://en.wikipedia.org/wiki?curid=8492" title="Discrete mathematics">
Discrete mathematics

Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying "smoothly", the objects studied in discrete mathematics – such as integers, graphs, and statements in logic – do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in "continuous mathematics" such as calculus and analysis. Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (sets that have the same cardinality as subsets of the natural numbers, including rational numbers but not real numbers). However, there is no exact definition of the term "discrete mathematics." Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.
The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.
Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in discrete steps and store data in discrete bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research.
Although the main objects of study in discrete mathematics are discrete objects, analytic methods from continuous mathematics are often employed as well.
Grand challenges, past and present.
The history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field. In graph theory, much research was motivated by attempts to prove the four color theorem, first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).
In logic, the second problem on David Hilbert's list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent. Gödel's second incompleteness theorem, proved in 1931, showed that this was not possible – at least not within arithmetic itself. Hilbert's tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. In 1970, Yuri Matiyasevich proved that this could not be done.
The need to break German codes in World War II led to advances in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England's Bletchley Park. At the same time, military requirements motivated advances in operations research. The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. Operations research remained important as a tool in business and project management, with the critical path method being developed in the 1950s. The telecommunication industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.
Computational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools.
Several fields of discrete mathematics, particularly theoretical computer science, graph theory, and combinatorics, are important in addressing the challenging bioinformatics problems associated with understanding the tree of life.
Currently, one of the most famous open problems in theoretical computer science is the P = NP problem, which involves the relationship between the complexity classes P and NP. The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof, along with prizes for six other mathematical problems.
Topics in discrete mathematics.
Theoretical computer science.
Theoretical computer science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and logic. Included within theoretical computer science is the study of algorithms for computing mathematical results. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time taken by computations. Automata theory and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.
Information theory.
Information theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.
Logic.
Logic is the study of the principles of valid reasoning and inference, as well as of consistency, soundness, and completeness. For example, in most systems of logic (but not in intuitionistic logic) Peirce's law ((("P"→"Q")→"P")→"P") is a theorem. For classical logic, it can be easily verified with a truth table. The study of mathematical proof is particularly important in logic, and has applications to automated theorem proving and formal verification of software.
Logical formulas are discrete structures, as are proofs, which form finite trees or, more generally, directed acyclic graph structures (with each inference step combining one or more premise branches to give a single conclusion). The truth values of logical formulas usually form a finite set, generally restricted to two values: "true" and "false", but logic can also be continuous-valued, e.g., fuzzy logic. Concepts such as infinite proof trees or infinite derivation trees have also been studied, e.g. infinitary logic.
Set theory.
Set theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.
In discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor's work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.
Combinatorics.
Combinatorics studies the way in which discrete structures can be combined or arranged.
Enumerative combinatorics concentrates on counting the number of certain combinatorial objects - e.g. the twelvefold way provides a unified framework for counting permutations, combinations and partitions.
Analytic combinatorics concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.
Design theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties.
Partition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, partition theory is now considered a part of combinatorics or an independent field.
Order theory is the study of partially ordered sets, both finite and infinite.
Graph theory.
Graph theory, the study of graphs and networks, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right. Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of topology, e.g. knot theory. Algebraic graph theory has close links with group theory. There are also continuous graphs, however for the most part research in graph theory falls within the domain of discrete mathematics.
Probability.
Discrete probability theory deals with events that occur in countable sample spaces. For example, count observations such as the numbers of birds in flocks comprise only natural number values {0, 1, 2, ...}. On the other hand, continuous observations such as the weights of birds comprise real number values and would typically be modeled by a continuous probability distribution such as the normal. Discrete probability distributions can be used to approximate continuous ones and vice versa. For highly constrained situations such as throwing dice or experiments with decks of cards, calculating the probability of events is basically enumerative combinatorics.
Number theory.
Number theory is concerned with the properties of numbers in general, particularly integers. It has applications to cryptography, cryptanalysis, and cryptology, particularly with regard to modular arithmetic, diophantine equations, linear and quadratic congruences, prime numbers and primality testing. Other discrete aspects of number theory include geometry of numbers. In analytic number theory, techniques from continuous mathematics are also used. Topics that go beyond discrete objects include transcendental numbers, diophantine approximation, p-adic analysis and function fields.
Algebra.
Algebraic structures occur as both discrete examples and continuous examples. Discrete algebras include: boolean algebra used in logic gates and programming; relational algebra used in databases; discrete and finite versions of groups, rings and fields are important in algebraic coding theory; discrete semigroups and monoids appear in the theory of formal languages.
Calculus of finite differences, discrete calculus or discrete analysis.
A function defined on an interval of the integers is usually called a sequence. A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a recurrence relation or difference equation. Difference equations are similar to a differential equations, but replace differentiation by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance where there are integral transforms in harmonic analysis for studying continuous functions or analog signals, there are discrete transforms for discrete functions or digital signals. As well as the discrete metric there are more general discrete or finite metric spaces and finite topological spaces.
Geometry.
Discrete geometry and combinatorial geometry are about combinatorial properties of "discrete collections" of geometrical objects. A long-standing topic in discrete geometry is tiling of the plane. Computational geometry applies algorithms to geometrical problems.
Topology.
Although topology is the field of mathematics that formalizes and generalizes the intuitive notion of "continuous deformation" of objects, it gives rise to many discrete topics; this can be attributed in part to the focus on topological invariants, which themselves usually take discrete values.
See combinatorial topology, topological graph theory, topological combinatorics, computational topology, discrete topological space, finite topological space, topology (chemistry).
Operations research.
Operations research provides techniques for solving practical problems in business and other fields — problems such as allocating resources to maximize profit, or scheduling project activities to minimize risk. Operations research techniques include linear programming and other areas of optimization, queuing theory, scheduling theory, network theory. Operations research also includes continuous topics such as continuous-time Markov process, continuous-time martingales, process optimization, and continuous and hybrid control theory.
Game theory, decision theory, utility theory, social choice theory.
Decision theory is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision.
Utility theory is about measures of the relative economic satisfaction from, or desirability of, consumption of various goods and services.
Social choice theory is about voting. A more puzzle-based approach to voting is ballot theory.
Game theory deals with situations where success depends on the choices of others, which makes choosing the best course of action more complex. There are even continuous games, see differential game. Topics include auction theory and fair division.
Discretization.
Discretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. Numerical analysis provides an important example.
Discrete analogues of continuous mathematics.
There are many concepts in continuous mathematics which have discrete versions, such as discrete calculus, discrete probability distributions, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, difference equations, discrete dynamical systems, and discrete vector measures.
In applied mathematics, discrete modelling is the discrete analogue of continuous modelling. In discrete modelling, discrete formulae are fit to data. A common method in this form of modelling is to use recurrence relation.
In algebraic geometry, the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field, and letting subvarieties or spectra of other rings provide the curves that lie in that space. Although the space in which the curves appear has a finite number of points, the curves are not so much sets of points as analogues of curves in continuous settings. For example, every point of the form formula_1 for formula_2 a field can be studied either as formula_3, a point, or as the spectrum formula_4 of the local ring at (x-c), a point together with a neighborhood around it. Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space, making many features of calculus applicable even in finite settings.
Hybrid discrete and continuous mathematics.
The time scale calculus is a unification of the theory of difference equations with that of differential equations, which has applications to fields requiring simultaneous modelling of discrete and continuous data. Another way of modeling such a situation is the notion of hybrid dynamical system.

</doc>
<doc id="8494" url="http://en.wikipedia.org/wiki?curid=8494" title="DDT">
DDT

DDT ("dichlorodiphenyltrichloroethane") is a colorless, crystalline, tasteless and almost odorless organochloride known for its insecticidal properties. DDT has been formulated in almost every conceivable form, including solutions in xylene or petroleum distillates, emulsifiable concentrates, water-wettable powders, granules, aerosols, smoke candles and charges for vaporisers and lotions.
First synthesized in 1874, DDT's insecticidal action was discovered by the Swiss chemist Paul Hermann Müller in 1939. It was then used in the second half of World War II to control malaria and typhus among civilians and troops. After the war, DDT was made available for use as an agricultural insecticide and its production and use duly increased. Müller was awarded the Nobel Prize in Physiology or Medicine "for his discovery of the high efficiency of DDT as a contact poison against several arthropods" in 1948. However, widespread agricultural use accelerated resistance among insect populations, in many cases reversing early successes against malaria-carrying mosquitos.
In 1962, the book "Silent Spring" by American biologist Rachel Carson was published. It catalogued the environmental impacts of indiscriminate DDT spraying in the United States and questioned the logic of releasing large amounts of chemicals into the environment without a sufficient understanding of their effects on ecology or human health. The book claimed that DDT and other pesticides had been shown to cause cancer and that their agricultural use was a threat to wildlife, particularly birds. Its publication was a seminal event for the environmental movement and resulted in a large public outcry that eventually led, in 1972, to a ban on the agricultural use of DDT in the United States. A worldwide ban on its agricultural use was later formalised under the Stockholm Convention, but its limited use in disease vector control continues to this day and remains controversial, because of its initial effectiveness in reducing deaths due to malaria, as well as the pesticide resistance among mosquito populations it engenders after several years of use.
Along with the passage of the Endangered Species Act, the US ban on DDT is cited by scientists as a major factor in the comeback of the bald eagle (the national bird of the United States) and the peregrine falcon from near-extinction in the contiguous United States.
Properties and chemistry.
DDT is similar in structure to the insecticide methoxychlor and the acaricide dicofol. Being highly hydrophobic, it is nearly insoluble in water but has good solubility in most organic solvents, fats and oils. DDT does not occur naturally, but is produced by the reaction of chloral () with chlorobenzene () in the presence of sulfuric acid as a catalyst. Trade names that DDT has been marketed under include Anofex (Geigy Chemical Corp.), Cezarex, Chlorophenothane, Clofenotane, Dicophane, Dinocide, Gesarol (Syngenta Corp.), Guesapon, Guesarol, Gyron (Ciba-Geigy Corp.), Ixodex, Neocid (Reckitt & Colman Ltd), Neocidol (Ciba-Geigy Corp.) and Zerdane.
Isomers and related compounds.
Commercial DDT is a mixture of several closely–related compounds. The major component (77%) is the "p","p' " isomer which is pictured at the top of this article. The "o","p' " isomer (pictured to the right) is also present in significant amounts (15%). Dichlorodiphenyldichloroethylene (DDE) and dichlorodiphenyldichloroethane (DDD) make up the balance. DDE and DDD are also the major metabolites and breakdown products in the environment. The term "total DDT" is often used to refer to the sum of all DDT related compounds ("p,p'-"DDT, "o,p'-"DDT, DDE, and DDD) in a sample.
Production and use statistics.
From 1950 to 1980, DDT was extensively used in agriculture – more than 40,000 tonnes were used each year worldwide – and it has been estimated that a total of 1.8 million tonnes have been produced globally since the 1940s. In the U.S., where it was manufactured by some 15 companies including Monsanto, Ciba, Montrose Chemical Company, Pennwalt and Velsicol Chemical Corporation, production peaked in 1963 at 82,000 tonnes per year. More than 600,000 tonnes (1.35 billion lbs) were applied in the U.S. before the 1972 ban. Usage peaked in 1959 at about 36,000 tonnes.
In 2009, 3314 tonnes were produced for the control of malaria and visceral leishmaniasis, hence it still qualifies as a High Production Volume Chemical. India is the only country still manufacturing DDT, with China having ceased production in 2007. India is the largest consumer.<ref name="DDTBP.1/2"></ref>
Mechanism of insecticide action.
In insects it opens sodium ion channels in neurons, causing them to fire spontaneously, which leads to spasms and eventual death. Insects with certain mutations in their sodium channel gene are resistant to DDT and other similar insecticides. DDT resistance is also conferred by up-regulation of genes expressing cytochrome P450 in some insect species, as greater quantities of some enzymes of this group accelerate metabolism of the toxin into inactive metabolites—in essence, these enzymes are natural antidotes to the poison.
In humans, however, it may affect health through genotoxicity or endocrine disruption (see Effects on human health below).
History.
DDT was first synthesized in 1874 by Othmar Zeidler under the supervision of Adolf von Baeyer. It was further described in 1929 in a dissertation by W. Bausch and in two subsequent publications in 1930. The insecticide properties of "multiple chlorinated aliphatic or fat-aromatic alcohols with at least one trichloromethane group" were described in a patent in 1934 by Wolfgang von Leuthold. DDT's insecticidal properties were not, however, discovered until 1939 by the Swiss scientist Paul Hermann Müller, who was awarded the 1948 Nobel Prize in Physiology and Medicine for his efforts.
Use in the 1940s and 1950s.
DDT is the best-known of several chlorine-containing pesticides used in the 1940s and 1950s. With pyrethrum in short supply, DDT was used extensively during World War II by the Allies to control the insect vectors of typhus – nearly eliminating the disease in many parts of Europe. In the South Pacific, it was sprayed aerially for malaria and dengue fever control with spectacular effects. While DDT's chemical and insecticidal properties were important factors in these victories, advances in application equipment coupled with a high degree of organization and sufficient manpower were also crucial to the success of these programs. In 1945, it was made available to farmers as an agricultural insecticide, and it played a minor role in the final elimination of malaria in Europe and North America. By the time DDT was introduced in the U.S., the disease had already been brought under control by a variety of other means. One CDC physician involved in the United States' DDT spraying campaign said of the effort that "we kicked a dying dog."
In 1955, the World Health Organization commenced a program to eradicate malaria worldwide, relying largely on DDT. The program was initially highly successful, eliminating the disease in "Taiwan, much of the Caribbean, the Balkans, parts of northern Africa, the northern region of Australia, and a large swath of the South Pacific" and dramatically reducing mortality in Sri Lanka and India. However widespread agricultural use led to resistant insect populations. In many areas, early victories partially or completely reversed and, in some cases, rates of transmission even increased. The program was successful in eliminating malaria only in areas with "high socio-economic status, well-organized healthcare systems, and relatively less intensive or seasonal malaria transmission".
DDT was less effective in tropical regions due to the continuous life cycle of mosquitoes and poor infrastructure. It was not applied at all in sub-Saharan Africa due to these perceived difficulties. Mortality rates in that area never declined to the same dramatic extent, and now constitute the bulk of malarial deaths worldwide, especially following the disease's resurgence as a result of resistance to drug treatments and the spread of the deadly malarial variant caused by "Plasmodium falciparum" . The goal of eradication was abandoned in 1969, and attention was focused on controlling and treating the disease. Spraying programs (especially using DDT) were curtailed due to concerns over safety and environmental effects, as well as problems in administrative, managerial and financial implementation, but mostly because mosquitoes were developing resistance to DDT. Efforts shifted from spraying to the use of bednets impregnated with insecticides and other interventions.
U.S. ban.
As early as the 1940s, scientists in the U.S. had begun expressing concern over possible hazards associated with DDT, and in the 1950s the government began tightening some of the regulations governing its use. However, these early events received little attention, and it was not until 1957, when the "New York Times" reported an unsuccessful struggle to restrict DDT use in Nassau County, New York, that the issue came to the attention of the popular naturalist-author, Rachel Carson. William Shawn, editor of "The New Yorker", urged her to write a piece on the subject, which developed into her famous book "Silent Spring", published in 1962. The book argued that pesticides, including DDT, were poisoning both wildlife and the environment and were also endangering human health.
"Silent Spring" was a best seller, and public reaction to it launched the modern environmental movement in the United States. The year after it appeared, President Kennedy ordered his Science Advisory Committee to investigate Carson's claims. The report the committee issued "add[ed] up to a fairly thorough-going vindication of Rachel Carson’s Silent Spring thesis," in the words of the journal "Science", and recommended a phaseout of "persistent toxic pesticides". DDT became a prime target of the growing anti-chemical and anti-pesticide movements, and in 1967 a group of scientists and lawyers founded the Environmental Defense Fund (EDF) with the specific goal of winning a ban on DDT. Victor Yannacone, Charles Wurster, Art Cooley and others associated with inception of EDF had all witnessed bird kills or declines in bird populations and suspected that DDT was the cause. In their campaign against the chemical, EDF petitioned the government for a ban and filed a series of lawsuits. Around this time, toxicologist David Peakall was measuring DDE levels in the eggs of peregrine falcons and California condors and finding that increased levels corresponded with thinner shells.
In response to an EDF suit, the U.S. District Court of Appeals in 1971 ordered the EPA to begin the de-registration procedure for DDT. After an initial six-month review process, William Ruckelshaus, the Agency's first Administrator rejected an immediate suspension of DDT's registration, citing studies from the EPA's internal staff stating that DDT was not an imminent danger to human health and wildlife. However, the findings of these staff members were criticized, as they were performed mostly by economic entomologists inherited from the United States Department of Agriculture, who many environmentalists felt were biased towards agribusiness and tended to minimize concerns about human health and wildlife. The decision not to ban thus created public controversy.
The EPA then held seven months of hearings in 1971–1972, with scientists giving evidence both for and against the use of DDT. In the summer of 1972, Ruckelshaus announced the cancellation of most uses of DDT – an exemption allowed for public health uses under some conditions. Immediately after the cancellation was announced, both EDF and the DDT manufacturers filed suit against the EPA, with the industry seeking to overturn the ban, and EDF seeking a comprehensive ban. The cases were consolidated, and in 1973 the U.S. Court of Appeals for the District of Columbia ruled that the EPA had acted properly in banning DDT.
The U.S. DDT ban took place amidst a growing public mistrust of industry, with the Surgeon General issuing a report on the negative effects of smoking tobacco in 1964, the Cuyahoga River catching fire in 1969, the fiasco surrounding the use of diethylstilbestrol (DES), and the well-publicized decline in the bald eagle population.
Some uses of DDT continued under the public health exemption. For example, in June 1979, the California Department of Health Services was permitted to use DDT to suppress flea vectors of bubonic plague. DDT also continued to be produced in the US for foreign markets until as late as 1985, when over 300 tons were exported.
Restrictions on usage.
In the 1970s and 1980s, agricultural use was banned in most developed countries, beginning with Hungary in 1968 then in Norway and Sweden in 1970, Germany and the United States in 1972, but not in the United Kingdom until 1984. By 1991 total bans on the use of DDT, including in disease control, were in place in at least 26 countries; for example Cuba in 1970, Singapore in 1984, Chile in 1985 and the Republic of Korea in 1986.
The Stockholm Convention, which took effect in 2004, outlawed several persistent organic pollutants, and restricted DDT use to vector control. The Convention has been ratified by more than 170 countries and is endorsed by most environmental groups. Recognizing that total elimination in many malaria-prone countries is currently unfeasible because there are few affordable or effective alternatives, the convention exempts public health use within World Health Organization (WHO) guidelines from the ban. Resolution 60.18 of the World Health Assembly commits the World Health Organization to the Stockholm Convention's aim of reducing and ultimately eliminating the use of DDT. Malaria Foundation International states, "The outcome of the treaty is arguably better than the status quo going into the negotiations. For the first time, there is now an insecticide which is restricted to vector control only, meaning that the selection of resistant mosquitoes will be slower than before."
Despite the worldwide ban, agricultural use continues in India, North Korea, and possibly elsewhere.
Today, about 3,000 to 4,000 tonnes of DDT are produced each year for disease vector control. DDT is applied to the inside walls of homes to kill or repel mosquitoes. This intervention, called indoor residual spraying (IRS), greatly reduces environmental damage. It also reduces the incidence of DDT resistance. For comparison, treating of cotton during a typical U.S. growing season requires the same amount of chemical as roughly 1,700 homes.
Environmental impact.
DDT is a persistent organic pollutant that is readily adsorbed to soils and sediments, which can act both as sinks and as long-term sources of exposure contributing to terrestrial organisms. Depending on conditions, its soil half life can range from 22 days to 30 years. Routes of loss and degradation include runoff, volatilization, photolysis and aerobic and anaerobic biodegradation. Due to hydrophobic properties, in aquatic ecosystems DDT and its metabolites are absorbed by aquatic organisms and adsorbed on suspended particles, leaving little DDT dissolved in the water itself. Its breakdown products and metabolites, DDE and DDD, are also highly persistent and have similar chemical and physical properties. DDT and its breakdown products are transported from warmer regions of the world to the Arctic by the phenomenon of global distillation, where they then accumulate in the region's food web.
Because of its lipophilic properties, DDT has a high potential to bioaccumulate, especially in predatory birds. DDT, DDE, and DDD magnify through the food chain, with apex predators such as raptor birds concentrating more chemicals than other animals in the same environment. They are very lipophilic and are stored mainly in body fat. DDT and DDE are very resistant to metabolism; in humans, their half-lives are 6 and up to 10 years, respectively. In the United States, these chemicals were detected in almost all human blood samples tested by the Centers for Disease Control in 2005, though their levels have sharply declined since most uses were banned in the US. Estimated dietary intake has also declined, although FDA food tests commonly detect it.
Marine macroalgae (seaweed) help reduce soil toxicity by up to 80% within six weeks.
Effects on wildlife and eggshell thinning.
DDT is toxic to a wide range of living organisms, including marine animals such as crayfish, daphnids, sea shrimp and many species of fish. DDT, through its metabolite DDE (dichlorodiphenyldichloroethylene), caused eggshell thinning and resulted in severe population declines in multiple North American and European bird of prey species. Eggshell thinning lowers the reproductive rate of certain bird species by causing egg breakage and embryo deaths. DDE related eggshell thinning is considered a major reason for the decline of the bald eagle, brown pelican, peregrine falcon, and osprey. However, different groups of birds vary greatly in their sensitivity to these chemicals. Birds of prey, waterfowl, and song birds are more susceptible to eggshell thinning than chickens and related species, and DDE appears to be more potent than DDT. Even in 2010, more than forty years after the U.S. ban, California condors which feed on sea lions at Big Sur which in turn feed in the Palos Verdes Shelf area of the Montrose Chemical Superfund site seemed to be having continued thin-shell problems. Scientists with the Ventana Wildlife Society and others are intensifying studies and remediations of the condors' problems.
The biological thinning mechanism is not entirely known, but there is strong evidence that p,p'-DDE inhibits calcium ATPase in the membrane of the shell gland and reduces the transport of calcium carbonate from blood into the eggshell gland. This results in a dose-dependent thickness reduction. There is also evidence that o,p'-DDT disrupts female reproductive tract development, impairing eggshell quality later. Multiple mechanisms may be at work, or different mechanisms may operate in different species. Some studies show that although DDE levels have fallen dramatically, eggshell thickness remains 10–12 percent thinner than before DDT was first used.
Effects on human health.
Potential mechanisms of action on humans are genotoxicity and endocrine disruption. DDT can be directly genotoxic, but may also induce enzymes to produce other genotoxic intermediates and DNA adducts. It is an endocrine disruptor. The DDT metabolite DDE acts as an antiandrogen, but not as an estrogen. p,p'-DDT, DDT's main component, has little or no androgenic or estrogenic activity. The minor component o,p'-DDT has weak estrogenic activity.
Acute toxicity.
DDT is classified as "moderately toxic" by the United States National Toxicology Program (NTP) and "moderately hazardous" by the World Health Organization (WHO), based on the rat oral of 113 mg/kg. DDT has on rare occasions been administered orally as a treatment for barbiturate poisoning.
Chronic toxicity.
Diabetes.
DDT and DDE have been linked to diabetes. A number of studies from the US, Canada, and Sweden have found that the prevalence of the disease in a population increases with serum DDT or DDE levels.
Developmental toxicity.
DDT and DDE, like other organochlorines, have been shown to have xenoestrogenic activity, meaning they are chemically similar enough to estrogens to trigger hormonal responses in animals. This endocrine disrupting activity has been observed in mice and rat toxicological studies, and available epidemiological evidence indicates that these effects may be occurring in humans as a result of DDT exposure. The US Environmental Protection Agency states that DDT exposure damages the reproductive system and reduces reproductive success. These effects may cause developmental and reproductive toxicity:
Other.
Occupational exposure in agriculture and malaria control has been linked to neurological problems (for example, Parkinson's disease) and asthma.
A 2014 study in "JAMA Neurology" reported that DDT levels were elevated 3.8 fold in Alzheimer's disease patients compared with healthy controls.
Carcinogenicity.
In 2002, the Centers for Disease Control reported that "Overall, in spite of some positive associations for some cancers within certain subgroups of people, there is no clear evidence that exposure to DDT/DDE causes cancer in humans." The NTP classifies it as "reasonably anticipated to be a carcinogen," the International Agency for Research on Cancer classifies it as a "possible" human carcinogen, and the EPA classifies DDT, DDE, and DDD as class B2 "probable" carcinogens. These evaluations are based mainly on the results of animal studies.
More recent evidence from epidemiological studies indicates that DDT causes cancers of the liver, pancreas and breast. There is mixed evidence that it contributes to leukemia, lymphoma and testicular cancer. Other epidemiological studies suggest that DDT/DDE does not cause multiple myeloma, or cancers of the prostate, endometrium, rectum, lung, bladder, or stomach.
Breast cancer.
The question of whether DDT or DDE are risk factors in breast cancer has been repeatedly studied. While individual studies conflict, the most recent reviews of all the evidence conclude that pre-puberty exposure increases the risk of subsequent breast cancer. Until recently, almost all studies measured DDT or DDE blood levels at the time of breast cancer diagnosis or after. This study design has been criticized, since the levels at diagnosis do not necessarily correspond to levels when the cancer started. Taken as a whole such studies "do not support the hypothesis that exposure to DDT is an important risk factor for breast cancer." The studies of this design have been extensively reviewed.
In contrast, a study published in 2007 strongly associated early exposure (the "p,p'-" isomer) and breast cancer later in life. Unlike previous studies, this prospective cohort study collected blood samples from young mothers in the 1960s while DDT was still in use, and their breast cancer status was then monitored over the years. In addition to suggesting that the "p,p'-" isomer is the more significant risk factor, the study also suggests that the timing of exposure is critical. For the subset of women born more than 14 years before agricultural use, there was no association between DDT and breast cancer. However, for younger women – exposed earlier in life – the third who were exposed most to "p,p'-"DDT had a fivefold increase in breast cancer incidence over the least exposed third, after correcting for the protective effect of "o,p'-"DDT. These results are supported by animal studies.
Use against malaria.
Malaria remains a major public health challenge in many countries. 2008 WHO estimates were 243 million cases, and 863,000 deaths. About 89% of these deaths occur in Africa, and mostly to children under the age of 5. DDT is one of many tools that public health officials use to fight the disease. Its use in this context has been called everything from a "miracle weapon [that is] like Kryptonite to the mosquitoes," to "toxic colonialism."
Before DDT, eliminating mosquito breeding grounds by drainage or poisoning with Paris green or pyrethrum was sometimes successful in fighting malaria. In parts of the world with rising living standards, the elimination of malaria was often a collateral benefit of the introduction of window screens and improved sanitation. Today, a variety of usually simultaneous interventions is the norm. These include antimalarial drugs to prevent or treat infection; improvements in public health infrastructure to quickly diagnose, sequester, and treat infected individuals; bednets and other methods intended to keep mosquitoes from biting humans; and vector control strategies such as larvaciding with insecticides, ecological controls such as draining mosquito breeding grounds or introducing fish to eat larvae, and indoor residual spraying (IRS) with insecticides, possibly including DDT. IRS involves the treatment of all interior walls and ceilings with insecticides, and is particularly effective against mosquitoes, since many species rest on an indoor wall before or after feeding. DDT is one of 12 WHO–approved IRS insecticides. How much of a role DDT should play in this mix of strategies is still controversial.
WHO's anti-malaria campaign of the 1950s and 1960s relied heavily on DDT and the results were promising, though temporary. Experts tie the resurgence of malaria to multiple factors, including poor leadership, management and funding of malaria control programs; poverty; civil unrest; and increased irrigation. The evolution of resistance to first-generation drugs (e.g. chloroquine) and to insecticides exacerbated the situation. Resistance was largely fueled by often unrestricted agricultural use. Resistance and the harm both to humans and the environment led many governments to restrict or curtail the use of DDT in vector control as well as agriculture. In 2006 the WHO reversed a longstanding policy against DDT by recommending that it be used as an indoor pesticide in regions where malaria is a major problem.
Once the mainstay of anti-malaria campaigns, as of 2008 only 12 countries used DDT, including India and some southern African states, though the number is expected to rise.
Initial effectiveness of DDT against malaria.
When it was first introduced in World War II, DDT was very effective in reducing malaria morbidity and mortality. The WHO's anti-malaria campaign, which consisted mostly of spraying DDT, was initially very successful as well. For example, in Sri Lanka, the program reduced cases from about one million per year before spraying to just 18 in 1963 and 29 in 1964. Thereafter the program was halted to save money and malaria rebounded to 600,000 cases in 1968 and the first quarter of 1969. The country resumed DDT vector control but the mosquitoes had evolved resistance in the interim, presumably because of continued agricultural use. The program switched to malathion, which though more expensive, proved effective.
Today, DDT remains on the WHO's list of insecticides recommended for IRS. Since the appointment of Arata Kochi as head of its anti-malaria division, WHO's policy has shifted from recommending IRS only in areas of seasonal or episodic transmission of malaria, to also advocating it in areas of continuous, intense transmission. The WHO has reaffirmed its commitment to eventually phasing out DDT, aiming "to achieve a 30% cut in the application of DDT world-wide by 2014 and its total phase-out by the early 2020s if not sooner" while simultaneously combating malaria. The WHO plans to implement alternatives to DDT to achieve this goal.
South Africa is one country that continues to use DDT under WHO guidelines. In 1996, the country switched to alternative insecticides and malaria incidence increased dramatically. Returning to DDT and introducing new drugs brought malaria back under control. According to DDT advocate Donald Roberts, malaria cases increased in South America after countries in that continent stopped using DDT. Research data shows a significantly strong negative relationship between DDT residual house sprayings and malaria rates. In a research from 1993 to 1995, Ecuador increased its use of DDT and resulted in a 61% reduction in malaria rates, while each of the other countries that gradually decreased its DDT use had large increase in malaria rates.
Mosquito resistance.
Resistance has greatly reduced DDT's effectiveness. WHO guidelines require that absence of resistance must be confirmed before using the chemical. Resistance is largely due to agricultural use, in much greater quantities than required for disease prevention. According to one study that attempted to quantify the lives saved by banning agricultural use and thereby slowing the spread of resistance, "it can be estimated that at current rates each kilo of insecticide added to the environment will generate 105 new cases of malaria."
Resistance was noted early in spray campaigns. Paul Russell, a former head of the Allied Anti-Malaria campaign, observed in 1956 that "resistance has appeared after six or seven years." DDT has lost much of its effectiveness in Sri Lanka, Pakistan, Turkey and Central America, and it has largely been replaced by organophosphate or carbamate insecticides, "e.g." malathion or bendiocarb.
In many parts of India, DDT has also largely lost its effectiveness. Agricultural uses were banned in 1989 and its anti-malarial use has been declining. Urban use has halted completely. Nevertheless, DDT is still manufactured and used, and one study had concluded that "DDT is still a viable insecticide in indoor residual spraying owing to its effectivity in well supervised spray operation and high excito-repellency factor."
Studies of malaria-vector mosquitoes in KwaZulu-Natal Province, South Africa found susceptibility to 4% DDT (the WHO susceptibility standard), in 63% of the samples, compared to the average of 86.5% in the same species caught in the open. The authors concluded that "Finding DDT resistance in the vector "An. arabiensis", close to the area where we previously reported pyrethroid-resistance in the vector "An. funestus" Giles, indicates an urgent need to develop a strategy of insecticide resistance management for the malaria control programmes of southern Africa."
DDT can still be effective against resistant mosquitoes, and the avoidance of DDT-sprayed walls by mosquitoes is an additional benefit of the chemical. For example, a 2007 study reported that resistant mosquitoes avoided treated huts. The researchers argued that DDT was the best pesticide for use in IRS (even though it did not afford the most protection from mosquitoes out of the three test chemicals) because the others pesticides worked primarily by killing or irritating mosquitoes – encouraging the development of resistance to these agents. Others argue that the avoidance behavior slows the eradication of the disease. Unlike other insecticides such as pyrethroids, DDT requires long exposure to accumulate a lethal dose; however its irritant property shortens contact periods. "For these reasons, when comparisons have been made, better malaria control has generally been achieved with pyrethroids than with DDT." In India, with its outdoor sleeping habits and frequent night duties, "the excito-repellent effect of DDT, often reported useful in other countries, actually promotes outdoor transmission." Genomic studies in the model genetic organism "Drosophila melanogaster" have revealed that high level DDT resistance is polygenic, involving multiple resistance mechanisms.
Residents' concerns.
For IRS to be effective, at least 80% of homes and barns in an area must be sprayed. Lower coverage rates can jeopardize program effectiveness. Many residents resist DDT spraying, objecting to the lingering smell, stains on walls, and the potential exacerbation of problems with other insect pests. Pyrethroid insecticides (e.g. deltamethrin and lambda-cyhalothrin) can overcome some of these issues, increasing participation.
Human exposure.
People living in areas where DDT is used for IRS have high levels of the chemical and its breakdown products in their bodies. Compared to contemporaries living where DDT is not used, South Africans living in sprayed homes have levels that are several orders of magnitude greater. Breast milk in regions where DDT is used against malaria greatly exceeds the allowable standards for breast-feeding infants. These levels are associated with neurological abnormalities in babies.
Most studies of DDT's human health effects have been conducted in developed countries where DDT is not used and exposure is relatively low. Many experts urge that alternatives be used instead of IRS. Epidemiologist Brenda Eskenazi argues, "We know DDT can save lives by repelling and killing disease-spreading mosquitoes. But evidence suggests that people living in areas where DDT is used are exposed to very high levels of the pesticide. The only published studies on health effects conducted in these populations have shown profound effects on male fertility. Clearly, more research is needed on the health of populations where indoor residual spraying is occurring, but in the meantime, DDT should really be the last resort against malaria rather than the first line of defense."
Illegal diversion to agriculture is also a concern as it is almost impossible to prevent and its subsequent use on crops is uncontrolled. For example, DDT use is widespread in Indian agriculture, particularly mango production, and is reportedly used by librarians to protect books. Other examples include Ethiopia, where DDT intended for malaria control is reportedly being used in coffee production, and Ghana where it is used for fishing." The residues in crops at levels unacceptable for export have been an important factor in recent bans in several tropical countries. Adding to this problem is a lack of skilled personnel and supervision.
Criticism of restrictions on DDT use.
Critics claim that restricting DDT in vector control have caused unnecessary deaths due to malaria. Estimates range from hundreds of thousands, to millions. Robert Gwadz of the US National Institutes of Health said in 2007, "The ban on DDT may have killed 20 million children." In his novel "State of Fear", author Michael Crichton wrote "Banning DDT killed more people than Hitler." These arguments have been dismissed as "outrageous" by former WHO scientist Socrates Litsios. May Berenbaum, University of Illinois entomologist, says, "to blame environmentalists who oppose DDT for more deaths than Hitler is worse than irresponsible." Investigative journalist Adam Sarvana and others characterize this notion as a "myth" promoted principally by Roger Bate of the pro-DDT advocacy group Africa Fighting Malaria (AFM).
Criticisms of a DDT "ban" often specifically reference the 1972 US ban (with the erroneous implication that this constituted a worldwide ban and prohibited use of DDT in vector control). Reference is often made to Rachel Carson's "Silent Spring," even though she never pushed for a ban on DDT specifically. John Quiggin and Tim Lambert wrote, "the most striking feature of the claim against Carson is the ease with which it can be refuted."
It has also been alleged that donor governments and agencies have refused to fund DDT spraying, or made aid contingent upon not using DDT. According to a report in the "British Medical Journal", use of DDT in Mozambique "was stopped several decades ago, because 80% of the country's health budget came from donor funds, and donors refused to allow the use of DDT." Roger Bate asserts, "many countries have been coming under pressure from international health and environment agencies to give up DDT or face losing aid grants: Belize and Bolivia are on record admitting they gave in to pressure on this issue from [USAID]."
The United States Agency for International Development (USAID) has been the focus of much criticism. While the agency is currently funding the use of DDT in some African countries, in the past it did not. When John Stossel accused USAID of not funding DDT because it wasn't "politically correct," Anne Peterson, the agency's assistant administrator for global health, replied that "I believe that the strategies we are using are as effective as spraying with DDT ... So, politically correct or not, I am very confident that what we are doing is the right strategy." USAID's Kent R. Hill states that the agency has been misrepresented: "USAID strongly supports spraying as a preventative measure for malaria and will support the use of DDT when it is scientifically sound and warranted." The Agency's website states that "USAID has never had a 'policy' as such either 'for' or 'against' DDT for IRS. The real change in the past two years [2006/07] has been a new interest and emphasis on the use of IRS in general – with DDT or any other insecticide – as an effective malaria prevention strategy in tropical Africa." The website further explains that in many cases alternative malaria control measures were judged to be more cost-effective that DDT spraying, and so were funded instead.
Alternatives.
Other insecticides.
Advocates of increased use of DDT in IRS claim that alternative insecticides are more expensive, more toxic, or not as effective. As discussed above, susceptibility of mosquitoes to DDT varies geographically. The same is true for alternative insecticides, so its relative effectiveness varies. Toxicity and cost-effectiveness comparisons lack data. Relative insecticide costs vary by location and ease of access, the habits of the local mosquitoes, the degrees of resistance exhibited by the mosquitoes, and the habits and compliance of the population, among other factors. The choice of insecticide has little impact on the total cost of a round of spraying, since product costs are only a fraction of campaign costs. IRS coverage needs to be maintained throughout the malaria season, making DDT's relatively long life an important cost savings.
Organophosphate and carbamate insecticides, "e.g." malathion and bendiocarb, respectively, are more expensive than DDT per kilogram and are applied at roughly the same dosage. Pyrethroids such as deltamethrin are also more expensive than DDT, but are applied more sparingly (0.02–0.3 g/m2 vs 1–2 g/m2), so the net cost per house is about the same over 6 months.
Non-chemical vector control.
Before DDT, malaria was successfully eradicated or curtailed in several tropical areas by removing or poisoning mosquito breeding grounds and larva habitats, for example by filling or applying oil to standing water. These methods have seen little application in Africa for more than half a century.
The relative effectiveness of IRS (with DDT or alternative insecticides) versus other malaria control techniques (e.g. bednets or prompt access to anti-malarial drugs) varies greatly and is highly dependent on local conditions.
A WHO study released in January 2008 found that mass distribution of insecticide-treated mosquito nets and artemisinin–based drugs cut malaria deaths in half in Rwanda and Ethiopia, countries with high malaria burdens. IRS with DDT did not play an important role in mortality reduction in these countries.
Vietnam has enjoyed declining malaria cases and a 97% mortality reduction after switching in 1991 from a poorly funded DDT-based campaign to a program based on prompt treatment, bednets, and pyrethroid group insecticides.
In Mexico, effective and affordable chemical and non-chemical strategies against malaria have been so successful that the Mexican DDT manufacturing plant ceased production due to lack of demand.
While the increased numbers of malaria victims since DDT usage collapsed document its value, many other factors contributed to the rise in cases.
A review of fourteen studies on the subject in sub-Saharan Africa, covering insecticide-treated nets, residual spraying, chemoprophylaxis for children, chemoprophylaxis or intermittent treatment for pregnant women, a hypothetical vaccine, and changing front–line drug treatment, found decision making limited by the gross lack of information on the costs and effects of many interventions, the very small number of cost-effectiveness analyses available, the lack of evidence on the costs and effects of packages of measures, and the problems in generalizing or comparing studies that relate to specific settings and use different methodologies and outcome measures. The two cost-effectiveness estimates of DDT residual spraying examined were not found to provide an accurate estimate of the cost-effectiveness of DDT spraying; furthermore, the resulting estimates may not be good predictors of cost-effectiveness in current programs.
However, a study in Thailand found the cost per malaria case prevented of DDT spraying ($1.87 US) to be 21% greater than the cost per case prevented of lambda-cyhalothrin–treated nets ($1.54 US), at very least casting some doubt on the unexamined assumption that DDT was the most cost-effective measure to use in all cases. The director of Mexico's malaria control program finds similar results, declaring that it is 25% cheaper for Mexico to spray a house with synthetic pyrethroids than with DDT. However, another study in South Africa found generally lower costs for DDT spraying than for impregnated nets.
A more comprehensive approach to measuring cost-effectiveness or efficacy of malarial control would not only measure the cost in dollars of the project, as well as the number of people saved, but would also consider ecological damage and negative aspects of insecticide use on human health. One preliminary study regarding the effect of DDT found that it is likely the detriment to human health approaches or exceeds the beneficial reductions in malarial cases, except perhaps in malarial epidemic situations. It is similar to the earlier mentioned study regarding estimated theoretical infant mortality caused by DDT and subject to the criticism also mentioned earlier.
A study in the Solomon Islands found that "although impregnated bed nets cannot entirely replace DDT spraying without substantial increase in incidence, their use permits reduced DDT spraying."
A comparison of four successful programs against malaria in Brazil, India, Eritrea, and Vietnam does not endorse any single strategy but instead states, "Common success factors included conducive country conditions, a targeted technical approach using a package of effective tools, data-driven decision-making, active leadership at all levels of government, involvement of communities, decentralized implementation and control of finances, skilled technical and managerial capacity at national and sub-national levels, hands-on technical and programmatic support from partner agencies, and sufficient and flexible financing."
DDT resistant mosquitoes have generally proved susceptible to pyrethroids. Thus far, pyrethroid resistance in "Anopheles" has not been a major problem.

</doc>
<doc id="8495" url="http://en.wikipedia.org/wiki?curid=8495" title="Data set">
Data set

A data set (or dataset) is a collection of data.
Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. 
The term data set may also be used more loosely, to refer to the data in a collection of closely related tables, corresponding to a particular experiment or event.
History.
Historically, the term originated in the mainframe field, where it had a well-defined meaning, very close to contemporary "computer file".
Properties.
Several characteristics define a data set's structure and properties. These include the number and types of the attributes or variables, and various statistical measures applicable to them, such as standard deviation and kurtosis.
The values may be numbers, such as real numbers or integers, for example representing a person's height in centimeters, but may also be nominal data (i.e., not consisting of numerical values), for example representing a person's ethnicity. More generally, values may be of any of the kinds described as a level of measurement. For each variable, the values are normally all of the same kind. However, there may also be "missing values", which must be indicated in some way.
In statistics, datasets usually come from actual observations obtained by sampling a statistical population, and each row corresponds to the observations on one element of that population. Datasets may further be generated by algorithms for the purpose of testing certain kinds of software. Some modern statistical analysis software such as SPSS still present their data in the classical dataset fashion
Classic datasets.
Several classic datasets have been used extensively in the statistical literature:

</doc>
<doc id="8496" url="http://en.wikipedia.org/wiki?curid=8496" title="DMA">
DMA

DMA can refer to:

</doc>
<doc id="8498" url="http://en.wikipedia.org/wiki?curid=8498" title="Diagnostic and Statistical Manual of Mental Disorders">
Diagnostic and Statistical Manual of Mental Disorders

The Diagnostic and Statistical Manual of Mental Disorders (DSM), published by the American Psychiatric Association, offers a common language and standard criteria for the classification of mental disorders. It is used, or relied upon, by clinicians, researchers, psychiatric drug regulation agencies, health insurance companies, pharmaceutical companies, the legal system, and policy makers together with alternatives such as the International Statistical Classification of Diseases and Related Health Problems (ICD), produced by the World Health Organization (WHO). The DSM is now in its fifth edition, DSM-5, published on May 18, 2013.
The DSM evolved from systems for collecting census and psychiatric hospital statistics, and from a United States Army manual. Revisions since its first publication in 1952 have incrementally added to the total number of mental disorders, although also removing those no longer considered to be mental disorders.
The International Statistical Classification of Diseases and Related Health Problems (ICD), produced by the World Health Organization (WHO), is the other commonly used manual for mental disorders. It is distinguished from the DSM in that it covers health as a whole. While the DSM is the official diagnostic system for mental disorders in the US, the ICD is used more widely in Europe and other parts of the world. The DSM-IV-TR (4th. ed.) contains, in Appendix G, an "ICD-9-CM Codes for Selected General Medical Conditions and Medication-Induced Disorders" that allows for comparisons between the DSM and the ICD manuals, which may not systematically match because revisions are not simultaneously coordinated.
While the DSM has been praised for standardizing psychiatric diagnostic categories and criteria, it has also generated controversy and criticism. Critics, including the National Institute of Mental Health, argue that the DSM represents an unscientific and subjective system. There are ongoing issues concerning the validity and reliability of the diagnostic categories; the reliance on superficial symptoms; the use of artificial dividing lines between categories and from ‘normality’; possible cultural bias; medicalization of human distress. The publication of the DSM, with tightly guarded copyrights, now makes APA over $5 million a year, historically totaling over $100 million.
Uses and definition.
Many mental health professionals use the manual to determine and help communicate a patient’s diagnosis after an evaluation; hospitals, clinics, and insurance companies in the US also generally require a DSM diagnosis for all patients treated. The DSM can be used clinically in this way, and also to categorize patients using diagnostic criteria for research purposes. Studies done on specific disorders often recruit patients whose symptoms match the criteria listed in the DSM for that disorder. An international survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found the former was more often used for clinical diagnosis while the latter was more valued for research.
DSM-5, and all previous editions, are registered trademarks owned by the American Psychiatric Association (APA).
The current version of the DSM characterizes a mental disorder as “a clinically significant behavioral or psychological syndrome or pattern that occurs in an individual [which] is associated with present distress…or disability…or with a significant increased risk of suffering.” It also notes that “…no definition adequately specifies precise boundaries for the concept of ‘mental disorder’…different situations call for different definitions”. It states that “there is no assumption that each category of mental disorder is a completely discrete entity with absolute boundaries dividing it from other mental disorders or from no mental disorder” (APA, 1994 and 2000). There are attempts to adjust the wording for the upcoming DSM-V.
History.
The initial impetus for developing a classification of mental disorders in the United States was the need to collect statistical information. The first official attempt was the 1840 census, which used a single category, “idiocy/insanity”. Three years later, the American Statistical Association made an official protest to the U.S. House of Representatives stating that “the most glaring and remarkable errors are found in the statements respecting nosology, prevalence of insanity, blindness, deafness, and dumbness, among the people of this nation” and pointing out that in many towns African-Americans were all marked as insane, and the statistics were essentially useless.
The Association of Medical Superintendents of American Institutions for the Insane was formed in 1844, changing its name in 1892 to the American Medico-Psychological Association, and in 1921 to the present American Psychiatric Association (APA).
Edward Jarvis and later Francis Amasa Walker helped expand the census, from 2 volumes in 1870 to 25 volumes in 1880. Frederick H. Wines was appointed to write a 582-page volume called “Report on the Defective, Dependent, and Delinquent Classes of the Population of the United States, As Returned at the Tenth Census (June 1, 1880)” (published 1888). Wines used seven categories of mental illness: dementia, dipsomania (uncontrollable craving for alcohol), epilepsy, mania, melancholia, monomania and paresis. These categories were also adopted by the Association.
In 1917, together with the National Commission on Mental Hygiene (now Mental Health America), the APA developed a new guide for mental hospitals called the “Statistical Manual for the Use of Institutions for the Insane”. This included 22 diagnoses and would be revised several times by the APA over the years. Along with the New York Academy of Medicine, the APA also provided the psychiatric nomenclature subsection of the US general medical guide, the "Standard Classified Nomenclature of Disease", referred to as the “Standard”.
DSM-I (1952).
World War II saw the large-scale involvement of US psychiatrists in the selection, processing, assessment, and treatment of soldiers. This moved the focus away from mental institutions and traditional clinical perspectives. A committee headed by psychiatrist Brigadier General William C. Menninger developed a new classification scheme called Medical 203, that was issued in 1943 as a War Department Technical Bulletin under the auspices of the Office of the Surgeon General. The foreword to the DSM-I states the US Navy had itself made some minor revisions but “the Army established a much more sweeping revision, abandoning the basic outline of the Standard and attempting to express present day concepts of mental disturbance. This nomenclature eventually was adopted by all Armed Forces”, and “assorted modifications of the Armed Forces nomenclature [were] introduced into many clinics and hospitals by psychiatrists returning from military duty.” The Veterans Administration also adopted a slightly modified version of Medical 203.
In 1949, the World Health Organization published the sixth revision of the International Statistical Classification of Diseases (ICD), which included a section on mental disorders for the first time. The foreword to DSM-1 states this “categorized mental disorders in rubrics similar to those of the Armed Forces nomenclature.” An APA Committee on Nomenclature and Statistics was empowered to develop a version specifically for use in the United States, to standardize the diverse and confused usage of different documents. In 1950, the APA committee undertook a review and consultation. It circulated an adaptation of Medical 203, the VA system, and the Standard’s Nomenclature to approximately 10% of APA members. 46% replied, of which 93% approved, and after some further revisions (resulting in its being called DSM-I), the "Diagnostic and Statistical Manual of Mental Disorders" was approved in 1951 and published in 1952. The structure and conceptual framework were the same as in Medical 203, and many passages of text were identical. The manual was 130 pages long and listed 106 mental disorders. These included several categories of “personality disturbance”, generally distinguished from “neurosis” (nervousness, egodystonic).
In 1952, the APA listed homosexuality in the DSM as a sociopathic personality disturbance. "", a large-scale 1962 study of homosexuality, was used to justify inclusion of the disorder as a supposed pathological hidden fear of the opposite sex caused by traumatic parent–child relationships. This view was widely influential in the medical profession. In 1956, however, the psychologist Evelyn Hooker performed a study that compared the happiness and well-adjusted nature of self-identified homosexual men with heterosexual men and found no difference. Her study stunned the medical community and made her a hero to many gay men and lesbians, but homosexuality remained in the DSM until May 1974.
DSM-II (1968).
In the 1960s, there were many challenges to the concept of mental illness itself. These challenges came from psychiatrists like Thomas Szasz, who argued that mental illness was a myth used to disguise moral conflicts; from sociologists such as Erving Goffman, who said mental illness was merely another example of how society labels and controls non-conformists; from behavioural psychologists who challenged psychiatry’s fundamental reliance on unobservable phenomena; and from gay rights activists who criticised the APA’s listing of homosexuality as a mental disorder. A study published in "Science" by Rosenhan received much publicity and was viewed as an attack on the efficacy of psychiatric diagnosis.
Although the APA was closely involved in the next significant revision of the mental disorder section of the ICD (version 8 in 1968), it decided to go ahead with a revision of the DSM. It was published in 1968, listed 182 disorders, and was 134 pages long. It was quite similar to the DSM-I. The term “reaction” was dropped, but the term “neurosis” was retained. Both the DSM-I and the DSM-II reflected the predominant psychodynamic psychiatry, although they also included biological perspectives and concepts from Kraepelin’s system of classification. Symptoms were not specified in detail for specific disorders. Many were seen as reflections of broad underlying conflicts or maladaptive reactions to life problems, rooted in a distinction between neurosis and psychosis (roughly, anxiety/depression broadly in touch with reality, or hallucinations/delusions appearing disconnected from reality). Sociological and biological knowledge was incorporated, in a model that did not emphasize a clear boundary between normality and abnormality. The idea that personality disorders did not involve emotional distress was discarded.
An influential 1974 paper by Robert Spitzer and Joseph L. Fleiss demonstrated that the second edition of the DSM (DSM-II) was an unreliable diagnostic tool. They found that different practitioners using the DSM-II were rarely in agreement when diagnosing patients with similar problems. In reviewing previous studies of 18 major diagnostic categories, Fleiss and Spitzer concluded that “there are no diagnostic categories for which reliability is uniformly high. Reliability appears to be only satisfactory for three categories: mental deficiency, organic brain syndrome (but not its subtypes), and alcoholism. The level of reliability is no better than fair for psychosis and schizophrenia and is poor for the remaining categories”.
Seventh printing of the DSM-II, 1974.
As described by Ronald Bayer, a psychiatrist and gay rights activist, specific protests by gay rights activists against the APA began in 1970, when the organization held its convention in San Francisco. The activists disrupted the conference by interrupting speakers and shouting down and ridiculing psychiatrists who viewed homosexuality as a mental disorder. In 1971, gay rights activist Frank Kameny worked with the Gay Liberation Front collective to demonstrate against the APA’s convention. At the 1971 conference, Kameny grabbed the microphone and yelled, “Psychiatry is the enemy incarnate. Psychiatry has waged a relentless war of extermination against us. You may take this as a declaration of war against you.”
This activism occurred in the context of a broader anti-psychiatry movement that had come to the fore in the 1960s and was challenging the legitimacy of psychiatric diagnosis. Anti-psychiatry activists protested at the same APA conventions, with some shared slogans and intellectual foundations.
Presented with data from researchers such as Alfred Kinsey and Evelyn Hooker, the seventh printing of the DSM-II, in 1974, no longer listed homosexuality as a category of disorder. After a vote by the APA trustees in 1973, and confirmed by the wider APA membership in 1974, the diagnosis was replaced with the category of “sexual orientation disturbance”.
DSM-III (1980).
In 1974, the decision to create a new revision of the DSM was made, and Robert Spitzer was selected as chairman of the task force. The initial impetus was to make the DSM nomenclature consistent with the International Statistical Classification of Diseases and Related Health Problems (ICD), published by the World Health Organization. The revision took on a far wider mandate under the influence and control of Spitzer and his chosen committee members. One goal was to improve the uniformity and validity of psychiatric diagnosis in the wake of a number of critiques, including the famous Rosenhan experiment. There was also a need to standardize diagnostic practices within the US and with other countries after research showed that psychiatric diagnoses differed markedly between Europe and the USA. The establishment of these criteria was an attempt to facilitate the pharmaceutical regulatory process.
The criteria adopted for many of the mental disorders were taken from the Research Diagnostic Criteria (RDC) and Feighner Criteria, which had just been developed by a group of research-orientated psychiatrists based primarily at Washington University in St. Louis and the New York State Psychiatric Institute. Other criteria, and potential new categories of disorder, were established by consensus during meetings of the committee, as chaired by Spitzer. A key aim was to base categorization on colloquial English descriptive language (which would be easier to use by federal administrative offices), rather than assumptions of etiology, although its categorical approach assumed each particular pattern of symptoms in a category reflected a particular underlying pathology (an approach described as “neo-Kraepelinian”). The psychodynamic or physiologic view was abandoned, in favor of a regulatory or legislative model. A new “multiaxial” system attempted to yield a picture more amenable to a statistical population census, rather than just a simple diagnosis. Spitzer argued that “mental disorders are a subset of medical disorders” but the task force decided on the DSM statement: “Each of the mental disorders is conceptualized as a clinically significant behavioral or psychological syndrome.” The personality disorders were placed on axis II along with mental retardation.
The first draft of the DSM-III was prepared within a year. Many new categories of disorder were introduced, while some were deleted or changed. A number of the unpublished documents discussing and justifying the changes have recently come to light. Field trials sponsored by the U.S. National Institute of Mental Health (NIMH) were conducted between 1977 and 1979 to test the reliability of the new diagnoses. A controversy emerged regarding deletion of the concept of neurosis, a mainstream of psychoanalytic theory and therapy but seen as vague and unscientific by the DSM task force. Faced with enormous political opposition, the DSM-III was in serious danger of not being approved by the APA Board of Trustees unless “neurosis” was included in some capacity; a political compromise reinserted the term in parentheses after the word “disorder” in some cases. Additionally, the diagnosis of ego-dystonic homosexuality replaced the DSM-II category of “sexual orientation disturbance”.
Finally published in 1980, the DSM-III was 494 pages and listed 265 diagnostic categories. It rapidly came into widespread international use and has been termed a revolution or transformation in psychiatry. However, Robert Spitzer later criticized his own work on it in an interview with Adam Curtis, saying it led to the medicalization of 20-30 percent of the population who may not have had any serious mental problems.
When DSM-III was published, the developers made extensive claims about the reliability of the radically new diagnostic system they had devised, which relied on data from special field trials. However, according to a 1994 article by Stuart A. Kirk:
Twenty years after the reliability problem became the central focus of DSM-III, there is still not a single multi-site study showing that DSM (any version) is routinely used with high reliably by regular mental health clinicians. Nor is there any credible evidence that any version of the manual has greatly increased its reliability beyond the previous version. There are important methodological problems that limit the generalisability of most reliability studies. Each reliability study is constrained by the training and supervision of the interviewers, their motivation and commitment to diagnostic accuracy, their prior skill, the homogeneity of the clinical setting in regard to patient mix and base rates, and the methodological rigor achieved by the investigator…
DSM-III-R (1987).
In 1987, the DSM-III-R was published as a revision of the DSM-III, under the direction of Spitzer. Categories were renamed and reorganized, and significant changes in criteria were made. Six categories were deleted while others were added. Controversial diagnoses, such as pre-menstrual dysphoric disorder and masochistic personality disorder, were considered and discarded. “Sexual orientation disturbance” was also removed and was largely subsumed under “sexual disorder not otherwise specified”, which can include “persistent and marked distress about one’s sexual orientation.” Altogether, the DSM-III-R contained 292 diagnoses and was 567 pages long. Further efforts were made for the diagnoses to be purely descriptive, although the introductory text stated that for at least some disorders, “particularly the Personality Disorders, the criteria require much more inference on the part of the observer” (p. xxiii).
DSM-IV (1994).
In 1994, DSM-IV was published, listing 297 disorders in 886 pages. The task force was chaired by Allen Frances. A steering committee of 27 people was introduced, including four psychologists. The steering committee created 13 work groups of 5–16 members. Each work group had approximately 20 advisers. The work groups conducted a three-step process: first, each group conducted an extensive literature review of their diagnoses; then, they requested data from researchers, conducting analyses to determine which criteria required change, with instructions to be conservative; finally, they conducted multicenter field trials relating diagnoses to clinical practice. A major change from previous versions was the inclusion of a clinical significance criterion to almost half of all the categories, which required that symptoms cause “clinically significant distress or impairment in social, occupational, or other important areas of functioning”. Some personality disorder diagnoses were deleted or moved to the appendix.
DSM-IV-TR (2000).
A “text revision” of the DSM-IV, known as the DSM-IV-TR, was published in 2000. The diagnostic categories and the vast majority of the specific criteria for diagnosis were unchanged. The text sections giving extra information on each diagnosis were updated, as were some of the diagnostic codes to maintain consistency with the ICD. The DSM-IV-TR was organized into a five-part axial system. The first axis incorporated clinical disorders. The second axis covered personality disorders and intellectual disabilities. The remaining axes covered medical, psychosocial, environmental, and childhood factors functionally necessary to provide diagnostic criteria for health care assessments.
DSM-5 (2013).
The fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM), the DSM-V, was approved by the Board of Trustees of the American Psychiatric Association (APA) on December 1, 2012. Published on May 18, 2013, the DSM-5 contains extensively revised diagnoses and, in some cases, broadens diagnostic definitions while narrowing definitions in other cases. The DSM-5 is the first major edition of the manual in twenty years, and the Roman numerals numbering system has been discontinued to allow for greater clarity in regard to revision numbers. A significant change in the fifth edition is the proposed deletion of the subtypes of schizophrenia. During the revision process, the APA website periodically listed several sections of the DSM-5 for review and discussion.
DSM-IV-TR.
Categorization.
The DSM-IV is a categorical classification system. The categories are prototypes, and a patient with a close approximation to the prototype is said to have that disorder. DSM-IV states, “there is no assumption each category of mental disorder is a completely discrete entity with absolute boundaries” but isolated, low-grade and noncriterion (unlisted for a given disorder) symptoms are not given importance. Qualifiers are sometimes used, for example mild, moderate or severe forms of a disorder. For nearly half the disorders, symptoms must be sufficient to cause “clinically significant distress or impairment in social, occupational, or other important areas of functioning,” although DSM-IV-TR removed the distress criterion from tic disorders and several of the paraphilias due to their egosyntonic nature. Each category of disorder has a numeric code taken from the ICD coding system, used for health service (including insurance) administrative purposes.
Multi-axial system.
The DSM-IV organizes each psychiatric diagnosis into five dimensions (axes) relating to different aspects of disorder or disability:
Common Axis I disorders include depression, anxiety disorders, bipolar disorder, ADHD, autism spectrum disorders, anorexia nervosa, bulimia nervosa, and schizophrenia.
Common Axis II disorders include personality disorders: paranoid personality disorder, schizoid personality disorder, schizotypal personality disorder, borderline personality disorder, antisocial personality disorder, narcissistic personality disorder, histrionic personality disorder, avoidant personality disorder, dependent personality disorder, obsessive-compulsive personality disorder; and intellectual disabilities.
Common Axis III disorders include brain injuries and other medical/physical disorders which may aggravate existing diseases or present symptoms similar to other disorders."
Cautions.
The DSM-IV-TR states, because it is produced for the completion of federal legislative mandates, its use by people without clinical training can lead to inappropriate application of its contents. Appropriate use of the diagnostic criteria is said to require extensive clinical training, and its contents “cannot simply be applied in a cookbook fashion”. The APA notes diagnostic labels are primarily for use as a “convenient shorthand” among professionals. The DSM advises laypersons should consult the DSM only to obtain information, not to make diagnoses, and people who may have a mental disorder should be referred to psychological counseling or treatment. Further, a shared diagnosis or label may have different causes or require different treatments; for this reason the DSM contains no information regarding treatment or cause. The range of the DSM represents an extensive scope of psychiatric and psychological issues or conditions, and it is not exclusive to what may be considered “illnesses”.
Sourcebooks.
The DSM-IV does not specifically cite its sources, but there are four volumes of “sourcebooks” intended to be APA’s documentation of the guideline development process and supporting evidence, including literature reviews, data analyses and field trials. The Sourcebooks have been said to provide important insights into the character and quality of the decisions that led to the production of DSM-IV, and hence the scientific credibility of contemporary psychiatric classification.
Criticism.
Reliability and validity concerns.
The revisions of the DSM from the 3rd Edition forward have been mainly concerned with diagnostic reliability—the degree to which different diagnosticians agree on a diagnosis. It was argued that a science of psychiatry can only advance if diagnosis is reliable. If clinicians and researchers frequently disagree about a diagnosis with a patient, then research into the causes and effective treatments of those disorders cannot advance. Hence, diagnostic reliability was a major concern of DSM-III. When the diagnostic reliability problem was thought to be solved, subsequent editions of the DSM were concerned mainly with “tweaking” the diagnostic criteria. Unfortunately, neither the issue of reliability (accurate measurement) or validity (do these disorders really exist) was settled. However, most psychiatric education post DSM-III focused on issues of treatment—especially drug treatment—and less on diagnostic concerns. In fact, Thomas R. Insel, M.D., Director of the NIMH, has recently stated the agency would no longer fund research projects that rely exclusively on DSM criteria due to its lack of validity. Field trials of DSM-5 brought the debate of reliability back into the limelight as some disorders showed poor reliability. For example Major Depressive Disorder a common mental illness had a poor reliability Kappa statistic of 0.28 indicating that clinicians frequently disagreed on this diagnosis in the same patients. The most reliable diagnosis was Major Neurocognitive Disorder with a Kappa of 0.78 
Superficial symptoms.
By design, the DSM is primarily concerned with the signs and symptoms of mental disorders, rather than the underlying causes. It claims to collect them together based on statistical or clinical patterns. As such, it has been compared to a naturalist’s field guide to birds, with similar advantages and disadvantages. The lack of a causative or explanatory basis, however, is not specific to the DSM, but rather reflects a general lack of pathophysiological understanding of psychiatric disorders. As DSM-III chief architect Robert Spitzer and DSM-IV editor Michael First outlined in 2005, “little progress has been made toward understanding the pathophysiological processes and etiology of mental disorders. If anything, the research has shown the situation is even more complex than initially imagined, and we believe not enough is known to structure the classification of psychiatric disorders according to etiology.”
The DSM’s focus on superficial symptoms is claimed to be largely a result of necessity (assuming such a manual is nevertheless produced), since there is no agreement on a more explanatory classification system. Reviewers note, however, that this approach is undermining research, including in genetics, because it results in the grouping of individuals who have very little in common except superficial criteria as per DSM or ICD diagnosis.
Despite the lack of consensus on underlying causation, advocates for specific psychopathological paradigms have nonetheless faulted the current diagnostic scheme for not incorporating evidence-based models or findings from other areas of science. A recent example is evolutionary psychologists’ criticism that the DSM does not differentiate between genuine cognitive malfunctions and those induced by psychological adaptations, a key distinction within evolutionary psychology, but one widely challenged within general psychology. Another example is a strong operationalist viewpoint, which contends that reliance on operational definitions, as purported by the DSM, necessitates that intuitive concepts such as depression be replaced by specific measurable concepts before they are scientifically meaningful. One critic states of psychologists that “Instead of replacing ‘metaphysical’ terms such as ‘desire’ and ‘purpose’, they used it to legitimize them by giving them operational definitions…the initial, quite radical operationalist ideas eventually came to serve as little more than a ‘reassurance fetish’ (Koch 1992) for mainstream methodological practice.”
A 2013 review published in the "European archives of psychiatry and clinical neuroscience" states “that psychiatry targets the phenomena of consciousness, which, unlike somatic symptoms and signs, cannot be grasped on the analogy with material thing-like objects.” As an example of the problem of the superficial characterization of psychiatric signs and symptoms, the authors gave the example of a patient saying they “feel depressed, sad, or down,” showing that such a statement could indicate various underlying experiences: “not only depressed mood but also, for instance, irritation, anger, loss of meaning, varieties of fatigue, ambivalence, ruminations of different kinds, hyper-reflectivity, thought pressure, psychic anxiety, varieties of depersonalization, and even voices with negative content, and so forth.” The structured interview comes with “danger of over confidence in the face value of the answers, as if a simple ‘yes’ or ‘no’ truly confirmed or denied the diagnostic criterion at issue.” The authors gave an example: A patient who was being administered the Structured Clinical Interview for the DSM-IV Axis I Disorders denied thought insertion, but during a “conversational, phenomenological interview,” a semi-structured interview tailored to the patient, the same patient admitted to experiencing thought insertion, along with a delusional elaboration. The authors suggested 2 reasons for this discrepancy: That the patient didn’t “recognize his own experience in the rather blunt, implicitly either/or formulation of the structured-interview question,” or the experience didn’t “fully articulate itself” until the patient started talking about his experiences.
Dividing lines.
Despite caveats in the introduction to the DSM, it has long been argued that its system of classification makes unjustified categorical distinctions between disorders, and uses arbitrary cut-offs between normal and abnormal. A 2009 psychiatric review noted that attempts to demonstrate natural boundaries between related DSM syndromes, or between a common DSM syndrome and normality, have failed. Some argue that rather than a categorical approach, a fully dimensional, spectrum or complaint-oriented approach would better reflect the evidence.
In addition, it is argued that the current approach based on exceeding a threshold of symptoms does not adequately take into account the context in which a person is living, and to what extent there is internal disorder of an individual versus a psychological response to adverse situations. The DSM does include a step (“Axis IV”) for outlining “Psychosocial and environmental factors contributing to the disorder” once someone is diagnosed with that particular disorder.
Because an individual’s degree of impairment is often not correlated with symptom counts, and can stem from various individual and social factors, the DSM’s standard of distress or disability can often produce false positives. On the other hand, individuals who do not meet symptom counts may nevertheless experience comparable distress or disability in their life.
Cultural bias.
Some psychiatrists also argue that current diagnostic standards rely on an exaggerated interpretation of neurophysiological findings and so understate the scientific importance of social-psychological variables. Advocating a more culturally sensitive approach to psychology, critics such as Carl Bell and Marcello Maviglia contend that the cultural and ethnic diversity of individuals is often discounted by researchers and service providers. In addition, current diagnostic guidelines have been criticized as having a fundamentally Euro-American outlook. Although these guidelines have been widely implemented, opponents argue that even when a diagnostic criteria set is accepted across different cultures, it does not necessarily indicate that the underlying constructs have any validity within those cultures; even reliable application can only demonstrate consistency, not legitimacy. Cross-cultural psychiatrist Arthur Kleinman contends that the Western bias is ironically illustrated in the introduction of cultural factors to the DSM-IV: the fact that disorders or concepts from non-Western or non-mainstream cultures are described as “culture-bound”, whereas standard psychiatric diagnoses are given no cultural qualification whatsoever, is to Kleinman revelatory of an underlying assumption that Western cultural phenomena are universal. Kleinman’s negative view toward the culture-bound syndrome is largely shared by other cross-cultural critics, common responses included both disappointment over the large number of documented non-Western mental disorders still left out, and frustration that even those included were often misinterpreted or misrepresented. Many mainstream psychiatrists have also been dissatisfied with these new culture-bound diagnoses, although not for the same reasons. Robert Spitzer, a lead architect of the DSM-III, has held the opinion that the addition of cultural formulations was an attempt to placate cultural critics, and that they lack any scientific motivation or support. Spitzer also posits that the new culture-bound diagnoses are rarely used in practice, maintaining that the standard diagnoses apply regardless of the culture involved. In general, the mainstream psychiatric opinion remains that if a diagnostic category is valid, cross-cultural factors are either irrelevant or are only significant to specific symptom presentations. One of the results was the development of the Azibo Nosology by Daudi Ajani Ya Azibo as an alternative to the DSM to treat African and African American patients.
Medicalization and financial conflicts of interest.
It has also been alleged that the way the categories of the DSM are structured, as well as the substantial expansion of the number of categories, are representative of an increasing medicalization of human nature, which may be attributed to disease mongering by psychiatrists and pharmaceutical companies, the power and influence of the latter having grown dramatically in recent decades. Of the authors who selected and defined the DSM-IV psychiatric disorders, roughly half have had financial relationships with the pharmaceutical industry at one time, raising the prospect of a direct conflict of interest. The same article concludes that the connections between panel members and the drug companies were particularly strong in those diagnoses where drugs are the first line of treatment, such as schizophrenia and mood disorders, where 100% of the panel members had financial ties with the pharmaceutical industry. In 2005, then American Psychiatric Association President Steven Sharfstein released a statement in which he conceded that psychiatrists had “allowed the biopsychosocial model to become the bio-bio-bio model”.
However, although the number of identified diagnoses has increased by more than 200% (from 106 in DSM-I to 365 in DSM-IV-TR), psychiatrists such as Zimmerman and Spitzer argue it almost entirely represents greater specification of the forms of pathology, thereby allowing better grouping of more similar patients. William Glasser, however, refers to the DSM as “phony diagnostic categories”, arguing that “it was developed to help psychiatrists – to help them make money”. In addition, the publishing of the DSM, with tightly guarded copyrights, has in itself earned over $100 million for the American Psychiatric Association.
Clients and survivors.
A client is a person who accesses psychiatric services and may have been given a diagnosis from the DSM, while a survivor self-identifies as a person who has endured a psychiatric intervention and the mental health system (which may have involved involuntary commitment and involuntary treatment). Some individuals are relieved to find that they have a recognized condition that they can apply a name to and this has led to many people self-diagnosing. Others, however, question the accuracy of the diagnosis, or feel they have been given a “label” that invites social stigma and discrimination (the terms “mentalism” and “sanism” have been used to describe such discriminatory treatment).
Diagnoses can become internalized and affect an individual’s self-identity, and some psychotherapists have found that the healing process can be inhibited and symptoms can worsen as a result. Some members of the psychiatric survivors movement (more broadly the consumer/survivor/ex-patient movement) actively campaign against their diagnoses, or the assumed implications, and/or against the DSM system in general. The Mad Pride movement has been particularly vocal in its criticism of the DSM Additionally, it has been noted that the DSM often uses definitions and terminology that are inconsistent with a recovery model, and such content can erroneously imply excess psychopathology (e.g. multiple “comorbid” diagnoses) or chronicity.
DSM-5 critiques.
Psychiatrist Allen Frances has been critical of proposed revisions to the DSM-5. In a 2012 "New York Times" editorial, Frances warned that if this DSM version is issued unamended by the APA, it will “medicalize normality and result in a glut of unnecessary and harmful drug prescription.” In a December 2, 2012 blog post in "Psychology Today", Frances lists the ten “most potentially harmful changes” to DSM-5: 
Frances and others have published debates on what they see as the six most essential questions in psychiatric diagnosis:
In 2011,a psychologist Brent Robbins co-authored a national letter for the Society for Humanistic Psychology that has brought thousands into the public debate about the DSM. Approximately 14,000 individuals and mental health professionals have signed a petition in support of the letter. Thirteen other American Psychological Association divisions have endorsed the petition. Robbins has noted that under the new guidelines, certain responses to grief could be labeled as pathological disorders, instead of being recognized as being normal human experiences.

</doc>
<doc id="8500" url="http://en.wikipedia.org/wiki?curid=8500" title="Dar es Salaam">
Dar es Salaam

Dar es Salaam ( ', literally "The harbour of peace"), formerly Mzizima, is Tanzania's largest and richest city and a regionally important economic centre. It is the capital of the Dar es Salaam Region administrative province and consists of three local government areas or administrative districts: northern Kinondoni, central Ilala, and southern Temeke. The region had a population of 4,364,541 as of the official 2012 census. Although Dar es Salaam lost its status as the nation's capital to Dodoma in 1974 (not completed until 1996), it remains the locus of the permanent central government bureaucracy.
History.
In the 19th century, Mzizima (Kiswahili for "healthy town") was a coastal fishing village on the periphery of Indian Ocean trade routes. In 1865 or 1866, Sultan Majid bin Said of Zanzibar began building a new city very close to Mzizima and named it Dar es Salaam. The name is commonly translated as "abode/home of peace", based on the Arabic "dar" ("house"), and the Arabic "es salaam" ("of peace"). Dar es Salaam fell into decline after Majid's death in 1870, but was revived in 1887 when the German East Africa Company established a station there. The town's growth was facilitated by its role as the administrative and commercial centre of German East Africa and industrial expansion resulting from the construction of the Central Railway Line in the early 1900s.
German East Africa was captured by the British during World War I and became Tanganyika, with Dar es Salaam the administrative and commercial centre. Under British indirect rule, separate European (e.g., Oyster Bay) and African (e.g., Kariakoo and Ilala) areas developed at a distance from the city centre. The city's population also included a large number of south Asians. After World War II, Dar es Salaam experienced a period of rapid growth.
Political developments, including the formation and growth of the Tanganyika African National Union, led to Tanganyika attaining independence from colonial rule in December 1961. Dar es Salaam continued to serve as its capital, even when in 1964 Tanganyika and Zanzibar merged to form Tanzania. In 1973, however, provisions were made to relocate the capital to Dodoma, a more centrally located city in the interior. The relocation process has not yet been completed, and Dar es Salaam remains Tanzania's primary city.
Geography.
Dar es Salaam is at 6°48' South, 39°17' East (−6.8000, 39.2833), on a natural harbour on the eastern coast of Africa, with sandy beaches in some areas.
Administratively, the Dar es Salaam region is divided into three districts: Ilala, Kinondoni, and Temeke.
Population.
Dar es Salaam is the largest city in Tanzania. With a population increase of 5.6 percent per year from 2002 to 2012, the city is the third fastest growing in Africa (ninth fastest in the world), after Bamako and Lagos. The metro population is expected to reach 5.12 million by 2020.
Economy and infrastructure.
Dar es Salaam is Tanzania's most important city for both business and government. The city contains high concentrations of trade and other services and manufacturing compared to other parts of Tanzania, which has about 80 percent of its population in rural areas. Downtown includes many small businesses, many of which are run by traders and proprietors whose families originated from the Middle East and Indian sub-continent—areas of the world with which the settlements of the Tanzanian coast have had long-standing trading relations.
Dar es Salaam has a problem with slums. According to a United Nations estimate, 70 percent of the city's population lives in informal settlements. The poorer residents crowd into downtown areas or large slums, many without running water or basic services. The more wealthy live in beachside mansions in the city's northern districts.
On a natural harbour on the Indian Ocean, it is the hub of the Tanzanian transportation system as the main railways and several highways originate in or near the city.
Dar es Salaam has had, in the past few years, a major construction boom. The Benjamin William Mkapa Pension Tower with more than 21 stories is the tallest building in the city and the country. Dar es Salaam has major infrastructural problems, including an outdated transport system and occasional power rationing.
Climate.
Because it is close to the equator and the warm Indian Ocean, the city experiences generally tropical climatic conditions, typified by hot and humid weather throughout much of the year. It has a tropical wet and dry climate. Annual rainfall is approximately , and in a normal year there are two rainy seasons: "the long rains" in April and May and "the short rains" in October and November.
Transportation.
The Julius Nyerere International Airport is the principal airport serving the country. Tanzania Railways operates the Central Line from Dar es Salaam to Kigoma. The TAZARA Railway connects Dar es Salaam to Zambia. 
Most intracity transport is by the dala dala (minibus) or Dar es Salaam commuter rail.
The bus rapid transit system under construction will be operated by the Dar Rapid Transit Agency (DART), a government entity, and is expected to open at the end of 2014. DART is being sponsored by the World Bank.
Dala dala minibuses are involved in many road accidents, accounting for a large percentage of the 4000+ yearly road deaths.
Dala dalas are cheap and often overcrowded. They are operated by a driver and a conductor: the conductor collects the fare and signals the driver to leave. They tend to be overcrowded, with passengers sometimes hanging outside the door.
Culture.
Dar es Salaam has heavy traffic during the daytime, but after sunset the area is relatively quiet as much of the city's nightlife is located in more residential districts away from the city's mainly commercial centre.
The sprawling suburbs furthest from the city centre are generally populated by Tanzanians of African descent, with the exception of Oyster Bay, where there is a large population of foreign expatriates. The edges of Dar es Salaam are spreading rapidly, severely taxing the transportation network (which aside from ferries, lacks any kind of mass transit facilities) and raising the prospect of future urban overcrowding.
Food.
Due in part to the growth of the expatriate community and the increasing importance of tourism, the number of international restaurants has risen very rapidly over recent years. The city now offers a rich and internationalized diversity of cuisine, ranging from traditional Tanzanian Barbecue style options such as "Nyama Choma" (Roasted meat—served with rice or ugali) and "Mishkaki" (Shish kebab—usually barbecued and served with salt, hot peppers, chapati, fries, and rice on the side), and the long-established traditional Indian and Zanzibari cuisine, to options from all corners of the globe including Chinese, Thai, Turkish, Italian, and Japanese food. People who prefer neither fast food nor traditional restaurants buy their food from street vendors, who usually sell food at low prices. Samosas are common street food items within the city.
Music.
There is also a lively music scene in Dar es Salaam which is divided between several styles. The longest standing segment is live dance music (muziki wa dansi) bands such as DDC Mlimani Park Orchestra. Taarab which was traditionally strong in Zanzibar has also found a niche but remains small compared both to dance music and "Bongo Flava", a broad category that represents the Tanzanian take on Hip Hop and R&B, which has quickly become the most popular locally produced music. Traditional music, which locally is used to refer to tribal music is still performed but typically only on family oriented occasions such as weddings.
This rap scene has been present and growing for the past ten years as city life has drawn much of the youth in surrounding areas have made the trek into a more urban lifestyle in search of a new better beginning.
In the 1970s, the Ministry of National Youth Culture aimed to create a national culture, which stressed the importance of music. Dar es Salaam became the new music center in Tanzania, with the local radio exposing new bands and dominating the music and cultural scene. With this ujamaa, or family, mentality governing culture and music a unified people’s culture was created. Dar es Salaam became a center of city crime, gangs, and violence, which led to the rise of hip hop music. Throughout the years, the radio in Dar es Salaam has played a major role in the dissemination of music because many people don’t have televisions and cassettes are used over CDs.
Tourism.
Dar es Salaam has two of the five museums comprising the National Museum of Tanzania consortium, namely the National Museum proper and the Village Museum. The National Museum is dedicated to the history of Tanzania; most notably, it exhibits some of the bones of "Paranthropus boisei" that were among the findings of Louis Leakey at Olduvai. The Village Museum, located in the outskirts of the city on the road to Bagamoyo, showcases traditional huts from 16 different Tanzanian ethnic groups. There are also examples of traditional cultivations, and traditional music and dance shows are held daily.
Close to the National Museum are also the botanical gardens, with some specimens of tropical plants and trees.
There are beaches on the Msasani peninsula north of Dar es Salaam and in Kigamboni to the south where residents and tourists alike frequently visit. Trips to the nearby islands of the Dar es Salaam Marine Reserve are a popular daytrip from the city and a favourite spot for snorkeling, swimming and sunbathing. In addition to that, Bongoyo Island can be reached by boat from the Msasani Slipway.
Art.
Dar es Salaam (and specifically the area of Oyster Bay) is home to the popular Tingatinga painting style. The Nyumba ya sanaa ("House of Art") is a well-known cultural centre, workshop and shop dedicated to Tanzanian art, showcasing and promoting Tanzanian craftmanship. Prominent Tanzania sculptor George Lilanga has contributed to the centre some of his works, including decorations of the building's main entrance.
Sport.
The National Stadium hosts Dar es Salaam's Young Africans Football Club, Simba Sports Club, other Tanzanian football clubs, and many other international matches.
Newspapers.
Dar has a considerable number of newspapers available, particularly from sellers prowling through stationary traffic at road intersections. English-language ones, with online presences, include "The Citizen" and "The Guardian" and the leading Kiswahili daily, Mwananchi.
Internet access.
Installation of a trans-Indian Ocean backbone cable in 2009 has, in theory, made Internet access much more readily available in Dar in particular and in East Africa in general. However, roll-out to end-users is slow, partly because of spotty telephone line coverage, partly due to the substantial prices and long contracts demanded for purchase of bandwidth for small ISPs. Mobile-telephone access to the Internet via 3G and 3.75G is still relatively expensive.
Internet cafes are fairly well distributed in the city centre.
The expressed aim of the SEACOM cable is to enable East Africa to develop economically through increased online trading.
Globalization.
Globalization has affected many of the cultural expressions in Dar es Salaam, in particular, hip hop music and culture. The hip hop scene in Dar es Salaam articulates a blending of local cultural struggles and the indigenization of global influences. Hip hop music and culture arrived in Tanzania, taking its cues from various African American styling.
Dar es Salaam, a city projected to have over 5 million inhabitants within the next decade, continues to be the one city in Tanzania to which villagers flock for better opportunities. Westerners and Asians are also settling in Dar es Salaam, and the surge of foreigners has put pressure on Dar es Salaam officials to implement laws better accommodating the growing diverse population of Dar es Salaam and its suburbs.
Safety.
Safety has become a noticeable feature in Dar es Salaam and many other Tanzanian cities. Although Dar es Salaam is one of the safest large cities in East Africa, violent crimes and homicides are becoming more frequent in Dar es Salaam. Chain snatching is relatively common in the Kariakoo area. Although pickpockets frequent the City Centre and dala-dalas and prey especially on foreigners, in the last couple of years, reports of violent crimes in Dar es Salaam have become more and more frequent.
Education.
Dar es Salaam is also the educational centre of Tanzania. The city is home to many Educational Institutions.
Suburbs.
Dar es Salaam is divided into three districts: Ilala, Kinondoni, and Temeke. All three are governed as municipal councils, and so all of the city's suburbs or wards are affiliated with them.
Kinondoni.
Kinondoni is the most populated amongst the districts, with half of the city's population residing within it. It is also home to many of the high-income suburbs. These include:
Ilala.
Ilala is the administrative district of the city where almost all government offices and ministries are housed. The Central Business District (locally called "Posta") is also located in this district. Furthermore, it is the transportation hub of the city, as the Julius Nyerere International Airport, Central Railway Station and Tazara Railway Station are all within the district boundaries. The residential areas are mainly middle to high-income, and some of these are:
Most famous gang groups are recognised by the color of the scarf(bandanna).These are the black gang,red gang and blues gang fighting for control and to maintain their territories and interests.
Temeke.
Temeke is the industrial district of the city, where the main manufacturing centers (with both heavy and light industries) are located. The Port of Dar es Salaam, which is the largest in the country, is also found here. Temeke is believed to have the largest concentration of low-income residents due to industry. Also, many port officials, military and police officers live here.
Sports.
Dar es Salaam is the sports center of Tanzania. Dar es Salaam hosts the second largest stadium in East and Central Africa (National Stadium), which can accommodate up to 60,000 people. The city is home of the most famous and rival soccer clubs, The Simba Sports Club (Simba) and Young Africans Sports Club (Yanga).
Apart from the National Stadium, Dar es salaam is home to the Uhuru Stadium (used mainly for local tournaments and political gatherings), Karume Memorial Stadium (the home of Tanzania Football Federation (TFF)), the Gymkhana Golf Courses (between the city center and the shores of the Indian Ocean), and also has tennis courts, squash courts, and a Fitness club. Outside the metropolitan districts, there is the Lugalo Military Golf Course (located in the Lugalo Military Barracks).
Twin towns—Sister cities.
Dar es Salaam is twinned with:

</doc>
<doc id="8501" url="http://en.wikipedia.org/wiki?curid=8501" title="Distributed computing">
Distributed computing

Distributed computing is a field of computer science that studies distributed systems. A "distributed system" is a software system in which components located on networked computers communicate and coordinate their actions by passing messages. The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
A computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs. There are many alternatives for the message passing mechanism, including RPC-like connectors and message queues. An important goal and challenge of distributed systems is location transparency.
Distributed computing also refers to the use of distributed systems to solve computational problems. In "distributed computing", a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other by message passing.
Introduction.
The word "distributed" in terms such as "distributed system", "distributed programming", and "distributed algorithm" originally referred to computer networks where individual computers were physically distributed within some geographical area. The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.
While there is no single definition of a distributed system, the following defining properties are commonly used:
In this article, the computational entities are called "computers" or "nodes".
A distributed system may have a common goal, such as solving a large computational problem. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.
Other typical properties of distributed systems include the following:
Architecture.
Client/Server System : 
The Client-server architecture is a way to provide a service from a central source. There is a single server that provides a service, and many clients that communicate with the server to consume its products. In this architecture, clients and servers have different jobs. The server's job is to respond to service requests from clients, while a client's job is to use the data provided in response in order to perform some tasks.
Peer-to-Peer System :
The term peer-to-peer is used to describe distributed systems in which labour is divided among all the components of the system. All the computers send and receive data, and they all contribute some processing power and memory. As a distributed system increases in size, its capacity of computational resources increases. In a peer-to-peer system, all components of the system contribute some processing power and memory to a distributed computation.
Parallel and distributed computing.
Distributed systems are groups of networked computers, which have the same goal for their work.
The terms "concurrent computing", "parallel computing", and "distributed computing" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as "parallel" and "distributed"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as "parallel" or "distributed" using the following criteria:
The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; as usual, the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.
The situation is further complicated by the traditional uses of the terms parallel and distributed "algorithm" that do not quite match the above definitions of parallel and distributed "systems"; see the section Theoretical foundations below for more detailed discussion. Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.
History.
The use of concurrent processes that communicate by message-passing has its roots in operating system architectures studied in the 1960s. The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.
ARPANET, the predecessor of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET, and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET, and its successor, the Internet, other early worldwide computer networks included Usenet and FidoNet from 1980s, both of which were used to support distributed discussion systems.
The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its European counterpart International Symposium on Distributed Computing (DISC) was first held in 1985.
Applications.
Reasons for using distributed systems and distributed computing may include:
Ghaemi "et al." define a distributed query as a query "that selects data from databases located at multiple sites in a network" and offer as an SQL example:
Examples.
Examples of distributed systems and applications of distributed computing include the following:
Theoretical foundations.
Models.
Many tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of "instances" together with a "solution" for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.
Theoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.
The field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by “solving a problem” in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?
The discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.
Three viewpoints are commonly used:
In the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network "is" the problem instance. This is illustrated in the following example.
An example.
Consider the computational problem of finding a coloring of a given graph "G". Different fields might take the following approaches:
While the field of parallel algorithms has a different focus than the field of distributed algorithms, there is a lot of interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.
Moreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing). The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).
Complexity measures.
In parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC. The class NC can be defined equally well by using the PRAM formalism or Boolean circuits – PRAM machines can simulate Boolean circuits efficiently and vice versa.
In the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. During each "communication round", all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbours. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.
This complexity measure is closely related to the diameter of the network. Let "D" be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2"D" communication rounds: simply gather all information in one location ("D" rounds), solve the problem, and inform each node about the solution ("D" rounds).
On the other hand, if the running time of the algorithm is much smaller than "D" communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their "local neighbourhood". Many distributed algorithms are known with the running time much smaller than "D" rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.
Other commonly used measures are the total number of bits transmitted in the network (cf. communication complexity).
Other problems.
Traditional computational problems take the perspective that we ask a question, a computer (or a distributed system) processes the question for a while, and then produces an answer and stops. However, there are also problems where we do not want the system to ever stop. Examples of such problems include the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.
There are also fundamental challenges that are unique to distributed computing. The first example is challenges that are related to "fault-tolerance". Examples of related problems include consensus problems, Byzantine fault tolerance, and self-stabilisation.
A lot of research is also focused on understanding the "asynchronous" nature of distributed systems:
Properties of distributed systems.
So far the focus has been on "designing" a distributed system that solves a given problem. A complementary research problem is "studying" the properties of a given distributed system.
The halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.
However, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete, i.e., it is decidable, but it is not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.
Coordinator election.
Coordinator election (sometimes called leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the "coordinator" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.
The network nodes communicate among themselves in order to decide which of them will get into the "coordinator" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.
The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.
Coordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.
Many other algorithms were suggested for different kind of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.
In order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.
Bully algorithm.
When using the Bully algorithm, any process sends a message to the current coordinator. If there is no response within a given time limit, the process tries to elect itself as leader.
Chang and Roberts algorithm.
The Chang and Roberts algorithm (or "Ring Algorithm") is a ring-based election algorithm used to find a process with the largest unique identification number .
Architectures.
Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.
Distributed programming typically falls into one of several basic architectures or categories: client–server, 3-tier architecture, "n"-tier architecture, distributed objects, loose coupling, or tight coupling.
Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. Alternatively, a "database-centric" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.

</doc>
<doc id="8504" url="http://en.wikipedia.org/wiki?curid=8504" title="Dublin">
Dublin

Dublin (; , ) is the capital and largest city of Ireland. Dublin is in the province of Leinster on Ireland's east coast, at the mouth of the River Liffey.
Founded as a Viking settlement, the Kingdom of Dublin became Ireland's principal city following the Norman invasion. The city expanded rapidly from the 17th century and was briefly the second largest city in the British Empire before the Act of Union in 1800. Following the partition of Ireland in 1922, Dublin became the capital of the Irish Free State and later the Republic of Ireland.
Dublin is administered by a City Council. The city is listed by the Globalization and World Cities Research Network (GaWC) as a global city, with a ranking of "Alpha-", placing it among the top thirty cities in the world. It is a historical and contemporary centre for education, the arts, administration, economy and industry.
History.
Toponymy.
Although the area of Dublin Bay has been inhabited by humans since prehistoric times, the writings of Ptolemy (the Greco-Roman astronomer and cartographer) in about 140 AD provide possibly the earliest reference to a settlement there. He called the settlement .
The name "Dublin" comes from the Irish name "Dubhlinn" or "Duibhlinn", meaning "black pool". This is made up of the elements "dubh" (black) and "linn" (pool). In most Irish dialects, "dubh" is pronounced . The original pronunciation is preserved in the names for the city in other languages such as Old English "Difelin", Old Norse "Dyflin", modern Icelandic "Dyflinn" and modern Manx "Divlyn". Other localities in Ireland also bear the name "Duibhlinn", variously anglicized as Devlin, Divlin and Difflin. Historically, scribes using the Gaelic script wrote "bh" with a dot over the "b", rendering or . Those without knowledge of Irish omitted the dot, spelling the name as "Dublin".
', meaning "town of the hurdled ford", is the common name for the city in modern Irish. ' is a place name referring to a fording point of the River Liffey near Father Mathew Bridge. "" was an early Christian monastery, believed to have been in the area of Aungier Street, currently occupied by Whitefriar Street Carmelite Church.
The subsequent Scandinavian settlement centred on the River Poddle, a tributary of the Liffey in an area now known as Wood Quay. The Dubhlinn was a small lake used to moor ships; the Poddle connected the lake with the Liffey. This lake was covered during the early 18th century as the city grew. The Dubhlinn lay where the Castle Garden is now located, opposite the Chester Beatty Library in Dublin Castle. "Táin Bó Cuailgne" ("The Cattle Raid of Cooley") refers to "Dublind rissa ratter Áth Cliath", meaning "Dublin, which is called Ath Cliath".
Middle Ages.
Dublin was established as a Viking settlement in the 9th century and, despite a number of rebellions by the native
Irish, it remained largely under Viking control until the Norman invasion of Ireland was launched from Wales in 1169. The King of Leinster, Diarmait Mac Murchada, enlisted the help of Strongbow, the Earl of Pembroke, to conquer Dublin. Following Mac Murrough's death, Strongbow declared himself King of Leinster after gaining control of the city. In response to Strongbow's successful invasion, King Henry II of England reaffirmed his sovereignty by mounting a larger invasion in 1171 and pronounced himself Lord of Ireland. Around this time, the county of the City of Dublin was established along with certain liberties adjacent to the city proper. This continued down to 1840 when the barony of Dublin City was separated from the barony of Dublin. Since 2001, both baronies have been redesignated the City of Dublin.
Dublin Castle, which became the centre of Norman power in Ireland, was founded in 1204 as a major defensive work on the orders of King John of England. Following the appointment of the first Lord Mayor of Dublin in 1229, the city expanded and had a population of 8,000 by the end of the 13th century. Dublin prospered as a trade centre, despite an attempt by King Robert I of Scotland to capture the city in 1317. It remained a relatively small walled medieval town during the 14th century and was under constant threat from the surrounding native clans. In 1348, the Black Death, a lethal plague which had ravaged Europe, took hold in Dublin and killed thousands over the following decade.
Dublin was incorporated into the English Crown as the Pale, which was a narrow strip of English settlement along the eastern seaboard. The Tudor conquest of Ireland in the 16th century spelt a new era for
Dublin, with the city enjoying a renewed prominence as the centre of administrative rule in Ireland. Determined to make Dublin a Protestant city, Queen Elizabeth I of England established Trinity College in 1592 as a solely Protestant university and ordered that the Catholic St. Patrick's and Christ Church cathedrals be converted to Protestant.
The city had a population of 21,000 in 1640 before a plague in 1649–51 wiped out almost half of the city's inhabitants. However, the city prospered again soon after as a result of the wool and linen trade with England, reaching a population of over 50,000 in 1700.
Early modern.
As the city continued to prosper during the 18th century, Georgian Dublin became, for a short period, the second largest city of the British Empire and the fifth largest city in Europe, with the population exceeding 130,000. The vast majority of Dublin's most notable architecture dates from this period, such as the Four Courts and the Custom House. Temple Bar and Grafton Street are two of the few remaining areas that were not affected by the wave of Georgian reconstruction and maintained their medieval character.
Dublin grew even more dramatically during the 18th century, with the construction of many famous districts and buildings, such as Merrion Square, Parliament House and the Royal Exchange. The Wide Streets Commission was established in 1757 at the request of Dublin Corporation to govern architectural standards on the layout of streets, bridges and buildings. In 1759, the founding of the Guinness brewery resulted in a considerable economic gain for the city. For much of the time since its foundation, the brewery was Dublin's largest employer.
Late modern and contemporary.
Dublin suffered a period of political and economic decline during the 19th century following the Act of Union of 1800, under which the seat of government was transferred to the Westminster Parliament in London. The city played no major role in the Industrial Revolution, but remained the centre of administration and a transport hub for most of the island. Ireland had no significant sources of coal, the fuel of the time, and Dublin was not a centre of ship manufacturing, the other main driver of industrial development in Britain and Ireland. Belfast developed faster than Dublin during this period on a mixture of international trade, factory-based linen cloth production and shipbuilding.
The Easter Rising of 1916, the Irish War of Independence, and the subsequent Irish Civil War resulted in a significant amount of physical destruction in central Dublin. The Government of the Irish Free State rebuilt the city centre and located the new parliament, the Oireachtas, in Leinster House. Since the beginning of Norman rule in the 12th century, the city has functioned as the capital in varying geopolitical entities: Lordship of Ireland (1171–1541), Kingdom of Ireland (1541–1800), island as part of the United Kingdom of Great Britain and Ireland (1801–1922), and the Irish Republic (1919–1922). Following the partition of Ireland in 1922, it became the capital of the Irish Free State (1922–1949) and now is the capital of the Republic of Ireland. One of the memorials to commemorate that time is the Garden of Remembrance.
Dublin was also victim to the Northern Irish Troubles. While during this 30 year conflict, violence mainly engulfed Northern Ireland. However, the Provisional IRA drew a lot of support from the Republic, specifically Dublin. This caused many Loyalist paramilitaries such as the Ulster Volunteer Force to Bomb the city. The most notable of atrocities carried out by loyalists during this time was the Dublin and Monaghan bombings in which 34 people died, mainly in Dublin itself.
Since 1997, the landscape of Dublin has changed immensely. The city was at the forefront of Ireland's rapid economic expansion during the Celtic Tiger period, with enormous private sector and state development of housing, transport and business.
Government.
Local.
From 1842, the boundaries of the city were comprehended by the baronies of Dublin City and the Barony of Dublin. In 1930, the boundaries were extended by the Local Government (Dublin) Act. Later, in 1953, the boundaries were again extended by the Local Government Provisional Order Confirmation Act.
Dublin City Council is a unicameral assembly of 63 members elected every five years from Local Election Areas. It is presided over by the Lord Mayor, who is elected for a yearly term and resides in Mansion House. Council meetings occur at Dublin City Hall, while most of its administrative activities are based in the Civic Offices on Wood Quay. The party or coalition of parties, with the majority of seats adjudicates committee members, introduces policies, and appoints the Lord Mayor. The Council passes an annual budget for spending on areas such as housing, traffic management, refuse, drainage, and planning. The Dublin City Manager is responsible for implementing City Council decisions.
National.
As the capital city, Dublin seats the national parliament of Ireland, the Oireachtas. It is composed of the President of Ireland, Seanad Éireann as the upper house, and Dáil Éireann as the lower house. The President resides in Áras an Uachtaráin in the Phoenix Park, while both houses of the Oireachtas meet in Leinster House, a former ducal palace on Kildare Street. It has been the home of the Irish parliament since the creation of the Irish Free State in 1922. The old Irish Houses of Parliament of the Kingdom of Ireland were located in College Green.
Government Buildings house the Department of the Taoiseach, the Council Chamber, the Department of Finance and the Office of the Attorney General. It consists of a main building (completed 1911) with two wings (completed 1921). It was designed by Thomas Manley Dean and Sir Aston Webb as the Royal College of Science. The First Dáil originally met in the Mansion House in 1919. The Irish Free State government took over the two wings of the building to serve as a temporary home for some ministries, while the central building became the College of Technology until 1989. Although both it and Leinster House were intended to be temporary, they became the permanent homes of parliament from then on.
For elections to Dáil Éireann the city is divided into five constituencies: Dublin Central (3 seats), Dublin Bay North (5 seats), Dublin North–West (3 seats), Dublin South–Central (4 seats) and Dublin Bay South (4 seats). Nineteen TD's are elected in total.
Politics.
In the past Dublin city was regarded as a stronghold for Fianna Fáil, however following the Irish local elections, 2004 the party was eclipsed by the centre-left Labour Party. In the 2011 general election the Dublin Region elected 18 Labour Party, 17 Fine Gael, 4 Sinn Féin,2 Socialist Party, 2 People Before Profit Alliance and 3 Independent TDs. Fianna Fáil lost all but one of its sitting TDs in the region.
Geography.
Landscape.
Dublin is situated at the mouth of the River Liffey and encompasses a land area of approximately 115 km2. It is bordered by a low mountain range to the south and surrounded by flat farmland to the north and west. The
Liffey divides the city in two between the Northside and the Southside. Each of these is further divided by two lesser rivers – the River Tolka running northwest from Dubin Bay, and the River Dodder running southwest from the mouth of the Liffey. Two further water bodies – the Grand Canal on the southside and the Royal Canal on the northside – ring the inner city on their way to the west and the River Shannon.
The River Liffey bends at Leixlip from a predominantly east-west direction to a southwesterly route, and this point also marks the change from urban development to more agricultural land usage.
Cultural divide.
A north-south division has traditionally existed, with the River Liffey as the divider. The Northside is generally seen as working class, while the Southside is seen as middle to upper-middle class. The divide is punctuated by examples of Dublin "sub-culture" stereotypes, with upper-middle class constituents seen as tending towards an accent and demeanour synonymous with the Southside, and working-class Dubliners seen as tending towards characteristics associated with Northside and inner-city areas. Dublin's economic divide is east-west as well as north-south. There are also social divisions evident between the coastal suburbs in the east of the city, including those on the Northside, and the newer developments further to the west.
Climate.
Similar to much of northwest Europe, Dublin experiences a maritime climate with cool winters, mild summers, and a lack of temperature extremes. The average maximum January temperature is , while the average maximum July temperature is . On average, the sunniest months are May and June, while the wettest month is October with of rain, and the driest month is February with . Rainfall is evenly distributed throughout the year.
Ringsend in the south of the city records the least amount of rainfall in Ireland, with an average annual precipitation of , with the average annual precipitation in the city centre being . The main precipitation in winter is rain; however snow showers do occur between November and March. Hail is more common than snow. The city experiences long summer days and short winter days. Strong Atlantic winds are most common in autumn. These winds can affect Dublin, but due to its easterly location it is least affected compared to other parts of the country. However, in winter, easterly winds render the city more prone to snow showers.
Places of interest.
Landmarks.
Dublin has many landmarks and monuments dating back hundreds of years. One of the oldest is Dublin Castle, which was first founded as a major defensive work on the orders of King John of England in 1204, shortly after the Norman invasion of Ireland in 1169, when it was commanded that a castle be built with strong walls and good ditches for the defence of the city, the administration of justice, and the protection of the King's treasure. Largely complete by 1230, the castle was of typical Norman courtyard design, with a central square without a keep, bounded on all sides by tall defensive walls and protected at each corner by a circular tower. Sited to the south-east of Norman Dublin, the castle formed one corner of the outer perimeter of the city, using the River Poddle as a natural means of defence.
One of Dublin's newest monuments is the Spire of Dublin, or officially titled "Monument of Light". It is a conical spire made of stainless steel and is located on O'Connell Street. It replaces Nelson's Pillar and is intended to mark Dublin's place in the 21st century. The spire was designed by Ian Ritchie Architects, who sought an "Elegant and dynamic simplicity bridging art and technology". During the day it maintains its steel look, but at dusk the monument appears to merge into the sky. The base of the monument is lit and the top is illuminated to provide a beacon in the night sky across the city.
Many people visit Trinity College, Dublin to see the Book of Kells in the library there. The Book of Kells is an illustrated manuscript created by Irish monks circa. 800 AD. The Ha'penny Bridge; an old iron footbridge over the River Liffey is one of the most photographed sights in Dublin and is considered to be one of Dublin's most iconic landmarks.
Other popular landmarks and monuments include the Mansion House, the Anna Livia monument, the Molly Malone statue, Christ Church Cathedral, St Patrick's Cathedral, Saint Francis Xavier Church on Upper Gardiner Street near Mountjoy Square, The Custom House, and Áras an Uachtaráin. The Poolbeg Towers are also iconic features of Dublin and are visible in many spots around the city.
Parks.
Dublin has more green spaces per square kilometre than any other European capital city, with 97% of city residents living within 300 metres of a park area. The city council provides of public green space per 1,000 people and 255 playing fields. The council also plants approximately 5,000 trees annually and manages over of parks.
There are many park areas around the city, including the Phoenix Park, Herbert Park and St Stephen's Green. The Phoenix Park is about west of the city centre, north of the River Liffey. Its perimeter wall encloses one of the largest walled city parks in Europe. It includes large areas of grassland and tree-lined avenues, and since the 17th century has been home to a herd of wild Fallow deer. The residence of the President of Ireland (Áras an Uachtaráin), which was built in 1751, is located in the park. The park is also home to Dublin Zoo, the official residence of the United States Ambassador, and Ashtown Castle. Music concerts have also been performed in the park by many singers and musicians.
St Stephen's Green is adjacent to one of Dublin's main shopping streets, Grafton Street, and to a shopping centre named for it, while on its surrounding streets are the offices of a number of public bodies and the city terminus of one of Dublin's Luas tram lines. Saint Anne's Park is a public park and recreational facility, shared between Raheny and Clontarf, both suburbs on the North Side of Dublin.
The park, the second largest municipal park in Dublin, is part of a former (500 acre) estate assembled by members of the Guinness family, beginning with Benjamin Lee Guinness in 1835 (the largest municipal park is nearby (North) Bull Island, also shared between Clontarf and Raheny).
Economy.
The Dublin region is the economic centre of Ireland, and was at the forefront of the country's rapid economic expansion during the Celtic Tiger period. In 2009, Dublin was listed as the fourth richest city in the world by purchasing power and 10th richest by personal income. According to "Mercer's 2011 Worldwide Cost of Living Survey", Dublin is the 13th most expensive city in the European Union (down from 10th in 2010) and the 58th most expensive place to live in the world (down from 42nd in 2010). As of 2005, approximately 800,000 people were employed in the Greater Dublin Area, of whom around 600,000 were employed in the services sector and 200,000 in the industrial sector.
Many of Dublin's traditional industries, such as food processing, textile manufacturing, brewing, and distilling have gradually declined, although Guinness has been brewed at the St. James's Gate Brewery since 1759. Economic improvements in the 1990s have attracted a large number of global pharmaceutical, information and communications technology companies to the city and Greater Dublin Area. Companies such as Microsoft, Google, Amazon, eBay, PayPal, Yahoo!, Facebook, Twitter and Pfizer now have European headquarters and/or operational bases in the city.
Financial services have also become important to the city since the establishment of Dublin's International Financial Services Centre in 1987, which is globally recognised as a leading location for a range of internationally traded financial services. More than 500 operations are approved to trade in under the IFSC programme. The centre is host to half of the world's top 50 banks and to half of the top 20 insurance companies. Many international firms have established major headquarters in the city, such as Citibank and Commerzbank. The Irish Stock Exchange (ISEQ), Internet Neutral Exchange (INEX) and Irish Enterprise Exchange (IEX) are also located in Dublin. The economic boom led to a sharp increase in construction, with large redevelopment projects in the Dublin Docklands and Spencer Dock. Completed projects include the Convention Centre, the 3Arena, and the Bord Gáis Energy Theatre.
Transport.
Road.
The road network in Ireland is primarily focused on Dublin. The M50 motorway, a semi-ring road which runs around the south, west and north of the city, connects important national primary routes to the rest of the country. In 2008, the West-Link toll bridge was replaced by the eFlow barrier-free tolling system, with a three-tiered charge system based on electronic tags and car pre-registration. The toll is currently €2.10 for vehicles with a pre-paid tag, €2.60 for vehicles whose number plates have been registered with eFlow, and €3.10 for unregistered vehicles.
The first phase of a proposed eastern bypass for the city is the Dublin Port Tunnel, which officially opened in 2006 to mainly cater for heavy vehicles. The tunnel connects Dublin Port and the M1 motorway close to Dublin Airport. The city is also surrounded by an inner and outer orbital route. The inner orbital route runs approximately around the heart of the Georgian city and the outer orbital route runs primarily along the natural circle formed by Dublin's two canals, the Grand Canal and the Royal Canal, as well as the North and South Circular Roads.
Dublin is served by an extensive network of nearly 200 bus routes which serve all areas of the city and suburbs. The majority of these are controlled by Dublin Bus, but a number of smaller companies also operate. Fares are generally calculated on a stage system based on distance travelled. There are several different levels of fares, which apply on most services. A "Real Time Passenger Information" system was introduced at Dublin Bus bus stops in 2012. Electronically displayed signs relay information about the time of the next bus' arrival based on its' GPS determined position. The National Transport Authority is responsible for integration of bus and rail services in Dublin and has been involved in introducing a pre-paid smart card, called a Leap card, which can be used on Dublin's public transport services.
Rail.
Heuston and Connolly stations are the two main railway stations in Dublin. Operated by Iarnród Éireann, the Dublin Suburban Rail network consists of five railway lines serving the Greater Dublin Area and commuter towns such as Drogheda and Dundalk in County Louth. One of these lines is the electrified Dublin Area Rapid Transit (DART) line, which runs primarily along the coast of Dublin, from Malahide and Howth southwards as far as Greystones in County Wicklow. Commuter rail operates on the other four lines using Irish Rail diesel multiple units. In 2012, passengers for DART and Dublin Suburban lines were 15.8 million and 9.9 million, respectively (around 70% of all Irish Rail passengers).
The Luas is a light rail system, run by Veolia Transport, has been operating since 2004 and now carries over 30 million passengers annually. The network consists of two lines; the Red Line links the Docklands and city centre with the south-western suburbs, while the Green Line connects the city centre with suburbs to the south of the city and together comprise a total 54 stations and of track. Construction of a 6 km extension to the Green Line, bringing it to the north of the city, commenced in June 2013.
Proposed multi-million euro projects such as the Dublin Metro and the DART Underground will also be considered in light of the current difficult economic climate.
Airport.
Dublin Airport is operated by the Dublin Airport Authority and is located north of Dublin City in the administrative county of Fingal. It is the headquarters of Ireland's flag carrier Aer Lingus, low-cost carrier Ryanair and regional airlines Aer Arann and CityJet. The airport offers an extensive short and medium haul network, as well as domestic services to many regional airports in Ireland. There are also extensive Long Haul services to the United States, Canada and the Middle East. Dublin Airport is the busiest airport in Ireland, followed by Cork and Shannon. Construction of a second terminal began in 2007 and was officially opened on 19 November 2010.
Dublin Airport currently ranks as the 25th busiest airport in Europe recording nearly 19 million passengers during 2011.
Cycling.
Dublin City Council began installing cycle lanes and tracks throughout the city in the 1990s, and the city has over of specific on- and off-road tracks for cyclists. In 2011, the city was ranked 9th of major world cities on the "Copenhagenize Index of Bicycle-Friendly Cities".
Dublinbikes is a self-service bicycle rental scheme which has been in operation in Dublin since 2009. Sponsored by JCDecaux, the scheme consists of 550 French-made unisex bicycles stationed at 44 terminals throughout the city centre. Users must make a subscription for either an annual Long Term Hire Card costing €20 or a 3 Day Ticket costing €2. The first 30 minutes of use is free, but after that a service charge depending on the extra length of use applies. Dublinbikes now has over 58,000 subscribers and there are plans to dramatically expand the service across the city and its suburbs to provide for up to 5,000 bicycles and approximately 300 terminals.
The 2011 Census revealed that 5.9 percent of commuters in Dublin cycled. A 2012 report by Dublin City Council on traffic flows crossing the canals in and out of the city found that just over 8% of all traffic was made up of cyclists, representing an increase of 17.5% on 2011 and a 70% increase on 2002 levels.
Higher education.
Dublin is the primary centre of education in Ireland, it is home to three universities, Dublin Institute of Technology and many other higher education institutions. There are 20 third-level institutes in the city and in surrounding towns and suburbs.
Dublin was European Capital of Science in 2012. The University of Dublin is the oldest university in Ireland dating from the 16th century, and is located in the city centre. Its sole constituent college, Trinity College, was established by Royal Charter in 1592 under Elizabeth I and was closed to Roman Catholics until Catholic Emancipation. The Catholic hierarchy then banned Roman Catholics from attending it until 1970. It is situated in the city centre, on College Green, and has 15,000 students.
The National University of Ireland (NUI) has its seat in Dublin, which is also the location of the associated "constituent university" of University College Dublin (UCD), has over 22,000 students. UCD's main campus is at Belfield, about from the city centre in the southeastern suburbs..
With a continuous history dating back to 1887, Dublin's principal institution for technological education and research Dublin Institute of Technology (DIT) is Ireland's largest higher education institution with over 23,000 students. Dublin Institute of Technology specialises in engineering, architecture, sciences, health, digital media, hospitality and business but also offers many art, design, music and humanities programmes. DIT currently has campuses, buildings and research facilities at multiple locations in central Dublin, it has commenced consolidation to a new city-centre campus in Grangegorman.
Dublin City University (DCU), formerly known as the National Institute for Higher Education (NIHE), specialises in business, engineering, science, and communication courses. It has around 10,000 students, and is located about from the city centre in the northern suburbs.
The Royal College of Surgeons in Ireland (RCSI) is a medical school which is a recognised college of the NUI, it is situated at St. Stephen's Green in the city centre.
The National University of Ireland, Maynooth, another constituent of the NUI, is in neighbouring Co. Kildare, about from the city centre. The Institute of European Affairs is also in Dublin.
Portobello College has its degrees conferred through the University of Wales. Dublin Business School (DBS) is Ireland's largest private third level institution with over 9,000 students located on Aungier Street.
The National College of Art and Design (NCAD) supports training and research in art, design and media. The National College of Ireland (NCI) is also based in Dublin. The Economic and Social Research Institute, a social science research institute, is based on Sir John Rogerson's Quay, Dublin 2.
The Irish public administration and management training centre has its base in Dublin, the Institute of Public Administration provides a range of undergraduate and post graduate awards via the National University of Ireland and in some instances, Queen's University Belfast. There are also smaller specialised colleges, including Griffith College Dublin, The Gaiety School of Acting and the New Media Technology College.
Outside of the city, the towns of Tallaght in South Dublin and Dún Laoghaire in Dún Laoghaire–Rathdown have regional colleges:
The Institute of Technology, Tallaght has full and part-time courses in a wide range of technical subjects and the Dún Laoghaire Institute of Art, Design and Technology (IADT) supports training and research in art, design, business, psychology and media technology.
The western suburb of Blanchardstown offers childcare and sports management courses along with languages and technical subjects at the Institute of Technology, Blanchardstown.
Demographics.
The City of Dublin is the area administered by Dublin City Council, but the term "Dublin" normally refers to the contiguous urban area which includes parts of the adjacent local authority areas of Dún Laoghaire–Rathdown, Fingal and South Dublin. Together, the four areas form the traditional County Dublin. This area is sometimes known as the Dublin Region. The population of the administrative area controlled by the City Council was 525,383 in the 2011 census, while the population of the urban area was 1,110,627. The County Dublin population was 1,273,069 and that of the Greater Dublin Area 1,804,156. The area's population is expanding rapidly, and it is estimated by the Central Statistics Office that it will reach 2.1 million by 2020.
Since the late 1990s, Dublin has experienced a significant level of net immigration, with the greatest numbers coming from the European Union, especially the United Kingdom, Poland and Lithuania. There is also a considerable number of immigrants from outside Europe, particularly from China and Nigeria. Dublin is home to a greater proportion of new arrivals than any other part of the country. Sixty percent of Ireland's Asian population lives in Dublin. Over 15% of Dublin's population was foreign-born in 2006.
Culture.
The arts.
Dublin has a world-famous literary history, having produced many prominent literary figures, including Nobel laureates William Butler Yeats, George Bernard Shaw and Samuel Beckett. Other influential writers and playwrights include Oscar Wilde, Jonathan Swift and the creator of Dracula, Bram Stoker. It is arguably most famous as the location of the greatest works of James Joyce, including "Ulysses", which is set in Dublin and full of topical detail. "Dubliners" is a collection of short stories by Joyce about incidents and typical characters of the city during the early 20th century. Other renowned writers include J. M. Synge, Seán O'Casey, Brendan Behan, Maeve Binchy, and Roddy Doyle. Ireland's biggest libraries and literary museums are found in Dublin, including the National Print Museum of Ireland and National Library of Ireland. In July 2010, Dublin was named as a UNESCO City of Literature, joining Edinburgh, Melbourne and Iowa City with the permanent title.
There are several theatres within the city centre, and various world famous actors have emerged from the Dublin theatrical scene, including Noel Purcell, Sir Michael Gambon, Brendan Gleeson, Stephen Rea, Colin Farrell, Colm Meaney and Gabriel Byrne. The best known theatres include the Gaiety, Abbey, Olympia, Gate, and Grand Canal. The Gaiety specialises in musical and operatic productions, and is popular for opening its doors after the evening theatre production to host a variety of live music, dancing, and films. The Abbey was founded in 1904 by a group that included Yeats with the aim of promoting indigenous literary talent. It went on to provide a breakthrough for some of the city's most famous writers, such as Synge, Yeats himself and George Bernard Shaw. The Gate was founded in 1928 to promote European and American Avant Garde works. The Grand Canal Theatre is a new 2,111 capacity theatre which opened in March 2010 in the Grand Canal Dock.
Apart from being the focus of the country's literature and theatre, Dublin is also the focal point for much of Irish art and the Irish artistic scene. The Book of Kells, a world-famous manuscript produced by Celtic Monks in AD 800 and an example of Insular art, is on display in Trinity College. The Chester Beatty Library houses the famous collection of manuscripts, miniature paintings, prints, drawings, rare books and decorative arts assembled by American mining millionaire (and honorary Irish citizen) Sir Alfred Chester Beatty (1875–1968). The collections date from 2700 BC onwards and are drawn from Asia, the Middle East, North Africa and Europe.
In addition public art galleries are found across the city, including the Irish Museum of Modern Art, the National Gallery, the Hugh Lane Municipal Gallery, the Douglas Hyde Gallery, the Project Arts Centre and the Royal Hibernian Academy. In recent years Dublin has become host to a thriving contemporary art scene. Some of the leading private galleries include Green on Red Gallery, Kerlin Gallery, Kevin Kavangh Gallery and Mother's Tankstation, each of which focuses on facilitating innovative, challenging and engaging contemporary visual art practice.
Three branches of the National Museum of Ireland are located in Dublin: Archaeology in Kildare Street, Decorative Arts and History in Collins Barracks and Natural History in Merrion Street. The same area is also home to many smaller museums such as Number 29 on Fitzwilliam Street and the Little Museum of Dublin on St. Stephen's Green. Dublin is home to the National College of Art and Design, which dates from 1746, and Dublin Institute of Design, founded in 1991.
Dublin has long been a city with a strong underground arts scene. Temple Bar was the home of many artists in the 1980s, and spaces such as the Project Arts Centre were hubs for collectives and new exhibitions. "The Guardian" noted that Dublin's independent and underground arts flourished during the economic recession of 2010. Dublin also has many acclaimed dramatic, musical and operatic companies, including Festival Productions, Lyric Opera Productions, the Pioneers' Musical & Dramatic Society, the Glasnevin Musical Society, Second Age Theatre Company, Opera Theatre Company and Opera Ireland. Ireland is well known for its love of baroque music, which is highly acclaimed at Trinity College.
Dublin was shortlisted to be World Design Capital 2014. Taoiseach Enda Kenny was quoted to say that Dublin "would be an ideal candidate to host the World Design Capital in 2014".
Entertainment.
Dublin has a vibrant nightlife and is reputedly one of Europe's most youthful cities, with an estimate of 50% of citizens being younger than 25. There are many pubs across the city centre, with the area around St. Stephen's Green and Grafton Street, especially Harcourt Street, Camden Street, Wexford Street and Leeson Street, having the most popular nightclubs and pubs.
The best known area for nightlife is Temple Bar, south of the River Liffey. The area has become popular among tourists, including stag and hen parties from Britain. It was developed as Dublin's cultural quarter and does retain this spirit as a centre for small arts productions, photographic and artists' studios, and in the form of street performers and small music venues. However, it has been criticised as overpriced, false and dirty by Lonely Planet. In 2014, Temple Bar was listed by the Huffington Post as one of the ten most disappointing destinations in the world. The areas around Leeson Street, Harcourt Street, South William Street and Camden/George's Street are popular nightlife spots for locals.
Live music is popularly played on streets and at venues throughout Dublin in general, and the city has produced several musicians and groups of international success, including U2, one member of Westlife, the Dubliners, the Thrills, Horslips, Jedward, the Boomtown Rats, Boyzone, Ronan Keating, Thin Lizzy, Paddy Casey, Sinéad O'Connor, the Script and My Bloody Valentine. The two best known cinemas in the city centre are the Savoy Cinema and the Cineworld Cinema, both north of the Liffey. Alternative and special-interest cinema can be found in the Irish Film Institute in Temple Bar, in the Screen Cinema on d'Olier Street and in the Lighthouse Cinema in Smithfield. Large modern multiscreen cinemas are located across suburban Dublin. The 3Arena venue in the Dublin Docklands has played host to many world renowned performers.
Shopping.
Dublin is a popular shopping destination for both locals and tourists. The city has numerous shopping districts, particularly around Grafton Street and Henry Street. The city centre is also the location of large department stores, most notably Arnotts, Brown Thomas and Clerys.
The city retains a thriving market culture, despite new shopping developments and the loss of some traditional market sites. Amongst several historic locations, Moore Street remains one of the city's oldest trading districts. There has also been a significant growth in local farmers' markets and other markets. In 2007, Dublin Food Co-op relocated to a larger warehouse in The Liberties area, where it is home to many market and community events. Suburban Dublin has several modern retail centres, including Dundrum Town Centre, Blanchardstown Centre, the Square in Tallaght, Liffey Valley Shopping Centre in Clondalkin, Omni Shopping Centre in Santry, Nutgrove Shopping Centre in Rathfarnham, and Pavilions Shopping Centre in Swords.
Restaurants.
As of 2014, Restaurant Patrick Guilbaud is the only two-Michelin starred restaurant along with four one-Michelin starred restaurants; (Bon Appétit, Chapter One, L'Ecrivain and Thornton's Restaurant).
Dublin also has a variety of ethnic restaurants.
Media.
Dublin is the centre of both media and communications in Ireland, with many newspapers, radio stations, television stations and telephone companies based there. RTÉ is Ireland's national state broadcaster, and is based in Donnybrook. Fair City is RTÉ's soap opera, located in the fictional Dublin suburb of "Carraigstown". TV3 and Setanta Sports are also based in the city. The headquarters of An Post and telecommunications companies such as Eircom, as well as mobile operators Meteor, Vodafone, O2 and 3 are all located there. Dublin is also the headquarters of important national newspapers such as "The Irish Times" and "Irish Independent", as well as local newspapers such as "The Evening Herald".
As well as being home to RTÉ Radio, Dublin also hosts the national radio networks Today FM and Newstalk, and numerous local stations. Commercial radio stations based in the city include 4fm (94.9 MHz), 98FM (98.1 MHz), Radio Nova 100FM (100.3 MHz), Q102 (102.2 MHz), Spin 1038 (103.8 MHz), FM104 (104.4 MHz), TXFM (105.2 MHz) and Sunshine 106.8 (106.8 MHz). There are also numerous community and special interest stations, including Dublin City FM (103.2 MHz), Dublin South FM (93.9 MHz), Liffey Sound FM (96.4 MHz), Near FM (90.3 MHz), Phoenix FM (92.5 MHz), Raidió na Life (106.4 MHz) and West Dublin Access Radio (96.0 MHz).
Sport.
Croke Park is the largest sports stadium in Ireland. The headquarters of the Gaelic Athletic Association, it has a capacity of 84,500. It is the fourth largest stadium in Europe after Nou Camp in Barcelona, Wembley Stadium in London and Santiago Bernabéu Stadium in Madrid. It hosts the premier Gaelic football and hurling games, international rules football and irregularly other sporting and non-sporting events including concerts. During the redevelopment of Lansdowne Road it played host to the Irish Rugby Union Team and Republic of Ireland national football team as well as hosting the Heineken Cup rugby 2008–09 semi-final between Munster and Leinster which set a world record attendance for a club rugby match. The Dublin GAA team plays most of their home league hurling and Gaelic Football games at Parnell Park.
I.R.F.U. Stadium Lansdowne Road was laid out in 1874. This was the venue for home games of both the Irish Rugby Union Team and the Republic of Ireland national football team. A joint venture between the Irish Rugby Football Union, the FAI and the Government, saw it redeveloped into a new state-of-the-art 51,500 seat Aviva Stadium, which opened in May 2010. Aviva Stadium hosted the 2011 UEFA Europa League Final. Rugby union team Leinster Rugby play their competitive home games in the RDS Arena & the Aviva Stadium while Donnybrook Stadium hosts their friendlies and A games, Ireland A and Women, Leinster Schools and Youths and the home club games of All Ireland League clubs Old Wesley and Bective Rangers. County Dublin is home for 13 of the senior rugby union clubs in Ireland including 5 of the 10 sides in the top division 1A.
County Dublin is home to five League of Ireland association clubs; Bohemians F.C, Shamrock Rovers, St Patrick's Athletic, University College Dublin and Shelbourne. Current premier division League Champions are St Patrick's Athletic. The first Irish side to reach the group stages of a European competition (2011–12 UEFA Europa League group stage) are Shamrock Rovers who play at Tallaght Stadium in South Dublin. Bohemians F.C play at Dalymount Park, St Patrick's Athletic play at Richmond Park, University College Dublin play their home games at the UCD Bowl in Dún Laoghaire–Rathdown, while Shelbourne is based at Tolka Park. Tolka Park, Dalymount Park, UCD Bowl and Tallaght Stadium, along with the Carlisle Grounds in Bray, hosted all Group 3 games in the intermediary round of the 2011 UEFA Regions' Cup.
The Dublin Marathon has been run since 1980 on the last Monday in October. The Women's Mini Marathon has been run since 1983 on the first Monday in June, which is also a bank holiday in Ireland. It is said to be the largest all female event of its kind in the world.
The Dublin area has several race courses including Shelbourne Park and Leopardstown. The Dublin Horse Show takes place at the RDS, which hosted the Show Jumping World Championships in 1982. The national boxing arena is located in The National Stadium on the South Circular Road. The National Basketball Arena is located in Tallaght, is the home of the Irish basketball team, is the venue for the basketball league finals and has also hosted Boxing and Wrestling events. The National Aquatic Centre in Blanchardstown is Ireland's largest indoor water leisure facility. Dublin has two ODI Cricket grounds in Castle Avenue, Clontarf and Malahide Cricket Club and College Park has Test status and played host to Ireland's only Test cricket match to date, a women's match against Pakistan in 2000. There are also Gaelic Handball, hockey and athletics stadia, most notably Morton Stadium in Santry, which held the athletics events of the 2003 Special Olympics.
Irish language.
There are 10,469 students in the Dublin region attending the 31 gaelscoileanna (Irish-language primary schools) and 8 gaelcholáistí (Irish-language secondary schools). Dublin has the highest number of Irish-medium schools in the country. There may be also up to another 10,000 Gaeltacht speakers living in Dublin. Two Irish language radio stations Raidió Na Life and RTÉ Raidió na Gaeltachta both have studios in the city, and the online and DAB station Raidió Rí-Rá broadcasts from studios in the city. Many other radio stations in the city broadcast at least an hour of Irish language programming per week. Many Irish language agencies are also located in the capital. Conradh na Gaeilge offers language classes, has a book shop and is a regular meeting place for different groups. The closest Gaeltacht to Dublin is the County Meath Gaeltacht of Ráth Cairn and Baile Ghib which is away.
Twinning.
Dublin is twinned with the following places:
The city is also in talks to twin with Rio de Janeiro, and Mexican city Guadalajara.

</doc>
<doc id="8506" url="http://en.wikipedia.org/wiki?curid=8506" title="DirectX">
DirectX

Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming and video, on Microsoft platforms. Originally, the names of these APIs all began with Direct, such as Direct3D, DirectDraw, DirectMusic, DirectPlay, DirectSound, and so forth. The name Direct"X" was coined as shorthand term for all of these APIs (the X standing in for the particular API names) and soon became the name of the collection. When Microsoft later set out to develop a gaming console, the X was used as the basis of the name Xbox to indicate that the console was based on DirectX technology. The X initial has been carried forward in the naming of APIs designed for the Xbox such as XInput and the Cross-platform Audio Creation Tool (XACT), while the DirectX pattern has been continued for Windows APIs such as Direct2D and DirectWrite.
Direct3D (the 3D graphics API within DirectX) is widely used in the development of video games for Microsoft Windows, Sega Dreamcast, Microsoft Xbox, Microsoft Xbox 360, and Microsoft Xbox One. Direct3D is also used by other software applications for visualization and graphics tasks such as CAD/CAM engineering. As Direct3D is the most widely publicized component of DirectX, it is common to see the names "DirectX" and "Direct3D" used interchangeably.
The DirectX software development kit (SDK) consists of runtime libraries in redistributable binary form, along with accompanying documentation and headers for use in coding. Originally, the runtimes were only installed by games or explicitly by the user. Windows 95 did not launch with DirectX, but DirectX was included with Windows 95 OEM Service Release 2. Windows 98 and Windows NT 4.0 both shipped with DirectX, as has every version of Windows released since. The SDK is available as a free download. While the runtimes are proprietary, closed-source software, source code is provided for most of the SDK samples. Starting with the release of Windows 8 Developer Preview, DirectX SDK has been integrated into Windows SDK.
Direct3D 9Ex, Direct3D 10, and Direct3D 11 are only available for Windows Vista and newer because each of these new versions was built to depend upon the new Windows Display Driver Model that was introduced for Windows Vista. The new Vista/WDDM graphics architecture includes a new video memory manager supporting virtualization of graphics hardware for various applications and services like the Desktop Window Manager.
Development history.
In late 1994, Microsoft was ready to release Windows 95, its next operating system. An important factor in the value consumers would place on it was the programs that would be able to run on it. Three Microsoft employees – Craig Eisler, Alex St. John, and Eric Engstrom – were concerned because programmers tended to see Microsoft's previous operating system, MS-DOS, as a better platform for game programming, meaning few games would be developed for Windows 95 and the operating system would not be as much of a success.
DOS allowed direct access to video cards, keyboards, mice, sound devices, and all other parts of the system, while Windows 95 - with its protected memory model - restricted access to all of these, working on a much more standardized model. Microsoft needed a quick solution for programmers; the operating system was only months away from being released. Eisler (development lead), St. John, and Engstrom (program manager) worked together to fix this problem, with a solution that they eventually named DirectX.
The first version of DirectX was released in September 1995 as the Windows Games SDK. It was the Win32 replacement for the DCI and WinG APIs for Windows 3.1. DirectX allowed all versions of Microsoft Windows, starting with Windows 95, to incorporate high-performance multimedia. Eisler wrote about the frenzy to build DirectX 1 through 5 in his blog.
Initial adoption of DirectX by game developers was slow. There were fears that DirectX could be replaced as WinG had been, there was a performance penalty in using Windows over DOS, and there were many die-hard DOS programmers.
DirectX 2.0 became a component of Windows itself with the releases of Windows 95 OSR2 and Windows NT 4.0 in mid-1996. Since Windows 95 was itself still new and few games had been released for it, Microsoft engaged in heavy promotion of DirectX to developers who were generally distrustful of Microsoft's ability to build a gaming platform in Windows. Alex St. John, the evangelist for DirectX, staged an elaborate event at the 1996 Computer Game Developers Conference which game developer Jay Barnson described as a Roman theme, including real lions, togas, and something resembling an indoor carnival. It was at this event that Microsoft first introduced Direct3D and DirectPlay, and demonstrated multiplayer "MechWarrior 2" being played over the Internet.
The DirectX team faced the challenging task of testing each DirectX release against an array of computer hardware and software. A variety of different graphics cards, audio cards, motherboards, CPUs, input devices, games, and other multimedia applications were tested with each beta and final release. The DirectX team also built and distributed tests that allowed the hardware industry to confirm that new hardware designs and driver releases would be compatible with DirectX.
Prior to DirectX, Microsoft had included OpenGL on their Windows NT platform. At the time, OpenGL required "high-end" hardware and was focused on engineering and CAD uses. Direct3D was intended to be a lightweight partner to OpenGL, focused on game use. As 3D gaming grew, OpenGL developed to include better support for programming techniques for interactive multimedia applications like games, giving developers choice between using OpenGL or Direct3D as the 3D graphics API for their applications. At that point a "battle" began between supporters of the cross-platform OpenGL and the Windows-only Direct3D. Incidentally, OpenGL was supported at Microsoft by the DirectX team. If a developer chose to use OpenGL 3D graphics API, the other APIs of DirectX are often combined with OpenGL in computer games because OpenGL does not include all of DirectX's functionality (such as sound or joystick support).
In a console-specific version, DirectX was used as a basis for Microsoft's Xbox and Xbox 360 console API. The API was developed jointly between Microsoft and Nvidia, who developed the custom graphics hardware used by the original Xbox. The Xbox API is similar to DirectX version 8.1, but is non-updateable like other console technologies. The Xbox was code named DirectXbox, but this was shortened to Xbox for its commercial name.
In 2002, Microsoft released DirectX 9 with support for the use of much longer shader programs than before with pixel and vertex shader version 2.0. Microsoft has continued to update the DirectX suite since then, introducing shader model 3.0 in DirectX 9.0c, released in August 2004. 
As of April 2005, DirectShow was removed from DirectX and moved to the Microsoft Platform SDK instead.
DirectX has been confirmed to be present in Microsoft's Windows Phone 8.
DirectX 10.
A major update to DirectX API, DirectX 10 ships with and is only available with Windows Vista and later; previous versions of Windows such as Windows XP are not able to run DirectX 10-exclusive applications. Rather, programs that are run on a Windows XP system with DirectX 10 hardware simply resort to the DirectX 9.0c code path, the latest available for Windows XP computers, although there are unofficial projects to port DirectX 10 to Windows XP.
Changes for DirectX 10 were extensive. Many former parts of DirectX API were deprecated in the latest DirectX SDK and are preserved for compatibility only: DirectInput was deprecated in favor of XInput, DirectSound was deprecated in favor of the Cross-platform Audio Creation Tool system (XACT) and additionally lost support for hardware accelerated audio, since the Vista audio stack renders sound in software on the CPU. The DirectPlay DPLAY.DLL was also removed and was replaced with dplayx.dll; games that rely on this DLL must duplicate it and rename it to dplay.dll.
In order to achieve backwards compatibility, DirectX in Windows Vista contains several versions of Direct3D:
Direct3D 10.1 is an incremental update of Direct3D 10.0 which shipped with, and required, Windows Vista Service Pack 1. This release mainly sets a few more image quality standards for graphics vendors, while giving developers more control over image quality. It also adds support for cube map arrays, separate blend modes per-MRT, coverage mask export from a pixel shader, ability to run pixel shader per sample, access to multi-sampled depth buffers and requires that the video card supports Shader Model 4.1 or higher and 32-bit floating-point operations. Direct3D 10.1 still fully supports Direct3D 10 hardware, but in order to utilize all of the new features, updated hardware is required.
DirectX 11.
Microsoft unveiled DirectX 11 at the Gamefest 08 event in Seattle, with the major scheduled features including GPGPU support (DirectCompute), and Direct3D 11 with tessellation support and improved multi-threading support to assist video game developers in developing games that better utilize multi-core processors. Direct3D 11 runs on Windows Vista, Windows 7 and Windows 8. Parts of the new API such as multi-threaded resource handling can be supported on Direct3D 9/10/10.1-class hardware. Hardware tessellation and Shader Model 5.0 require Direct3D 11 supporting hardware. Microsoft has since released the Direct3D 11 Technical Preview. Direct3D 11 is a strict superset of Direct3D 10.1 — all hardware and API features of version 10.1 are retained, and new features are added only when necessary for exposing new functionality. This helps to keep backwards compatibility with previous versions of DirectX.
Microsoft released the Final Platform Update for Windows Vista on October 27, 2009, which was 5 days after the initial release of Windows 7 (launched with Direct3D 11 as a base standard).
DirectX 11.1 is included in Windows 8. It supports WDDM 1.2 for increased performance, features improved integration of Direct2D (now at version 1.1), Direct3D, and DirectCompute, and includes DirectXMath, XAudio2, and XInput libraries from the XNA framework. It also features stereoscopic 3D support for gaming and video. DirectX 11.1 was also partially backported to Windows 7, via the Windows 7 platform update.
DirectX 11.2 is included in Windows 8.1 (including the RT version) and Windows Server 2012 R2. It added some new features to Direct2D like geometry realizations. It also added swap chain composition, which allows some elements of the scene to be rendered at lower resolutions and then composted via hardware overlay with other parts rendered at higher resolution.
DirectX 11.X is a superset of DirectX 11.2 running on the Xbox One. It actually includes some features, such as draw bundles, that were later announced as part of DirectX 12.
DirectX 12.
DirectX 12 has been announced by Microsoft at GDC on the 20th of March 2014 and is targeted for Holiday 2015 games. The main goal of Direct3D 12 is to reduce driver overhead, similarly to AMD's Mantle (API); in the words of lead developer Max McMullen, the goal is to achieve "console-level efficiency". Windows 10 will ship with DirectX 12.
DirectX 12 will be essentially supported on all Fermi and later Nvidia GPUs, on AMD's GCN-based chips and on Intel's Haswell and later processors' graphics units.
At SIGGRAPH 2014, Intel released a demo showing a computer generated asteroid field, in which DirectX 12 was claimed to be 50%-70% more efficient than DirectX 11 in rendering speed and CPU power consumption.
Logos.
The original logo resembled a deformed radiation warning symbol. Controversially, the original name for the DirectX project was the "Manhattan Project", a reference to the US nuclear weapons initiative. Alex St. John, head of Microsoft DirectX evangelism at the time, claims that the connotation of the ultimate outcome of the Manhattan Project (the nuclear bombing of Japan) is intentional, and that DirectX and its sister project, the Xbox (which shares a similar logo), were meant to displace Japanese videogame-makers from their dominance of the video-game industry. However, Microsoft publicly denies this account, instead claiming that the logo is merely an artistic design.
Components.
Microsoft encourages the use of these DirectX components:
Microsoft has deprecated, but still supports, these DirectX components:
DirectX functionality is provided in the form of COM-style objects and interfaces. Additionally, while not DirectX components themselves, managed objects have been built on top of some parts of DirectX, such as Managed Direct3D and the XNA graphics library on top of Direct3D 9.
Compatibility.
Various releases of Windows have included and supported various versions of DirectX, allowing newer versions of the operating system to continue running applications designed for earlier versions of DirectX until those versions can be gradually phased out in favor of newer APIs, drivers, and hardware.
APIs such as Direct3D and DirectSound need to interact with hardware, and they do this through a device driver. Hardware manufacturers have to write these drivers for a particular DirectX version's device driver interface (or DDI), and test each individual piece of hardware to make them DirectX compatible. Some hardware devices have only DirectX compatible drivers (in other words, one must install DirectX in order to use that hardware). Early versions of DirectX included an up-to-date library of all of the DirectX compatible drivers currently available. This practice was stopped however, in favor of the web-based Windows Update driver-update system, which allowed users to download only the drivers relevant to their hardware, rather than the entire library.
Prior to DirectX 10, DirectX runtime was designed to be "backward compatible" with older drivers, meaning that newer versions of the APIs were designed to interoperate with older drivers written against a previous version's DDI. The application programmer had to query the available hardware capabilities using a complex system of "cap bits" each tied to a particular hardware feature. Direct3D 7 and earlier would work on any version of the DDI, Direct3D 8 requires a minimum DDI level of 6 and Direct3D 9 requires a minimum DDI level of 7.
However, the Direct3D 10 runtime in Windows Vista cannot run on older hardware drivers due to the significantly updated DDI, which requires a unified feature set and abandons the use of "cap bits".
Direct3D 10.1 introduces "feature levels" 10_0 and 10_1, which allow use of only the hardware features defined in the specified version of Direct3D API. Direct3D 11 adds level 11_0 and "10 Level 9" - a subset of the Direct3D 10 API designed to run on Direct3D 9 hardware, which has three feature levels (9_1, 9_2 and 9_3) grouped by common capabilities of "low", "med" and "high-end" video cards; the runtime directly uses Direct3D 9 DDI provided in all WDDM drivers. Feature level 11_1 has been introduced with Direct3D 11.1.
.NET Framework.
In 2002, Microsoft released a version of DirectX compatible with the Microsoft .NET Framework, thus allowing programmers to take advantage of DirectX functionality from within .NET applications using compatible languages such as managed C++ or the use of the C# programming language. This API was known as "Managed DirectX" (or MDX for short), and claimed to operate at 98% of performance of the underlying native DirectX APIs. In December 2005, February 2006, April 2006, and August 2006, Microsoft released successive updates to this library, culminating in a beta version called Managed DirectX 2.0. While Managed DirectX 2.0 consolidated functionality that had previously been scattered over multiple assemblies into a single assembly, thus simplifying dependencies on it for software developers, development on this version has subsequently been discontinued, and it is no longer supported. The Managed DirectX 2.0 library expired on October 5, 2006.
During the GDC 2006, Microsoft presented the XNA Framework, a new managed version of DirectX (similar but not identical to Managed DirectX) that is intended to assist development of games by making it easier to integrate DirectX, High Level Shader Language (HLSL) and other tools in one package. It also supports the execution of managed code on the Xbox 360. The XNA Game Studio Express RTM was made available on December 11, 2006, as a free download for Windows XP. Unlike the DirectX runtime, Managed DirectX, XNA Framework or the Xbox 360 APIs (XInput, XACT etc.) have not shipped as part of Windows. Developers are expected to redistribute the runtime components along with their games or applications.
No Microsoft product including the latest XNA releases provides DirectX 10 support for the .NET Framework.
The other approach for DirectX in managed languages is to use third-party libraries like SlimDX for Direct3D. , DirectInput (including Direct3D 10), for DirectShow subset or which is an open source library from Microsoft.
Alternatives.
There are alternatives to the DirectX family of APIs, with OpenGL and Mantle having the most features comparable to Direct3D. Examples of other APIs include SDL, Allegro, OpenMAX, OpenML, OpenAL, OpenCL, FMOD, SFML etc. Many of these libraries are cross-platform or have open codebases. There are also alternative implementations that aim to provide the same API, such as the one in Wine. Furthermore, the developers of ReactOS are trying to reimplement DirectX under the name "ReactX".

</doc>
<doc id="8508" url="http://en.wikipedia.org/wiki?curid=8508" title="Slalom skiing">
Slalom skiing

Slalom is an alpine skiing and alpine snowboarding discipline, involving skiing between poles (gates) spaced much closer together than in giant slalom, super giant slalom (super-G) or downhill, necessitating quicker and shorter turns.
Slalom and giant slalom (GS) are the technical events of alpine ski racing. This category separates them from the speed events of super-G and downhill.
Slalom skiing may also refer to waterskiing with only one ski. Similar to the alpine version, the skier must pass around buoys on either side of the tow boat.
Origins.
The word "slalom" is from the Morgedal/Seljord dialect of Norwegian slalåm: "sla," meaning slightly inclining hillside, and "låm," meaning track after skis. The inventors of modern skiing classified their trails according to their difficulty. "Slalåm" was a trail used in Telemark by boys and girls not yet able to try themselves on the more challenging runs. "Ufsilåm" was a trail with one obstacle ("ufse") like a jump, a fence, a difficult turn, a gorge, a cliff (often more than high) and more. "Uvyrdslåm" was a trail with several obstacles.
Proper definition.
Slalom and giant slalom make up the main technical events in alpine ski racing. This category separates them from the speed events of super-G and downhill.
A course is constructed by laying out a series of gates. Gates are formed by alternating pairs of red and blue poles. The skier must pass between the two poles forming the gate, with the tips of both skis and the skier's feet passing between the poles. A course has 55 to 75 gates for men and forty to sixty gates for women. The vertical drop for a men's course is and slightly less for women.
For slalom, the vertical offset between gates is around and the horizontal offset around , although these figures have changed in recent times because of significant technical developments in ski equipment (namely, increased sidecut) that have revolutionized the sport. The gates are arranged in a variety of different configurations to challenge the competitor, including delay gates and vertical combinations known as hairpins and flushes. A hairpin is a series of gates including two gates with one closing gate. A flush is a series of gates including three or more gates with one closing gate. The worldwide governing body, FIS (Federation Internationale de Ski) has a set of regulations detailing what configurations are allowed or mandated for an official course.
Because the offsets are relatively small in slalom, ski racers take a fairly direct line and often knock the poles out of the way as they pass, which is known as blocking. (The main blocking technique in modern slalom is cross-blocking, in which the skier takes such a tight line and angulates so strongly that he or she is able to block the gate with the outside hand.) In modern slalom, a variety of protective equipment is used such as shin pads, hand guards, helmets and face guards.
History.
The rules for the modern slalom were developed by Sir Arnold Lunn in 1922 for the British National Ski Championships, tried by the FIS in 1928, and adopted for the 1936 Winter Olympics. Under his rules, the gates were marked by pairs of flags rather than single ones, were arranged so that the racers had to use a variety of turn lengths to negotiate them, and scoring was on the basis of time alone, not time and style.
Innovation and rule changes.
In the early 1980s, bamboo poles (also called "gates") were replaced by hard plastic poles, hinged at the base with a rubber universal joint, invented in 1979 by Peter Laehy and Stefan Dag of Aspen-based Rapidgate, Inc. The rigid nature of the old-style bamboo gates had forced skiers to maneuver their entire body around each gate, while the hinged gates require only that the skis and boots of the skier (as the FIS rules state) go around each gate. The older gates commonly came out of the snow and had to be returned by course workers; the freed gate was often a moving obstacle and hazard for the racers. The new gates allowed a much more direct path down a slalom course through the process of "cross-blocking" or "shinning" the gates. Cross-blocking is a technique in which the legs go around the gate with the upper body inclined toward, or even across, the gate; in this case the racer's outside pole and shinguards hit the gate, knocking it down and out of the way. In the early 1990s, flags were removed completely from slalom gates in international competition.
Equipment.
With the innovation of shaped skis around the turn of the 21st century, equipment used for slalom in international competition changed drastically. World Cup skiers commonly skied on slalom skis at a length of in the 1980s and 1990s but by the 2002 Winter Olympics in Salt Lake City, the majority of competitors were using skis measuring or less.
The downside of the shorter skis was that athletes found that recoveries were more difficult with a smaller platform underfoot. Over concern for the safety of athletes, the FIS began to set minimum ski lengths for international slalom competition. The minimum was initially set at for men and for women, but was increased to for men and for women for the 2003-2004 season.
American Bode Miller hastened the shift to the shorter, more radical sidecut skis when he achieved unexpected success after becoming the first Junior Olympic athlete to adopt the equipment in giant slalom and super G in 1996. A few years later, the technology was adapted to slalom skis as well.

</doc>
<doc id="8518" url="http://en.wikipedia.org/wiki?curid=8518" title="Dachshund">
Dachshund

The dachshund ( or or ;) is a short-legged, long-bodied dog breed belonging to the hound family. The standard size dachshund was bred to scent, chase, and flush out badgers and other burrow-dwelling animals, while the miniature dachshund was developed to hunt smaller prey such as rabbits. In the American West they have also been used to hunt prairie dogs. Today, they are bred for conformation shows and as family pets. Some dachshunds participate in earthdog trials. According to the AKC, the dachshund continues to remain one of the top 10 dog breeds in the United States of America.
Etymology.
The name "dachshund" is of German origin and literally means "badger dog", from "Dachs" ("badger") and "Hund" ("dog"). The pronunciation varies widely in English: variations of the first and second syllables include , , and , , . Although "dachshund" is a German word, in modern German they are more commonly known by the name Dackel or, among hunters, Teckel. The German word is pronounced .
Because of their long, narrow build, they are often nicknamed wiener dog or sausage dog.
Classification.
While classified in the hound group or scent hound group in the United States and Great Britain, there are some who consider this classification to be arguable, speculating that it arose from the fact that the word "Hund" is similar to the English word "hound". Many dachshunds, especially the wire-haired subtype, may exhibit behavior and appearance that are similar to that of the terrier group of dogs. An argument can be made for the scent (or hound) group classification because the breed was developed to use scent to trail and hunt animals, and probably descended from scent hounds, such as bloodhounds, pointers, Basset Hounds, or even Bruno Jura Hounds; but with the persistent personality and love for digging that probably developed from the terrier, it can also be argued that they could belong in the terrier, or "earth dog", group. In the Fédération Cynologique Internationale (World Canine Federation), or FCI, the dachshund is actually in its own group, Group 4, which is the dachshund group. Part of the controversy is because the dachshund is the only certifiable breed of dog to hunt both above and below ground.
Characteristics.
Appearance.
A typical dachshund is long-bodied and muscular, with short, stubby legs. Its front paws are unusually large and paddle-shaped, for extreme digging. Long coated dachshunds have a silky coat and short featherings on legs and ears. It has skin that is loose enough not to tear while tunneling in tight burrows to chase prey. The dachshund has a deep chest that provides increased lung capacity for stamina when hunting prey underground. Its snout is long with an increased nose area that absorbs odors.
Coat and color.
Dachshunds exhibit three coat varieties: smooth coat (short hair), long hair, and wire-hair. Wirehaired is the least commonly seen coat in the US (it is the most common in Germany) and the most recent coat to appear in breeding standards.
Dachshunds have a wide variety of colors and patterns. They can be single-colored, single-colored with spots ("dappled"-called "merle" in other dog breeds), and single-colored with tan points plus any pattern. Dachshunds in the same litter may be born in different coat colors. Dachshunds also come in piebald. The dominant color is red, the most common along with black and tan. Two-colored dogs can be black, wild boar, chocolate, fawn, with tan "points", or markings over the eyes, ears, paws, and tail, of tan or cream. A two-colored dachshund would be called by its dominant color first followed by the point color, such as "black and tan" or "chocolate and cream". Other patterns include piebald, in which a white pattern is imposed upon the base color or any other pattern, and a lighter "boar" red. The reds range from coppers to deep rusts, with or without somewhat common black hairs peppered along the back, face and ear edges, lending much character and an almost burnished appearance; this is referred to among breeders and enthusiasts as a "stag" or an "overlay" or "sable". True sable is a dachshund with each single hair banded with three colors: light at the base of the hair, red in the middle, black at the end. An additional striking coat marking is the brindle pattern. "Brindle" refers to dark stripes over a solid background—usually red. If a dachshund is brindled on a dark coat and has tan points, it will have brindling on the tan points only. Even one single, lone stripe of brindle is a brindle. If a dachshund has one single spot of dapple, it is a dapple.
Solid black and solid chocolate dachshunds occur, and even though dogs with such coloration are often considered handsome, the colors are nonstandard, that is, the dogs are frowned upon in the conformation ring in the US and Canada. Chocolate is commonly confused with dilute red. Additionally, according to the conformation judges of the Dachshund Club of America (DCA) and the American Kennel Club (AKC) the piebald pattern is nonstandard. However, the piebald dachshund can still be shown. The only disqualifying fault in Dachshunds is knuckling over. While some judges choose to dismiss a dog of color, many choose to judge them and those who are actually judging the dog will look past the cosmetic color of a dog and judge the conformation of the dog first. There were several piebald dachshunds that became AKC Champions in 2008. All things being equal between the dogs in the ring, the traditional colors which are listed in the Official AKC Standard (governed by DCA) should be visibly listed.
Dogs that are double-dappled have the merle pattern of a dapple, but with distinct white patches that occur when the dapple gene expresses itself twice in the same area of the coat. The DCA excluded the wording "double-dapple" from the standard in 2007 and now strictly use the wording "dapple" as the double dapple gene is commonly responsible for blindness and deafness.
Breeders may also breed a piebald dapple brindle; and although dogs with this coloring are increasingly popular due to their unique markings, they are not considered standard and are not allowed to show.
Size.
Dachshunds come in three sizes: standard, miniature, and "kaninchen" (German for "rabbit"). Although the standard and miniature sizes are recognized almost universally, the rabbit size is not recognized by clubs in the United States and the United Kingdom, but is recognized by all of the clubs within the Fédération Cynologique Internationale (World Canine Federation) (FCI), which contain kennel clubs from 83 countries all over the world. An increasingly common size for family pets falls between the miniature and the standard size, frequently referred to as "tweenies," not an official classification.
A full-grown standard dachshund averages to , while the miniature variety normally weighs less than . The kaninchen weighs to . According to kennel club standards, the miniature (and kaninchen, where recognized) differs from the full-size only by size and weight, thus offspring from miniature parents must never weigh more than the miniature standard to be considered a miniature as well. While many kennel club size divisions use weight for classification, such as the American Kennel Club, other kennel club standards determine the difference between the miniature and standard by chest circumference; some kennel clubs, such as in Germany, even measure chest circumference in addition to height and weight.
H. L. Mencken said that "A dachshund is a half-dog high and a dog-and-a-half long," although they have been referred to as "two dogs long". This characteristic has led them to be quite a recognizable breed, and they are featured in many a joke and cartoon, particularly "The Far Side" by Gary Larson.
Eye color.
Light-colored dachshunds can sport amber, light brown, or green eyes; however, kennel club standards state that the darker the eye color, the better. They can also have eyes of two different colors; however, this is only found in dapple and double dapple dachshunds. Dachshunds can have a blue and a brown eye. Blue eyes, partially blue eyes, or a blue eye and a brown eye are called "wall" coloring, and are considered a non-desirable trait in kennel club standards. Dappled eyes are also possible.
Wall-eye is permissible according to DCA standards. Piebald-patterned dachshunds will never have blue in their eyes, unless the dapple pattern is present.
Temperament.
Dachshunds are playful, but as hunting dogs can be quite stubborn, and are known for their propensity for chasing small animals, birds, and tennis balls with great determination and ferocity. Many dachshunds are stubborn, making them a challenge to train.
Dachshunds are statistically more aggressive to both strangers and other dogs. Despite this, they are rated in the intelligence of dogs as an average working dog with a persistent ability to follow trained commands 50% of the time or more. They rank 49th in Stanley Coren's "Intelligence of Dogs", being of average working and obedience intelligence.
They can have a loud bark. Some bark quite a lot and may need training to stop, while others will not bark much at all. Dachshunds are known for their devotion and loyalty to their owners, though they can be standoffish towards strangers. If left alone, many dachshunds will whine until they have companionship. Like many dogs if left alone too frequently, some dachshunds are prone to separation anxiety and may chew objects in the house to relieve stress.
Dachshunds are burrowers by nature and are likely to burrow in blankets and other items around the house, when bored or tired.
Dachshunds can be difficult to housebreak, and patience and consistency is often needed in this endeavor.
According to the American Kennel Club's breed standards, "the dachshund is clever, lively and courageous to the point of rashness, persevering in above and below ground work, with all the senses well-developed. Any display of shyness is a serious fault." Their temperament and body language give the impression that they do not know or care about their relatively small size. Like many small hunting dogs, they will challenge a larger dog. Indulged dachshunds may become snappy or extremely obstinate.
Many dachshunds do not like unfamiliar people, and many will growl or bark at them. Although the dachshund is generally an energetic dog, some are sedate. This dog's behavior is such that it is not the dog for everyone. A bored, untrained dachshund will become destructive. If raised improperly and not socialized at a young age, dachshunds can become aggressive or fearful. They require a caring, loving owner who understands their need for entertainment and exercise.
Dachshunds may not be the best pets for small children. Like any dog, dachshunds need a proper introduction at a young age. Well trained Dachshunds and well behaved children usually get along fine. Otherwise, they may be aggressive and bite an unfamiliar child, especially one that moves quickly around them or teases them. However, many Dachshunds are very tolerant and loyal to children within their family, but these children should be mindful of the vulnerability of the breed's back.
A 2008 University of Pennsylvania study of 6,000 dog owners who were interviewed indicated that dogs of smaller breeds were more likely to be "genetically predisposed towards aggressive behaviour". Dachshunds were rated the most aggressive, with 20% having bitten strangers, as well as high rates of attacks on other dogs and their owners. The study noted that attacks by small dogs were unlikely to cause serious injuries and because of this were probably under-reported.
Health.
The breed is known to have spinal problems, especially intervertebral disk disease (IVDD), due in part to an extremely long spinal column and short rib cage. The risk of injury may be worsened by obesity, jumping, rough handling, or intense exercise, which place greater strain on the vertebrae. About 20–25% of Dachshunds will develop IVDD.
Treatment consists of combinations of crate confinement and courses of anti-inflammatory medications (steroids and non-steroidal anti-inflammatory drugs like carprofen and meloxicam), or chronic pain medications, like tramadol. Serious cases may require surgery to remove the troublesome disk contents. A dog may need the aid of a cart to get around if paralysis occurs.
A new minimally invasive procedure called "percutaneous laser disk ablation" has been developed at the Oklahoma State University Veterinary Hospital. Originally, the procedure was used in clinical trials only on dachshunds that had suffered previous back incidents. Since dachshunds are prone to back issues, the goal is to expand this treatment to dogs in a normal population.
In addition to back problems, the breed is also prone to patellar luxation which is where the kneecap can become dislodged. Dachshunds may also be affected by Osteogenesis imperfecta (brittle bone disease). The condition seems to be mainly limited to wire-haired Dachshunds, with 17% being carriers. A genetic test is available to allow breeders to avoid breeding carriers to carriers. In such pairings, each puppy will have a 25% chance of being affected.
In some double dapples, there are varying degrees of vision and hearing loss, including reduced or absent eyes. Not all double dapples have problems with their eyes and/or ears, which may include degrees of hearing loss, full deafness, malformed ears, congenital eye defects, reduced or absent eyes, partial or full blindness, or varying degrees of both vision and hearing problems; but heightened problems can occur due to the genetic process in which two dapple genes cross, particularly in certain breeding lines. Dapple genes, which are dominant genes, are considered "dilution" genes, meaning whatever color the dog would have originally carried is lightened, or diluted, randomly; two dominant "dilution" genes can cancel each other out, or "cross", removing all color and producing a white recessive gene, essentially a white mutation. When this happens genetically within the eyes or ears, this white mutation can be lethal to their development, causing hearing or vision problems.
Other dachshund health problems include hereditary epilepsy, granulomatous meningoencephalitis, dental issues, Cushing's syndrome, thyroid problems, various allergies and atopies, and various eye conditions including cataracts, glaucoma, progressive retinal atrophy, corneal ulcers, nonucerative corneal disease, sudden acquired retinal degeneration, and cherry eye. Dachshunds are also 2.5 times more likely than other breeds of dogs to develop patent ductus arteriosus, a congenital heart defect. Dilute color dogs (Blue, Isabella, and Cream) are very susceptible to Color Dilution Alopecia, a skin disorder that can result in hair loss and extreme sensitivity to sun. Since the occurrence and severity of these health problems is largely hereditary, breeders are working to eliminate these.
History.
 
Some writers and dachshund experts have theorized that the early roots of the dachshund go back to ancient Egypt, where engravings were made featuring short-legged hunting dogs. Recent discoveries by the American University in Cairo of mummified dachshund-like dogs from ancient Egyptian burial urns may lend credibility to this theory. In its modern incarnation, the dachshund is a creation of German breeders and includes elements of German, French, and English hounds and terriers. Dachshunds have been kept by royal courts all over Europe, including that of Queen Victoria, who was particularly enamored of the breed. They were originally bred for hunting badgers by trailing scent.
The first verifiable references to the dachshund, originally named the "Dachs Kriecher" ("badger crawler") or "Dachs Krieger" ("badger warrior"), came from books written in the early 18th century. Prior to that, there exist references to "badger dogs" and "hole dogs", but these likely refer to purposes rather than to specific breeds. The original German dachshunds were larger than the modern full-size variety, weighing between , and originally came in straight-legged and crook-legged varieties (the modern dachshund is descended from the latter). Though the breed is famous for its use in exterminating badgers and badger-baiting, dachshunds were also commonly used for rabbit and fox hunting, for locating wounded deer, and in packs were known to hunt game as large as wild boar and as fierce as the wolverine.
There are huge differences of opinion as to when dachshunds were specifically bred for their purpose of badger hunting, as the American Kennel Club states the dachshund was bred in the 15th century, while the Dachshund Club of America states that foresters bred the dogs in the 18th or 19th century.
Double-dapple dachshunds, which are prone to eye disease, blindness, or hearing problems, are generally believed to have been introduced to the United States between 1879 and 1885.
The flap-down ears and famous curved tail of the dachshund have deliberately been bred into the dog. In the case of the ears, this is to keep grass seeds, dirt, and other matter from entering the ear canal. The curved tail is dual-purposed: to be seen more easily in long grass and, in the case of burrowing dachshunds, to help haul the dog out if it becomes stuck in a burrow.
The smooth-haired dachshund, the oldest style, may be a cross between the German Shorthaired Pointer, a Pinscher, and a Bracke (a type of bloodhound), or to have been produced by crossing a short Bruno Jura Hound with a pinscher. Others believe it was a cross from a miniature French pointer and a pinscher; others claim that is was developed from the St. Hubert Hound, also a bloodhound, in the 18th century, and still others believe that they were descended from Basset Hounds, based upon their scent abilities and general appearance.
The exact origins of the dachshund are therefore unknown. According to William Loeffler, from " The American Book of the Dog (1891)", in the chapter on Dachshunds: "The origin of the Dachshund is in doubt, our best authorities disagreeing as to the beginning of the breed." What can be agreed on, however, is that the short-haired dachshund gave rise to both the long-haired and the wire-haired varieties.
There are two theories about how the standard longhair dachshund came about. One theory is that smooth Dachshunds would occasionally produce puppies which had slightly longer hair than their parents. By selectively breeding these animals, breeders eventually produced a dog which consistently produced longhair offspring, and the longhair dachshund was born. Another theory is that the standard longhair dachshund was developed by breeding smooth dachshunds with various land and water spaniels. The long-haired dachshund may be a cross among any of the small dog breeds in the spaniel group, including the German Stoberhund, and the smooth-haired dachshund.
The wire-haired dachshund, the last to develop, was bred in the late 19th century. There is a possibility the wire-haired dachshund was a cross between the smooth dachshund and various hard-coated terriers and wire-haired pinschers, such as the Schnauzer, the Dandie Dinmont Terrier, the German Wirehaired Pointer, or perhaps the Scottish Terrier.
Symbol of Germany.
Dachshunds have traditionally been viewed as a symbol of Germany. Political cartoonists commonly used the image of the dachshund to ridicule Germany. During World War I the dachshunds' popularity in the United States plummeted because of this association and there are even anecdotes such as a Dachshund being stoned to death on the high street of Berkhamsted, England at this time because of its association with the enemy . As a result they were often called "liberty hounds" by their owners similar to "liberty cabbage" becoming a term for sauerkraut. The stigma of the association was revived to a lesser extent during World War II, though it was comparatively short-lived. Kaiser Wilhelm II and German Field Marshal Erwin Rommel were known for keeping dachshunds.
Due to the association of the breed with Germany, as well as its popularity among dog keepers in Munich, the dachshund was chosen to be the first official mascot for the 1972 Summer Olympics in Munich, with the name Waldi.
Sports.
Some people train and enter their dachshund to compete in dachshund races, such as the Wiener Nationals. Several races across the United States routinely draw several thousand attendees, including races in Buda, Texas; Davis, California; Phoenix, Arizona; Los Alamitos, California; Findlay, Ohio; Milwaukee, Wisconsin; Oklahoma City, Oklahoma; Kansas City, Kansas; Palo Alto, California; and Shakopee, Minnesota. There is also an annual dachshund run in Kennywood, located in Pittsburgh, Pennsylvania, called the Wiener 100, and in Huntington, West Virginia called the Dachshund Dash.
Despite the popularity of these events, the Dachshund Club of America opposes "wiener racing", as many greyhound tracks use the events to draw large crowds to their facilities. The DCA is also worried about potential injuries to dogs, due to their predisposition to back injuries. Another favorite sport is earthdog trials, in which dachshunds enter tunnels with dead ends and obstacles attempting to locate an artificial bait or live but caged and protected rats.
"Dackel" versus "Teckel".
In Germany, dachshunds are widely called "Dackel" (both singular and plural). Among hunters, they are mainly referred to as "Teckel". There are kennels which specialize in breeding hunting dachshunds, the so-called "jagdliche Leistungszucht" ("hunting performance breed"), as opposed to breeding family dogs. Therefore it is sometimes believed that "Teckel" is either a name for the hunting breed or a mark for passing the test for a trained hunting dog (called "VGP", "Verband-Gebrauchsprüfung") in Germany. It is not.
Popularity.
Dachshunds are one of the most popular dogs in the United States, ranking 10th in the 2012 AKC registration statistics. They are popular with urban and apartment dwellers, ranking among the top ten most popular breeds in 76 of 190 major US cities surveyed by the AKC.
One will find varying degrees of organized local dachshund clubs in most major American cities, including New York, New Orleans, Portland, Los Angeles, and Chicago. The breed is most popular in Europe.

</doc>
<doc id="8519" url="http://en.wikipedia.org/wiki?curid=8519" title="Data structure">
Data structure

In computer science, a data structure is a particular way of organizing data in a computer so that it can be used efficiently.
Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, B-trees are particularly well-suited for implementation of databases, while compiler implementations usually use hash tables to look up identifiers.
Data structures provide a means to manage large amounts of data efficiently, such as large databases and internet indexing services. Usually, efficient data structures are a key in designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.
Overview.
Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by an address a bit string that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations; while the linked data structures are based on storing addresses of data items within the structure itself. Many data structures use both principles, sometimes combined in non-trivial ways (as in XOR linking).
The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).
Examples.
There are numerous types of data structures, generally built upon simpler primitive data types:
Language support.
Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.
Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and Microsoft's .NET Framework.
Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java and Smalltalk may use classes for this purpose.
Many known data structures have concurrent versions that allow multiple computing threads to access the data structure simultaneously.

</doc>
<doc id="8520" url="http://en.wikipedia.org/wiki?curid=8520" title="Dmitri Shostakovich">
Dmitri Shostakovich

Dmitri Dmitriyevich Shostakovich (, ; 25 September 19069 August 1975) was a Russian composer and pianist, and a prominent figure of 20th-century music.
Shostakovich achieved fame in the Soviet Union under the patronage of Soviet chief of staff Mikhail Tukhachevsky, but later had a complex and difficult relationship with the government. Nevertheless, he received accolades and state awards and served in the Supreme Soviet of the RSFSR (1947–1962) and the Supreme Soviet of the Soviet Union (from 1962 until his death).
After a period influenced by Sergei Prokofiev and Igor Stravinsky, Shostakovich developed a hybrid style, as exemplified by "Lady Macbeth of the Mtsensk District" (1934). This single work juxtaposed a wide variety of trends, including the neo-classical style (showing the influence of Stravinsky) and post-Romanticism (after Gustav Mahler). Sharp contrasts and elements of the grotesque characterize much of his music.
Shostakovich's orchestral works include 15 symphonies and six concerti. His chamber output includes 15 string quartets, a piano quintet, two piano trios, and two pieces for string octet. His piano works include two solo sonatas, an early set of preludes, and a later set of 24 preludes and fugues. Other works include three operas, several song cycles, ballets, and a substantial quantity of film music; especially well known is "The Second Waltz", Op. 99, music to the film "" (1955–1956).
Biography.
Early life.
Born at 2 Podolskaya Ulitsa in Saint Petersburg, Russia, Shostakovich was the second of three children of Dmitri Boleslavovich Shostakovich and Sofiya Vasilievna Kokoulina. Shostakovich's paternal grandfather, originally surnamed Szostakowicz, was of Polish Roman Catholic descent (his family roots trace to the region of the town of Vileyka in today's Belarus), but his immediate forebears came from Siberia. A Polish revolutionary in the January Uprising of 1863–4, Bolesław Szostakowicz would be exiled to Narym (near Tomsk) in 1866 in the crackdown that followed Dmitri Karakozov's assassination attempt on Tsar Alexander II. When his term of exile ended, Szostakowicz decided to remain in Siberia. He eventually became a successful banker in Irkutsk and raised a large family. His son, Dmitri Boleslavovich Shostakovich, the composer's father, was born in exile in Narim in 1875 and attended Saint Petersburg University, graduating in 1899 from the faculty of physics and mathematics. After graduation, Dmitri Boleslavovich went to work as an engineer under Dmitri Mendeleev at the Bureau of Weights and Measures in Saint Petersburg. In 1903, he married Sofiya Vasilievna Kokoulina, another Siberian transplant to the capital. Sofiya herself was one of six children born to Vasiliy Yakovlevich Kokoulin, a Russian Siberian native.
Dmitri Dmitriyevich Shostakovich was a child prodigy as a pianist and composer, his talent becoming apparent after he began piano lessons with his mother at the age of nine. On several occasions, he displayed a remarkable ability to remember what his mother had played at the previous lesson, and would get "caught in the act" of pretending to read, playing the previous lesson's music when different music was placed in front of him. In 1918, he wrote a funeral march in memory of two leaders of the Kadet party, murdered by Bolshevik sailors.
In 1919, at the age of 13, he was allowed to enter the Petrograd Conservatory, then headed by Alexander Glazunov. Glazunov monitored Shostakovich's progress closely and promoted him. Shostakovich studied piano with Leonid Nikolayev after a year in the class of Elena Rozanova, composition with Maximilian Steinberg, and counterpoint and fugue with Nikolay Sokolov, with whom he became friends. Shostakovich also attended Alexander Ossovsky's history of music classes. Steinberg tried to guide Shostakovich in the path of the great Russian composers, but was disappointed to see him wasting his talent and imitating Igor Stravinsky and Sergei Prokofiev. He also suffered for his perceived lack of political zeal, and initially failed his exam in Marxist methodology in 1926. His first major musical achievement was the First Symphony (premiered 1926), written as his graduation piece at the age of nineteen.
Early career.
After graduation, Shostakovich initially embarked on a dual career as concert pianist and composer, but his dry style of playing (his American biographer, Laurel Fay, comments on his "emotional restraint" and "riveting rhythmic drive") was often unappreciated. He nevertheless won an "honorable mention" at the First International Chopin Piano Competition in Warsaw in 1927. After the competition Shostakovich met the conductor Bruno Walter, who was so impressed by the composer's First Symphony that he conducted it at its Berlin premiere later that year. Leopold Stokowski was equally impressed and gave the work its U.S. premiere the following year in Philadelphia and also made the work's first recording.
Thereafter, Shostakovich concentrated on composition, and soon limited his performances primarily to those of his own works. In 1927 he wrote his Second Symphony (subtitled "To October"), a patriotic piece with a great pro-Soviet choral finale. Due to its experimental nature, as with the subsequent Third Symphony, the pieces were not critically acclaimed with the enthusiasm as granted to the First.
The year 1927 also marked the beginning of Shostakovich's relationship with Ivan Sollertinsky, who remained his closest friend until the latter's death in 1944. Sollertinsky introduced the composer to the music of Gustav Mahler, which had a strong influence on his music from the Fourth Symphony onwards.
While writing the Second Symphony, Shostakovich also began work on his satirical opera "The Nose", based on the story by Gogol. In June 1929, the opera was given a concert performance, against Shostakovich's own wishes, and was ferociously attacked by the Russian Association of Proletarian Musicians (RAPM). Its stage premiere on 18 January 1930 opened to generally poor reviews and widespread incomprehension amongst musicians.
Shostakovich composed his first film score for the 1929 silent movie, "The New Babylon," set during the 1871 Paris Commune.
In the late 1920s and early 1930s, Shostakovich worked at TRAM, a proletarian youth theatre. Although he did little work in this post, it shielded him from ideological attack. Much of this period was spent writing his opera, "Lady Macbeth of the Mtsensk District", which was first performed in 1934. It was immediately successful, on both popular and official levels. It was described as "the result of the general success of Socialist construction, of the correct policy of the Party", and as an opera that "could have been written only by a Soviet composer brought up in the best tradition of Soviet culture."
Shostakovich married his first wife, Nina Varzar, in 1932. Initial difficulties led to a divorce in 1935, but the couple soon remarried when Nina became pregnant with their first child.
First denunciation.
In 1936, Shostakovich fell from official favour. The year began with a series of attacks on him in "Pravda", in particular an article entitled, "Muddle Instead of Music". Shostakovich was away on a concert tour in Arkhangelsk when he heard news of the first "Pravda" article. Two days before the article was published on the evening of 28 January, a friend had advised Shostakovich to attend the Bolshoi Theatre production of "Lady Macbeth". When he arrived, he saw that Joseph Stalin and the Politburo were there. In letters written to his friend Ivan Sollertinsky, Shostakovich recounted the horror with which he watched as Stalin shuddered every time the brass and percussion played too loudly. Equally horrifying was the way Stalin and his companions laughed at the love-making scene between Sergei and Katerina. Eyewitness accounts testify that Shostakovich was "white as a sheet" when he went to take his bow after the third act.
The article condemned "Lady Macbeth" as formalist, "coarse, primitive and vulgar". Consequently, commissions began to fall off, and his income fell by about three quarters. Even Soviet music critics who had praised the opera were forced to recant in print, saying they "failed to detect the shortcomings of "Lady Macbeth" as pointed out by "Pravda"". Shortly after the "Muddle Instead of Music" article, "Pravda" published another, "Ballet Falsehood," that criticized Shostakovich’s ballet "The Limpid Stream". Shostakovich did not expect this second article because the general public and press already accepted this music as "democratic" - that is, tuneful and accessible. However, "Pravda" criticized "The Limpid Stream" for incorrectly displaying peasant life on the collective farm.
More widely, 1936 marked the beginning of the Great Terror, in which many of the composer's friends and relatives were imprisoned or killed: these included his patron Marshal Tukhachevsky (shot months after his arrest); his brother-in-law Vsevolod Frederiks (a distinguished physicist, who was eventually released but died before he got home); his close friend Nikolai Zhilyayev (a musicologist who had taught Tukhachevsky; shot shortly after his arrest); his mother-in-law, the astronomer Sofiya Mikhaylovna Varzar (sent to a camp in Karaganda); his friend the Marxist writer Galina Serebryakova (20 years in camps); his uncle Maxim Kostrykin (died); and his colleagues Boris Kornilov and Adrian Piotrovsky (executed). His only consolation in this period was the birth of his daughter Galina in 1936; his son Maxim was born two years later.
Withdrawal of the Fourth Symphony.
The publication of the "Pravda" editorials coincided with the composition of Shostakovich's Fourth Symphony. The work marked a great shift in style for the composer, due to the substantial influence of Gustav Mahler, as well as multiple Western-style elements. The symphony gave Shostakovich compositional trouble, as he attempted to reform his style into a new idiom. The composer was well into the work when the fatal articles appeared. Despite this, Shostakovich continued to compose the symphony and planned a premiere at the end of 1936. Rehearsals began that December, but after a number of rehearsals Shostakovich, for reasons still debated today, decided to withdraw the symphony from the public. A number of his friends and colleagues, such as Isaak Glikman, have suggested that it was in fact an official ban which Shostakovich was persuaded to present as a voluntary withdrawal. Whatever the case, it seems possible that this action saved the composer's life: during this time Shostakovich feared for himself and his family. Yet Shostakovich did not repudiate the work: it retained its designation as his Fourth Symphony. A piano reduction was published in 1946, and the work was finally premiered in 1961, well after Stalin's death.
During 1936 and 1937, in order to maintain as low a profile as possible between the Fourth and Fifth symphonies, Shostakovich mainly composed film music, a genre favored by Stalin and lacking in dangerous personal expression.
"An artist's creative response to just criticism".
The composer's response to his denunciation was the Fifth Symphony of 1937, which was musically more conservative than his earlier works. Premiering on 21 November 1937 in Leningrad, it was a phenomenal success: many in the Leningrad audience had lost family or friends to the mass executions. The Fifth drove many to tears and welling emotions. Later Shostakovich wrote in his memoirs: "I'll never believe that a man who understood nothing could feel the Fifth Symphony." Of course they understood, they understood what was happening around them and they understood what the Fifth was about."
The success put Shostakovich in good standing once again. Music critics and the authorities alike, including those who had earlier accused Shostakovich of formalism, claimed that he had learned from his mistakes and had become a true Soviet artist. The composer Dmitry Kabalevsky, who had been among those who disassociated himself from Shostakovich when the "Pravda" article was published, praised the Fifth Symphony and congratulated Shostakovich for "not having given in to the seductive temptations of his previous ‘erroneous’ ways."
It was also at this time that Shostakovich composed the first of his string quartets. His chamber works allowed him to experiment and express ideas which would have been unacceptable in his more public symphonic pieces. In September 1937, he began to teach composition at the Leningrad Conservatory, which provided some financial security but interfered with his own creative work.
Second World War.
In 1939, before the Soviet forces invaded Finland, the Party Secretary of Leningrad Andrei Zhdanov commissioned a celebratory piece from Shostakovich, entitled "Suite on Finnish Themes" to be performed as the marching bands of the Red Army would be parading through the Finnish capital Helsinki. The Winter War was a bitter experience for the Red Army, the parade never happened, and Shostakovich would never lay claim to the authorship of this work. It was not performed until 2001.
After the outbreak of war between the Soviet Union and Germany in 1941, Shostakovich initially remained in Leningrad. He tried to enlist for the military but was turned away because of his poor eyesight. To compensate, Shostakovich became a volunteer for the Leningrad Conservatory’s firefighter brigade and delivered a radio broadcast to the Soviet people "". The photograph for which he posed was published in newspapers throughout the country.
But his greatest and most famous wartime contribution was the Seventh Symphony. The composer wrote the first three movements in Leningrad and completed the work in Kuibyshev (now Samara) where he and his family had been evacuated. Whether or not Shostakovich really conceived the idea of the symphony with the siege of Leningrad in mind, it was officially claimed as a representation of the people of Leningrad’s brave resistance to the German invaders and an authentic piece of patriotic art at a time when morale needed boosting. The symphony was first premiered by the Bolshoi Theatre orchestra in Kuibyshev and was soon performed abroad in London and the United States. However, the most compelling performance was the Leningrad premiere by the Radio Orchestra in the besieged city. The orchestra had only 14 musicians left, so the conductor Karl Eliasberg had to recruit anyone who could play a musical instrument to perform the symphony. The Leningrad Shostakovich reportedly had in mind was not the one that withstood the German siege. Rather, it was the one "that Stalin destroyed and Hitler merely finished off."
In spring 1943, the family moved to Moscow. At the time of the Eighth Symphony's premiere, the tide had turned for the Red Army. Therefore the public, and most importantly the authorities, wanted another triumphant piece from the composer. Instead, they got the Eighth Symphony, perhaps the ultimate in sombre and violent expression within Shostakovich's output. In order to preserve the image of Shostakovich (a vital bridge to the people of the Union and to the West), the government assigned the name "Stalingrad" to the symphony, giving it the appearance of a mourning of the dead in the bloody Battle of Stalingrad. However, the symphony did not escape criticism. Shostakovich is reported to have said: "When the Eighth was performed, it was openly declared counter-revolutionary and anti-Soviet. They said, 'Why did Shostakovich write an optimistic symphony at the beginning of the war and a tragic one now? At the beginning we were retreating and now we're attacking, destroying the Fascists. And Shostakovich is acting tragic, that means he's on the side of the fascists.'" The work was unofficially but effectively banned until 1956.
The Ninth Symphony (1945), in contrast, is an ironic Haydnesque parody, which intentionally failed to satisfy Stalin's demands for a "hymn of victory". The war was won, and Shostakovich's "pretty" symphony was interpreted as a mockery of the Soviet Union’s victory rather than a celebratory piece. Shostakovich continued to compose chamber music, notably his Second Piano Trio (Op. 67), dedicated to the memory of Sollertinsky, with a bittersweet, Jewish-themed "totentanz" finale.
Second denunciation.
In 1948 Shostakovich, along with many other composers, was again denounced for formalism in the Zhdanov decree. Andrei Zhdanov, Chairman of the RSFSR Supreme Soviet, accused Shostakovich and other composers (such as Sergei Prokofiev and Aram Khachaturian) for writing inappropriate and formalist music. This was part of an ongoing anti-formalism campaign intended to root out all Western compositional influence as well as any perceived "non-Russian" output. The conference resulted in the publication of the Central Committee’s Decree "On V. Muradeli’s opera "The Great Friendship"," which was targeted towards all Soviet composers and demanded that they only write "proletarian" music, or music for the masses. The accused composers, including Shostakovich, were summoned to make public apologies in front of the committee. Most of Shostakovich's works were banned, and his family had privileges withdrawn. Yuri Lyubimov says that at this time "he waited for his arrest at night out on the landing by the lift, so that at least his family wouldn't be disturbed."
The consequences of the decree for composers were harsh. Shostakovich was among those who were dismissed from the Conservatoire altogether. For Shostakovich, the loss of money was perhaps the largest blow. Others still in the Conservatory experienced an atmosphere that was thick with suspicion. No one wanted their work to be understood as formalist, so many resorted to accusing their colleagues of writing or performing anti-proletarian music.
In the next few years he composed three categories of work: film music to pay the rent, official works aimed at securing official rehabilitation, and serious works "for the desk drawer". The latter included the Violin Concerto No. 1 and the song cycle "From Jewish Folk Poetry." The cycle was written at a time when the post-war anti-Semitic campaign was already under way, with widespread arrests including of I. Dobrushin and Yiditsky, the compilers of the book from which Shostakovich took his texts.
The restrictions on Shostakovich's music and living arrangements were eased in 1949, when Stalin decided that the Soviets needed to send artistic representatives to the Cultural and Scientific Congress for World Peace in New York City, and that Shostakovich should be amongst them. For Shostakovich, it was a humiliating experience culminating in a New York press conference where he was expected to read a prepared speech. Nicolas Nabokov, who was present in the audience, witnessed Shostakovich starting to read "in a nervous and shaky voice" before he had to break off "and the speech was continued in English by a suave radio baritone". Fully aware that Shostakovich was not free to speak his mind, Nabokov publicly asked the composer whether he supported the then recent denunciation of Stravinsky's music in the Soviet Union. Shostakovich, who was a great admirer of Stravinsky and had been influenced by his music, had no alternative but to answer in the affirmative. Nabokov did not hesitate to publish that this demonstrated that Shostakovich was "not a free man, but an obedient tool of his government." Shostakovich never forgave Nabokov for this public humiliation. That same year Shostakovich was obliged to compose the cantata "Song of the Forests," which praised Stalin as the "great gardener." In 1951 the composer was made a deputy to the Supreme Soviet of RSFSR.
Stalin's death in 1953 was the biggest step towards Shostakovich's rehabilitation as a creative artist, which was marked by his Tenth Symphony. It features a number of musical quotations and codes (notably the DSCH and Elmira motifs, Elmira Nazirova being a pianist and composer who had studied under Shostakovich in the year prior to his dismissal from the Moscow Conservatoire), the meaning of which is still debated, whilst the savage second movement, according to "Testimony", is intended as a musical portrait of Stalin himself. The Symphony ranks alongside the Fifth and Seventh as one of his most popular works. 1953 also saw a stream of premieres of the "desk drawer" works.
During the forties and fifties Shostakovich had close relationships with two of his pupils: Galina Ustvolskaya and Elmira Nazirova. In the background to all this remained Shostakovich's first, open marriage to Nina Varzar until her death in 1954. He taught Ustvolskaya from 1937 to 1947. The nature of their relationship is far from clear: Mstislav Rostropovich described it as "tender". Ustvolskaya rejected a proposal of marriage from him after Nina's death. Shostakovich's daughter, Galina, recalled her father consulting her and Maxim about the possibility of Ustvolskaya becoming their stepmother. Ustvolskaya's friend, Viktor Suslin, said that she had been "deeply disappointed" in Shostakovich by the time of her graduation in 1947. The relationship with Nazirova seems to have been one-sided, expressed largely through his letters to her, and can be dated to around 1953 to 1956. He married his second wife, Komsomol activist Margarita Kainova, in 1956; the couple proved ill-matched, and divorced three years later.
In 1954, Shostakovich wrote the Festive Overture, opus 96, that was used as the theme music for the 1980 Summer Olympics. In addition his '"Theme from the film "Pirogov", Opus 76a: Finale" was played as the cauldron was lit at the 2004 Summer Olympics in Athens, Greece.
In 1959, Shostakovich appeared on stage in Moscow at the end of a concert performance of his Fifth Symphony, congratulating Leonard Bernstein and the New York Philharmonic Orchestra for their performance (part of a concert tour of the Soviet Union). Later that year, Bernstein and the New York Philharmonic recorded the symphony in Boston for Columbia Records.
Joining the Party.
The year 1960 marked another turning point in Shostakovich's life: he joined the Communist Party. The government wanted to appoint him General Secretary of the Composers' Union, but in order to hold that position he was required to attain Party membership. It was understood that Nikita Khrushchev, the First Secretary of the Communist Party from 1958 to 1964, was looking for support from the leading ranks of the intelligentsia in an effort to create a better relationship with the Soviet Union’s artists. This event has been interpreted variously as a show of commitment, a mark of cowardice, the result of political pressure, or as his free decision. On the one hand, the apparat was undoubtedly less repressive than it had been before Stalin's death. On the other, his son recalled that the event reduced Shostakovich to tears, and he later told his wife Irina that he had been blackmailed. Lev Lebedinsky has said that the composer was suicidal. Once he joined the Party, several articles denouncing individualism in music were published in "Pravda" under his name, though he did not actually write them. In addition, in joining the party, Shostakovich was also committing himself to finally writing the homage to Lenin that he had promised before. His Twelfth Symphony, which portrays the Bolshevik Revolution and was completed in 1961, was dedicated to Vladimir Lenin and called "The Year 1917." Around this time, his health also began to deteriorate.
Shostakovich's musical response to these personal crises was the Eighth String Quartet, composed in only three days. He subtitled the piece, "To the victims of fascism and war", ostensibly in memory of the Dresden fire bombing that took place in 1945. Yet, like the Tenth Symphony, this quartet incorporates quotations from several of his past works and his musical monogram: Shostakovich confessed to his friend Isaak Glikman "I started thinking that if some day I die, nobody is likely to write a work in memory of me, so I had better write one myself." Several of Shostakovich's colleagues, including Natalya Vovsi-Mikhoels and the cellist Valentin Berlinsky, were also aware of the Eighth Quartet's biographical intent.
In 1962 he married for the third time, to Irina Supinskaya. In a letter to Glikman, he wrote "her only defect is that she is 27 years old. In all other respects she is splendid: clever, cheerful, straightforward and very likeable." According to Galina Vishnevskaya, who knew the Shostakoviches well, this marriage was a very happy one: "It was with her that Dmitri Dmitriyevich finally came to know domestic peace... Surely, she prolonged his life by several years." In November he made his only venture into conducting, conducting a couple of his own works in Gorky; otherwise he declined to conduct, citing nerves and ill health as his reasons.
That year saw Shostakovich again turn to the subject of anti-Semitism in his Thirteenth Symphony (subtitled "Babi Yar"). The symphony sets a number of poems by Yevgeny Yevtushenko, the first of which commemorates a massacre of Ukrainian Jews during the Second World War. Opinions are divided how great a risk this was: the poem had been published in Soviet media, and was not banned, but it remained controversial. After the symphony's premiere, Yevtushenko was forced to add a stanza to his poem which said that Russians and Ukrainians had died alongside the Jews at Babi Yar.
In 1965 Shostakovich raised his voice in defense of poet Joseph Brodsky, who was sentenced to five years of exile and hard labor. Shostakovich co-signed protests together with Yevtushenko and fellow Soviet artists Kornei Chukovsky, Anna Akhmatova, Samuil Marshak, and the French philosopher Jean-Paul Sartre. After the protests the sentence was commuted, and Brodsky returned to Leningrad. Shostakovich also joined a group of 25 distinguished intellectuals in signing a letter to Leonid Brezhnev asking not to rehabilitate Stalin.
Later life.
In 1964 Shostakovich composed the music for the Russian film "Hamlet", which was favourably reviewed by "New York Times": "But the lack of this aural stimulation - of Shakespeare's eloquent words - is recompensed in some measure by a splendid and stirring musical score by Dmitri Shostakovich. This has great dignity and depth, and at times an appropriate wildness or becoming levity".
In later life, Shostakovich suffered from chronic ill health, but he resisted giving up cigarettes and vodka. Beginning in 1958 he suffered from a debilitating condition that particularly affected his right hand, eventually forcing him to give up piano playing; in 1965 it was diagnosed as poliomyelitis. He also suffered heart attacks the following year and again in 1971, and several falls in which he broke both his legs; in 1967 he wrote in a letter:
"Target achieved so far: 75% (right leg broken, left leg broken, right hand defective). All I need to do now is wreck the left hand and then 100% of my extremities will be out of order."
A preoccupation with his own mortality permeates Shostakovich's later works, among them the later quartets and the Fourteenth Symphony of 1969 (a song cycle based on a number of poems on the theme of death). This piece also finds Shostakovich at his most extreme with musical language, with twelve-tone themes and dense polyphony used throughout. Shostakovich dedicated this score to his close friend Benjamin Britten, who conducted its Western premiere at the 1970 Aldeburgh Festival. The Fifteenth Symphony of 1971 is, by contrast, melodic and retrospective in nature, quoting Wagner, Rossini and the composer's own Fourth Symphony.
Shostakovich died of lung cancer on 9 August 1975 and after a civic funeral was interred in the Novodevichy Cemetery, Moscow. Even before his death he had been commemorated with the naming of the Shostakovich Peninsula on Alexander Island, Antarctica.
He was survived by his third wife, Irina; his daughter, Galina; and his son, Maxim, a pianist and conductor who was the dedicatee and first performer of some of his father's works. Shostakovich himself left behind several recordings of his own piano works, while other noted interpreters of his music include his friends Emil Gilels, Mstislav Rostropovich, Tatiana Nikolayeva, Maria Yudina, David Oistrakh, and members of the Beethoven Quartet.
His last work was his Viola Sonata, which was first performed on 28 December 1975, four months after his death.
Shostakovich's musical influence on later composers outside the former Soviet Union has been relatively slight, although Alfred Schnittke took up his eclecticism, and his contrasts between the dynamic and the static, and some of André Previn's music shows clear links to Shostakovich's style of orchestration. His influence can also be seen in some Nordic composers, such as Lars-Erik Larsson. Many of his Russian contemporaries, and his pupils at the Leningrad Conservatory, however, were strongly influenced by his style (including German Okunev, Boris Tishchenko, whose 5th Symphony of 1978 is dedicated to Shostakovich's memory, Sergei Slonimsky, and others). Shostakovich's conservative idiom has grown increasingly popular with audiences both within and beyond Russia, as the avant-garde has declined in influence and debate about his political views has developed.
Music.
Overview.
Shostakovich's works are broadly tonal and in the Romantic tradition, but with elements of atonality and chromaticism. In some of his later works (e.g., the Twelfth Quartet), he made use of tone rows. His output is dominated by his cycles of symphonies and string quartets, each totaling fifteen works. The symphonies are distributed fairly evenly throughout his career, while the quartets are concentrated towards the latter part. Among the most popular are the Fifth and Seventh Symphonies and the Eighth and Fifteenth Quartets. Other works include the operas "Lady Macbeth of Mtsensk", "The Nose" and the unfinished "The Gamblers" based on the comedy of Nikolai Gogol; six concertos (two each for piano, violin and cello); two piano trios; and a large quantity of film music.
Shostakovich's music shows the influence of many of the composers he most admired: Bach in his fugues and passacaglias; Beethoven in the late quartets; Mahler in the symphonies and Berg in his use of musical codes and quotations. Among Russian composers, he particularly admired Modest Mussorgsky, whose operas "Boris Godunov" and "Khovanshchina" he re-orchestrated; Mussorgsky's influence is most prominent in the wintry scenes of "Lady Macbeth" and the Eleventh Symphony, as well as in his satirical works such as "Rayok". Prokofiev's influence is most apparent in the earlier piano works, such as the first sonata and first concerto. The influence of Russian church and folk music is very evident in his works for unaccompanied choir of the 1950s.
Shostakovich's relationship with Stravinsky was profoundly ambivalent; as he wrote to Glikman, "Stravinsky the composer I worship. Stravinsky the thinker I despise." He was particularly enamoured of the Symphony of Psalms, presenting a copy of his own piano version of it to Stravinsky when the latter visited the USSR in 1962. (The meeting of the two composers was not very successful, however; observers commented on Shostakovich's extreme nervousness and Stravinsky's "cruelty" to him.)
Many commentators have noted the disjunction between the experimental works before the 1936 denunciation and the more conservative ones that followed; the composer told Flora Litvinova, "without 'Party guidance' ... I would have displayed more brilliance, used more sarcasm, I could have revealed my ideas openly instead of having to resort to camouflage." Articles published by Shostakovich in 1934 and 1935 cited Berg, Schoenberg, Krenek, Hindemith, "and especially Stravinsky" among his influences. Key works of the earlier period are the First Symphony, which combined the academicism of the conservatory with his progressive inclinations; "The Nose" ("The most uncompromisingly modernist of all his stage-works"); "Lady Macbeth." which precipitated the denunciation; and the Fourth Symphony, described in Grove's Dictionary as "a colossal synthesis of Shostakovich's musical development to date". The Fourth Symphony was also the first in which the influence of Mahler came to the fore, prefiguring the route Shostakovich was to take to secure his rehabilitation, while he himself admitted that the preceding two were his least successful.
In the years after 1936, Shostakovich's symphonic works were outwardly musically conservative, regardless of any subversive political content. During this time he turned increasingly to chamber works, a field that permitted the composer to explore different and often darker ideas without inviting external scrutiny. While his chamber works were largely tonal, they gave Shostakovich an outlet for sombre reflection not welcomed in his more public works. This is most apparent in the late chamber works, which portray what is described in Grove's Dictionary as a "world of purgatorial numbness"; in some of these he included the use of tone rows, although he treated these as melodic themes rather than serially. Vocal works are also a prominent feature of his late output, setting texts often concerned with love, death and art.
Jewish themes.
Even before the Stalinist anti-Semitic campaigns in the late 1940s and early 1950s, Shostakovich showed an interest in Jewish themes. He was intrigued by Jewish music’s "ability to build a jolly melody on sad intonations". Examples of works that included Jewish themes are the Fourth String Quartet (1949), the First Violin Concerto (1948), and the "Four Monologues on Pushkin Poems" (1952), as well as the Piano Trio in E minor (1944). He was further inspired to write with Jewish themes when he examined Moisei Beregovski’s thesis on the theme of Jewish folk music in 1946.
In 1948, Shostakovich acquired a book of Jewish folk songs, and from this he composed the song cycle "From Jewish Poetry". He initially wrote eight songs that were meant to represent the hardships of being Jewish in the Soviet Union. However in order to disguise this, Shostakovich ended up adding three more songs meant to demonstrate the great life Jews had under the Soviet regime. Despite his efforts to hide the real meaning in the work, the Union of Composers refused to approve his music in 1949 under the pressure of the anti-Semitism that gripped the country. "From Jewish Poetry" could not be performed until after Stalin’s death in March 1953, along with all the other works that were forbidden.
Posthumous publications.
In 2004, the musicologist Olga Digonskaya discovered a trove of Shostakovich manuscripts at the Glinka State Central Museum of Musical Culture, Moscow. In a cardboard file were some "300 pages of musical sketches, pieces and scores" in the hand of Shostakovich. "A composer friend bribed Shostakovich's housemaid to regularly deliver the contents of Shostakovich's office waste bin to him, instead of taking it to the garbage. Some of those cast-offs eventually found their way into the Glinka. ... The Glinka archive 'contained a huge number of pieces and compositions which were completely unknown or could be traced quite indirectly,' Digonskaya said."
Among these were Shostakovich's piano and vocal sketches for a prologue to an opera, "Orango" (1932). They have been orchestrated by the British composer Gerard McBurney and this work was premiered in December 2011 by the Los Angeles Philharmonic.
Criticism.
According to Shostakovich scholar Gerard McBurney, opinion is divided on whether his music is "of visionary power and originality, as some maintain, or, as others think, derivative, trashy, empty and second-hand". William Walton, his British contemporary, described him as "the greatest composer of the 20th century". Musicologist David Fanning concludes in Grove's Dictionary that, "Amid the conflicting pressures of official requirements, the mass suffering of his fellow countrymen, and his personal ideals of humanitarian and public service, he succeeded in forging a musical language of colossal emotional power."
Some modern composers have been critical. Pierre Boulez dismissed Shostakovich's music as "the second, or even third of Mahler". The Romanian composer and Webern disciple Philip Gershkovich called Shostakovich "a hack in a trance". A related complaint is that Shostakovich's style is vulgar and strident: Stravinsky wrote of "Lady Macbeth": "brutally hammering ... and monotonous". English composer and musicologist Robin Holloway described his music as "battleship-grey in melody and harmony, factory-functional in structure; in content all rhetoric and coercion."
In the 1980s, the Finnish conductor and composer Esa-Pekka Salonen was critical of Shostakovich and refused to conduct his music. For instance, he said in 1987:
Shostakovich is in many ways a polar counter-force for Stravinsky. [...] When I have said that the 7th symphony of Shostakovich is a dull and unpleasant composition, people have responded: "Yes, yes, but think of the background of that symphony." Such an attitude does no good to anyone.
However, Salonen has since performed and recorded several of Shostakovich's works, including the Piano Concertos Nos. 1 and 2 (1999), the Violin Concerto No. 1 (2010), the Prologue to "Orango" and the Symphony No. 4 (2012).
It is certainly true that Shostakovich borrows extensively from the material and styles both of earlier composers and of popular music; the vulgarity of "low" music is a notable influence on this "greatest of eclectics". McBurney traces this to the avant-garde artistic circles of the early Soviet period in which Shostakovich moved early in his career, and argues that these borrowings were a deliberate technique to allow him to create "patterns of contrast, repetition, exaggeration" that gave his music the large-scale structure it required.
Personality.
Shostakovich was in many ways an obsessive man: according to his daughter he was "obsessed with cleanliness"; he synchronised the clocks in his apartment; he regularly sent cards to himself to test how well the postal service was working. Elizabeth Wilson's "Shostakovich: A Life Remembered" (1994 edition) indexes 26 references to his nervousness. Mikhail Druskin remembers that even as a young man the composer was "fragile and nervously agile". Yuri Lyubimov comments, "The fact that he was more vulnerable and receptive than other people was no doubt an important feature of his genius". In later life, Krzysztof Meyer recalled, "his face was a bag of tics and grimaces".
In his lighter moods, sport was one of his main recreations, although he preferred spectating or umpiring to participating (he was a qualified football referee). His favourite football club was Zenit Leningrad, which he would watch regularly. He also enjoyed playing card games, particularly patience.
He was fond of satirical writers such as Gogol, Chekhov and Mikhail Zoshchenko. The influence of the latter in particular is evident in his letters, which include wry parodies of Soviet officialese. Zoshchenko himself noted the contradictions in the composer's character: "he is ... frail, fragile, withdrawn, an infinitely direct, pure child ... [but he is also] hard, acid, extremely intelligent, strong perhaps, despotic and not altogether good-natured (although cerebrally good-natured)".
He was diffident by nature: Flora Litvinova has said he was "completely incapable of saying 'No' to anybody." This meant he was easily persuaded to sign official statements, including a denunciation of Andrei Sakharov in 1973; on the other hand he was willing to try to help constituents in his capacities as chairman of the Composers' Union and Deputy to the Supreme Soviet. Oleg Prokofiev commented that "he tried to help so many people that ... less and less attention was paid to his pleas." When asked if he believed in God, Shostakovich said "No, and I am very sorry about it."
Orthodoxy and revisionism.
Shostakovich's response to official criticism and, what is more important, the question of whether he used music as a kind of covert dissidence is a matter of dispute. He outwardly conformed to government policies and positions, reading speeches and putting his name to articles expressing the government line. But it is evident he disliked many aspects of the regime, as confirmed by his family, his letters to Isaak Glikman, and the satirical cantata "Rayok", which ridiculed the "anti-formalist" campaign and was kept hidden until after his death. He was a close friend of Marshal of the Soviet Union Mikhail Tukhachevsky, who was executed in 1937 during the Great Purge.
It is also uncertain to what extent Shostakovich expressed his opposition to the state in his music. The revisionist view was put forth by Solomon Volkov in the 1979 book "Testimony", which was claimed to be Shostakovich's memoirs dictated to Volkov. The book alleged that many of the composer's works contained coded anti-government messages, that would place Shostakovich in a tradition of Russian artists outwitting censorship that goes back at least to the early 19th century poet Alexander Pushkin. It is known that he incorporated many quotations and motifs in his work, most notably his signature DSCH theme. His longtime collaborator Yevgeny Mravinsky said that "Shostakovich very often explained his intentions with very specific images and connotations."
The revisionist perspective has subsequently been supported by his children, Maxim and Galina, and many Russian musicians. Volkov has further argued, both in "Testimony" and in "Shostakovich and Stalin", that Shostakovich adopted the role of the "yurodivy" or holy fool in his relations with the government. Other prominent revisionists are Ian MacDonald, whose book "The New Shostakovich" put forward further revisionist interpretations of his music, and Elizabeth Wilson, whose "Shostakovich: A Life Remembered" provides testimony from many of the composer's acquaintances.
Musicians and scholars including Laurel Fay and Richard Taruskin contest the authenticity and debate the significance of "Testimony", alleging that Volkov compiled it from a combination of recycled articles, gossip, and possibly some information direct from the composer. Fay documents these allegations in her 2002 article 'Volkov's "Testimony" reconsidered', showing that the only pages of the original "Testimony" manuscript that Shostakovich had signed and verified are word-for-word reproductions of earlier interviews given by the composer, none of which are controversial. (Against this, it has been pointed out by Allan B. Ho and Dmitry Feofanov that at least two of the signed pages contain controversial material: for instance, "on the first page of chapter 3, where [Shostakovich] notes that the plaque that reads 'In this house lived [Vsevolod] Meyerhold' should also say 'And in this house his wife was brutally murdered'.") More broadly, Fay and Taruskin view Shostakovich's music as more significant than the details of his life, and that to seek political messages in the music detracts from, rather than enhances, its artistic value.
Recorded legacy.
In May 1958, during a visit to Paris, Shostakovich recorded his two piano concertos with André Cluytens, as well as some short piano works. These were issued by EMI on an LP, reissued by Seraphim Records on LP, and eventually digitally remastered and released on CD. Shostakovich recorded the two concertos in stereo in Moscow for Melodiya. Shostakovich also played the piano solos in recordings of the Cello Sonata, Op. 40 with cellist Daniil Shafran and also with Mstislav Rostropovich; the Violin Sonata, Op. 134, with violinist David Oistrakh; and the Piano Trio, Op. 67 with violinist David Oistrakh and cellist Miloš Sádlo. There is also a short sound film of Shostakovich as soloist in a 1930s concert performance of the closing moments of his first piano concerto. A colour film of Shostakovich supervising one of his operas, from his last year, was also made.

</doc>
<doc id="8521" url="http://en.wikipedia.org/wiki?curid=8521" title="Doom (1993 video game)">
Doom (1993 video game)

Doom (typeset as DOOM in official documents) is a 1993 science fiction horror-themed first-person shooter (FPS) video game by id Software. It is considered one of the most significant and influential titles in the video game industry, for having ushered in the popularity of the first-person shooter genre. The original game is divided into three nine-level episodes and distributed via shareware and mail order. The Ultimate Doom, an updated release of the original game featuring a fourth episode, was released in 1995 and sold at retail.
In "Doom", players assume the role of an unnamed space marine, who became popularly known as "Doomguy", fighting his way through hordes of invading demons from Hell. With one third of the game, nine levels, distributed as shareware, "Doom" was played by an estimated 10 million people within two years of its release, popularizing the mode of gameplay and spawning a gaming subculture. In addition to popularizing the FPS genre, it pioneered immersive 3D graphics, networked multiplayer gaming, and support for customized additions and modifications via packaged files in a data archive known as "WADs". As a sign of its effect on the industry, first-person shooter games from the genre's boom in the 1990s, helped in no small part by the game's release, became known simply as ""Doom" clones". Its graphic violence, as well as satanic imagery, made Doom the subject of controversy.
The "Doom" franchise was later continued with the follow-up "" (1994) and numerous mission packs, including "Master Levels for Doom II" (1995), and "Final Doom" (1996). Originally released for PC DOS, the games have later been ported to numerous other platforms. Once the game's source code was released in 1997, it spawned even more adaptations, as fans further ported the code to countless devices. The series started to lose mainstream appeal as the technology of the "Doom" game engine was surpassed in the mid-1990s, although fans have continued making WADs, speedruns, and modifications to the original. The franchise again received popular attention in 2004 with the release of "Doom 3", a retelling of the original game using new technology, and an associated 2005 "Doom" motion picture. "Doom 4" was announced as in production in 2008 and was later retitled simply as "Doom".
Plot.
"Doom", a science fiction/horror themed video game, has a background which is given in the game's instruction manual; the rest of the story is advanced with short messages displayed between each section of the game (called "episodes"), the action as the player character progresses through the levels, and some visual cues.
The player takes the role of an unnamed space marine ("Doomguy") who has been punitively posted to Mars after assaulting his commanding officer, who ordered his unit to fire on civilians. The Martian space marine base acts as security for the Union Aerospace Corporation, a multi-planetary conglomerate, which is performing secret experiments with teleportation by creating gateways between the two moons of Mars, Phobos and Deimos. Mars is considered by space marines to be the dullest assignment imaginable. This all changes when the UAC experiments go horribly wrong. Computer systems on Phobos malfunction, Deimos disappears entirely, and "something fragging evil" starts pouring out of the gateway, killing or possessing all UAC personnel.
Responding to a frantic distress call from the overrun scientists, the Martian marine unit is quickly sent by ship from Mars to Phobos to investigate, where the player character is left to guard the perimeter with only a pistol while the rest of the group proceeds inside. The marine hears assorted radio messages, gunfire, and screams, followed by silence: "Seems your buddies are dead." The player cannot navigate the ship off of Phobos alone and sees that the only way out is to fight through the Phobos complex.
As the last man standing, the player character's mission is to fight through the entire onslaught of demonic enemies by himself in order to keep them from attacking Earth. "Knee-Deep in the Dead", the first episode and the only one in the shareware version, is set in the high-tech military bases, power plants, computer centers and geological anomalies on Phobos. It ends with the player character entering the teleporter leading to Deimos, only to be overwhelmed by monsters.
In the second episode, "The Shores of Hell", the marine has successfully teleported to Deimos. He fights his way through installations on Deimos, similar to those on Phobos, but warped and distorted from the demon invasion and interwoven with beastly architecture. After defeating the titanic Cyberdemon, the marine discovers the truth about the vanished moon: it is floating above Hell.
The third episode, called "Inferno", begins after the marine climbs off Deimos to the surface. The marine fights his way through Hell and defeats the Spider Mastermind that planned the invasion. Then a hidden doorway back to Earth opens for the hero, who has "proven too tough for Hell to contain". However, a burning city and a rabbit's head impaled on a stake (named in "The Ultimate Doom" as the marine's pet rabbit, Daisy) show that the demons have invaded Earth, setting the stage for "". The retcons the events of "Doom" as an alien invasion of the Mars moon bases.
In "The Ultimate Doom" expansion, in the fourth episode "Thy Flesh Consumed", it tells that the marine fought valiantly against the hordes of demons that the Spider Mastermind sent through that hidden doorway but ultimately the forces of Hell prevailed in the invasion of Earth. The locales of "Thy Flesh Consumed" are varied, including a mix of high-tech bases and demonic temples, though the atmosphere appears to be Earth.
Gameplay.
Being a first-person shooter, "Doom" is experienced through the eyes of the main character. This character is not named throughout the game. The game's designer, John Romero, has pointed out that this is so the player feels more involved in the game: "There was never a name for the DOOM marine because it's supposed to be you." At its core, the gameplay is similar to classic shooter games, presenting the player with the challenge of surviving while shooting every enemy in sight, but with its pseudo-3D first-person perspective giving environments a spatial representation that has a major effect on the level design and gameplay experience.
In order for the game to be completed, the marine must fight through Phobos, Deimos, and then Hell itself, each presented as an episode containing eight distinct levels, along with an optional ninth hidden level for each one. "The Ultimate Doom", the retail store version of the game, adds a fourth episode, "Thy Flesh Consumed." Set between the end of "Doom" and before "Doom II" and featuring the first contribution of Tim Willits to the Doom franchise, the fourth episode was designed for expert Doom players seeking a major challenge (being considerably more difficult than the original episodes).
The objective of each level is simply to locate the exit room that leads to the next area, marked with an exit sign and/or a special kind of door, while surviving all hazards on the way. Among the obstacles are demonic monsters, pits of toxic or radioactive slime, ceilings that lower and crush anything below them, and locked doors which require a keycard, skull-shaped key device, or a remote switch to be located. The levels are sometimes labyrinthine and feature plenty of items such as additional ammo, health increases and other "power-ups" along the way, as well as the occasional secret areas which are not immediately obvious as a reward for players who explore more carefully. To ease navigation through the levels, a full screen automap is available and shows the areas explored to that point. Many versions of "Doom" (and its sequels) include secret levels which are accessed by the player discovering alternate exits, often hidden behind secret doors, hidden passageways, or in areas which are difficult to reach. Despite carrying masses of high-tech weaponry, the main character can still run at blistering speeds.
"Doom" is notable for the weapons arsenal available to the marine, which became prototypical for first-person shooters. The player character starts armed only with a pistol, and fists in case the ammunition runs out, but larger weapons can be picked up: these are a chainsaw, a shotgun, a chaingun, a rocket launcher, a plasma rifle, and finally the immensely powerful BFG 9000. There is a wide array of power-ups, such as a backpack that increases the player character's ammunition-carrying capacity, armor, first aid kits to restore health, the berserk pack which both restores health and causes the player's punching attack to deal enormous damage, supernatural blue orbs (named "soul spheres" in the manuals) that boost the player character's health up to a maximum of 200%, nightvision, computer maps (which show every area of the level), partial invisibility, and protective suits that allow the player to survive in toxic acids.
The enemy monsters in "Doom" make up the central gameplay element. The player character faces them in large numbers, with the number generally increased when the higher of the game's five difficulty levels is chosen when starting a new game. There are 10 types of monsters, including possessed undead humans as well as demons, all which vary in many ways. The monsters have very simple behavior, consisting of either walking toward their opponent, or attacking by throwing fireballs, biting, and scratching. They will fight each other if one monster is accidentally harmed by another (though most monsters are not harmed by the ranged attacks of their own kind).
Aside from the single player game mode, "Doom" features two multiplayer modes playable over a network: "cooperative", in which two to four players team up, and "deathmatch", in which two to four players play against each other. Online multiplayer was eventually made available through the DWANGO service.
Development.
The development of "Doom" started in 1992, when John D. Carmack developed a new 3D game engine, the Doom engine, while the rest of the id Software team finished the "Wolfenstein 3D" prequel, "Spear of Destiny". When the game design phase began in late 1992, the main thematic influences were the science fiction action film "Aliens" and the horror film "Evil Dead II". The title of the game was picked by John Carmack: "There is a scene in "The Color of Money" where Tom Cruise shows up at a pool hall with a custom pool cue in a case. 'What do you have in there?' asks someone. 'Doom.' replied Cruise with a cocky grin. That, and the resulting carnage, was how I viewed us springing the game on the industry."
Designer Tom Hall wrote an elaborate design document called the "Doom Bible", according to which the game would feature a detailed storyline, multiple player characters, and a number of interactive features. However, many of his ideas were discarded during development in favor of simpler design primarily advocated by John Carmack, resulting in Hall in the end being forced to resign due to not contributing effectively in the direction the rest of the team was going. Most of the level design that ended up in the final game is that of John Romero and Sandy Petersen. The graphics, by Adrian Carmack, Kevin Cloud and Gregor Punchatz, were modelled in various ways: although much was drawn or painted, several of the monsters were built from sculptures in clay or latex, and some of the weapons are toy guns from Toys "R" Us. A heavy metal-ambient soundtrack was supplied by Bobby Prince.
Engine technology.
"Doom"s primary distinguishing feature at the time of its release was its relatively realistic 3D graphics. The advance from id Software's previous game "Wolfenstein 3D" was enabled by several new features in the "Doom" engine, including height differences (all rooms in "Wolfenstein 3D" have the same height), non-perpendicular walls (all walls in "Wolfenstein 3D" run along a rectangular grid), full texture mapping of all surfaces (in "Wolfenstein 3D", floors and ceilings are flat colors) and varying light levels and custom palettes (all areas in "Wolfenstein 3D" are fully lit at the same brightness). The latter contributed to "Doom"s visual authenticity, atmosphere and gameplay, as the use of darkness to frighten or confuse the player was nearly unheard of in games released prior to "Doom"; palette modifications were used to enhance effects such as the berserk power which tints the player's vision red.
In contrast to the static levels of "Wolfenstein 3D", those in "Doom" are highly dynamic: platforms can lower and rise, floors can rise sequentially to form staircases, and bridges can rise and fall. The lifelike environment was enhanced further by the stereo sound system, which made it possible to roughly determine the direction and distance of a sound effect. The player is kept on guard by the grunts and growls of monsters, and receives occasional clues to finding secret areas in the form of sounds of hidden doors opening remotely. As in "Wolfenstein 3D", enemies can also become aware of the player's presence by hearing distant gunshots.
John Carmack had to make use of several tricks for these features to run smoothly on home computers of 1993. Most significantly, the "Doom" engine and levels are not truly three-dimensional; they are internally represented on a single plane, with height differences stored separately as displacements. (A similar trick is still used by many games to create huge outdoor environments.) This allows a two point perspective projection, with several design limitations: for example, it is not possible for the "Doom" engine to render one room over another. However, thanks to its two-dimensional property, the environment can be rendered very quickly, using a binary space partitioning method in conjunction with raycasting. Another benefit was the clarity of the automap, as that could be rendered with 2D vectors without any risk of overlapping. Additionally, the BSP tree technology created by Bruce Naylor was used.
Another important feature of the "Doom" engine is its modular data files, which allow most of the game's content to be replaced by loading custom WAD files. "Wolfenstein 3D" was not designed to be expandable, but fans had nevertheless figured out how to create their own levels for it, and "Doom" was designed to further extend the possibilities. The ability to create custom scenarios contributed significantly to the game's popularity (see the section on WADs, below).
Release.
The development of "Doom" was surrounded by much anticipation. The large number of posts in Internet newsgroups about "Doom" led to the SPISPOPD joke, to which a nod was given in the game in the form of a cheat code. In addition to news, rumors and screenshots, unauthorized leaked alpha versions also circulated online. Many years later these alpha versions were sanctioned by id Software because of historical interest; they reveal how the game progressed from its early design stages. The first public version of "Doom" was uploaded to Software Creations BBS and an FTP server at the University of Wisconsin–Madison on December 10, 1993.
"Doom" was released as shareware, with people encouraged to distribute it further. Although most users did not purchase the registered version, over one million copies have been sold, and the popularity helped the sales of later games in the "Doom" series that were not released as shareware. In 1995, "The Ultimate Doom" (version 1.9, including Episode IV) was released, making this the first time that "Doom" was sold commercially in stores.
In a press release dated January 1, 1993, id Software had written that they expected "Doom" to be "the number one cause of decreased productivity in businesses around the world." This prediction came true at least in part: "Doom" became a major problem at workplaces, both occupying the time of employees and clogging computer networks with traffic caused by deathmatches. Intel, Lotus Development and Carnegie Mellon University are among many organizations reported to form policies specifically disallowing "Doom"-playing during work hours. At the Microsoft campus, "Doom" was by one account equal to a "religious phenomenon".
In late 1995, "Doom" was estimated to be installed on more computers worldwide than Microsoft's new operating system Windows 95, despite million-dollar advertising campaigns for the latter. The game's popularity prompted Bill Gates to briefly consider buying id Software, and led Microsoft to develop a Windows 95 port of "Doom" to promote the operating system as a gaming platform. One such presentation to promote Windows 95 had Bill Gates digitally superimposed into the game. The 1995 release of Microsoft Excel 95 included a "Doom-esque" secret level as an Easter egg containing portraits of the programmers among other things.
The game was made available on Steam on August 3, 2007, running on the DOSBox DOS emulator.
Expansions and Ports.
The popularity of "Doom" led to the development of expansion packs and alternate versions based on the same game engine, including "The Ultimate Doom" (1995), "Final Doom" (1996), and "Doom 64" (1997). "Doom" became a "killer app" that all capable consoles and operating systems were expected to have, and versions of "Doom" have subsequently been released for the following systems: DOS, Microsoft Windows, Linux, Amiga, Apple Macintosh, SNES, Sega 32X, Sony PlayStation, Game Boy Advance, iOS, Symbian OS, RISC OS, Atari Jaguar, Sega Saturn, Nintendo 64, Tapwave Zodiac, 3DO, Xbox, and Xbox Live Arcade. Some of these were bestsellers even many years after initial release. In 2013, a version of "Doom" was ported to the Commodore VIC-20, a vintage 1980 introductory home computer having only 3,583 bytes of BASIC RAM, an 8 bit 6502A processor with rudimentary graphics, no sprites and just 8 colors.
WADs.
The ability for others to create custom levels and otherwise modify the game, in the form of custom WAD files (short for "Where's All the Data?"), turned out to be a particularly popular aspect of "Doom". Gaining the first large mod-making community, "Doom" affected the culture surrounding first-person shooters, and also the industry. Several future professional game designers started their careers making "Doom" WADs as a hobby, among them Tim Willits, who later became the lead designer at id Software.
The first level editors appeared in early 1994, and additional tools have been created that allow most aspects of the game to be edited. Although the majority of WADs contain one or several custom levels mostly in the style of the original game, others implement new monsters and other resources, and heavily alter the gameplay; several popular movies, television series, other video games and other brands from popular culture have been turned into "Doom" WADs by fans, including "Aliens", "Star Wars", "The X-Files", "Teenage Mutant Ninja Turtles","The Simpsons", "South Park", "Sailor Moon", "Dragon Ball Z", "Red Faction", "The Thing", "Pokémon", "Batman" and "Sonic the Hedgehog". Some works, like the "Theme Doom Patch", combined enemies from several films, such as "Aliens", "Predator" and "The Terminator". Some add-on files were also made that changed the sounds made by the various characters and weapons.
Around 1994 and 1995, WADs were primarily distributed online over bulletin board systems or sold in collections on compact discs in computer shops, sometimes bundled with editing guide books. FTP servers became the primary method in later years. A few WADs have been released commercially, including the "Master Levels for Doom II", which was released in 1995 along with "Maximum Doom", a CD containing 1,830 WADs that had been downloaded from the Internet. Several thousand WADs have been created in total: the "idgames" FTP archive contains over 13,000 files, and this represents only a fraction of the complete output of "Doom" fans. Third party programs were also written to handle the loading of various WADs, since the game is a DOS game and all commands had to be entered on the command line to run. A typical launcher would allow the player to select which files to load from a menu, making it much easier to start. In 1995, WizardWorks Software released the "D!Zone" pack featuring hundreds of levels for "Doom" and "Doom II". "D!Zone" was reviewed in "Dragon" by Jay & Dee; Jay gave the pack 1 out of 5 stars, while Dee gave the pack 1½ stars.
Reception.
"Doom" was widely praised in the gaming press and is broadly considered to be one of the most important and influential titles in gaming history. In 1996, "Computer Gaming World" ranked it as the fifth best video game of all time. In 2001, "Doom" was voted the number one game of all time in a poll among over 100 game developers and journalists conducted by GameSpy. In 2003, IGN ranked it as the 44th top video game of all time and also called it ""the" breakthrough game of 1993", adding: "Its arsenal of powerful guns (namely the shotgun and BFG), intense level of gore and perfect balance of adrenaline-soaked action and exploration kept this gamer riveted for years." "PC Gamer" proclaimed "Doom" the most influential game of all time in its ten-year anniversary issue in April 2004. In 2004, readers of "Retro Gamer" voted "Doom" as the ninth top retro game, with the editors commenting: "Only a handful of games can claim that they’ve changed the gaming world, and "Doom" is perhaps the most qualified of them all." In 2005, IGN ranked it as the 39th top game. On March 12, 2007, "The New York Times" reported that "Doom" was named to a list of the ten most important video games of all time, the so called game canon. The Library of Congress took up this video game preservation proposal and began with the games from this list.
In 2009, GameTrailers ranked "Doom" as number one "breakthrough PC game". That same year, "Game Informer" put "Doom" sixth on their list of the games of all time, stating that it gave "the genre the kick start it needed to rule the gaming landscape two decades later." "Game Informer" staff also put it sixth on their 2001 list of the 100 best games ever. In 2012, "Time" named it one of the 100 greatest video games of all time as "it established the look and feel of later shooters as surely as Xerox PARC established the rules of the virtual desktop," adding that "its impact also owes a lot to the gonzo horror sensibility of its designers, including John Romero, who showed a bracing lack of restraint in their deployment of gore and Satanic iconography." Including "Doom" on the list of the greatest games of all time, GameSpot wrote that "despite its numerous appearances in other formats and on other media, longtime fans will forever remember the original 1993 release of "Doom" as the beginning of a true revolution in action gaming."
The game was ported to numerous console gaming platforms both domestically and abroad where it maintained its popularity, receiving generally favorable critical reception.
Controversies.
"Doom" was notorious for its high levels of graphic violence and satanic imagery, which generated controversy from a broad range of groups. Yahoo! Games listed it as one of the top ten most controversial games of all time. It was criticized by religious organizations for its diabolic undertones and was dubbed a "mass murder simulator" by critic and Killology Research Group founder David Grossman. "Doom" prompted fears that the then-emerging virtual reality technology could be used to simulate extremely realistic killing.
The game again sparked controversy throughout a period of school shootings in the United States when it was found that Eric Harris and Dylan Klebold, who committed the Columbine High School massacre in 1999, were avid players of the game. While planning for the massacre, Harris said that the killing would be "like playing "Doom"", and "it'll be like the LA riots, the Oklahoma bombing, WWII, Vietnam, "Duke Nukem" and "Doom" all mixed together", and that his shotgun was "straight out of the game". A rumor spread afterwards that Harris had designed a "Doom" level that looked like the high school, populated with representations of Harris's classmates and teachers, and that Harris practiced for his role in the shootings by playing the level over and over. Although Harris did design "Doom" levels, none of them were based on Columbine High School.
While "Doom" and other violent video games have been blamed for nationally covered school shootings, 2008 research featured by Greater Good Science Center shows that the two are not closely related. Harvard medical school researchers Cheryl Olson and Lawrence Kutner found that violent video games did not correlate to school shootings. The U.S. Secret Service and Department of Education analyzed 37 incidents of school violence and sought to develop a profile of school shooters, they discovered that the most common traits among shooters were that they were male and had histories of depression and attempted suicide. While many of the killers—like the vast majority of young teenage boys—did play video games, this study did not find a relationship between game play and school shootings. In fact, only one eighth of the shooters showed any special interest in violent video games; far less than the number of shooters who seemed attracted to books and movies with violent content.
Excess Intranet/LAN usage.
When "Doom" came out, complaints soon started from Internet node managers about excess load on their servers caused by "Doom" inter-player communication packages, and to some server managers' antivirus software was added an "Antidoom" package that blocked "Doom"-related packages and sent a "Doom" "end this game" code to any "Doom" players detected.
Legacy.
"Doom" franchise.
"Doom" has appeared in several forms in addition to video games, including a "Doom" comic book, four novels by Dafydd Ab Hugh and Brad Linaweaver (loosely based on events and locations in the games), a and a live-action film starring Karl Urban and The Rock released in 2005. The game's development and impact on popular culture is also the subject of the book "" by David Kushner.
The "Doom" series remained dormant between 1996 and 2000, when "Doom 3" was announced. A retelling of the original "Doom" using entirely new graphics technology, "Doom 3" was hyped to provide as large a leap in realism and interactivity as the original game and helped renew interest in the franchise when it was released in 2004. After the "Doom 4" project development was scrapped in 2013, id's Tim Willits said that the next game in the "Doom" series was still the team's focus, but it has not been confirmed to be titled "Doom 4". It was renamed to simply "Doom" in 2014.
Clones.
"Doom" was influential and dozens of new first-person shooter titles appeared following "Doom" release, and they were often referred to as ""Doom" clones" rather than "first-person shooters". The term ""Doom" clone" was used to described the style of gameplay in "Doom"-like games. While the term was initially popular, it was after 1996 gradually replaced by "first-person shooter", around 1998 the phrase "first-person shooter" had firmly superseded ""Doom" clone". Some of these were certainly "clones"—hastily assembled and quickly forgotten—others explored new grounds of the genre and were highly acclaimed. Many of the games closely imitated features in "Doom" such as the selection of weapons and cheat codes. "Doom" principal rivals were Apogee's "Rise of the Triad" and Looking Glass Studios' "System Shock". The popularity of "Star Wars"-themed WADs is rumored to have been the factor that prompted LucasArts to create their first-person shooter "Dark Forces".
Also, the Doom game engine was licensed by id Software to several other companies as well, who released their own games based on it, including "Heretic", "", "Strife" and "HacX". A "Doom"-based game called "Chex Quest" was released in 1996 by Ralston Foods as a promotion to increase cereal sales, and the United States Marine Corps released "Marine Doom".
When, three years later, 3D Realms released "Duke Nukem 3D", a tongue-in-cheek science fiction shooter based on Ken Silverman's technologically similar "Build" engine, id Software had nearly finished "Quake", its next-generation game, which mirrored "Doom"s success for much of the remainder of the 1990s and significantly reduced interest in its predecessor.
Community.
In addition to the thrilling nature of the single-player game, the deathmatch mode was an important factor in the game's popularity. "Doom" was not the first first-person shooter with a deathmatch mode—"Maze War" was a multiplayer FPS in 1973, and by 1977 was running over ethernet on Xerox computers. However, the widespread distribution of PC systems and the violence and gore of "Doom" made deathmatching particularly attractive. Two-player multiplayer was also possible over a phone line by using a modem, or by linking two PCs with a null-modem cable. Due to its widespread distribution, "Doom" hence became the game that introduced deathmatching to a large audience and was also the first game to use the term "deathmatch".
Although the popularity of the "Doom" games dropped with the release of more modern first-person shooters, the game had still retained a strong fan base that continues to this day by playing competitively and creating WADs, and "Doom"-related news is still tracked at multiple websites such as Doomworld. Interest in "Doom" was renewed in 1997, when the source code for the "Doom" engine was released (it was also placed under the GNU General Public License in 1999). Fans then began porting the game to various operating systems, even to previously unsupported platforms such as the Dreamcast. As for the PC, over 50 different "Doom" source ports have been developed. New features such as OpenGL rendering and scripting allow WADs to alter the gameplay more radically.
Devoted players have spent years creating speedruns for "Doom", competing for the quickest completion times and sharing knowledge about routes through the levels and how to exploit bugs in the "Doom" engine for shortcuts. Achievements include the completion of both "Doom" and "Doom II" on the difficulty setting "Ultra-Violence" in less than 30 minutes each. In addition, a few players have also managed to complete "Doom II" in a single run on the difficulty setting "Nightmare!", on which monsters are more aggressive, launch faster projectiles (or, in the case of the Pinky Demon, simply move faster), and respawn roughly 30 seconds after they have been killed (level designer John Romero characterized the idea of such a run as "[just having to be] impossible"). Movies of most of these runs are available from the COMPET-N website.
Online co-op and deathmatch play still continued on fan created services.
In 2011, a modification for Doom"" was released with the intent of replicating the design style of the original game.

</doc>
<doc id="8522" url="http://en.wikipedia.org/wiki?curid=8522" title="Denver">
Denver

Denver, officially the City and County of Denver (; Arapaho: Niinéniiniicíihéhe'), is the largest city and capital of the State of Colorado. Denver is also the second-most populous county in Colorado after El Paso County. Denver is a consolidated city and county located in the South Platte River Valley on the western edge of the High Plains just east of the Front Range of the Rocky Mountains. The Denver downtown district is located immediately east of the confluence of Cherry Creek with the South Platte River, approximately 12 miles (19 km) east of the foothills of the Rocky Mountains. Denver is nicknamed the "Mile-High City" because its official elevation is exactly one mile () above sea level, making it one of the highest major cities in the United States. The 105th meridian west of Greenwich, the longitudinal reference for the Mountain Time Zone, passes directly through Denver Union Station.
Denver is ranked as a beta world city by the Globalization and World Cities Research Network. With a 2013 estimated population of 649,495, Denver ranks as the 22nd-most populous U.S. city. The 10-county Denver-Aurora-Lakewood, CO Metropolitan Statistical Area had an estimated 2013 population of 2,697,476 and ranked as the 21st most populous U.S. metropolitan statistical area. The 12-county Denver-Aurora, CO Combined Statistical Area had an estimated 2013 population of 3,277,309, which ranks as the 16th most populous U.S. metropolitan area. Denver is the most populous city of the Front Range Urban Corridor, an oblong urban region stretching across three states with population of 5,467,633 in 2010. Denver is the most-populous city within a radius and the third most-populous city in the Mountain West and the Southwestern United States after Phoenix, Arizona and El Paso, Texas.
History.
Denver City was founded in November 1858 as a mining town during the Pike's Peak Gold Rush in western Kansas Territory. That summer, a group of gold prospectors from Lawrence, Kansas, had arrived and established Montana City on the banks of the South Platte River. This was the first settlement in what was later to become the city of Denver. The site faded quickly, however, and by the summer of 1859 it was abandoned in favor of Auraria (named after the gold mining town of Auraria, Georgia), and St. Charles City.
On November 22, 1858, General William Larimer, a land speculator from eastern Kansas Territory, placed cottonwood logs to stake a claim on the bluff overlooking the confluence of the South Platte River and Cherry Creek, across the creek from the existing mining settlement of Auraria, and on the site of the existing townsite of St. Charles. Larimer named the town site Denver City to curry favor with Kansas Territorial Governor James W. Denver. Larimer hoped that the town's name would help make it the county seat of Arapaho County, but unknown to him Governor Denver had already resigned from office. The location was accessible to existing trails and was across the South Platte River from the site of seasonal encampments of the Cheyenne and Arapaho. The site of these first towns is now the site of Confluence Park in downtown Denver. Larimer, along with associates in the St. Charles City Land Company, sold parcels in the town to merchants and miners, with the intention of creating a major city that would cater to new emigrants. Denver City was a frontier town, with an economy based on servicing local miners with gambling, saloons, livestock and goods trading. In the early years, land parcels were often traded for grubstakes or gambled away by miners in Auraria. In May 1859, Denver City residents donated 53 lots to the Leavenworth & Pike's Peak Express in order to secure the region’s first overland wagon route. Offering daily service for "passengers, mail, freight, and gold," the Express reached Denver on a trail that trimmed westward travel time from twelve days to six. In 1863, Western Union furthered Denver’s dominance of the region by choosing the city for its regional terminus.
The Colorado Territory was created on February 28, 1861, Arapahoe County was formed on November 1, 1861, and Denver City was incorporated on November 7, 1861. Denver City served as the Arapahoe County Seat from 1861 until consolidation in 1902. In 1867, Denver City became the Territorial Capital. With its new-found importance, Denver City shortened its name to Denver. On August 1, 1876, Colorado was admitted to the Union.
Although by the close of the 1860s, Denver residents could look with pride at their success establishing a vibrant supply and service center, the decision to route the nation's first transcontinental railroad through Cheyenne, rather than Denver, threatened the prosperity of the young town. A daunting 100 miles away, citizens mobilized to build a railroad to connect Denver to the transcontinental railroad. Spearheaded by visionary leaders including Territorial Governor John Evans, David Moffat, and Walter Cheesman, fundraising began. Within three days, $300,000 had been raised, and citizens were optimistic. Fundraising stalled before enough was raised, forcing these visionary leaders to take control of the debt-ridden railroad. Despite challenges, on June 24, 1870, citizens cheered as the Denver Pacific completed the link to the transcontinental railroad, ushering in a new age of prosperity for Denver.
Finally linked to the rest of the nation by rail, Denver prospered as a service and supply center. The young city grew during these years, attracting millionaires with their mansions, as well as the poverty and crime of a rapidly growing city. Denver citizens were proud when the rich chose Denver and were thrilled that Horace Tabor, the Leadville mining millionaire, built an impressive business block at 16th and Larimer as well as the elegant Tabor Grand Opera House. Luxurious hotels, including the much-loved Brown Palace Hotel, soon followed, as well as splendid homes for millionaires like the Croke, Patterson, Campbell Mansion at 11th and Pennsylvania and the now-demolished Moffat Mansion at 8th and Grant. Intent on transforming Denver into one of the world's great cities, leaders wooed industry and enticed laborers to work in these factories. Soon, in addition to the elite and a large middle class, Denver had a growing population of German, Italian, and Chinese laborers, soon followed by African-Americans and Spanish-surname workers. Unprepared for this influx, the Silver Crash of 1893 unsettled political, social, and economic balances, laying the foundation for ethnic bigotry, such as the Red Scare and the rise of the Ku Klux Klan, as well as corruption and crime.
Between 1880 and 1895 the city experienced a huge rise in corruption, as crime bosses, such as Soapy Smith, worked side by side with elected officials and the police to control elections, gambling, and the bunko gangs. The city also experienced a depression in 1893 after the crash of silver prices. In 1887, the precursor to the international charity United Way was formed in Denver by local religious leaders who raised funds and coordinated various charities to help Denver's poor. By 1890, Denver had grown to be the second-largest city west of Omaha, Nebraska, but by 1900 it had dropped to third place behind San Francisco and Los Angeles. In 1900, whites represented 96.8% of Denver's population.
In 1901, the Colorado General Assembly voted to split Arapahoe County into three parts: a new consolidated City and County of Denver, a new Adams County, and the remainder of the Arapahoe County to be renamed South Arapahoe County. A ruling by the Colorado Supreme Court, subsequent legislation, and a referendum delayed the creation of the City and County of Denver until November 15, 1902.
Denver has hosted the Democratic National Convention twice, during the years of 1908, and again in 2008, taking the opportunity to promote the city's status on the national, political, and socioeconomic stage.
Early in the 20th century, Denver, like many other cities, was home to a pioneering Brass Era car company. The Colburn Automobile Company made cars copied from the contemporary Renault.
From 1953 to 1989, the Rocky Flats Plant, a DOE nuclear weapon facility formerly located about 15 miles from Denver, produced fissile plutonium "pits" for nuclear warheads. A major fire at the facility in 1957, as well as leakage from nuclear waste stored at the site between 1958 and 1968, resulted in the contamination of some parts of Denver, to varying degrees, with plutonium-239, a harmful radioactive substance with a half-life of 24,200 years. A study by the Jefferson County health director, Dr. Carl Johnson, in 1981 linked the contamination to an increase in birth defects and cancer incidence in central Denver and nearer Rocky Flats. Later studies confirmed many of his findings. Plutonium contamination was still present outside the former plant site , and presents risks to building the envisioned Jefferson Parkway, which would complete Denver's automotive beltway.
Denver was selected in 1970 to host the 1976 Winter Olympics to coincide with Colorado's centennial celebration, but in November 1972 Colorado voters struck down ballot initiatives allocating public funds to pay for the high costs of the games, which were subsequently moved to Innsbruck, Austria. The notoriety of becoming the only city ever to decline to host an Olympiad after being selected has made subsequent bids difficult. The movement against hosting the games was based largely on environmental issues and was led by State Representative Richard Lamm, who was subsequently elected to three terms (1975–87) as Colorado governor. Denver explored a potential bid for the 2022 Winter Olympics, but no bid will be submitted.
In 2010, Denver adopted a comprehensive update of its zoning code. The new zoning was developed to guide development as envisioned in adopted plans such as Blueprint Denver, Transit Oriented Development Strategic Plan, Greenprint Denver, and the Strategic Transportation Plan.
Denver has also been known historically as the "Queen City of the Plains" and the "Queen City of the West", because of its important role in the agricultural industry of the high-plains region in eastern Colorado and along the foothills of the Colorado Front Range. Several US Navy ships have been named USS "Denver" in honor of the city.
Geography.
Denver is located in the center of the Front Range Urban Corridor, between the Rocky Mountains to the west and the High Plains to the east. Denver's topography consists of plains in the city center with hilly areas to the north, west and south. According to the United States Census Bureau the city has a total area of , of which is land and (1.1%) is water. The City and County of Denver is surrounded by only three other counties: Adams County to the north and east, Arapahoe County to the south and east, and Jefferson County to the west.
Although Denver's nickname is the "Mile-High City" because its official elevation is one mile above sea level, defined by the elevation of the spot of a benchmark on the steps of the State Capitol building, the elevation of the entire city ranges from . According to Geographic Names Information System (GNIS) and the National Elevation Dataset, the city's elevation is , which is reflected on various websites such as that of the National Weather Service.
Neighborhoods.
As of January 2013, the City and County of Denver has defined 78 official neighborhoods that the city and community groups use for planning and administration. Although the city's delineation of the neighborhood boundaries is somewhat arbitrary, it corresponds roughly to the definitions used by residents. These "neighborhoods" should not be confused with cities or suburbs, which may be separate entities within the metro area.
The character of the neighborhoods varies significantly from one to another and includes everything from large skyscrapers to houses from the late 19th century to modern, suburban style developments. Generally, the neighborhoods closest to the city center are denser, older and contain more brick building material. Many neighborhoods away from the city center were developed after World War II, and are built with more modern materials and style. Some of the neighborhoods even farther from the city center, or recently redeveloped parcels anywhere in the city have either very suburban characteristics or are new urbanist developments that attempt to recreate the feel of older neighborhoods. Most neighborhoods contain parks or other features that are the focal point for the neighborhood.
Denver does not have larger area designations, unlike the City of Chicago which has larger areas that house the neighborhoods (IE: Northwest Side). Denver residents use the terms "north" "south" "east" and "west" loosely.
Denver also has a number of neighborhoods not reflected in the administrative boundaries. These neighborhoods may reflect the way people in an area identify themselves or they might reflect how others, such as real estate developers, have defined those areas. Well-known non-administrative neighborhoods include the historic and trendy LoDo (short for "Lower Downtown"), part of the city's Union Station neighborhood; Uptown, straddling North Capitol Hill and City Park West; Curtis Park, part of the Five Points neighborhood; Alamo Placita, the northern part of the Speer neighborhood; Park Hill, a successful example of intentional racial integration; and Golden Triangle, in the Civic Center.
Climate.
Denver lies within the semi-arid, continental climate zone (Köppen climate classification "BSk"). It has four distinct seasons and receives a modest amount of precipitation spread through the year. Due to its inland location on the High Plains, at the foot of the Rocky Mountains, Denver, like all cities along the eastern edge of the Rocky Mountains, is subject to sudden changes in weather. The climate is very sunny, averaging 3,106 hours or 300 days of sunshine a year. July is the warmest month, with a daily average temperature of . Summers range from mild to hot with occasional afternoon thunderstorms and high temperatures reaching on 38 days annually, and occasionally . December, the coldest month of the year, has a daily average temperature of . Winters range from mild to occasional bitter cold, with periods of snow and low temperatures alternating with periods of mild weather, the result of chinook winds. In winter, highs can occasionally reach up to and can also sometimes fail to reach . Snowfall is common throughout the late fall, winter and early spring, averaging for 1981−2010. The average window for measurable (≥) snow is October 17 through April 27 although measurable snowfall has fallen in Denver as early as September 4 and as late as June 3. Extremes in temperature range from on January 9, 1875 up to as recently as June 25 and 26, 2012. Tornadoes are rare in Denver, though one notable exception was an F3 tornado that struck 4.4 miles south of downtown on June 15, 1988. However, the suburbs of Denver, and the city's east-northeastern extension which is the Denver International Airport, can see a few small tornadoes in the spring and summer months, especially during June in the Denver Convergence Vorticity Zone (DCVZ). The DCVZ, also known as the Denver Cyclone, is a variable vortex of storm-forming air flow usually found north and east of downtown, and which often includes the airport. Heavy weather from the DCVZ can disrupt airport operations.
Demographics.
As of the 2010 census, the population of the City and County of Denver was 600,158, making it the 24th most populous U.S. city. The Denver-Aurora-Lakewood, CO Metropolitan Statistical Area had an estimated 2013 population of 2,697,476 and ranked as the 21st most populous U.S. metropolitan statistical area, and the larger Denver-Aurora-Boulder Combined Statistical Area had an estimated 2013 population of 3,277,309 and ranked as the 16th most populous U.S. metropolitan area. Denver is the most populous city within a radius centered in the city and of magnitude. Denverites is a term used for residents of Denver.
According to the 2010 census, the City and County of Denver contains 600,158 people and 285,797 households. The population density is 3,698 inhabitants per square mile (1,428/km²) including the airport. There are 285,797 housing units at an average density of 1,751 per square mile (676/km²). However, the average density throughout most Denver neighborhoods tends to be higher. Without the 80249 zip code (47.3 sq mi, 8,407 residents) near the airport, the average density increases to around 5,470 per square mile.
According to the 2010 United States Census, the racial composition of Denver was as follows:
Approximately 70.3% of the population (over five years old) spoke only English at home. An additional 23.5% of the population spoke Spanish at home. In terms of ancestry, 31.2% were Mexican, 14.6% of the population were of German ancestry, 9.7% were of Irish ancestry, 8.9% were of English ancestry, and 4.0% were of Italian ancestry.
There are 250,906 households, of which 23.2% have children under the age of 18 living with them, 34.7% are married couples living together, 10.8% have a female householder with no husband present, and 50.1% are non-families. 39.3% of all households are made up of individuals and 9.4% have someone living alone who is 65 years of age or older. The average household size is 2.27 and the average family size is 3.14.
Age distribution is 22.0% under the age of 18, 10.7% from 18 to 24, 36.1% from 25 to 44, 20.0% from 45 to 64, and 11.3% who are 65 years of age or older. The median age is 33 years. For every 100 females there are 102.1 males.
The median household income is $45,438, and the median family income is $48,195. Males have a median income of $36,232 versus $33,768 for females. The per capita income for the city is $24,101. 19.1% of the population and 14.6% of families are below the poverty line. Out of the total population, 25.3% of those under the age of 18 and 13.7% of those 65 and older are living below the poverty line.
Languages.
As of 2010, 72.28% (386,815) of Denver residents aged five and older spoke only English at home, while 21.42% (114,635) spoke Spanish, 0.85% (4,550) Vietnamese, 0.57% (3,073) African languages, 0.53% (2,845) Russian, 0.50% (2,681) Chinese, 0.47% (2,527) French, and German by 0.46% (2,465) of the population over the age of five. In total, 27.72% (148,335) of Denver's population age five and older spoke a language other than English.
Economy.
The Denver MSA has a gross metropolitan product of $157.6 billion in 2010, making it the 18th largest metro economy in the United States. Denver's economy is based partially on its geographic position and its connection to some of the major transportation systems of the country. Because Denver is the largest city within , it has become a natural location for storage and distribution of goods and services to the Mountain States, Southwest states, as well as all western states. Another benefit for distribution is that Denver is nearly equidistant from large cities of the Midwest, such as Chicago and St. Louis and some large cities of the West Coast, such as Los Angeles and San Diego.
Over the years, the city has been home to other large corporations in the central United States, making Denver a key trade point for the country. Several well-known companies originated in or have relocated to Denver. William Ainsworth opened the Denver Instrument Company in 1895 to make analytical balances for gold assayers. Its factory is now in Arvada. AIMCO (NYSE: AIV)—the largest owner and operator of apartment communities in the United States, with approximately 870 communities comprising nearly 136,000 units in 44 states—is headquartered in Denver, employing approximately 3,500 people. Also Samsonite Corp., the world's largest luggage manufacturer, began in Denver in 1910 as Shwayder Trunk Manufacturing Company, but Samsonite closed its NE Denver factory in 2001, and moved its headquarters to Massachusetts after a change of ownership in 2006. The Mountain States Telephone & Telegraph Company, founded in Denver in 1911, is now a part of telecommunications giant CenturyLink.
MediaNews Group purchased the "Denver Post" in 1987; the company is based in Denver. The Gates Corporation, the world's largest producer of automotive belts and hoses, was established in S. Denver in 1919. Russell Stover Candies Inc. made its first chocolate candy in Denver in 1923, but moved to Kansas City in 1969. The Wright & McGill Company has been making its Eagle Claw brand of fishing gear in NE Denver since 1925. The original Frontier Airlines began operations at Denver's old Stapleton International Airport in 1950. Frontier was reincarnated at DIA in 1994. Scott's Liquid Gold, Inc., has been making furniture polish in Denver since 1954. Village Inn restaurants began as a single pancake house in Denver in 1958. Big O Tires, LLC, of Centennial opened its first franchise in 1962 in Denver. The Shane Company sold its first diamond jewelry in 1971 in Denver. Johns Manville Corp., a manufacturer of insulation and roofing products, relocated its headquarters to Denver from New York in 1972. CH2M HILL Inc., an engineering and construction firm, relocated from Oregon to the Denver Technological Center in 1980. The Ball Corporation sold its glass business in Indiana in the 1990s and moved to suburban Broomfield. Ball has several operations in greater Denver. 
Molson Coors Brewing Company established its U.S. headquarters in Denver in 2005. Its subsidiary and regional wholesale distributor, Coors Distributing Company, is in NW Denver. The Newmont Mining Corporation, the 2nd largest gold producer in North America and one of the largest in the world, is headquartered in Denver.
Large Denver-area employers that have headquarters elsewhere include Lockheed Martin Corp., United Airlines, Kroger Co. and Xcel Energy, Inc. , an online site for maps, directions and business listings, is headquartered in Denver's LODO district. Online Trading Academy, a professional trader education company, has an office and center that is operating in Denver.
Geography also allows Denver to have a considerable government presence, with many federal agencies based or having offices in the Denver area. Along with federal agencies come many companies based on US defense and space projects, and more jobs are brought to the city by virtue of its being the capital of the state of Colorado. The Denver area is home to the former nuclear weapons plant Rocky Flats, the Denver Federal Center, the Denver Mint and the National Renewable Energy Laboratory.
In 2005, a $310.7 million expansion for the Colorado Convention Center was completed, doubling its size. The hope was that the center's expansion would elevate the city to one of the top 10 cities in the nation for holding a convention.
Denver's position near the mineral-rich Rocky Mountains encouraged mining and energy companies to spring up in the area. In the early days of the city, gold and silver booms and busts played a large role in the economic success of the city. In the 1970s and early 1980s, the energy crisis in America and resulting high oil prices created an energy boom in Denver captured in the soap opera "Dynasty". Denver was built up considerably during this time with the construction of many new downtown skyscrapers. When the price of oil dropped from $34 a barrel in 1981 to $9 a barrel in 1986, the Denver economy dropped with it, leaving almost 15,000 oil industry workers in the area unemployed (including former mayor and current governor John Hickenlooper, a former geologist), and the highest office vacancy rate in the nation (30%). Since then, the industry has recovered and there remain 700 employed petroleum engineers in the region. Advances is hydraulic fracturing has made the DJ Basin of Colorado into an accessible and lucrative oil play. Energy and mining are still important in Denver's economy today, with companies such as EnCana, Halliburton, Smith International, Rio Tinto Group, Newmont Mining, Noble Energy, and Anadarko.
Denver's west-central geographic location in the Mountain Time Zone (UTC−7) also benefits the telecommunications industry by allowing communication with both North American coasts, South America, Europe, and Asia in the same business day. Denver's location on the 105th meridian at over one mile (1.6 km) in elevation also enables it to be the largest city in the U.S. to offer a "one-bounce" real-time satellite uplink to six continents in the same business day. Qwest Communications, Dish Network Corporation, Starz-Encore, DIRECTV, and Comcast are a few of the many telecommunications companies with operations in the Denver area. These and other high-tech companies had a boom in Denver in the mid to late 1990s. Denver had one of the lowest unemployment rates in the nation at 3.8% in October 2007. The Downtown region has seen increased real estate investment with the construction of several new skyscrapers set to be completed in 2010–2013.
Denver has also enjoyed success as a pioneer in the fast casual restaurant industry, with many popular national chain restaurants founded and based in Denver. Chipotle Mexican Grill, Quizno's, and Smashburger were founded and headquartered in Denver. Qdoba Mexican Grill, Noodles & Company, and Good Times Burgers & Frozen Custard originated in Denver, but have moved their headquarters to the nearby suburbs of Wheat Ridge, Broomfield, and Golden.
In 2013, Denver ranked No. 6 on "Forbes"' list of the Best Places for Business and Careers.
Culture and contemporary life.
Apollo Hall opened quickly after the city's founding in 1859 and staged many plays for eager settlers. In the 1880s Horace Tabor built Denver's first Opera House. After the start of the 20th century, city leaders embarked on a city beautification program that created many of the city's parks, parkways, museums, and the Municipal Auditorium, which was home to the 1908 Democratic National Convention and is now known as the Ellie Caulkins Opera House. Denver and the metropolitan areas around it continued to support culture. In 1988, voters in the Denver Metropolitan Area approved the Scientific and Cultural Facilities Tax (commonly known as SCFD), a 1 cent sales tax that contributes money to various cultural and scientific facilities and organizations throughout the Metro area. The tax was renewed by voters in 1994 and 2004 and allows the SCFD to operate until 2018.
Denver is home to many nationally recognized museums, including a new wing for the Denver Art Museum by world-renowned architect Daniel Libeskind, the second largest Performing Arts Center in the nation after Lincoln Center in New York City and bustling neighborhoods such as LoDo, filled with art galleries, restaurants, bars and clubs. That is part of the reason why Denver was recently recognized for the third year in a row as the best city for singles. Denver's neighborhoods also continue their influx of diverse people and businesses while the city's cultural institutions grow and prosper. The city acquired the estate of abstract expressionist painter Clyfford Still in 2004 and built a museum to exhibit his works near the Denver Art Museum.
The Denver Museum of Nature and Science currently holds an aquamarine specimen valued at over one million dollars, as well as specimens of the state mineral, rhodochrosite. Every September the Denver Mart, located at 451 E. 58th Avenue hosts a gem and mineral show. The state history museum, History Colorado Center, opened in April 2012. It features hands-on and interactive exhibits, artifacts and programs about Colorado history. It was named in 2013 by "True West Magazine" as one of the top-ten "must see" history museums in the country. History Colorado's Byers-Evans House Museum and the Molly Brown House are nearby.
While Denver may not be as recognized for historical musical prominence as some other American cities, it still manages to have a very active pop, jazz, jam, folk, and classical music scene, which has nurtured several artists and genres to regional, national, and even international attention. Of particular note is Denver's importance in the folk scene of the 1960s and 1970s. Well-known folk artists such as Bob Dylan, Judy Collins and John Denver lived in Denver at various points during this time, and performed at local clubs. Also, three members of the widely popular group Earth, Wind, and Fire are from Denver. More recent Denver-based artists include The Lumineers, Air Dubai, The Fray, Flobots, Cephalic Carnage, Axe Murder Boyz, Deuce Mob, and Five Iron Frenzy.
Because of its proximity to the mountains, and generally sunny weather, Denver has gained a reputation as being a very active, outdoor oriented city. Many Denver residents spend the weekends in the mountains; either skiing in the winter or hiking, climbing, kayaking and camping in the summer.
Additionally, Denver and the surrounding cities of the Front Range are home to a large number of local and national breweries. Many restaurants in the region have on-site breweries, and some of the larger brewers, including Coors and the New Belgium Brewing Company, offer tours. The city also welcomes visitors from around the world when it hosts the annual Great American Beer Festival each fall.
Denver used to be a major trading center for beef and livestock when ranchers would drive (or later transport) cattle to the Denver Union Stockyards for sale. As a celebration of that history, each year for more than a century, Denver hosts the National Western Stock Show,
attracting as many as 10,000 animals and 700,000 attendees. The National Western Stock Show is held every January at the National Western Complex, northeast of downtown.
Denver hosts four large Mexican American celebrations: Cinco de Mayo (with over 500,000 attendees), in May, El Grito de la Independencia, in September, the annual Lowrider show, and the Dia De Los Muertos art shows/events in North Denver's Highland neighborhood, and the Lincoln Park neighborhood in the original section of West Denver.
Denver is also famous for its dedication to New Mexican cuisine and the Chile. It's best known for its Green and Red Chile sauce, Colorado Burrito, Southwest (Denver) Omelette, Breakfast Burrito, Chiles rellenos, and Tamales most notably. Denver has a very large population of Mexican Americans (one of the country's largest), and is famous for many other southwest cuisine dishes as well. Denver is also well known for other types of food such as, Rocky Mountain oysters, Rainbow trout, and the Denver sandwich.
The Dragon Boat Festival in July, Moon Festival in September and Chinese New Year are annual events in Denver for the Chinese and Asian residents. Chinese hot pot (huo guo) and Korean BBQ restaurants have been growing in popularity. The Denver area has 2 Chinese newspapers, the and the .
Denver is also the setting for "The Bill Engvall Show", and the setting for of MTV's "The Real World". It was also the setting for the prime time drama "Dynasty" from 1981 to 1989 (although the show was mostly filmed in Los Angeles). From 1998 to 2002, the city's Alameda East Veterinary Hospital was home to the Animal Planet series "Emergency Vets", which spun off three one-off documentary specials and the current Animal Planet series "E-Vet Interns". The city is also the setting for the Disney Channel Original TV Show, "Good Luck Charlie", which is currently in its third season.
Sports.
Denver is home to a variety of sports teams and is one of the U.S. cities with teams from four major sports. The Denver Broncos of the National Football League, have drawn crowds of over 70,000 since their American Football League origins in the early 1960s, and continue to draw fans today to their current home Sports Authority Field at Mile High. The Broncos have a sold out every home game (except for strike-replacement games) since 1970. The Broncos are the current American Football Conference champions, but lost Super Bowl XLVIII to the Seattle Seahawks. In total, the Broncos have advanced to the Super Bowl seven times and won back-to-back titles in 1998 and '99. In the 1980s and 1990s, one of the top priorities of former Mayor Federico Peña was bringing major league baseball to the city, an effort which culminated in the creation of the Colorado Rockies as an expansion franchise in 1993 and the opening of Coors Field in 1995. The Rockies advanced to the playoffs in 1995, but were eliminated in the first round. In 2007, their late-season winning streak saw them advance to the playoffs as a wild-card entrant, win the NL Championship Series, and bring the World Series to Denver for the first time, where they were swept in four games by the Boston Red Sox.
Denver is also home to the Colorado Avalanche, a National Hockey League team that relocated from Quebec City in 1995. They have won two Stanley Cups (1996 and 2001) while in Denver, and play at Pepsi Center. The Denver Nuggets of the National Basketball Association also play at the Pepsi Center. The Major League Soccer team Colorado Rapids play in Dick's Sporting Goods Park, an 18,000 seat soccer-specific stadium opened for the 2007 MLS season, located in the Denver suburb of Commerce City. The Rapids won the MLS Cup in 2010.
Denver submitted the winning bid to host the 1976 Winter Olympics, but subsequently withdrew, giving it the dubious distinction of being the only city to back out after winning a bid to host the Olympics. In 2006 Denver established a Major League Lacrosse team, the Denver Outlaws. They play in Sports Authority Field at Mile High. In 2006, the Denver Outlaws won the Western Conference Championship. The Colorado Mammoth of the National Lacrosse League play at the Pepsi Center.
Parks and recreation.
As of 2006, Denver had over 200 parks, from small mini-parks all over the city to the giant City Park. Denver also has 29 recreation centers providing places and programming for resident's recreation and relaxation.
Many of Denver's parks were acquired from state lands in the late 19th and early 20th centuries. This coincided with the City Beautiful movement, and Denver mayor Robert Speer (1904–12 and 1916–18) set out to expand and beautify the city's parks. Reinhard Schuetze was the city's first landscape architect, and he brought his German-educated landscaping genius to Washington Park, Cheesman Park, and City Park among others. Speer used Schuetze as well as other landscape architects such as Frederick Law Olmsted, Jr. and Saco Rienk DeBoer to design not only parks such as Civic Center Park, but many city parkways and tree-lawns. All of this greenery was fed with South Platte River water diverted through the city ditch.
In addition to the parks within Denver itself, the city acquired land for mountain parks starting in the 1911s. Over the years, Denver has acquired, built and maintained approximately of mountain parks, including Red Rocks Park, which is known for its scenery and musical history revolving around the unique Red Rocks Amphitheatre. Denver also owns the mountain on which the Winter Park Resort ski area is operated in Grand County, west of Denver. City parks are important places for both Denverites and visitors, inciting controversy with every change. Denver continues to grow its park system with the development of many new parks along the Platte River through the city, and with Central Park and Bluff Lake Nature Center in the Stapleton neighborhood redevelopment. All of these parks are important gathering places for residents and allow what was once a dry plain to be lush, active, and green. Denver is also home to a large network of public community gardens, most of which are managed by Denver Urban Gardens, a non-profit organization.
Since 1974, Denver and the surrounding jurisdictions have rehabilitated the urban South Platte River and its tributaries for recreational use by hikers and cyclists. The main stem of the South Platte River Greenway runs along the South Platte from Chatfield Reservoir into Adams County in the north. The Greenway project is recognized as one of the best urban reclamation projects in the U.S., winning, for example, the Silver Medal Rudy Bruner Award for Urban Excellence in 2001.
In its 2013 ParkScore ranking, The Trust for Public Land, a national land conservation organization, reported that Denver had the 17th best park system among the 50 most populous U.S. cities.
Government.
Denver is a consolidated city-county with a mayor elected on a nonpartisan ballot, a 13-member city council and an auditor. The Denver City Council is elected from 11 districts with two at-large council-members and is responsible for passing and changing all laws, resolutions, and ordinances, usually after a public hearing, and can also call for misconduct investigations of Denver's departmental officials. All elected officials have four-year terms, with a maximum of three terms. The current mayor is Michael Hancock.
Denver has a strong mayor/weak city council government. The mayor can approve or veto any ordinances or resolutions approved by the council, makes sure all contracts with the city are kept and performed, signs all bonds and contracts, is responsible for the city budget, and can appoint people to various city departments, organizations, and commissions. However, the council can override the mayor's veto with a nine out of thirteen member vote, and the city budget must be approved and can be changed by a simple majority vote of the council. The auditor checks all expenditures and may refuse to allow specific ones, usually based on financial reasons.
The Denver Department of Safety oversees three branches: the Denver Police Department, Denver Fire Department, and Denver Sheriff Department. The Denver County Court is an integrated Colorado County Court and Municipal Court and is managed by Denver instead of the state.
Politics.
While Denver elections are non-partisan, Democrats have long held the majority sway on Denver politics with most officials elected citywide having Democratic Party affiliation. In federal elections, Denverites also tend to vote for Democratic candidates, voting for the Democratic Presidential nominee in every election since 1960 (excluding 1980 and 1972). The office of Denver's Mayor has been occupied by a Democrat since the municipal general election of 1963. Denver is represented at the federal level by congresswoman Diana DeGette, a Democrat representing Colorado's 1st congressional district, which includes all of Denver and parts of Arapahoe County.
Benjamin F. Stapleton was the mayor of Denver, Colorado, for two periods, the first from 1923 to 1931 and the second from 1935 to 1947. Stapleton was responsible for many civic improvements during his term, notably during his second stint as mayor when he had access to funds and manpower from the New Deal. During this time, the park system was considerably expanded and the Civic Center completed. His signature project was the construction of Denver Municipal Airport, which began in 1929 amidst heavy criticism. It was later renamed Stapleton International Airport in his honor. Today, the airport no longer stands, but has been replaced by a neighborhood also named Stapleton. Stapleton Street continues to bear his name.
During the 1960s and 1970s, Denver was one of the epicenters of the Chicano Movement. The boxer-turned-activist Rodolfo "Corky" Gonzales formed an organization called the Crusade for Justice, which battled police brutality, fought for bilingual education, and, most notably, hosted the First National Chicano Youth Liberation Conference in March 1969.
In recent years, Denver has taken a stance on helping people who are or become homeless, particularly under the administrations of mayors John Hickenlooper and Wellington Webb. Denver's homeless population is considerably lower than many other major cities, but residents of the city streets suffer Denver winters. Although mild and dry much of the time, Denver winters can have brief periods of cold temperatures and snow.
In 2005, Denver became the first major city in the U.S. to vote to make the private possession of less than an ounce of marijuana legal for adults 21 and older. The city voted 53.5 percent in favor of the marijuana legalization measure, which, as then-mayor John Hickenlooper pointed out, was without effect, because the city cannot usurp state law, which at that time treated marijuana possession in much the same way as a speeding ticket, with fines of up to $100 and no jail time. Denver passed an initiative in the fourth quarter of 2007 requiring the mayor to appoint an 11 member review panel to monitor the city's compliance with the 2005 ordinance. In 2012, Colorado Amendment 64 was signed into law by Governor John Hickenlooper, making Colorado the first state to legalize recreational marijuana use.
Former Denver mayor John Hickenlooper was a member of the Mayors Against Illegal Guns Coalition, an organization formed in 2006 and co-chaired by New York City mayor Michael Bloomberg and Boston mayor Thomas Menino.
Denver hosted the 2008 Democratic National Convention, which was the centennial of the city's first hosting of the landmark 1908 convention. It also hosted the G7 (now G8) summit between June 20 and June 22 in 1997 and the 2000 National Convention of the Green Party.
On October 31, 2011 it was announced that The University of Denver in Denver was selected as the host of the first of three 2012 presidential debates to be held on October 3, 2012.
Taxes.
The City and County of Denver levies an Occupational Privilege Tax (OPT or Head Tax) on employers and employees.
Education.
Denver Public Schools (DPS) is the public school system in Denver. It currently educates about 73,000 students in 73 elementary schools, 15 K-8 schools, 17 middle schools, 14 high schools, and 19 charter schools. The first school of what is now DPS was a log cabin that opened in 1859 on the corner of 12th Street between Market and Larimer Streets. The district boundaries are coextensive with the city limits. The Cherry Creek School District serves some areas with Denver postal addresses that are outside the city limits.
Denver's many colleges and universities range in age and study programs. Three major public schools constitute the Auraria Campus, University of Colorado Denver, Metropolitan State University of Denver, and Community College of Denver. The private University of Denver was the first institution of higher learning in the city and was founded in 1864. Other prominent Denver higher education institutions include Johnson & Wales University, Catholic (Jesuit) Regis University and the city has Roman Catholic and Jewish institutions, as well as a health sciences school. In addition to those schools within the city, there are a number of schools located throughout the surrounding metro area.
Media.
The Denver Metropolitan Area is served by a variety of media outlets in print, radio, television, and the Internet.
Television stations.
Denver is the 16th-largest market in the country for television, according to the 2009–2010 from Nielsen Media Research.
Radio stations.
Denver is also served by over 40 AM and FM radio stations, covering a wide variety of formats and styles. Denver-Boulder radio is the No. 19 market in the United States, according to the Spring 2011 ranking (up from No. 20 in Fall 2009).
For a list of radio stations, see Radio Stations in Colorado
Print.
After a continued rivalry between Denver's two main newspapers, the "Denver Post" and "Rocky Mountain News", the papers merged operations in 2001 under a Joint Operating Agreement which formed the Denver Newspaper Agency until February 2009 when E. W. Scripps Company, the owner of the Rocky Mountain News closed the paper. There are also several alternative or localized newspapers published in Denver, including the "Westword", "The Onion" and "Out Front Colorado". Denver is home to multiple regional magazines such as "5280", which takes its name from the city's mile-high () elevation.
Transportation.
City streets.
Most of Denver has a straightforward street grid oriented to the four cardinal directions. Blocks are usually identified in hundreds from the median streets, identified as "00", which are Broadway (the east–west median, running north–south) and Ellsworth Avenue (the north–south median, running east–west). Colfax Avenue, a major east–west artery through Denver, is 15 blocks (1500) north of the median. Avenues north of Ellsworth are numbered (with the exception of Colfax Avenue and several others, such as Martin Luther King, Jr. Blvd and Montview Blvd.), while avenues south of Ellsworth are named.
There is also an older downtown grid system that was designed to be parallel to the confluence of the South Platte River and Cherry Creek. Most of the streets downtown and in LoDo run northeast–southwest and northwest–southeast. This system has an unplanned benefit for snow removal; if the streets were in a normal N–S/E–W grid, only the N–S streets would receive sunlight. With the grid oriented to the diagonal directions, the NW–SE streets receive sunlight to melt snow in the morning and the NE–SW streets receive it in the afternoon. This idea was from Henry Brown the founder of the Brown Palace Hotel. There is now a plaque across the street from the Brown Palace Hotel which honors this idea. The NW–SE streets are numbered, while the NE–SW streets are named. The named streets start at the intersection of Colfax Avenue and Broadway with the block-long Cheyenne Place. The numbered streets start underneath the Colfax and I-25 viaducts. There are 27 named and 44 numbered streets on this grid. There are also a few vestiges of the old grid system in the normal grid, such as Park Avenue, Morrison Road, and Speer Boulevard. Larimer Street, named after William Larimer, Jr., the founder of Denver, which is located in the heart of LoDo, is the oldest street in Denver.
All roads in the downtown grid system are streets (e.g. 16th Street, Stout Street). Roads outside that system that travel east/west are given the suffix "avenue" and those that head north and south are given the "street" suffix (e.g. Colfax Avenue, Lincoln Street). Boulevards are higher capacity streets and travel any direction (more commonly north and south). Smaller roads are sometimes referred to as places, drives (though not all drives are smaller capacity roads, some are major thoroughfares) or courts. Most streets outside the area between Broadway and Colorado Boulevard are organized alphabetically from the city's center.
Many Denver streets have bicycle lanes, and there are over 850 miles of paved, off-road, bike paths in Denver parks and along bodies of water, like Cherry Creek and the South Platte. This allows for a significant portion of Denver's population to be bicycle commuters and has led to Denver being known as a bicycle friendly city. In addition to the many bike paths, Denver launched B-Cycle – a city-wide bicycle sharing program – in late April 2010. The B-Cycle network was the largest in the United States at the time of its launch, boasting 400 bicycles.
The Denver Boot, a car-disabling device was first used in Denver.
Cycling.
The League of American Bicyclists has rated Colorado as the sixth most bicycle-friendly state in the nation for the year 2014. This is due in large part to Front Range cities like Boulder, Fort Collins and Denver placing an emphasis on legislation, programs and infrastructure developments that promote cycling as a mode of transportation. Walk score has rated Denver as the third most bicycle-friendly large city in the United States.
Many Denver streets have bicycle lanes, and there are over 850 miles of paved, off-road, bike paths in Denver parks and along bodies of water, like Cherry Creek and the South Platte. This allows for a significant portion of Denver's population to be bicycle commuters and has led to Denver being known as a bicycle friendly city. According to data from the 2011 American Community Survey, Denver ranks 6th among US cities with populations over 400,000 in terms of the percentage of workers who commute by bicycle at 2.2% of commuters. In addition to the many bike paths, Denver launched B-Cycle – a city-wide bicycle sharing program – in late April 2010. The B-Cycle network was the largest in the United States at the time of its launch, boasting 400 bicycles. Through the acquisition of new grants, the program has continued to expand each year, adding dozens of new stations, hundreds of bikes, and by beginning service during the winter months.
Walkability.
A 2011 study by Walk Score ranked Denver sixteenth most walkable of fifty largest U.S. cities.
Freeways and highways.
Denver is primarily served by the interstate freeways I-25 and I-70. The intersection of the two interstates is referred to locally as "the mousetrap", because when viewed from the air, the junction (and subsequent vehicles) resemble mice in a large trap.
Denver also has a nearly complete beltway known as "the 470's". These are SH 470 (also known as C-470), a freeway in the southwest Metro area, and two toll highways, E-470 (from southeast to northeast) and Northwest Parkway (from terminus of E-470 to US 36). SH 470 was originally intended to be I-470 and built with federal highway funds, but the funding was redirected to complete conversion of downtown Denver's 16th Street to a pedestrian mall. As a result, construction was delayed until 1980 after state and local legislation was passed. I-470 was also once called "The Silver Stake Highway", from Gov. Lamm's declared intention to drive a silver stake through it and kill it.
A highway expansion and transit project for the southern I-25 corridor, dubbed T-REX (Transportation Expansion Project), was completed on November 17, 2006. The project installed wider and additional highway lanes, and improved highway access and drainage. The project also includes a light rail line that traverses from downtown to the south end of the metro area at Lincoln Avenue. The project spanned almost along the highway with an additional line traveling parallel to part of I-225, stopping just short of Parker Road.
Metro Denver highway conditions can be accessed on the Colorado Department of Transportation website Traffic Conditions.
Mass transportation.
Mass transportation throughout the Denver metropolitan area is managed and coordinated by the Regional Transportation District (RTD). RTD currently operates more than 1,000 buses serving over 10,000 bus stops in 38 municipal jurisdictions in eight counties around the Denver and Boulder metropolitan areas. Additionally, RTD operates six light rail lines, the C, D, E, F, W, and H with a total of of track, serving 36 stations. FasTracks is a light rail/bus/rail expansion project approved by voters in 2004 which will serve neighboring suburbs and communities. The W line, or West line, opened in April 2013 serving Golden/Federal Center. Currently, RTD is expanding the rail through Aurora along I-225 and building a commuter line along I-70 to connect Downtown with Stapleton and the Denver International Airport.
Greyhound Lines, the intercity bus operator, has a major hub in Denver, with routes to New York City, Portland, Reno, Las Vegas, and their headquarters, Dallas. Subsidiary Autobuses Americanos provides service to El Paso. Allied bus operators Black Hills Trailways, and Burlington Trailways provide service to Billings, Omaha, Indianapolis, and Alamosa. 
Amtrak, the national passenger rail system, provides service to Denver, operating its "California Zephyr" daily in both directions between Chicago and Emeryville, California, across the bay from San Francisco. Amtrak Thruway service operated by private bus companies links the Denver station with Rocky Mountain points.
At Albuquerque, New Mexico, Denver Thruway connections are made daily with the Amtrak "Southwest Chief". Additionally, the Ski Train operated on the former Denver & Rio Grande Western Railroad, which took passengers between Denver and the Winter Park Ski Resort, but it is no longer in service. The Ski Train made its final run to Winter Park on March 29, 2009.
Denver's early years as a major train hub of the west are still very visible today. Trains stop in Denver at historic Union Station, where travelers can access RTD's 16th Street Free MallRide or use light rail to tour the city. Union Station will also serve as the main juncture for rail travel in the metro area, at the completion of FasTracks.
Visitors to Union Station can also experience Railroading in the Rockies from the 1950s by checking out the model railroad clubs in the basement. The Denver Society of Model Railroaders opens its display on the last Friday of the month except during the summer. The Platte Valley & Western Model Railroad Club is open every Friday night year round and offers visitors a chance to view how Denver looked during the 1950s. The club also offers behind the scenes tours as well as educational events and information to visitors. The Railroad Club is currently closed due to construction in Union Station and will reopen on November 28, 2014
Airports.
Denver International Airport (IATA: DEN, ICAO: KDEN), commonly known as DIA, serves as the primary airport for a large region surrounding Denver. DIA is located east-northeast of the Colorado State Capitol. DIA is the tenth busiest airport in the world and ranks fourth in the United States, with 51,245,334 passengers passing through it in 2008. It covers more than , making it the largest airport by land area in the United States and larger than the island of Manhattan. Denver serves as a major hub for United Airlines, is the headquarters for Frontier Airlines, and is the fastest-growing focus city for Southwest Airlines.
Three general aviation airports serve the Denver area. Rocky Mountain Metropolitan Airport (KBJC) is north-northwest, Centennial Airport (KAPA) is south-southeast, and Front Range Airport (KFTG) is located east of the state capitol.
In the past, Denver has been home to several other airports that are no longer operational. Stapleton International Airport was closed in 1995 when it was replaced by DIA. Lowry Air Force Base was a military flight training facility that ceased flight operations in 1966, with the base finally being closed in 1994. It is currently being used for residential purposes. Buckley Air Force Base, a former Air National Guard base, is currently the only military facility in the Denver-Metro area.
Sister cities.
Denver's relationship with Brest, France, began in 1948, making it the second-oldest sister city in the United States. Since then, Denver has established relationships with additional sister cities, and currently has a total of ten partnerships:
In addition to these, the Denver Regional Council of Governments (consisting of the city and 51 other local governments) has established a "sister city" relationship with the Baghdad Governorate, one of Iraq's eighteen provinces.

</doc>
<doc id="8524" url="http://en.wikipedia.org/wiki?curid=8524" title="Deuterium">
Deuterium

Deuterium (symbol ' or ', also known as heavy hydrogen) is one of two stable isotopes of hydrogen. The nucleus of deuterium, called a deuteron, contains one proton and one neutron, whereas the far more common hydrogen isotope, protium, has no neutron in the nucleus. It has a natural abundance in Earth's oceans of about one atom in of hydrogen. Thus deuterium accounts for approximately 0.0156% (or on a mass basis: 0.0312%) of all the naturally occurring hydrogen in the oceans, while the most common isotope (hydrogen-1 or protium) accounts for more than 99.98%. The abundance of deuterium changes slightly from one kind of natural water to another (see Vienna Standard Mean Ocean Water).
The deuterium isotope's name is formed from the Greek "deuteros" meaning "second", to denote the two particles composing the nucleus. Deuterium was discovered and named in 1931 by Harold Urey, earning him a Nobel Prize in 1934. This followed the discovery of the neutron in 1932, which made the nuclear structure of deuterium obvious. Soon after deuterium's discovery, Urey and others produced samples of "heavy water" in which the deuterium has been highly concentrated with respect to the protium.
Because deuterium is destroyed in the interiors of stars faster than it is produced, and because other natural processes are thought to produce only an insignificant amount of deuterium, it is thought that nearly all deuterium found in nature was produced in the Big Bang 13.8 billion years ago, and that the basic or primordial ratio of hydrogen-1 (protium) to deuterium (about 26 atoms of deuterium per million hydrogen atoms) has its origin from that time. This is the ratio found in the gas giant planets, such as Jupiter. However, different astronomical bodies are found to have different ratios of deuterium to hydrogen-1, and this is thought to be as a result of natural isotope separation processes that occur from solar heating of ices in comets. Like the water-cycle in Earth's weather, such heating processes may enrich deuterium with respect to protium. In fact, the discovery of deuterium/protium ratios in a number of comets very similar to the mean ratio in Earth's oceans (156 atoms of deuterium per million hydrogens) has led to theories that much of Earth's ocean water has a cometary origin. 
Deuterium/protium ratios thus continue to be an active topic of research in both astronomy and climatology.
Differences between deuterium and common hydrogen (protium).
Chemical symbol.
Deuterium is frequently represented by the chemical symbol D. Since it is an isotope of hydrogen with mass number 2, it is also represented by . IUPAC allows both D and , although is preferred. A distinct chemical symbol is used for convenience because of the isotope's common use in various scientific processes. Also, its large mass difference with protium () (deuterium has a mass of , compared to the mean hydrogen atomic weight of , and protium's mass of ) confers non-negligible chemical dissimilarities with protium-containing compounds, whereas the isotope weight ratios within other chemical elements are largely insignificant in this regard.
Spectroscopy.
In quantum mechanics the energy levels of electrons in atoms depend on the reduced mass of the system of electron and nucleus. For the hydrogen atom, the role of reduced mass is most simply seen in the Bohr model of the atom, where the reduced mass appears in a simple calculation of the Rydberg constant and Rydberg equation, but the reduced mass also appears in the Schrödinger equation, and the Dirac equation for calculating atomic energy levels.
The reduced mass of the system in these equations is close to the mass of a single electron, but differs from it by a small amount about equal to the ratio of mass of the electron to the atomic nucleus. For hydrogen, this amount is about 1837/1836, or 1.000545, and for deuterium it is even smaller: 3671/3670, or 1.0002725. The energies of spectroscopic lines for deuterium and light-hydrogen (hydrogen-1) therefore differ by the ratios of these two numbers, which is 1.000272. The wavelengths of all deuterium spectroscopic lines are shorter than the corresponding lines of light hydrogen, by a factor of 1.000272. In astronomical observation, this corresponds to a blue Doppler shift of 0.000272 times the speed of light, or 81.6 km/s.
The differences are much more pronounced in vibrational spectroscopy such as infrared spectroscopy and Raman spectroscopy, and in rotational spectra such as microwave spectroscopy because the reduced mass of the deuterium is markedly higher than that of protium.
Deuterium and Big Bang nucleosynthesis.
Deuterium is thought to have played an important role in setting the number and ratios of the elements that were formed in the Big Bang. Combining thermodynamics and the changes brought about by cosmic expansion, one can calculate the fraction of protons and neutrons based on the temperature at the point that the universe cooled enough to allow formation of nuclei. This calculation indicates seven protons for every neutron at the beginning of nucleogenesis, a ratio that would remain stable even after nucleogenesis was over. This fraction was in favor of protons initially, primarily because the lower mass of the proton favored their production. As the universe expanded, it cooled. Free neutrons and protons are less stable than helium nuclei, and the protons and neutrons had a strong energetic reason to form helium-4. However, forming helium-4 requires the intermediate step of forming deuterium.
Through much of the few minutes after the big bang during which nucleosynthesis could have occurred, the temperature was high enough that the mean energy per particle was greater than the binding energy of weakly bound deuterium; therefore any deuterium that was formed was immediately destroyed. This situation is known as the deuterium bottleneck. The bottleneck delayed formation of any helium-4 until the universe became cool enough to form deuterium (at about a temperature equivalent to 100 keV). At this point, there was a sudden burst of element formation (first deuterium, which immediately fused to helium). However, very shortly thereafter, at twenty minutes after the Big Bang, the universe became too cool for any further nuclear fusion and nucleosynthesis to occur. At this point, the elemental abundances were nearly fixed, with the only change as some of the radioactive products of BBN (such as tritium) decay. The deuterium bottleneck in the formation of helium, together with the lack of stable ways for helium to combine with hydrogen or with itself (there are no stable nuclei with mass numbers of five or eight) meant that insignificant carbon, or any elements heavier than carbon, formed in the Big Bang. These elements thus required formation in stars. At the same time, the failure of much nucleogenesis during the Big Bang ensured that there would be plenty of hydrogen in the later universe available to form long-lived stars, such as our Sun.
Abundance.
Deuterium occurs in trace amounts naturally as deuterium gas, written 2 or D2, but most natural occurrence in the universe is bonded with a typical atom, a gas called hydrogen deuteride (HD or ).
The existence of deuterium on Earth, elsewhere in the solar system (as confirmed by planetary probes), and in the spectra of stars, is also an important datum in cosmology. Gamma radiation from ordinary nuclear fusion dissociates deuterium into protons and neutrons, and there are no known natural processes other than the Big Bang nucleosynthesis, which might have produced deuterium at anything close to the observed natural abundance of deuterium (deuterium is produced by the rare cluster decay, and occasional absorption of naturally occurring neutrons by light hydrogen, but these are trivial sources). There is thought to be little deuterium in the interior of the Sun and other stars, as at temperatures there nuclear fusion reactions that consume deuterium happen much faster than the proton-proton reaction that creates deuterium. However, deuterium persists in the outer solar atmosphere at roughly the same concentration as in Jupiter, and this has probably been unchanged since the origin of the Solar System. The natural abundance of deuterium seems to be a very similar fraction of hydrogen, wherever hydrogen is found, unless there are obvious processes at work that concentrate it.
The existence of deuterium at a low but constant primordial fraction in all hydrogen is another one of the arguments in favor of the Big Bang theory over the Steady State theory of the universe. The observed ratios of hydrogen to helium to deuterium in the universe are difficult to explain except with a Big Bang model. It is estimated that the abundances of deuterium have not evolved significantly since their production about . Measurements of Milky Way galactic deuterium from ultraviolet spectral analysis show a ratio of as much as 23 atoms of deuterium per million hydrogen atoms in undisturbed gas clouds, which is only 15% below the WMAP estimated primordial ratio of about 27 atoms per million from the Big Bang. This has been interpreted to mean that less deuterium has been destroyed in star formation in our galaxy than expected, or perhaps deuterium has been replenished by a large in-fall of primordial hydrogen from outside the galaxy. In space a few hundred light years from the Sun, deuterium abundance is only 15 atoms per million, but this value is presumably influenced by differential adsorption of deuterium onto carbon dust grains in interstellar space.
The abundance of deuterium in the atmosphere of Jupiter has been directly measured (by the Galileo space probe as 26 atoms per million hydrogen atoms. ISO-SWS observations find 22 atoms per million hydrogen atoms in Jupiter. and this abundance is thought to represent close to the primordial solar system ratio. This is about 17% of the terrestrial deuterium-to-hydrogen ratio of 156 deuterium atoms per million hydrogen atoms.
Cometary bodies such as Comet Hale Bopp and Halley's Comet have been measured to contain relatively more deuterium (about 200 atoms D per million hydrogens), ratios which are enriched with respect to the presumed protosolar nebula ratio, probably due to heating, and which are similar to the ratios found in Earth seawater. The recent measurement of deuterium amounts of 161 atoms D per million hydrogen in Comet 103P/Hartley (a former Kuiper belt object), a ratio almost exactly that in Earth's oceans, emphasizes the theory that Earth's surface water may be largely comet-derived. 
Deuterium has also observed to be concentrated over the mean solar abundance in other terrestrial planets, in particular Mars and Venus.
Production.
Deuterium is produced for industrial, scientific and military purposes, by starting with ordinary water—a small fraction of which is naturally-occurring heavy water—and then separating out the heavy water by the Girdler sulfide process, distillation, or other methods.
The world's leading supplier of deuterium was Atomic Energy of Canada Limited, in Canada, until 1997, when the last heavy water plant was shut down. Canada uses heavy water as a neutron moderator for the operation of the CANDU reactor design.
Properties.
Physical properties.
The physical properties of deuterium compounds can exhibit significant kinetic isotope effects and other physical and chemical property differences from the hydrogen analogs; for example, D2O is more viscous than H2O.
Chemically, deuterium behaves similarly to ordinary hydrogen, but there are differences in bond energy and length for compounds of heavy hydrogen isotopes which are larger than the isotopic differences in any other element. Bonds involving deuterium and tritium are somewhat stronger than the corresponding bonds in hydrogen, and these differences are enough to make significant changes in biological reactions.
Deuterium can replace the normal hydrogen in water molecules to form heavy water (D2O), which is about 10.6% denser than normal water (enough that ice made from it sinks in ordinary water). Heavy water is slightly toxic in eukaryotic animals, with 25% substitution of the body water causing cell division problems and sterility, and 50% substitution causing death by cytotoxic syndrome (bone marrow failure and gastrointestinal lining failure). Prokaryotic organisms, however, can survive and grow in pure heavy water (though they grow more slowly). Consumption of heavy water does not pose a health threat to humans, it is estimated that a person might drink 4.8 liters of heavy water without serious consequences. Small doses of heavy water (a few grams in humans, containing an amount of deuterium comparable to that normally present in the body) are routinely used as harmless metabolic tracers in humans and animals.
Quantum properties.
The deuteron has spin +1 ("triplet") and is thus a boson. The NMR frequency of deuterium is significantly different from common light hydrogen. Infrared spectroscopy also easily differentiates many deuterated compounds, due to the large difference in IR absorption frequency seen in the vibration of a chemical bond containing deuterium, versus light hydrogen. The two stable isotopes of hydrogen can also be distinguished by using mass spectrometry.
The triplet deuteron nucleon is barely bound at EB = , so all the higher energy states are not bound. The singlet deuteron is a virtual state, with a negative binding energy of . There is no such stable particle, but this virtual particle transiently exists during neutron-proton inelastic scattering, accounting for the unusually large neutron scattering cross-section of the proton.
Nuclear properties (the deuteron).
Deuteron mass and radius.
The nucleus of deuterium is called a deuteron. It has a mass of 
The charge radius of the deuteron is 
Spin and energy.
Deuterium is one of only five stable nuclides with an odd number of protons and odd number of neutrons. (, , , , ; also, the long-lived radioactive nuclides , , , occur naturally.) Most odd-odd nuclei are unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects. Deuterium, however, benefits from having its proton and neutron coupled to a spin-1 state, which gives a stronger nuclear attraction; the corresponding spin-1 state does not exist in the two-neutron or two-proton system, due to the Pauli exclusion principle which would require one or the other identical particle with the same spin to have some other different quantum number, such as orbital angular momentum. But orbital angular momentum of either particle gives a lower binding energy for the system, primarily due to increasing distance of the particles in the steep gradient of the nuclear force. In both cases, this causes the diproton and dineutron nucleus to be unstable.
The proton and neutron making up deuterium can be dissociated through neutral current interactions with neutrinos. The cross section for this interaction is comparatively large, and deuterium was successfully used as a neutrino target in the Sudbury Neutrino Observatory experiment.
Isospin singlet state of the deuteron.
Due to the similarity in mass and nuclear properties between the proton and neutron, they are sometimes considered as two symmetric types of the same object, a nucleon. While only the proton has an electric charge, this is often negligible due to the weakness of the electromagnetic interaction relative to the strong nuclear interaction. The symmetry relating the proton and neutron is known as isospin and denoted "I" (or sometimes "T").
Isospin is an SU(2) symmetry, like ordinary spin, so is completely analogous to it. The proton and neutron form an isospin doublet, with a "down" state (↓) being a neutron, and an "up" state (↑) being a proton.
A pair of nucleons can either be in an antisymmetric state of isospin called singlet, or in a symmetric state called triplet. In terms of the "down" state and "up" state, the singlet is
This is a nucleus with one proton and one neutron, i.e. a deuterium nucleus. The triplet is
and thus consists of three types of nuclei, which are supposed to be symmetric: a deuterium nucleus (actually a highly excited state of it), a nucleus with two protons, and a nucleus with two neutrons. The latter two nuclei are not stable or nearly stable, and therefore so is this type of deuterium (meaning that it is indeed a highly excited state of deuterium).
Approximated wavefunction of the deuteron.
The deuteron wavefunction must be antisymmetric if the isospin representation is used (since a proton and a neutron are not identical particles, the wavefunction
need not be antisymmetric in general). Apart from their isospin, the two nucleons also have spin and spatial distributions of their wavefunction. The latter is symmetric if the deuteron is symmetric under parity (i.e. have an "even" or "positive" parity), and antisymmetric if the deuteron is antisymmetric under parity (i.e. have an "odd" or "negative" parity). The parity is fully determined by the total orbital angular momentum of the two nucleons: if it is even then the parity is even (positive), and if it is odd then the parity is odd (negative).
The deuteron, being an isospin singlet, is antisymmetric under nucleons exchange due to isospin, and therefore must be symmetric under the double exchange of their spin and location. Therefore it can be in either of the following two different states:
In the first case the deuteron is a spin triplet, so that its total spin "s" is 1. It also has an even parity and therefore even orbital angular momentum "l" ; The lower its orbital angular momentum, the lower its energy. Therefore the lowest possible energy state has , .
In the second case the deuteron is a spin singlet, so that its total spin "s" is 0. It also has an odd parity and therefore odd orbital angular momentum "l". Therefore the lowest possible energy state has , .
Since gives a stronger nuclear attraction, the deuterium ground state is in the , state.
The same considerations lead to the possible states of an isospin triplet having , or , . Thus the state of lowest energy has , , higher than that of the isospin singlet.
The analysis just given is in fact only approximate, both because isospin is not an exact symmetry, and more importantly because the strong nuclear interaction between the two nucleons is related to angular momentum in spin-orbit interaction that mixes different "s" and "l" states. That is, "s" and "l" are not constant in time (they do not commute with the Hamiltonian), and over time a state such as , may become a state of , . Parity is still constant in time so these do not mix with odd "l" states (such as , ). Therefore the quantum state of the deuterium is a superposition (a linear combination) of the , state and the , state, even though the first component is much bigger. Since the total angular momentum "j" is also a good quantum number (it is a constant in time), both components must have the same "j", and therefore . This is the total spin of the deuterium nucleus.
To summarize, the deuterium nucleus is antisymmetric in terms of isospin, and has spin 1 and even (+1) parity. The relative angular momentum of its nucleons "l" is not well defined, and the deuteron is a superposition of mostly with some .
Magnetic and electric multipoles.
In order to find theoretically the deuterium magnetic dipole moment µ, one uses the formula for a nuclear magnetic moment
with
g(l) and g(s) are g-factors of the nucleons.
Since the proton and neutron have different values for g(l) and g(s), one must separate their contributions. Each gets half of the deuterium orbital angular momentum formula_5 and spin formula_6. One arrives at
where subscripts p and n stand for the proton and neutron, and .
By using the same identities as here and using the value , we arrive at the following result, in nuclear magneton units
For the , state (), we obtain
For the , state (), we obtain
The measured value of the deuterium magnetic dipole moment, is . This suggests that the state of the deuterium is indeed only approximately , state, and is actually a linear combination of (mostly) this state with , state.
The electric dipole is zero as usual.
The measured electric quadrupole of the deuterium is . While the order of magnitude is reasonable, since the deuterium radius is of order of 1 femtometer (see below) and its electric charge is e, the above model does not suffice for its computation. More specifically, the electric quadrupole does not get a contribution from the "l" =0 state (which is the dominant one) and does get a contribution from a term mixing the "l" =0 and the "l" =2 states, because the electric quadrupole operator does not commute with angular momentum.
The latter contribution is dominant in the absence of a pure contribution, but cannot be calculated without knowing the exact spatial form of the nucleons wavefunction inside the deuterium.
Higher magnetic and electric multipole moments cannot be calculated by the above model, for similar reasons.
Applications.
Deuterium has a number of commercial and scientific uses. These include:
Nuclear reactors.
Deuterium is used in heavy water moderated fission reactors, usually as liquid D2O, to slow neutrons without high neutron absorption of ordinary hydrogen. This is a common commercial use for larger amounts of deuterium.
In research reactors, liquid D2 is used in cold sources to moderate neutrons to very low energies and wavelengths appropriate for scattering experiments.
Experimentally, deuterium is the most common nuclide used in nuclear fusion reactor designs, especially in combination with tritium, because of the large reaction rate (or nuclear cross section) and high energy yield of the D–T reaction. There is an even higher-yield D– fusion reaction, though the breakeven point of D– is higher than that of most other fusion reactions; together with the scarcity of , this makes it implausible as a practical power source until at least D–T and D–D fusion reactions have been performed on a commercial scale. However, commercial nuclear fusion is not yet an accomplished technology.
NMR spectroscopy.
Deuterium is useful in hydrogen nuclear magnetic resonance spectroscopy (proton NMR). NMR ordinarily requires compounds of interest to be analyzed as dissolved in solution. Because of deuterium's nuclear spin properties which differ from the light hydrogen usually present in organic molecules, NMR spectra of hydrogen/protium are highly differentiable from that of deuterium, and in practice deuterium is not "seen" by an NMR instrument tuned to light-hydrogen. Deuterated solvents (including heavy water, but also compounds like deuterated chloroform, CDCl3) are therefore routinely used in NMR spectroscopy, in order to allow only the light-hydrogen spectra of the compound of interest to be measured, without solvent-signal interference.
Deuterium NMR spectra are especially informative in the solid state because of its relatively small quadrupole moment in comparison with those of bigger quadrupolar nuclei such as chlorine-35, for example.
Tracing.
In chemistry, biochemistry and environmental sciences, deuterium is used as a non-radioactive, stable isotopic tracer, for example, in the doubly labeled water test. In chemical reactions and metabolic pathways, deuterium behaves somewhat similarly to ordinary hydrogen (with a few chemical differences, as noted). It can be distinguished from ordinary hydrogen most easily by its mass, using mass spectrometry or infrared spectrometry. Deuterium can be detected by femtosecond infrared spectroscopy, since the mass difference drastically affects the frequency of molecular vibrations; deuterium-carbon bond vibrations are found in locations free of other signals.
Measurements of small variations in the natural abundances of deuterium, along with those of the stable heavy oxygen isotopes 17O and 18O, are of importance in hydrology, to trace the geographic origin of Earth's waters. The heavy isotopes of hydrogen and oxygen in rainwater (so-called meteoric water) are enriched as a function of the environmental temperature of the region in which the precipitation falls (and thus enrichment is related to mean latitude). The relative enrichment of the heavy isotopes in rainwater (as referenced to mean ocean water), when plotted against temperature falls predictably along a line called the global meteoric water line (GMWL). This plot allows samples of precipitation-originated water to be identified along with general information about the climate in which it originated. Evaporative and other processes in bodies of water, and also ground water processes, also differentially alter the ratios of heavy hydrogen and oxygen isotopes in fresh and salt waters, in characteristic and often regionally distinctive ways. The ratio of concentration of 2H to 1H is usually indicated with a delta as δ2H and the geographic patterns of these values are plotted in maps termed as isoscapes. Stable isotope are incorporated into plants and animals and an analysis of the ratios in a migrant bird or insect can help suggest a rough guide to their origins.
Contrast properties.
Neutron scattering techniques particularly profit from availability of deuterated samples: The H and D cross sections are very distinct and different in sign, which allows contrast variation in such experiments. Further, a nuisance problem of ordinary hydrogen is its large incoherent neutron cross section, which is nil for D. The substitution of deuterium atoms for hydrogen atoms thus reduces scattering noise.
Hydrogen is an important and major component in all materials of organic chemistry and life science, but it barely interacts with X-rays. As hydrogen (and deuterium) interact strongly with neutrons, neutron scattering techniques, together with a modern deuteration facility, fills a niche in many studies of macromolecules in biology and many other areas.
Nuclear weapons.
This is discussed below. It is notable that although most stars (including our Sun) generate energy over most of their lives by fusing hydrogen into heavier elements, such fusion of light hydrogen (protium) has never been successful in the conditions attainable on Earth. Thus, all artificial fusion, including the hydrogen fusion that occurs in so-called hydrogen bombs, requires heavy hydrogen (either tritium or deuterium, or both) in order for the process to work.
Suggested neurological effects of natural abundance variation.
The natural deuterium content of water has been suggested from preliminary correlative epidemiology to influence the incidence of affective disorder-related pathophysiology and major depression, which might be mediated by the serotonergic mechanisms.
History.
Suspicion of lighter element isotopes.
The existence of nonradioactive isotopes of lighter elements had been suspected in studies of neon as early as 1913, and proven by mass spectrometry of light elements in 1920. The prevailing theory at the time, however, was that the isotopes were due to the existence of differing numbers of "nuclear electrons" in different atoms of an element. It was expected that hydrogen, with a measured average atomic mass very close to , the known mass of the proton, always had a nucleus composed of a single proton (a known particle), and therefore could not contain any nuclear electrons without losing its charge entirely. Thus, hydrogen could have no heavy isotopes.
Deuterium detected.
It was first detected spectroscopically in late 1931 by Harold Urey, a chemist at Columbia University. Urey's collaborator, Ferdinand Brickwedde, distilled five liters of cryogenically produced liquid hydrogen to of liquid, using the low-temperature physics laboratory that had recently been established at the National Bureau of Standards in Washington, D.C. (now the National Institute of Standards and Technology). The technique had previously been used to isolate heavy isotopes of neon. The cryogenic boiloff technique concentrated the fraction of the mass-2 isotope of hydrogen to a degree that made its spectroscopic identification unambiguous.
Naming of the isotope and Nobel Prize.
Urey created the names protium, deuterium, and tritium in an article published in 1934. The name is based in part on advice from G. N. Lewis who had proposed the name "deutium". The name is derived from the Greek deuteros (second), and the nucleus to be called "deuteron" or "deuton". Isotopes and new elements were traditionally given the name that their discoverer decided. Some British chemists, like Ernest Rutherford, wanted the isotope to be called "diplogen", from the Greek diploos (double), and the nucleus to be called diplon.
The amount inferred for normal abundance of this heavy isotope of hydrogen was so small (only about 1 atom in 6400 hydrogen atoms in ocean water (156 deuteriums per million hydrogens) that it had not noticeably affected previous measurements of (average) hydrogen atomic mass. This explained why it hadn't been experimentally suspected before. Urey was able to concentrate water to show partial enrichment of deuterium. Lewis had prepared the first samples of pure heavy water in 1933. The discovery of deuterium, coming before the discovery of the neutron in 1932, was an experimental shock to theory, but when the neutron was reported, making deuterium's existence more explainable, deuterium won Urey the Nobel Prize in chemistry in 1934. Lewis was embittered by being passed over for this recognition given to his former student.
"Heavy water" experiments in World War II.
Shortly before the war, Hans von Halban and Lew Kowarski moved their research on neutron moderation from France to England, smuggling the entire global supply of heavy water (which had been made in Norway) across in twenty-six steel drums.
During World War II, Nazi Germany was known to be conducting experiments using heavy water as moderator for a nuclear reactor design. Such experiments were a source of concern because they might allow them to produce plutonium for an atomic bomb. Ultimately it led to the Allied operation called the "Norwegian heavy water sabotage", the purpose of which was to destroy the Vemork deuterium production/enrichment facility in Norway. At the time this was considered important to the potential progress of the war.
After World War II ended, the Allies discovered that Germany was not putting as much serious effort into the program as had been previously thought. They had been unable to sustain a chain reaction. The Germans had completed only a small, partly built experimental reactor (which had been hidden away). By the end of the war, the Germans did not even have a fifth of the amount of heavy water needed to run the reactor, partially due to the Norwegian heavy water sabotage operation. However, even had the Germans succeeded in getting a reactor operational (as the U.S. did with a graphite reactor in late 1942), they would still have been at least several years away from development of an atomic bomb with maximal effort. The engineering process, even with maximal effort and funding, required about two and a half years (from first critical reactor to bomb) in both the U.S. and U.S.S.R, for example.
Deuterium in thermonuclear weapons.
The 62-ton Ivy Mike device built by the United States and exploded on 1 November 1952, was the first fully successful "hydrogen bomb" or thermonuclear bomb. In this context, it was the first bomb in which most of the energy released came from nuclear reaction stages that followed the primary nuclear fission stage of the atomic bomb. The Ivy Mike bomb was a factory-like building, rather than a deliverable weapon. At its center, a very large cylindrical, insulated vacuum flask or cryostat, held cryogenic liquid deuterium in a volume of about 1000 liters (160 kilograms in mass, if this volume had been completely filled). Then, a conventional atomic bomb (the "primary") at one end of the bomb was used to create the conditions of extreme temperature and pressure that were needed to set off the thermonuclear reaction.
Within a few years, so-called "dry" hydrogen bombs were developed that did not need cryogenic hydrogen. Released information suggests that all thermonuclear weapons built since then contain chemical compounds of deuterium and lithium in their secondary stages. The material that contains the deuterium is mostly lithium deuteride, with the lithium consisting of the isotope lithium-6. When the lithium-6 is bombarded with fast neutrons from the atomic bomb, tritium (hydrogen-3) is produced, and then the deuterium and the tritium quickly engage in thermonuclear fusion, releasing abundant energy, helium-4, and even more free neutrons.
Data for elemental deuterium.
Formula: D2 or 2
Data at approximately for D2 (triple point):
Anti-deuterium.
An antideuteron is the antiparticle of the nucleus of deuterium, consisting of an antiproton and an antineutron. The antideuteron was first produced in 1965 at the Proton Synchrotron at CERN and the Alternating Gradient Synchrotron at Brookhaven National Laboratory. A complete atom, with a positron orbiting the nucleus, would be called "antideuterium", but as of 2005 antideuterium has not yet been created. The proposed symbol for antideuterium is , that is, D with an overbar.

</doc>
<doc id="8525" url="http://en.wikipedia.org/wiki?curid=8525" title="Digital signal processing">
Digital signal processing

Digital signal processing (DSP) is the mathematical manipulation of an information signal to modify or improve it in some way. It is characterized by the representation of discrete time, discrete frequency, or other discrete domain signals by a sequence of numbers or symbols and the processing of these signals. 
The goal of DSP is usually to measure, filter and/or compress continuous real-world analog signals. Usually, the first step is conversion of the signal from an analog to a digital form, by sampling and then digitizing it using an analog-to-digital converter (ADC), which turns the analog signal into a stream of discrete digital values. Often, however, the required output signal is also analog, which requires a digital-to-analog converter (DAC). Even if this process is more complex than analog processing and has a discrete value range, the application of computational power to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression.
Digital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech signal processing, sonar and radar signal processing, sensor array processing, spectral estimation, statistical signal processing, digital image processing, signal processing for communications, control of systems, biomedical signal processing, seismic data processing, among others. DSP algorithms have long been run on standard computers, as well as on specialized processors called digital signal processors, and on purpose-built hardware such as application-specific integrated circuit (ASICs). Currently, there are additional technologies used for digital signal processing including more powerful general purpose microprocessors, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors, among others.
Digital signal processing can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains.
Signal sampling.
The increasing use of computers has resulted in the increased use of, and need for, digital signal processing. To digitally analyze and manipulate an analog signal, it must be digitized with an analog-to-digital converter. Sampling is usually carried out in two stages, discretization and quantization. In the discretization stage, the space of signals is partitioned into equivalence classes and quantization is carried out by replacing the signal with representative signal of the corresponding equivalence class. In the quantization stage, the representative signal values are approximated by values from a finite set.
The Nyquist–Shannon sampling theorem states that a signal can be exactly reconstructed from its samples if the sampling frequency is greater than twice the highest frequency of the signal, but this requires an infinite number of samples. In practice, the sampling frequency is often significantly higher than twice that required by the signal's limited bandwidth.
Some (continuous-time) periodic signals become non-periodic after sampling, and some non-periodic signals become periodic after sampling. In general, for a periodic signal with period "T" to be periodic (with period "N") after sampling with sampling interval "Ts", the following must be satisfied:
where "k is an integer.
DSP domains.
In DSP, engineers usually study digital signals in one of the following domains: time domain (one-dimensional signals), spatial domain (multidimensional signals), frequency domain, and wavelet domains. They choose the domain in which to process a signal by making an informed assumption (or by trying different possibilities) as to which domain best represents the essential characteristics of the signal. A sequence of samples from a measuring device produces a temporal or spatial domain representation, whereas a discrete Fourier transform produces the frequency domain information, that is, the frequency spectrum. Autocorrelation is defined as the cross-correlation of the signal with itself over varying intervals of time or space.
Time and space domains.
The most common processing approach in the time or space domain is enhancement of the input signal through a method called filtering. Digital filtering generally consists of some linear transformation of a number of surrounding samples around the current sample of the input or output signal. There are various ways to characterize filters; for example:
A filter can be represented by a block diagram, which can then be used to derive a sample processing algorithm to implement the filter with hardware instructions. A filter may also be described as a difference equation, a collection of zeroes and poles or, if it is an FIR filter, an impulse response or step response.
The output of a linear digital filter to any given input may be calculated by convolving the input signal with the impulse response.
Frequency domain.
Signals are converted from time or space domain to the frequency domain usually through the Fourier transform. The Fourier transform converts the signal information to a magnitude and phase component of each frequency. Often the Fourier transform is converted to the power spectrum, which is the magnitude of each frequency component squared.
The most common purpose for analysis of signals in the frequency domain is analysis of signal properties. The engineer can study the spectrum to determine which frequencies are present in the input signal and which are missing.
In addition to frequency information, phase information is often needed. This can be obtained from the Fourier transform. With some applications, how the phase varies with frequency can be a significant consideration.
Filtering, particularly in non-realtime work can also be achieved by converting to the frequency domain, applying the filter and then converting back to the time domain. This is a fast, O(n log n) operation, and can give essentially any filter shape including excellent approximations to brickwall filters.
There are some commonly used frequency domain transformations. For example, the cepstrum converts a signal to the frequency domain through Fourier transform, takes the logarithm, then applies another Fourier transform. This emphasizes the harmonic structure of the original spectrum.
Frequency domain analysis is also called "spectrum-" or "spectral analysis".
Z-plane analysis.
Whereas analog filters are usually analyzed in terms of transfer functions in the s plane using Laplace transforms, digital filters are analyzed in the z plane in terms of Z-transforms. A digital filter may be described in the z plane by its characteristic collection of zeroes and poles. The z plane provides a means for mapping digital frequency (samples/second) to real and imaginary z components, where formula_2 for continuous periodic signals and formula_3 (formula_4 is the digital frequency). This is useful for providing a visualization of the frequency response of a digital system or signal.
Wavelet.
In numerical analysis and functional analysis, a discrete wavelet transform (DWT) is any wavelet transform for which the wavelets are discretely sampled. As with other wavelet transforms, a key advantage it has over Fourier transforms is temporal resolution: it captures both frequency "and" location information (location in time).
Applications.
The main applications of DSP are audio signal processing, audio compression, digital image processing, video compression, speech processing, speech recognition, digital communications, radar, sonar, Financial signal processing, seismology and biomedicine. Specific examples are speech compression and transmission in digital mobile phones, room correction of sound in hi-fi and sound reinforcement applications, weather forecasting, economic forecasting, seismic data processing, analysis and control of industrial processes, medical imaging such as CAT scans and MRI, MP3 compression, computer graphics, image manipulation, hi-fi loudspeaker crossovers and equalization, and audio effects for use with electric guitar amplifiers.
Implementation.
Depending on the requirements of the application, digital signal processing tasks can be implemented on general purpose computers.
Often when the processing requirement is not real-time, processing is economically done with an existing general-purpose computer and the signal data (either input or output) exists in data files. This is essentially no different from any other data processing, except DSP mathematical techniques (such as the FFT) are used, and the sampled data is usually assumed to be uniformly sampled in time or space. For example: processing digital photographs with software such as "Photoshop".
However, when the application requirement is real-time, DSP is often implemented using specialized microprocessors such as the DSP56000, the TMS320, or the SHARC. These often process data using fixed-point arithmetic, though some more powerful versions use floating point. For faster applications FPGAs might be used.
Beginning in 2007, multicore implementations of DSPs have started to emerge from companies including Freescale and Stream Processors, Inc. For faster applications with vast usage, ASICs might be designed specifically. For slow applications, a traditional slower processor such as a microcontroller may be adequate. Also a growing number of DSP applications are now being implemented on embedded systems using powerful PCs with multi-core processors.

</doc>
<doc id="8527" url="http://en.wikipedia.org/wiki?curid=8527" title="Discordianism">
Discordianism

Discordianism is a religion and subsequent philosophy based on the veneration or worship of Eris (also known as Discordia), the Greek goddess of chaos, or archetypes or ideals associated with her. It was founded after the 1965 publication of its (first) holy book, the "Principia Discordia", written by two individuals working under the pseudonyms Malaclypse the Younger and Omar Khayyam Ravenhurst.
The religion has been likened to Zen, based on similarities with absurdist interpretations of the Rinzai school, as well as Taoist philosophy. Discordianism is centered on the idea that both order and disorder are illusions imposed on the universe by the human nervous system, and that neither of these illusions of apparent order and disorder is any more accurate or objectively true than the other.
There is some division as to whether it should be regarded as a parody religion, and if so to what degree. Discordians use subversive humor to spread their philosophy and to prevent their beliefs from becoming dogmatic . It is difficult to estimate the number of Discordians because they are not required to hold Discordianism as their only belief system, and because there is an encouragement to form schisms and cabals.
Founding.
The foundational document of Discordianism is the "Principia Discordia", fourth edition, written by Malaclypse the Younger, an alias of Greg Hill. This book contains many references to an earlier source, "The Honest Book of Truth" (HBT). From the quotations, the HBT seems to be arranged like the Bible, consisting of verses grouped into chapters grouped into books grouped into the HBT itself. The "Principia" includes a large portion (or possibly all) of a chapter of "The Book of Explanations" which recounts how the HBT was revealed to Lord Omar Khayyam Ravenhurst. The tale of the discovery of the HBT contains many similarities to the tale of the discovery of the Book of Mormon, and Ravenhurst had been a Mormon. It also includes part of the next chapter, telling how the HBT was taken by a garbage collector, who refused to return it.
The "Principia Discordia" often hints that Discordianism was founded as a dialectic antithesis to more popular religions based on order, although the rhetoric throughout the book describes chaos as a much more underlying impulse of the universe. This may have been done with the intention of merely "balancing out" the creative forces of order and disorder, but the focus is on the more disorderly aspects of the world — at times the forces of order are even vilified. There are other religions that revere the principles of harmony and order in the Universe, but few that show a respect for the disorder which we all face.
Organization.
The very idea of a Discordian organization is something of a paradox. Nevertheless, some structure is indicated in "Principia Discordia". The most general group, presumably including all Discordians (and potentially others), is The Discordian Society, whose definition is “The Discordian Society has no definition”. Within the society are sects of Discordianism, each under the direction of an “Episkopos” ("overseer" or "bishop" in Koine Greek, the same word used in the New Testament for Christian Bishops.)
Discordians who do not form their own sects, whether they belong to someone else's sect or not, make up the Legion of Dynamic Discord, and may be referred to as Legionnaires. Would-be Discordians are told in the "Principia Discordia":
POEE.
The sect of Discordianism founded by Malaclypse the Younger and Omar Khayyam Ravenhurst is known as the "Paratheo-Anametamystikhood Of Eris Esoteric" (POEE). the "Principia" contains some details about the structure of POEE. In particular:
Episkopos.
Episkoposes are the Overseers of sects of Discordianism, who have presumably created their own sect of Discordianism. They speak to Eris through the use of their pineal gland. It is said in the "Principia Discordia" that Eris says different things to each listener. She may even say radically different things to each Episkopos but, all of what she says is equally her word (even if it contradicts another iteration of her word).
Philosophy.
The tenets of the faith, as described in the "Principia Discordia" address several key philosophical concepts as follows.<br>
Epistemology
The Discordian concept of "pure chaos" may be compared to the noumenon described by Plato and Kant.<br>
Relativism
The statements above are consistent with anthropological relativism, a strategy for furthering objectivity by setting aside cultural preconceptions. This is to be distinguished from philosophical relativism, a viewpoint which denies the existence of any objective truth outside of a particular frame of reference. Here the "Principia" defines noumenal reality as not humanly knowable. This is an assertion about the epistemology of truth: human knowledge is based upon "grids" and absolute "(capital-T) Truth" is not accessible through such human inventions as "concept" or "belief". The statement "(capital-T) Truth, metaphysical reality, is irrelevant to grids" also contains an assertion about the ontology of truth. Although absolute truth is not epistemologically accessible, it does exist, independent of our ability (or, in this case, inability) to perceive it or to describe it.
Overview.
The Principia Discordia holds three core principles: the Aneristic Principle (order), Eristic Principle (disorder) and the notion that both are mere illusions. The following excerpt summarizes these principles quite well:
Sacred Chao.
This glyph is a variation on the Chinese Taijitu symbol. The original symbol represents the dual concepts of yin and yang; the Discordian variant has been altered to symbolize the Aneristic Principle and Eristic Principle (see above, Epistemology).
Law of Fives.
The Law of Fives is summarized in the "Principia Discordia":
A stylized description of confirmation bias.
Original Snub.
The Original Snub is the Discordian name for the events preceding the Judgment of Paris, although more focus is put on the actions of Eris. Zeus believes that Eris is a troublemaker, so he does not invite her to Peleus and Thetis's wedding. This is “The Doctrine of the Original Snub”.
Having been snubbed, Eris creates a golden apple with the word "kallisti" (Ancient Greek: "καλλίστῃ", to the prettiest one) inscribed in it. This, the Apple of Discord, is a notable symbol in Discordianism for its inclusion in the Holy Chao. The apple is traditionally described as being made of gold, but the "Principia Discordia" notes a debate over whether the “gold” described was “metallic gold or Acapulco.” Because of the original snub "a Discordian is to partake of No Hot Dog Buns". (Buns being snub backwards.)
When the female wedding guests disagree about who the apple is meant for, Zeus decides to leave the decision to Paris of Troy. Aphrodite bribes Paris, leading to the Trojan War, which “is said to be The First War among men.”
Some recent interpretations of the Original Snub place Eris as being not at all mischievous with her delivery of the apple, but instead suggest that Eris was simply bringing the apple as a wedding present for Thetis. This interpretation would see Eris as innocent and her causing of chaos as a by-product of the other wedding guests' reaction upon seeing her at the wedding.
Curse of Greyface.
According to the "Principia", Greyface existed and had followers who he encouraged to "Look at all the order around you" ("Principia Discordia" page 00042), and he somehow convinced mankind to agree with his ideas about Serious Order. The "Principia" notes that it is something of a mystery why Greyface gained so many followers when anyone could have looked at all of the "disorder" in the world.
In addition to the generic advice of cultivating your natural love of chaos and playing with Her, the "Principia Discordia" provides "The Turkey Curse Revealed by the Apostle Dr. Van Van Mojo" to counteract The Curse of Greyface. The Turkey Curse is designed to counteract destructive order. It derives its name from the fact that the incantation resembles the sounds of a turkey.
Pentabarf.
The Pentabarf as follows:
The 5th law mirrors both the nature of Taoist sayings ("the Tao that can be named is not the true Tao") and Zen kōans ("If you meet the Buddha on your path, kill him"). It is also similar to the Epimenides paradox ("All Cretans are liars"), and several other paradoxes. Similarly, the third and fourth laws are designed to belie each other.
Pineal gland.
"Consult your pineal gland" is a common saying in Discordianism. This might be a reference to the ideas of past philosophers, such as René Descartes, who dedicated much time to the study of the pineal gland, called it the "principal seat of the soul". He believed it to be the point of connection between the intellect and the body.
Although it has never been proven, the pineal gland is theorized by some, such as Rick Strassman, to produce trace amounts of DMT (dimethyltryptamine), a psychedelic chemical which is believed to play a role in dreaming and other mystical states.
Discordian calendar.
The Discordian or Erisian calendar is an alternative calendar used by some Discordians. It is specified on page 00034 of the Principia Discordia. The Discordian year 1 YOLD is 1166 BC. Elsewhere in the "Principia Discordia", it is mentioned that the Curse of Greyface occurred in 1166 BC, so this is presumably the start date of the calendar. As a reference, 2015 AD is YOLD (Year of Our Lady of Discord). The abbreviation "YOLD" isn't used in the Principia, and the phrase "Year of Our Lady of Discord" is only mentioned once.

</doc>
<doc id="8528" url="http://en.wikipedia.org/wiki?curid=8528" title="Disjunction introduction">
Disjunction introduction

Disjunction introduction or addition is a simple valid argument form, an immediate inference and a rule of inference of propositional logic. The rule makes it possible to introduce disjunctions to logical proofs. It is the inference that if "P" is true, then "P or Q" must be true.
The rule can be expressed as:
where the rule is that whenever instances of "formula_2" appear on lines of a proof, "formula_3" can be placed on a subsequent line.
Disjunction introduction is controversial in paraconsistent logic because in combination with other rules of logic, it leads to explosion (i.e. everything becomes provable). See Tradeoffs in Paraconsistent logic.
Formal notation.
The "disjunction introduction" rule may be written in sequent notation:
where formula_5 is a metalogical symbol meaning that formula_3 is a syntactic consequence of formula_2 in some logical system;
and expressed as a truth-functional tautology or theorem of propositional logic:
where formula_2 and formula_10 are propositions expressed in some formal system.

</doc>
<doc id="8529" url="http://en.wikipedia.org/wiki?curid=8529" title="Disjunction elimination">
Disjunction elimination

In propositional logic, disjunction elimination (sometimes named proof by cases or case analysis), is the valid argument form and rule of inference that allows one to eliminate a disjunctive statement from a logical proof. It is the inference that if a statement formula_1 implies a statement formula_2 and a statement formula_3 also implies formula_2, then if either formula_1 or formula_3 is true, then formula_2 has to be true. The reasoning is simple: since at least one of the statements P and R is true, and since either of them would be sufficient to entail Q, Q is certainly true.
It is the rule can be stated as:
where the rule is that whenever instances of "formula_9", and "formula_10" and "formula_11" appear on lines of a proof, "formula_2" can be placed on a subsequent line.
Formal notation.
The "disjunction elimination" rule may be written in sequent notation:
where formula_14 is a metalogical symbol meaning that formula_2 is a syntactic consequence of formula_9, and formula_10 and formula_11 in some logical system;
and expressed as a truth-functional tautology or theorem of propositional logic:
where formula_1, formula_2, and formula_3 are propositions expressed in some formal system.

</doc>
<doc id="8530" url="http://en.wikipedia.org/wiki?curid=8530" title="Dead Sea">
Dead Sea

The Dead Sea (, ', "Sea of Salt", also , ', "The Sea of Death"; ,), also called the Salt Sea, is a salt lake bordering Jordan to the east, and Palestine and Israel to the west. Its surface and shores are below sea level, Earth's lowest elevation on land. The Dead Sea is deep, the deepest hypersaline lake in the world. With 34.2% salinity (in 2011), it is also one of the world's saltiest bodies of water, though Lake Vanda in Antarctica (35%), Lake Assal (Djibouti) (34.8%), Lagoon Garabogazköl in the Caspian Sea (up to 35%) and some hypersaline ponds and lakes of the McMurdo Dry Valleys in Antarctica (such as Don Juan Pond (44%)) have reported higher salinities. It is 9.6 times as salty as the ocean. This salinity makes for a harsh environment in which animals cannot flourish, hence its name. The Dead Sea is long and wide at its widest point. It lies in the Jordan Rift Valley, and its main tributary is the Jordan River.
The Dead Sea has attracted visitors from around the Mediterranean basin for thousands of years. Biblically, it was a place of refuge for King David. It was one of the world's first health resorts (for Herod the Great), and it has been the supplier of a wide variety of products, from balms for Egyptian mummification to potash for fertilizers. People also use the salt and the minerals from the Dead Sea to create cosmetics and herbal sachets.
The Dead Sea seawater has a density of 1.240 kg/L, which makes swimming similar to floating.
Etymology and toponymy.
In Hebrew, the Dead Sea is "" (), meaning "sea of salt" (Genesis 14:3). In the Bible, the Dead Sea is called the Salt Sea, the Sea of the Arabah, and the Eastern Sea. The designation "Dead Sea" never appears in the Bible. 
In prose sometimes the term ' (, "sea of death") is used, due to the scarcity of aquatic life there. In Arabic the Dead Sea is called ("the Dead Sea"), or less commonly ' (, "the Sea of Lot"). Another historic name in Arabic was the "Sea of Zoʼar", after a nearby town in biblical times. The Greeks called it "Lake Asphaltites" (Attic Greek , ', "the Asphaltite sea"). The Bible also refers to it as ' (, "the Eastern sea") and "" (, "Sea of the Arabah").
Geography.
The Dead Sea is an endorheic lake located in the Jordan Rift Valley, a geographic feature formed by the Dead Sea Transform (DST). This left lateral-moving transform fault lies along the tectonic plate boundary between the African Plate and the Arabian Plate. It runs between the East Anatolian Fault zone in Turkey and the northern end of the Red Sea Rift offshore of the southern tip of Sinai. It is here that the Upper Jordan River/Sea of Galilee/Lower Jordan River water system comes to an end. 
The Jordan River is the only major water source flowing into the Dead Sea, although there are small perennial springs under and around the Dead Sea, forming pools and quicksand pits along the edges. There are no outlet streams.
Rainfall is scarcely per year in the northern part of the Dead Sea and barely in the southern part. The Dead Sea zone's aridity is due to the rainshadow effect of the Judaean Mountains. The highlands east of the Dead Sea receive more rainfall than the Dead Sea itself.
To the west of the Dead Sea, the Judaean mountains rise less steeply and are much lower than the mountains to the east. Along the southwestern side of the lake is a tall halite formation called "Mount Sodom".
Natural history.
There are two contending hypotheses about the origin of the low elevation of the Dead Sea. The older hypothesis is that it lies in a true rift zone, an extension of the Red Sea Rift, or even of the Great Rift Valley of eastern Africa. A more recent hypothesis is that the Dead Sea basin is a consequence of a "step-over" discontinuity along the Dead Sea Transform, creating an extension of the crust with consequent subsidence.
Around 3.7 million years ago, what is now the valley of the Jordan River, Dead Sea, and the northern Wadi Arabah was repeatedly inundated by waters from the Mediterranean Sea. The waters formed in a narrow, crooked bay, called by geologists the Sedom Lagoon, which was connected to the sea through what is now the Jezreel Valley. The floods of the valley came and went depending on long-scale climate change. The Sedom Lagoon deposited beds of salt that eventually became thick.
Approximately two million years ago, the land between the Rift Valley and the Mediterranean Sea rose to such an extent that the ocean could no longer flood the area. Thus, the long lagoon became a landlocked lake. The Sedom Lagoon extended at its maximum from the Sea of Galilee in the north to somewhere around south of the current southern end of the Dead Sea, and the subsequent lakes obviously never surpassed this expanse. The Hula Depression was never part of any of these water bodies due to its higher elevation and the high threshold of the Korazin block separating it from the Sea of Galilee basin.
The first prehistoric lake to follow the Sedom Lagoon is named Lake Amora, followed by Lake Lisan and finally by the Dead Sea. The water levels and salinity of these lakes have either risen or fallen as an effect of the tectonic dropping of the valley bottom, and due to climate variation. As the climate became more arid, Lake Lisan finally shrank and became saltier, leaving the Dead Sea as its last remainder.
In prehistoric times, great amounts of sediment collected on the floor of Lake Amora. The sediment was heavier than the salt deposits and squeezed the salt deposits upwards into what are now the Lisan Peninsula and Mount Sodom (on the southwest side of the lake). Geologists explain the effect in terms of a bucket of mud into which a large flat stone is placed, forcing the mud to creep up the sides of the bucket. When the floor of the Dead Sea dropped further due to tectonic forces, the salt mounts of Lisan and Mount Sodom stayed in place as high cliffs (see salt dome).
From 70,000 to 12,000 years ago, the lake's level was to higher than its current level. This lake, Lake Lisan, fluctuated dramatically, rising to its highest level around 26,000 years ago, indicating a very wet climate in the Near East. Around 10,000 years ago, the lake's level dropped dramatically, probably to even lower than today's. During the last several thousand years, the lake has fluctuated approximately , with some significant drops and rises. Current theories as to the cause of this dramatic drop in levels rule out volcanic activity; therefore, it may have been a seismic event.
Climate.
The Dead Sea's climate offers year-round sunny skies and dry air. It has less than mean annual rainfall and a summer average temperature between . Winter average temperatures range between . The region has weakened ultraviolet radiation, particularly the UVB (erythrogenic rays). Given the heavier atmospheric pressure, the air has a slightly higher oxygen content (3.3% in summer to 4.8% in winter) as compared to oxygen density at sea level. Barometric pressures at the Dead Sea were measured between 796 and 799 mmHg and clinically compared with health effects at higher altitude. (This barometric measure is about 5% higher than sea level standard atmospheric pressure of 760 mmHg, which is the global ocean mean or ATM.) The Dead Sea affects temperatures nearby because of the moderating effect a large body of water has on climate. During the winter, sea temperatures tend to be higher than land temperatures, and vice versa during the summer months. This is the result of the water's mass and specific heat capacity. On average, there are 192 days above 30C (86F) annually.
Chemistry.
Until the winter of 1978–79, when a major mixing event took place, the Dead Sea was composed of two stratified layers of water that differed in temperature, density, age, and salinity. The topmost or so of the Dead Sea had an average salinity of 342 parts per thousand (in 2002), and a temperature that swung between and . Underneath a zone of transition, the lowest level of the Dead Sea had waters of a consistent temperature and complete saturation of sodium chloride (NaCl). Since the water near the bottom is saturated, the salt precipitates out of solution onto the sea floor.
Beginning in the 1960s, water inflow to the Dead Sea from the Jordan River was reduced as a result of large-scale irrigation and generally low rainfall. By 1975, the upper water layer was saltier than the lower layer. Nevertheless, the upper layer remained suspended above the lower layer because its waters were warmer and thus less dense. When the upper layer cooled so its density was greater than the lower layer, the waters mixed (1978–79). For the first time in centuries, the lake was a homogeneous body of water. Since then, stratification has begun to redevelop.
The mineral content of the Dead Sea is very different from that of ocean water. The exact composition of the Dead Sea water varies mainly with season, depth and temperature. In the early 1980s, the concentration of ionic species (in g/kg) of Dead Sea surface water was Cl− (181.4), Br− (4.2), SO42− (0.4), HCO3− (0.2), Ca2+ (14.1), Na+ (32.5), K+ (6.2) and Mg2+ (35.2). The total salinity was 276 g/kg. These results show that the composition of the salt, as anhydrous chlorides on a weight percentage basis, was calcium chloride (CaCl2) 14.4%, potassium chloride (KCl) 4.4%, magnesium chloride (MgCl2) 50.8% and sodium chloride (common salt, NaCl) 30.4%. In comparison, the salt in the water of most oceans and seas is approximately 97% sodium chloride. The concentration of sulfate ions (SO42−) is very low, and the concentration of bromide ions (Br−) is the highest of all waters on Earth.
The salt concentration of the Dead Sea fluctuates around 31.5%. This is unusually high and results in a nominal density of 1.24 kg/l. Anyone can easily float in the Dead Sea because of natural buoyancy. In this respect the Dead Sea is similar to the Great Salt Lake in Utah in the United States.
An unusual feature of the Dead Sea is its discharge of asphalt. From deep seeps, the Dead Sea constantly spits up small pebbles and blocks of the black substance. Asphalt coated figurines and bitumen coated Neolithic skulls from archaeological sites have been found. Egyptian mummification processes used asphalt imported from the Dead Sea region.
Health effects and therapies.
The Dead Sea area has become a major center for health research and treatment for several reasons. The mineral content of the water, the very low content of pollens and other allergens in the atmosphere, the reduced ultraviolet component of solar radiation, and the higher atmospheric pressure at this great depth each have specific health effects. For example, persons experiencing reduced respiratory function from diseases such as cystic fibrosis seem to benefit from the increased atmospheric pressure.
The region's climate and low elevation have made it a popular center for several types of therapies:
Treatment for psoriasis.
Climatotherapy at the Dead Sea is an effective therapy for patients with psoriasis, who benefit from sunbathing for long periods in the area due to its position below sea level and subsequent result that many of the sun's harmful UV rays are reduced.
Treatment for rhinosinusitis.
Rhinosinusitis patients receiving Dead Sea saline nasal irrigation exhibited significantly better symptom relief compared to standard hypertonic saline spray.
Treatment for osteoarthritis.
Dead Sea mud pack therapy has been suggested to temporarily relieve pain in patients with osteoarthritis of the knees. According to researchers of the Ben Gurion University of the Negev, treatment with mineral-rich mud compresses can be used to augment conventional medical therapy.
Fauna and flora.
The sea is called "dead" because its high salinity prevents macroscopic aquatic organisms, such as fish and aquatic plants, from living in it, though minuscule quantities of bacteria and microbial fungi are present.
In times of flood, the salt content of the Dead Sea can drop from its usual 35% to 30% or lower. The Dead Sea temporarily comes to life in the wake of rainy winters. In 1980, after one such rainy winter, the normally dark blue Dead Sea turned red. Researchers from Hebrew University of Jerusalem found the Dead Sea to be teeming with a type of alga called "Dunaliella". "Dunaliella" in turn nourished carotenoid-containing (red-pigmented) halobacteria, whose presence caused the color change. Since 1980, the Dead Sea basin has been dry and the algae and the bacteria have not returned in measurable numbers. Recently a group of scientists from Be'er Sheva, Israel and Germany discovered fissures in the floor of the Dead Sea by scuba diving and observing the surface. These fissures allow fresh water to enter the Dead Sea. They sampled biofilms surrounding the fissures and discovered a very significant number of species of Bacteria and Archea. This new research may change the current dogma that the Dead Sea cannot support life.
Many animal species live in the mountains surrounding the Dead Sea. Hikers can see camels, ibex, hares, hyraxes, jackals, foxes, and even leopards. Hundreds of bird species inhabit the zone as well. Both Jordan and Israel have established nature reserves around the Dead Sea.
The delta of the Jordan River was formerly a jungle of papyrus and palm trees. The Jewish historian Flavius Josephus described Jericho as "the most fertile spot in Judea". In Roman and Byzantine times, sugarcane, henna, and sycamore fig all made the lower Jordan valley wealthy. One of the most valuable products produced by Jericho was the sap of the balsam tree, which could be made into perfume. By the 19th century, Jericho's fertility had disappeared.
Human settlement.
There are several small communities near the Dead Sea. These include Ein Gedi, Neve Zohar and the Israeli settlements in the Megilot Regional Council: Kalya, Mitzpe Shalem and Avnat. There is a nature preserve at Ein Gedi, and several Dead Sea hotels are located on the southwest end at Ein Bokek near Neve Zohar. Highway 90 runs north-south on the Israeli side for a total distance of from Metula on the Lebanese border in the north to its southern terminus at the Egyptian border near the Red Sea port of Eilat.
Potash City is a small community on the Jordanian side of the Dead Sea, and others including Suweima. Highway 65 runs north-south on the Jordanian side from Lebanon down past the Dead Sea to the port of Aqaba.
Human history.
Biblical period.
Dwelling in caves near the Dead Sea is recorded in the Hebrew Bible as having taken place before the Israelites came to Canaan, and extensively at the time of King David. 
Just north of the Dead Sea is Jericho. Somewhere, perhaps on the southeastern shore, would be the cities mentioned in the Book of Genesis which were said to have been destroyed in the time of Abraham: Sodom and Gomorra (Genesis 18) and the three other "Cities of the Plain", Admah, Zeboim and Zoar (Deuteronomy 29:23). Zoar escaped destruction when Abraham's nephew Lot escaped to Zoar from Sodom (Genesis 19:21-22). Before the destruction, the Dead Sea was a valley full of natural tar pits, which was called the vale of Siddim. King David was said to have hidden from Saul at Ein Gedi nearby.
In there is a specific prophecy that the sea will ".. be healed" and "made fresh", becoming a normal lake capable of supporting marine life. A similar prophecy is stated in , which says that "Living waters will go out from Jerusalem, half of them to the eastern sea (likely the Dead Sea) and half to the western sea (the Mediterranean)..."
Greek and Roman period.
Aristotle wrote about the remarkable waters. The Nabateans and others discovered the value of the globs of natural asphalt that constantly floated to the surface where they could be harvested with nets. The Egyptians were steady customers, as they used asphalt in the embalming process that created mummies. The Ancient Romans knew the Dead Sea as "Palus Asphaltites" (Asphalt Lake).
King Herod the Great built or rebuilt several fortresses and palaces on the western bank of the Dead Sea. The most famous was Masada, where in 70 CE a small group of Jewish zealots fled after the fall of the destruction of the Second Temple. The zealots survived until 73 CE, when a siege by the X Legion ended in the deaths by suicide of its 960 inhabitants. Another historically important fortress along the western bank was Machaerus where, according to Josephus, John the Baptist was imprisoned by Herod Antipas and died.
Also in Roman times, some Essenes settled on the Dead Sea's western shore; Pliny the Elder identifies their location with the words, "on the west side of the Dead Sea, away from the coast ... [above] the town of Engeda" ("Natural History", Bk 5.73); and it is therefore a hugely popular but contested hypothesis today, that same Essenes are identical with the settlers at Qumran and that "the Dead Sea Scrolls" discovered during the 20th century in the nearby caves had been their own library.
Josephus identified the Dead Sea in geographic proximity to the ancient Biblical city of Sodom. However, he referred to the lake by its Greek name, Asphaltites.
Various sects of Jews settled in caves overlooking the Dead Sea. The best known of these are the Essenes of Qumran, who left an extensive library known as the Dead Sea Scrolls. The town of Ein Gedi, mentioned many times in the Mishna, produced persimmon for the temple's fragrance and for export, using a secret recipe. "Sodomite salt" was an essential mineral for the temple's holy incense, but was said to be dangerous for home use and could cause blindness. The Roman camps surrounding Masada were built by Jewish slaves receiving water from the towns around the lake. These towns had drinking water from the Ein Feshcha springs and other sweetwater springs in the vicinity.
Byzantine period.
Intimately connected with the Judean wilderness to its northwest and west, the Dead Sea was a place of escape and refuge. The remoteness of the region attracted Greek Orthodox monks since the Byzantine era. Their monasteries, such as Saint George in Wadi Kelt and Mar Saba in the Judaean Desert, are places of pilgrimage.
Modern times.
Explorers and scientists arrived in the area to analyze the minerals and research the unique climate. In the late 1940s and early 1950s, hundreds of religious documents dated between 150 BCE and 70 CE were found in caves near the ancient settlement of Qumran, about a mile inland from the northwestern shore of the Dead Sea (presently in the West Bank). They became known and famous as the Dead Sea Scrolls. A golf course named for Sodom and Gomorrah was built by the British at Kalia on the northern shore.
The world's lowest roads, Highway 90, run along the Israeli and West Bank shores of the Dead Sea, along with Highway 65 on the Jordanian side, at below sea level.
The first major hotels were built in nearby Arad, and since the 1960s at the Neve Zohar resort complex. On the Jordanian side, six international franchises have opened seaside resort hotels near the King Hussein Bin Talal Convention Center, along with resort apartments, on the eastern shore of the Dead Sea.
Industry and tourism.
British mandate period.
In the early part of the 20th century, the Dead Sea began to attract interest from chemists who deduced the sea was a natural deposit of potash (potassium chloride) and bromine. The Palestine Potash Company was chartered in 1929, after its founder, Siberian Jewish engineer and pioneer of Lake Baikal exploitation, Moses Novomeysky, worked for the charter for over ten years. The first plant was on the north shore of the Dead Sea at Kalya and produced potash by solar evaporation of the brine. Employing Arabs and Jews, it was an island of peace in turbulent times. The company quickly grew into the largest industrial site in the Middle East, and in 1934 built a second plant on the southwest shore, in the Mount Sodom area, south of the 'Lashon' region of the Dead Sea. Palestine Potash Company supplied half of Britain's potash during World War II, but ultimately became a casualty of the 1948 Arab–Israeli War. 
Israel.
Following the 1948 war, the Kalya plant in the West Bank was shut down, having been dismantled by the Palmach prior to their retreat. Operations restarted at the southern Sodom plant in 1952, and have continued to the present. The remnants of the Palestine Potash Company were nationalised and Dead Sea Works Ltd. was established in 1952 in its stead as a state-owned company to extract potash and other minerals from the Dead Sea. In 1995, the company was privatized and it is currently owned by Israel Chemicals.
From the Dead Sea brine, Israel produces (2001) 1.77 million tons potash, 206,000 tons elemental bromine, 44,900 tons caustic soda, 25,000 tons magnesium metal, and sodium chloride. 
Israeli companies generate around USD 3 billion annually from the sale of Dead Sea minerals (primarily potash and bromine) and from other products, which are derived from Dead Sea Minerals.
Israel has 15 hotels along the Dead Sea shore, generating total revenues of $291 million in 2012. Most Israeli hotels and resorts on the Dead Sea are on a six kilometer stretch of the southern shore.
West Bank.
The Palestinian Dead Sea Coast is about 40 kilometers long. The Palestinian economy is unable to benefit from Dead Sea chemicals due to restricted access, permit issues and the uncertainties of the investment climate. The World Bank estimates that a Palestinian Dead Sea chemicals industry could generate $918m incremental value added per year, "almost equivalent to the contribution of the entire manufacturing sector of Palestinian territories today".
The World Bank estimates that a Palestinian Dead Sea tourism industry could generate $290 million of revenues per year and 2,900 jobs. However, Palestinians have been unable to obtain construction permits for tourism-related investments on the Dead Sea. According to the World Bank, Officials in the Palestinian Ministry of Tourism and Antiquities state that the only way to apply for such permits is through the Joint Committees established under the Oslo Agreement, but the relevant committee has not met with any degree of regularity since 2000.
Jordan.
On the Jordanian side of the Dead Sea, Arab Potash (APC), formed in 1956, produces 2.0 million tons of potash annually, as well as sodium chloride and bromine. The plant is located at Safi, South Aghwar Department, in the Karak Governorate.
Jordanian Dead Sea mineral industries generate about $1.2 billion in sales (equivalent to 4 percent of Jordan’s GDP). The Jordanian shore has 5 hotels that are classified as either 5-star or 4-star, generating total revenues of $128 million in 2012.
Extraction.
Both companies use extensive salt evaporation pans that have essentially diked the entire southern end of the Dead Sea for the purpose of producing carnallite, potassium magnesium chloride, which is then processed further to produce potassium chloride. The ponds are separated by a central dike that runs roughly north-south along the international border. The power plant on the Israeli side allows production of magnesium metal (by a subsidiary, Dead Sea Magnesium Ltd.).
Due to the popularity of the sea's therapeutic and healing properties, several companies have also shown interest in the manufacturing and supplying of Dead Sea salts as raw materials for body and skin care products.
Recession and environmental concerns.
Since 1930, when its surface was and its level was below sea level, the Dead Sea has been monitored continuously. In recent decades, the Dead Sea has been rapidly shrinking because of diversion of incoming water from the Jordan River to the north. The southern end is fed by a canal maintained by the Dead Sea Works, a company that converts the sea's raw materials. From a water surface of below sea level in 1970 it fell to below sea level in 2006, reaching a drop rate of per year. As the water level decreases, the characteristics of the Sea and surrounding region may substantially change.
The Dead Sea level drop has been followed by a groundwater level drop, causing brines that used to occupy underground layers near the shoreline to be flushed out by freshwater. This is believed to be the cause of the recent appearance of large sinkholes along the western shore—incoming freshwater dissolves salt layers, rapidly creating subsurface cavities that subsequently collapse to form these sinkholes.
In May 2009 at the World Economic Forum, Jordan announced its plans to construct the "Jordan National Red Sea Development Project" (JRSP). This is a plan to convey seawater from the Red Sea near Aqaba to the Dead Sea. Water would be desalinated along the route to provide fresh water to Jordan, with the brine discharge sent to the Dead Sea for replenishment. The early planning called for developer and financier selection to be completed by year's end, with detailed design to begin in early 2010, and water delivery by 2017. Israel expressed its support and will likely benefit from some of the water delivery to its Negev region. Some hydro-power will be collected near the Dead Sea from the dramatic change in elevation on the downhill side of the project. 
At a regional conference in July 2009, officials expressed increased concerns about the declining water levels. Some suggested various industrial activities around the Dead Sea might need to be reduced. Others advised a range of possible environmental measures to restore conditions. This might include increasing the volume of flow from the Jordan River to replenish the Dead Sea. Currently, only sewage and effluent from fish ponds run in the river's channel. Experts also asserted a need for strict conservation efforts. They also said agriculture should not be expanded, sustainable support capabilities should be incorporated into the area and pollution sources should be reduced.
Sources: Jewish Virtual Library, Israel Oceanographic and Limnological Research, Jordan Valley Authority.
In October 2009, the Jordanians announced accelerated plans to extract around 300 million cubic meters of water per year from the Red Sea, desalinate it for use as fresh water and send the waste water to the Dead Sea by tunnel, despite concerns about inadequate time to assess the potential environmental impact. According to Jordan's minister for water, General Maysoun Zu'bi, this project could be considered as the first phase of the Red Sea–Dead Sea Project.
In December 2013, Israel, Jordan and the Palestinian Authority signed an agreement for laying a water pipeline to link the Red Sea with the Dead Sea. The pipeline will be 110 miles long and is estimated to take up to five years to complete.

</doc>
<doc id="8531" url="http://en.wikipedia.org/wiki?curid=8531" title="Dragon">
Dragon

A dragon is a legendary creature, typically with serpentine or reptilian traits, that features in the myths of many cultures. There are two distinct cultural traditions of dragons: the European dragon, derived from European folk traditions and ultimately related to Greek and Middle Eastern mythologies, and the Chinese dragon, with counterparts in Japan (namely the Japanese dragon), Korea and other East Asian countries.
The two traditions may have evolved separately, but have influenced each other to a certain extent, particularly with the cross-cultural contact of recent centuries. The English word derives from Greek ("drákōn"), "dragon, serpent of huge size, water-snake".
Name.
The word "dragon" entered the English language in the early 13th century from Old French "dragon", which in turn comes from Latin "draconem" (nominative "draco") meaning "huge serpent, dragon," from the Greek word δράκων, "drakon" (genitive "drakontos", δράκοντος) "serpent, giant seafish". The Greek and Latin term referred to any great serpent, not necessarily mythological, and this usage was also current in English up to the 18th century.
Morphology.
A dragon is a mythological representation of a reptile. In antiquity, dragons were mostly envisaged as serpents, but since the Middle Ages, it has become common to depict them with legs, resembling a lizard.
Dragons are usually shown in modern times with a body like a huge lizard, or a snake with two pairs of lizard-type legs, and able to emit fire from their mouths. The European dragon has bat-like wings growing from its back. A dragon-like creature with wings but only a single pair of legs is known as a wyvern.
Comparative mythology.
The association of the serpent with a monstrous opponent overcome by a heroic deity has its roots in the mythology of the Ancient Near East, including Canaanite (Hebrew, Ugaritic), Hittite and Mesopotamian. Humbaba, the fire-breathing dragon-fanged beast first described in the Epic of Gilgamesh is sometimes described as a dragon with Gilgamesh playing the part of dragon-slayer. The legless serpent ("Chaoskampf") motif entered Greek mythology and ultimately Christian mythology, although the serpent motif may already be part of prehistoric Indo-European mythology as well, based on comparative evidence of Indic and Germanic material.
Although dragons occur in many legends around the world, different cultures have varying stories about monsters that have been grouped together under the dragon label. Some dragons are said to breathe fire or to be poisonous, such as in the Old English poem Beowulf. They are commonly portrayed as serpentine or reptilian, hatching from eggs and possessing typically scaly or feathered bodies. They are sometimes portrayed as hoarding treasure. Some myths portray them with a row of dorsal spines. European dragons are more often winged, while Chinese dragons resemble large snakes. Dragons can have a variable number of legs: none, two, four, or more when it comes to early European literature.
Dragons are often held to have major spiritual significance in various religions and cultures around the world. In many Asian cultures dragons were, and in some cultures still are, revered as representative of the primal forces of nature, religion and the universe. They are associated with wisdom—often said to be wiser than humans—and longevity. They are commonly said to possess some form of magic or other supernatural power, and are often associated with wells, rain, and rivers. In some cultures, they are also said to be capable of human speech. In some traditions dragons are said to have taught humans to talk.
Narratives about dragons often involve them being killed by a hero. This topos can be traced to the "Chaoskampf" of the mythology of the Ancient Near East (e.g. Hadad vs. Yam, Marduk vs. Tiamat, Teshub vs. Illuyanka, etc.; the Biblical Leviathan presumably reflects a corresponding opponent of an early version of Yahweh). The motif is continued in Greek Apollo, and the early Christian narratives about Archangel Michael and Saint George. The slaying of Vrtra by Indra in the Rigveda also belongs in this category. The theme survives into medieval legend and folklore, with dragon slayers such as Beowulf, Sigurd, Tristan, Margaret the Virgin, Heinrich von Winkelried, Dobrynya Nikitich, Skuba Dratewka/Krakus. In Biblical myth, the archetype is alluded to in the descendants of Adam crushing the head of the Serpent, and in Christian mythology, this was interpreted as corresponding to Christ as the "New Adam" crushing the Devil.
The blood of a slain dragon is depicted as either beneficent or as poisonous in medieval legend and literary fiction. In German legend, dragon blood has the power to render invincible skin or armor bathed in it, as is the case with Siegfried's skin or Ortnit's armor. In the Slavic myth, the Earth refuses it as it is so vile that Mother Earth wishes not to have it within her womb, and it remains above ground for all eternity. The blood of the dragon in "Beowulf" has acidic qualities, allowing it to seep through iron. Heinrich von Winkelried dies after the blood of the dragon slain by him accidentally drips on him.
Europe.
Greek mythology.
In Ancient Greece the first mention of a "dragon" is derived from the "Iliad" where Agamemnon is described as having a blue dragon motif on his sword belt and an emblem of a three-headed dragon on his breast plate. However, the Greek word used (δράκων "drákōn", genitive δράκοντοϛ "drákontos") could also mean "snake".
In , Flavius Philostratus () discussed dragons (δράκων, drákōn) in India in "The Life of Apollonius of Tyana" (II,17 and III,6–8). The Loeb Classical Library translation (by F.C. Conybeare) mentions (III,7) that "In most respects the tusks resemble the largest swine's, but they are slighter in build and twisted, and have a point as unabraded as sharks' teeth."
According to a collection of books by Claudius Aelianus () called "On Animals", Ethiopia was inhabited by a species of dragon that hunted elephants. It could grow to a length of 180 feet (55 m) and had a lifespan rivaling that of the most enduring of animals.
European.
European dragons exist in folklore and mythology among the overlapping cultures of Europe. Dragons are generally depicted as living in rivers or having an underground lair or cave. They are commonly described as having hard or armoured hide, and are rarely described as flying, despite often being depicted with wings.
European dragons are usually depicted as malevolent under Christianity; pre-Christian dragons, such as Y Ddraig Goch, the Red Dragon of Wales, are seen as benevolent
Slavic dragon.
In Slavic mythology, the words "zmey", "zmiy" or "zmaj" are used to describe dragons. These words are masculine forms of the Slavic word for "snake", which are normally feminine (like Russian "zmeya"). In Romania, there is a similar figure, derived from the Slavic dragon and named "zmeu". Exclusively in Polish and Belarusian folklore, as well as in the other Slavic folklores, a dragon is also called (variously) "смок", "цмок", or "smok". In South Slavic folklores, the same thing is also called "lamya" (ламйа, ламjа, lamja). Although quite similar to other European dragons, Slavic dragons have their peculiarities.
Russian dragons usually have heads in multiples of three. Some have heads that grow back if every single head isn't cut off. In Ukraine and Russia, a particular dragon-like creature, "Zmey Gorynych", has three heads and spits fire. According to one bylina, Zmey Gorynych was killed by bogatyr Dobrynya Nikitich.
Other Russian dragons (such as Tugarin Zmeyevich) have Turkic names, probably symbolizing the Mongols and other nomadic steppe peoples. Accordingly, St George (symbolizing Christianity) killing the Dragon (symbolizing Satan) is represented on the coat of arms of Moscow. Some prehistoric structures, notably the Serpent's Wall near Kiev, have been associated with dragons.
South and West Asia.
Ancient India.
In the early Vedic religion, Vritra (Sanskrit: वृत्र (Devanāgarī) or (IAST)) "the enveloper", was an Asura and also a "naga" (serpent) () or possibly dragon-like creature, the personification of drought and enemy of Indra. Vritra was also known in the Vedas as Ahi ("snake") (), and he is said to have had three heads.
The "Life of Apollonius of Tyana" by Flavius Philostratus: contains a long detailed description of India heavily infested with dragons, but this does not correspond with modern Indian belief, and likely not with Indian belief as it was in his time, whether Apollonius invented this story, or whether he believed someone else who told him it.
Persian.
Aži Dahāka is the source of the modern Persian word azhdahā or ezhdehā اژدها (Middle Persian azdahāg) meaning "dragon", often used of a dragon depicted upon a banner of war. The Persians believed that the baby of a dragon will be the same color as the mother's eyes. In Middle Persian he is called Dahāg or Bēvar-Asp, the latter meaning "[he who has] 10,000 horses."
Several other dragons and dragon-like creatures, all of them malevolent, are mentioned in Zoroastrian scripture. (See Zahhāk).
Jewish.
In Jewish religious texts, the first mention of a dragon-like creature is in the Biblical works of Job (26:13), and Isaiah (27:1) where it is called "Nachash Bare'ach", or a "Pole Serpent". This is identified in the Midrash Rabba to Genesis 1:21 as Leviathan from the word "Taninim" (תנינים) "and God created the great sea-monsters." In modern Hebrew the word "Taninim" is used for Crocodiles but this is a 20th-century usage unconnected with the original Biblical meaning.
In later Biblical texts, the Book of Isaiah, the Book of Job, and Psalm 89 refer to a sea-demon called Rahab (not to be confused with Rahab, the woman of Jericho mentioned in the Book of Joshua). equates this Rahab with a dragon or monster. "Rahab" is the English transliteration of רהב ("reb") with the several meanings: pride, a mythical sea-monster, or Egypt (as an emblematic name). In the Douay-Rheims version, translated via Medieval Latin from the Vulgate, the word "reb" is rendered "the proud one" in and and "the power of the sea" in (Psalm 88 is equivalent to Psalm 89 in other versions due to different verse numbering in the Vulgate). The connection between the sea-monster and "Leviathan the serpent" is made in .
In Jewish astronomy this is also identified with the North Pole, the star Thuban which, around 4,500 years ago, was the star in the Draco constellation's "tail". However this can also have been either the celestial pole or the ecliptic pole. The ancient observers noted that Draco was at the top of the celestial pole, giving the appearance that stars were "hanging" from it, and in Hebrew it is referred to as "Teli", from talah (תלה) – to hang. Hebrew writers from Arabic-speaking locations identified the "Teli" as "Al Jaz'har", which is a Persian word for a "knot" or a "node" because of the intersection of the inclination of the orbit of a planet from the elliptic that forms two such nodes. In modern astronomy these are called the ascending node and the descending node, but in medieval astronomy they were referred to as "dragon's head" and "dragon's tail".
The Merthyr Synagogue features a dragon on the front gable.
East Asia.
In East Asia, the concept of dragon appears largely in a form of a "Long", a beneficent dragon-like creature from Chinese folklore. Another dragon-like creature which appears in the form of "Naga", which is prevalent in some Southeast Asian countries with more direct influence from Vedic religion, will be described largely in the article Naga.
Chinese dragon.
In China, depiction of the dragon (traditional:龍;simplified:龙) can be found in artifacts from the Shang and Zhou dynasties with examples dating back to the 16th centuryBC. Archaeologist Zhōu Chong-Fa believes that the Chinese word for dragon is an onomatopoeia of the sound of thunder. The Chinese name for dragon is pronounced "lóng" in Mandarin Chinese or "lùhng" in Cantonese. Sometime after the 9th century AD, Japan adopted the Chinese dragon through the spread of Buddhism. Although the indigenous name for a dragon in Japanese is ', a few of the Japanese words for dragon stem from the Chinese word for dragon, namely, ' or "" (traditional:龍;simplified:竜). The Vietnamese word for dragon is () and the Korean word for dragon is "ryong" (hangul:용, hanja:龍).
The Chinese dragon () is the highest-ranking animal in the Chinese animal hierarchy, strongly associated at one time with the emperor and hence power and majesty (the mythical bird fenghuang was the symbol of the Chinese empress), still recognized and revered. Its origins are vague, but its "ancestors can be found on Neolithic pottery as well as Bronze Age ritual vessels." Tradition has it composed of nine different animals, with nine sons, each with its own imagery and affiliations. It is the only mythological animal of the 12 animals that represent the Chinese calendar. 2012 was the Chinese year of the Water Dragon.
Japanese.
Japanese dragon myths amalgamate native legends with imported stories about dragons from China, Korea and India. Like these other Asian dragons, most Japanese ones are water deities associated with rainfall and bodies of water, and are typically depicted as large, wingless, serpentine creatures with clawed feet. Gould writes (1896:248), the Japanese dragon is "invariably figured as possessing three claws".
Bhutan.
The Druk (), also known as 'Thunder Dragon', is one of the National symbols of Bhutan. In the Dzongkha language, Bhutan is known as "Druk Yul" "Land of Druk", and Bhutanese leaders are called Druk Gyalpo, "Thunder Dragon Kings". The druk was adopted as an emblem by the Drukpa Lineage, which originated in Tibet and later spread to Bhutan.
Manipur.
Pakhangba is a mythical hybrid dragon of Manipur which originated in an ancient deity of the Meithei people preceding Hinduism in the region. It was the traditional heraldic emblem of the Princely state of Manipur. A Pakhangba is a dragon with deer antlers. It usually has the body of a snake, but in some sculptures at the Kangla Palace in Imphal it is represented with a short body and four sturdy legs, looking more like a lion.
Vietnam.
Vietnamese dragons ( or ) are symbolic creatures in the folklore and mythology of Vietnam. According to an ancient creation myth, the Vietnamese people are descended from a dragon and a fairy.
To Vietnamese people, the dragon brings rain, essential for agriculture. It represents the emperor, the prosperity and power of the nation. Like the Chinese dragon, the Vietnamese dragon is the symbol of yang, representing the universe, life, existence, and growth.
Extant references to the Vietnamese Dragon are rare now, due to the fierce changes in history that accompanied the sinicization of the Nguyễn Dynasty.
Modern depictions.
In the early 20th century sculpture of the Norwegian artist Gustav Vigeland, inspired by Medieval art, dragons are a frequent theme—as symbols of sin but also as a nature force, fighting against man.
Dragons and dragon motifs are featured in many works of modern literature, particularly within the fantasy genre. Prominent works depicting dragons include J.R.R. Tolkien's "Silmarillion" and "The Hobbit", J. K. Rowling's "Harry Potter" novels, Anne McCaffrey's "Dragonriders of Pern", George R. R. Martin's series "A Song of Ice and Fire", and Christopher Paolini's tetralogy "Inheritance Cycle".
The popular role playing game system "Dungeons & Dragons" (D&D) makes heavy use of dragons, and has served as inspiration for many other games' dragons. Though dragons usually serve as adversaries, they can be either good or evil, with their alignment being determined by their species. For example, a red dragon is evil and breathes fire.
Animals that may have inspired dragons.
It has been speculated that accounts of spitting cobras may be the origin of the myths of fire-breathing dragons.
Nile crocodiles, today very restricted in range, were in ancient times occasionally found in Southern Europe, having swum across the Mediterranean. Such wayward crocodiles may have inspired dragon myths. Skeletons of whales, as well as dinosaur and mammalian fossils may have been occasionally mistaken for the bones of dragons and other mythological creatures; for example, a discovery in in Wucheng, Sichuan, China, was labeled as such by Chang Qu. Adrienne Mayor has written on the subject of fossils as the inspiration for myths in her book "The First Fossil Hunters", and in an entry in the "Encyclopedia of Geology" she wrote: "Fossil remains generated a variety of geomyths speculating on the creatures' identity and cause of their destruction. Many ancient cultures, from China and India to Greece, America, and Australia, told tales of dragons, monsters, and giant heroes.."
In Australia, stories of such creatures may have referred to the land crocodiles, "Quinkana" sp., a terrestrial crocodile which grew to 5 to possibly 7 metres long, or the monitor lizard "Varanus priscus" (formerly "Megalania prisca") a giant carnivorous goanna that might have grown to , and weighed up to , or rainbow serpents (possibly "Wonambi naracoortensis") that were part of the extinct megafauna of Australia. Today the Komodo monitor lizard "Varanus komodoensis" is known in English as the Komodo dragon.
In the book "An Instinct for Dragons" anthropologist David E. Jones suggests a hypothesis that humans just like monkeys have inherited instinctive reactions to snakes, large cats and birds of prey. Dragons have features that are combinations of these three. An instinctive fear for these three would explain why dragons with similar features occur in stories from independent cultures on all continents.
In Slovenia, Janez Vajkard Valvasor compile folk stories on the Olm, a subterranean salamander, in "The Glory of the Duchy of Carniola". It is mentioned as a baby dragon. Heavy rains of Slovenia would wash the olms up from their subterranean habitat, giving rise to the folklore belief that great dragons lived beneath the Earth's crust, and the olms were the undeveloped offspring of these mythical beasts.
Cartography.
There is a widespread belief that earlier cartographers used the Latin phrase "hic sunt dracones", "i.e.", "the dragons are here", or "here are dragons", to denote dangerous or unexplored territories, in imitation of the infrequent medieval practice of putting sea serpents and other mythological creatures in the blank areas of maps. However, the only known use of this exact phrase is in the Latin form "HC SVNT DRACONES" on the Lenox Globe (ca. 1503–07).
Another map that contains dragons is the one of Bishop Olaus Magnus's. The Carta Marina map of Scandinavia (1539) has many monsters in the northern sea, as well as a winged, bipedal, predatory land animal resembling a dragon in northern Lapland.

</doc>
<doc id="8533" url="http://en.wikipedia.org/wiki?curid=8533" title="Depeche Mode">
Depeche Mode

Depeche Mode are an English electronic band formed in 1980 in Basildon, Essex. The group's original line-up consisted of Dave Gahan (lead vocals, occasional songwriter since 2005), Martin Gore (keyboards, guitar, vocals, chief songwriter after 1981), Andy Fletcher (keyboards), and Vince Clarke (keyboards, chief songwriter 1980–81). Depeche Mode released their debut record in 1981, "Speak & Spell", bringing the band onto the British new wave scene. Clarke left the band after the release of the album, leaving the band as a trio to record "A Broken Frame", released the following year. Alan Wilder (keyboards, drums, occasional songwriter) officially joined the band in late-1982, replacing Clarke, while Gore took over lead songwriting duties, establishing a line up that would continue for the next thirteen years.
The band's last albums of the 1980s; "Black Celebration" and "Music for the Masses" established them as a dominant force on the mainstream electronic music scene. A highlight of this era was the band's concert at the Pasadena Rose Bowl where they drew a crowd in excess of 60,000 people. In the new decade, Depeche Mode released "Violator", catapulting them to massive mainstream success. The subsequent album, "Songs of Faith and Devotion" and the supporting Devotional Tour exacerbated tensions within the band to the point where Alan Wilder quit in 1995, leading to intense media and fan speculation that the band would split. Now a trio once again, the band released "Ultra" in 1997, recorded at the height of Gahan's near-fatal drug abuse, Gore's alcoholism and Fletcher's depression. The release of "Exciter" confirmed Depeche Mode's willingness to remain together, the subsequent, and very successful, Exciter Tour being their first tour in support of an original album in eight years since the Devotional Tour although the band had toured in 1998 to support "The Singles 86>98" compilation album.
Depeche Mode have had 50 songs in the UK Singles Chart and thirteen top 10 albums in the UK charts, two of which debuted at No. 1. Depeche Mode have sold over 100 million records worldwide, making them the most commercially successful electronic band and one of the world's best-selling bands in music history. "Q" magazine calls Depeche Mode "The most popular electronic band the world has ever known" and included the band in the list of the "50 Bands That Changed the World!".
History.
Formation and debut album (1977–1981).
Depeche Mode's origins date to 1977, when schoolmates Vince Clarke and Andy Fletcher formed a The Cure-influenced band called No Romance In China, with Clarke on vocals and guitar and Fletcher on bass. Fletcher would later recall, "Why am I in the band? It was accidental right from the beginning. I was actually forced to be in the band. I played the guitar and I had a bass; it was a question of them roping me in." In 1979, Clarke played guitar in an "Ultravox rip-off band", The Plan, with friends Robert Marlow and Paul Langwith. In 1978–79, Martin Gore played guitar in an acoustic duo, Norman and The Worms, with school friend Phil Burdett on vocals. In 1979, Marlow, Gore, and friend Paul Redmond formed a band called The French Look, with Marlow on vocals/keyboards, Gore on guitar and Redmond on keyboards. In March 1980, Clarke, Gore and Fletcher formed a band called Composition of Sound, with Clarke on vocals/guitar, Gore on keyboards, and Fletcher on bass.
Soon after the formation of Composition of Sound, Clarke heard "Electricity", the debut single by Wirral electronic duo Orchestral Manoeuvres in the Dark (OMD). He said of the song: "It sounded so different from anything I'd heard; that really made me want to make electronic music, 'cause it was so unique." Along with OMD, other early influences included The Human League, Daniel Miller and Fad Gadget. Clarke and Fletcher switched to synthesisers, working odd jobs in order to buy or borrow the instruments from friends. Dave Gahan joined the band in 1980 after Clarke heard him perform at a local scout hut jam session, singing a rendition of David Bowie's "Heroes", and Depeche Mode were born. When explaining the choice for the new name taken from a French fashion magazine, "Dépêche mode" (from French "dépêche" that means here "dispatch" (from Old French "despesche/despeche") or "news report", and "mode" that means "fashion"), Gore said, "It means hurried fashion or fashion dispatch. I like the sound of that." But, in French, the real and only meaning of the magazine's name (and hence the band's) is "Fashion News" or "Fashion Update". Gore recollects that the first time the band played as Depeche Mode was a school gig in May 1980. There is a plaque commemorating the gig at the James Hornsby School in Basildon where Gore and Fletcher were pupils. The band made their recording debut in 1980 on the "Some Bizzare Album" with the song "Photographic", which was later re-recorded for their debut album "Speak & Spell".
The band made a demo tape but, instead of mailing the tape to record companies, they would go in and personally deliver it. They would demand the companies play it; according to Dave Gahan, "most of them would tell us to fuck off. They'd say 'leave the tape with us' and we'd say 'it's our only one'. Then we'd say goodbye and go somewhere else."<ref name=record/ref>Rolling Stone magazine, "This band wants your respect – Depeche Mode may sell millions of albums and play to capacity crowds in huge football stadiums but these techno-pop idols still aren't happy" by Jeff Giles, with photography by John Stoddart, pages 84–87, 11 July 1990. Retrieved 4 May 2012.</ref>
According to Gahan, prior to securing their record contract, they were receiving offers from all the major labels. Phonogram offered them "money you could never have imagined and all sorts of crazy things like clothes allowances".
While playing a live gig at the Bridge House in Canning Town, the band were approached by Daniel Miller, an electronic musician and founder of Mute Records, who was interested in their recording a single for his burgeoning label. The result of this verbal contract was their first single, "Dreaming of Me", recorded in December 1980 and released in February 1981. It reached number 57 in the UK charts. Encouraged by this, the band recorded their second single, "New Life", which climbed to number 11 in the UK charts and got them to appear on "Top of the Pops". The band actually went to London by train, dragging their synthesisers all the way to the BBC's studios.
The band's next single was "Just Can't Get Enough". This relentlessly upbeat piece of synthpop became the band's first UK top ten hit and it remains one of their best known songs. It was also the first Depeche Mode song to get a music video and is the only one of the band's videos to feature Vince Clarke. Depeche Mode's debut album, "Speak & Spell", was released in October 1981 and peaked at number ten on the UK album charts. Critical reviews were mixed – "Melody Maker" described it as a "great album... one they had to make to conquer fresh audiences and please the fans who just can't get enough", while "Rolling Stone" was more critical, calling the album "PG-rated fluff".
Clarke departs, Wilder joins (1981–1982).
During the touring and promotion for "Speak & Spell", Clarke privately began to voice his discomfort at the direction the band were taking. He later expressed his dissatisfaction, saying "there was never enough time to do anything. Not with all the interviews and photo sessions.". In November 1981 Clarke publicly announced that he was leaving Depeche Mode. It also was claimed Clarke was sick of touring, which Gahan said years later was "bullshit to be quite honest". Gahan went on to say he "suddenly lost interest in it and he started getting letters from fans asking what kind of socks he wore".
Soon afterwards, Clarke joined up with blues singer Alison Moyet to form Yazoo (Yaz in the US) and later, the duo Erasure with Andy Bell. Initial talk of Clarke's continuing to write material for the group ultimately amounted to nothing. According to third-party sources, Clarke offered the remaining members of Depeche Mode the track "Only You", but they declined. Clarke, however, denied in an interview that such an offer ever took place saying, "I don't know where that came from. That's not true." The song went on to become a UK Top 3 hit for Yazoo. Gore, who had written "Tora! Tora! Tora!" and the instrumental "Big Muff" for "Speak & Spell", was forced to become the band's new songwriter.
In late 1981, the band placed an anonymous ad in "Melody Maker" looking for another musician; it said "Name band, synthesise, must be under twenty-one." Alan Wilder, a keyboardist from West London, responded and, after two auditions and despite being 22 years old, he was hired in early 1982, initially on a trial basis as a touring member. Wilder would later be called the "Musical Director" of the band, responsible for the band's sound until his departure in 1995. As producer Flood would later say, "[Alan] is sort of the craftsman, Martin's the idea man and [Dave] is the attitude."
In January 1982, the band released "See You", their first single without Clarke, which managed to beat all three Clarke-penned singles in the UK charts, reaching number six. The tour that followed the release of the single saw the band playing their first shows in North America. Two more singles, "The Meaning of Love", and "Leave in Silence", were released ahead of the band's second studio album. Depeche Mode began work on their second album in July 1982. Daniel Miller informed Wilder that he was not needed for the recording of the album, as the band wanted to prove that they could succeed without Vince Clarke. "A Broken Frame" was released that September and the following month the band set off on their second tour of 1982. A non-album single "Get the Balance Right!" was released in January 1983, and was the first Depeche Mode track to be recorded with Wilder.
"Construction Time Again" (1983).
For their third LP "Construction Time Again", Depeche Mode worked with producer Gareth Jones, at John Foxx's Garden Studios and at Hansa Studios in West Berlin (where much of David Bowie's trilogy of seminal electronic albums featuring Brian Eno had been produced). The album saw a dramatic shift in the group's sound, due in part to Wilder's introduction of the Synclavier and E-mu Emulator samplers. By sampling the noises of everyday objects, the band created an eclectic, industrial-influenced sound, with similarities to groups such as the Art of Noise and Einstürzende Neubauten, the latter later having work released on the Mute label.
Along with the music, Gore's songwriting was also rapidly evolving, focusing increasingly on political and social issues. A good example of the new sound was on the first single from the album "Everything Counts", a commentary on the perceived greed of multinational corporations. In a retrospective review of the single, Allmusic journalist Ned Raggett wrote that the song marked a change in the band "with Martin Gore's songwriting abilities matched with an increasing ambition of the band as a whole."
"Everything Counts" got to number six in the UK, also reaching the top 30 in Ireland, South Africa, Switzerland, Sweden and West Germany. Wilder also contributed two songs to the album, "The Landscape Is Changing" and "Two Minute Warning". In September 1983, to promote "Construction Time Again" the band launched Construction Time Again Tour, a concert tour all over Europe.
"Some Great Reward" (1984) and increasing international success.
In their early years, Depeche Mode had only really attained success in Europe and Australia, however this changed in March 1984 when they released the single "People Are People". The song became a big hit, reaching No. 2 in Ireland and Poland, No. 4 in UK and Switzerland and No. 1 in West Germany – the first time a DM single topped a country's singles chart – where it was used as the theme to West German TV's coverage of the 1984 Olympics. But, beyond this European success, the song also reached No. 13 on the US charts in mid-1985, which was the first appearance of a DM single on the "Billboard" Hot 100 (this song was a Top 20 hit in Canada at the same period too). "People Are People" has since become an anthem for the LGBT community and was regularly played at gay establishments and gay pride festivals in the late 1980s. Sire, the band's North American record label, released a compilation of the same name which included tracks from "A Broken Frame" and "Construction Time Again" as well as several b-sides.
It was the tour in America that year where the band noticed things were changing. They were, according to Gore, "shocked by the way the fans were turning up in droves at the concerts". He said that although the concerts were selling well, they were still struggling to sell records.
In September 1984, "Some Great Reward" was released. "Melody Maker" claimed that the album made one "sit up and take notice of what is happening here, right under your nose." In contrast to the political and environmental subjects addressed on the previous album, the songs on "Some Great Reward" were mostly concerned with more personal themes such as sexual politics ("Master and Servant"), adulterous relationships ("Lie to Me"), and arbitrary divine justice ("Blasphemous Rumours"). Also included was the first Martin Gore ballad ("Somebody") – such songs would become a feature of all following albums. "Somebody" was released as a double a-side with "Blasphemous Rumours" and was the first single with Gore on lead vocals. "Some Great Reward" was the first Depeche Mode album to enter the US album charts, and it made the Top 10 in several European countries.
"The World We Live In and Live in Hamburg" was the band's first video release. It is an almost complete film of a concert from their 1984 Some Great Reward Tour, in Hamburg, Germany. In July 1985, the band played their first-ever concerts behind the Iron Curtain, in Budapest and Warsaw. In October 1985, Mute Records released a compilation, "The Singles 81>85" ("Catching Up with Depeche Mode" in the US), which included the two non-album hit-singles "Shake the Disease" and "It's Called a Heart".
During this period, in some circles, the band became associated with the gothic subculture, which had begun in Britain in the early-1980s, and was now slowly gaining popularity in the United States. There, the band's music had first gained prominence on college radio and modern rock stations such as KROQ in Los Angeles, KQAK ("The Quake") in San Francisco, WFNX in Boston and WLIR on Long Island, New York, and hence they appealed primarily to an alternative audience who were disenfranchised with the predominance of "soft rock and 'disco hell'" on the radio. This view of the band was in sharp contrast to how the band was perceived in Europe, despite the increasingly dark and serious tone in their songs. In Germany, France and other European countries, Depeche Mode were considered teen idols and were regularly featured in European teen magazines, becoming one of the most famous synthpop bands in the mid-80's.
"Black Celebration" (1986).
Depeche Mode's musical style shifted slightly again in 1986 with the release of their fifteenth single "Stripped", and its accompanying album "Black Celebration". Retaining their often imaginative sampling and beginning to move away from the "industrial-pop" sound that had characterised their previous two LPs, the band introduced an ominous, highly atmospheric and textured sound. Gore's lyrics also took on a darker tone and became even more pessimistic.
The music video for "A Question of Time" was the first to be directed by Anton Corbijn, beginning a working relationship that continues to the present day. Corbijn has directed a further 19 of the band's videos (the latest being 2006's "Suffer Well"). He has also filmed some of their live performances and designed stage sets and album and single covers.
"Music for the Masses" and "101" (1987–1988).
1987's "Music for the Masses" saw further alterations in the band's sound and working methods. For the first time a producer not related to Mute, Dave Bascombe, was called to assist with the recording sessions (although, according to Alan Wilder, his role ended up being more that of an engineer). In making the album the band largely eschewed sampling in favour of more synth experimentation. While the chart performance of the singles "Strangelove", "Never Let Me Down Again" and "Behind the Wheel" proved to be disappointing in the UK, they performed well in countries such as Canada, Brazil, West Germany, South Africa, Sweden and Switzerland, often reaching the top 10. "Record Mirror" described "Music for the Masses" as "the most accomplished and sexy Mode album to date" and it made a breakthrough in the American market, something which the band had failed to achieve with their previous albums.
The Music for the Masses Tour followed the release of the album. On 7 March 1988 they played an unofficial gig (as it was not officially announced that Depeche Mode were the band performing that night) in the Werner-Seelenbinder-Halle, East Berlin. At that time the communist regime were still in power and Depeche Mode were among the very few western bands that ever played in the former GDR. Around the same period, they also gave concerts in Budapest and Prague (1988) in the then still communist Hungary and Czechoslovakia respectively.
The world tour ended on 18 June 1988 with a concert at the Pasadena Rose Bowl with paid attendance of 60,453 (the highest in eight years for the venue). The tour was a breakthrough for the band and a massive success in the United States. It was documented in "101" – a concert film by D. A. Pennebaker and its accompanying soundtrack album. The film is notable for its portrayal of fan interaction. Alan Wilder is credited with coming up with the name; the performance was the 101st and final performance of the tour. On 7 September 1988, Depeche Mode performed "Strangelove" at the 1988 MTV Video Music Awards at the Universal Amphitheatre in Los Angeles.
"Violator" and worldwide fame (1989–1991).
In mid-1989, the band began recording in Milan with producer Flood and engineer François Kevorkian. The initial result of this session was the single "Personal Jesus". Prior to its release, a marketing campaign was launched with advertisements placed in the personal columns of UK regional newspapers with the words "Your own personal Jesus." Later, the ads included a phone number one could dial to hear the song. The resulting furore helped propel the single to number 13 on the UK charts, becoming one of their biggest sellers to date; in the US, it was their first gold single and their first Top 40 hit since "People Are People", eventually becoming the biggest-selling 12-inch single in Warner Bros. Records' history up to that point.
Released in January 1990, "Enjoy the Silence" became one of Depeche Mode's most successful singles to date, reaching number six in the UK (the first Top 10 hit in that country since "Master And Servant"). A few months later it became Depeche Mode's biggest hit in the US, reaching number eight and earning the band a second gold single. It won 'Best British single' at the 1991 Brit Awards. To promote their new album "Violator", the band held an in-store autograph signing at Wherehouse Entertainment in Los Angeles. The event attracted approximately 20,000 fans and turned into a near riot. Some of those who attended were injured by being pressed against the store's glass by the crowd. As an apology to the fans who were injured, the band released a limited edition cassette tape to fans living in Los Angeles, which was distributed through radio station KROQ (the sponsor of the Wherehouse event). "Violator" went on to reach Top 10 in the UK and US. "Violator" was the first of the band's albums to enter the Top 10 of the "Billboard" 200— reaching No. 7 and staying 74 weeks in the chart. It has also been certified triple platinum in America, selling over 4.5 million units there. It remains the band's best selling album worldwide. Two more singles from the album "Policy of Truth" and "World in My Eyes" were hits in the UK with the former also charting in the US.
The World Violation Tour marked another high point in Depeche Mode's popularity and saw the band play several stadium shows in the US. 42,000 tickets were sold within four hours for a show at Giants Stadium and 48,000 tickets were sold within half-an-hour of going on sale for a show at Dodger Stadium. It was estimated that 1.2 million fans saw this tour worldwide.
In 1991, Depeche Mode contribution, "Death's Door" was released on the accompanying Warner Brothers album, "Until the End of the World: original motion picture soundtrack" for the film "Until the End of the World". Musical artists were challenged by film director Wim Wenders to write music the way they imagined they would in the year 2000, the setting of the movie.
"Songs of Faith and Devotion" and Wilder's departure (1992–1996).
The members of Depeche Mode regrouped in Madrid in January 1992, Dave Gahan had become interested in the new grunge scene sweeping the U.S. and was influenced by the likes of Jane's Addiction, Soundgarden and Nirvana.
In 1993 "Songs of Faith and Devotion" again with producer Flood, saw them experimenting with more organic arrangements, based as much on heavily distorted electric guitars and live drums (played by Alan Wilder, whose debut as a studio drummer had come on the "Violator" track "Clean") as synthesisers. Live strings, uilleann pipes and female gospel vocals were other new additions to the band's sound. The album debuted at number one in both the UK and the US, only the sixth British act to achieve such a distinction to date. The first single from the album was the grunge-influenced "I Feel You". The gospel influences are most noticeable on the album's third single, "Condemnation". A symptom of the slow fracturing of the band, interviews given by the band during this period tended to be conducted separately, unlike earlier albums, where the band was interviewed as a group.
The Devotional world tour followed. It was documented by a concert film of the same name. The film was directed by Anton Corbijn and in 1995 earned the band their first Grammy nomination.
The band's second live album, "Songs of Faith and Devotion Live", was released in December 1993.
The tour continued into 1994 with the Exotic Tour, which began in February 1994 in South Africa and ended in April in Mexico. The final leg of the tour, consisting of more North American dates, followed shortly thereafter and ran until July. As a whole, the Devotional Tour is to date the longest and most geographically diverse Depeche Mode tour, spanning fourteen months and 159 individual performances. "Q" magazine listed 1993 Devotional Tour as "The Most Debauched Rock'n'Roll Tour Ever".
Dave Gahan's heroin addiction was starting to affect his behaviour, causing him to become more erratic and introverted. Martin Gore experienced seizures and Andy Fletcher declined to participate in the second half of the Exotic Tour due to "mental instability". During that period, he was replaced on-stage by Daryl Bamonte, who had worked with the band as a personal assistant for many years.
In June 1995, Alan Wilder announced that he was leaving Depeche Mode, explaining,
He continued to work on his personal project Recoil, releasing a fourth album ("Unsound Methods") in 1997. Following Wilder's departure, many were sceptical of whether Depeche Mode would ever record again. Gahan's mental state and drug habit became a major source of concern, with a near-fatal overdose at a hotel in Los Angeles.
"Ultra" (1997–2000).
Despite Gahan's increasingly severe personal problems, Gore tried repeatedly during 1995 and 1996 to get the band recording again. However, Gahan would rarely turn up to scheduled sessions, and when he did, it would take weeks to get any vocals recorded. One six-week session at Electric Lady in New York produced just one usable vocal (for "Sister Of Night"), and even that was pieced together from multiple takes. Gore was forced to contemplate breaking the band up, and releasing the songs he had written as a solo album. In mid-1996, Gahan entered a court ordered drug rehabilitation program to battle his cocaine-heroin addiction after his near fatal overdose. With Gahan out of rehab in 1996, Depeche Mode held recording sessions with producer Tim Simenon. The album "Ultra" was released in April 1997, its release was preceded by two singles, "Barrel of a Gun" and "It's No Good". The album debuted at No. 1 in the UK (as well as Germany), and No. 5 in the US. The band did not tour in support of the album but as part of the promotion for its release they did perform two short concerts in London and Los Angeles called Ultra Parties. "Ultra" spawned two further singles, "Home" and "Useless".
A second singles compilation "The Singles 86–98" was released in 1998, preceded by the new single "Only When I Lose Myself", which had been recorded during the "Ultra" sessions. In April 1998 Depeche Mode held a press conference at the Hyatt Hotel in Cologne to announce The Singles Tour. The tour was the first one to feature two backing musicians in place of Alan Wilder – Austrian drummer Christian Eigner and British keyboardist Peter Gordeno.
"Exciter" (2001–2004).
In 2001, Depeche Mode released "Exciter", which was produced by Mark Bell (of the pioneering techno group LFO). Bell introduced a minimalist, digital sound to much of the album, influenced by IDM and glitch. "Dream On", "I Feel Loved", "Freelove" and "Goodnight Lovers" were released as singles in 2001 and 2002. The critical response to the album was mixed. Whilst it received reasonably positive reviews from some magazines ("NME", "Rolling Stone" and "LA Weekly"), others (including "Q" magazine, PopMatters, and Pitchfork Media) derided it as sounding underproduced, dull and lacklustre.
In March 2001 Depeche Mode held a press conference at the Valentino Hotel in Hamburg to announce the Exciter Tour. In total the tour featured 84 performances for over 1.5 million fans in 24 countries. The concerts held in Paris at the Palais Omnisports de Paris-Bercy were filmed and later released in May 2002 as a live DVD entitled "One Night in Paris".
In October 2002 the band won the first-ever "Q" magazine "Innovation Award".
In 2003 Dave Gahan released his first solo album, "Paper Monsters", and toured to promote the record. Also released in 2003 was Martin Gore's second solo album "Counterfeit²". Andrew Fletcher also founded his own record label, Toast Hawaii, specialising in promoting electronic music.
A new remix compilation album "Remixes 81–04" was released in 2004, featuring new and unreleased promo mixes of the band's singles from 1981 to 2004. A new version of "Enjoy the Silence", remixed by Mike Shinoda, entitled "Enjoy the Silence 04" was released as a single and reached No. 7 on the UK charts.
"Playing the Angel" (2005–2007).
In October 2005, the band released their 11th studio album "Playing the Angel". Produced by Ben Hillier the album peaked at No. 1 in 18 countries and featured the hit single "Precious". This is the first Depeche Mode album to feature lyrics written by Gahan and, consequently, the first album since 1984's "Some Great Reward" featuring songs not written by Gore. "Suffer Well", was the first ever post-Clarke Depeche Mode single not to be written by Gore (lyrics by Gahan, music by Philpott/Eigner). The final single from the album was "John the Revelator", an uptempo electronic track with a running religious theme, accompanied by "Lilian", a lush track that was a hit in many clubs all over the world.
To promote "Playing the Angel" the band launched Touring the Angel, a concert tour of Europe and North America that began in November 2005 and ran for nine months. During the last two legs of the tour Depeche Mode headlined a number of festivals including the Coachella Valley Music and Arts Festival and the O2 Wireless Festival. In total the band played to more than 2.8 million people across 31 countries and the tour was one of the highest grossing and critically acclaimed tours of 2005/06. Speaking about the tour, Gahan praised it as "probably the most enjoyable, rewarding live shows we've ever done. The new material was just waiting to be played live. It took on a life of its own. With the energy of the crowds, it just came to life". Two shows at Milan's Fila Forum
were filmed and edited into a concert film which was released on DVD as "".
A "best-of" compilation was released in November 2006, entitled "The Best Of, Volume 1" featuring a new single "Martyr", an outtake from the "Playing the Angel" sessions. Later that month Depeche Mode received the MTV Europe Music Award in the Best Group category.
In December 2006, iTunes released "The Complete Depeche Mode" as its fourth ever digital box-set (following "The Complete U2" in 2004, "The Complete Stevie Wonder" in 2005, and "" earlier in 2006).
In August 2007, during promotion for Dave Gahan's second solo album, "Hourglass", it was announced that Depeche Mode were heading back in studio in early 2008 to work on a new album.
"Sounds of the Universe" (2008–2011).
In May 2008, the band returned to the studio with producer Ben Hillier to work on some songs that Martin Gore had demoed at his home studio in Santa Barbara, California. Later that year it was announced that Depeche Mode were splitting from their long-term US label, Warner Music, and signing with EMI Music worldwide.
On 15 January 2009, the official Depeche Mode website announced that the band's 12th studio album would be called "Sounds of the Universe". The album was released in April 2009, it was also made available through an iTunes Pass, where the buyer received individual tracks in the weeks leading up to official release date. Andy Fletcher says the idea for their iTunes Pass was a combination of the band's and iTunes': "I think the digital and record companies are starting to get their act together. They were very lazy in the first 10 years when downloads came in. Now they're collaborating more and coming up with interesting ideas for fans to buy products." The album went to number one in 21 countries. Critical response was generally positive and it was nominated for a Grammy in the "Best Alternative Album" category.
"Wrong" was the first single from the album, released digitally in February 2009. Subsequent singles were "Peace" and the double a-side "Fragile Tension / Hole to Feed". In addition, "Perfect" was released as a promotional-only (non-commercial) single in the US.
Depeche Mode performed on "Jimmy Kimmel Live!" in the Hollywood Boulevard in Los Angeles on 23 April 2009, drawing more than 12,000 fans, which was the largest audience the show had ever seen since premiering in 2003.
In May 2009 the band embarked on a concert tour in support of the album – called Tour of the Universe, it had been announced at a press conference in October 2008 at the Olympiastadion in Berlin. There was a warm up show in Luxembourg and it officially started on 10 May 2009 in Tel Aviv. The first leg of the tour was disrupted when Dave Gahan was struck down with gastroenteritis. During treatment doctors found and removed a low grade tumour from the singer's bladder. Gahan's illness caused 16 concerts to be cancelled, but several of the shows were rescheduled for 2010. The band headlined the Lollapalooza festival during the North American leg of the tour. The tour also took the band back to South America for the first time since 1994's Exotic Tour. During the final European leg the band played a show at London's Royal Albert Hall in aid of the Teenage Cancer Trust, where former member Alan Wilder joined Martin Gore on stage for a performance of "Somebody". In total the band played to more than 2.7 million people across 32 countries and the tour was one of the most profitable in America in 2009. The concerts held at Palau Sant Jordi, Barcelona, Spain were filmed and later released on DVD and Blu-ray Disc release entitled "".
In March 2010, Depeche Mode won the award for "Best International Group – Rock / Pop" at the ECHO Awards in Germany.
As a conclusion of the 3-year working relationship with EMI, on 6 June 2011, the band released a remix compilation album, entitled "" that features remixes by former members Vince Clarke and Alan Wilder. Other remixers involved with the project were Nick Rhodes of Duran Duran, Röyksopp, Karlsson & Winnberg of Miike Snow, Eric Prydz, Clark and more. A new remix of "Personal Jesus" by Stargate, entitled "Personal Jesus 2011", was released as a single on 30 May 2011, in support of the compilation.
Depeche Mode contributed their cover of the U2 song "So Cruel" to the tribute album "AHK-toong BAY-bi Covered" honouring the 20th anniversary of "Achtung Baby", a 1991 album by U2. The compilation CD was released with the December 2011 issue of Q Magazine.
"Delta Machine" (2012–present).
In October 2012 during a press conference in Paris, Dave Gahan, Martin Gore and Andy Fletcher announced plans to release a new album and a worldwide tour starting from Tel Aviv and continuing in Europe and North America in the year 2013. Martin Gore revealed that Flood mixed the album, marking the producer's first studio collaboration with the band since 1993's "Songs of Faith and Devotion".
In December 2012, the band officially announced signing a worldwide deal with Columbia Records and releasing a new album in March 2013. On 24 January 2013, it was confirmed that the album was called "Delta Machine". "Heaven", the debut single from "Delta Machine" was released commercially on Friday 1 February 2013 (although not in the UK). The release date in the UK was pushed back to 18 March 2013 (17 March 2013 on iTunes), currently with no explanation. Bafflingly, the physical release still bore the Mute Records logo, even though the band have now severed ties with their long standing label. Andy Fletcher mentioned in an interview this was due to their "devotion" to the label and with the band's insistence.
In March, the band announced North American dates to its 'Delta Machine' summer tour, starting 22 August from Detroit and ending 8 October in Phoenix. In June, other European dates were confirmed for early 2014. The final gig of "Delta Machine Tour" took place in Moscow (Russia) on 7 March 2014, at Olimpiski venue.
In March 2014, Depeche Mode won the award for "Best International Group – Rock / Pop" at the ECHO Awards in Germany. Also they were nominated at the category "Album des Jahres (national oder international)" for "Delta Machine", but lost against Helene Fischer "Farbenspiel".
On October 8, 2014 the band announced "Depeche Mode Live in Berlin", the new video and audio release filmed and recorded at the O2 World in Berlin, Germany on November 25 and 27, 2013 during the Delta Machine Tour. It will be released on November 17, 2014 worldwide.
Artistry.
Musical style.
Depeche Mode drew its artistic influences from a wide range of artists and scenes, such as Kraftwerk, David Bowie, The Clash, blues music, the American grunge scene and Velvet Underground. The genres Depeche Mode are typically associated with include synthpop, alternative dance, new wave, and alternative rock. The band also experimented with various genres throughout its career, including avant-garde, electronica, pop, soul, techno and industrial rock.
Depeche Mode were recognised in their early years as a synthpop band and were considered teen idols. Following the departure of Vince Clarke, their music began to take on a darker and a gothic tone as Martin Gore assumed lead songwriting duties. Gore's lyrical artistry has been recognised as encapsulating themes such as sex, religion, and politics, so much so that many labeled the band's lyrical and musical themes as dark and bleak. In response, Gore stated he feels lyrical themes which tackle issues related to solitude and loneliness present more of a realistic character and are a better representation of reality, whereas he finds 'happy songs' fake and unrealistic. Gore also added; "I've never seen our music as being over-dark. I think that there is always an element of hope in our music." 
Influence.
Depeche Mode have been recognised as making a significant impact on the development of various popular music genres, leading to many artists citing them as an inspiration including (but not limited to); Pet Shop Boys, Derrick May, Kevin Saunderson, Juan Atkins, Brandon Flowers, Chester Bennington and Mike Shinoda of Linkin Park, Televizor, Chino Moreno and Stephen Carpenter of Deftones, The Crystal Method, Mad at the World, Raymond Herrera of Fear Factory, Funeral for a Friend, Shakira, Coldplay, Muse, Rammstein, Magne Furuholmen of a-ha, Arcade Fire Nine Inch Nails, Lady Gaga, Gary Numan and Chvrches.
Depeche Mode have been cited as a major influence on the Detroit techno scene, indie rock and industrial metal music.
Legacy.
One of the most influential musical groups over the past three decades, Depeche Mode have released a total of 13 studio albums, ten compilation albums, six live albums, eight box sets, 13 video albums, 70 music videos and 53 singles. The band have sold over 100 million records and have played to in excess of 30 million people, making them one of the world's best selling music artists. The band have had forty-eight songs in the UK Singles Chart, one US and two UK number one albums. The band's album Songs of Faith and Devotion hit #1 in the UK and USA simultaneously, making them one of only eleven UK acts to do so, the others being The Beatles, Led Zeppelin, Rod Stewart, Pink Floyd, John Lennon, Phil Collins, Radiohead, Coldplay, Susan Boyle and Adele. In addition, all of their studio albums have reached the UK Top 10 and their albums have spent over 210 weeks on the UK Charts.
Music critic Sasha Frere-Jones claimed that "the last serious English influence was Depeche Mode, who seem more and more significant as time passes." Depeche Mode have been nominated for five Grammy Awards; Devotional for Grammy Award for Best Long Form Music Video, I Feel Loved and Suffer Well, both for Best Dance Recording, Sounds of the Universe for Best Alternative Album and Wrong for Best Short Form Music Video. In addition, Depeche Mode have been honoured with a Brit Award for Enjoy The Silence in the Best British Single category, the first ever Q Magazine Innovation Award and an Ivor Novello Award for Martin Gore in the category of International Achievement.
Depeche Mode are frequently praised by the music press; they became "The most popular electronic band the world has ever known" according to "Q" magazine, "One of the greatest British pop groups of all time" according to the "Sunday Telegraph", and "The quintessential eighties techno-pop band" according to "Rolling Stone" magazine and "MTV". Depeche Mode were ranked #2 on Electronic Music Realm's list of The 100 Greatest Artists of Electronic Music, #3 in a list of The Most Influential Electronic Music Pioneers by Ranker, the band were also ranked #144 on Acclaimed Music's list of Top 1000 Artists of All Time and Q Magazine included them on their list of 50 artists who changed the world.
Discography.
Studio albums

</doc>
<doc id="8536" url="http://en.wikipedia.org/wiki?curid=8536" title="Differential cryptanalysis">
Differential cryptanalysis

Differential cryptanalysis is a general form of cryptanalysis applicable primarily to block ciphers, but also to stream ciphers and cryptographic hash functions. In the broadest sense, it is the study of how differences in information input can affect the resultant difference at the output. In the case of a block cipher, it refers to a set of techniques for tracing differences through the network of transformations, discovering where the cipher exhibits non-random behavior, and exploiting such properties to recover the secret key.
History.
The discovery of differential cryptanalysis is generally attributed to Eli Biham and Adi Shamir in the late 1980s, who published a number of attacks against various block ciphers and hash functions, including a theoretical weakness in the Data Encryption Standard (DES). It was noted by Biham and Shamir that DES is surprisingly resistant to differential cryptanalysis but small modifications to the algorithm would make it much more susceptible.
In 1994, a member of the original IBM DES team, Don Coppersmith, published a paper stating that differential cryptanalysis was known to IBM as early as 1974, and that defending against differential cryptanalysis had been a design goal.
According to author Steven Levy, IBM had discovered differential cryptanalysis on its own, and the NSA was apparently well aware of the technique.
IBM kept some secrets, as Coppersmith explains: "After discussions with NSA, it was decided that disclosure of the design considerations would reveal the technique of differential cryptanalysis, a powerful technique that could be used against many ciphers. This in turn would weaken the competitive advantage the United States enjoyed over other countries in the field of cryptography."
Within IBM, differential cryptanalysis was known as the "T-attack" or "Tickle attack".
While DES was designed with resistance to differential cryptanalysis in mind, other contemporary ciphers proved to be vulnerable. An early target for the attack was the FEAL block cipher. The original proposed version with four rounds (FEAL-4) can be broken using only eight chosen plaintexts, and even a 31-round version of FEAL is susceptible to the attack. In contrast, the scheme can successfully cryptanalyze DES with an effort on the order 247 chosen plaintexts.
Attack mechanics.
Differential cryptanalysis is usually a chosen plaintext attack, meaning that the attacker must be able to obtain ciphertexts for some set of plaintexts of his choosing. There are, however, extensions that would allow a known plaintext or even a ciphertext-only attack. The basic method uses pairs of plaintext related by a constant "difference"; difference can be defined in several ways, but the eXclusive OR (XOR) operation is usual. The attacker then computes the differences of the corresponding ciphertexts, hoping to detect statistical patterns in their distribution. The resulting pair of differences is called a differential. Their statistical properties depend upon the nature of the S-boxes used for encryption, so the attacker analyses differentials (Δ"X", Δ"Y"), where Δ"Y" = "S"("X" ⊕ Δ"X") ⊕ "S"("X") (and ⊕ denotes exclusive or) for each such S-box "S". In the basic attack, one particular ciphertext difference is expected to be especially frequent; in this way, the cipher can be distinguished from random. More sophisticated variations allow the key to be recovered faster than exhaustive search.
In the most basic form of key recovery through differential cryptanalysis, an attacker requests the ciphertexts for a large number of plaintext pairs, then assumes that the differential holds for at least "r" − 1 rounds, where "r" is the total number of rounds. The attacker then deduces which round keys (for the final round) are possible, assuming the difference between the blocks before the final round is fixed. When round keys are short, this can be achieved by simply exhaustively decrypting the ciphertext pairs one round with each possible round key. When one round key has been deemed a potential round key considerably more often than any other key, it is assumed to be the correct round key.
For any particular cipher, the input difference must be carefully selected for the attack to be successful. An analysis of the algorithm's internals is undertaken; the standard method is to trace a path of highly probable differences through the various stages of encryption, termed a "differential characteristic".
Since differential cryptanalysis became public knowledge, it has become a basic concern of cipher designers. New designs are expected to be accompanied by evidence that the algorithm is resistant to this attack, and many, including the Advanced Encryption Standard, have been proven secure against the attack.
Attack in detail.
The attack relies primarily on the fact that a given input/output difference pattern only occurs for certain values of inputs. Usually the attack is applied in essence to the non-linear components as if they were a solid component (usually they are in fact look-up tables or "sboxes"). Observing the desired output difference (between two chosen or known plaintext inputs) "suggests" possible key values.
For example, if a differential of 1 => 1 (implying a difference in the least significant bit (LSB) of the input leads to an output difference in the LSB) occurs with probability of 4/256 (possible with the non-linear function in the AES cipher for instance) then for only 4 values (or 2 pairs) of inputs is that differential possible. Suppose we have a non-linear function where the key is XOR'ed before evaluation and the values that allow the differential are {2,3} and {4,5}. If the attacker sends in the values of {6, 7} and observes the correct output difference it means the key is either 6 xor K = 2 or 6 xor K = 4, meaning the key is either K = {2,4}.
In essence, for an n-bit non-linear function one would ideally seek as close to 2-(n-1) as possible to achieve "differential uniformity". When this happens, the differential attack requires as much work to determine the key as simply brute forcing the key.
The AES non-linear function has a maximum differential probability of 4/256 (most entries however are either 0 or 2). Meaning that in theory one could determine the key with half as much work as brute force, however, the high branch of AES prevents any high probability trails from existing over multiple rounds. In fact, the AES cipher would be just as immune to differential and linear attacks with a much "weaker" non-linear function. The incredibly high branch (active sbox count) of 25 over 4R means that over 8 rounds no attack involves fewer than 50 non-linear transforms, meaning that the probability of success does not exceed Pr[attack] ≤ Pr[best attack on sbox]50. For example, with the current sbox AES emits no fixed differential with a probability higher than (4/256)50 or 2−300 which is far lower than the required threshold of 2−128 for a 128-bit block cipher. This would have allowed room for a more efficient sbox, even if it is 16-uniform the probability of attack would have still been 2−200.
There exist no bijections for even sized inputs/outputs with 2-uniformity. They exist in odd fields (such as GF(27)) using either cubing or inversion (there are other exponents that can be used as well). For instance S(x) = x3 in any odd binary field is immune to differential and linear cryptanalysis. This is in part why the MISTY designs use 7- and 9-bit functions in the 16-bit non-linear function. What these functions gain in immunity to differential and linear attacks they lose to algebraic attacks. That is, they are possible to describe and solve via a SAT solver. This is in part why AES (for instance) has an affine mapping after the inversion.

</doc>
<doc id="8537" url="http://en.wikipedia.org/wiki?curid=8537" title="Document type definition">
Document type definition

A document type definition (DTD) is a set of "markup declarations" that define a "document type" for an SGML-family markup language (SGML, XML, HTML).
A Document Type Definition (DTD) defines the legal building blocks of an XML document. It defines the document structure with a list of legal elements and attributes. A DTD can be declared inline inside an XML document, or as an external reference. 
XML uses a subset of SGML DTD.
, newer XML namespace-aware schema languages (such as W3C XML Schema and ISO RELAX NG) have largely superseded DTDs. A namespace-aware version of DTDs is being developed as Part 9 of ISO DSDL. DTDs persist in applications that need special publishing characters, such as the XML and HTML Character Entity References, which derive from larger sets defined as part of the ISO SGML standard effort.
Associating DTDs with documents.
A DTD is associated with an XML or SGML document by means of a document type declaration (DOCTYPE). The DOCTYPE appears in the syntactic fragment "doctypedecl" near the start of an XML document. The declaration establishes that the document is an instance of the type defined by the referenced DTD.
DTDs make two sorts of declaration:
The declarations in the internal subset form part of the DOCTYPE in the document itself. The declarations in the external subset are located in a separate text file. The external subset may be referenced via a "public identifier" and/or a "system identifier". Programs for reading documents may not be required to read the external subset.
Note that any valid SGML or XML document that references an "external subset" in its DTD, or whose body contains references to "parsed external entities" declared in its DTD (including those declared within its "internal subset"), may only be partially parsed but cannot be fully validated by "validating" SGML or XML parsers in their "standalone" mode (this means that these validating parsers don't attempt to retrieve these external entities, and their replacement text is not accessible).
However, such documents are still fully parsable in the "non"-standalone mode of validating parsers, which signals an error if it can't locate these external entities with their specified "public identifier" (FPI) or "system identifier" (a URI), or are inaccessible. (Notations declared in the DTD are also referencing external entities, but these unparsed entities are not needed for the validation of documents in the "standalone" mode of these parsers: the validation of all external entities referenced by notations is left to the application using the SGML or XML parser). Non-validating parsers "may" eventually attempt to locate these external entities in the "non"-standalone mode (by partially interpreting the DTD only to resolve their declared parsable entities), but do not validate the content model of these documents.
Examples.
The following example of a DOCTYPE contains both public and system identifiers:
All HTML 4.01 documents conform to one of three SGML DTDs. The public identifiers of these DTDs are constant and are as follows:
The system identifiers of these DTDs, if present in the DOCTYPE, are URI references. A system identifier usually points to a specific set of declarations in a resolvable location. SGML allows mapping public identifiers to system identifiers in catalogs that are optionally available to the URI resolvers used by document parsing software.
Note that this DOCTYPE can only appear "after" the optional XML declaration, and before the document body, if the document syntax conforms to XML. This includes XHTML documents:
An additional internal subset can also be provided after the external subset:
Alternatively, only the internal subset may be provided:
Finally, the document type definition may include no subset at all; in that case, it just specifies that the document has a single top-level element (this is an implicit requirement for all valid XML and HTML documents, but not for document fragments or for all SGML documents, whose top-level elements may be different from the implied root element), and it indicates the type name of the root element:
Markup declarations.
DTDs describe the structure of a class of documents via element and attribute-list declarations. Element declarations name the allowable set of elements within the document, and specify whether and how declared elements and runs of character data may be contained within each element. Attribute-list declarations name the allowable set of attributes for each declared element, including the type of each attribute value, if not an explicit set of valid value(s).
DTD markup declarations declare which element types, attribute lists, entities, and notations are allowed in the structure of the corresponding class of XML documents.
Element type declarations.
An element type declaration defines an element and its possible content. A valid XML document contains only elements that are defined in the DTD.
Various keywords and characters specify an element’s content:
For example:
Note that element type declarations are ignored by "non-validating" SGML and XML parsers (in which cases, any elements are accepted in any order, and in any number of occurrences in the parsed document), but these declarations are still checked for form and validity.
Attribute list declarations.
An attribute list specifies for a given element type the list of all possible attribute associated with that type. For each possible attribute, it contains:
For example:
Here are some attribute types supported by both SGML and XML:
A default value can define whether an attribute must occur (#REQUIRED) or not (#IMPLIED), or whether it has a fixed value (#FIXED), or which value should be used as a default value ("…") in case the given attribute is left out in an XML tag.
Note that attribute list declarations are ignored by "non-validating" SGML and XML parsers (in which cases any attribute is accepted within all elements of the parsed document), but these declarations are still checked for well-formedness and validity.
Entity declarations.
An entity is similar to a macro. The entity declaration assigns it a value that is retained throughout the document. A common use is to have a name more recognizable than a numeric character reference for an unfamiliar character. Entities help to improve legibility of an XML text. In general, there are two types: internal and external.
An example of internal entity declarations (here in an internal DTD subset of an SGML document) is:
Note that internal entities may be defined in any order, as long as they are not referenced and parsed in the DTD or in the body of the document, in their order of parsing: it is valid to include a reference to a still undefined entity within the content of a parsed entity, but it is invalid to include anywhere else any named entity reference before this entity has been fully defined, including all other internal entities referenced in its defined content (this also prevents circular or recursive definitions of internal entities). This document is parsed as if it was:
Note that reference to the "author" internal entity is not substituted in the replacement text of the "signature" internal entity. Instead, it is replaced only when the "signature" entity reference is parsed within the content of the "sgml" element, but only by validating parsers (non-validating parsers do not substitute entity references occurring within contents of element or within attribute values, in the body of the document.
This is possible because the replacement text specified in the internal entity definitions permits a distinction between parameter entity references (that are introduced by the "%" character and whose replacement applies to the parsed DTD contents) and general entity references (that are introduced by the "&" character and whose replacement is delayed until they are effectively parsed and validated). The "%" character for introducing parameter entity references in the DTD loses its special role outside of the DTD and it becomes a literal character.
However, the references to predefined numeric character entities are substituted wherever they occur, without needing a validating parser (they are only introduced by the "&" character).
Notation declarations.
Notations are used in SGML or XML. They provide a complete reference to unparsed external entities whose interpretation is left to the application (which interprets them directly or retrieves the external entity themselves), by assigning them a simple name, which is usable in the body of the document. For example, notations may be used to reference non-XML data in an XML 1.1 document. For example, to annotate SVG images to associate them with a specific renderer:
This declares the MIME type of external images with this type, and associates it with a notation name "type-image-svg". However, notation names usually follow a naming convention that is specific to the application generating or using the notation: notations are interpreted as additional meta-data whose effective content is an external entity and either a PUBLIC FPI, registered in the catalogs used by XML or SGML parsers, or a SYSTEM URI, whose interpretation is application dependent (here a MIME type, interpreted as a relative URI, but it could be an absolute URI to a specific renderer, or a URN indicating an OS-specific object identifier such as a UUID).
The declared notation name must be unique within all the document type declaration, i.e. in the external subset as well as the internal subset, at least for conformance with XML.
Notations can be associated to unparsed external entities included in the body of the SGML or XML document. The PUBLIC or SYSTEM parameter of these external entities specifies the FPI and/or the URI where the unparsed data of the external entity is located, and the additional NDATA parameter of these defined entities specifies the additional notation (i.e., effectively the MIME type here). For example:
Within the body of the SGML document, these referenced external entities (whose name is specified between "&" and ";") are "not" replaced like usual named entities (defined with a CDATA value), but are left as distinct unparsed tokens that may be used either as the value of an element attribute (like above) or within the element contents, provided that either the DTD allows such external entities in the declared content type of elements or in the declared type of attributes (here the ENTITY type for the data attribute), or the SGML parser is not validating the content.
Notations may also be associated directly to elements as additional meta-data, without associating them to another external entity, by giving their names as possible values of some additional attributes (also declared in the DTD within the <!ATTLIST ...> declaration of the element). For example:
The example above shows a notation named "type-image-svg" that references the standard public FPI and the system identifier (the standard URI) of an SVG 1.1 document, instead of specifying just a system identifier as in the first example (which was a relative URI interpreted locally as a MIME type). This annotation is referenced directly within the unparsed "type" attribute of the "img" element, but its content is not retrieved. It also declares another notation for a vendor-specific application, to annotate the "sgml" root element in the document. In both cases, the declared notation named is used directly in a declared "type" attribute, whose content is specified in the DTD with the "NOTATION" attribute type (this "type" attribute is declared for the "sgml" element, as well as for the "img" element).
However, the "title" attribute of the "img" element specifies the internal entity "example1SVGTitle" whose declaration that does not define an annotation, so it is parsed by validating parsers and the entity replacement text is "Title of example1.svg".
The content of the "img" element references another external entity "example1SVG" whose declaration also does not define an notation, so it is also parsed by validating parsers and the entity replacement text is located by its defined SYSTEM identifier "example1.svg" (also interpreted as a relative URI). The effective content for the "img" element be the content of this second external resource. The difference with the GIF image, is that the SVG image is parsed within the SGML document, according to the declarations in the DTD, where the GIF image is just referenced as an opaque external object (which is not parsable with SGML) via its "data" attribute (whose value type is an opaque ENTITY).
Only one notation name may be specified in the value of ENTITY attributes (there's no support in SGML, XML 1.0 or XML 1.1 for multiple notation names in the same declared external ENTITY, so separate attributes are needed). However multiple external entities may be referenced (in a space-separated list of names) in attributes declared with type ENTITIES, and where each named external entity is also declared with its own notation).
Notations are also completely opaque for XML and SGML parsers, so they are not differentiated by the type of the external entity that they may reference (for these parsers they just have a unique name associated to a public identifier (an FPI) and/or a system identifier (a URI)).
Some applications (but not XML or SGML parsers themselves) also allow referencing notations indirectly by naming them in the "URN:"name"" value of a standard CDATA attribute, everywhere a URI can be specified. However this behaviour is application-specific, and requires that the application maintains a catalog of known URNs to resolve them into the notations that have been parsed in a standard SGML or XML parser. This use allows notations to be defined only in a DTD stored as an external entity and referenced only as the external subset of documents, and allows these documents to remain compatible with validating XML or SGML parsers that have no direct support for notations.
Notations are not used in HTML, or in basic profiles for XHTML and SVG, because:
Note also that even in validating SGML or XML 1.0 or XML 1.1 parsers, the external entities referenced by an FPI and/or URI in declared notations are not retrieved automatically by the parsers themselves. Instead, these parsers just provide to the application the parsed FPI and/or URI associated to the notations found in the parsed SGML or XML document, and with a facility for a dictionary containing all notation names declared in the DTD; these validating parsers also check the uniqueness of notation name declarations, and report a validation error if some notation names are used anywhere in the DTD or in the document body but not declared:
XML DTDs and schema validation.
The XML DTD syntax is one of several XML schema languages. However, many of the schema languages do not fully replace the XML DTD. Notably, the XML DTD allows defining entities and notations that have no direct equivalents in DTD-less XML (because internal entities and parsable external entities are not part of XML schema languages, and because other unparsed external entities and notations have no simple equivalent mappings in most XML schema languages).
Most XML schema languages are only replacements for element declarations and attribute list declarations, in such a way that it becomes possible to parse XML documents with "non-validating" XML parsers (if the only purpose of the external DTD subset was to define the schema). In addition, documents for these XML schema languages must be parsed separately, so validating the schema of XML documents in pure standalone mode is not really possible with these languages: the document type declaration remains necessary for at least identifying (with a XML Catalog) the schema used in the parsed XML document and that is validated in another language.
A common misconception holds that a "non-validating" XML parser does not have to read document type declarations, when in fact, the document type declarations must still be scanned for correct syntax as well as validity of declarations, and the parser must still parse all entity declarations in the "internal subset", and substitute the replacement texts of internal entities occurring anywhere in the document type declaration or in the document body.
A "non-validating" parser may, however, elect not to read parsable "external entities" (including the "external subset"), and does not have to honor the content model restrictions defined in element declarations and in attribute list declarations.
If the XML document depends on parsable external entities (including the specified "external subset", or parsable external entities declared in the "internal subset"), it should assert codice_4 in its XML declaration. The validating DTD may be identified by using XML Catalogs to retrieve its specified "external subset".
In the example below, the XML document is declared with codice_4 because it has an external subset in its document type declaration:
If the XML document type declaration includes any SYSTEM identifier for the external subset, it can't be safely processed as standalone: the URI should be retrieved, otherwise there may be unknown named character entities whose definition may be needed to correctly parse the effective XML syntax in the internal subset or in the document body (the XML syntax parsing is normally performed "after" the substitution of all named entities, excluding the five entities that are predefined in XML and that are implicitly substituted "after" parsing the XML document into lexical tokens). If it just includes any PUBLIC identifier, it "may" be processed as standalone, if the XML processor knows this PUBLIC identifier in its local catalog from where it can retrieve an associated DTD entity.
XML DTD schema example.
An example of a very simple external XML DTD to describe the schema of a list of persons might consist of:
Taking this line by line:
An example of an XML file that uses and conforms to this DTD follows. The DTD is referenced here as an external subset, via the SYSTEM specifier and a URI. It assumes that we can identify the DTD with the relative URI reference "example.dtd"; the "people_list" after "!DOCTYPE" tells us that the root tags, or the first element defined in the DTD, is called "people_list":
One can render this in an XML-enabled browser (such as Internet Explorer or Mozilla Firefox) by pasting and saving the DTD component above to a text file named "example.dtd" and the XML file to a differently-named text file, and opening the XML file with the browser. The files should both be saved in the same directory. However, many browsers do not check that an XML document conforms to the rules in the DTD; they are only required to check that the DTD is syntactically correct. For security reasons, they may also choose not to read the external DTD.
The same DTD can also be embedded directly in the XML document itself as an internal subset, by encasing it within [square brackets] in the document type declaration, in which case the document no longer depends on external entities and can be processed in standalone mode:
Alternatives to DTDs (for specifying schemas) are available:
Security.
An XML DTD can be used to create a denial of service (DoS) attack by defining nested entities that expand exponentially, or by sending the XML parser to an external resource that never returns.
For this reason, .NET Framework provides a property that allows prohibiting or skipping DTD parsing, and recent versions of Microsoft Office applications (Microsoft Office 2010 and higher) refuse to open XML files that contain DTD declarations.

</doc>
<doc id="8539" url="http://en.wikipedia.org/wiki?curid=8539" title="Devil">
Devil

The Devil (from Greek: διάβολος or "diábolos" = slanderer or accuser) is believed in many religions, myths and cultures to be a supernatural entity that is the personification of evil and the enemy of God and humankind. The nature of the role varies greatly, ranging from being an effective opposite force to the creator god, locked in an eons long struggle for human souls on what may seem even terms (to the point of dualistic ditheism/bitheism), to being a comical figure of fun or an abstract aspect of the individual human condition.
While mainstream Judaism contains no overt concept of a devil, Christianity and Islam have variously regarded the Devil as a rebellious fallen angel or jinn that tempts humans to sin, if not committing evil deeds himself. In these religions – particularly during periods of division or external threat – the Devil has assumed more of a dualistic status commonly associated with heretics, infidels, and other unbelievers. As such, the Devil is seen as an allegory that represents a crisis of faith, individualism, free will, wisdom and enlightenment.
In mainstream Islam and Christianity, God and the Devil are usually portrayed as fighting over the souls of humans. The Devil rules Hell, where he and his demons punish the damned. The Devil commands a force of evil spirits, commonly known as demons. The Hebrew Bible (or Old Testament) describes the Adversary (ha-satan) as an angel who instigates tests upon humankind. Many other religions have a trickster or tempter figure that is similar to the Devil. Modern conceptions of the Devil include the concept that it symbolizes humans' own lower nature or sinfulness.
Etymology.
Devil descends from the Middle English devel, from Old English dēofol, that in turn represents an early Germanic borrowing of Latin "diabolus". This in turn was borrowed from Ancient Greek "diábolos" (διάβολος), "slanderer", from diaballein "to slander": dia- "across, through" + ballein "to hurl". In the New Testament, "Satan" occurs more than 30 times in passages alongside "diábolos", referring to the same person or thing as Satan.
Abrahamic religions.
Judaism.
In mainstream Judaism there is no concept of a devil like in mainstream Christianity or Islam. Texts make no direct link between the serpent that tempts Eve in the Garden of Eden from Genesis and references to a Satan in the first book of Chronicles and in Job. In Hebrew, the biblical word "ha-satan" (השָׂטָן) means "the adversary" or "the obstacle", or even "the prosecutor" (recognizing that God is viewed as the ultimate Judge). As much as the Devil exists in any form of Judaism, his role is as an adversary and an accuser which is assigned rather than assumed.
For the Hasidim of the eighteenth century, ha-satan was "Baal Davar".
Apocrypha/Deuterocanon.
In the Book of Wisdom, the devil is represented as the one who brought death into the world. The Second Book of Enoch contains references to a Watcher angel called Satanael, describing him as the prince of the "Grigori" who was cast out of heaven and an evil spirit who knew the difference between what was "righteous" and "sinful". A similar story is found in 1 Enoch; however, in that book, the leader of the Grigori is called Semjâzâ. In the apocryphal literature, Satan rules over a host of angels. Mastema, who induced God to test Abraham through the sacrifice of Isaac, is identical with Satan in both name and nature. The Book of Enoch contains references to Sathariel, thought also to be Sataniel and Satan'el. The similar spellings mirror that of his angelic brethren Michael, Raphael, Uriel and Gabriel, previous to his expulsion from Heaven.
Christianity.
In mainstream Christianity the Devil is known as Satan and sometimes as Lucifer, although it has been noted that the reference in Isaiah 14:12 to Lucifer, or the Son of the Morning, is a reference to the Babylonian king. Some modern Christians consider the Devil to be an angel who, along with one-third of the angelic host (the demons) rebelled against God and has consequently been condemned to the Lake of Fire. He is described as hating all humanity (or more accurately creation), opposing God, spreading lies and wreaking havoc on the souls of mankind. Other Christians consider the devil in the Bible to refer figuratively to human sin and temptation and to any human system in opposition to God.
Satan is often identified as the serpent who convinced Eve to eat the forbidden fruit; thus, Satan has often been depicted as a serpent. Though this identification is not present in the Adam and Eve narrative, this interpretation goes back at least as far as the time of the writing of the book of Revelation, which specifically identifies Satan as being the serpent (Rev. 20:2).
In the Bible, the devil is identified with "the dragon" and "the old serpent" in the Book of Revelation 12:9, 20:2 have also been identified with Satan, as have "the prince of this world" in the Book of John 12:31, 14:30; "the prince of the power of the air" also called Meririm, and "the spirit that now worketh in the children of disobedience" in the Book of Ephesians 2:2; and "the god of this world" in 2 Corinthians 4:4. He is also identified as the dragon in the Book of Revelation (e.g.), and the tempter of the Gospels (e.g.).
Beelzebub is originally the name of a Philistine god (more specifically a certain type of Baal, from "Ba‘al Zebûb", lit. "Lord of Flies") but is also used in the New Testament as a synonym for Satan. A corrupted version, "Belzeboub", appears in The Divine Comedy.
In other, non-mainstream, Christian beliefs (e.g. the beliefs of the Christadelphians) the word "satan" in the Bible is not regarded as referring to a supernatural, personal being but to any 'adversary' and figuratively refers to human sin and temptation.
Islam.
In Islam the Devil is referred to as Iblis or sometimes the Shaytan (Arabic: Like the usage of the word "satan" in the Hebrew Bible, "Shaytan" is also a word used to refer to beings called "demons" in the Christian Bible, especially the New Testament). According to the Qur'an, God created Iblis, along with all of the other jinn, out of "smokeless fire". The primary characteristic of the Devil, besides hubris, is that he has no power other than the power to cast evil suggestions into the hearts of men and women.
According to Muslim theology, Iblis was expelled from the grace of God when he disobeyed God by choosing not to pay homage to Adam, the father of all mankind. He claimed to be superior to Adam, on the grounds that man was created of earth unlike himself. As for the angels, they prostrated before Adam to show their homage and obedience to God. However, Iblis, adamant in his view that man is inferior, and unlike angels was given the ability to choose, made a choice of not obeying God. This caused him to be expelled by God, a fact that Iblis blamed on humanity. Initially, the Devil was successful in deceiving Adam, but once his intentions became clear, Adam and Eve repented to God and were freed from their misdeeds and forgiven. God gave them a strong warning about Iblis and the fires of Hell and asked them and their children (humankind) to stay away from the deceptions of their senses caused by the Devil.
According to the verses of the Qur’an, the Devil's mission until the Qiyamah or Resurrection Day ("yaum-ul-qiyama") is to deceive Adam's children (mankind). After that, he will be put into the fires of Hell along with those whom he has deceived. The Devil is also referred to as one of the jinn, as they are all created from the smokeless fire. The Qur'an does not depict Iblis as the enemy of God, as God is supreme over all his creations and Iblis is just one of his creations. Iblis's single enemy is humanity. He intends to discourage humans from obeying God. Thus, humankind is warned to struggle ("jihad") against the mischiefs of Satan and temptations he puts them in. The ones who succeed in this are rewarded with Paradise ("jannath ul firdaus"), attainable only by righteous conduct.
Bahá'í Faith.
In the Bahá'í Faith, a malevolent, superhuman entity such as a "devil" or "satan" is not believed to exist. These terms do, however, appear in the Bahá'í writings, where they are used as metaphors for the base nature of man. Human beings are seen to have free will, and are thus able to turn towards God and develop spiritual qualities or turn away from God and become immersed in their self-centered desires. Individuals who follow the temptations of the self and do not develop spiritual virtues are often described in the Bahá'í writings with the word "satanic". The Bahá'í writings also state that the devil is a metaphor for the "insistent self" or "lower self" which is a self-serving inclination within each individual. Those who follow their lower nature are also described as followers of "the Evil One".
Yazidism.
An alternate name for the main deity in the tentatively Indo-European pantheon of the Yazidi, Melek Taus, is Shaitan. Rather than Satanic, however, Yazidism is better understood as a remnant of a pre-Islamic Middle Eastern religion, and/or a "ghulat" Sufi movement founded by Sheikh Adi ibn Musafir. The connection with Satan, originally made by Muslim outsiders, attracted the interest of 19th-century European travelers and esoteric writers.
Other religions.
Neopaganism.
Christian tradition has frequently identified pagan religions and witchcraft with the influence of Satan. In the Early Modern Period, the Church accused alleged witches of consorting and conspiring with Satan. Several modern conservative Christian writers, such as Jack Chick and James Dobson, have depicted today's neopagan and witchcraft religions as explicitly Satanic.
Few neopagan reconstructionist traditions recognize Satan or the Devil outright. However, many neopagan groups worship some sort of Horned God, for example as a consort of the Great Goddess in Wicca. These gods usually reflect mythological figures such as Cernunnos or Pan, and any similarity they may have to the Christian Devil seems to date back only to the 19th century, when a Christian reaction to Pan's growing importance in literature and art resulted in his image being translated to that of the Devil.
New Age movement.
Participants in the New Age movement have widely varied views about Satan, the Devil, and so forth. In some forms of Esoteric Christianity Satan remains as a being of evil, or at least a metaphor for sin and materialism, but the most widespread tendency is to deny his existence altogether. Lucifer, on the other hand, in the original Roman sense of "light-bringer", occasionally appears in the literature of certain groups as a metaphorical figure quite distinct from Satan, and without any implications of evil. For example, Theosophy founder Madame Blavatsky named her journal "Lucifer" since she intended it to be a "bringer of light". Many New Age schools of thought follow a nondualistic philosophy that does not recognize a primal force for evil.
Even when a dualistic model is followed, this is more often akin to the Chinese system of yin and yang, in which good and evil are explicitly not a complementary duality. Schools of thought that do stress a spiritual war between good and evil or light and darkness include the philosophy of Rudolf Steiner, Agni Yoga, and the Church Universal and Triumphant.
Satanism.
Some religions worship the Devil. This can be in a polytheistic sense where "God", Satan, and others are all deities with Satan as the preferred patron; or it can be from a more monotheistic viewpoint, where God is regarded as a true god, but is nevertheless defied.
Some variants deny the existence of God and the Devil altogether, but still call themselves Satanists, such as Anton LaVey's Church Of Satan which sees Satan as a representation of the primal and natural state of mankind.
Much "Satanic" lore does not originate from actual Satanists, but from Christians. Best-known would be the medieval folklore and theology surrounding demons and witches. A more recent example is the Satanic ritual abuse scare of the 1980s – beginning with the memoir "Michelle Remembers" – which depicts Satanism as a vast (and unsubstantiated) conspiracy of elites with a predilection for child abuse and human sacrifice. This genre regularly describes Satan as actually appearing in person in order to receive worship.
Zoroastrianism.
In the Gathas, the oldest texts of the Zoroastrian Avesta, believed to have been composed by Zoroaster himself, the poet does not mention a manifest adversary. Ahura Mazda's Creation is "truth", "asha". The "lie" ("druj") is manifest only as decay or chaos, not an entity.
Later, in Zurvanism (Zurvanite Zoroastrianism), Ahura Mazda and the principle of evil, Angra Mainyu, are the "twin" offspring of Zurvan, 'Time'. No trace of Zurvanism exists after the 10th century.
Today, the Parsis of India largely accept the 19th century interpretation that Angra Mainyu is the 'Destructive Emanation' of Ahura Mazda. Instead of struggling against Mazda himself, Angra Mainyu battles Spenta Mainyu, Mazda's 'Creative Emanation.'
Hinduism.
In contrast to Christianity and Islam, Hinduism does not recognize any central evil force or entity such as the Devil opposing God and man. Hinduism does recognize that different beings (e.g., asuras) and entities can perform evil acts, under the temporary dominance of the guna of "tamas", and cause worldly sufferings. The Rajasic and Tamasic Gunas of Maya are considered especially close to the Abrahamic concept, the hellish parts of the Ultimate Delusion called "Prakriti". An embodiment of this is the concept of Advaita (non-dualism) where there is no good or evil but simply different levels of realization.
On the other hand in Hinduism, which provides plenty of room for counterpoint, there is also the notion of dvaita (dualism) where there is interplay between good and evil tendencies. A prominent asura is Rahu whose characteristics are similar to those of the Devil. However, Hindus, and Vaishnavites in particular, believe that an avatar of Vishnu incarnates to defeat evil when evil reaches its greatest strength. The concept of Guna and Karma also explain evil to a degree, rather than the influence of a devil.
To be more specific, Hindu philosophy defines that the only existing thing (Truth) is the Almighty God. So, all the asuric tendencies are inferior and mostly exist as illusions in the mind. Asuras are also different people in whom bad motivations and intentions (tamas) have temporarily outweighed the good ones (Sattva). Different beings like "siddha", "gandharva", "yaksha" etc. are considered beings unlike mankind, and in some ways superior to men.
In Ayyavazhi, officially an offshoot of Hinduism prominent in Tamil Nadu (a southern state in India with Dravidian heritage), followers, unlike most other branches of Hinduism, believes in a Satan-like figure, Kroni. Kroni, according to Ayyavazhi is the primordial manifestation of evil and manifests in various forms of evil, i.e., Ravana, Duryodhana, etc., in different ages or yugas. In response to such manifestation of evil, believers, in Ayya-Vazhi religion believe that God, as Vishnu manifests in His Avatars such as Rama and Krishna to defeat evil. Eventually, the Ekam with the spirit (the spirit taken by Narayana only for incarnating in the world) of Narayana incarnates in the world as Ayya Vaikundar to destroy the final manifestaion of Kroni, Kaliyan.
Kroni, the spirit of Kali Yuga is said to be omnipresent in this age and that is one reason followers of Ayya Vazhi, like most Hindus, believe that the current yuga, Kali Yuga is so degraded.
Buddhism.
A devil-like figure in Buddhism is Mara. He is a tempter, who also tempted Gautama Buddha by trying to seduce him with the vision of beautiful women who, in various legends, are often said to be Mara's daughters. Mara personifies unskillfulness, the "death" of the spiritual life. He tries to distract humans from practicing the spiritual life by making the mundane alluring or the negative seem positive. Another interpretation of Mara is that he is the desires that are present in one's own mind preventing the person from seeing the truth. So in a sense Mara is not an independent being but a part of one's own being that has to be defeated.
In daily life of the Buddha the role of devil has been given to Devadatta.
Ancient Egypt.
In the Ausarian drama we find that Ausar (Greek: Osiris) is chopped into 13 pieces by Set. Auset (Isis) collects all of his pieces save his phallus. Horus, son of Ausar and Auset sets out to avenge the death and dismemberment of his father by confronting Set. Horus is victorious over Set and Ausar, being brought back from the dead becomes lord of the underworld. It is this drama that gives us the cosmic conflict between good and evil, evil being embodied by Set. This is not to say that Set was always seen as an evil character in Ancient Egyptian theology. There are many times in Ancient Egyptian history where conflicts between different "houses" lead to the depreciation of one god relative to another.
As in most polytheistic faiths, the characters involved differentiate themselves from the Western tradition of a devil in that all the gods are closely related. In this case, numerous historic texts suggest that Set is the Uncle or Brother of Horus and in the "defeat" of Set, we see another separation from the norm in the devouring/assimilation of Set into Horus with the result of Horus having depictions of both the falcon head and the (unknown animal) head of Set. This (like Buddhism) represents a dissolution of dichotomy.
World folklore.
In the Western Christian tradition, the Devil has entered popular folklore, particularly in his role as a trickster figure. As such, he is found as a character in a wide number of traditional folktales and legends from Ireland, Newfoundland, Italy and the United Kingdom, where he often attempts to trick or outwit other characters. In some of these tales, the Devil is portrayed as more of a folk villain than as the personification of evil. The Devil also features prominently in a number of hagiographical tales, or tales of the saints such as the popular tale of St. Dunstan, many of which may fall outside the authorized religious canon. The Devil is also a recurring feature in tales explaining the etymology of geographical names, lending his name to natural formations such as The Devil's Chimney.
David Ferriero, Archivist of the United States, claims to have only one piece of correspondence with the Devil in the nation's vast and varied collections. A letter sent from Baltimore at the end of the American Civil War to Confederate leader Jefferson Davis bemoans the rebellion against the United States and is signed by "the Devil".
A series of video games was created "in honor" of the devil, by Blizzard North company. "Diablo" is an action role-playing series of games, in which Diablo terrorizes the world and the hero gets rid of him, again and again.
Bob Dylan refers to the Satan in his song "Man of peace", from his album Infidels. Dylan claims that the Satan can be wearing even the most pleasant disguise and can be found in everyone, even the least expected.
Other names.
Demons.
In some religions and traditions, these titles are separate demons; others identify these names as guises of The Devil. Even when thought of as individual demons, some are often thought of being under the Devil's direct control. This identifies only those thought of as the Devil; List of demons has a more general listing.
Titles.
These are titles that almost always refer to the Devil.
A list of liturgical names for the Devil may be found in Jeffrey Burton Russell, "Lucifer, the Devil in the Middle Ages" (Cornell University Press, 1986), p. 128, note 76 
God as the Devil.
Several religious authors throughout history have advanced the notion that the god of the Bible is consistent in character with the devil. They make the case that the Biblical God is a divine force that wreaks suffering, death and destruction and that tempts or commands humanity into committing mayhem and genocide. Tertullian accuses Marcion of Sinope, the first great heretic of Christianity in the 1st century, that he The Church condemned his writings as heretical. John Arendzen (1909) in the Catholic Encyclopedia (1913) mentions that Eusebius accused Apelles, the 2nd-century AD Gnostic, of considering the Inspirer of Old-Testament prophecies to be not a god, but an evil angel. Hegemonius (4th century) accuses the Persian prophet Mani, founder of the Manichaean sect in the 3rd century AD, identified Jehovah as "the devil god which created the world" and said that "he who spoke with Moses, the Jews, and the priests  is the [Prince] of Darkness,  not the god of truth."
These writings refer to the Abrahamic God variously as "a demiurgus", "an evil angel", "the devil god", "the Prince of Darkness", "the source of all evil", "the Devil",
"a demon", "a cruel, wrathful, warlike tyrant", "Satan"
and "the first beast of the book of Revelation".

</doc>
<doc id="8540" url="http://en.wikipedia.org/wiki?curid=8540" title="Diesel engine">
Diesel engine

The diesel engine (also known as a compression-ignition engine) is an internal combustion engine that uses the heat of compression to initiate ignition and burn the fuel that has been injected into the combustion chamber. This contrasts with spark-ignition engines such as a petrol engine (gasoline engine) or gas engine (using a gaseous fuel as opposed to gasoline), which use a spark plug to ignite an air-fuel mixture.
The diesel engine has the highest thermal efficiency of any standard internal or external combustion engine due to its very high compression ratio and inherent lean burn (non-stochiometric) which facilitates unburnt gasses to scavenge waste heat. A small efficiency loss is also avoided at valve overlap since unburnt fuel is not present until TDC. Low-speed diesel engines (as used in ships and other applications where overall engine weight is relatively unimportant) can have a thermal efficiency that exceeds 50%.
Diesel engines are manufactured in two-stroke and four-stroke versions. They were originally used as a more efficient replacement for stationary steam engines. Since the 1910s they have been used in submarines and ships. Use in locomotives, trucks, heavy equipment and electric generating plants followed later. In the 1930s, they slowly began to be used in a few automobiles. Since the 1970s, the use of diesel engines in larger on-road and off-road vehicles in the USA increased. According to the British Society of Motor Manufacturing and Traders, the EU average for diesel cars account for 50% of the total sold, including 70% in France and 38% in the UK.
The world's largest diesel engine is currently a Wärtsilä-Sulzer RTA96-C Common Rail marine diesel of about at 102 rpm output.
History.
In 1885, the English inventor Herbert Akroyd Stuart began investigating the possibility of using paraffin oil (very similar to modern-day diesel) for an engine, which unlike petrol would be difficult to vaporise in a carburettor as its volatility is not sufficient to allow this.
His engines, built from 1891 by Richard Hornsby and Sons, were the first internal combustion engine to use a pressurised fuel injection system. The Hornsby-Akroyd engine used a comparatively low compression ratio, so that the temperature of the air compressed in the combustion chamber at the end of the compression stroke was not high enough to initiate combustion. Combustion instead took place in a separated combustion chamber, the "vaporizer" (also called the "hot bulb") mounted on the cylinder head, into which fuel was sprayed. Self-ignition occurred from contact between the fuel-air mixture and the hot walls of the vaporizer. As the engine's load increased, so did the temperature of the bulb, causing the ignition period to advance; to counteract pre-ignition, water was dripped into the air intake.
The modern Diesel engine incorporates the features of direct (airless) injection and compression-ignition. Both ideas were patented by Akroyd Stuart and Charles Richard Binney in May 1890. Another patent was taken out on 8 October 1890, detailing the working of a complete engine - essentially that of a diesel engine - where air and fuel are introduced separately. The difference between the Akroyd engine and the modern Diesel engine was the requirement to supply extra heat to the cylinder to start the engine from cold. By 1892, Akroyd Stuart had produced an updated version of the engine that no longer required the additional heat source, a year before Diesel's engine.
In 1892, Akroyd Stuart patented a water-jacketed vaporiser to allow compression ratios to be increased. In the same year, Thomas Henry Barton at Hornsbys built a working high-compression version for experimental purposes, whereby the vaporiser was replaced with a cylinder head, therefore not relying on air being preheated, but by combustion through higher compression ratios. It ran for six hours - the first time automatic ignition was produced by compression alone. This was five years before Rudolf Diesel built his well-known high-compression prototype engine in 1897.
Rudolf Diesel was, however, subsequently credited with the innovation, and he was able to improve the engine further, whereas Akroyd Stuart stopped development on his engine in 1893.
In 1892 Diesel received patents in Germany, Switzerland, the United Kingdom and the United States for "Method of and Apparatus for Converting Heat into Work". In 1893 he described a "slow-combustion engine" that first compressed air thereby raising its temperature above the igniting-point of the fuel, then gradually introducing fuel while letting the mixture expand "against resistance sufficiently to prevent an essential increase of temperature and pressure", then cutting off fuel and "expanding without transfer of heat". In 1894 and 1895 he filed patents and addenda in various countries for his Diesel engine; the first patents were issued in Spain (No. 16,654), France (No. 243,531) and Belgium (No. 113,139) in December 1894, and in Germany (No. 86,633) in 1895 and the United States (No. 608,845) in 1898. He operated his first successful engine in 1897.
At Augsburg, on August 10, 1893, Rudolf Diesel's prime model, a single iron cylinder with a flywheel at its base, ran on its own power for the first time. Diesel spent two more years making improvements and in 1896 demonstrated another model with a theoretical efficiency of 75%, in contrast to the 10% efficiency of the steam engine. By 1898, Diesel had become a millionaire. His engines were used to power pipelines, electric and water plants, automobiles and trucks, and marine craft. They were soon to be used in mines, oil fields, factories, and transoceanic shipping.
Timeline.
1890s
1900s
1910s
1920s
1930s
1940s
1950s
1960s
1970s
1980s
1990s
2000s
2010s
Operating principle.
The diesel internal combustion engine differs from the gasoline powered Otto cycle by using highly compressed hot air to ignite the fuel rather than using a spark plug ("compression ignition" rather than "spark ignition").
In the true diesel engine, only air is initially introduced into the combustion chamber. The air is then compressed with a compression ratio typically between 15:1 and 22:1 resulting in pressure compared to in the petrol engine. This high compression heats the air to . At about the top of the compression stroke, fuel is injected directly into the compressed air in the combustion chamber. This may be into a (typically toroidal) void in the top of the piston or a "pre-chamber" depending upon the design of the engine. The fuel injector ensures that the fuel is broken down into small droplets, and that the fuel is distributed evenly. The heat of the compressed air vaporizes fuel from the surface of the droplets. The vapour is then ignited by the heat from the compressed air in the combustion chamber, the droplets continue to vaporise from their surfaces and burn, getting smaller, until all the fuel in the droplets has been burnt. The start of vaporisation causes a delay period during ignition and the characteristic diesel knocking sound as the vapour reaches ignition temperature and causes an abrupt increase in pressure above the piston. The rapid expansion of combustion gases then drives the piston downward, supplying power to the crankshaft.
As well as the high level of compression allowing combustion to take place without a separate ignition system, a high compression ratio greatly increases the engine's efficiency. Increasing the compression ratio in a spark-ignition engine where fuel and air are mixed before entry to the cylinder is limited by the need to prevent damaging pre-ignition. Since only air is compressed in a diesel engine, and fuel is not introduced into the cylinder until shortly before top dead centre (TDC), premature detonation is not an issue and compression ratios are much higher.
Early fuel injection systems.
Diesel's original engine injected fuel with the assistance of compressed air, which atomized the fuel and forced it into the engine through a nozzle (a similar principle to an aerosol spray). The nozzle opening was closed by a pin valve lifted by the camshaft to initiate the fuel injection before top dead centre (TDC). This is called an air-blast injection. Driving the three stage compressor used some power but the efficiency and net power output was more than any other combustion engine at that time.
Diesel engines in service today raise the fuel to extreme pressures by mechanical pumps and deliver it to the combustion chamber by pressure-activated injectors without compressed air. With direct injected diesels, injectors spray fuel through 4 to 12 small orifices in its nozzle. The early air injection diesels always had a superior combustion without the sharp increase in pressure during combustion. Research is now being performed and patents are being taken out to again use some form of air injection to reduce the nitrogen oxides and pollution, reverting to Diesel's original implementation with its superior combustion and possibly quieter operation. In all major aspects, the modern diesel engine holds true to Rudolf Diesel's original design, that of igniting fuel by compression at an extremely high pressure within the cylinder. With much higher pressures and high technology injectors, present-day diesel engines use the so-called solid injection system applied by Herbert Akroyd Stuart for his hot bulb engine. The indirect injection engine could be considered the latest development of these low speed "hot bulb" ignition engines.
Fuel delivery.
Diesel engines are also produced with two significantly different injection locations. "Direct" and "Indirect". Indirect injected engines place the injector in a pre-combustion chamber in the head which due to thermal losses generally require a "glow plug" to start and very high compression ratio. Usually in the range of 21:1 to 23:1 ratio. Direct injected engines use a generally donut shaped combustion chamber void on the top of the piston. Thermal efficiency losses are significantly lower in DI engines which facilitates a much lower compression ratio generally between 14:1 and 20:1 but most DI engines are closer to 17:1. The direct injected process is significantly more internally violent and thus requires careful design, and more robust construction. The lower compression ratio also creates challenges for emissions due to partial burn. Turbocharging is particularly suited to DI engines since the low compression ratio facilitates meaningful forced induction, and the increase in airflow allows capturing additional fuel efficiency not only from more complete combustion, but also from lowering parasitic efficiency losses when properly operated, by widening both power and efficiency curves. The violent combustion process of direct injection also creates more noise, but modern designs using "split shot" injectors or similar multi shot processes have dramatically amended this issue by firing a small charge of fuel before the main delivery which pre-charges the combustion chamber for a less abrupt and in most cases slightly cleaner burn.
A vital component of all diesel engines is a mechanical or electronic governor which regulates the idling speed and maximum speed of the engine by controlling the rate of fuel delivery. Unlike Otto-cycle engines, incoming air is not throttled and a diesel engine without a governor cannot have a stable idling speed and can easily overspeed, resulting in its destruction. Mechanically governed fuel injection systems are driven by the engine's gear train. These systems use a combination of springs and weights to control fuel delivery relative to both load and speed. Modern electronically controlled diesel engines control fuel delivery by use of an electronic control module (ECM) or electronic control unit (ECU). The ECM/ECU receives an engine speed signal, as well as other operating parameters such as intake manifold pressure and fuel temperature, from a sensor and controls the amount of fuel and start of injection timing through actuators to maximise power and efficiency and minimise emissions. Controlling the timing of the start of injection of fuel into the cylinder is a key to minimizing emissions, and maximizing fuel economy (efficiency), of the engine. The timing is measured in degrees of crank angle of the piston before top dead centre. For example, if the ECM/ECU initiates fuel injection when the piston is 10° before TDC, the start of injection, or timing, is said to be 10° BTDC. Optimal timing will depend on the engine design as well as its speed and load, and is usually 4° BTDC in 1,350-6,000 HP, net, "medium speed" locomotive, marine and stationary diesel engines.
Advancing the start of injection (injecting before the piston reaches to its SOI-TDC) results in higher in-cylinder pressure and temperature, and higher efficiency, but also results in increased engine noise due to faster cylinder pressure rise and increased oxides of nitrogen (NOx) formation due to higher combustion temperatures. Delaying start of injection causes incomplete combustion, reduced fuel efficiency and an increase in exhaust smoke, containing a considerable amount of particulate matter and unburned hydrocarbons.
Major advantages.
Diesel engines have several advantages over other internal combustion engines:
Mechanical and electronic injection.
Many configurations of fuel injection have been used over the course of the 20th century.
Most present-day diesel engines use a mechanical single plunger high-pressure fuel pump driven by the engine crankshaft. For each engine cylinder, the corresponding plunger in the fuel pump measures out the correct amount of fuel and determines the timing of each injection. These engines use injectors that are very precise spring-loaded valves that open and close at a specific fuel pressure. Separate high-pressure fuel lines connect the fuel pump with each cylinder. Fuel volume for each single combustion is controlled by a slanted groove in the plunger which rotates only a few degrees releasing the pressure and is controlled by a mechanical governor, consisting of weights rotating at engine speed constrained by springs and a lever. The injectors are held open by the fuel pressure. On high-speed engines the plunger pumps are together in one unit. The length of fuel lines from the pump to each injector is normally the same for each cylinder in order to obtain the same pressure delay.
A cheaper configuration on high-speed engines with fewer than six cylinders is to use an axial-piston distributor pump, consisting of one rotating pump plunger delivering fuel to a valve and line for each cylinder (functionally analogous to points and distributor cap on an Otto engine).
Many modern systems have a single fuel pump which supplies fuel constantly at high pressure with a common rail (single fuel line common) to each injector. Each injector has a solenoid operated by an electronic control unit, resulting in more accurate control of injector opening times that depend on other control conditions, such as engine speed and loading, and providing better engine performance and fuel economy.
Both mechanical and electronic injection systems can be used in either direct or indirect injection configurations.
Two-stroke diesel engines with mechanical injection pumps can be inadvertently run in reverse, albeit in a very inefficient manner, possibly damaging the engine.
Large ship two-stroke diesels are designed to run in either direction, obviating the need for a gearbox.
Indirect injection.
An indirect injection diesel engine delivers fuel into a chamber off the combustion chamber, called a pre-chamber or ante-chamber, where combustion begins and then spreads into the main combustion chamber, assisted by turbulence created in the chamber. This system allows for a smoother, quieter running engine, and because combustion is assisted by turbulence, injector pressures can be lower, about , using a single orifice tapered jet injector. Mechanical injection systems allowed high-speed running suitable for road vehicles (typically up to speeds of around . The pre-chamber had the disadvantage of increasing heat loss to the engine's cooling system, and restricting the combustion burn, which reduced the efficiency by 5–10%. Indirect injection engines are cheaper to build and it is easier to produce smooth, quiet-running vehicles with a simple mechanical system. In road-going vehicles most prefer the greater efficiency and better controlled emission levels of direct injection. Indirect injection diesels can still be found in the many ATV diesel applications.
Direct injection.
Direct injection diesel engines have injectors mounted at the top of the combustion chamber. The injectors are activated using one of two methods - hydraulic pressure from the fuel pump, or an electronic signal from an engine controller.
Hydraulic pressure activated injectors can produce harsh engine noise. Fuel consumption is about 15–20% lower than indirect injection diesels. The extra noise is generally not a problem for industrial uses of the engine, but for automotive usage, buyers have to decide whether or not the increased fuel efficiency would compensate for the extra noise.
Electronic control of the fuel injection transformed the direct injection engine by allowing much greater control over the combustion.
Unit direct injection.
Unit direct injection also injects fuel directly into the cylinder of the engine. In this system the injector and the pump are combined into one unit positioned over each cylinder controlled by the camshaft. Each cylinder has its own unit eliminating the high-pressure fuel lines, achieving a more consistent injection. This type of injection system, also developed by Bosch, is used by Volkswagen AG in cars (where it is called a "Pumpe-Düse-System"—literally "pump-nozzle system") and by Mercedes-Benz ("PLD") and most major diesel engine manufacturers in large commercial engines (CAT, Cummins, Detroit Diesel, Electro-Motive Diesel, Volvo). With recent advancements, the pump pressure has been raised to , allowing injection parameters similar to common rail systems.
Common rail direct injection.
In common rail systems, the separate pulsing high-pressure fuel line to each cylinder's injector is also eliminated. Instead, a high-pressure pump pressurizes fuel at up to , in a "common rail". The common rail is a tube that supplies each computer-controlled injector containing a precision-machined nozzle and a plunger driven by a solenoid or piezoelectric actuator.
Cold weather.
Starting.
In cold weather, high speed diesel engines can be difficult to start because the mass of the cylinder block and cylinder head absorb the heat of compression, preventing ignition due to the higher surface-to-volume ratio. Pre-chambered engines make use of small electric heaters inside the pre-chambers called glowplugs, while the direct-injected engines have these glowplugs in the combustion chamber.
Many engines use resistive heaters in the intake manifold to warm the inlet air for starting, or until the engine reaches operating temperature. Engine block heaters (electric resistive heaters in the engine block) connected to the utility grid are used in cold climates when an engine is turned off for extended periods (more than an hour), to reduce startup time and engine wear. Block heaters are also used for emergency power standby Diesel-powered generators which must rapidly pick up load on a power failure. In the past, a wider variety of cold-start methods were used. Some engines, such as Detroit Diesel engines used a system to introduce small amounts of ether into the inlet manifold to start combustion. Others used a mixed system, with a resistive heater burning methanol. An impromptu method, particularly on out-of-tune engines, is to manually spray an aerosol can of ether-based engine starter fluid into the intake air stream (usually through the intake air filter assembly).
Gelling.
Diesel fuel is also prone to "waxing" or "gelling" in cold weather; both are terms for the solidification of diesel oil into a partially crystalline state. The crystals build up in the fuel line (especially in fuel filters), eventually starving the engine of fuel and causing it to stop running. Low-output electric heaters in fuel tanks and around fuel lines are used to solve this problem. Also, most engines have a "spill return" system, by which any excess fuel from the injector pump and injectors is returned to the fuel tank. Once the engine has warmed, returning warm fuel prevents waxing in the tank.
Due to improvements in fuel technology with additives, waxing rarely occurs in all but the coldest weather when a mix of diesel and kerosene may be used to run a vehicle. Gas stations in regions with a cold climate are required to offer winterized diesel in the cold seasons that allow operation below a specific Cold Filter Plugging Point. In Europe these diesel characteristics are described in the EN 590 standard.
Supercharging and turbocharging.
Most diesels are now turbocharged and some are both turbo charged and supercharged. Because diesels do not have fuel in the cylinder before combustion is initiated, more than one bar (100 kPa) of air can be loaded in the cylinder without preignition. A turbocharged engine can produce significantly more power than a naturally aspirated engine of the same configuration, as having more air in the cylinders allows more fuel to be burned and thus more power to be produced. A supercharger is powered mechanically by the engine's crankshaft, while a turbocharger is powered by the engine exhaust, not requiring any mechanical power. Turbocharging can improve the fuel economy of diesel engines by recovering waste heat from the exhaust, increasing the excess air factor, and increasing the ratio of engine output to friction losses.
A two-stroke engine does not have a discrete exhaust and intake stroke and thus is incapable of self-aspiration. Therefore all two-stroke engines must be fitted with a blower to charge the cylinders with air and assist in dispersing exhaust gases, a process referred to as scavenging. In some cases, the engine may also be fitted with a turbocharger, whose output is directed into the blower inlet.
A few designs employ a hybrid turbocharger (a turbo-compressor system) for scavenging and charging the cylinders, which device is mechanically driven at cranking and low speeds to act as a blower, but which acts as a true turbocharger at higher speeds and loads. A hybrid turbocharger can revert to compressor mode during commands for large increases in engine output power.
As turbocharged or supercharged engines produce more power for a given engine size as compared to naturally aspirated engines, attention must be paid to the mechanical design of components, lubrication, and cooling to handle the power. Pistons are usually cooled with lubrication oil sprayed on the bottom of the piston. Large engines may use water, sea water, or oil supplied through telescoping pipes attached to the crosshead.
Types.
Size groups.
There are three size groups of Diesel engines
Basic types.
There are two basic types of Diesel Engines
Early engines.
Rudolf Diesel based his engine on the design of the Gas engine created by Nikolaus Otto in 1876 with the goal of improving its efficiency. He patented his Diesel engine concepts in patents that were set forth in 1892 and 1893. As such, diesel engines in the late 19th and early 20th centuries used the same basic layout and form as industrial steam engines, with long-bore cylinders, external valve gear, cross-head bearings and an open crankshaft connected to a large flywheel. Smaller engines would be built with vertical cylinders, while most medium- and large-sized industrial engines were built with horizontal cylinders, just as steam engines had been. Engines could be built with more than one cylinder in both cases. The largest early diesels resembled the triple-expansion steam reciprocating engine, being tens of feet high with vertical cylinders arranged in-line. These early engines ran at very slow speeds—partly due to the limitations of their air-blast injector equipment and partly so they would be compatible with the majority of industrial equipment designed for steam engines; maximum speeds of 100–300 rpm were common. Engines were usually started by allowing compressed air into the cylinders to turn the engine, although smaller engines could be started by hand.
In 1897, when the first Diesel engine was completed Adolphus Busch traveled to Cologne and negotiated exclusive right to produce the Diesel engine in the USA and Canada. In his examination of the engine, it was noted that the Diesel at that time operated at thermodynamic efficiencies of 32–35%, while a typical triple expansion steam engine would operate at about 18%.
In the early decades of the 20th century, when large diesel engines were first being used, the engines took a form similar to the compound steam engines common at the time, with the piston being connected to the connecting rod by a crosshead bearing. Following steam engine practice some manufacturers made double-acting
two-stroke and four-stroke diesel engines to increase power output, with combustion taking place on both sides of the piston, with two sets of valve gear and fuel injection. While it produced large amounts of power and was very efficient, the double-acting diesel engine's main problem was producing a good seal where the piston rod passed through the bottom of the lower combustion chamber to the crosshead bearing, and no more were built. By the 1930s turbochargers were fitted to some engines. Crosshead bearings are still used to reduce the wear on the cylinders in large long-stroke main marine engines.
Modern high and medium-speed engines.
As with petrol engines, there are two classes of diesel engines in current use: two-stroke and four-stroke. The four-stroke type is the "classic" version, tracing its lineage back to Rudolf Diesel's prototype. It is also the most commonly used form, being the preferred power source for many motor vehicles, especially buses and trucks. Much larger engines, such as used for railroad locomotion and marine propulsion, are often two-stroke units, offering a more favourable power-to-weight ratio, as well as better fuel economy. The most powerful engines in the world are two-stroke diesels of mammoth dimensions.
Two-stroke diesel engine operation is similar to that of petrol counterparts, except that fuel is not mixed with air before induction, and the crankcase does not take an active role in the cycle. The traditional two-stroke design relies upon a mechanically driven positive displacement blower to charge the cylinders with air before compression and ignition. The charging process also assists in expelling (scavenging) combustion gases remaining from the previous power stroke.
The archetype of the modern form of the two-stroke diesel is the (high-speed) Detroit Diesel Series 71 engine, designed by Charles F. "Boss" Kettering and his colleagues at General Motors Corporation in 1938, in which the blower pressurizes a chamber in the engine block that is often referred to as the "air box". The (very much larger medium-speed) Electro-Motive Diesel engine is used as the prime mover in EMD diesel-electric locomotive, marine and stationary applications, and was designed by the same team, and is built to the same principle. However, a significant improvement built into most later EMD engines is the mechanically-assisted turbo-compressor, which provides charge air using mechanical assistance during starting (thereby obviating the necessity for Roots-blown scavenging), and provides charge air using an exhaust gas-driven turbine during normal operations—thereby providing true turbocharging and additionally increasing the engine's power output by at least fifty percent.
In a two-stroke diesel engine, as the cylinder's piston approaches the bottom dead centre exhaust ports or valves are opened relieving most of the excess pressure after which a passage between the air box and the cylinder is opened, permitting air flow into the cylinder. The air flow blows the remaining combustion gases from the cylinder—this is the scavenging process. As the piston passes through bottom centre and starts upward, the passage is closed and compression commences, culminating in fuel injection and ignition. Refer to two-stroke diesel engines for more detailed coverage of aspiration types and supercharging of two-stroke diesel engines.
Normally, the number of cylinders are used in multiples of two, although any number of cylinders can be used as long as the load on the crankshaft is counterbalanced to prevent excessive vibration. The inline-six-cylinder design is the most prolific in light- to medium-duty engines, though small V8 and larger inline-four displacement engines are also common. Small-capacity engines (generally considered to be those below five litres in capacity) are generally four- or six-cylinder types, with the four-cylinder being the most common type found in automotive uses. Five-cylinder diesel engines have also been produced, being a compromise between the smooth running of the six-cylinder and the space-efficient dimensions of the four-cylinder. Diesel engines for smaller plant machinery, boats, tractors, generators and pumps may be four, three or two-cylinder types, with the single-cylinder diesel engine remaining for light stationary work. Direct reversible two-stroke marine diesels need at least three cylinders for reliable restarting forwards and reverse, while four-stroke diesels need at least six cylinders.
The desire to improve the diesel engine's power-to-weight ratio produced several novel cylinder arrangements to extract more power from a given capacity. The uniflow opposed-piston engine uses two pistons in one cylinder with the combustion cavity in the middle and gas in- and outlets at the ends. This makes a comparatively light, powerful, swiftly running and economic engine suitable for use in aviation. An example is the Junkers Jumo 204/205. The Napier Deltic engine, with three cylinders arranged in a triangular formation, each containing two opposed pistons, the whole engine having three crankshafts, is one of the better known.
Modern low-speed engines.
Low-speed diesel engines (as used in ships and other applications where overall engine weight is relatively unimportant) often have a thermal efficiency which exceeds 50%.
Gas generator.
Before 1950, Sulzer started experimenting with two-stroke engines with boost pressures as high as 6 atmospheres, in which all the output power was taken from an exhaust gas turbine. The two-stroke pistons directly drove air compressor pistons to make a positive displacement gas generator. Opposed pistons were connected by linkages instead of crankshafts. Several of these units could be connected to provide power gas to one large output turbine. The overall thermal efficiency was roughly twice that of a simple gas turbine. This system was derived from Raúl Pateras Pescara's work on free-piston engines in the 1930s.
Advantages and disadvantages versus spark-ignition engines.
Fuel economy.
The MAN S80ME-C7 low speed diesel engines use of fuel per kWh for an overall energy conversion efficiency of 54.4%, which is the highest conversion of fuel into power by any single-cycle internal or external combustion engine (The efficiency of a combined cycle gas turbine system can exceed 60%.) Diesel engines are more efficient than gasoline (petrol) engines of the same power rating, resulting in lower fuel consumption. A common margin is 40% more miles per gallon for an efficient turbodiesel. For example, the current model Škoda Octavia, using Volkswagen Group engines, has a combined Euro rating of for the petrol engine and for the diesel engine.
However, such a comparison does not take into account that diesel fuel is denser and contains about 15% more energy by volume. Although the calorific value of the fuel is slightly lower at 45.3 MJ/kg (megajoules per kilogram) than petrol at 45.8 MJ/kg, liquid diesel fuel is significantly denser than liquid petrol. This is significant because volume of fuel, in addition to mass, is an important consideration in mobile applications.
Adjusting the numbers to account for the energy density of diesel fuel, the overall energy efficiency is still about 20% greater for the diesel version.
While a higher compression ratio is helpful in raising efficiency, diesel engines are much more efficient than gasoline (petrol) engines when at low power and at engine idle. Unlike the petrol engine, diesels lack a butterfly valve (throttle) in the inlet system, which closes at idle. This creates parasitic loss and destruction of availability of the incoming air, reducing the efficiency of petrol engines at idle. In many applications, such as marine, agriculture, and railways, diesels are left idling and unattended for many hours, sometimes even days. These advantages are especially attractive in locomotives (see dieselisation).
Even though diesel engines have a theoretical fuel efficiency of 75%, in practice it is lower. Engines in large diesel trucks, buses, and newer diesel cars can achieve peak efficiencies around 45%, and could reach 55% efficiency in the near future. However, average efficiency over a driving cycle is lower than peak efficiency. For example, it might be 37% for an engine with a peak efficiency of 44%.
Torque.
Diesel engines produce more torque than petrol engines for a given displacement due to their higher compression ratio. Higher pressure in the cylinder and higher forces on the connecting rods and crankshaft require stronger, heavier components. Heavier rotating components prevent diesel engines from reving as high as petrol engines for a given displacement. Diesel engines generally have similar power and inferior power to weight to petrol engines. Petrol engines must be geared lower to get the same torque as a comparable diesel but since petrol engines rev higher both will have similar acceleration. An arbitrary amount of torque at the wheels can be gained by gearing any power source down sufficiently (including a hand crank). For example, a theoretical engine with a constant 200 ft/lbs of torque and a 3000 rpm rev limit has just as much power (a little over 114 hp) as another theoretical engine with a constant maximum 100 ft/lbs of torque and a 6000 rpm rev limit. A (lossless) 2 to 1 reduction gear on the second engine will output a constant maximum 200 ft/lbs of torque at a maximum of 3000 rpm, with no change in power. Comparing engines based on (maximum) torque is just as useful as comparing them based on (maximum) rpm.
Power.
In diesel engines, conditions in the engine differ from the spark-ignition engine, since power is directly controlled by the fuel supply, rather than by controlling the air supply.
The average diesel engine has a poorer power-to-weight ratio than the petrol engine. This is because the diesel must operate at lower engine speeds and because it needs heavier, stronger parts to resist the operating pressure caused by the high compression ratio of the engine and the large amounts of torque generated to the crankshaft. In addition, diesels are often built with stronger parts to give them longer lives and better reliability, important considerations in industrial applications.
Diesel engines usually have longer stroke lengths chiefly to facilitate achieving the necessary compression ratios, but also to reduce the optimal operating speed (rpm). As a result piston and connecting rods are heavier and more force must be transmitted through the connecting rods and crankshaft to change the momentum of the piston. This is another reason that a diesel engine must be stronger for the same power output as a petrol engine.
Yet it is this characteristic that has allowed some enthusiasts to acquire significant power increases with turbocharged engines by making fairly simple and inexpensive modifications. A petrol engine of similar size cannot put out a comparable power increase without extensive alterations because the stock components cannot withstand the higher stresses placed upon them. Since a diesel engine is already built to withstand higher levels of stress, it makes an ideal candidate for performance tuning at little expense. However, it should be said that any modification that raises the amount of fuel and air put through a diesel engine will increase its operating temperature, which will reduce its life and increase service requirements. These are issues with newer, lighter, "high-performance" diesel engines which are not "overbuilt" to the degree of older engines and they are being pushed to provide greater power in smaller engines.
Forced induction.
The addition of a turbocharger or supercharger to the engine greatly assists in increasing fuel economy and power output, mitigating the fuel-air intake speed limit mentioned above for a given engine displacement. Boost pressures can be higher on diesels than on petrol engines, due to the latter's susceptibility to knock, and the higher compression ratio allows a diesel engine to be more efficient than a comparable spark ignition engine. Because the burned gases are expanded further in a diesel engine cylinder, the exhaust gas is cooler, meaning turbochargers require less cooling, and can be more reliable, than with spark-ignition engines.
Without the risk of knocking, boost pressure in a diesel engine can be much higher; it is possible to run as much boost as the engine will physically stand before breaking apart.
A combination of improved mechanical technology (such as multi-stage injectors which fire a short "pilot charge" of fuel into the cylinder to warm the combustion chamber before delivering the main fuel charge), higher injection pressures that have improved the atomisation of fuel into smaller droplets, and electronic control (which can adjust the timing and length of the injection process to optimise it for all speeds and temperatures) have mitigated most of these problems in the latest generation of common-rail designs, while greatly improving engine efficiency. Poor power and narrow torque bands have been addressed by superchargers, turbochargers, (especially variable geometry turbochargers), intercoolers, and a large efficiency increase from about 35% for IDI to 45% for the latest engines in the last 15 years.
Emissions.
Since the diesel engine uses less fuel than the petrol engine per unit distance, the diesel produces less carbon dioxide (CO2) per unit distance. Recent advances in production and changes in the political climate have increased the availability and awareness of biodiesel, an alternative to petroleum-derived diesel fuel with a much lower net-sum emission of CO2, due to the absorption of CO2 by plants used to produce the fuel. Although concerns are now being raised as to the negative effect this is having on the world food supply, as the growing of crops specifically for biofuels takes up land that could be used for food crops and uses water that could be used by both humans and animals. However, the use of waste vegetable oil, sawmill waste from managed forests in Finland, and advances in the production of vegetable oil from algae demonstrate great promise in providing feed stocks for sustainable biodiesel that are not in competition with food production.
When a diesel engine runs at low power, there is enough oxygen present to burn the fuel- diesel engines only make significant amounts of carbon monoxide when running under a load.
Diesel fuel is injected just before the power stroke. As a result, the fuel cannot burn completely unless it has a sufficient amount of oxygen. This can result in incomplete combustion and black smoke in the exhaust if more fuel is injected than there is air available for the combustion process. Modern engines with electronic fuel delivery can adjust the timing and amount of fuel delivery (by changing the duration of the injection pulse), and so operate with less waste of fuel. In a mechanical system, the injection timing and duration must be set to be efficient at the anticipated operating rpm and load, and so the settings are less than ideal when the engine is running at any other RPM than what it is timed for. The electronic injection can "sense" engine revs, load, even boost and temperature, and continuously alter the timing to match the given situation. In the petrol engine, air and fuel are mixed for the entire compression stroke, ensuring complete mixing even at higher engine speeds.
Diesel exhaust is well known for its characteristic smell; but this smell in recent years has become much less because the sulfur is now removed from the fuel in the oil refinery.
Diesel exhaust has been found to contain a long list of toxic air contaminants. Among these pollutants, fine particle pollution is perhaps the most important as a cause of diesel's harmful health effects.
Noise.
The distinctive noise of a diesel engine is variably called diesel clatter, diesel nailing, or diesel knock. Diesel clatter is caused largely by the diesel combustion process; the sudden ignition of the diesel fuel when injected into the combustion chamber causes a pressure wave. Engine designers can reduce diesel clatter through: indirect injection; pilot or pre-injection; injection timing; injection rate; compression ratio; turbo boost; and exhaust gas recirculation (EGR). Common rail diesel injection systems permit multiple injection events as an aid to noise reduction. Diesel fuels with a higher cetane rating modify the combustion process and reduce diesel clatter. CN (Cetane number) can be raised by distilling higher quality crude oil, by catalyzing a higher quality product or by using a cetane improving additive.
A combination of improved mechanical technology such as multi-stage injectors which fire a short "pilot charge" of fuel into the cylinder to initiate combustion before delivering the main fuel charge, higher injection pressures that have improved the atomisation of fuel into smaller droplets, and electronic control (which can adjust the timing and length of the injection process to optimise it for all speeds and temperatures), have partially mitigated these problems in the latest generation of common-rail designs, while improving engine efficiency.
Reliability.
For most industrial or nautical applications, reliability is considered more important than light weight and high power.
The lack of an electrical ignition system greatly improves the reliability. The high durability of a diesel engine is also due to its overbuilt nature (see above), a benefit that is magnified by the lower rotating speeds in diesels. Diesel fuel is a better lubricant than petrol and thus, it is less harmful to the oil film on piston rings and cylinder bores; it is routine for diesel engines to cover or more without a rebuild.
Due to the greater compression ratio and the increased weight of the stronger components, starting a diesel engine is harder than starting a gasoline engine of similar design and displacement. More torque is required to push the engine through compression.
Either an electrical starter or an air-start system is used to start the engine turning. On large engines, pre-lubrication and slow turning of an engine, as well as heating, are required to minimise the amount of engine damage during initial start-up and running. Some smaller military diesels can be started with an explosive cartridge, called a Coffman starter, which provides the extra power required to get the machine turning. In the past, Caterpillar and John Deere used a small petrol "pony" engine in their tractors to start the primary diesel engine. The pony engine heated the diesel to aid in ignition and used a small clutch and transmission to spin up the diesel engine. Even more unusual was an International Harvester design in which the diesel engine had its own carburetor and ignition system, and started on petrol. Once warmed up, the operator moved two levers to switch the engine to diesel operation, and work could begin. These engines had very complex cylinder heads, with their own petrol combustion chambers, and were vulnerable to expensive damage if special care was not taken (especially in letting the engine cool before turning it off).
Quality and variety of fuels.
Petrol/gasoline engines are limited in the variety and quality of the fuels they can burn. Older petrol engines fitted with a carburetor required a volatile fuel that would vaporise easily to create the necessary air-fuel ratio for combustion. Because both air and fuel are admitted to the cylinder, if the compression ratio of the engine is too high or the fuel too volatile (with too low an octane rating), the fuel will ignite under compression, as in a diesel engine, before the piston reaches the top of its stroke. This pre-ignition causes a power loss and over time major damage to the piston and cylinder. The need for a fuel that is volatile enough to vaporise but not too volatile (to avoid pre-ignition) means that petrol engines will only run on a narrow range of fuels. There has been some success at dual-fuel engines that use petrol and ethanol, petrol and propane, and petrol and methane.
In diesel engines, a mechanical injector system vaporizes the fuel directly into the combustion chamber or a pre-combustion chamber (as opposed to a Venturi jet in a carburetor, or a fuel injector in a fuel injection system vaporising fuel into the intake manifold or intake runners as in a petrol engine). This "forced vaporisation" means that less-volatile fuels can be used. More crucially, because only air is inducted into the cylinder in a diesel engine, the compression ratio can be much higher as there is no risk of pre-ignition provided the injection process is accurately timed. This means that cylinder temperatures are much higher in a diesel engine than a petrol engine, allowing less volatile fuels to be used.
Diesel fuel is a form of light fuel oil, very similar to kerosene (paraffin), but diesel engines, especially older or simple designs that lack precision electronic injection systems, can run on a wide variety of other fuels. Some of the most common alternatives are Jet A-1 type jet fuel or vegetable oil from a very wide variety of plants. Some engines can be run on vegetable oil without modification, and most others require fairly basic alterations. Biodiesel is a pure diesel-like fuel refined from vegetable oil and can be used in nearly all diesel engines. Requirements for fuels to be used in diesel engines are the ability of the fuel to flow along the fuel lines, the ability of the fuel to lubricate the injector pump and injectors adequately, and its ignition qualities (ignition delay, cetane number). Inline mechanical injector pumps generally tolerate poor-quality or bio-fuels better than distributor-type pumps. Also, indirect injection engines generally run more satisfactorily on bio-fuels than direct injection engines. This is partly because an indirect injection engine has a much greater 'swirl' effect, improving vaporisation and combustion of fuel, and because (in the case of vegetable oil-type fuels) lipid depositions can condense on the cylinder walls of a direct-injection engine if combustion temperatures are too low (such as starting the engine from cold).
It is often reported that Diesel designed his engine to run on peanut oil, but this is false. Patent number 608845 describes his engine as being designed to run on pulverulent solid fuel (coal dust). Diesel stated in his published papers, "at the Paris Exhibition in 1900 ("Exposition Universelle") there was shown by the Otto Company a small diesel engine, which, at the request of the French Government ran on Arachide (earth-nut or peanut) oil (see biodiesel), and worked so smoothly that only a few people were aware of it. The engine was constructed for using mineral oil, and was then worked on vegetable oil without any alterations being made. The French Government at the time thought of testing the applicability to power production of the Arachide, or earth-nut, which grows in considerable quantities in their African colonies, and can easily be cultivated there." Diesel himself later conducted related tests and appeared supportive of the idea.
Most large marine diesels run on heavy fuel oil (sometimes called "bunker oil"), which is a thick, viscous and almost flameproof fuel which is very safe to store and cheap to buy in bulk as it is a waste product from the petroleum refining industry. The fuel must be heated to thin it out (often by the exhaust header) and is often passed through multiple injection stages to vaporise it.
Fuel and fluid characteristics.
Diesel engines can operate on a variety of different fuels, depending on configuration, though the eponymous diesel fuel derived from crude oil is most common. The engines can work with the full spectrum of crude oil distillates, from natural gas, alcohols, petrol, wood gas to the "fuel oils" from diesel oil to residual fuels. Many automotive diesel engines would run on 100% biodiesel without any modifications. This would be such a potential advantage since biodiesel can be made so much more cheaply than it takes to have traditional diesel fuel from your fuel station's pump.
The type of fuel used is selected to meet a combination of service requirements, and fuel costs. Good-quality diesel fuel can be synthesised from vegetable oil and alcohol. Diesel fuel can be made from coal or other carbon base using the Fischer-Tropsch process. Biodiesel is growing in popularity since it can frequently be used in unmodified engines, though production remains limited. Recently, biodiesel from coconut, which can produce a very promising coco methyl ester (CME), has characteristics which enhance lubricity and combustion giving a regular diesel engine without any modification more power, less particulate matter or black smoke, and smoother engine performance. The Philippines pioneers in the research on Coconut based CME with the help of German and American scientists. Petroleum-derived diesel is often called "petrodiesel" if there is need to distinguish the source of the fuel.
Pure plant oils are increasingly being used as a fuel for cars, trucks and remote combined heat and power generation especially in Germany where hundreds of decentralised small- and medium-sized oil presses cold press oilseed, mainly rapeseed, for fuel. There is a Deutsches Institut für Normung fuel standard for rapeseed oil fuel.
"Residual fuels" are the "dregs" of the distillation process and are a thicker, heavier oil, or oil with higher viscosity, which are so thick that they are not readily pumpable unless heated. Residual fuel oils are cheaper than clean, refined diesel oil, although they are dirtier. Their main considerations are for use in ships and very large generation sets, due to the cost of the large volume of fuel consumed, frequently amounting to many tonnes per hour. The poorly refined biofuels straight vegetable oil (SVO) and waste vegetable oil (WVO) can fall into this category, but can be viable fuels on non-common rail or TDI PD diesels with the simple conversion of fuel heating to 80 to 100 degrees Celsius to reduce viscosity, and adequate filtration to OEM standards. Engines using these heavy oils have to start and shut down on standard diesel fuel, as these fuels will not flow through fuel lines at low temperatures. Moving beyond that, use of low-grade fuels can lead to serious maintenance problems because of their high sulphur and lower lubrication properties. Most diesel engines that power ships like supertankers are built so that the engine can safely use low-grade fuels due to their separate cylinder and crankcase lubrication.
Normal diesel fuel is more difficult to ignite and slower in developing fire than petrol because of its higher flash point, but once burning, a diesel fire can be fierce.
Fuel contaminants such as dirt and water are often more problematic in diesel engines than in petrol engines. Water can cause serious damage, due to corrosion, to the injection pump and injectors; and dirt, even very fine particulate matter, can damage the injection pumps due to the close tolerances that the pumps are machined to. All diesel engines will have a fuel filter (usually much finer than a filter on a petrol engine), and a water trap. The water trap (which is sometimes part of the fuel filter) often has a float connected to a warning light, which warns when there is too much water in the trap, and must be drained before damage to the engine can result. The fuel filter must be replaced much more often on a diesel engine than on a petrol engine, changing the fuel filter every 2-4 oil changes is not uncommon for some vehicles.
Safety.
Fuel flammability.
Diesel fuel has low flammability, leading to a low risk of fire caused by fuel in a vehicle equipped with a diesel engine.
In yachts, diesel engines are often used because the petrol (gasoline) that fuels spark-ignition engines releases combustible vapors which can lead to an explosion if it accumulates in a confined space such as the bottom of a vessel. Ventilation systems are mandatory on petrol-powered vessels.
The United States Army and NATO use only diesel engines and turbines because of fire hazard. Although neither gasoline nor diesel is explosive in liquid form, both can create an explosive air/vapor mix under the right conditions. However, diesel fuel is less prone due to its lower vapor pressure, which is an indication of evaporation rate. The Material Safety Data Sheet for ultra-low sulfur diesel fuel indicates a vapor explosion hazard for diesel indoors, outdoors, or in sewers.
US Army gasoline-engined tanks during World War II were nicknamed Ronsons, because of their greater likelihood of catching fire when damaged by enemy fire. (Although tank fires were usually caused by detonation of the ammunition rather than fuel), while diesel tanks such as the Soviet T-34 were less prone to catching fire.
Maintenance hazards.
Fuel injection introduces potential hazards in engine maintenance due to the high fuel pressures used. Residual pressure can remain in the fuel lines long after an injection-equipped engine has been shut down. This residual pressure must be relieved, and if it is done so by external bleed-off, the fuel must be safely contained. If a high-pressure diesel fuel injector is removed from its seat and operated in open air, there is a risk to the operator of injury by hypodermic jet-injection, even with only pressure. The first known such injury occurred in 1937 during a diesel engine maintenance operation.
Cancer.
Diesel exhaust has been classified as an IARC Group 1 carcinogen. It causes lung cancer and is associated with an increased risk for bladder cancer.
Applications.
The characteristics of diesel have different advantages for different applications.
Passenger cars.
Diesel engines have long been popular in bigger cars and have been used in smaller cars such as superminis like the Peugeot 205, in Europe since the 1980s. Diesel engines tend to be more economical at regular driving speeds and are much better at city speeds. Their reliability and life-span tend to be better (as detailed). Some 40% or more of all cars sold in Europe are diesel-powered where they are considered a low CO2 option. Mercedes-Benz in conjunction with Robert Bosch GmbH produced diesel-powered passenger cars starting in 1936 and very large numbers are used all over the world (often as "Grande Taxis" in the Third World).
Railroad rolling stock.
Diesel engines have eclipsed steam engines as the prime mover on all non-electrified railroads in the industrialized world. The first diesel locomotives appeared in the early 20th century, and diesel multiple units soon after. While electric locomotives have replaced the diesel locomotive for some passenger traffic in Europe and Asia, diesel is still today very popular for cargo-hauling freight trains and on tracks where electrification is not feasible. Most modern diesel locomotives are actually diesel-electric locomotives: the diesel engine is used to power an electric generator that in turn powers electric traction motors with no mechanical connection between diesel engine and traction. After 2000, environmental requirements has caused higher development cost for engines, and it has become common for passenger multiple units to use engines and automatic mechanical gearboxes made for trucks. Up to four such combinations might be used to get enough power in a train.
Other transport uses.
Larger transport applications (trucks, buses, etc.) also benefit from the Diesel's reliability and high torque output. Diesel displaced paraffin (or tractor vaporising oil, TVO) in most parts of the world by the end of the 1950s with the U.S. following some 20 years later.
In merchant ships and boats, the same advantages apply with the relative safety of Diesel fuel an additional benefit. The German pocket battleships were the largest Diesel warships, but the German torpedo-boats known as E-boats ("Schnellboot") of the Second World War were also Diesel craft. Conventional submarines have used them since before World War I, relying on the almost total absence of carbon monoxide in the exhaust. American World War II Diesel-electric submarines operated on two-stroke cycle, as opposed to the four-stroke cycle that other navies used.
Non-road diesel engines.
Non-road diesel engines include mobile equipment and vehicles that are not used on the public roadways such as construction equipment and agricultural tractors.
Military fuel standardisation.
NATO has a single vehicle fuel policy and has selected diesel for this purpose. The use of a single fuel simplifies wartime logistics. NATO and the United States Marine Corps have even been developing a diesel military motorcycle based on a Kawasaki off road motorcycle the KLR 650, with a purpose designed naturally aspirated direct injection diesel at Cranfield University in England, to be produced in the USA, because motorcycles were the last remaining gasoline-powered vehicle in their inventory. Before this, a few civilian motorcycles had been built using adapted stationary diesel engines, but the weight and cost disadvantages generally outweighed the efficiency gains.
Non-transport uses.
Diesel engines are also used to power permanent, portable, and backup generators, irrigation pumps, corn grinders, and coffee de-pulpers.
Engine speeds.
Within the diesel engine industry, engines are often categorized by their rotational speeds into three unofficial groups:
High- and medium-speed engines are predominantly four-stroke engines; except for the Detroit Diesel two-stroke range. Medium-speed engines are physically larger than high-speed engines and can burn lower-grade (slower-burning) fuel than high-speed engines. Slow-speed engines are predominantly large two-stroke crosshead engines, hence very different from high- and medium-speed engines. Due to the lower rotational speed of slow- and medium-speed engines, there is more time for combustion during the power stroke of the cycle, allowing the use of slower-burning fuels than high-speed engines.
High-speed engines.
High-speed (approximately 1,000 rpm and greater) engines are used to power trucks (lorries), buses, tractors, cars, yachts, compressors, pumps and small electrical generators. As of 2008, most high-speed engines have direct injection. Many modern engines, particularly in on-highway applications, have common rail direct injection, which is cleaner burning.
Medium-speed engines.
Medium-speed engines are used in large electrical generators, ship propulsion and mechanical drive applications such as large compressors or pumps. Medium speed diesel engines operate on either diesel fuel or heavy fuel oil by direct injection in the same manner as low-speed engines.
Engines used in electrical generators run at approximately 300 to 1000 rpm and are optimized to run at a set synchronous speed depending on the generation frequency (50 or 60 hertz) and provide a rapid response to load changes. Typical synchronous speeds for modern medium-speed engines are 500/514 rpm (50/60 Hz), 600 rpm (both 50 and 60 Hz), 720/750 rpm, and 900/1000 rpm.
As of 2009, the largest medium-speed engines in current production have outputs up to approximately . and are supplied by companies like MAN B&W, Wärtsilä, and Rolls-Royce (who acquired Ulstein Bergen Diesel in 1999). Most medium-speed engines produced are four-stroke machines, however there are some two-stroke medium-speed engines such as by EMD (Electro-Motive Diesel), and the Fairbanks Morse OP (Opposed-piston engine) type.
Typical cylinder bore size for medium-speed engines ranges from 20 cm to 50 cm, and engine configurations typically are offered ranging from in-line 4-cylinder units to V-configuration 20-cylinder units. Most larger medium-speed engines are started with compressed air direct on pistons, using an air distributor, as opposed to a pneumatic starting motor acting on the flywheel, which tends to be used for smaller engines. There is no definitive engine size cut-off point for this.
It should also be noted that most major manufacturers of medium-speed engines make natural gas-fueled versions of their diesel engines, which in fact operate on the Otto cycle, and require spark ignition, typically provided with a spark plug. There are also dual (diesel/natural gas/coal gas) fuel versions of medium and low speed diesel engines using a lean fuel air mixture and a small injection of diesel fuel (so-called "pilot fuel") for ignition. In case of a gas supply failure or maximum power demand these engines will instantly switch back to full diesel fuel operation.
Low-speed engines.
Also known as "slow-speed", or traditionally "oil engines", the largest diesel engines are primarily used to power ships, although there are a few land-based power generation units as well. These extremely large two-stroke engines have power outputs up to approximately , operate in the range from approximately 60 to 200 rpm and are up to tall, and can weigh over . They typically use direct injection running on cheap low-grade heavy fuel, also known as bunker C fuel, which requires heating in the ship for tanking and before injection due to the fuel's high viscosity. Often, the waste heat recovery steam boilers attached to the engine exhaust ducting generate the heat required for fuel heating. Provided the heavy fuel system is kept warm and circulating, engines can be started and stopped on heavy fuel.
Large and medium marine engines are started with compressed air directly applied to the pistons. Air is applied to cylinders to start the engine forwards or backwards because they are normally directly connected to the propeller without clutch or gearbox, and to provide reverse propulsion either the engine must be run backwards or the ship will use an adjustable propeller. At least three cylinders are required with two-stroke engines and at least six cylinders with four-stroke engines to provide torque every 120 degrees.
Companies such as MAN B&W Diesel, (formerly Burmeister & Wain) and Wärtsilä (which acquired Sulzer Diesel) design such large low-speed engines. They are unusually narrow and tall due to the addition of a crosshead bearing. As of 2007, the 14-cylinder Wärtsilä-Sulzer 14RTFLEX96-C turbocharged two-stroke diesel engine built by Wärtsilä licensee Doosan in Korea is the most powerful diesel engine put into service, with a cylinder bore of delivering . It was put into service in September 2006, aboard the world's largest container ship "Emma Maersk" which belongs to the A.P. Moller-Maersk Group. Typical bore size for low-speed engines ranges from approximately . As of 2008, all produced low-speed engines with crosshead bearings are in-line configurations; no Vee versions have been produced.
Current and future developments.
As of 2008, many common rail and unit injection systems already employ new injectors using stacked piezoelectric wafers in lieu of a solenoid, giving finer control of the injection event.
Variable geometry turbochargers have flexible vanes, which move and let more air into the engine depending on load. This technology increases both performance and fuel economy. Boost lag is reduced as turbo impeller inertia is compensated for.
Accelerometer pilot control (APC) uses an accelerometer to provide feedback on the engine's level of noise and vibration and thus instruct the ECU to inject the minimum amount of fuel that will produce quiet combustion and still provide the required power (especially while idling).
The next generation of common rail diesels is expected to use variable injection geometry, which allows the amount of fuel injected to be varied over a wider range, and variable valve timing (see Mitsubishi's 4N13 diesel engine) similar to that of petrol engines. Particularly in the United States, coming tougher emissions regulations present a considerable challenge to diesel engine manufacturers. Ford's HyTrans Project has developed a system which starts the ignition in 400 ms, saving a significant amount of fuel on city routes, and there are other methods to achieve even more efficient combustion, such as homogeneous charge compression ignition, being studied.
Japanese and Swedish vehicle manufacturers are also developing diesel engines that run on dimethyl ether (DME).
Some recent diesel engine models utilize a copper alloy heat exchanger technology (CuproBraze) to take advantage of benefits in terms of thermal performance, heat transfer efficiency, strength/durability, corrosion resistance, and reduced emissions from higher operating temperatures.
Low heat rejection engines.
A special class of experimental prototype internal combustion piston engines have been developed over several decades with the goal of improving efficiency by reducing heat loss. These engines are variously called adiabatic engines, due to better approximation of adiabatic expansion, low heat rejection engines, or high temperature engines. They are generally piston engines with combustion chamber parts lined with ceramic thermal barrier coatings. Some make used of titanium pistons and other titanium parts due to its low thermal conductivity and mass. Some designs are able to eliminate the use of a cooling system and associated parasitic losses altogether. Developing lubricants able to withstand the higher temperatures involved has been a major barrier to commercialization. Since diesel engines are largely immune to premature ignition they are generally better suited than Otto engines.

</doc>
<doc id="8541" url="http://en.wikipedia.org/wiki?curid=8541" title="Dark Star">
Dark Star

Dark Star or Darkstar may refer to:

</doc>
<doc id="8544" url="http://en.wikipedia.org/wiki?curid=8544" title="Drawing">
Drawing

Drawing is a form of visual art that makes use of any number of drawing instruments to mark a two-dimensional medium. Instruments used include graphite pencils, pen and ink, inked brushes, wax color pencils, crayons, charcoal, chalk, pastels, various kinds of erasers, markers, styluses, various metals (such as silverpoint) and electronic drawing.
An artist who practices or works in technical drawing may be called a "drafter" or "draftsman" or "draughtsman".
A small amount of material is released onto a surface, leaving a visible mark. The most common support for drawing is paper, although other materials, such as cardboard, plastic, leather, canvas, and board, may be used. Temporary drawings may be made on a blackboard or whiteboard or indeed almost anything. The medium has been a popular and fundamental means of public expression throughout human history. It is one of the simplest and most efficient means of communicating visual ideas. The wide availability of drawing instruments makes drawing one of the most common artistic activities.
Overview.
Drawing is one of the major forms of expression within the visual arts. It is generally concerned with the marking of lines and areas of tone onto paper, where the accurate representation of the visual world is expressed upon a plane surface. Traditional drawings were monochrome, or at least had little colour, while modern colored-pencil drawings may approach or cross a boundary between drawing and painting. In Western terminology, drawing is distinct from painting, even though similar media often are employed in both tasks. Dry media, normally associated with drawing, such as chalk, may be used in pastel paintings. Drawing may be done with a liquid medium, applied with brushes or pens. Similar supports likewise can serve both: painting generally involves the application of liquid paint onto prepared canvas or panels, but sometimes an underdrawing is drawn first on that same support.
Drawing is often exploratory, with considerable emphasis on observation, problem-solving and composition. Drawing is also regularly used in preparation for a painting, further obfuscating their distinction. Drawings created for these purposes are called studies.
There are several categories of drawing, including figure drawing, cartooning, doodling and shading. There are also many drawing methods, such as line drawing, stippling, shading, the surrealist method of entopic graphomania (in which dots are made at the sites of impurities in a blank sheet of paper, and lines are then made between the dots), and tracing (drawing on a translucent paper, such as "tracing paper", around the outline of preexisting shapes that show through the paper).
A quick, unrefined drawing may be called a "sketch".
In fields outside art, technical drawings or plans of buildings, machinery, circuitry and other things are often called "drawings" even when they have been transferred to another medium by printing.
History.
Drawing as a Form of Communication
Drawing is one of the oldest forms of human expression, with evidence for its existence preceding that of written communication. It is believed that drawing was used as a specialised form of communication before the invent of the written language, demonstrated by the production of cave and rock paintings created by Homo sapien sapiens around 30,000 years ago. These drawings, known as pictograms, depicted objects and abstract concepts. The sketches and paintings produced in prehistoric times were eventually stylised and simplified, leading to the development of the written language as we know it today.
Drawing in the Arts
Drawing is used to express one's creativity, and therefore has been prominent in the world of art. Throughout much of history, drawing was regarded as the foundation for artistic practise. Initially, artists used and re-used wooden tablets for the production of their drawings. Following the widespread availability of paper in the 14th century, the use drawing in the arts increased. At this point, drawing was commonly used as a tool for thought and investigation, acting as a study medium whilst artists were preparing for their final pieces of work. In a period of artistic flourish, the Renaissance brought about drawings exhibiting realistic representational qualities, where there was a lot of influence from geometry and philosophy.
The invention of the first widely available form of photography led to a shift in the use of drawing in the arts. Photography took over from drawing as a more superior method for accurately representing the visual world. Following the invention of photography, artists began to abandon traditional drawing practises. Modernism in the arts encouraged 'imaginative originality' and artists became more abstract in their approach to drawing.
Drawing Outside of the Arts
Although the use of drawing is extensive in the arts, its practice is not confined purely to this field. Before the widespread availability of paper, 12th century monks in European monasteries used intricate drawings to prepare illustrated, illuminated manuscripts on vellum and parchment. Drawing has also been used extensively in the field of science, as a method of discovery, understanding and explanation. In 1616, astronomer Galileo Galilei explained the changing phases of the moon through his observational telescopic drawings. Additionally, in 1924, geophysicist Alfred Wegener used illustrations to visually demonstrate the origin of the continents.
Notable draftsmen.
Since the 14th century, each century has produced artists who have created great drawings.
Materials.
The medium is the means by which ink, pigment or color are delivered onto the drawing surface. Most drawing media are either dry (e.g. graphite, charcoal, pastels, Conté, silverpoint), or use a fluid solvent or carrier (marker, pen and ink). Watercolor pencils can be used dry like ordinary pencils, then moistened with a wet brush to get various painterly effects. Very rarely, artists have drawn with (usually decoded) invisible ink. Metalpoint drawing usually employs either of two metals: silver or lead. More rarely used are gold, platinum, copper, brass, bronze, and tinpoint.
Paper comes in a variety of different sizes and qualities, ranging from newspaper grade up to high quality and relatively expensive paper sold as individual sheets. Papers can vary in texture, hue, acidity, and strength when wet. Smooth paper is good for rendering fine detail, but a more "toothy" paper will hold the drawing material better. Thus a coarser material is useful for producing deeper contrast.
Newsprint and typing paper may be useful for practice and rough sketches. Tracing paper is used to experiment over a half-finished drawing, and to transfer a design from one sheet to another. Cartridge paper is the basic type of drawing paper sold in pads. Bristol board and even heavier acid-free boards, frequently with smooth finishes, are used for drawing fine detail and do not distort when wet media (ink, washes) are applied. Vellum is extremely smooth and suitable for very fine detail. Coldpressed watercolor paper may be favored for ink drawing due to its texture.
Acid-free, archival quality paper keeps its color and texture far longer than wood pulp based paper such as newsprint, which will turn yellow and become brittle much sooner.
The basic tools are a drawing board or table, pencil sharpener and eraser, and for ink drawing, blotting paper. Other tools used are circle compass, ruler, and set square. Fixative is used to prevent pencil and crayon marks from smudging. Drafting tape is used to secure paper to drawing surface, and also to mask an area to keep it free of accidental marks sprayed or spattered materials and washes. An easel or slanted table is used to keep the drawing surface in a suitable position, which is generally more horizontal than the position used in painting.
Technique.
Almost all draftsmen use their hands and fingers to apply the media, with the exception of some handicapped individuals who draw with their mouth or feet.
Prior to working on an image, the artist will likely want to gain an understanding of how the various media will work. The different drawing implements can be tried on practice sheets in order to determine value and texture, and how to apply the implement in order to produce various effects.
The drawing strokes used control the appearance of the image. Pen and ink drawings often use hatching, which consists of groups of parallel lines. Cross-hatching uses hatching in two or more different directions to create a darker tone. Broken hatching, or lines with intermittent breaks, can be used to form lighter tones, and by controlling the density of the breaks a gradation of tone can be achieved. Stippling, uses dots to produce tone, texture or shade. Different textures can be achieved depending on the method used to build tone.
Drawings in dry media often use similar techniques, although with pencils and drawing sticks continuous variations in tone can be achieved. Typically a drawing will be filled in based on which hand the artist favors. A right-handed artist will want to draw from left to right in order to avoid smearing the image. Erasers can be used with many media to remove unwanted lines, lighten tones and clean up stray marks.
In a sketch or outline drawing, the lines drawn often follow the contour of the subject being drawn, creating depth by looking like shadows cast from a light in the artist's position.
Sometimes the artist will want to leave a section of the image untouched while filling in the remainder of the picture. The shape of the area to be preserved can be painted on with masking fluid or cut out of a frisket and applied to the drawing surface, protecting the surface from stray marks until the mask is removed.
Another method to preserve a section of the image is to apply a spray-on "fixative" to the surface. This will hold loose material more firmly to the sheet and prevent it from smearing. However the fixative spray typically uses chemicals that can harm the respiratory system, so it should be employed in a well-ventilated area such as outdoors.
Another technique is subtractive drawing in which the drawing surface is covered with graphite or charcoal and then erased to make the image.
Tone.
Shading is the technique of varying the tonal values on the paper to represent the shade of the material as well as the placement of the shadows. Careful attention to reflected light, shadows and highlights can result in a very realistic rendition of the image.
Blending uses an implement to soften or spread the original drawing strokes. Blending is most easily done with a medium that does not immediately fix itself, such as graphite, chalk, or charcoal, although freshly applied ink can be smudged, wet or dry, for some effects. For shading and blending, the artist can use a blending stump, tissue, a kneaded eraser, a fingertip, or any combination of them. A piece of chamois is useful for creating smooth textures, and for removing material to lighten the tone. Continuous tone can be achieved with graphite on a smooth surface without blending, but the technique is laborious, involving small circular or oval strokes with a somewhat blunt point.
Shading techniques that also introduce texture to the drawing include hatching and stippling. There are a number of other methods for producing texture in the picture: in addition to choosing a suitable paper, the type of drawing material and the drawing technique will result in different textures. Texture can be made to appear more realistic when it is drawn next to a contrasting texture; a coarse texture will be more obvious when placed next to a smoothly blended area. A similar effect can be achieved by drawing different tones close together; a light edge next to a dark background will stand out to the eye, and almost appear to float above the surface.
Form and proportion.
Measuring the dimensions of a subject while blocking in the drawing is an important step in producing a realistic rendition of the subject. Tools such as a compass can be used to measure the angles of different sides. These angles can be reproduced on the drawing surface and then rechecked to make sure they are accurate. Another form of measurement is to compare the relative sizes of different parts of the subject with each other. A finger placed at a point along the drawing implement can be used to compare that dimension with other parts of the image. A ruler can be used both as a straightedge and a device to compute proportions.
When attempting to draw a complicated shape such as a human figure, it is helpful at first to represent the form with a set of primitive shapes. Almost any form can be represented by some combination of the cube, sphere, cylinder, and cone. Once these basic shapes have been assembled into a likeness, then the drawing can be refined into a more accurate and polished form. The lines of the primitive shapes are removed and replaced by the final likeness. Drawing the underlying construction is a fundamental skill for representational art and is taught in many books and schools, as its correct application will resolve most uncertainties about smaller details and make the final image look self-consistent.
A more refined art of figure drawing relies upon the artist possessing a deep understanding of anatomy and the human proportions. A trained artist is familiar with the skeleton structure, joint location, muscle placement, tendon movement, and how the different parts work together during movement. This allows the artist to render more natural poses that do not appear artificially stiff. The artist is also familiar with how the proportions vary depending on the age of the subject, particularly when drawing a portrait.
Perspective.
Linear perspective is a method of portraying objects on a flat surface so that the dimensions shrink with distance. Each set of parallel, straight edges of any object, whether a building or a table, will follow lines that eventually converge at a vanishing point. Typically this point of convergence will be along the horizon, as buildings are built level with the flat surface. When multiple structures are aligned with each other, such as buildings along a street, the horizontal tops and bottoms of the structures will all typically converge at a vanishing point.
When both the fronts and sides of a building are drawn, then the parallel lines forming a side converge at a second point along the horizon (which may be off the drawing paper.) This is a two-point perspective. Converging the vertical lines to a third point above or below the horizon then produces a three-point perspective.
Depth can also be portrayed by several techniques in addition to the perspective approach above. Objects of similar size should appear ever smaller the further they are from the viewer. Thus the back wheel of a cart will appear slightly smaller than the front wheel. Depth can be portrayed through the use of texture. As the texture of an object gets further away it becomes more compressed and busy, taking on an entirely different character than if it was close. Depth can also be portrayed by reducing the contrast in more distant objects, and by making their colors less saturated. This will reproduce the effect of atmospheric haze, and cause the eye to focus primarily on objects drawn in the foreground.
Artistry.
The composition of the image is an important element in producing an interesting work of artistic merit. The artist plans the placement of elements in the art in order to communicate ideas and feelings with the viewer. The composition can determine the focus of the art, and result in a harmonious whole that is aesthetically appealing and stimulating.
The illumination of the subject is also a key element in creating an artistic piece, and the interplay of light and shadow is a valuable method in the artist's toolbox. The placement of the light sources can make a considerable difference in the type of message that is being presented. Multiple light sources can wash out any wrinkles in a person's face, for instance, and give a more youthful appearance. In contrast, a single light source, such as harsh daylight, can serve to highlight any texture or interesting features.
When drawing an object or figure, the skilled artist pays attention to both the area within the silhouette and what lies outside. The exterior is termed the negative space, and can be as important in the representation as the figure. Objects placed in the background of the figure should appear properly placed wherever they can be viewed.
A study is a draft drawing that is made in preparation for a planned final image. Studies can be used to determine the appearances of specific parts of the completed image, or for experimenting with the best approach for accomplishing the end goal. However a well-crafted study can be a piece of art in its own right, and many hours of careful work can go into completing a study.
The Drawing Process.
Individuals display differences in their ability to produce visually accurate drawings. A visually accurate drawing is described as being "recognized as a particular object at a particular time and in a particular space, rendered with little addition of visual detail that can not be seen in the object represented or with little deletion of visual detail”.
Investigative studies have aimed to explain the reasons why some individuals are better at drawing than others. 
A study explained that the perception of objects being drawn, the ability to make good representational decisions, the motor skills required for mark making and the perception of one’s drawing were the four stages involved in the process of drawing. Following this explanation, several studies have been conducted to conclude which of these processes are the most significant in affecting the accuracy of drawings.
Motor Function
Motor function has been recognised as an important physical component in the 'Production Phase' of the drawing process. It has been suggested that motor function plays a role in drawing ability, although its effects are not significant.
Perception
It has been suggested that an individual's ability to perceive an object they are drawing is the most important stage in the drawing process. This suggestion is supported by the discovery of a robust relationship between perception and drawing ability.
This evidence acted at the basis of Betty Edwards' how-to drawing book, Drawing on the Right Side of the Brain. Edwards aimed to teach her readers how to draw, based on the development of the reader's perceptual abilities.
Furthermore, the influential artist and art critic John Ruskin emphasised the importance perception in the drawing process in his book The Elements of Drawing. He stated that "For I am nearly convinced, that once we see keenly enough, there is very little difficult in drawing what we see".
Visual Memory
Visual memory has also been shown to influence one's ability to create visually accurate drawings. 
Short-term memory plays an important part in drawing as one’s gaze shifts between the object they are drawing and the drawing itself.

</doc>
<doc id="8545" url="http://en.wikipedia.org/wiki?curid=8545" title="Dedham, Massachusetts">
Dedham, Massachusetts

Dedham is a town in and the county seat of Norfolk County, Massachusetts, United States. The population was 24,729 at the 2010 census. It is located on Boston's southwest border. On the northwest it is bordered by Needham, on the southwest by Westwood and on the southeast by Canton.
History.
Settled in 1635 by people from Roxbury and Watertown, Dedham was incorporated in 1636. It became the county seat of Norfolk County when the county formed on March 26, 1793. When the Town was originally incorporated, the residents wanted to name it Contentment. The Massachusetts General Court overruled them and named the town after Dedham, Essex in England, where some of the original inhabitants were born.
At the first public meeting on August 15, 1636, eighteen men signed the town covenant. They swore that they would "in the fear and reverence of our Almighty God, mutually and severally promise amongst ourselves and each to profess and practice one truth according to that most perfect rule, the foundation whereof is ever lasting love."
They also agreed that "we shall by all means labor to keep off from us all such as are contrary minded, and receive only such unto us as may be probably of one heart with us, [and such] as that we either know or may well and truly be informed to walk in a peaceable conversation with all meekness of spirit, [this] for the edification of each other in the knowledge and faith of the Lord Jesus…" The covenant also stipulated that if differences were to arise between townsmen, they would seek arbitration for resolution and each would pay his fair share for the common good.
In November 1798, David Brown led a group in Dedham protesting the federal government; they set up a liberty pole, as people had before the American Revolution. It carried the words, "No Stamp Act, No Sedition Act, No Alien Bills, No Land Tax, downfall to the Tyrants of America; peace and retirement to the President; Long Live the Vice President," referring to then-President John Adams and Vice President Thomas Jefferson. Brown was arrested in Andover but because he could not afford the $4,000 bail, he was taken to Salem for trial. Brown was tried in June 1799. Although he wanted to plead guilty, Justice Samuel Chase urged him to name those who had helped him or subscribed to his writings in exchange for freedom. Brown refused, was fined $480, and sentenced to eighteen months in prison. It was the most severe sentence up to then imposed under the Alien and Sedition Acts.
Dedham is home to the Fairbanks House, the oldest surviving timber-frame house in the United States, scientifically dated to 1637. On January 1, 1643, by unanimous vote, Dedham authorized the first taxpayer-funded public school, "the seed of American education." Its first teacher, Rev. Ralph Wheelock, was paid 20 pounds annually to instruct the youth of the community. Descendants of these students would become presidents of Dartmouth College, Yale University and Harvard University.
The first man-made canal in North America, Mother Brook, was created in Dedham in 1639. It linked the Charles River to the Neponset River. Although both are slow-moving rivers, they are at different elevations. The difference in elevation made the canal's current swift enough to power several local mills.
In 1818, though citizens were still taxed for the support of ministers and other "public teachers of religion", Dedham set a precedent toward the separation of church and state. Residents selected a minister different than that chosen by the church selectmen; their right of selection was confirmed by the Supreme Judicial Court. The shift in power to the congregation led to the rise of the Congregational Churches.
The local Endicott Estate burned to the ground in 1904 after the local volunteer fire department, responding to three separate fires burning simultaneously, reached the Endicott fire last. By the time they arrived, only ashes remained. It is said that the estate's owner, Henry Bradford Endicott (also founder of the Endicott Johnson Corporation) took the burning of the homestead as a divine command to rebuild (which he did). The rebuilt Endicott Estate is listed on the National Register of Historic Places. The estate and surrounding grounds are open to the public, upholding Henry's stepdaughter Katherine's wish to use the house and property for "educational, civic, social and recreational purposes."
In 1921, the historic Sacco and Vanzetti trial was held in the Norfolk County Courthouse in Dedham.
Dedham Pottery is a cherished class of antiques, characterized by a distinctive crackle glaze, blue-and-white color scheme, and a frequent motif of rabbits and other animals. Dedham is sometimes called the "mother of towns" because 14 present-day communities were included within its original broad borders.
Geography.
Dedham is located at (42.244609, −71.165531). On the northeast corner of High Street and Court Street the U.S. Coast & Geodetic Survey, now the U.S. National Geodetic Survey, has placed a small medallion into a granite block showing an elevation of .
Dedham is made up of a number of neighborhoods:
According to the United States Census Bureau, the town has a total area of , of which, of it is land and of it (1.79%) is water.
Demographics.
As of the census of 2000, there were 23,464 people, 8,654 households, and 6,144 families residing in the town. The population density was 2,244.6 people per square mile (866.9/km²). There were 8,908 housing units at an average density of 852.2 per square mile (329.1/km²). The racial makeup of the town was 94.51% White, 1.54% Black or African American, 0.16% Native American, 1.87% Asian, 0.04% Pacific Islander, 0.80% from other races, and 1.08% from two or more races. 2.42% of the population were Hispanic or Latino of any race.
There are 8,654 households, of which 30.1% have children under the age of 18 living with them. 56.3% were married couples living together, 11.1% had a female householder with no husband present, and 29.0% were non-families. 23.9% of all households were made up of individuals and 10.4% had someone living alone who was 65 years of age or older. The average household size was 2.61 and the average family size was 3.14.
Dedham's population is spread out with 22.2% under the age of 18, 5.8% from 18 to 24, 31.1% from 25 to 44, 24.2% from 45 to 64, and 16.6% who were 65 years of age or older. The median age was 40 years. For every 100 females there were 93.4 males. For every 100 females age 18 and over, there were 92.0 males.
The median income for a household in the town was $61,699, and the median income for a family was $72,330. Males had a median income of $46,216 versus $35,682 for females. The per capita income for the town was $28,199. About 3.2% of families and 4.6% of the population were below the poverty line, including 3.9% of those under age 18 and 6.5% of those age 65 or over.
Seal and flag.
The town's seal has several features. In the center is a crest containing the Old Avery Oak. When the tree was finally felled the gavel used by the Moderator at Town Meeting was carved out of it. Above the tree are the scales of justice, representing Dedham as the county seat and home to Norfolk County's courts. On the left of the tree are agricultural instruments and on the right is a factory, showing Dedham's history first as a town of farmers and then a one with a number of mills and factories, particularly along Mother Brook. Below the tree is a banner with the word "Contentment."
The town flag is red with the seal prominent and in the center. In the lower left corner is part of the Avery Oak and in the lower right is part of the Fairbanks House. It hangs in the selectmen's chambers at town hall and in the Great Hall of the Massachusetts State House.
Government.
A charter adopted in 1998 lays out the basic structure of the Town government, although it has been amended occasionally over the years. A seven member Charter Advisory Committee, appointed in 2012, recommended six substantial changes and numerous minor changes be made to the document. The Selectmen consolidated them into six articles for Town Meeting's consideration, and five were presented to the Meeting in 2013. Voters approved four of them in 2014. A version of the sixth and final proposal was adopted at the Spring 2014 Annual Town Meeting.
Town Meeting.
According to Dedham's , the "administration of all the fiscal, prudential, and municipal affairs of the town, with the government thereof, shall be vested in a legislative branch, to consist of a representative town meeting." Town Meeting is to consist of no less than 270 members, but not more than necessary to achieve an equal number coming from each precinct. There are currently seven districts, but could be as few as six or as many as nine, with lines drawn by the Board of Selectmen and the Registrars of Voters every ten years.
Votes are by voice unless members call for a standing or roll call vote, either of which can be called for by the Moderator. All Town officers are required to attend Town Meeting and multiple member bodies must send at least one representative who have all the privileges of a Member except the right to vote. If 5% of Town voters petition the Board of Selectmen within 14 days of Town Meeting any action taken may be submitted to voters. The final result is to be determined by majority vote, but Town Meeting can not be overruled unless 20% of registered voters participate.
Town Meeting sets its own rules and keeps a journal of proceedings. The Town Meeting may establish various ad-hoc and standing committees on which any Town Meeting Member or voter may serve.
Town Meeting members.
Currently Town Meeting consists of 273 members, or representatives, with each of the seven districts, or precincts, electing 39. Thirteen are elected from each precinct each year and serve a three-year term. Each precinct elects from its own members a Chairman, Vice Chairman, and Secretary.
To be eligible, candidates must have 10 registered voters from their precinct sign nomination papers. Town Meeting Members can not serve on any other elected board or on the Finance Committee. Members who move from the district or are removed by redistricting may serve until the next Town Election, however any member who moves out of the Town immediately ceases to be a Member.
In case of a vacancy, the remaining term is to be filled at the next town election. If no election is to take place within 120 days of the vacancy then the district chairman is to call together the members of the district and they are to elect a member who will serve until the next town election.
Warrant.
The Warrant at Town Meeting includes the articles to be voted on. Any elected or appointed board, committee, town officer, or any ten voters may place an article on the warrant. Each article to be voted on is directed by the Board of Selectmen to an appropriate board or committee to hear and provide the original motion at Town Meeting. All articles expending funds are directed to the Finance Committee; articles dealing with planning and zoning to the Planning Board; articles relating to by-laws to the By-Law Committee. The Finance Committee recommendation has the force of the original motion on all articles except those related to zoning. The Planning Board makes the original motion for those.
Mini Town Meeting.
While it is not called for in the Charter, there is a tradition in Dedham for the Chairmen of the several districts to elect from amongst themselves a chairman. This Chairman of the Chairmen hosts what is officially known as the District Chairmen's Warrant Review Meeting but is much more commonly referred to as Mini Town Meeting. The "Mini" is generally one week before the actual Town Meeting. The purpose of the Mini is to air out several of the contentious issues before bringing them to the floor of Town Meeting.
Board of Selectmen.
The executive branch of the Town Government is "headed" by a Board of Selectmen. The Board of Selectmen have five members who are elected for three-year terms and are the chief policy making body for the town. They appoint a Town Manager who runs the day-to-day affairs of the Town. They also appoint constables, registrars of voters and other election officers, the board of appeals, conservation commission, historic district commission, and members of several other multiple member boards.
They set policy for all departments below it, but are not involved in the day-to-day affairs of the Town. They issue licenses and can investigate the affairs and the conduct of any town agency.
Town Clerk.
The Elected Town Clerk serves a three-year term and works full-time for the Town. The Clerk is "the keeper of vital statistics of the town and the custodian of the town seal and all public records, administer[s] the oaths of office to all town officers... [and is] the clerk of the town meeting." In the role as clerk of town meeting he notifies the public and members of the Town Meeting and keeps a verbatim record of proceedings.
Town Moderator.
Town Meetings are presided over by the Town Moderator, but he has no vote unless all the Members present and voting are equally divided. At the first Town Meeting following the annual town election he is to appoint, subject to Town Meeting's confirmation, a Deputy Moderator from the elected Members. The Deputy serves in case of the Moderator's absence or disability.
Other boards and committees.
The seven members of the School Committee are elected for three-year terms and appoint a Superintendent of Schools. They also set policy for the School Department.
The three elected members of the Board of Assessors serve three-year terms and annually make a fair cash valuation of all property within the town.
The three elected members of the Board of Health are responsible for the formulation and enforcement of rules and regulations affecting the environment and the public health.
The Board of Library Trustees has five members, each of whom serve three-year terms, and have care of the Town's . They are responsible for all library policy, the library budget, and hiring and firing the library director.
The five elected members of the Planning Board make studies and prepare plans concerning the resources, possibilities and needs of the town. It also prepares the Master Plan.
There are five elected Commissioners of the Trust Funds who manage and control all funds left, given, bequeathed or devised to the town, and distribute the income in accordance with the terms of the respective trusts.
There are five members of the Housing Authority. Four are elected by the Town and one is appointed by the Commonwealth Commissioner of Community Affairs. As a Board they have all of the powers and duties which are given to housing authorities under the constitution and laws of the Commonwealth.
Economy.
The theater company and Viacom/ CBS Corporation parent National Amusements is based in Dedham.
Dedham is also home to The Norfolk & Dedham Group, a regional mutual insurance company.
Education.
Dedham has seven public schools, and is known for the first implementation of a tax supported, free public school system, now used nationally.
Community organizations.
Dedham is home to a number of community organizations, including
Places of worship.
Boston United Hand in Hand Cemetery is located on Lower East Street straddling the
West Roxbury line. Dating back to 1875, the original plot was full by 1896 but subsequently expanded multiple times. There are graves as recent as 1980 in the West Roxbury portion; the Dedham portion is still active. Chestnut Hill's Congregation Mishka Tefila currently owns the property.
Transportation.
Commuter rail service from Boston's South Station is provided by the MBTA with stops at Endicott and
Dedham Corporate Center on its Franklin Line. Also MBTA Bus route 34 Dedham Line to Forest Hills serves Washington Street. Bus route 34E Walpole Center to Forest Hills serves Washington Street, Dedham Square, and the Dedham Mall. Bus route 35 Dedham Mall to Forest Hills serves Washington Street.

</doc>
<doc id="8547" url="http://en.wikipedia.org/wiki?curid=8547" title="Book of Deuteronomy">
Book of Deuteronomy

The Book of Deuteronomy (from Greek Δευτερονόμιον, "Deuteronomion", "second law"; , "Devarim", "[spoken] words") is the fifth book of the Hebrew Bible, and of the Jewish Torah. The Hebrew title is taken from the opening phrase "Eleh ha-devarim", "These are the words..."; the English title is from a Greek mis-translation of the Hebrew phrase "mishneh ha-torah ha-zoth", "a copy of this law", in , as "to deuteronomion touto" – "this second law".
The book consists of three sermons or speeches delivered to the Israelites by Moses on the plains of Moab, shortly before they enter the Promised Land. The first sermon recapitulates the forty years of wilderness wanderings which have led to this moment, and ends with an exhortation to observe the law (or teachings), later referred to as the Law of Moses; the second reminds the Israelites of the need for exclusive allegiance to one God and observance of the laws (or teachings) he has given them, on which their possession of the land depends; and the third offers the comfort that even should Israel prove unfaithful and so lose the land, with repentance all can be restored.
Traditionally seen as the words of Moses delivered before the conquest of Canaan, the modern scholarly consensus sees its origins in traditions from Israel (the northern kingdom) brought south to the Kingdom of Judah in the wake of the Assyrian destruction of Samaria (8th century BC) and then adapted to a program of nationalist reform in the time of King Josiah (late 7th century), with the final form of the modern book emerging in the milieu of the return from the Babylonian exile during the late 6th century.
One of its most significant verses is , the Shema, which has become the definitive statement of Jewish identity: "Hear, O Israel: the our God, the is one." Verses 6:4–5 were also quoted by Jesus in as part of the Great Commandment.
Contents.
Structure.
Patrick D. Miller in his commentary on Deuteronomy suggests that different views of the structure of the book will lead to different views on what it is about. The structure is often described as a series of three speeches or sermons (chapters 1:1–4:43, 4:44–29:1, 29:2–30:20) followed by a number of short appendices – Miller refers to this as the "literary" structure; alternatively, it is sometimes seen as a ring-structure with a central core (chapters 12–26, the Deuteronomic code) and an inner and an outer frame (chapters 4–11/27–30 and 1–3/31–34) – Miller calls this the covenantal substructure; and finally the theological structure revealed in the theme of the exclusive worship of Yahweh established in the first of the Ten Commandments ("Thou shalt have no other god before me") and the shema ("Hear O Israel, the our God is One!")
Summary.
"(The following "literary" outline of Deuteronomy is from John Van Seters; it can be contrasted with Alexander Rofé's "covenantal" analysis in his "Deuteronomy: Issues and Interpretation".)"
The final verses, Deuteronomy 34:10–12, "never again did there arise in Israel a prophet like Moses," make a claim for the authoritative the Deuteronomistic view of theology and its insistence that the worship of the Hebrew God as the sole deity of Israel was the only permissible religion, having been sealed by the greatest of prophets.
Deuteronomic code.
, the Deuteronomic Code, is its oldest part of the book and the core around which the rest developed. It is a series of mitzvot ("commands") to the Israelites regarding how they ought to conduct themselves in Canaan, the land promised by Yahweh, God of Israel. The following list organizes most of the laws into thematic groups:
Composition.
Composition history.
Since the evidence was first put forward by W.M.L de Wette in 1805, scholars have accepted that the core of Deuteronomy was composed in Jerusalem in the 7th century BC in the context of religious reforms advanced by King Josiah (reigned 641–609 BC). A broad consensus exists that sees its history in the following general terms:
Sources.
The prophet Isaiah, active in Jerusalem about a century before Josiah, makes no mention of the Exodus, covenants with God, or disobedience to God's laws; in contrast Isaiah's contemporary Hosea, active in the northern kingdom of Israel, makes frequent reference to the Exodus, the wilderness wanderings, a covenant, the danger of foreign gods and the need to worship Yahweh alone; this has led scholars to the view that these traditions behind Deuteronomy have a northern origin. Whether the Deuteronomic code – the set of laws at chapters 12–26 which form the original core of the book – was written in Josiah's time (late 7th century) or earlier is subject to debate, but many of the individual laws are older than the collection itself. The two poems at chapters 32–33 – the Song of Moses and the Blessing of Moses were probably originally independent.
Position in the Hebrew Bible.
Deuteronomy occupies a puzzling position in the Bible, linking the story of the Israelites' wanderings in the wilderness to the story of their history in Canaan without quite belonging totally to either. The wilderness story could end quite easily with Numbers, and the story of Joshua's conquests could exist without it, at least at the level of the plot; but in both cases there would be a thematic (theological) element missing. Scholars have given various answers to the problem. The Deuteronomistic history theory is currently the most popular (Deuteronomy was originally just the law code and covenant, written to cement the religious reforms of Josiah, and later expanded to stand as the introduction to the full history); but there is an older theory which sees Deuteronomy as belonging to Numbers, and Joshua as a sort of supplement to it. This idea still has supporters, but the mainstream understanding is that Deuteronomy, after becoming the introduction to the history, was later detached from it and included with Genesis-Exodus-Leviticus-Numbers because it already had Moses as its central character. According to this hypothesis, the death of Moses was originally the ending of Numbers, and was simply moved from there to the end of Deuteronomy.
Themes.
Overview.
Deuteronomy stresses the uniqueness of God, the need for drastic centralisation of worship, and a concern for the position of the poor and disadvantaged. Its many themes can be organised around the three poles of Israel, Israel's God, and the covenant which binds them together.
Israel.
The themes of Deuteronomy in relation to Israel are election, faithfulness, obedience, and God's promise of blessings, all expressed through the covenant: "obedience is not primarily a duty imposed by one party on another, but an expression of covenantal relationship." Yahweh has chosen ("elected") Israel as his special property (Deuteronomy 7:6 and elsewhere), and Moses stresses to the Israelites the need for obedience to God and covenant, and the consequences of unfaithfulness and disobedience. Yet the first several chapters of Deuteronomy are a long retelling of Israel's past disobedience – but also God's gracious care, leading to a long call to Israel to choose life over death and blessing over curse (chapters 7–11).
Dillard and Longman note that the centralization of worship is an important and repeated theme in Deuteronomy, and that this is designed to focus the hearer's attention on the unique and exclusive holiness of Yahweh.
God.
Deuteronomy's concept of God changed over time: the earliest 7th century layer is monolatrous, not denying the reality of other gods but enforcing the worship of Yahweh in Jerusalem alone; in the later, Exilic layers from the mid-6th century, especially chapter 4, this becomes monotheism, the idea that only one god exists. God is simultaneously present in the Temple and in heaven – an important and innovative concept called "name theology."
After the review of Israel's history in chapters 1 to 4, there is a restatement of the Decalogue in chapter 5. This arrangement of material highlights God's sovereign relationship with Israel prior to the giving of establishment of the Law. The Decalogue in turn then provides the foundational principles for the subsequent, more detailed laws. Some scholars go so far as to see a correlation between each of the laws of the Decalogue and each of the more detailed 'case-law' of the rest of the book. This foundational aspect of the Decalogue is also demonstrated by the emphasis to actively remember the law of God (), immediately after the Decalogue. The Law as it is broadly presented across Deuteronomy defines Israel both as a community and defines their relationship with Yahweh. There is throughout the law a sense of justice. For example the demand for multiple witness (), cities of refuge (), or the provision of judges ().
Covenant.
The core of Deuteronomy is the Biblical covenant which binds Yahweh and Israel by oaths of fidelity (Yahweh and Israel each faithful to the other) and obedience (Israel obedient to Yahweh).
God will give Israel blessings of the land, fertility, and prosperity so long as Israel is faithful to God's teaching; disobedience will lead to curses and punishment. But, according to the Deuteronomists, Israel's prime sin is lack of faith, apostasy: contrary to the first and fundamental commandment ("Thou shalt have no other gods before me") the people have entered into relations with other gods.
The covenant is based on 7th century Assyrian suzerain-vassal treaties by which the Great King (the Assyrian suzerain) regulated relationships with lesser rulers; Deuteronomy is thus making the claim that Yahweh, not the Assyrian monarch, is the Great King to whom Israel owes loyalty. The terms of the treaty are that Israel holds the land from Yahweh, but Israel's tenancy of the land is conditional on keeping the covenant, which in turn necessitates tempered rule by state and village leaders who keep the covenant: "These beliefs", says Norman Gottwald, "dubbed biblical Yahwism, are widely recognized in biblical scholarship as enshrined in Deuteronomy and the Deuteronomistic History (Joshua through Kings)."
Dillard and Longman in their "Introduction to the Old Testament" stress the living nature of the covenant between Yahweh and Israel as a nation: The people of Israel are addressed by Moses as a unity, and their allegiance to the covenant is not one of obeisance, but comes out of a pre-existing relationship between God and Israel, established with Abraham and attested to by the Exodus event, so that the laws of Deuteronomy set the nation of Israel apart, signaling the unique status of the Jewish nation. The land is God's gift to Israel, and many of the laws, festivals and instructions in Deuteronomy are given in the light of Israel's occupation of the land. Dillard and Longman note that "In 131 of the 167 times the verb "give" occurs in the book, the subject of the action is Yahweh." Deuteronomy makes the Torah the ultimate authority for Israel, one to which even the king is subject.
Influence on Judaism and Christianity.
Judaism.
Deuteronomy 6:4–5: "Hear ("shema"), O Israel, the is our God, the is one!" has become the basic credo of Judaism, and its twice-daily recitation is a mitzvah (religious commandment). The shema goes on: "Thou shalt love the thy God with all thy heart and all thy soul and all thy might"; it has therefore also become identified with the central Jewish concept of the love of God, and the rewards that come with this.
Christianity.
In the Gospel of Matthew, Jesus cited Deuteronomy 6:5 as a Great Commandment. The earliest Christian authors interpreted Deuteronomy's prophecy of the restoration of Israel as having been fulfilled (or superseded) in Jesus Christ and the establishment of the Christian Church (Luke 1–2, Acts 2–5), and Jesus was interpreted to be the "one (i.e., prophet) like me" predicted by Moses in Deuteronomy 18:15 (Acts 3:22–23). In place of the elaborate code of laws ("mitzvah") set out in Deuteronomy, Paul the Apostle, drawing on , claimed that the keeping of the Mosaic covenant was superseded by faith in Jesus and the gospel (the New Covenant).

</doc>
<doc id="8550" url="http://en.wikipedia.org/wiki?curid=8550" title="Down">
Down

Down may refer to:

</doc>
<doc id="8551" url="http://en.wikipedia.org/wiki?curid=8551" title="David">
David

David (; ; "Dawid"; ""; "Dawid"; ; Strong's: "Daveed") was, according to the Bible, the second king of the United Kingdom of Israel and Judah, and according to the New Testament Gospels of Matthew and Luke, an ancestor of Jesus. His life is conventionally dated to c. 1040–970 BC, his reign over Judah c. 1010–1002 BC, and his reign over the United Kingdom c. 1002–970 BC.
The Books of Samuel, 1 Kings, and 1 Chronicles are the only sources of information on David, although the Tel Dan Stele (dated c. 850–835 BC) contains the phrase ("Beit David"), read as "House of David", which most scholars take as confirmation of the existence in the mid-9th century BC of a Judean royal dynasty called the House of David.
He is depicted as a righteous king, although not without faults, as well as an acclaimed warrior, musician, and poet, traditionally credited for composing many of the psalms contained in the Book of Psalms.
David is an important figure to members of the Jewish, Christian and Islamic faiths. Biblical tradition maintains the Messiah's direct descent from the line of David. In Islam, he is considered a prophet.
Biblical narrative.
Saul rejected.
According to the Biblical narrative, God appointed Saul to be the first king of Israel, after the leading elders of the land demanded a king to replace the Judges who had previously ruled the country. Although successful at first, Saul quickly fell afoul of God by disobeying his instructions. He was told that God had "rejected" him from being king, and that he would give the kingdom instead to "a man after [my] own heart" who was "better than you."
The Bible next says that the prophet Samuel sought a new king from the sons of Jesse of Bethlehem. Samuel examined seven of Jesse's sons, but said to him, "The Lord has not chosen these.” Samuel inquired if Jesse had any other sons. "There is still the youngest", Jesse answered, referring to David. Samuel said, "Send for him," and had him brought in. Then the Lord said, "Rise and anoint him; this is the one." So Samuel took the horn of oil and anointed him.
At the court of Saul.
As punishment for his previous misdeeds, Saul was tormented by an "evil spirit from the Lord" () and it was suggested he send for David, a young warrior famed for bravery and his lyre playing. Saul did so, and made David one of his armor-bearers. From then on, whenever "the spirit from God came on Saul, David would take up his lyre and play. Then relief would come to Saul; he would feel better, and the evil spirit would leave him."
David and Goliath.
According to 1 Samuel 17, the 'men of Israel' under King Saul faced the Philistines near the Valley of Elah. David heard the Philistine giant Goliath challenge the Israelites to send their own champion to decide the outcome in single combat; Goliath used to regularly stand opposite the Israelite camp and shout insults concerning King Saul and the Israelite people. David told Saul he was prepared to face Goliath alone. David picked five smooth stones from a nearby brook, and struck Goliath in the forehead with a stone from his sling. Goliath fell dead, and David took Goliath's sword and beheaded him. The Philistines fled in terror. Saul inquired about the name of the young champion and David told him that he was the son of Jesse. In 2 Samuel 22, David credited God for delivering him from the hand of the Philistines and saving him from "the snares of death," in his psalm, "David’s Song of Praise."
In some translations states that Goliath was killed by Elhanan son of Jair from Bethlehem. "Most likely, storytellers displaced the deed from the otherwise obscure Elhanan onto the more famous character, David."
The King James version reads 1 Samuel 21:19 And there was again a battle in Gob with the Philistines, where Elhanan the son of Jaare–oregim, a Beth–lehemite, slew the brother of Goliath the Gittite, the staff of whose spear was like a weaver's beam.
David and Jonathan.
Saul made David a commander over his armies and offered him his daughter Michal in marriage for bringing 100 foreskins of the Philistines but David brought back 200, saying "God was with me". David was successful in many battles, and his popularity awakened Saul's fears. Saul tried to arrange for David's death, but the plots only endeared David further to the people, and especially to Saul's son Jonathan, who loved David (1 Samuel 18:1, 2 Samuel 1:25–26). Jonathan warned David, who fled into the wilderness, gathered a band of followers and became the champion of the oppressed while evading Saul's pursuit. He accepted the town of Ziklag from the Philistine king Achish of Gath, but continued secretly to champion the Israelites. Achish marched against Saul, but David was excused from the war after suspicion from Philistine nobles that his loyalty could not be trusted.
Proclaimed king.
Jonathan and Saul were killed in battle with the Philistines at Mount Gilboa. David mourned their deaths, especially that of Jonathan. He travelled to Hebron, where he was anointed king over Judah. In the north, Saul's son Ish-Bosheth was anointed by Abner as King of Israel. War ensued between Ish-Bosheth and David, until Ish-Bosheth was murdered. The assassins brought the head of Ish-Bosheth to David hoping for a reward, but David executed them for their crime. With the death of Saul's son, the elders of Israel came to Hebron and David was anointed King over Israel and Judah.
Jerusalem and the Davidic covenant.
David conquered the Jebusite fortress of Jerusalem, and made it his capital. David brought the Ark of the Covenant to Jerusalem, intending to build a temple. The prophet Nathan, announced that the temple would be built at a future date by one of David's sons (Solomon). Nathan told David that God had made a covenant with David, promising to establish the house of David: "Your throne shall be established forever."
David wins victories over the Philistines, and the Moabites and Hadadezer of Zobah paid tribute.
Bathsheba and Uriah the Hittite.
David committed adultery with Bathsheba, the wife of Uriah the Hittite. Bathsheba became pregnant. David sent for Uriah, who was with the Israelite army at the siege of Rabbah, so that he could sleep with his wife and conceal the identity of the child's father. Uriah refused to do so while his companions are in the field of battle and David sent him back to Joab, the commander, with a message instructing him to ensure that Uriah died in battle. David married Bathsheba and she bore his child, "but the thing that David had done displeased the Lord." The prophet Nathan confronted David, saying: "Why have you despised the word of God, to do what is evil in his sight? You have smitten Uriah the Hittite with the sword, and have taken his wife to be your wife." Nathan presented three punishments from God. First, that the "sword shall never depart from your house" (2 Samuel 12:10); second, that "Before your very eyes I will take your wives and give them to one who is close to you, and he will sleep with your wives in broad daylight", and finally, that "the son born to you will die" ().
David repented, yet David's child died. David left his lamentations, dressed himself, went to the House of the Lord and worshiped, and then returned home to eat. His servants asked why he wept when the baby was alive, but ended his mourning when the child dies. David replied: "While the child was still alive, I fasted and wept. I thought, 'Who knows? The may be gracious to me and let the child live.' But now that he is dead, why should I fast? Can I bring him back again? I will go to him, but he will not return to me." ()
David's son Absalom rebels.
David's son Absalom rebelled, forcing David to flee Jerusalem as the kingdom plunged into civil war. However, David sent his servant Hushai to Absalom's court as a double agent both to thwart the counsel of Absalom's chief adviser, fellow traitor Ahitophel and relay intelligence to David's forces. Hushai is successful in persuading Absalom from immediately pursuing his father in favor of better preparing Absalom's own forces for a major battle, thus allowing David to regroup for it. In the battle of the Wood of Ephraim, Absalom's forces were defeated, and his head was caught in the branches of an oak, and David’s general Joab killed Absalom. When the news of the victory was brought to David, he was grief-stricken, and he cried out "O my son Absalom, my son, my son Absalom! Would I had died instead of you, O Absalom, my son, my son!"
Death.
When David had become old and bedridden, Adonijah, his eldest surviving son and natural heir, declared himself king. Bathsheba, David's favorite wife, and Nathan the prophet went to David and obtained his agreement that Solomon, Bathsheba's son should become king. David gave his final instructions, to Solomon including his promise that the line of Solomon and David will inherit the throne of Judah forever, and his request that Solomon kill his oldest enemies on his behalf. David died and was buried on Mount Zion.
Family.
David was born in Bethlehem, in the territory of the Tribe of Judah. His grandfather was Obed, whose mother was the Moabite Ruth and whose grandmother was the former prostitute Rahab. David's father was Jesse. His mother is not named in the Bible, but the Talmud identifies her as Nitzevet daughter of Adael. David had seven older brothers and two sisters, Zeruiah and Abigail.
David had eight wives: Michal, the second daughter of King Saul; Ahinoam the Jezreelite; Abigail the Carmelite, previously wife of Nabal; Maachah, daughter of Talmai, king of Geshur; Haggith; Abital; Eglah; and Bathsheba. The Book of Chronicles lists his sons by various wives and concubines. In Hebron, David had six sons : Amnon, by Ahinoam; Daniel, by Abigail; Absalom, by Maachah; Adonijah, by Haggith; Shephatiah, by Abital; and Ithream, by Eglah. By Bathsheba, his sons were: Shammua; Shobab; Nathan; and Solomon. David's sons born in Jerusalem by other wives included: Ibhar; Elishua; Eliphelet; Nogah; Nepheg; Japhia; Elishama; and Eliada. According to , Jerimoth, who is not mentioned in any of the genealogies, is mentioned as another of his sons. According to , David also welcomed Jonathan's son Mephibosheth to his table after giving him the land which previously belonged to King Saul.
David also had at least one daughter, Tamar, by Maachah, who was raped by Amnon, Tamar's half-brother. Tamar's rape leads to Amnon's death. Absalom, Amnon's half-brother and Tamar's full-brother, waits two years, then avenges his sister by sending his servants to kill Amnon at a feast to which Absalom had invited all the king's sons. 
Historicity.
Archaeology.
Two archaeological finds, the Tel Dan Stele and the Mesha Stele, have direct bearing on the question of the existence of a historical David. The first of these is an Aramean victory stele (inscribed stone) discovered in 1993 at Tel Dan and dated c. 850–835 BC: it contains the phrase ("bytdwd"), which has been interpreted as "House of David". The Mesha Stele from Moab, dating from approximately the same period, may also contain the name David in line 12, where the interpretation is uncertain, and in line 31, where one destroyed letter must be supplied, but apparently no other letter produces a word that makes sense in the context.
The evidence from surface surveys indicates that Judah at the time of David was a small tribal kingdom. The Bronze and Iron Age remains of the City of David, the original urban core of Jerusalem identified with the reigns of David and Solomon, were investigated extensively in the 1970s and 1980s under the direction of Yigal Shiloh of the Hebrew University, but failed to discover significant evidence of occupation during the 10th century BC. In 2005 Eilat Mazar reported the discovery of a Large Stone Structure which she claimed was David's palace, but the site is contaminated and cannot be accurately dated.
Academic views on the biblical account.
The biblical account about David comes from the Books of Samuel and the Books of Chronicles. Chronicles merely retells Samuel from a different theological vantage point, and contains little (if any) information not available there, and the biblical evidence for David is therefore dependent almost exclusively on the material contained in the chapters from 1 Samuel 16 to 1 Kings 2.
Since Martin Noth put forward his analysis of the Deuteronomistic history, biblical scholars have accepted that these two books form part of a continuous history of Israel, compiled no earlier than the late 7th century BC, but incorporating earlier works and fragments. Samuel's account of David "seems to have undergone two separate acts of editorial slanting". The original writers show a strong bias against Saul, and in favour of David and Solomon. Many years later the Deuteronomists edited the material in a manner that conveyed their religious message, inserting reports and anecdotes that strengthened their monotheistic doctrine. Some of the materials in Samuel I and II—notably the boundary, allotment and administrative lists—are believed to be very early, since they correspond closely to what we know of the territorial conditions of the late Davidic-early Solomonic period.
Beyond this, the full range of possible interpretations is available. The late John Bright, in his "History of Israel" takes Samuel at face value. Donald B. Redford, however, thinks all reconstructions from Biblical sources for the United Monarchy period are examples of "academic wishful thinking". Thomas L. Thompson rejects the historicity of the biblical narrative, "The history of Palestine and of its peoples is very different from the Bible's narratives, whatever political claims to the contrary may be. An independent history of Judea during the Iron I and Iron II periods has little room for historicizing readings of the stories of I-II Samuel and I Kings." Amihai Mazar however, concludes that based on recent archeological findings, like those in City of David, Khirbet Qeiyafa, Tel Dan, Tel Rehov, Khirbet en-Nahas and others "the deconstruction of United Monarchy and the devaluation of Judah as a state in 9th century is unacceptable interpretation of available historic data". According to Mazar, based on archeological evidences, United Monarchy can be described as a "state in development".
Some studies of David have been written: Baruch Halpern has pictured David as a lifelong vassal of Achish, the Philistine king of Gath; Israel Finkelstein and Neil Asher Silberman have identified as the oldest and most reliable section of Samuel those chapters which describe David as the charismatic leader of a band of outlaws who captures Jerusalem and makes it his capital. Steven McKenzie, Associate Professor of the Hebrew Bible at Rhodes College and author of "King David: A Biography", states the belief that David actually came from a wealthy family, was "ambitious and ruthless" and a tyrant who murdered his opponents, including his own sons.
Critical Bible scholarship holds that the biblical account of David’s rise to power is a political apology—an answer to contemporary charges against him, of his involvement in murders and regicide.
Jacob L. Wright Associate Professor of Hebrew Bible at Emory University has written that the most popular legends about David, including his killing of Goliath, his affair with Bathsheba, and his ruling of a United Kingdom of Israel rather than just Judah, are the creation of those who lived generations after him, in particular those living in the late Persian or Hellenistic period.
Physical descriptions.
1 Samuel 16:12 "And he sent and brought him in. Now he was ruddy and had beautiful eyes and was handsome. And the LORD said, "Arise, anoint him, for this is he." - Holy Bible; English Standard Version. 
1 Samuel 17:41-43 "And the Philistine moved forward and came near to David, with his shield-bearer in front of him. And when the Philistine looked and saw David, he disdained him, for he was but a youth, ruddy and handsome in appearance."
The Hebrew word for 'ruddy' used in the above passages is "admoni" (אדמני), from the root ADM (אדם, see also Adam and Edom). "Admoni", reddish-brown, was the ideal colour for men, and indicates David's heroic nature. Despite the fact his hair is not mentioned in the passages, the description led to a later Sephardic and Ashkenazi tradition that David was a red-head.
Abrahamic religious traditions.
David as Psalmist.
While almost half of the Psalms are headed "A Psalm of David" (though the phrase can also be translated as "to David" or "for David") and tradition identifies several with specific events in David’s life (e.g., Psalms 3, 7, 18, 34, 51, 52, 54, 56, 57, 59, 60, 63 and 142), the headings are late additions and no psalm can be attributed to David with certainty.
 is attributed to David on the occasion of his escape from the Abimelech (king) Achish by pretending to be insane. According to the narrative in 1 Samuel 21, instead of killing the man who had exacted so many casualties from him, Abimelech allows David to depart, exclaiming, "Am I so short of madmen that you have to bring this fellow here to carry on like this in front of me? Must this man come into my house?"
Judaism.
David is an important figure in Judaism. Historically, David's reign represented the formation of a coherent Jewish kingdom centered in Jerusalem. David is an important figure within the context of Jewish messianism. In the Hebrew Bible, it is written that a human descendant of David will occupy the throne of a restored kingdom and usher in a messianic age.
David is also viewed as a tragic figure; his acquisition of Bathsheba, and the loss of his son are viewed as his central tragedies.
Many legends have grown around the figure of David. According to one Rabbinic tradition, David was raised as the son of his father Jesse and spent his early years herding his father's sheep in the wilderness while his brothers were in school. Only at his anointing by Samuel—when the oil from Samuel's flask turned to diamonds and pearls—was his true identity as Jesse's son revealed.
David's adultery with Bathsheba was only an opportunity to demonstrate the power of repentance, and the Talmud states that it was not adultery at all, quoting a Jewish practice of divorce on the eve of battle. Furthermore, according to Talmudic sources, the death of Uriah was not to be considered murder, on the basis that Uriah had committed a capital offence by refusing to obey a direct command from the King. However, in tractate Sanhedrin, David expressed remorse over his transgressions and sought forgiveness. God ultimately forgave David and Bathsheba but would not remove their sins from Scripture.
According to midrashim, Adam gave up 70 years of his life for the life of David. Also, according to the Talmud Yerushalmi, David was born and died on the Jewish holiday of Shavuot (Feast of Weeks). His piety was said to be so great that his prayers could bring down things from Heaven.
Christianity.
The concept of the Messiah is important in Christianity. Originally an earthly king ruling by divine appointment ("the anointed one", as the title Messiah had it), the "son of David" became in the last two pre-Christian centuries the apocalyptic and heavenly one who would deliver Israel and usher in a new kingdom. This was the background to the concept of Messiahship in early Christianity, which interpreted the career of Jesus "by means of the titles and functions assigned to David in the mysticism of the Zion cult, in which he served as priest-king and in which he was the mediator between God and man". The early Church believed that "the life of David [foreshadowed] the life of Christ; Bethlehem is the birthplace of both; the shepherd life of David points out Christ, the Good Shepherd; the five stones chosen to slay Goliath are typical of the five wounds; the betrayal by his trusted counsellor, Achitophel, and the passage over the Cedron remind us of Christ's Sacred Passion. Many of the Davidic Psalms, as we learn from the New Testament, are clearly typical of the future Messiah."
In the Middle Ages, "Charlemagne thought of himself, and was viewed by his court scholars, as a 'new David'. [This was] not in itself a new idea, but [one whose] content and significance were greatly enlarged by him". The linking of David to earthly kingship was reflected in later Medieval cathedral windows all over Europe through the device of the Tree of Jesse, its branches demonstrating how divine kingship descended from Jesse, through his son David, to Jesus.
Western Rite churches (Lutheran, Roman Catholic) celebrate his feast day on 29 December, Eastern-rite on 19 December. The Eastern Orthodox Church and Eastern Catholic Churches celebrate the feast day of the "Holy Righteous Prophet and King David" on the Sunday of the Holy Forefathers (two Sundays before the Great Feast of the Nativity of the Lord), when he is commemorated together with other ancestors of Jesus. He is also commemorated on the Sunday after the Nativity, together with Joseph and James, the Brother of the Lord.
Latter Day Saints.
In the Latter Day Saint movement, the Book of Mormon offers a negative commentary on David's practice of polygamy. In the Book of Jacob, the Nephite nation begins to practice polygamy, justifying it by the example of David and Solomon. In response the prophet Jacob denounces both David's taking of "many wives" and the Nephites' taking of multiple wives, though he stops short of denouncing polygamy altogether.
Editions of the Doctrine and Covenants utilized by The Church of Jesus Christ of Latter-day Saints, the largest Latter Day Saint denomination, state that of David's sexual relationships, only his relationship with Bathsheba was a sin. However, in consequence of this sin and the further sin of killing Uriah, David had "fallen from exaltation" and would not be married to any of his wives in the next life.
The Foundation for Apologetic Information & Research argues that there is no contradiction between the Book of Mormon and Doctrine and Covenants because the Lord authorized David to have some wives, but not "many" wives. They see a parallel between and , which some rabbis interpreted as a limit of four wives per husband. When David took Bathsheba, he crossed the line into having "many" wives, which he was not authorized to do. Jacob's denunciation then becomes, not a complete denunciation of David's polygamy, but a denunciation of unauthorized indulgence in polygamy.
The Community of Christ, the second-largest Latter Day Saint faction, does not accept the validity of 132nd section of the LDS Doctrine and Covenants; nor does the Church of Christ (Temple Lot), the Church of Jesus Christ (Cutlerite), and many other smaller factions. Although the Church of Jesus Christ of Latter Day Saints (Strangite) accepted the validity of polygamy as an institution, they do not accept Doctrine and Covenants 132, nor do they believe that Joseph Smith instituted or taught it (they believe that James Strang was responsible for that, when he released his Book of the Law of the Lord in 1850).
Islam.
David (Arabic داود, "Dāwūd") is a highly important figure in Islam as one of the major prophets sent by God to guide the Israelites. David is mentioned several times in the Qur'an, often with his son Solomon. The actual Arabic equivalent to the Hebrew Davīd is Dawūd. In the Qur'an: David killed Goliath (II: 251), Goliath was a powerful king who used to invade random kingdoms and villages. Goliath was spreading evil and corruption. When David killed Goliath, God granted him kingship and wisdom and enforces it (XXXVIII: 20). David is made God's "vicegerent on earth" (XXXVIII: 26) and God further gives David sound judgment (XXI: 78; XXXVII: 21–24, 26) as well as the Psalms, which are regarded as books of divine wisdom (IV: 163; XVII, 55). The birds and mountains unite with David in uttering praise to God (XXI: 79; XXXIV: 10; XXXVIII: 18), while God made iron soft for David (), God also instructed David in the art of fashioning chain-mail out of iron (); an indication of the first use of Wrought iron, this knowledge gave David a major advantage over his bronze and cast iron-armed opponents, not to mention the cultural and economic impact. Together with Solomon, David gives judgment in a case of damage to the fields (XXI: 78) and David judges in the matter between two disputants in his prayer chamber (XXXVIII: 21–23). Since there is no mention in the Qur'an of the wrong David did to Uriah nor is there any reference to Bathsheba, Muslims reject this narrative.
Muslim tradition and the "hadith" stress David's zeal in daily prayer as well as in fasting. Qur'an commentators, historians and compilers of the numerous "Stories of the Prophets" elaborate upon David's concise Qur'anic narratives and specifically mention David's gift in singing his Psalms as well as his musical and vocal talents. His voice is described as having had a captivating power, weaving its influence not only over man but over all beasts and nature, who would unite with him to praise God.
Baha'i Faith.
In the Baha'i Faith, David is described as a reflection of God and one among a long line of prophets who came in the shadow of the dispensation of Moses to develop and consolidate the process he set in motion. The Kitáb-i-Íqán describes David as being "among the more exalted Manifestations who have appeared during the intervening period between the revelations of Moses and Muhammad, ever altered the law of the Qiblih".
Legend and legacy.
In European Christian culture of the Middle Ages, David was made a member of the Nine Worthies, a group of heroes encapsulating all the ideal qualities of chivalry. His life was thus proposed as a valuable subject for study by those aspiring to chivalric status. This aspect of David in the Nine Worthies was popularised firstly through literature, and was thereafter adopted as a frequent subject for painters and sculptors.
David was considered as a model ruler and a symbol of the God-ordained monarchy throughout medieval Western Europe and Eastern Christendom. David was perceived as the biblical predecessor to Christian Roman and Byzantine emperors and the name "New David" was used as an honorific reference to these rulers. The Georgian Bagratids and the Solomonic dynasty of Ethiopia claimed a direct biological descent from him. Likewise, the Frankish Carolingian dynasty frequently connected themselves to David; Charlemagne himself occasionally used the name of David as his pseudonym.
Representation in art and literature.
Art.
Famous sculptures of David include (in chronological order) those by:
Cards.
For a considerable period, starting in the 15th century and continuing until the 19th, French playing card manufacturers assigned to each of the court cards names taken from history or mythology. In this context, the King of Spades was often known as "David".

</doc>
<doc id="8556" url="http://en.wikipedia.org/wiki?curid=8556" title="Diablo II">
Diablo II

Diablo II is an action role-playing hack and slash video game developed by Blizzard North and published by Blizzard Entertainment in 2000 for Windows and Mac OS computers.
The game, with its dark fantasy and horror themes, was conceptualized and designed by David Brevik and Erich Schaefer, who with Max Schaefer acted as project leads on the game. The producers were Matthew Householder and Bill Roper.
Building on the success of its predecessor "Diablo" (1996), "Diablo II" was one of the most popular games of 2000. Major factors that contributed to "Diablo II"s success include its continuation of popular fantasy themes from the previous game and its access to Blizzard's free online play service Battle.net. An expansion to "Diablo II", "", was released in 2001. A sequel, "Diablo III", was announced in 2008, and was released on May 15, 2012.
Gameplay.
"Diablo II"'s storyline progresses through four chapters or "Acts". Each act follows a more or less predetermined path, although there is some random-level generation in wilderness areas and dungeons between key cities. The player progresses through the story by completing a series of quests within each act, with optional quests providing additional rewards. In contrast to the first "Diablo," whose levels consisted of descending deeper and deeper into a Gothic-themed dungeon and Hell, "Diablo II's" environments are much more varied. While Act I is similar to the original, Act II mimics Canaan desert while Act III is supposedly based on the Maya civilization jungles. Act IV takes place in Hell and is the shortest, with just three quests compared to the other Acts that have six.
The "Lord of Destruction" expansion adds Act V which continues the story where Act IV left off. Act V's style is mainly mountainous as the player ascends Mount Arreat, with ice tunnels/caverns in the mountains, as well as hellish subterranean pits (reminiscent of Hell in Act IV) for extra monsters and experience. After reaching the summit of Arreat, the player gains access to the Worldstone Keep (whose architecture may be reminiscent of Angkor Wat and other Hindu temples).
In addition to the acts, there are three sequential difficulty levels: Normal, Nightmare, and Hell; completing the game (four Acts in the original or five Acts in the expansion) on a difficulty setting will open up the next level. On higher difficulties, monsters are stronger and are resistant to an element, experience is penalized on dying, and the player's resistances are handicapped. However, better items are rewarded to players as they go through higher difficulties. A character retains all abilities and items between difficulties, and may return to a lower difficulty at any time.
Players can also create a "hardcore" character. In normal mode, the player can resurrect their character if killed and resume playing, while a hardcore character has only one life. If killed, the character is permanently dead and unplayable, and all items and equipment on that character will be lost unless another friendly character has the "loot" icon checked.
Item system.
Diablo II uses a system of randomly generated equipment similar to the original Diablo, but more complicated. Weapons and armor are divided into several quality levels, including "normal", "magical", "set" and "unique". "Normal" quality items are base items with a fixed set of basic properties, such as attribute requirements, maximum durability, armor rating (on armor), block chance (on shields) and damage and attack speed (on weapons). "Magical" quality items have violet names and one or two randomly selected bonuses, such as bonuses attributes, skills or damage, indicated by a prefix or suffix. "Rare" quality items have randomly generated yellow names and 2 to 6 random properties. "Unique" quality items have fixed names in gold text, and instead of randomized properties, they have a set of 3 to 8 preselected properties. Green-named "set items" have fixed names and preselected properties like "unique" items, and belong to specific named sets of 2 to 6 items, as well as possessing additional properties known as "set bonuses" which are activated by equipping multiple items from the same set. Additionally, items can possess "sockets", which can be used to upgrade items by adding gems for various bonuses.
Diablo II includes an item crafting system. An item known as the "Horadric Cube" is used to combine specific recipes of 2 or more items to create a new item. For example, 3 identical lower quality gems can be combined to create a single higher quality gem, and 3 small rejuvenation potions can be combined to create a single, more powerful rejuvenation potion.
Character classes.
"Diablo II" allows the player to choose between five different character classes: Amazon, Necromancer, Barbarian, Sorceress, and Paladin. Each character has different strengths and weaknesses and sets of skills to choose from, as well as varying beginning attributes. The maximum level that any character can obtain is level 99.
The player can enlist the help of one hireling (computer-controlled mercenaries) from a mercenary captain in the town; the Rogue Scouts, Desert Mercenaries, Ironwolves, and Barbarians, from Acts I, II, III, and V ( only). The allows players to retain their mercenary throughout the entire game as well as equipping them with armor and weapons. Hirelings gain experience and attributes like the player, although their level cannot surpass that of their master character. Typically players choose a hireling that provides something missing from their character class; for instance the melee-focused Paladin may choose a Rogue for missile support.
Multiplayer.
"Diablo II" can be played multiplayer on a LAN or Battle.net. Unlike the original "Diablo", "Diablo II" was made specifically with online gaming in mind. Several spells (such as auras or war cries) multiply their effectiveness if they are cast within a party, and although dungeons still exist, they were largely replaced by open spaces.
Multiplayer is achieved through Blizzard's Battle.net free online service, or via a LAN. Battle.net is divided into "Open" and "Closed" realms. Players may play their single-player characters on open realms; characters in closed realms are stored on Blizzard's servers, as a measure against cheating, where they must be played every 90 days to avoid expiration. Originally these closed realms served their purpose of preventing cheating, as open games were subject to many abuses as the characters were stored on players' own hard drives. Within the last few years, however, many cheats are (and continue to be) used on these closed realms. Hacks, bots, and programs which allow the player to run multiple instances of the game at the same time are not allowed by Blizzard but are very commonly used. Spambots, (programs which advertise sites selling Diablo II's virtual items for real-world currency) run rampant on the service and a player hosting a public game can expect a visit from one every few minutes. Due to the surplus of virtual items provided by the automated bots, which repeatedly kill bosses to obtain items, supply is well in excess of demand, and items which used to trade well are now often given away for nothing.
As the game can be played cooperatively (Players vs. Environment, PvE), groups of players with specific sets of complementary skills can finish some of the game's climactic battles in a matter of seconds, providing strong incentives for party-oriented character builds. Up to eight players can be in one game; they can either unite as a single party, play as individuals, or form multiple opposing parties. Experience gained, monsters' hit points and damage, and the number of items dropped are all increased as more players join a game, though not in a strictly proportional manner. Players are allowed to duel each other with all damage being reduced in player vs player (PvP). The bounty for a successful kill in PvP is a portion of the gold and the "ear" of the defeated player (with the previous owner's name and level at the time of the kill).
The Ladder System can be reset at various intervals to allow for all players to start fresh with new characters on an equal footing. Ladder seasons have lasted from as short as six months to over a year. When a ladder season ends all ladder characters are transferred to the non-ladder population. Certain rare items are available only within ladder games, although they can be traded for and exchanged on non-ladder after the season has ended.
On March 3, 2009, Blizzard announced a new "Diablo II" content patch, 9 years after the game's release.
The game has been patched extensively; the precise number of patches is impossible to determine as Battle.net has the capability of making minor server-side patches to address immediate issues. The game is currently in version 1.13d. The latest major patch was released on March 23, 2010. Through the patch history, several exploits and issues have been addressed (such as illegal item duplication, though it still exists), as well as major revamps to the game's balance (such as the ability to redo skills and attributes). Not all patches have affected "Diablo II" directly, as several were designed to address issues in the expansion to the game and had minimal effects on "Diablo II".
Plot.
"Diablo II" takes place after the end of the previous game, "Diablo", in the world of Sanctuary. In "Diablo", an unnamed warrior defeated Diablo and attempted to contain the Lord of Terror's essence within his own body. Since then, the hero has become corrupted by the demon's spirit, causing demons to enter the world around him and wreak havoc.
A band of adventurers who pass through the Rogue Encampment hear these stories of destruction and attempt to find out the cause of the evil, starting with this corrupted "Dark Wanderer." As the story develops, the truth behind this corruption is revealed: the soulstones were originally designed to capture the Prime Evils who were banished to the mortal realm after being overthrown by the Lesser Evils. With the corruption of Diablo's soulstone, the demon is able to control the Dark Wanderer. The soulstone of another demon, Baal, was united with the mage Tal-Rasha, who volunteered to absorb Baal's spirit in his own body and be imprisoned in a tomb.
As the story progresses, cut scenes show the Dark Wanderer's journey as a drifter named Marius follows him. The player realizes that the Dark Wanderer's mission is to reunite with the other prime evils, Baal and Mephisto. The story is divided up into four acts:
In the epilogue, Marius, speaking in a prison cell, indicates he was too weak to enter Hell, and that he fears the stone's effects on him. He gives the soulstone to his visitor. The visitor reveals himself to be Baal, the last surviving Prime Evil now in possession of his own soulstone. He then kills Marius and sets the prison cell on fire.
The story continues in the expansion "" where Baal attempts to corrupt the mythical Worldstone on Mount Arreat. Upon returning to the Pandemonium Fortress after defeating Diablo, Tyrael opens a portal to send the adventurers to Arreat.
Development.
 The game was originally to be released in 1999, after being shown off at E3 1998. According to Erich Schaefer, a designer and project lead for "Diablo II", had stated that "Diablo II never had an official, complete design document... for the most part we just started making up new stuff." The game was slated to have two years of development work, but it had taken Blizzard North over three years to finish. "Diablo II", despite having less than one percent of the original code from Diablo I and having much of its content and internal coding done from scratch, was seen by the testers as "more of the same." The game was meant to be released simultaneously both in North America and internationally. This allowed the marketing and PR department for Blizzard North to focus their efforts in building up excitement in players worldwide for the first week of sales, contributing to the game's success.
Music.
The score was composed by Matt Uelmen and integrates creepy ambience with melodic pieces. The style of the score is ambient industrial and experimental. It was recorded in Redwood City, Oakland, and San Mateo, California, from April 1997 to March 2000.
Some tracks were created by reusing the tracks from the original game, while others by rearranging tracks that were out-takes. Other scores are combinations of parts that were created more than a year after the first game's release. A single track usually integrates recorded samples from sound libraries, live recorded instrument interpretation samples specially meant for the game (guitar, flute, oriental percussion), and electronic instruments also, making difficult the tracks for later live interpretations.
While the player visits the town, the game recreates the peaceful atmosphere from the first "Diablo" game, so for that the theme from Act I called "Rogue" comes back with the same chords of the original piece, reproducing only a part of the original "Diablo" town theme. For Act II Mustafa Waiz, a percussionist, and Scott Petersen, the game's sound designer, worked on the drum samples. Waiz played on the dumbek, djembe, and finger cymbals which gave Matt Uelmen a base upon which to build tracks around.
The town theme from Act II, "Toru", makes strong statement of departure from the world of Act I while also maintaining a thematic connection to what had come before. It is the first time in the series to be used some radically different elements than the guitars and choral sounds that dominate both the original Diablo and the opening quarter of "Diablo II". The foundation of the "Toru" piece is found in exciting dynamics of a Chinese wind gong. The instrument radically changes color from a steady mysterious drone to a harsh, fearsome noise, that gives exotic feeling and at the same time the pacing of the second town. In all sequences of Act II with deserts and valleys, Arabic percussion sounds dominate.
The composer was impressed by two of the Spectrasonics music libraries, "Symphony of Voices" and "Heart of Asia." He used samples from "Heart of Asia" in the "Harem" piece from Act II. The "Crypt" track uses a sample from "Symphony of Voices"; the choral phrase "Miserere". Voice samples from "Heart of Asia", "Heart of Africa", and "Symphony of Voices" by Spectrasonics.
The "Harem" track samples from "Heart of Asia" the Sanskrit Female 1 samples.
Release.
The game was released in "Collector's Edition" format, containing bonus collector's material, a copy of the "Diablo" "Dungeons & Dragons" pen-and-paper campaign setting, and promotional movies for other Blizzard games. The "Diablo II: Exclusive Gift Set" (2000) similarly contained exclusive collector's material and promotional videos, as well as a copy of the official strategy guide. The "Diablo Gift Pack" (2000) contained copies of "Diablo" and "Diablo II", but no expansions. The "Diablo: Battle Chest" (2001) contained copies of "Diablo II", "Diablo II: Lord of Destruction", the official strategy guide, and the original "Diablo". Recently, however, the "Battle Chest" edition no longer contains the original "Diablo".
Original CD release worked on Windows 95/98/Me/NT4SP5, but current one downloadable from Battle.net requires at least Windows 2000/XP.
The announcement of "Diablo III" renewed the interest in its predecessor and brought more attention to the many mods available for the game.
Reception.
"Diablo II" had a positive reception. The PC versions of the game achieved an overall score of 88 on Metacritic and 88.58% at GameRankings. The Mac version achieved 83.00% on Game Rankings. Gamespy awarded the game an 86 out of 100, IGN awarded the game an 8.3 out of 10, and GameSpot awarded the game an 8.5 out of 10 along with earning the 2000 runner-up Reader's Choice Award for role-playing game of the year. It was awarded a spot in the Guinness Book of World Records 2000 edition for being the fastest selling computer game ever sold, with more than 1 million units sold in the first two weeks of availability. ', ', ', ' and "Diablo III" have since surpassed "Diablo II"s record to become fastest-selling computer games ever at their times of release, according to Blizzard. As of August 29, 2001, "Diablo II" has sold 4 million copies worldwide. The game has received the "Computer Game of the Year", "Computer Role Playing Game of the Year", and "Game of the Year" awards from the Academy of Interactive Arts and Sciences at the 2001 Interactive Achievement Awards.
Copies of "Diablo: Battle Chest" continue to be sold in retail stores, appearing on the NPD Group's top 10 PC games sales list as recently as 2010. Even more remarkably, the "Diablo: Battle Chest" was the 19th best selling PC game of 2008 – a full seven years after the game's initial release – and 11 million users still played "Diablo II" and "StarCraft" over Battle.net in 2010.
Secret Cow Level.
The "Secret Cow Level" is the result of a running joke from the original "Diablo" that spawned from an Internet rumor about the cows that appear in the game, seemingly without purpose. Supposedly, if the cow was clicked a certain number of times, a portal to a secret level would open. The rumor turned out to be a hoax, but the legend was born, and player after player asked Blizzard about how to access the level.
In "", an add-on for "Diablo" created by third-party developer Synergistic Software, it was possible to change a parameter in a specific text file, so that the farmer was dressed in a cow suit, with appropriate new dialogue ("Moo." "I said Moo!"). To stop the rumors, Blizzard included a cheat in "StarCraft" that read "There is no cow level", adding to the official denial of the cow level. On April 1, 1999, a "Diablo II Screenshot of the Week" featured cows fighting. People wondered if the screenshot was an April Fool's joke or if there really was a Secret Cow Level planned for "Diablo II", which turned out to be true. The "Secret Cow Level" is considered one of gaming's top ten Easter eggs according to IGN. 

</doc>
<doc id="8560" url="http://en.wikipedia.org/wiki?curid=8560" title="Design">
Design

Design is the creation of a plan or convention for the construction of an object or a system (as in architectural blueprints, engineering drawings, business processes, circuit diagrams and sewing patterns). Design has different connotations in different fields (see design disciplines below). In some cases the direct construction of an object (as in pottery, engineering, management, cowboy coding and graphic design) is also considered to be design.
More formally design has been defined as follows.
Another definition for design is "a roadmap or a strategic approach for someone to achieve a unique expectation. It defines the specifications, plans, parameters, costs, activities, processes and how and what to do within legal, political, social, environmental, safety and economic constraints in achieving that objective."
Here, a "specification" can be manifested as either a plan or a finished product, and "primitives" are the elements from which the design object is composed.
With such a broad denotation, there is no universal language or unifying institution for designers of all disciplines. This allows for many differing philosophies and approaches toward the subject (see Philosophies and studies of design, below).
The person designing is called a "designer", which is also a term used for people who work professionally in one of the various design areas, usually also specifying which area is being dealt with (such as a "fashion designer", "concept designer" or "web designer"). A designer's sequence of activities is called a design process. The scientific study of design is called design science.
Designing often necessitates considering the aesthetic, functional, economic and sociopolitical dimensions of both the design object and design process. It may involve considerable research, thought, modeling, interactive adjustment, and re-design. Meanwhile, diverse kinds of objects may be designed, including clothing, graphical user interfaces, skyscrapers, corporate identities, business processes and even methods of designing.
Design as a process.
Substantial disagreement exists concerning how designers in many fields, whether amateur or professional, alone or in teams, produce designs. Dorst and Dijkhuis argued that "there are many ways of describing design processes" and discussed "two basic and fundamentally different ways", both of which have several names. The prevailing view has been called "The Rational Model", "Technical Problem Solving" and "The Reason-Centric Perspective". The alternative view has been called "Reflection-in-Action", "Evolutionary Design", "co-evolution" and "The Action-Centric Perspective".
The Rational Model.
The Rational Model was independently developed by Simon and Pahl and Beitz. It posits that:
The Rational Model is based on a rationalist philosophy and underlies the Waterfall Model, Systems Development Life Cycle and much of the engineering design literature. According to the rationalist philosophy, design is informed by research and knowledge in a predictable and controlled manner. Technical rationality is at the center of the process.
Example sequence of stages.
Typical stages consistent with The Rational Model include the following.
Each stage has many associated best practices.
Criticism of the Rational Model.
The Rational Model has been widely criticized on two primary grounds
The Action-Centric Model.
The Action-Centric Perspective is a label given to a collection of interrelated concepts, which are antithetical to The Rational Model. It posits that:
The Action-Centric Perspective is based on an empiricist philosophy and broadly consistent with the Agile approach and amethodical development. Substantial empirical evidence supports the veracity of this perspective in describing the actions of real designers. Like the Rational Model, the Action-Centric model sees design as informed by research and knowledge. However, research and knowledge are brought into the design process through the judgment and common sense of designers – by designers "thinking on their feet" – more than through the predictable and controlled process stipulated by the Rational Model. Designers' context-dependent experience and professional judgment take center stage more than technical rationality.
Descriptions of design activities.
At least two views of design activity are consistent with the Action-Centric Perspective. Both involve three basic activities.
In the Reflection-in-Action paradigm, designers alternate between "framing," "making moves," and "evaluate moves." "Framing" refers to conceptualizing the problem, i.e., defining goals and objectives. A "move" is a tentative design decision. The evaluation process may lead to further moves in the design.
In the Sensemaking-Coevolution-Implementation Framework, designers alternate between its three titular activities. Sensemaking includes both framing and evaluating moves. Implementation is the process of constructing the design object. Coevolution is "the process where the design agent simultaneously refines its mental picture of the design object based on its mental picture of the context, and vice versa."
The concept of the Design Cycle describes the reflective and repetitive structure of design processes, assuming that this structure is underlaying all such processes. The Design Cycle is understood as a circular time structure, which may start with the thinking of an idea, then expressing it by the use of visual and/or verbal means of communication (design tools), the sharing and perceiving of the expressed idea, and finally starting a new cycle with the critical rethinking of the perceived idea. Anderson points out that this concept emphasizes the importance of the means of expression, which at the same time are means of perception of any design ideas.
Criticism of the Action-Centric Perspective.
As this perspective is relatively new, it has not yet encountered much criticism. One possible criticism is that it is less intuitive than The Rational Model.
Philosophies and studies of design.
There are countless philosophies for guiding design as the design values and its accompanying aspects within modern design vary, both between different schools of thought and among practicing designers. Design philosophies are usually for determining design goals. A design goal may range from solving the least significant individual problem of the smallest element, to the most holistic influential utopian goals. Design goals are usually for guiding design. However, conflicts over immediate and minor goals may lead to questioning the purpose of design, perhaps to set better long term or ultimate goals.
Philosophies for guiding design.
Design philosophies are fundamental guiding principles that dictate how a designer approaches his/her practice. Reflections on material culture and environmental concerns (Sustainable design) can guide a design philosophy. One example is the First Things First manifesto which was launched within the graphic design community and states "We propose a reversal of priorities in favor of more useful, lasting and democratic forms of communication – a mindshift away from product marketing and toward the exploration and production of a new kind of meaning. The scope of debate is shrinking; it must expand. Consumerism is running uncontested; it must be challenged by other perspectives expressed, in part, through the visual languages and resources of design."
In "The Sciences of the Artificial" by polymath Herbert A. Simon the author asserts design to be a meta-discipline of all professions. "Engineers are not the only professional designers. Everyone designs who devises courses of action aimed at changing existing situations into preferred ones. The intellectual activity that produces material artifacts is no different fundamentally from the one that prescribes remedies for a sick patient or the one that devises a new sales plan for a company or a social welfare policy for a state. Design, so construed, is the core of all professional training; it is the principal mark that distinguishes the professions from the sciences. Schools of engineering, as well as schools of architecture, business, education, law, and medicine, are all centrally concerned with the process of design."
Approaches to design.
A design approach is a general philosophy that may or may not include a guide for specific methods. Some are to guide the overall goal of the design. Other approaches are to guide the tendencies of the designer. A combination of approaches may be used if they don't conflict.
Some popular approaches include:
Methods of designing.
Design Methods is a broad area that focuses on:
Terminology.
The word "design" is often considered ambiguous, as it is applied differently in a varying contexts.
Design and art.
Today the term design is widely associated with the Applied arts as initiated by Raymond Loewy and teachings at the Bauhaus and Ulm School of Design (HfG Ulm) in Germany during the 20th Century.
The boundaries between art and design are blurred, largely due to a range of applications both for the term 'art' and the term 'design'. Applied arts has been used as an umbrella term to define fields of industrial design, graphic design, fashion design, etc. The term 'decorative arts' is a traditional term used in historical discourses to describe craft objects, and also sits within the umbrella of Applied arts. In graphic arts (2D image making that ranges from photography to illustration) the distinction is often made between fine art and commercial art, based on the context within which the work is produced and how it is traded.
To a degree, some methods for creating work, such as employing intuition, are shared across the disciplines within the Applied arts and Fine art. Mark Getlein suggests the principles of design are "almost instinctive", "built-in", "natural", and part of "our sense of 'rightness'." However, the intended application and context of the resulting works will vary greatly.
Design and engineering.
In engineering, design is a component of the engineering process. Many overlapping methods and processes can be seen when comparing Product design, Industrial design and Engineering. The American Heritage Dictionary defines design as: "To conceive or fashion in the mind; invent," and "To formulate a plan", and defines engineering as: "The application of scientific and mathematical principles to practical ends such as the design, manufacture, and operation of efficient and economical structures, machines, processes, and systems.". Both are forms of problem-solving with a defined distinction being the application of "scientific and mathematical principles". The increasingly scientific focus of engineering in practice, however, has raised the importance of new more "human-centered" fields of design. How much science is applied in a design is a question of what is considered "science". Along with the question of what is considered science, there is social science versus natural science. Scientists at Xerox PARC made the distinction of design versus engineering at "moving minds" versus "moving atoms" (probably in cotradiction to the origin of term "engineering - engineer" from Latin "in genio" in meaning of a "genius" what assumes existence of a "mind" not of an "atom").
Design and production.
The relationship between design and production is one of planning and executing. In theory, the plan should anticipate and compensate for potential problems in the execution process. Design involves problem-solving and creativity. In contrast, production involves a routine or pre-planned process. A design may also be a mere plan that does not include a production or engineering processes although a working knowledge of such processes is usually expected of designers. In some cases, it may be unnecessary and/or impractical to expect a designer with a broad multidisciplinary knowledge required for such designs to also have a detailed specialized knowledge of how to produce the product.
Design and production are intertwined in many creative professional careers, meaning problem-solving is part of execution and the reverse. As the cost of rearrangement increases, the need for separating design from production increases as well. For example, a high-budget project, such as a skyscraper, requires separating (design) architecture from (production) construction. A Low-budget project, such as a locally printed office party invitation flyer, can be rearranged and printed dozens of times at the low cost of a few sheets of paper, a few drops of ink, and less than one hour's pay of a desktop publisher.
This is not to say that production never involves problem-solving or creativity, nor that design always involves creativity. Designs are rarely perfect and are sometimes repetitive. The imperfection of a design may task a production position (e.g. production artist, construction worker) with utilizing creativity or problem-solving skills to compensate for what was overlooked in the design process. Likewise, a design may be a simple repetition (copy) of a known preexisting solution, requiring minimal, if any, creativity or problem-solving skills from the designer.
Process design.
"Process design" (in contrast to "design process" mentioned above) refers to the planning of routine steps of a process aside from the expected result. Processes (in general) are treated as a product of design, not the method of design. The term originated with the industrial designing of chemical processes. With the increasing complexities of the information age, consultants and executives have found the term useful to describe the design of business processes as well as manufacturing processes.

</doc>
<doc id="8561" url="http://en.wikipedia.org/wiki?curid=8561" title="Denormalization">
Denormalization

In computing, denormalization is the process of attempting to optimize the read performance of a database by adding redundant data or by grouping data. In some cases, denormalization is a means of addressing performance or scalability in relational database software.
A normalized design will often store different but related pieces of information in separate logical tables (called relations). If these relations are stored physically as separate disk files, completing a database query that draws information from several relations (a "join operation") can be slow. If many relations are joined, it may be prohibitively slow. There are two strategies for dealing with this. The preferred method is to keep the logical design normalized, but allow the database management system (DBMS) to store additional redundant information on disk to optimise query response. In this case it is the DBMS software's responsibility to ensure that any redundant copies are kept consistent. This method is often implemented in SQL as indexed views (Microsoft SQL Server) or materialised views (Oracle, PostgreSQL). A view represents information in a format convenient for querying, and the index ensures that queries against the view are optimised.
The more usual approach is to denormalize the logical data design. With care this can achieve a similar improvement in query response, but at a cost—it is now the database designer's responsibility to ensure that the denormalized database does not become inconsistent. This is done by creating rules in the database called "constraints", that specify how the redundant copies of information must be kept synchronised. It is the increase in logical complexity of the database design and the added complexity of the additional constraints that make this approach hazardous. Moreover, constraints introduce a trade-off, speeding up reads (codice_1 in SQL) while slowing down writes (codice_2, codice_3, and codice_4). This means a denormalized database under heavy write load may actually offer "worse" performance than its functionally equivalent normalized counterpart.
A denormalized data model is not the same as a data model that has not been normalized, and denormalization should only take place after a satisfactory level of normalization has taken place and that any required constraints and/or rules have been created to deal with the inherent anomalies in the design. For example, all the relations are in third normal form and any relations with join and multi-valued dependencies are handled appropriately.
Examples of denormalization techniques include:
Denormalization techniques are often used to improve the scalability of Web applications.

</doc>
<doc id="8562" url="http://en.wikipedia.org/wiki?curid=8562" title="Differential topology">
Differential topology

In mathematics, differential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.
Description.
Differential topology considers the properties and structures that require only a smooth structure on a manifold to be defined. Smooth manifolds are 'softer' than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences and deformations that exist in differential topology. For instance, volume and Riemannian curvature are invariants that can distinguish different geometric structures on the same smooth manifold—that is, one can smoothly "flatten out" certain manifolds, but it might require distorting the space and affecting the curvature or volume.
On the other hand, smooth manifolds are more rigid than the topological manifolds. John Milnor discovered that some spheres have more than one smooth structure—see exotic sphere and Donaldson's theorem. Kervaire exhibited topological manifolds with no smooth structure at all. Some constructions of smooth manifold theory, such as the existence of tangent bundles, can be done in the topological setting with much more work, and others cannot.
One of the main topics in differential topology is the study of special kinds of smooth mappings between manifolds, namely immersions and submersions, and the intersections of submanifolds via transversality. More generally one is interested in properties and invariants of smooth manifolds which are carried over by diffeomorphisms, another special kind of smooth mapping. Morse theory is another branch of differential topology, in which topological information about a manifold is deduced from changes in the rank of the Jacobian of a function.
For a list of differential topology topics, see the following reference: List of differential geometry topics.
Differential topology versus differential geometry.
Differential topology and differential geometry are first characterized by their "similarity". They both study primarily the properties of differentiable manifolds, sometimes with a variety of structures imposed on them.
One major difference lies in the nature of the problems that each subject tries to address. In one view, differential topology distinguishes itself from differential geometry by studying primarily those problems which are "inherently global".
Consider the example of a coffee cup and a donut (see ). From the point of view of differential topology, the donut and the coffee cup are "the same" (in a sense). This is an inherently global view, though, because there is no way for the differential topologist to tell whether the two objects are the same (in this sense) by looking at just a tiny ("local") piece of either of them. He or she must have access to each entire ("global") object.
From the point of view of differential geometry, the coffee cup and the donut are "different" because it is impossible to rotate the coffee cup in such a way that its configuration matches that of the donut. This is also a global way of thinking about the problem. But an important distinction is that the geometer does not need the entire object to decide this. By looking, for instance, at just a tiny piece of the handle, he can decide that the coffee cup is different from the donut because the handle is thinner (or more curved) than any piece of the donut.
To put it succinctly, differential topology studies structures on manifolds which, in a sense, have no interesting local structure. Differential geometry studies structures on manifolds which do have an interesting local (or sometimes even infinitesimal) structure.
More mathematically, for example, the problem of constructing a diffeomorphism between two manifolds of the same dimension is inherently global since "locally" two such manifolds are always diffeomorphic. Likewise, the problem of computing a quantity on a manifold which is invariant under differentiable mappings is inherently global, since any local invariant will be "trivial" in the sense that it is already exhibited in the topology of Rn. Moreover, differential topology does not restrict itself necessarily to the study of diffeomorphism. For example, symplectic topology — a subbranch of differential topology — studies global properties of symplectic manifolds. Differential geometry concerns itself with problems — which may be local "or" global — that always have some non-trivial local properties. Thus differential geometry may study differentiable manifolds equipped with a "connection", a "metric" (which may be Riemannian, pseudo-Riemannian, or Finsler), a special sort of "distribution" (such as a CR structure), and so on.
This distinction between differential geometry and differential topology is blurred, however, in questions specifically pertaining to local diffeomorphism invariants such as the tangent space at a point. Differential topology also deals with questions like these, which specifically pertain to the properties of differentiable mappings on Rn (for example the tangent bundle, jet bundles, the Whitney extension theorem, and so forth).
Nevertheless, the distinction becomes clearer in abstract terms. Differential topology is the study of the (infinitesimal, local, and global) properties of structures on manifolds having "no" non-trivial local moduli, whereas differential geometry is the study of the (infinitesimal, local, and global) properties of structures on manifolds having non-trivial local moduli.

</doc>
<doc id="8564" url="http://en.wikipedia.org/wiki?curid=8564" title="Diffeomorphism">
Diffeomorphism

In mathematics, a diffeomorphism is an isomorphism of smooth manifolds. It is an invertible function that maps one differentiable manifold to another, such that both the function and its inverse are smooth. 
Definition.
Given two manifolds "M" and "N", a differentiable map "f" : "M" → "N" is called a diffeomorphism if it is a bijection and its inverse "f"−1 : "N" → "M" is differentiable as well. If these functions are "r" times continuously differentiable, "f" is called a "Cr"-diffeomorphism).
Two manifolds "M" and "N" are diffeomorphic (symbol usually being ≃) if there is a diffeomorphism "f" from "M" to "N". They are "Cr" diffeomorphic if there is an "r" times continuously differentiable bijective map between them whose inverse is also "r" times continuously differentiable.
Diffeomorphisms of subsets of manifolds.
Given a subset "X" of a manifold "M" and a subset "Y" of a manifold "N", a function "f" : "X" → "Y" is said to be smooth if for all "p" in "X" there is a neighborhood "U" ⊂ "M" of "p" and a smooth function "g" : "U" → "N" such that the restrictions agree formula_1 (note that "g" is an extension of "f"). We say that "f" is a diffeomorphism if it is bijective, smooth and its inverse is smooth.
Local description.
Model Example. If "U", "V" are connected open subsets of R"n" such that "V" is simply connected, a differentiable map "f" : "U" → "V" is a diffeomorphism, if it is proper and if the differential "Dfx" : R"n" → R"n" is bijective at each point "x" in "U".
Remark 1. It is essential for "V" to be simply connected for the function "f" to be globally invertible (under the sole condition that its derivative is a bijective map at each point). For example, consider the "realification" of the complex square function 
Then "f" is surjective and its satisfies 
thus "Dfx" is bijective at each point yet "f" is not invertible, because it fails to be injective, e.g., "f"(1,0) = (1,0) = "f"(−1,0).
Remark 2. Since the differential at a point (for a differentiable function) 
is a linear map it has a well defined inverse if, and only if, "Dfx" is a bijection. The matrix representation of "Dfx" is the "n" × "n" matrix of first order partial derivatives whose entry in the "i"-th row and "j"-th column is formula_5. We often use this so-called Jacobian matrix for explicit computations.
Remark 3. Diffeomorphisms are necessarily between manifolds of the same dimension. Imagine that "f" were going from dimension "n" to dimension "k". If "n" < "k" then "Dfx" could never be surjective, and if "n" > "k" then "Dfx" could never be injective. So in both cases "Dfx" fails to be a bijection.
Remark 4. If "Dfx" is a bijection at "x" then we say that "f" is a local diffeomorphism (since by continuity "Dfy" will also be bijective for all "y" sufficiently close to "x").
Remark 5. Given a smooth map from dimension "n" to dimension "k", if "Df" (resp. "Dfx") is surjective then we say that "f" is a submersion (resp. local submersion), and if "Df" (resp. "Dfx") is injective we say that "f" is an immersion (resp. local immersion).
Remark 6. A differentiable bijection is "not" necessarily a diffeomorphism, e.g. "f"("x") = "x"3 is not a diffeomorphism from R to itself because its derivative vanishes at 0 (and hence its inverse is not differentiable at 0). This is an example of a homeomorphism that is not a diffeomorphism.
Remark 7. "f" being a diffeomorphism is a stronger condition than "f" being a homeomorphism (when "f" is a map between "differentiable" manifolds). For a diffeomorphism we need "f" and its inverse to be differentiable. For a homeomorphism we only require that "f" and its inverse be continuous. Thus every diffeomorphism is a homeomorphism, but the converse is false: not every homeomorphism is a diffeomorphism.
Now, "f" : "M" → "N" is called a diffeomorphism if in coordinates charts it satisfies the definition above. More precisely, pick any cover of "M" by compatible coordinate charts, and do the same for "N". Let φ and ψ be charts on "M" and "N" respectively, with "U" being the image of φ and "V" the image of ψ. Then the conditions says that the map ψ"f"φ−1 : "U" → "V" is a diffeomorphism as in the definition above (whenever it makes sense). One has to check that for every pair of charts φ, ψ of two given atlases, but once checked, it will be true for any other compatible chart. Again we see that dimensions have to agree.
Examples.
Since any manifold can be locally parametrised, we can consider some explicit maps from R2 into R2. 
Diffeomorphism group.
Let "M" be a differentiable manifold that is second-countable and Hausdorff. The diffeomorphism group of "M" is the group of all "Cr" diffeomorphisms of "M" to itself, and is denoted by Diff"r"("M") or Diff("M") when "r" is understood. This is a 'large' group, in the sense that it is not locally compact (provided "M" is not zero-dimensional).
Topology.
The diffeomorphism group has two natural topologies, called the "weak" and "strong" topology . When the manifold is compact, these two topologies agree. The weak topology is always metrizable. When the manifold is not compact, the strong topology captures the behavior of functions "at infinity", and is not metrizable. It is, however, still Baire.
Fixing a Riemannian metric on "M", the weak topology is the topology induced by the family of metrics
as "K" varies over compact subsets of "M". Indeed, since "M" is σ-compact, there is a sequence of compact subsets "K""n" whose union is "M". Then, define
The diffeomorphism group equipped with its weak topology is locally homeomorphic to the space of "Cr" vector fields . Over a compact subset of "M", this follows by fixing a Riemannian metric on "M" and using the exponential map for that metric. If "r" is finite and the manifold is compact, the space of vector fields is a Banach space. Moreover, the transition maps from one chart of this atlas to another are smooth, making the diffeomorphism group into a Banach manifold. If "r" = ∞ or if the manifold is σ-compact, the space of vector fields is a Fréchet space. Moreover, the transition maps are smooth, making the diffeomorphism group into a Fréchet manifold.
Lie algebra.
In particular, the Lie algebra of the diffeomorphism group of "M" consists of all vector fields on "M", equipped with the Lie bracket of vector fields. Somewhat formally, this is seen by making a small change to the coordinate x at each point in space:
so the infinitesimal generators are the vector fields
Transitivity.
For a connected manifold "M" the diffeomorphism group acts transitively on "M". More generally, the diffeomorphism group acts transitively on the configuration space "CkM". If the dimension of "M" is at least two the diffeomorphism group acts transitively on the configuration space "FkM": the action on "M" is multiply transitive .
Extensions of diffeomorphisms.
In 1926, Tibor Radó asked whether the harmonic extension of any homeomorphism (or diffeomorphism) of the unit circle to the unit disc yields a diffeomorphism on the open disc. An elegant proof was provided shortly afterwards by Hellmuth Kneser and a completely different proof was discovered in 1945 by Gustave Choquet, apparently unaware that the theorem was already known. 
The (orientation-preserving) diffeomorphism group of the circle is pathwise connected. This can be seen by noting that any such diffeomorphism can be lifted to a diffeomorphism "f" of the reals satisfying ; this space is convex and hence path connected. A smooth eventually constant path to the identity gives a second more elementary way of extending a diffeomorphism from the circle to the open unit disc (this is a special case of the Alexander trick). Moreover, the diffeomorphism group of the circle has the homotopy-type of the orthogonal group O(2).
The corresponding extension problem for diffeomorphisms of higher-dimensional spheres S"n"−1 was much studied in the 1950s and 1960s, with notable contributions from René Thom, John Milnor and Stephen Smale. An obstruction to such extensions is given by the finite Abelian group Γ"n", the "group of twisted spheres", defined as the quotient of the Abelian component group of the diffeomorphism group by the subgroup of classes extending to diffeomorphisms of the ball "B""n".
Connectedness.
For manifolds the diffeomorphism group is usually not connected. Its component group is called the mapping class group. In dimension 2, i.e. for surfaces, the mapping class group is a finitely presented group, generated by Dehn twists (Dehn, Lickorish, Hatcher). Max Dehn and Jakob Nielsen showed that it can be identified with the outer automorphism group of the fundamental group of the surface. 
William Thurston refined this analysis by classifying elements of the mapping class group into three types: those equivalent to a periodic diffeomorphism; those equivalent to a diffeomorphism leaving a simple closed curve invariant; and those equivalent to pseudo-Anosov diffeomorphisms. In the case of the torus S1 × S1 = R2/Z2, the mapping class group is just the modular group SL(2, Z) and the classification reduces to the classical one in terms of elliptic, parabolic and hyperbolic matrices. Thurston accomplished his classification by observing that the mapping class group acted naturally on a compactification of Teichmüller space; since this enlarged space was homeomorphic to a closed ball, the Brouwer fixed-point theorem became applicable.
If "M" is an oriented smooth closed manifold, it was conjectured by Smale that the identity component of the group of orientation-preserving diffeomorphisms is simple. This had first been proved for a product of circles by Michel Herman; it was proved in full generality by Thurston.
Homeomorphism and diffeomorphism.
It is easy to find a homeomorphism that is not a diffeomorphism, but it is more difficult to find a pair of homeomorphic manifolds that are not diffeomorphic. In dimensions 1, 2, 3, any pair of homeomorphic smooth manifolds are diffeomorphic. In dimension 4 or greater, examples of homeomorphic but not diffeomorphic pairs have been found. The first such example was constructed by John Milnor in dimension 7. He constructed a smooth 7-dimensional manifold (called now Milnor's sphere) that is homeomorphic to the standard 7-sphere but not diffeomorphic to it. There are in fact 28 oriented diffeomorphism classes of manifolds homeomorphic to the 7-sphere (each of them is the total space of a fiber bundle over the 4-sphere with the 3-sphere as the fiber).
Much more extreme phenomena occur for 4-manifolds: in the early 1980s, a combination of results due to Simon Donaldson and Michael Freedman led to the discovery of exotic R4s: there are uncountably many pairwise non-diffeomorphic open subsets of R4 each of which is homeomorphic to R4, and also there are uncountably many pairwise non-diffeomorphic differentiable manifolds homeomorphic to R4 that do not embed smoothly in R4.
References.
Chaudhuri, Shyamoli, Hakuru Kawai and S.-H Henry Tye. "Path-integral formulation of closed strings," Phys. Rev. D, 36: 1148, 1987.

</doc>
<doc id="8567" url="http://en.wikipedia.org/wiki?curid=8567" title="Dune Messiah">
Dune Messiah

Dune Messiah is a science fiction novel by Frank Herbert, the second in a series of six novels. It was originally serialized in "Galaxy" magazine in 1969. The American and British editions have different prologues summarizing events in the previous novel. The novels "Dune Messiah" and "Children of Dune" were adapted by the Sci-Fi Channel in 2003 into a mini-series entitled "Frank Herbert's Children of Dune". In 2002, the Science Fiction Book Club also published the two novels in one volume.
Plot summary.
Twelve years after the events described in "Dune" (1965), Paul "Muad'Dib" Atreides rules as Emperor. By accepting the role of messiah to the Fremen, Paul had unleashed a jihad which conquered most of the known universe. While Paul is the most powerful Emperor ever known, he is powerless to stop the lethal excesses of the religious juggernaut he has created. Although sixty-one billion people have perished, Paul's prescient visions indicate that this is far from the worst possible outcome for humanity. Motivated by this knowledge, Paul hopes to set humanity on a course that will not inevitably lead to stagnation and destruction, while at the same time acting as ruler of the Empire and focal point of the Fremen religion.
The Bene Gesserit, Spacing Guild and Tleilaxu enter into a conspiracy to dethrone Paul, the Bene Gesserit Reverend Mother Gaius Helen Mohiam enlisting Paul's own consort Princess Irulan, daughter of the deposed Padishah Emperor Shaddam Corrino IV. Paul has refused to father a child with Irulan (or even touch her), but his Fremen concubine Chani has also failed to produce an heir, causing tension within his monarchy. Desperate both to secure her place in the Atreides dynasty and to preserve the Atreides bloodline for the Bene Gesserit breeding program, Irulan has secretly been giving contraceptives to Chani. Paul is aware of this fact, but has foreseen that the birth of his heir will bring Chani's death, and does not want to lose her. He sees this in a terrifying vision of a moon that falls from the sky of Dune. Because of the way oracles interfere with one another's prescience, the Guild Navigator Edric is able to shield the conspiracy from Paul's visions of the future. The Tleilaxu Face Dancer Scytale gives Paul a gift he cannot resist: a Tleilaxu-grown ghola of the deceased Duncan Idaho, Paul's childhood teacher and friend, now called "Hayt". The conspirators hope the presence of Hayt will undermine Paul's ability to rule by forcing Paul to question himself and the empire he has created. Furthermore, Paul's acceptance of the gift weakens his support among the Fremen, who see the Tleilaxu and their tools as unclean. Chani, taking matters into her own hands, switches to a traditional Fremen fertility diet, preventing Irulan from being able to tamper with her food, and soon becomes pregnant.
Otheym, one of Paul's former Fedaykin death commandos, reveals evidence of a Fremen conspiracy against Paul. Otheym gives Paul his dwarf Tleilaxu servant Bijaz who, like a recording machine, can remember faces, names, and details. Paul accepts reluctantly, seeing the strands of a Tleilaxu plot. As Paul's soldiers attack the conspirators, others set off an atomic weapon called a stone burner, purchased from the Tleilaxu, that destroys the area and blinds Paul. By tradition, all blind Fremen are abandoned in the desert, but Paul shocks the Fremen and entrenches his godhead by proving he can still see, even without eyes. His oracular powers have become so developed that he can foresee in his mind everything that happens, as though his eyes still function. By moving through his life in lockstep with his visions, he can see even the slightest details of the world around him. The disadvantage of this is his inability to change any part of his destiny, trapping him in a hellish boredom. The unraveling of the Fremen conspiracy reveals that Korba, a former Fedaykin and now high priest of Paul's church, is among Paul's enemies.
Duncan interrogates Bijaz, but the little man—actually an agent of the Tleilaxu—uses a specific humming intonation that renders Duncan open to implanted commands. Bijaz programs Duncan to offer Paul a bargain when Chani dies: Chani's rebirth as a ghola, and the hope that Duncan Idaho's memories might be reawakened, in return for Paul sacrificing the throne and going into exile. Bijaz also implants a compulsion that will force Duncan to attempt to kill Paul, given the appropriate circumstances. Duncan remains oblivious of the programming. Eventually news is brought that Chani has died giving birth. The grief of his loss is the falling moon that he foresaw in an earlier vision, and Paul stumbles, truly blind now, having removed himself from the prison of his own precise vision. Paul's reaction to his wife's death triggers the compulsions in the mind of Duncan, who attempts to kill Paul. But rather than kill his beloved Paul, Duncan's ghola body reacts against its own programming and recovers Duncan's full consciousness. He remains conscious of the Zen-Sunni and Mentat training given to Duncan by the Tleilaxu, but is no longer bound to their programming.
Paul and Chani's newborn twins are "pre-born", like Paul's sister Alia had been, and come into the world fully conscious with Kwisatz Haderach-like access to ancestral memories thanks to a combination of their genes and an in utero exposure to the quantities of spice in Chani's special pregnancy diet. Scytale offers to revive Chani as a ghola in return for all of Paul's CHOAM holdings. Paul refuses to submit to the possibility that the Tleilaxu might program Chani in some diabolical way, and Scytale threatens the infants with a knife while he negotiates with Alia. By successfully escaping the oracular trap and setting the universe on a new path, Paul has been rendered completely blind, yet he is able to kill Scytale with an accurately aimed dagger thanks to a vision from his son's perspective.
Now prophetically as well as physically blind, Paul chooses to embrace the Fremen tradition of a blind man walking alone into the desert, winning the fealty of the Fremen for his children, who will inherit his mantle of Emperor. Paul leaves Alia, now romantically involved with Duncan, as regent for the twins, whom he has named Leto and Ghanima. Duncan notes the irony that Paul and Chani's deaths had enabled them to triumph against their enemies, and that Paul has escaped deification by walking into the desert as a man, while guaranteeing Fremen support for the Atreides line.
Reception.
Spider Robinson noted that he enjoyed the book, "even as [he] was driving a truck through the holes in its logic, because it had the same majestic rolling grandeur of the previous book."

</doc>
