<doc id="7893" url="http://en.wikipedia.org/wiki?curid=7893" title="Dale Earnhardt">
Dale Earnhardt

Ralph Dale Earnhardt, Sr. (April 29, 1951February 18, 2001) was an American race car driver and team owner, best known for his involvement in stock car racing for NASCAR. Earnhardt began his career in 1975 when he drove in the 1975 World 600 at Charlotte Motor Speedway as part of the Winston Cup Series (later the Sprint Cup Series).
Considered one of the best NASCAR drivers of all time, Earnhardt won a total of 76 races over the course of his career, including one Daytona 500 victory in 1998. He earned 7 NASCAR Winston Cup Championships, which is tied for the most all time with Richard Petty. His aggressive driving style earned him the nickname "The Intimidator".
On February 18, 2001 at Daytona International Speedway while driving in the 2001 Daytona 500, Earnhardt was involved in a last-lap crash, and died of a basilar skull fracture. He has been inducted into numerous halls of fame, including the inaugural class of the NASCAR Hall of Fame.
Biography.
Early and personal life.
Earnhardt had German ancestry. He was born in Kannapolis, North Carolina, on April 29, 1951, to Martha Coleman and Ralph Lee Earnhardt, who was then one of the best short-track drivers in North Carolina. Ralph won his one and only NASCAR Sportsman Championship in 1956 at Greenville Pickens Speedway in Greenville, South Carolina. Although Ralph did not want his son to follow in his footsteps, Earnhardt would not be persuaded to give up his dream of racing, dropping out of school to race. Ralph was a hard teacher for Earnhardt, and after Ralph died of a heart attack at his home in 1973, it took many years before Earnhardt felt as though he had finally "proven" himself to his father. Earnhardt had four siblings, Danny, Randy (died 2013), Cathy, and Kaye.
At age 17, Earnhardt married his first wife, Latane Brown, in 1968. Brown gave birth to Earnhardt's first son, Kerry Earnhardt, in 1969. They were subsequently divorced in 1970. In 1971, Earnhardt married his second wife, Brenda Gee - the daughter of NASCAR car builder Robert Gee. With Gee, Earnhardt had two more children: a daughter, Kelley King, in 1972, and a son, Dale Earnhardt, Jr., in 1974. Not long after Dale Jr. was born, Dale Sr. and Brenda divorced. Earnhardt then married his last wife, Teresa Houston (Tommy Houston's niece) in 1982, who gave birth to their daughter, Taylor Nicole in 1988.
NASCAR career.
Early Winston Cup career 1975-1978.
Earnhardt began his professional career at the Winston Cup in 1975, making his debut at the Charlotte Motor Speedway in North Carolina in the longest race on the Cup circuit, the World 600. Earnhardt drove an Ed Negre Dodge Charger (#8) and finished 22nd in the race, one place ahead of his future car owner, Richard Childress. Earnhardt competed in 8 more races until 1979. 
Rod Osterlund Racing 1979-1980.
when he joined car owner Rod Osterlund Racing, in a season that included a rookie class of future stars inculding Earnhardt, Harry Gant and Terry Labonte.
In his rookie season, Earnhardt won one race at Bristol, captured four poles, had 11 Top 5 finishes, 17 Top 10 finishes, and finished 7th in the points standings, in spite of missing four races because of a broken collarbone, winning Rookie of the Year honors.
In his sophomore season, Earnhardt, now with 20-year old Doug Richert as his crew chief, began the season winning the Busch Clash. With wins at Atlanta, Bristol, Nashville, Martinsville, and Charlotte, Earnhardt won his first Winston Cup championship. To this day, Earnhardt is the first and only driver in NASCAR Winston Cup history to follow a Rookie of the Year title with a NASCAR Winston Cup Championship the next season. He was the third driver in NASCAR history to win both the Rookie of the Year and Cup Series championship in his career, joining David Pearson and Richard Petty. Only 5 drivers have joined this exclusive club since – Rusty Wallace, Alan Kulwicki, Jeff Gordon, Tony Stewart, and Matt Kenseth.
1981 with Rod Osterlund Racing, Stacy Racing and Richard Childress Racing.
In 1981, after Osterlund sold his team to J.D. Stacy, Earnhardt left for Richard Childress Racing, and finished the season 7th in the points standings but winless. 
Bud Moore Engineering 1982-1983.
The following year, at Childress's suggestion, Earnhardt joined car owner Bud Moore for the 1982 and 1983 seasons driving the No. 15 Wrangler Jeans Ford Thunderbird (Earnhardt's only full-time Ford ride in his career). During the 1982 season, Earnhardt struggled. Although he won at Darlington, he failed to finish 15 races, and completed the season 12th in points, the worst of his career. He also suffered a broken knee cap at Pocono Raceway when he flipped after contact with Tim Richmond. In 1983, Earnhardt rebounded and won his first of 12 Twin 125 Daytona 500 qualifying races. Earnhardt won at Nashville and at Talladega, finishing eighth in the points standings.
Return to Richard Childress Racing 1984-2001.
After the 1983 season, Earnhardt returned to Richard Childress Racing, replacing Ricky Rudd in the #3. Rudd went to Bud Moore's No. 15, replacing Earnhardt. Wrangler sponsored both drivers at their respective teams. During the 1984 and 1985 seasons, Earnhardt visited victory lane six times, at Talladega, Atlanta, Richmond, Bristol (twice), and Martinsville, where he finished fourth and eighth in the season standings, respectively.
The 1986 season saw Earnhardt win his second career Winston Cup Championship and the first owner's championship for RCR. He won five races and had ten Top 5 and sixteen Top 10 finishes. Earnhardt successfully defended his championship the following year, visiting victory lane eleven times and winning the championship by 489 points over Bill Elliott. In the process, Earnhardt set a NASCAR modern era record of four consecutive wins and won five of the first seven races. In the 1987 season, Earnhardt earned his nickname "The Intimidator" after spinning out Elliott in the final segment of "The Winston", a non-points event now known as the NASCAR Sprint All-Star Race. During this race, Earnhardt was briefly forced into the infield grass, but kept control of his car and returned to the track without giving up his lead. The maneuver is now referred to as the "Pass in the Grass," even though Earnhardt did not pass anyone while he was off the track.
The 1988 season saw Earnhardt racing with a new sponsor, GM Goodwrench, which replaced Wrangler Jeans. During this season Earnhardt garnered a second nickname, "The Man in Black", owing to the black paint scheme in which the No. 3 car was painted. He was also called "Darth Vader" more than once because of the black uniform and car, adding to his notoriety as a driver who would wreck any car he could not pass. He won three times in 1988, finishing third in the points standings behind Bill Elliott and Rusty Wallace. The following year, Earnhardt won five times, but a late spin out at North Wilkesboro arguably cost him the 1989 championship, as Rusty Wallace edged out Earnhardt for the championship.
The 1990 season started for Earnhardt with victories in the Busch Clash and his heat of the Gatorade Twin 125s. Near the end of the Daytona 500, he had a four-second lead when the final caution flag came out with a handful of laps to go. When the green flag waved, Earnhardt was leading Derrike Cope. On the final lap, Earnhardt ran over a piece of metal, later revealed to be a bell housing,in the final turn, cutting a tire. Cope, in an upset, won the race while Earnhardt finished fifth. The No. 3 Goodwrench Chevy team took the flat tire that cost them the win and hung it on the shop wall as a reminder of how close they'd come to winning the Daytona 500. Earnhardt went on to win nine races that season and won his fourth Winston Cup title, beating Mark Martin by 26 points. Earnhardt also became the first repeat winner of the annual all-star race, The Winston.
The 1991 season saw Earnhardt win his fifth Winston Cup championship. He scored just four wins, but won the championship by 195 points over Ricky Rudd. One of his wins that year came at North Wilkesboro, in a race where Harry Gant had a chance to set a single-season record by winning his fifth consecutive race, breaking a record held by Earnhardt. Late in the race, Gant lost his brakes, which gave Earnhardt the chance he needed to make the pass for the win and maintain his record.
Earnhardt's only win of the 1992 season came at Charlotte, in the Coca-Cola 600, ending a 13-race win streak by Ford teams. Earnhardt finished a career-low 12th in the points for the second time in his career, and the only time he had finished that low since joining RCR. Earnhardt still made the trip to the annual Awards Banquet with Rusty Wallace but did not have the best seat in the house. Wallace stated he and Earnhardt had to sit on the backs of their chairs to see and Earnhardt said "This sucks, I could have gone hunting". At the end of the year, longtime crew chief Kirk Shelmerdine left to become a driver. Andy Petree took over as crew chief.
Hiring Petree turned out to be beneficial, as the No. 3 GM Goodwrench Chevy returned to the front in 1993. Earnhardt once again came close to a win at the Daytona 500, and dominated Speedweeks before finishing second to Dale Jarrett on a last-lap pass. Earnhardt scored six wins en route to his sixth Winston Cup title, including wins in the Coca-Cola 600 and The Winston at Charlotte, and the Pepsi 400 at Daytona. Earnhardt beat Rusty Wallace for the championship by 80 points.
In 1994, Earnhardt achieved a feat that he himself had believed to be impossible – he scored his seventh Winston Cup championship, tying Richard Petty. Earnhardt was very consistent, scoring four wins, and after Ernie Irvan was sidelined due to a near-deadly crash at Michigan (the two were neck-and-neck at the top of the points up until the crash), won title by over 400 points over Mark Martin. Earnhardt sealed the deal at Rockingham by winning the race over Rick Mast. It would be his final NASCAR championship.
Earnhardt started off the 1995 season by finishing second in the Daytona 500 to Sterling Marlin. He won five races in 1995, including his first road course victory at Sears Point. He also won the Brickyard 400 at Indianapolis Motor Speedway, a win he called the biggest of his career. But in the end, Earnhardt lost the championship to Jeff Gordon by 34 points.
1996 for Earnhardt started just as it had done in 1993 – he dominated Speedweeks only to finish second in the Daytona 500 to Dale Jarrett for a second time. Earnhardt won early in the year, scoring consecutive victories at Rockingham and Atlanta. In late July in the DieHard 500 at Talladega, he was in the points lead and looking for his eighth title despite the departure of crew chief Andy Petree. Late in the race, Ernie Irvan lost control of his No. 28 Havoline Ford Thunderbird, made contact with the No. 4 Kodak Chevy Monte Carlo of Sterling Marlin, and igniting a frightening crash that saw Earnhardt's No. 3 Chevrolet hit the tri-oval wall nearly head-on at almost 200 miles per hour. After hitting the wall, Earnhardt's car flipped and slid across the track, in front of race-traffic. His car was hit in the roof and windshield. This accident, as well as a similar accident that led to the death of Russell Phillips at Charlotte, led NASCAR to mandate the "Earnhardt Bar", a metal brace located in the center of the windshield that reinforces the roof in case of a similar crash. This bar is also required in NASCAR-owned United SportsCar Racing and its predecessors for road racing.
Rain-delays had canceled the live telecast of the race and most fans first learned of the accident during the night's sports newscasts. Video of the crash showed what appeared to be a fatal incident, but once medical workers arrived at the car, Earnhardt climbed out and waved to the crowd, refusing to be loaded onto a stretcher despite a broken collarbone, sternum, and shoulder blade. Many thought the incident would end his season early, but Earnhardt refused to give up. The next week at Indianapolis, he started the race but exited the car on the first pit stop, allowing Mike Skinner to take the wheel. When asked, Earnhardt said that vacating the No. 3 car was the hardest thing he'd ever done. The following weekend at Watkins Glen, he drove the No. 3 Goodwrench Chevrolet to the fastest time in qualifying, earning the "True Grit" pole. T-shirts emblazoned with Earnhardt's face were quickly printed up, brandishing the caption, "It Hurt So Good". Earnhardt led for most of the race and looked to have victory in hand, but fatigue finally took its toll and Earnhardt ended up sixth, behind race winner Geoff Bodine. Earnhardt did not win again in 1996, but still finished fourth in the standings behind Terry Labonte, Jeff Gordon and Dale Jarrett. David Smith departed as crew chief of the No. 3 team and RCR at the end of the year for personal reasons, and was replaced by Larry McReynolds.
In 1997, Earnhardt went winless for only the second time in his career. The only (non-points) win came during Speedweeks at Daytona in the Twin 125-mile qualifying race, his record 8th-straight win in the event. Once again in the hunt for the Daytona 500 with 10 laps to go, Earnhardt was taken out of contention by a late crash which sent his car upside down on the backstretch. Earnhardt hit the low point of his year when he blacked out early in the Mountain Dew Southern 500 at Darlington in September, causing him to hit the wall. Afterward, he was disoriented and it took several laps before he could find his pit stall. When asked, Earnhardt complained of double vision which made it difficult to pit. Mike Dillon (Richard Childress's son-in-law) was brought in to relieve Earnhardt for the remainder of the race. Earnhardt was evaluated at a local hospital and cleared to race the very next week, but the cause of the blackout and double vision was never determined. Despite no wins, the RCR team finished the season 5th in the final standings.
1998 saw Earnhardt finally win the Daytona 500 after being shut out in his previous 19 attempts. Earnhardt began the season by winning his Twin 125-mile qualifier race for the ninth straight year. On race day, Earnhardt showed himself to be a contender early. Halfway through the race, however, it seemed that Jeff Gordon had the upper hand. But by lap 138, Earnhardt had taken the lead, and thanks to a push by teammate Mike Skinner, he was able to maintain it. Earnhardt made it to the caution checkered flag before Bobby Labonte. Afterwards, there was a large show of respect for Earnhardt, in which every crew member of every team lined pit road to shake his hand as he made his way to victory lane. Earnhardt then drove his No. 3 into the infield grass, starting a trend of post-race celebrations. He spun the car twice, throwing grass and leaving tire tracks in the shape of a No. 3 in the grass. Earnhardt then spoke about the victory, saying "I have had a lot of great fans and people behind me all through the years and I just can't thank them enough. The Daytona 500 is ours. We won it! We won it! We won it!" The rest of the season did not go as well, and Daytona was his only victory that year. He slipped to 12th in the standings halfway through the season, and Richard Childress decided to make a crew chief change, taking Mike Skinner's crew chief Kevin Hamlin and putting him with Earnhardt while giving Skinner Larry McReynolds. Earnhardt finished eighth in the final standings.
Before the 1999 season, fans began discussing Earnhardt's age and speculating that with his son, Dale Jr., getting into racing, Earnhardt might be contemplating retirement. Earnhardt swept both races for the year at Talladega, leading most observers to conclude that Earnhardt's talent had become limited to the restrictor plate tracks, which require a unique skill set and an exceptionally powerful car to win. But halfway through the year, Earnhardt began to show some of the old spark. In the August race at Michigan International Speedway, Earnhardt led laps late in the race and nearly pulled off his first win on a non-restrictor-plate track since 1996.
One week later, he provided NASCAR with one of its most controversial moments. At the August Bristol race, Earnhardt found himself in contention to win his first short track race since Martinsville in 1995. When a caution came out with 15 laps to go, leader Terry Labonte got hit from behind by the lapped car of Darrell Waltrip. His spin put Earnhardt in the lead with five cars between him and Labonte with 5 laps to go. Labonte had four fresh tires and Earnhardt was driving on old tires, which made Earnhardt's car considerably slower. Labonte caught Earnhardt and passed him coming to the white flag, but Earnhardt drove hard into turn two, bumping Labonte and spinning him around. Earnhardt went on to collect the win while spectators booed and made obscene gestures. "I didn't mean to turn him around, I just wanted to rattle his cage", Earnhardt said of the incident. Earnhardt finished seventh in the standings that year.
In the 2000 season, Earnhardt had a resurgence, which was commonly attributed to neck surgery he underwent to correct a lingering injury from his 1996 Talladega crash. He scored what were considered the two most exciting wins of the year – winning by .006 seconds over Bobby Labonte at Atlanta, then gaining seventeen positions in the final four laps to win at Talladega, claiming his only No Bull million dollar bonus. Earnhardt also had second-place runs at Richmond and Martinsville, tracks where he'd struggled through the late 1990s. On the strength of those performances, Earnhardt was able to get to second in the standings. However, poor performances at the road course of Watkins Glen, where he wrecked coming out of the chicane, a wreck with Kenny Irwin Jr. while leading the spring race at Bristol, and mid-pack runs at intermediate tracks like Charlotte and Dover in a season dominated by the Ford Taurus in those tracks from Roush, Yates, and Penske, coupled with Labonte's extreme consistency, denied Earnhardt an eighth championship title.
Death.
From February 3, 2001, to February 4, 2001, Earnhardt participated in the Rolex 24 endurance race at the Daytona International Speedway. The team composed of Earnhardt, Earnhardt, Jr., Andy Pilgrim, and Kelly Collins finished 4th overall and 2nd in class.
At the 2001 Daytona 500 on February 18, 2001, Earnhardt started in 7th place. He was involved in an accident during the final lap, in which Earnhardt's car was turned from behind after contacting the car driven by Sterling Marlin into the outside wall nose-first, into the path of Ken Schrader's car. Michael Waltrip won the race, with Dale Earnhardt, Jr. in second place. Earnhardt, Sr. and Schrader slid off the track's asphalt banking toward the infield grass just inside of turn four. Earnhardt Sr. was taken to Halifax Medical Center after he was extricated from his car, and was pronounced dead at 5:16 p.m. Hours later, Mike Helton, president of NASCAR announced to the officials, drivers and fans that Earnhardt had died from the accident. He was 49 years old.
An autopsy concluded that Earnhardt died instantly of blunt force trauma to the head.
Earnhardt's funeral was held on February 22, 2001, at the Calvary Church in Charlotte, North Carolina.
Aftermath.
After Earnhardt's death, a police investigation and a NASCAR-sanctioned investigation commenced; nearly every detail of the crash was made public. The allegations of seatbelt failure resulted in Bill Simpson's resignation from the company bearing his name, which manufactured the seatbelts used in Earnhardt's car and nearly every other NASCAR driver's car.
The effect that Earnhardt's death had on motorsports and the media frenzy that followed—not only in the United States, but all over the world—were both massive. Auto racing had not experienced a death of this magnitude since that of Brazilian Formula One driver Ayrton Senna in 1994. Senna was regarded as highly in Formula One as Earnhardt was in NASCAR; Earnhardt won the NASCAR Talladega race in 1994 on the day that Senna was killed, and in victory lane he expressed his sorrow for the Senna family.
NASCAR implemented rigorous safety improvements, such as making the HANS device mandatory. (Earnhardt had refused to wear it because he found it restrictive and uncomfortable.) Several press conferences were held in the days following Earnhardt's death. Some angry Earnhardt fans sent hate mail and death threats to Sterling Marlin and his relatives. In response, Michael Waltrip and Dale Earnhardt, Jr. absolved Marlin of any responsibility.
Richard Childress made a public pledge that the number 3 would never again adorn the side of a black car sponsored by GM Goodwrench. Childress, who currently holds the rights from NASCAR to the No. 3, has placed a moratorium on using it; the number returned for the 2014 season, driven by Childress's grandson Austin Dillon.
Immediately after Earnhardt's death, his team was re-christened as the No. 29 team, with the same sponsor but with a new look (an inverted color scheme – white with black numerals and a black stripe on the bottom) for the following races at Rockingham and Las Vegas. For Atlanta, a new GM Goodwrench scheme was introduced, with angled red stripes and a thin blue pinstripe, resembling the Childress AC Delco Chevrolets driven in the Busch Series.
Childress' second-year Busch Series driver Kevin Harvick was named as Earnhardt's replacement driver, beginning with the race following Earnhardt's death held at the North Carolina Speedway. Special pennants bearing the No. 3 were distributed to everyone at the track to honor Earnhardt, and the Childress team wore blank uniforms out of respect, something which disappeared quickly and was soon replaced by the previous GM Goodwrench Service Plus uniforms. Harvick's car always displayed the Earnhardt stylized number 3 on the "B" posts (metal portion on each side of the car to the rear of the front windows) above the number 29, until the end of 2013, when Harvick departed for Stewart-Haas Racing.
Fans began honoring Earnhardt by holding three fingers aloft on the third lap of every race, a black screen of number 3 in the beginning of "NASCAR Thunder 2002" before the EA Sports logo, and the television coverage of NASCAR on Fox and NASCAR on NBC went silent for each third lap from Rockingham to the following year's race there in honor of Earnhardt. On-track incidents brought out the caution flag on the third lap. Three weeks after Earnhardt's death, Harvick scored his first career Cup win at Atlanta driving a car that had been prepared for Earnhardt. In the final lap of the 2001 Cracker Barrel Old Country Store 500, Harvick beat Jeff Gordon by .006 seconds, the same margin that Earnhardt had won over Bobby Labonte at the same race a year prior, and the images of Earnhardt's longtime gas man, Danny "Chocolate" Myers, crying after the victory, Harvick's tire-smoking burnout on the frontstretch with three fingers held aloft outside the driver's window, and the Fox television call by Mike Joy, Larry McReynolds, and Darrell Waltrip, concluding with "Just like a year ago (With Earnhardt and Bobby Labonte) but he (Harvick) is gonna get him though...Gordon got loose... it's Harvick! Harvick by inches!" are memorable to many NASCAR fans. The win was also considered cathartic for a sport whose epicenter had been ripped away. Harvick would win another race at Chicagoland en route to a ninth place finish in the final points, and won Rookie of the Year honors.
Earnhardt's team, DEI, won four races in the regular 2001 season, with Steve Park winning the Rockingham race one week after Earnhardt's death. Dale Earnhardt, Jr. and Michael Waltrip finished 1-2 in the series' return to Daytona that July in the Pepsi 400, the reverse of their Daytona 500 finish. Earnhardt, Jr. also won the fall races at Dover and Talladega en route to an eighth place points finish.
Earnhardt was buried on his farm in Mooresville, North Carolina.
No. 3 car.
Earnhardt drove the No. 3 car for most of his career, spanning the early 1980s until his death in 2001. Although he had other sponsors during his career, his No. 3 is associated in fans' minds with his last sponsor, GM Goodwrench, and his last color scheme — a predominantly black car with bold red and silver trim. The black and red No. 3 continues to be one of the most famous logos in North American motor racing.
A common misconception is that Richard Childress Racing "owns the rights" to the No. 3 in NASCAR competition (fueled by the fact that Kevin Harvick's car has a little No. 3 as an homage to Earnhardt and the usage of the No. 3 on the Camping World Series truck of Ty Dillon), but in fact no team owns the rights to this or any other number. However, according to established NASCAR procedures, RCR would have priority over other teams if and when the time came to reuse the number. RCR owns the stylized No. 3 logos used during Earnhardt's lifetime; however these rights may not prevent a future racing team from using a different No. 3 design (also, a new No. 3 team would most likely, in any case, need to create logos which fit with their sponsor's logos).
In 2004, ESPN released a made-for-TV movie entitled "" which used a new (but similarly colored) No. 3 logo. The movie was a sympathetic portrayal of Earnhardt's life, but the producers were sued for using the No. 3 logo. In December 2006, the ESPN lawsuit was settled, but details were not released to the public.
It is generally believed that current NASCAR owners have agreed never to use the No. 3 in Sprint Cup competition again, although this is not official NASCAR policy. Dale Earnhardt Jr. made two special appearances in 2002 in a No. 3 Busch Series car: these appearances were at the track where his father died (Daytona) and the track where his father made his first Winston Cup start (Charlotte). Earnhardt Jr. won the first of those two races, which was the season-opening event at Daytona. He also raced a No. 3 sponsored by Wrangler on July 2, 2010 for Richard Childress Racing at Daytona. In a green-white- checker finish he outran Joey Logano to win his second race in the 3.
Otherwise, the No. 3 was missing from the national touring series until September 5, 2009, when Austin Dillon, the 19-year-old grandson of Richard Childress debuted an RCR-owned No. 3 truck in the Camping World Truck Series. Austin Dillon and his younger brother Ty Dillon drove #3's in various lower level competitions for several years, including the Camping World East Series. In 2012, Austin Dillon began driving the Nationwide Series full-time, using the #3. (He had previously used the #33 while driving that series part-time.)
Richard Childress Racing entered the number 3 in the Daytona Truck race on February 13, 2010 painted identically to when Earnhardt drove it, but with Bass Pro Shops as a sponsor. It was driven by Austin Dillon. Oddly, the number 3 was involved in a wreck almost identical to that which took the life of Earnhardt: being spun out, colliding with another vehicle and being turned into the outside wall in turn number four. Dillon again returned to a number 3 marked racecar when he started 5th in the 2012 Daytona Nationwide Series opener in an Advocare sponsored black Chevrolet Impala. On December 11, 2013, RCR announced that Austin Dillon will drive the No. 3 car in the upcoming 2014 season.
The owner of Earnhardt’s car, Richard Childress, his old friend, retired the No. 3 after Earnhardt’s death, said it never would be used again. However Childress’s grandson, Austin Dillon, made his NASCAR debut on February 23, 2014 at the Daytona 500 as a driver and was in a car with the No. 3 on the side.
Only the former International Race of Champions actually retired the No. 3, which they did in a rule change effective in 2004. Until the series folded in 2007, anyone wishing to use the No. 3 again had to use No. 03 instead.
Formula 1 driver Daniel Ricciardo chose the number 3 as his permanent racing number when F1's rules changed to allow drivers to choose their own numbers for 2014, and stated on Twitter that part of the reason for his choice was that he was a fan of Earnhardt's. And his helmet design features the number stylized in the same way.
Legacy.
Earnhardt has a street in his hometown of Kannapolis named after him. Dale Earnhardt Boulevard (originally Earnhardt Road) is marked as Exit 60 off Interstate 85, northeast of Charlotte. Dale Earnhardt Drive is also the start of The Dale Journey Trail, a self-guided driving tour of landmarks in the lives of Dale and his family. A road between Kannapolis and Mooresville, near the headquarters of DEI, formerly NC 136, was switched by the North Carolina Department of Transportation with State Highway 3 which was in Currituck County. In addition, Exit 73 off Interstate 35W, one of the entrances to Texas Motor Speedway, is named "Dale Earnhardt Way".
In 2000, shortly before his death, Earnhardt became a part-owner of the minor league baseball team in Kannapolis, and the team was renamed the Kannapolis Intimidators shortly thereafter. After his death, the team retired the jersey number 3 in Earnhardt's honor, and a "3" flag flies beyond the left field wall during every game. 
Atlanta Braves assistant coach Ned Yost was a friend of Earnhardt, and Richard Childress. When Yost was named Milwaukee Brewers manager, he changed jersey numbers, from No. 5 to No. 3 in Earnhardt's honor. (#3 is retired by the Braves in honor of outfielder Dale Murphy, so Yost could not make the change while in Atlanta.) When Yost was named Kansas City Royals assistant coach, he wore No. 2 for the 2010 season, even when he was named manager in May 2010, but for the 2011 season, he switched back to #3.
Between the 2004 and 2005 JGTC (subsequently renamed Super GT from 2005) season, Hasemi Sport competed in the series with a sole black G'Zox sponsored Nissan 350Z with the same number and letterset as Earnhardt on the roof.
During the April 29, 2006 – May 1, 2006 NASCAR weekend races at Talladega Superspeedway, the Dale Earnhardt Inc cars competed in identical special black paint schemes on Dale Earnhardt Day, held annually on his birthday, April 29. Martin Truex Jr won the Aaron's 312 in the black car, painted to reflect Earnhardt's Intimidating Black No. 3 NASCAR Busch Grand National series car. In the Nextel Cup race on May 1, No. 8 Dale Earnhardt Jr., No. 1 Martin Truex Jr., and No. 15 Paul Menard competed in cars with the same type of paint scheme.
On June 18, 2006 at Michigan for the 3M Performance 400 Dale Earnhardt Jr ran a special vintage Budweiser car to honor his father and his grandfather Ralph Earnhardt. He finished 3rd after rain caused the race to be cut short. The car was painted to resemble Ralph's 1956 dirt cars, and carried 1956-era Budweiser logos to complete the throwback look.
In the summer of 2007, Dale Earnhardt, Inc. (DEI) with the Dale Earnhardt Foundation, announced it will fund an annual undergraduate scholarship at Clemson University in Clemson, South Carolina for students interested in motorsports and automotive engineering. Scholarship winners are also eligible to work at DEI in internships. The first winner was William Bostic, a senior at Clemson majoring in mechanical engineering.
"Earnhardt Tower", a seating section at Daytona International Speedway, the track where Earnhardt was killed, was opened and named in his honor shortly before his death.
In 2008, on the 50th anniversary of the first Daytona 500 race, DEI and RCR teamed up to make a special COT sporting Earnhardt's 1998 Daytona 500 paint scheme to honor the tenth anniversary of his Daytona 500 victory. In a tribute to all previous Daytona 500 winners, the winning drivers appeared in a lineup on stage, in chronological order. The throwback No. 3 car stood in the infield, in the approximate position Earnhardt would have taken in the processional. The throwback car featured the authentic 1998-era design on a current-era car, a concept similar to modern throwback jerseys in other sports. The car was later sold in 1:64 and 1:24 scale models.
The Intimidator 305 roller coaster has been up and running since April 2010 at Kings Dominion in Doswell, Virginia. Named after Earnhardt, the ride's trains will be modeled after Dale Earnhardt's black-and-red Chevrolet.
 Another Intimidator was built at Carowinds, in Charlotte, NC.
During the third lap of the 2011 Daytona 500 (a decade since Earnhardt's death), the commentators on FOX fell silent while fans each raised three fingers in a similar fashion to the tributes throughout 2001.
The north entrance to New Avondale City Center in Arizona will bear the name Dale Earnhardt Drive. Avondale is where Earnhardt won a Cup race in 1990.
His helmet from the 1998 season is at the Smithsonian museum in Washington DC.
External links.
 
 
 
 
 
 
 

</doc>
<doc id="7896" url="http://en.wikipedia.org/wiki?curid=7896" title="List of games based on Dune">
List of games based on Dune

A number of games have been published which are based on the "Dune" universe created by Frank Herbert.
Video games.
To date, there have been five licensed "Dune"-related video games released. There have also been many "Dune"-based MUDs (Multi-User Dimension) and browser-based online games, all created and run by fans.
"Dune" (1992).
1992's "Dune" from Cryo Interactive/Virgin Interactive blends adventure with strategy. Loosely following the story of the 1965 novel "Dune" and using many visual elements from the 1984 film of the same name by David Lynch, the game casts the player as Paul Atreides, with the ultimate goal of driving the Harkonnens from the planet Dune and taking control of its valuable export, the spice. Key to success is the management of spice mining, military forces, and ecology as the player amasses allies and skills.
"Dune II" (1992).
"Dune II: The Building of a Dynasty", later retitled "Dune II: Battle for Arrakis" for the Mega Drive/Genesis port, was released in December 1992 from Westwood Studios/Virgin Interactive. Often considered to be the first "mainstream modern real-time strategy game", "Dune II" established many conventions of the genre. Only loosely connected to the plot of the novels or films, the game pits three interplanetary houses — the Atreides, the Harkonnens, and the Ordos — against each other for control of the planet Arrakis and its valuable spice, all while fending off the destructive natural forces of the harsh desert planet itself.
"Dune 2000" (1998).
"Dune 2000", a 1998 remake of "Dune II" from Intelligent Games/Westwood Studios/Virgin Interactive, added improved graphics and live-action cutscenes. Though gameplay is similar to its predecessor, "Dune 2000" features an enhanced storyline and functionality.
"Emperor: Battle for Dune" (2001).
"Emperor: Battle for Dune" (Intelligent Games/Westwood Studios/Electronic Arts) was released on June 12, 2001. A sequel to "Dune 2000", the real-time strategy game features 3D graphics and live-action cutscenes and casts players as Atreides, Harkonnens, or Ordos.
"Frank Herbert's Dune" (2001).
Released in 2001 by Cryo Interactive/DreamCatcher Interactive, "Frank Herbert's Dune" is a 3D video game based on the 2000 Sci Fi Channel miniseries of the same name. As Paul Muad'Dib Atreides, the player must become leader of the Fremen, seize control of Dune, and defeat the evil Baron Harkonnen. The game was not a commercial or critical success, and Cryo subsequently filed for bankruptcy in July 2002.
<div id="DUNE Generations"/><div id="Dune Generations"/>
"DUNE Generations" (2001, cancelled).
In 2001, Cryonetworks disclosed information about "Dune Generations", an online, 3D real-time strategy game set in the "Dune" universe. An official website for the upcoming game featured concept images, a brief background story and description of the persistent gameworld, and a list of frequently asked questions. The game would be constructed using Cryo's own online multimedia development framework SCOL.
Within "the infrastructure of a permanent and massive multiplayer world that exists online," "DUNE Generations" would let players assume control of a dynasty in the "Dune" universe, with the goal of first mastering the natural resources of their own homeworlds and ultimately rising in power and influence through conflicts and alliances with other player dynasties. Each of the three available dynasty types — traders, soldiers, or mercenaries — would provide a different playing experience, all with the long-term goal of gaining control of Arrakis and its valuable spice.
A preview video trailer was released in November 2001. The game was still in the alpha testing stage in February 2002, and the project was ultimately halted after Cryo filed for bankruptcy in July 2002.
Online games.
DuneMUSH.
Based on the PennMUSH text environment the popular Multi-User Shared Hallucination, DuneMUSH, began in December 1992 and ran through August of 1994. Another MUSH, Dune III, was later created on MUSHPark. Its timeline was set 100 years prior to that of the novel, and ran for a few years before closing in 2011.
Dune MUD.
Around since 1992, Dune MUD is a text based virtual environment that promotes the hack and slash gameplay mechanic, and is currently accessible at .

</doc>
<doc id="7900" url="http://en.wikipedia.org/wiki?curid=7900" title="List of Dune characters">
List of Dune characters

This is a list of the characters who appear in the novels of the fictional Dune universe, created by Frank Herbert and later expanded by Brian Herbert and Kevin J. Anderson.
This article provides links to many of the main characters in the "Dune" universe. They are grouped by primary allegiances. In some cases these allegiances change or reveal themselves to be different throughout the novels.
Titans and Neo-Cymeks.
Titans:
Neo-Cymeks:

</doc>
<doc id="7901" url="http://en.wikipedia.org/wiki?curid=7901" title="Vladimir Harkonnen">
Vladimir Harkonnen

The Baron Vladimir Harkonnen is a fictional character and antagonist from the "Dune" universe created by Frank Herbert. He is primarily featured in the 1965 novel "Dune" and is also a prominent character in the "Prelude to Dune" prequel trilogy (1999-2001) by Brian Herbert and Kevin J. Anderson. The character is brought back as a ghola in the Herbert/Anderson sequels which conclude the original series, "Hunters of Dune" (2006) and "Sandworms of Dune" (2007).
"Appendix IV: The Almanak en-Ashraf (Selected Excerpts of the Noble Houses)" in "Dune" says of him (in part):
VLADIMIR HARKONNEN (10,110-10,193) Commonly referred to as Baron Harkonnen, his title is officially Siridar (planetary governor) Baron. Vladimir Harkonnen is the direct-line male descendant of the Bashar Abulurd Harkonnen who was banished for cowardice after the Battle of Corrin. The return of House Harkonnen to power generally is ascribed to adroit manipulation of the whale fur market and later consolidation with melange wealth from Arrakis.
Characteristics.
In "Dune", Herbert notes that the Baron possesses a "basso voice" and is so "grossly and immensely fat" that he requires anti-gravity devices known as suspensors to support his weight.
As ruthless and cruel as he is intelligent and cunning, the Baron's greatest skill is his talent for the subtle and clever manipulation of others through their weaknesses or his understanding of human nature. His sexual preference for young men is implied in "Dune" and "Children of Dune". It is noted, however, that he "once permitted himself to be seduced" in the liaison which produced his secret daughter.
Plotlines.
"Dune".
As "Dune" begins, a longstanding feud exists between the Harkonnens of Giedi Prime and the Atreides of Caladan. The Baron's intent to exterminate the Atreides line seems close to fruition as Duke Leto Atreides is lured to the desert planet Arrakis on the pretense of taking over the valuable melange operation there. The Baron has an agent in the Atreides household: Leto's own physician, the trusted Suk doctor Wellington Yueh. Though Suk Imperial Conditioning supposedly makes the subject incapable of inflicting harm, the Baron's twisted Mentat Piter De Vries notes:
It's assumed that ultimate conditioning cannot be removed without killing the subject. However, as someone once observed, given the right lever you can move a planet. We found the lever that moved the doctor.
The Baron has taken Yueh's wife Wanna prisoner, threatening her with interminable torture unless Yueh complies with his demands. Harkonnen also distracts Leto's Mentat Thufir Hawat from discovering Yueh by guiding Hawat toward another suspect: Leto's Bene Gesserit concubine Lady Jessica. The Atreides are soon attacked by Harkonnen forces (secretly supplemented by the seemingly unstoppable Imperial Sardaukar) as Yueh disables the protective shields around the Atreides palace on Arrakis. As instructed, Yueh takes Leto prisoner; however, desiring to slay the Baron, Yueh provides the captive Leto with a fake tooth filled with poisonous gas as a means of simultaneous assassination and suicide. De Vries kills Yueh but he also dies with Leto in the assassination attempt; however Harkonnen survives. The Baron then manipulates Hawat into his service, insuring his control over the Mentat by secretly administering to him a residual poison invented by De Vries; to avoid death, the antidote for this poison must be taken regularly and continuously.
Leto and Jessica's son Paul Atreides flee into the desert with Jessica, and both are presumed dead. Paul's prescience helps him determine the identity of Jessica's father, the "maternal grandfather who cannot be named" — the Baron himself. Over the next two years, Harkonnen learns that both of his nephews Glossu Rabban and Feyd-Rautha are conspiring against him to obtain his throne; he lets them continue to do so, reasoning that they have to somehow learn to organize a conspiracy. As punishment for a failed assassination attempt against him, Harkonnen forces Feyd to single-handedly slaughter all the female slaves who serve as Feyd's lovers. He explains that Feyd has to learn the price of failure.
The Baron's plan to assure Feyd's power is to install him as ruler of Arrakis after a period of tyrannical misrule by Rabban, making Feyd appear to be the savior of the people. However, a crisis on Arrakis begins when the mysterious Muad'Dib emerges as a leader of the native Fremen tribes against the rule of the Harkonnens. Eventually, a series of Fremen victories against Beast Rabban threaten to disrupt the trade of the spice. The Padishah Emperor Shaddam Corrino IV decides to intervene himself and arrives on Arrakis along with legions of Sardaukar forces. Shaddam and the Baron are shocked to learn that Muad'Dib is, of course, a very-much-alive Paul Atreides. The Imperial forces fall prey to a surprise attack by the Fremen. Part of the Fremen/Atreides strategy is to wait until a sandstorm shorts out the force field shields of the Harkonnen/Imperial transport ships, disable them with projectile weapons, and then attack with a vast assault force, using giant sandworms under cover of the severe weather to break the enemy lines. The Sardaukar and Harkonnen forces are trapped on the planet, astonished at the sandworm mounts and vast numbers of their attackers. Their past ruthlessness gives them little hope of quarter from the enraged Fremen. 
Rabban dies in the initial part of the battle; the Harkonnen army is massacred to the last man and almost all the Imperial Sardaukar are killed. Baron Harkonnen himself is poisoned with a gom jabbar by Paul's young sister Alia Atreides, his own granddaughter, and dies at the age of 83. Paul then kills Feyd in ritual combat. House Harkonnen's virtual extermination removes it as a galactic power, but Paul's ascension to the Imperial throne in Shaddam's place guarantees that Vladimir's descendants will long reign as the Imperial House Atreides.
"Children of Dune".
Alia had been born with her ancestral memories in the womb, a circumstance the Bene Gesserit refer to as Abomination, because in their experience it is inevitable that the individual will become possessed by the personality of one of their ancestors. In "Children of Dune", Alia falls victim to this prediction when she shares control of her body with the ego-memory of the Baron Harkonnen, and eventually falls under his power. Alia eventually commits suicide, realizing that Harkonnen's consciousness has surpassed her abilities to contain him.
"Prelude to Dune".
In the "Prelude to Dune" prequel series by Brian Herbert and Anderson, it is established that Baron Vladimir Harkonnen is the son and heir of Dmitri Harkonnen and his wife Victoria. Harkonnen's father had been the head of House Harkonnen and ruled the planet Giedi Prime. Trained since youth as a possible successor, Vladimir had been eventually chosen over his half-brother Abulurd (namesake of the original). Unhappy with his brother's doings, Abulurd eventually marries Emmi Rabban and renounces the family name and his rights to the title. Under the name Abulurd Rabban, he reigns as governor of the secondary Harkonnen planet Lankiveil. Abulurd and his wife have two sons: Glossu Rabban (later nicknamed "Beast Rabban" after he murders his own father) and Feyd-Rautha; Vladimir later adopts the boys back into House Harkonnen, and Feyd becomes his designated heir.
The Baron's most prominent political rival is Duke Leto Atreides; the Harkonnens and the Atreides have been bitter enemies for millennia, since the Battle of Corrin that ended the Butlerian Jihad. When Emperor Shaddam IV orchestrates a plot to destroy the "Red Duke" Leto, the Baron eagerly lends his aid. 
The young Baron Vladimir is described as an exceedingly handsome man, possessing red hair and a near-perfect physique. The Bene Gesserit Reverend Mother Gaius Helen Mohiam is instructed by the Sisterhood to collect the genetic material of Baron Vladimir Harkonnen (through conception) for their breeding program. As the Baron's homosexuality is something of an open secret, Mohiam blackmails him into having sexual relations with her and conceives his child. When that daughter proves genetically undesirable, Mohiam kills her and returns to Harkonnen for a second try; at this point he drugs and viciously rapes her. She exacts her retribution by infecting him with a rare, incurable disease that later causes his obesity. Mohiam's second child with the Baron is Jessica.
In ', the deteriorating Baron at first walks with the assistance of a cane, then relies on belt-mounted suspensors to retain mobility. He consults numerous doctors in the expanse of time between the ' and "Dune: House Harkonnen", up to and including his future instrument Dr. Yueh, all of whom are ultimately no help. To conceal this debilitation, he pretends that his obesity is due to intentional overindulgence, lest the Landsraad remove him from power. When he determines that Mohiam inflicted him with the disease, he attempts to coerce her into revealing the cure, but soon discovers that there is none.
The Baron, Duke Leto, and Jessica herself are unaware that Jessica is secretly the Baron's daughter or that he has even fathered one; in the year 10,176, the Baron's grandson Paul is born to Leto and Jessica.
"Hunters of Dune".
In "Hunters of Dune" (2006), the continuation of the original series by Brian Herbert and Kevin J. Anderson, the Baron is resurrected as a ghola (5,029 years after the death of Alia) by the Lost Tleilaxu Uxtal, acting on orders from the Face Dancer Khrone. Khrone intends to use the Baron ghola to manipulate a ghola of Paul Atreides, named Paolo. Khrone tries various torture techniques for three years to awaken the 12-year-old Baron's genetic memories; these methods fail due to the Baron's sadomasochistic nature. Khrone is successful when he imprisons the Baron in a sensory deprivation tank for a prolonged period; the Baron's memories of his former life return. Ironically, the reincarnated Baron is soon haunted by the voice of Alia in his mind.
In adaptations.
In David Lynch's 1984 film, Baron Harkonnen was portrayed by Kenneth McMillan. In this characterization, he is grotesquely overweight, dressed in filthy garments and covered in large, black pustules which require constant draining and treatment. This version of the character is more overtly unstable than in the novel, screaming and laughing incoherently at any given moment and even drinking the blood of a servant after removing a "heart plug." 
British actor Ian McNeice's interpretation of the Baron in the 2000 Sci-Fi Channel miniseries "Frank Herbert's Dune" (and its sequel, 2003's "Children of Dune") is, though dramatic, somewhat lighter, more eloquent and sane in comparison to Lynch's version, and therefore more consistent with the novel. Though the Baron still takes sadistic enjoyment in the suffering of others, he is portrayed as somewhat flamboyant, pompous, calculating and self-indulgent, with a tendency to speak in iambic pentameter when the mood strikes him.
Though Herbert's novel "Dune" seems to describe Harkonnen's suspensor belt as simply enabling him to stand and walk upright rather than actually "fly," both the 1984 film and the 2000 miniseries feature the Baron utilizing the suspensors to levitate off the ground and float through the air in a flying-like manner. The later "Prelude to Dune" prequels also employ this floating ability. Herbert does, however, note Harkonnen floating slightly off the floor after he is killed.
Other media.
The video game "", whose in-game cut scenes are visually inspired by David Lynch's film, features a character named Baron Rakan Harkonnen, portrayed by Mike McShane. This Harkonnen is nearly identical to the film's version of Vladimir in both appearance (minus the belt-mounted suspensors) and personality, and also dies by poisoning.

</doc>
<doc id="7902" url="http://en.wikipedia.org/wiki?curid=7902" title="Piter De Vries">
Piter De Vries

Piter De Vries is a fictional character from the "Dune" universe created by Frank Herbert. He is featured in 1965's "Dune", the original novel in the science fiction series, as well as the "Prelude to Dune" prequel trilogy (1999–2001) by Brian Herbert and Kevin J. Anderson.
In David Lynch's 1984 adaptation of the first novel, De Vries was played by Brad Dourif. He was portrayed by Jan Unger in the 2000 Sci Fi Channel "Dune" miniseries.
Character.
In the service of the ruthless Baron Vladimir Harkonnen, De Vries is a Mentat — a human specially trained to perform mental functions rivaling computers, which are forbidden universe-wide. In addition, De Vries has been "twisted" (made into an amoral sadist) by his Tleilaxu creators.
De Vries is so loyal to Harkonnen that he continues to serve the Baron with great enthusiasm even though his Mentat abilities and great intelligence confirm his suspicions that his master plans to eventually kill him. As he says in "Dune":
DeVries is described in the novel "Dune" (though not portrayed on screen) as being addicted to the drug melange, which has colored both the sclera and irises of his eyes a characteristic deep blue.
"Dune".
In "Dune", it is established that De Vries had pioneered a type of toxin called "residual poison" which remains in the body for years and requires an antidote to be administered regularly. One such fatal poison is secretly administered by the Harkonnens to Thufir Hawat, the Mentat of House Atreides, in order to keep Hawat's allegiance as the only provider of the antidote (in the 1984 movie version, it is shown that Hawat has to milk a gruesome captive cat for the antidote every day).
De Vries is generally regarded as architect of the plan to destroy House Atreides, long-time enemy of the Harkonnens, while restoring the Baron's stewardship over the planet Arrakis. It was Piter's techniques and torture that broke Wellington Yueh, the Atreides Suk doctor's Imperial Conditioning against taking a life. Yueh eventually betrays House Atreides. Yueh gives the captured Leto a false tooth containing a poisonous gas. When the tooth is crushed, intended victim Baron Harkonnen escapes, but Leto and De Vries die.
"Prelude to Dune".
In "" (published in 2001 and the third novel in the "Prelude to Dune" prequel series by Brian Herbert and Kevin J. Anderson), Piter De Vries discovers the Harkonnen heritage of Lady Jessica and her newborn son Paul, and attempts to kidnap and ransom the infant.
The plot is thwarted and the secret preserved - the Reverend Mother Gaius Helen Mohiam kills the Mentat and arranges for his corpse to be shipped home to Giedi Prime. An enraged Baron is left with no choice but to order a duplicate from the Bene Tleilax: the Mentat De Vries featured in Herbert's original novel "Dune".

</doc>
<doc id="7903" url="http://en.wikipedia.org/wiki?curid=7903" title="Diffie–Hellman key exchange">
Diffie–Hellman key exchange

Diffie–Hellman key exchange (D–H) is a specific method of exchanging cryptographic keys. It is one of the earliest practical examples of key exchange implemented within the field of cryptography. The Diffie–Hellman key exchange method allows two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure communications channel. This key can then be used to encrypt subsequent communications using a symmetric key cipher.
The scheme was first published by Whitfield Diffie and Martin Hellman in 1976, although it had been separately invented a few years earlier within GCHQ, the British signals intelligence agency, by James H. Ellis, Clifford Cocks and Malcolm J. Williamson but was kept classified.
Although Diffie–Hellman key agreement itself is an "anonymous" (non-"authenticated") key-agreement protocol, it provides the basis for a variety of authenticated protocols, and is used to provide perfect forward secrecy in Transport Layer Security's ephemeral modes (referred to as EDH or DHE depending on the cipher suite).
The method was followed shortly afterwards by RSA, an implementation of public key cryptography using asymmetric algorithms.
In 2002, Hellman suggested the algorithm be called Diffie–Hellman–Merkle key exchange in recognition of Ralph Merkle's contribution to the invention of public-key cryptography (Hellman, 2002), writing:
The system...has since become known as Diffie–Hellman key exchange. While that system was first described in a paper by Diffie and me, it is a public key distribution system, a concept developed by Merkle, and hence should be called 'Diffie–Hellman–Merkle key exchange' if names are to be associated with it. I hope this small pulpit might help in that endeavor to recognize Merkle's equal contribution to the invention of public key cryptography.
, from 1977 is now expired and describes the algorithm. It credits Hellman, Diffie, and Merkle as inventors.
Description.
Diffie–Hellman establishes a shared secret that can be used for secret communications while exchanging data over a public network. The following diagram illustrates the general idea of the key exchange by using colors instead of a very large number. The crucial part of the process is that Alice and Bob exchange their secret colors in a mix only. Finally this generates an identical key that is computationally difficult (impossible for modern supercomputers to do in a reasonable amount of time) to reverse for another party that might have been listening in on them. Alice and Bob now use this common secret to encrypt and decrypt their sent and received data. Note that the starting color (yellow) is arbitrary, but is agreed on in advance by Alice and Bob. The starting color is assumed to be known to any eavesdropping opponent. It may even be public.
Cryptographic explanation.
The simplest and the original implementation of the protocol uses the multiplicative group of integers modulo "p", where "p" is prime, and a primitive root modulo "p". Here is an example of the protocol, with non-secret values in blue, and secret values in red. 
Both Alice and Bob have arrived at the same value, because ("ga")"b" (for Bob, "815" mod "23" = ("ga" mod "p")"b" mod "p" = ("ga")"b" mod "p") and ("gb")"a" are equal mod "p". Note that only "a", "b", and "(gab" mod "p" = "gba" mod "p)" are kept secret. All the other values – "p", "g", "ga" mod "p", and "gb" mod "p" – are sent in the clear. Once Alice and Bob compute the shared secret they can use it as an encryption key, known only to them, for sending messages across the same open communications channel.
Of course, much larger values of "a", "b", and "p" would be needed to make this example secure, since there are only 23 possible results of "n" mod 23. However, if "p" is a prime of at least 300 digits, and "a" and "b" are at least 100 digits long, then even the fastest modern computers cannot find "a" given only "g", "p", "gb" mod "p" and "ga" mod "p". The problem such a computer needs to solve is called the discrete logarithm problem. The computation of "ga" mod "p" is known as modular exponentiation and can be done efficiently even for large numbers.
Note that "g" need not be large at all, and in practice is usually a small prime (like 2, 3, 5...) because primitive roots usually are quite numerous.
Generalization to other groups.
Here's a more general description of the protocol, in which all the powers are computed modulo p:
Both Alice and Bob are now in possession of the group element "gab", which can serve as the shared secret key. The values of ("gb")"a" and ("ga")"b" are the same because groups are power associative. (See also exponentiation.)
In order to decrypt a message "m", sent as "mgab", Bob (or Alice) must first compute ("gab")−1, as follows:
Bob knows |"G"|, "b", and "ga". Lagrange's theorem in group theory establishes that from the construction of "G", "x"|"G"| = 1 for all "x" in "G".
Bob then calculates ("ga")|"G"|−"b" = "g""a"(|"G"|−"b") = "g""a"|"G"|−"ab" = "g""a"|"G"|"g"−"ab" = ("g"|"G"|)"a""g"−"ab" = 1"a""g"−"ab" = "g"−"ab" = ("gab")−1.
When Alice sends Bob the encrypted message, "mgab", Bob applies ("gab")−1 and recovers "mgab"("gab")−1 = "m"(1) = "m".
Secrecy chart.
The chart below depicts who knows what, again with non-secret values in blue, and secret values in red. Here Eve is an eavesdropper—she watches what is sent between Alice and Bob, but she does not alter the contents of their communications.
Note: It should be difficult for Alice to solve for Bob's private key or for Bob to solve for Alice's private key. If it is not difficult for Alice to solve for Bob's private key (or vice versa), Eve may simply substitute her own private / public key pair, plug Bob's public key into her private key, produce a fake shared secret key, and solve for Bob's private key (and use that to solve for the shared secret key. Eve may attempt to choose a public / private key pair that will make it easy for her to solve for Bob's private key). Another demonstration of Diffie-Hellman (also using numbers too small for practical use) is given here 
Operation with more than two parties.
Diffie–Hellman key agreement is not limited to negotiating a key shared by only two participants. Any number of users can take part in an agreement by performing iterations of the agreement protocol and exchanging intermediate data (which does not itself need to be kept secret). For example, Alice, Bob, and Carol could participate in a Diffie–Hellman agreement as follows, with all operations taken to be modulo formula_1:
An eavesdropper has been able to see formula_7, formula_10, formula_13, formula_19, formula_20, and formula_21, but cannot use any combination of these to reproduce formula_22.
To extend this mechanism to larger groups, two basic principles must be followed:
These principles leave open various options for choosing in which order participants contribute to keys. The simplest and most obvious solution is to arrange the formula_25 participants in a circle and have formula_25 keys rotate around the circle, until eventually every key has been contributed to by all formula_25 participants (ending with its owner) and each participant has contributed to formula_25 keys (ending with their own). However, this requires that every participant perform formula_25 modular exponentiations.
By choosing a more optimal order, and relying on the fact that keys can be duplicated, it is possible to reduce the number of modular exponentiations performed by each participant to formula_32 using a divide-and-conquer-style approach, given here for eight participants:
Once this operation has been completed all participants will possess the secret formula_42, but each participant will have performed only four modular exponentiations, rather than the eight implied by a simple circular arrangement.
Security.
The protocol is considered secure against eavesdroppers if "G" and "g" are chosen properly. The eavesdropper ("Eve") would have to solve the Diffie–Hellman problem to obtain "g""ab". This is currently considered difficult. An efficient algorithm to solve the discrete logarithm problem would make it easy to compute "a" or "b" and solve the Diffie–Hellman problem, making this and many other public key cryptosystems insecure. Fields of small characteristic may be less secure.
The order of "G" should have a large prime factor to prevent use of the Pohlig–Hellman algorithm to obtain "a" or "b". For this reason, a Sophie Germain prime "q" is sometimes used to calculate , called a safe prime, since the order of "G" is then only divisible by 2 and "q". "g" is then sometimes chosen to generate the order "q" subgroup of "G", rather than "G", so that the Legendre symbol of "ga" never reveals the low order bit of "a".
If Alice and Bob use random number generators whose outputs are not completely random and can be predicted to some extent, then Eve's task is much easier.
The secret integers "a" and "b" are discarded at the end of the session.
Therefore, Diffie–Hellman key exchange by itself trivially achieves perfect forward secrecy because no long-term private keying material exists to be disclosed.
In the original description, the Diffie–Hellman exchange by itself does not provide authentication of the communicating parties and is thus vulnerable to a man-in-the-middle attack. Mallory may establish two distinct key exchanges, one with Alice and the other with Bob, effectively masquerading as Alice to Bob, and vice versa, allowing her to decrypt, then re-encrypt, the messages passed between them. Note that Mallory must continue to be in the middle, transferring messages every time Alice and Bob communicate. If she is ever absent, her previous presence is then revealed to Alice and Bob. They will know that all of their private conversations had been intercepted and decoded by someone in the channel.
A method to authenticate the communicating parties to each other is generally needed to prevent this type of attack. Variants of Diffie–Hellman, such as STS protocol, may be used instead to avoid these types of attacks.
Other uses.
Password-authenticated key agreement.
When Alice and Bob share a password, they may use a password-authenticated key agreement (PAKE) form of Diffie–Hellman to prevent man-in-the-middle attacks. One simple scheme is to compare the hash of s concatenated with the password calculated independently on both ends of channel. A feature of these schemes is that an attacker can only test one specific password on each iteration with the other party, and so the system provides good security with relatively weak passwords. This approach is described in ITU-T Recommendation X.1035, which is used by the G.hn home networking standard.
Public key.
It is also possible to use Diffie–Hellman as part of a public key infrastructure. Alice's public key is simply formula_43. To send her a message, Bob chooses a random "b" and then sends Alice formula_44 (un-encrypted) together with the message encrypted with symmetric key formula_45. Only Alice can decrypt the message because only she has "a" (the private key). A preshared public key also prevents man-in-the-middle attacks.
In practice, Diffie–Hellman is not used in this way, with RSA being the dominant public key algorithm. This is largely for historical and commercial reasons, namely that RSA Security created a certificate authority for key signing that became Verisign. Diffie–Hellman cannot be used to sign certificates. However, the ElGamal and DSA signature algorithms are mathematically related to it, as well as MQV, STS and the IKE component of the IPsec protocol suite for securing Internet Protocol communications.
Cryptocurrency.
The sender can produce only the public part of the key, whereas only the receiver can compute the private part. Because of that, the receiver is the only one who can release the funds after the transaction is committed. They need to perform a single-formula check on each transactions to establish if it belongs to them. This process involves their private key, therefore no third party can perform this check and discover the link between the one-time key generated by the sender and the receiver's unique public address.

</doc>
<doc id="7906" url="http://en.wikipedia.org/wiki?curid=7906" title="Destry Rides Again">
Destry Rides Again

Destry Rides Again is a 1939 western starring Marlene Dietrich and James Stewart, and directed by George Marshall. The supporting cast includes Mischa Auer, Charles Winninger, Brian Donlevy, Allen Jenkins, Irene Hervey, Billy Gilbert, Bill Cody, Jr., Lillian Yarbo, and Una Merkel. It bears no relation to Max Brand's popular novel; the characters and story are completely different and unrelated.
In 1996, "Destry Rides Again" was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant".
Plot.
Saloon owner Kent (Brian Donlevy), the unscrupulous boss of the fictional Western town of Bottleneck, has the town's Sheriff, Keogh, killed when the Sheriff asks one too many questions about a rigged poker game. Kent and "Frenchy" (Marlene Dietrich), his girlfriend and the dance hall queen, now have a stranglehold over the local cattle ranchers. The crooked town's mayor, Hiram J. Slade (Samuel S. Hinds), who is in collusion with Kent, appoints the town drunk, Washington Dimsdale (Charles Winninger), as the new sheriff, assuming that he'll be easy to control and manipulate. But what the mayor doesn't know is that Dimsdale was a deputy under the famous lawman, Tom Destry and is able to call upon the equally formidable Tom Destry, Jr. (James Stewart) to help him make Bottleneck a lawful, respectable town. 
Destry confounds the townsfolk by refusing to strap on a gun in spite of demonstrating that he is an expert marksman. He still carries out the "letter of the law", as deputy Sheriff, and earns their respect. A final confrontation between Destry and Kent's gang is inevitable, but "Frenchy" is won over by Destry and changes sides. A final gunfight ensues where Frenchy is killed in the crossfire, and the rule of law wins the day.
Cast.
As appearing in screen credits:
Songs.
Marlene Dietrich as Frenchy performs the songs "See What the Boys in the Back Room Will Have" and "You've Got That Look", written by Frank Loesser, set to music by Frederick Hollander, which have become classics.
Production.
Famed Western writer Max Brand contributed the novel, "Destry Rides Again", but the film also owes its origins to Brand's serial "Twelve Peers", published in a pulp-magazine. In the original work, Harrison (or "Harry") Destry was not a pacifist. As filmed in 1932, with Tom Mix in the starring role, the central character differed in that Destry did wear six-guns in that version.
The film was James Stewart's first western (he would not return to the genre until 1950, with "Broken Arrow" and "Winchester 73"), and was also notable for a ferocious cat-fight between Marlene Dietrich and Una Merkel, which apparently caused a mild censorship problem at the time of release.
According to writer/director Peter Bogdanovich, Marlene Dietrich told him during an aircraft flight that she and James Stewart had an affair during shooting and that she became pregnant and had the baby surreptitiously aborted without telling Stewart.
Internationally, the film was released under the alternate titles "" in French and "Arizona" in Spanish.
Reception.
"Destry Rides Again" was generally well accepted by the public, as well as critics. It was reviewed by Frank S. Nugent in "The New York Times," who noted that the film did not follow the usual Hollywood type-casting. On Dietrich's role, he characterized, "It's difficult to reconcile Miss Dietrich's Frenchy, the cabaret girl of the Bloody Gulch Saloon, with the posed and posturing Dietrich we last saw in Mr. Lubitsch's 'Angel'." Stewart's contribution was similarly treated, "turning in an easy, likable, pleasantly humored performance."

</doc>
<doc id="7921" url="http://en.wikipedia.org/wiki?curid=7921" title="Derivative">
Derivative

The derivative of a function of a real variable measures the sensitivity to change of a quantity (a function or dependent variable) which is determined by another quantity (the independent variable). It is a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time is advanced. The derivative measures the "instantaneous" rate of change of the function, as distinct from its "average" rate of change, and is defined as the limit of the average rate of change in the function as the length of the interval on which the average is computed tends to zero.
The derivative of a function at a chosen input value describes the best linear approximation of the function near that input value. In fact, the derivative at a point of a function of a single variable is the slope of the tangent line to the graph of the function at that point.
The notion of derivative may be generalized to functions of several real variables. The generalized derivative is a linear map called the differential. Its matrix representation is the Jacobian matrix, which reduces to the gradient vector in the case of real-valued function of several variables.
The process of finding a derivative is called differentiation. The reverse process is called "antidifferentiation". The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.
Differentiation.
"Differentiation" is the action of computing a derivative. The derivative of a function "f"("x") of a variable "x" is a measure of the rate at which the value of the function changes with respect to the change of the variable. It is called the "derivative" of "f" with respect to "x". If "x" and "y" are real numbers, and if the graph of "f" is plotted against "x", the derivative is the slope of this graph at each point.
The simplest case, apart from the trivial case of a constant function, is when "y" is a linear function of "x", meaning that the graph of "y" divided by "x" is a line. In this case, "y" = "f"("x") = "m" "x" + "b", for real numbers "m" and "b", and the slope "m" is given by
where the symbol Δ (Delta) is an abbreviation for "change in." This formula is true because
It follows that Δ"y" = "m" Δ"x".
This gives an exact value for the slope of a line.
If the function "f" is not linear (i.e. its graph is not a line), however, then the change in "y" divided by the change in "x" varies: differentiation is a method to find an exact value for this rate of change at any given value of "x".
The idea, illustrated by Figures 1 to 3, is to compute the rate of change as the limit value of the ratio of the differences Δ"y" / Δ"x" as Δ"x" becomes infinitely small.
Notation.
Two distinct notations are commonly used for the derivative, one deriving from Leibniz and the other from Joseph Louis Lagrange.
In Leibniz's notation, an infinitesimal change in "x" is denoted by "dx", and the derivative of "y" with respect to "x" is written
suggesting the ratio of two infinitesimal quantities. (The above expression is read as "the derivative of "y" with respect to "x"", "d y by d x", or "d y over d x". The oral form "d y d x" is often used conversationally, although it may lead to confusion.)
In Lagrange's notation, the derivative with respect to "x" of a function "f"("x") is denoted "f"("x") (read as "f prime of x") or "f"x""("x") (read as "f prime x of x"), in case of ambiguity of the variable implied by the derivation. Lagrange's notation is sometimes incorrectly attributed to Newton.
Rigorous definition.
The most common approach to turn this intuitive idea into a precise definition is to define the derivative as a limit of difference quotients of real numbers. This is the approach described below.
Let "f" be a real valued function defined in an open neighborhood of a real number "a". In classical geometry, the tangent line to the graph of the function "f" at "a" was the unique line through the point ("a", "f"("a")) that did "not" meet the graph of "f" transversally, meaning that the line did not pass straight through the graph. The derivative of "y" with respect to "x" at "a" is, geometrically, the slope of the tangent line to the graph of "f" at ("a", "f"("a")). The slope of the tangent line is very close to the slope of the line through ("a", "f"("a")) and a nearby point on the graph, for example . These lines are called secant lines. A value of "h" close to zero gives a good approximation to the slope of the tangent line, and smaller values (in absolute value) of "h" will, in general, give better approximations. The slope "m" of the secant line is the difference between the "y" values of these points divided by the difference between the "x" values, that is, 
This expression is Newton's difference quotient. Passing from an approximation to an exact answer is done using a limit. Geometrically, the limit of the secant lines is the tangent line. Therefore, the limit of the difference quotient as "h" approaches zero, if it exists, should represent the slope of the tangent line to . This limit is defined to be the derivative of the function "f" at "a":
When the limit exists, "f" is said to be "differentiable" at "a". Here "f"′ ("a") is one of several common notations for the derivative (see below).
Equivalently, the derivative satisfies the property that
which has the intuitive interpretation (see Figure 1) that the tangent line to "f" at "a" gives the "best linear approximation"
to "f" near "a" (i.e., for small "h"). This interpretation is the easiest to generalize to other settings (see below).
Substituting 0 for "h" in the difference quotient causes division by zero, so the slope of the tangent line cannot be found directly using this method. Instead, define "Q"("h") to be the difference quotient as a function of "h":
"Q"("h") is the slope of the secant line between and . If "f" is a continuous function, meaning that its graph is an unbroken curve with no gaps, then "Q" is a continuous function away from . If the limit formula_8 exists, meaning that there is a way of choosing a value for "Q"(0) that makes "Q" a continuous function, then the function "f" is differentiable at "a", and its derivative at "a" equals "Q"(0).
In practice, the existence of a continuous extension of the difference quotient "Q"("h") to is shown by modifying the numerator to cancel "h" in the denominator. Such manipulations can make the limit value of "Q" for small "h" clear even though "Q" is still not defined at . This process can be long and tedious for complicated functions, and many shortcuts are commonly used to simplify the process.
Example.
The squaring function is differentiable at , and its derivative there is 6. This result is established by calculating the limit as "h" approaches zero of the difference quotient of "f"(3):
The last expression shows that the difference quotient equals when and is undefined when , because of the definition of the difference quotient. However, the definition of the limit says the difference quotient does not need to be defined when . The limit is the result of letting "h" go to zero, meaning it is the value that tends to as "h" becomes very small:
Hence the slope of the graph of the squaring function at the point is 6, and so its derivative at is .
More generally, a similar computation shows that the derivative of the squaring function at is .
Continuity and differentiability.
If is differentiable at "a", then "f" must also be continuous at "a". As an example, choose a point "a" and let "f" be the step function that returns a value, say 1, for all "x" less than "a", and returns a different value, say 10, for all "x" greater than or equal to "a". "f" cannot have a derivative at "a". If "h" is negative, then is on the low part of the step, so the secant line from "a" to is very steep, and as "h" tends to zero the slope tends to infinity. If "h" is positive, then is on the high part of the step, so the secant line from "a" to has slope zero. Consequently the secant lines do not approach any single slope, so the limit of the difference quotient does not exist.
However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function is continuous at , but it is not differentiable there. If "h" is positive, then the slope of the secant line from "0" to "h" is one, whereas if "h" is negative, then the slope of the secant line from "0" to "h" is negative one. This can be seen graphically as a "kink" or a "cusp" in the graph at . Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function is not differentiable at .
In summary: for a function "f" to have a derivative it is "necessary" for the function "f" to be continuous, but continuity alone is not "sufficient".
Most functions that occur in practice have derivatives at all points or at almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions, for example if the function is a monotone function or a Lipschitz function, this is true. However, in 1872 Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere. This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly any continuous functions have a derivative at even one point.
Function.
Let "f" be a function that has a derivative at every point "a" in the domain of "f". Because every point "a" has a derivative, there is a function that sends the point "a" to the derivative of "f" at "a". This function is written "f"′("x") and is called the "derivative function" or the "derivative" of "f". The derivative of "f" collects all the derivatives of "f" at all the points in the domain of "f".
Sometimes "f" has a derivative at most, but not all, points of its domain. The function whose value at "a" equals "f"′("a") whenever "f"′("a") is defined and elsewhere is undefined is also called the derivative of "f". It is still a function, but its domain is strictly smaller than the domain of "f".
Using this idea, differentiation becomes a function of functions: The derivative is an operator whose domain is the set of all functions that have derivatives at every point of their domain and whose range is a set of functions. If we denote this operator by "D", then "D"("f") is the function "f"′("x"). Since "D"("f") is a function, it can be evaluated at a point "a". By the definition of the derivative function, .
For comparison, consider the doubling function ; "f" is a real-valued function of a real number, meaning that it takes numbers as inputs and has numbers as outputs:
The operator "D", however, is not defined on individual numbers. It is only defined on functions:
Because the output of "D" is a function, the output of "D" can be evaluated at a point. For instance, when "D" is applied to the squaring function,
"D" outputs the doubling function,
which we named "f"("x"). This output function can then be evaluated to get , , and so on.
Higher derivatives.
Let "f" be a differentiable function, and let "f"′("x") be its derivative. The derivative of "f"′("x") (if it has one) is written "f"′′("x") and is called the "second derivative of f". Similarly, the derivative of a second derivative, if it exists, is written and is called the "third derivative of f". Continuing this process, one can define, if it exists, the "n"th derivative as the derivative of the ("n"-1)th derivative. These repeated derivatives are called "higher-order derivatives". The "n"th derivative is also called the derivative of order "n".
If "x"("t") represents the position of an object at time "t", then the higher-order derivatives of "x" have physical interpretations. The second derivative of "x" is the derivative of "x"′("t"), the velocity, and by definition this is the object's acceleration. The third derivative of "x" is defined to be the jerk, and the fourth derivative is defined to be the jounce.
A function "f" need not have a derivative, for example, if it is not continuous. Similarly, even if "f" does have a derivative, it may not have a second derivative. For example, let
Calculation shows that "f" is a differentiable function whose derivative is
 is twice the absolute value function, and it does not have a derivative at zero. Similar examples show that a function can have "k" derivatives for any non-negative integer "k" but no th-order derivative. A function that has "k" successive derivatives is called "k times differentiable". If in addition the "k"th derivative is continuous, then the function is said to be of differentiability class "Ck". (This is a stronger condition than having "k" derivatives. For an example, see differentiability class.) A function that has infinitely many derivatives is called "infinitely differentiable" or "smooth".
On the real line, every polynomial function is infinitely differentiable. By standard differentiation rules, if a polynomial of degree "n" is differentiated "n" times, then it becomes a constant function. All of its subsequent derivatives are identically zero. In particular, they exist, so polynomials are smooth functions.
The derivatives of a function "f" at a point "x" provide polynomial approximations to that function near "x". For example, if "f" is twice differentiable, then
in the sense that
If "f" is infinitely differentiable, then this is the beginning of the Taylor series for "f" evaluated at around "x".
Inflection point.
A point where the second derivative of a function changes sign is called an "inflection point". At an inflection point, the second derivative may be zero, as in the case of the inflection point of the function , or it may fail to exist, as in the case of the inflection point of the function . At an inflection point, a function switches from being a convex function to being a concave function or vice versa.
Notational systems.
Leibniz's notation.
The notation for derivatives introduced by Gottfried Leibniz is one of the earliest. It is still commonly used when the equation is viewed as a functional relationship between dependent and independent variables. Then the first derivative is denoted by
and was once thought of as an infinitesimal quotient. Higher derivatives are expressed using the notation
for the "n"th derivative of (with respect to "x"). These are abbreviations for multiple applications of the derivative operator. For example,
With Leibniz's notation, we can write the derivative of "y" at the point in two different ways:
Leibniz's notation allows one to specify the variable for differentiation (in the denominator). This is especially relevant for partial differentiation. It also makes the chain rule easy to remember:
Lagrange's notation.
Sometimes referred to as "prime notation", one of the most common modern notation for differentiation is due to Joseph-Louis Lagrange and uses the prime mark, so that the derivative of a function "f"("x") is denoted "f"′("x") or simply "f"′. Similarly, the second and third derivatives are denoted
To denote the number of derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses:
The latter notation generalizes to yield the notation "f" ("n") for the "n"th derivative of "f" – this notation is most useful when we wish to talk about the derivative as being a function itself, as in this case the Leibniz notation can become cumbersome.
Newton's notation.
Newton's notation for differentiation, also called the dot notation, places a dot over the function name to represent a time derivative. If , then
denote, respectively, the first and second derivatives of "y" with respect to "t". This notation is used exclusively for time derivatives, meaning that the independent variable of the function represents time. It is very common in physics and in mathematical disciplines connected with physics such as differential equations. While the notation becomes unmanageable for high-order derivatives, in practice only very few derivatives are needed.
Fluent and fluxions.
Newton tried to explain calculus using fluent and fluxions. He said that the rate of generation is the fluxion of the fluent, which is denoted by the variable with a dot over it. Then the rate of the fluxion is the second fluxion, which has two dots over it. These fluxions were thought of, as very close to zero but not quite zero. But when you multiply two fluxions together you get something that is so close to zero that it is treated as zero. How Newton took derivatives is he replaced all the "x" values with formula_30 and all the y values with formula_31 and then used derivative rules to take the derivative and solve for formula_32 
Here is an example:
formula_33
Using the fact that formula_34 we can see formula_35 and formula_36 so formula_37.
Newton described mathematical quantities to be like continuous motion. This motion, he said, could be thought of in the same way that a point traces a curve. He defined this quantity and called it a “fluent”. He went on to name the rate at which these quantities change. Newton called this the “fluxion of the fluent” and he represented it by formula_38.
So, if the fluent was represented by "x", Newton denoted its fluxion by formula_38, the second fluxion by formula_40, and so on. This can be related to the modern language we use to describe derivatives. In modern language, the fluxion of the variable "x" relative to an independent time-variable "t" would be its velocity . In other words, the derivative of "f"("x") with respect to time, "t", is .
Moment.
Newton called "o" the moment of the fluent. The moment of the fluent represents the infinitely small part by which a fluent was increased in a small time interval. Once he allowed himself to divide through by "o" (although "o" can not be treated as zero because that would make the division illegitimate). Newton decided it was justifiable to drop all terms containing "o".
Euler's notation.
Euler's notation uses a differential operator "D", which is applied to a function "f" to give the first derivative "Df". The second derivative is denoted "D"2"f", and the "n"th derivative is denoted "D""n""f".
If is a dependent variable, then often the subscript "x" is attached to the "D" to clarify the independent variable "x".
Euler's notation is then written
although this subscript is often omitted when the variable "x" is understood, for instance when this is the only variable present in the expression.
Euler's notation is useful for stating and solving linear differential equations.
Computation.
The derivative of a function can, in principle, be computed from the definition by considering the difference quotient, and computing its limit. In practice, once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using "rules" for obtaining derivatives of more complicated functions from simpler ones.
Elementary functions.
Most derivative computations eventually require taking the derivative of some common functions. The following incomplete list gives some of the most frequently used functions of a single real variable and their derivatives.
where "r" is any real number, then
wherever this function is defined. For example, if formula_45, then
and the derivative function is defined only for positive "x", not for . When , this rule implies that "f"′("x") is zero for , which is almost the constant rule (stated below).
Rules.
In many cases, complicated limit calculations by direct application of Newton's difference quotient can be avoided using differentiation rules. Some of the most basic rules are the following.
Example computation.
The derivative of
is
Here the second term was computed using the chain rule and third using the product rule. The known derivatives of the elementary functions "x"2, "x"4, sin("x"), ln("x") and , as well as the constant 7, were also used.
Higher dimensions.
Vector valued functions.
A vector-valued function y("t") of a real variable sends real numbers to vectors in some vector space R"n". A vector-valued function can be split up into its coordinate functions "y"1("t"), "y"2("t"), …, "y""n"("t"), meaning that . This includes, for example, parametric curves in R2 or R3. The coordinate functions are real valued functions, so the above definition of derivative applies to them. The derivative of y("t") is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is,
Equivalently,
if the limit exists. The subtraction in the numerator is subtraction of vectors, not scalars. If the derivative of y exists for every value of "t", then y′ is another vector valued function.
If e1, …, e"n" is the standard basis for R"n", then y("t") can also be written as . If we assume that the derivative of a vector-valued function retains the linearity property, then the derivative of y("t") must be
because each of the basis vectors is a constant.
This generalization is useful, for example, if y("t") is the position vector of a particle at time "t"; then the derivative y′("t") is the velocity vector of the particle at time "t".
Partial derivatives.
Suppose that "f" is a function that depends on more than one variable. For instance,
"f" can be reinterpreted as a family of functions of one variable indexed by the other variables:
In other words, every value of "x" chooses a function, denoted "fx", which is a function of one real number. That is,
Once a value of "x" is chosen, say "a", then determines a function "fa" that sends "y" to :
In this expression, "a" is a "constant", not a "variable", so "fa" is a function of only one real variable. Consequently the definition of the derivative for a function of one variable applies:
The above procedure can be performed for any choice of "a". Assembling the derivatives together into a function gives a function that describes the variation of "f" in the "y" direction:
This is the partial derivative of "f" with respect to "y". Here ∂ is a rounded "d" called the partial derivative symbol. To distinguish it from the letter "d", ∂ is sometimes pronounced "der", "del", or "partial" instead of "dee".
In general, the partial derivative of a function in the direction "xi" at the point ("a"1 …, "a""n") is defined to be:
In the above difference quotient, all the variables except "xi" are held fixed. That choice of fixed values determines a function of one variable
and, by definition,
In other words, the different choices of "a" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.
An important example of a function of several variables is the case of a scalar-valued function on a domain in Euclidean space R"n" (e.g., on R2 or R3). In this case "f" has a partial derivative ∂"f"/∂"x""j" with respect to each variable "x""j". At the point "a", these partial derivatives define the vector
This vector is called the gradient of "f" at "a". If "f" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇"f" that takes the point "a" to the vector ∇"f"("a"). Consequently the gradient determines a vector field.
Directional derivatives.
If "f" is a real-valued function on Rn, then the partial derivatives of "f" measure its variation in the direction of the coordinate axes. For example, if "f" is a function of "x" and "y", then its partial derivatives measure the variation in "f" in the "x" direction and the "y" direction. They do not, however, directly measure the variation of "f" in any other direction, such as along the diagonal line . These are measured using directional derivatives. Choose a vector
The directional derivative of "f" in the direction of v at the point x is the limit
In some cases it may be easier to compute or estimate the directional derivative after changing the length of the vector. Often this is done to turn the problem into the computation of a directional derivative in the direction of a unit vector. To see how this works, suppose that . Substitute into the difference quotient. The difference quotient becomes:
This is λ times the difference quotient for the directional derivative of "f" with respect to u. Furthermore, taking the limit as "h" tends to zero is the same as taking the limit as "k" tends to zero because "h" and "k" are multiples of each other. Therefore . Because of this rescaling property, directional derivatives are frequently considered only for unit vectors.
If all the partial derivatives of "f" exist and are continuous at x, then they determine the directional derivative of "f" in the direction v by the formula:
This is a consequence of the definition of the total derivative. It follows that the directional derivative is linear in v, meaning that .
The same definition also works when "f" is a function with values in Rm. The above definition is applied to each component of the vectors. In this case, the directional derivative is a vector in Rm.
Total derivative, total differential and Jacobian matrix.
When "f" is a function from an open subset of R"n" to R"m", then the directional derivative of "f" in a chosen direction is the best linear approximation to "f" at that point and in that direction. But when , no single directional derivative can give a complete picture of the behavior of "f". The total derivative, also called the (total) differential, gives a complete picture by considering all directions at once. That is, for any vector v starting at a, the linear approximation formula holds:
Just like the single-variable derivative, is chosen so that the error in this approximation is as small as possible.
If "n" and "m" are both one, then the derivative is a number and the expression is the product of two numbers. But in higher dimensions, it is impossible for to be a number. If it were a number, then would be a vector in R"n" while the other terms would be vectors in R"m", and therefore the formula would not make sense. For the linear approximation formula to make sense, must be a function that sends vectors in R"n" to vectors in R"m", and must denote this function evaluated at v.
To determine what kind of function it is, notice that the linear approximation formula can be rewritten as
Notice that if we choose another vector w, then this approximate equation determines another approximate equation by substituting w for v. It determines a third approximate equation by substituting both w for v and for a. By subtracting these two new equations, we get
If we assume that v is small and that the derivative varies continuously in a, then is approximately equal to , and therefore the right-hand side is approximately zero. The left-hand side can be rewritten in a different way using the linear approximation formula with substituted for v. The linear approximation formula implies:
This suggests that is a linear transformation from the vector space R"n" to the vector space R"m". In fact, it is possible to make this a precise derivation by measuring the error in the approximations. Assume that the error in these linear approximation formula is bounded by a constant times ||v||, where the constant is independent of v but depends continuously on a. Then, after adding an appropriate error term, all of the above approximate equalities can be rephrased as inequalities. In particular, is a linear transformation up to a small error term. In the limit as v and w tend to zero, it must therefore be a linear transformation. Since we define the total derivative by taking a limit as v goes to zero, must be a linear transformation.
In one variable, the fact that the derivative is the best linear approximation is expressed by the fact that it is the limit of difference quotients. However, the usual difference quotient does not make sense in higher dimensions because it is not usually possible to divide vectors. In particular, the numerator and denominator of the difference quotient are not even in the same vector space: The numerator lies in the codomain R"m" while the denominator lies in the domain R"n". Furthermore, the derivative is a linear transformation, a different type of object from both the numerator and denominator. To make precise the idea that is the best linear approximation, it is necessary to adapt a different formula for the one-variable derivative in which these problems disappear. If , then the usual definition of the derivative may be manipulated to show that the derivative of "f" at "a" is the unique number such that
This is equivalent to
because the limit of a function tends to zero if and only if the limit of the absolute value of the function tends to zero. This last formula can be adapted to the many-variable situation by replacing the absolute values with norms.
The definition of the total derivative of "f" at a, therefore, is that it is the unique linear transformation such that
Here h is a vector in R"n", so the norm in the denominator is the standard length on R"n". However, "f"′(a)h is a vector in R"m", and the norm in the numerator is the standard length on R"m". If "v" is a vector starting at "a", then is called the pushforward of v by "f" and is sometimes written .
If the total derivative exists at a, then all the partial derivatives and directional derivatives of "f" exist at a, and for all v, is the directional derivative of "f" in the direction v. If we write "f" using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix. This matrix is called the Jacobian matrix of "f" at a:
The existence of the total derivative "f"′(a) is strictly stronger than the existence of all the partial derivatives, but if the partial derivatives exist and are continuous, then the total derivative exists, is given by the Jacobian, and depends continuously on a.
The definition of the total derivative subsumes the definition of the derivative in one variable. That is, if "f" is a real-valued function of a real variable, then the total derivative exists if and only if the usual derivative exists. The Jacobian matrix reduces to a 1×1 matrix whose only entry is the derivative "f"′("x"). This 1×1 matrix satisfies the property that is approximately zero, in other words that
Up to changing variables, this is the statement that the function formula_95 is the best linear approximation to "f" at "a".
The total derivative of a function does not give another function in the same way as the one-variable case. This is because the total derivative of a multivariable function has to record much more information than the derivative of a single-variable function. Instead, the total derivative gives a function from the tangent bundle of the source to the tangent bundle of the target.
The natural analog of second, third, and higher-order total derivatives is not a linear transformation, is not a function on the tangent bundle, and is not built by repeatedly taking the total derivative. The analog of a higher-order derivative, called a jet, cannot be a linear transformation because higher-order derivatives reflect subtle geometric information, such as concavity, which cannot be described in terms of linear data such as vectors. It cannot be a function on the tangent bundle because the tangent bundle only has room for the base space and the directional derivatives. Because jets capture higher-order information, they take as arguments additional coordinates representing higher-order changes in direction. The space determined by these additional coordinates is called the jet bundle. The relation between the total derivative and the partial derivatives of a function is paralleled in the relation between the "k"th order jet of a function and its partial derivatives of order less than or equal to "k".
By repeatedly taking the total derivative, one obtains higher versions of the Fréchet derivative, specialized to R"p". The "k"th order total derivative may be interpreted as a map
which takes a point x in Rn and assigns to it an element of the space of "k"-linear maps from Rn to Rm – the "best" (in a certain precise sense) "k"-linear approximation to "f" at that point. By precomposing it with the diagonal map Δ, , a generalized Taylor series may be begun as
where f(a) is identified with a constant function, are the components of the vector , and and are the components of and as linear transformations.
Generalizations.
The concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point.
History.
The creation of Calculus was one of the greatest achievements of the 1600s, but the inventor of calculus is widely disputed: Was it Isaac Newton or Gottfried Wilhelm Leibniz? When Isaac Newton and Gottfried Wilhelm Leibniz first formulated differential calculus they effectively made use of the concept of an infinitesimal, which they referred to as an infinitely small number. At that time before non-standard analysis, the concept of infinitesimals was very fuzzy and bothered many mathematicians. However, the concept of infinitesimals was essential to the development of differential calculus.
Newton's method involved taking ratios of infinitesimals. Those terms for the ratio that which had an infinitesimal as a factor were treated as zero and thus the product of infinitesimals is equal to zero. He explained it, “terms which have [an infinitesimal] as a factor will be equivalent to nothing in respect to the others. I therefore cast them out…” Ultimately Cauchy, Weierstrass, and Riemann, reformulated Calculus in terms of limits rather than infinitesimals. Therefore the need for these infinitely small (and nonexistent) quantities was removed, and replaced by a notion of quantities being "close" to others. So the derivative and the integral were both reformulated in terms of limits.
In the nineteenth century the German mathematician Karl Weierstrass introduced the epsilon-delta process, which provided a rigorous basis for Calculus and discouraged students from using the infinitesimal concept. Then in 1960 Abraham Robinson found a way to provide a foundation for infinitesimals and thus infinitesimals were acceptable. Robinson called his formulation non-standard analysis. The purpose of this material is to explain, illustrate and justify the non-standard analysis formulation of infinitesimals.

</doc>
<doc id="7922" url="http://en.wikipedia.org/wiki?curid=7922" title="Dravidian languages">
Dravidian languages

The Dravidian languages are a language family spoken mainly in southern India and parts of eastern and central India as well as in northeastern Sri Lanka, Pakistan, Nepal, Bangladesh, and overseas in other countries such as Malaysia and Singapore. The Dravidian languages with the most speakers are Tamil, Telugu, Kannada, and Malayalam. There are also small groups of Dravidian-speaking scheduled tribes, who live beyond the mainstream communities. It is often speculated that Dravidian languages are native to India. Epigraphically the Dravidian languages have been attested since the 2nd century BCE. Only two Dravidian languages are exclusively spoken outside India, Brahui in Pakistan and Dhangar, a dialect of Kurukh, in Nepal.
Dravidian place-names along the northwest coast, in Maharashtra, Goa, Gujarat, and to a lesser extent in Sindh, as well as Dravidian grammatical influence such as clusivity in the Marathi, Konkani, Gujarati, Marwari, and to a lesser extent Sindhi languages, suggest that Dravidian languages were once spoken more widely across the Indian subcontinent.
Dravidian studies.
The existence of the Dravidian language family was first suggested in 1816 by Alexander D. Campbell in his "A Grammar of the Teloogoo Language", in which he and Francis W. Ellis argued that Tamil and Telugu were descended from a common, non-Indo-European ancestor. However, it was not until 1856 that Robert Caldwell published his "Comparative grammar of the Dravidian or South-Indian family of languages", which considerably expanded the Dravidian umbrella and established it as one of the major language groups of the world. Caldwell coined the term "Dravidian" for this family of languages, based on the usage of the Sanskrit word "" in the work "Tantravārttika" by . In his own words, Caldwell says,
The 1961 publication of the " by T. Burrow and M. B. Emeneau was a landmark event in Dravidian linguistics.
Origin of the word ".
As for the origin of the Sanskrit word ' itself there have been various theories proposed. Basically the theories are about the direction of derivation between ' and "".
There is no definite philological and linguistic basis for asserting unilaterally that the name ' also forms the origin of the word "Tamil" (Dravida → Dramila → Tamizha or Tamil). Kamil Zvelebil cites the forms such as "dramila" (in 's Sanskrit work "Avanisundarīkathā") ' (found in Ceylonese chronicle Mahavamsa) and then goes on to say, "The forms "damiḷa"/"damila" almost certainly provide a connection of ' " and "... ' < ' ...whereby the further development might have been *' > *' > '- / "damila"- and further, with the intrusive, 'hypercorrect' (or perhaps analogical) -"r"-, into "". The -"m"-/-"v"- alternation is a common enough phenomenon in Dravidian phonology"
Zvelebil in his earlier treatise states, "It is obvious that the Sanskrit ', Pali "damila", ' and Prakrit ' are all etymologically connected with '" and further remarks "The "r" in ' → ' is a hypercorrect insertion, cf. an analogical case of DED 1033 Ta. "kamuku", Tu. "kangu" "areca nut": Skt. "kramu(ka)"."
Further, another Dravidian linguist Bhadriraju Krishnamurti in his book "Dravidian Languages" states,
Based on what Krishnamurti states referring to a scholarly paper published in the International Journal of Dravidian Linguistics, the Sanskrit word ' itself is later than ' since the dates for the forms with -r- are centuries later than the dates for the forms without -r- (', '-, "damela"- etc.).
The "Monier-Williams Sanskrit Dictionary" lists for the Sanskrit word "" a meaning of "collective Name for 5 peoples, viz. the and ".
Classification.
The Dravidian languages form a close-knit family – much more closely related than, say, the Indo-European languages. There is reasonable agreement on how they are related to each other. Most scholars agree on four groups: North, Central (Kolami–Parji), South-Central (Telugu–Kui) and South Dravidian. Earlier classifications grouped Central and South-Central Dravidian in a single branch. Some authors deny that North Dravidian forms a valid subgroup, splitting it into Northeast (Kurukh–Malto) and Northwest (Brahui).
The classification below follows Krishnamurti in grouping South-Central and South Dravidian.
Languages recognized as official languages of India appear here in boldface.
In addition, "Ethnologue" lists several unclassified Dravidian languages: Allar, Bazigar, Bharia, Malankuravan (possibly a dialect of Malayalam), Vishavan, as well as the otherwise unclassified Southern Dravidian languages Mala Malasar, Malasar, Thachanadan, Ullatan, Kalanadi, Kumbaran, Kunduvadi, Kurichiya, Attapady Kurumba, Muduga, Pathiya and Wayanad Chetti to Tamil-Kannada.
Distribution.
About 24% of India's population spoke Dravidian languages in 1981. This proportion is slowly falling due to higher birth rates in the Indo-Aryan-speaking Ganges Plain and in 2001 census it was around 21.5% or 220 millions of total population of 1,028,610,328 and it would be much lesser at present as per the trend.
History.
The origins of the Dravidian languages, as well as their subsequent development and the period of their differentiation are unclear, partially due to the lack of comparative linguistic research into the Dravidian languages.
Although in modern times speakers of the various Dravidian languages have mainly occupied the southern portion of India, nothing definite is known about the ancient domain of the Dravidian parent speech. It is, however, a well-established and well-supported hypothesis that Dravidian speakers must have been widespread throughout much of India before the arrival of Indo-European speakers. The Brahui, Kurukh and Malto have myths about external origins. The Kurukh have traditionally claimed to be from the Deccan Peninsula, more specifically Karnataka. The same tradition has existed of the Brahui. They call themselves immigrants. Many scholars hold this same view of the Brahui such as L. H. Horace Perera and M. Ratnasabapathy.
Proto-Dravidian is thought to have differentiated into Proto-North Dravidian, Proto-Central Dravidian, Proto South-Central Dravidian and Proto-South Dravidian around 500 BCE, although some linguists have argued that the degree of differentiation between the sub-families points to an earlier split.
Relationship to other language families.
Despite many proposals, scholars have not shown a systematic relationship between the Dravidian languages and any other language family. Nonetheless, while there are no readily detectable genealogical connections, Dravidian shares strong areal features with the Indo-Aryan languages, which have been attributed to a substratum influence from Dravidian.
The earliest known Dravidian inscriptions are 76 Old Tamil inscriptions on cave walls in Madurai and Tirunelveli districts in Tamil Nadu, dating from the 2nd century BCE.
Proposed larger groupings.
The Dravidian family has defied all of the attempts to show a connection with other languages, including Indo-European, Hurrian, Basque, Sumerian, and Korean. Comparisons have been made not just with the other language families of the Indian subcontinent (Indo-European, Austroasiatic, Tibeto-Burman, and Nihali), but with all typologically similar language families of the Old World.
Dravidian languages display typological similarities with the Uralic language group, suggesting to some a prolonged period of contact in the past. This idea is popular amongst Dravidian linguists and has been supported by a number of scholars, including Robert Caldwell, Thomas Burrow, Kamil Zvelebil, and Mikhail Andronov. This hyphothesis has, however, been rejected by some specialists in Uralic languages, and has in recent times also been criticised by other Dravidian linguists such as Bhadriraju Krishnamurti.
Dravidian is one of the primary language families in the Nostratic proposal, which would link most languages in North Africa, Europe and Western Asia into a family with its origins in the Fertile Crescent sometime between the last Ice Age and the emergence of proto-Indo-European 4–6 thousand years BCE. However, the general consensus is that such deep connections are not, or not yet, demonstrable.
On a less ambitious scale, McAlpin (1975) proposed linking Dravidian languages with the ancient Elamite language of what is now southwestern Iran. However, despite decades of research, this Elamo-Dravidian language family has not been demonstrated to the satisfaction of other historical linguists.
Dravidian substratum influence on Sanskrit.
Dravidian languages show extensive lexical (vocabulary) borrowing, but only a few traits of structural (either phonological or grammatical) borrowing from Indo-Aryan, whereas Indo-Aryan shows more structural than lexical borrowings from the Dravidian languages.
Many of these features are already present in the oldest known Indo-Aryan language, the language of the "Rigveda" (c. 1500 BCE), which also includes over a dozen words borrowed from Dravidian.
Vedic Sanskrit has retroflex consonants (/, ) with about 88 words in the "Rigveda" having unconditioned retroflexes. Some sample words are ', ',', ', ' and '.
Since other Indo-European languages, including other Indo-Iranian languages, lack retroflex consonants, their presence in Indo-Aryan is often cited as evidence of substrate influence from close contact of the Vedic speakers with speakers of a foreign language family rich in retroflex consonants. The Dravidian family is a serious candidate since it is rich in retroflex phonemes reconstructible back to the Proto-Dravidian stage.
In addition, a number of grammatical features of Vedic Sanskrit not found in its sister Avestan language appear to have been borrowed from Dravidian languages. These include the gerund, which has the same function as in Dravidian, and the quotative marker "iti".
Some linguists explain this asymmetrical borrowing by arguing that Middle Indo-Aryan languages were built on a Dravidian substratum.
These scholars argue that the most plausible explanation for the presence of Dravidian structural features in Indic is language shift, that is, native Dravidian speakers learning and adopting Indic languages.
Although each of the innovative traits in Indic could be accounted for by internal explanations, early Dravidian influence is the only explanation that can account for all of the innovations at once; moreover, it accounts for the several of the innovative traits in Indic better than any internal explanation that has been proposed.
The Brahui population of Balochistan has been taken by some as the linguistic equivalent of a relict population, perhaps indicating that Dravidian languages were formerly much more widespread and were supplanted by the incoming Indo-Aryan languages.
However it has been argued that the absence of any Old Iranian (Avestan) loanwords in Brahui suggests that the Brahui migrated to Balochistan from central India less than 1000 years ago. The main Iranian contributor to Brahui vocabulary, Balochi, is a western Iranian language like Kurdish, and arrived in the area from the west only around 1000 CE.
Sound changes shared with Kurukh and Malto also suggest that Brahui was originally spoken near them in central India.
Grammar.
The most characteristic grammatical features of Dravidian languages are:
Phonology.
Dravidian languages are noted for the lack of distinction between aspirated and unaspirated stops. While some Dravidian languages have accepted large numbers of loan words from Sanskrit and other Indo-Iranian languages in addition to their already vast vocabulary, in which the orthography shows distinctions in voice and aspiration, the words are pronounced in Dravidian according to different rules of phonology and phonotactics: aspiration of plosives is generally absent, regardless of the spelling of the word. This is not a universal phenomenon and is generally avoided in formal or careful speech, especially when reciting.
For instance, Tamil does not distinguish between voiced and voiceless stops. In fact, the Tamil alphabet lacks symbols for voiced and aspirated stops.
Dravidian languages are also characterized by a three-way distinction between dental, alveolar, and retroflex places of articulation as well as large numbers of liquids.
Proto-Dravidian.
Proto-Dravidian had five short and long vowels: "*a", "*ā", "*i", "*ī", "*u", "*ū", "*e", "*ē", "*o", "*ō". There were no diphthongs; "ai" and "au" are treated as *"ay" and *"av" (or *"aw").
The five-vowel system is largely preserved in the descendent subgroups.
The following consonantal phonemes are reconstructed:
Words starting with vowels.
A substantial number of words also begin and end with vowels, which helps the languages' agglutinative property.
karanu (cry), elumbu (bone), athu (that), avide (there), ithu (this), illai (no, absent)
adu-idil-illai (adu = that, idu = this, il= suffix form of "in", illai = absent, so → that-this-in-absent → that-in this-absent → that is absent in this)
Numerals.
The numerals from 1 to 10 in various Dravidian and Indo-Aryan languages (here exemplified by Hindi, Sanskrit and Marathi).

</doc>
<doc id="7923" url="http://en.wikipedia.org/wiki?curid=7923" title="Dracula">
Dracula

Dracula is an 1897 Gothic horror novel by Irish author Bram Stoker.
Famous for introducing the character of the vampire Count Dracula, the novel tells the story of Dracula's attempt to move from Transylvania to England so he may find new blood and spread undead curse, and the battle between Dracula and a small group of men and women led by Professor Abraham Van Helsing.
"Dracula" has been assigned to many literary genres including vampire literature, horror fiction, the gothic novel and invasion literature. The novel touches on themes such as the role of women in Victorian culture, sexual conventions, immigration, colonialism, and post-colonialism. Although Stoker did not invent the vampire, he defined its modern form, and the novel has spawned numerous theatrical, film and television interpretations.
Plot summary.
The story is told in epistolary format, as a series of letters, diary entries, and ships' log entries, whose narrators are the novel's protagonists, and occasionally supplemented with newspaper clippings relating events not directly witnessed. The events portrayed in the novel take place largely in England and Transylvania during 1893.
The tale begins with Jonathan Harker, a newly qualified English solicitor, visiting Count Dracula in the Carpathian Mountains on the border of Transylvania, Bukovina, and Moldavia, to provide legal support for a real estate transaction overseen by Harker's employer. At first enticed by Dracula's gracious manners, Harker soon realizes that he is Dracula's prisoner. Wandering the Count's castle against Dracula's admonition, Harker encounters three female vampires, called "the sisters", from whom he is rescued by Dracula. After the preparations are made, Dracula leaves Transylvania and abandons Harker to the sisters. Harker barely escapes from the castle with his life.
Not long afterward, a Russian ship, the "Demeter", having weighed anchor at Varna, runs aground on the shores of Whitby. The captain's log narrates the gradual disappearance of the entire crew, until the captain alone remained, himself bound to the helm to maintain course. An animal resembling "a large dog" is seen leaping ashore. The ship's cargo is described as silver sand and boxes of "mould", or earth, from Transylvania.
Soon Dracula is tracking Harker's fiancée, Wilhelmina "Mina" Murray, and her friend, Lucy Westenra. Lucy receives three marriage proposals from Dr. John Seward, Quincey Morris, and the Hon. Arthur Holmwood (later Lord Godalming). Lucy accepts Holmwood's proposal while turning down Seward and Morris, but all remain friends. Dracula communicates with Seward's patient Renfield, an insane man who wishes to consume insects, spiders, birds, and rats to absorb their "life force", and therefore assimilated to Dracula himself. Renfield is able to detect Dracula's presence and supplies clues accordingly.
When Lucy begins to waste away suspiciously, Seward invites his old teacher, Abraham Van Helsing, who immediately determines the cause of Lucy's condition but refuses to disclose it. While both doctors are absent, Lucy and her mother are attacked by a wolf; Mrs. Westenra, who has a heart condition, dies of fright, and Lucy dies soon after. Following Lucy's death, the newspapers report children being stalked in the night by, in their words, a "bloofer lady" (i.e., "beautiful lady"). Van Helsing, knowing Lucy has become a vampire, confides in Seward, Lord Godalming, and Morris. The suitors and Van Helsing track her down and, after a confrontation with her, stake her heart, behead her, and fill her mouth with garlic. Around the same time, Jonathan Harker arrives from Budapest, where Mina marries him after his escape, and he and Mina join the coalition against Dracula.
After Dracula learns of Van Helsing's plot against him, he attacks Mina on three occasions, and feeds Mina his own blood to control her. Under his influence, Mina oscillates from consciousness to a semi-trance during which she perceives Dracula's surroundings and actions. After the protagonists sterilize all of his lairs in London by putting pieces of consecrated host in each box of Transylvanian earth, Dracula flees to Transylvania, pursued by Van Helsing and the others under the guidance of Mina. In Transylvania, Van Helsing repulses and later destroys the vampire "sisters". Upon discovering Dracula being transported by Gypsies, Harker shears Dracula through the throat with a kukri while the mortally wounded Quincey stabs the Count in the heart with a Bowie knife. Dracula crumbles to dust, and Mina is restored to health.
The book closes with a note on Mina's and Jonathan's married life and the birth of their son, whom they name after all four members of the party, but address as "Quincey".
Deleted ending.
The original final chapter was removed, in which Dracula's castle falls apart as he dies, hiding the fact that vampires were ever there.
Background.
Between 1879 and 1898, Stoker was a business manager for the world-famous Lyceum Theatre in London, where he supplemented his income by writing a large number of sensational novels, his most famous being the vampire tale "Dracula" published on 26 May 1897. Parts of it are set around the town of Whitby, where he spent summer holidays. Throughout the 1880s and 1890s, authors such as H. Rider Haggard, Rudyard Kipling, Robert Louis Stevenson, Arthur Conan Doyle, and H. G. Wells wrote many tales in which fantastic creatures threatened the British Empire. Invasion literature was at a peak, and Stoker's formula of an invasion of England by continental European influences was by 1897 very familiar to readers of fantastic adventure stories. Victorian readers enjoyed it as a good adventure story like many others, but it would not reach its iconic legendary status until later in the 20th century when film versions began to appear. 
Before writing "Dracula", Stoker spent seven years researching European folklore and stories of vampires, being most influenced by Emily Gerard's 1885 essay, "Transylvania Superstitions". Later he would also claim that he had a nightmare about a "vampire king" rising from his grave, caused by eating too much crab meat covered with mayonnaise sauce. He also told a story about meeting future American president Roosevelt, then the Chief of Police in New York, which encouraged him to write a story about a supernatural criminal.
Despite being the most widely known vampire novel, "Dracula" was not the first. It was preceded and partly inspired by Sheridan Le Fanu's 1871 "Carmilla", about a lesbian vampire who preys on a lonely young woman, and by "Varney the Vampire", a lengthy penny dreadful serial from the mid-Victorian period by James Malcolm Rymer. The image of a vampire portrayed as an aristocratic man, like the character of Dracula, was created by John Polidori in "The Vampyre" (1819), during the summer spent with "Frankenstein" creator Mary Shelley, her husband, the poet Percy Bysshe Shelley and Lord Byron in 1816. The Lyceum Theatre, where Stoker worked between 1878 and 1898, was headed by the actor-manager Henry Irving, who was Stoker's real-life inspiration for Dracula's mannerisms and who Stoker hoped would play Dracula in a stage version. Although Irving never did agree to do a stage version, Dracula's dramatic sweeping gestures and gentlemanly mannerisms drew their living embodiment from Irving.
"The Dead Un-Dead" was one of Stoker's original titles for "Dracula", and up until a few weeks before publication, the manuscript was titled simply "The Un-Dead". Stoker's notes for "Dracula" show that the name of the count was originally "Count Wampyr", but while doing research, Stoker became intrigued by the name "Dracula", after reading William Wilkinson's book "Account of the Principalities of Wallachia and Moldavia with Political Observations Relative to Them" (London 1820), which he found in the Whitby Library, and consulted a number of times during visits to Whitby in the 1890s. The name Dracula was the patronym ("Drăculea") of the descendants of Vlad II of Wallachia, who took the name "Dracul" after being invested in the Order of the Dragon in 1431. In the Romanian language, the word "dracul" (Romanian "drac" "dragon" + "-ul" "the") can mean either "the dragon" or, especially in the present day, "the devil".
The novel has been in the public domain in the United States since its original publication because Stoker failed to follow proper copyright procedure. In the United Kingdom and other countries following the Berne Convention on copyrights, however, the novel was under copyright until April 1962, fifty years after Stoker's death. When F. W. Murnau's unauthorized film adaptation "Nosferatu" was released in 1922, the popularity of the novel increased considerably, owing to the controversy caused when Stoker's widow tried to have the film removed from public circulation.
Reaction and scholarly criticism.
When it was first published, in 1897, "Dracula" was not an immediate bestseller, although reviewers were unstinting in their praise. The contemporary "Daily Mail" ranked Stoker's powers above those of Mary Shelley and Edgar Allan Poe as well as Emily Brontë's "Wuthering Heights".
According to literary historians Nina Auerbach and David Skal in the Norton Critical Edition, the novel has become more significant for modern readers than it was for contemporary Victorian readers, most of whom enjoyed it just as a good adventure story; it only reached its broad iconic legendary classic status later in the 20th century when the movie versions appeared. It did not make much money for Stoker; the last year of his life he was so poor that he had to petition for a compassionate grant from the Royal Literary Fund, and in 1913 his widow was forced to sell his notes and outlines of the novel at a Sotheby's auction, where they were purchased for a little over 2 pounds. But when W. Murnau's unauthorized adaptation of the story in the form of "Nosferatu" was released in theatres in 1922, Stoker's widow took affaire, and during the legal battle that followed, the novel's popularity started to grow. "Nosferatu" was followed by a highly successful stage adaptation, touring the UK for three years before arriving in US where Stoker's creation caught Hollywood's attention, and after the American 1931 movie version was released, the book has never been out of print. However, some Victorian fans were ahead of the time, describing it as "the sensation of the season" and "the most blood-curdling novel of the paralysed century". Sherlock Holmes author Sir Arthur Conan Doyle wrote to Stoker in a letter, "I write to tell you how very much I have enjoyed reading "Dracula". I think it is the very best story of diablerie which I have read for many years." The "Daily Mail" review of 1 June 1897 proclaimed it a classic of Gothic horror, "In seeking a parallel to this weird, powerful, and horrorful story our mind reverts to such tales as "The Mysteries of Udolpho", "Frankenstein", "The Fall of the House of Usher" ... but Dracula is even more appalling in its gloomy fascination than any one of these."
Similarly good reviews appeared when the book was published in the U.S. in 1899. The first American edition was published by Doubleday and McClure in New York.
In the last several decades, literary and cultural scholars have offered diverse analyses of Stoker's novel and the character of Count Dracula. C.F. Bentley reads Dracula as an embodiment of the Freudian id. Carol A. Senf reads the novel as a response to the powerful New Woman. while Christopher Craft sees Dracula as embodying latent homosexuality. Stephen D. Arata interprets the events of the novel as anxiety over colonialism and racial mixing, and Talia Schaffer construes the novel as an indictment of Oscar Wilde. Franco Moretti reads Dracula as a figure of monopoly capitalism, though Hollis Robbins suggests that Dracula's inability to participate in social conventions and to forge business partnerships undermines his power. Richard Noll reads "Dracula" within the context of 19th century alienism (psychiatry)and asylum medicine. D. Bruno Starrs understands the novel to be a pro-Catholic pamphlet promoting proselytization.
Historical and geographical references.
Although "Dracula" is a work of fiction, it does contain some historical references. The historical connections with the novel and how much Stoker knew about the history are a matter of conjecture and debate.
Following the publication of "In Search of Dracula" by Radu Florescu and Raymond McNally in 1972, the supposed connections between the historical Transylvanian-born Vlad III Dracula of Wallachia and Bram Stoker's fictional Dracula attracted popular attention. During his main reign (1456–1462), "Vlad the Impaler" is said to have killed from 40,000 to 100,000 European civilians (political rivals, criminals, and anyone he considered "useless to humanity"), mainly by impaling. The sources depicting these events are records by Saxon settlers in neighbouring Transylvania, who had frequent clashes with Vlad III. Vlad III is revered as a folk hero by Romanians for driving off the invading Ottoman Turks, of which his impaled victims are said to have included as many as 100,000.
Historically, the name "Dracula" is derived from a Chivalric order called the Order of the Dragon, founded by Sigismund of Luxembourg (then king of Hungary) to uphold Christianity and defend the Empire against the Ottoman Turks. Vlad II Dracul, father of Vlad III, was admitted to the order around 1431, after which Vlad II wore the emblem of the order and later, as ruler of Wallachia, his coinage bore the dragon symbol. The name Dracula means "Son of Dracul".
Stoker came across the name Dracula in his reading on Romanian history, and chose this to replace the name ("Count Wampyr") originally intended for his villain. Some Dracula scholars, led by Elizabeth Miller, argue that Stoker knew little of the historic Vlad III except for the name "Dracula", whereas in the novel, Stoker mentions the Dracula who fought against the Turks, and was later betrayed by his brother, historical facts which unequivocally point to Vlad III:
The Count's identity is later speculated on by Professor Van Helsing:
Many of Stoker's biographers and literary critics have found strong similarities to the earlier Irish writer Sheridan Le Fanu's classic of the vampire genre, "Carmilla". In writing "Dracula", Stoker may also have drawn on stories about the sídhe, some of which feature blood-drinking women. The folkloric figure of Abhartach has also been suggested as a source.
In 1983, McNally additionally suggested that Stoker was influenced by the history of Hungarian Countess Elizabeth Bathory, who tortured and killed between 36 and 700 young women. It was later commonly believed that she committed these crimes to bathe in their blood, believing that this preserved her youth.
In her book "The Essential Dracula", Clare Haword-Maden opined the castle of Count Dracula was inspired by Slains Castle, at which Bram Stoker was a guest of the 19th Earl of Erroll. According to Miller, he first visited Cruden Bay in 1893, three years after work on "Dracula" had begun. Haining and Tremaine maintain that during this visit, Stoker was especially impressed by Slains Castle's interior and the surrounding landscape. Miller and Leatherdale question the stringency of this connection. Possibly, Stoker was not inspired by a real edifice at all, but by Jules Verne's novel "The Carpathian Castle" (1892) or Anne Radcliffe's "The Mysteries of Udolpho" (1794). A third possibility is that he copied information about a castle at Vécs from one of his sources on Transylvania, the book by Major E.C. Johnson. A further option is that Stoker saw an illustration of Castle Bran (Törzburg) in the book on Transylvania by Charles Boner, or read about it in the books by Mazuchelli or Crosse. 
Many of the scenes in Whitby and London are based on real places that Stoker frequently visited, although in some cases he distorts the geography for the sake of the story. One scholar has suggested that Stoker chose Whitby as the site of Dracula's first appearance in England because of the Synod of Whitby, given the novel's preoccupation with timekeeping and calendar disputes. 
Daniel Farson, Leonard Wolf, and Peter Haining have suggested that Stoker received much historical information from Ármin Vámbéry, a Hungarian professor he met at least twice. Miller argues "there is nothing to indicate that the conversation included Vlad, vampires, or even Transylvania" and that, "furthermore, there is no record of any other correspondence between Stoker and Vámbéry, nor is Vámbéry mentioned in Stoker's notes for Dracula."
Adaptations.
The story of "Dracula" has been the basis for numerous films and plays. Stoker himself wrote the first theatrical adaptation, which was presented at the Lyceum Theatre under the title "Dracula, or The Undead" shortly before the novel's publication and performed only once. Popular films include "Dracula" (1931), "Dracula" (alternative title: "The Horror of Dracula") (1958), and "Dracula" (also known as "Bram Stoker's Dracula") (1992). "Dracula" was also adapted as "Nosferatu" (1922), a film directed by the German director F. W. Murnau, without permission from Stoker's widow; the filmmakers attempted to avoid copyright problems by altering many of the details, including changing the name of the villain to "Count Orlok".
The character of Count Dracula has remained popular over the years, and many films have used the character as a villain, while others have named him in their titles, including "Dracula's Daughter" and "The Brides of Dracula". As of 2009, an estimated 217 films feature Dracula in a major role, a number second only to Sherlock Holmes (223 films).
Most adaptations do not include all the major characters from the novel. The Count is always present, and Jonathan and Mina Harker, Dr. Seward, Dr. Van Helsing, and Renfield usually appear as well. The characters of Mina and Lucy are often combined into a single female role. Jonathan Harker and Renfield are also sometimes reversed or combined. Quincey Morris and Arthur Holmwood are usually omitted entirely ("Bram Stoker's Dracula" being a notable exception).
"Dracula's Guest".
In 1914, two years after Stoker's death, the short story "Dracula's Guest" was posthumously published. It was, according to most contemporary critics, the deleted first (or second) chapter from the original manuscript and the one which gave the volume its name, but which the original publishers deemed unnecessary to the overall story.
"Dracula's Guest" follows an unnamed Englishman traveller as he wanders around Munich before leaving for Transylvania. It is Walpurgis Night, and in spite of the coachman's warnings, the young Englishman foolishly leaves his hotel and wanders through a dense forest alone. Along the way he feels he is being watched by a tall and thin stranger (possibly Count Dracula).
The short story climaxes in an old graveyard, where in a marble tomb (with a large iron stake driven into it), the Englishman encounters a sleeping female vampire called "Countess Dolingen". This malevolent and beautiful vampire awakens from her marble bier to conjure a snowstorm before being struck by lightning and returning to her eternal prison. However, the Englishman's troubles are not quite over, as he is dragged away by an unseen force and rendered unconscious. He awakes to find a "gigantic" wolf lying on his chest and licking at his throat; however, the wolf merely keeps him warm and protects him until help arrives.
When the Englishman is finally taken back to his hotel, a telegram awaits him from his expectant host Dracula, with a warning about "dangers from snow and wolves and night".

</doc>
<doc id="7925" url="http://en.wikipedia.org/wiki?curid=7925" title="David Hume">
David Hume

David Hume (; 25 August 1776) was a Scottish philosopher, historian, economist, and essayist known especially for his philosophical empiricism and scepticism. He was one of the most important figures in the history of Western philosophy and the Scottish Enlightenment. Hume is often grouped with John Locke, George Berkeley, and a handful of others as a British Empiricist.
Beginning with his "A Treatise of Human Nature" (1739), Hume strove to create a total naturalistic "science of man" that examined the psychological basis of human nature. In stark opposition to the rationalists who preceded him, most notably Descartes, he concluded that desire rather than reason governed human behaviour, saying: "Reason is, and ought only to be the slave of the passions." A prominent figure in the sceptical philosophical tradition and a strong empiricist, he argued against the existence of innate ideas. He concluded instead that humans have knowledge only of things they directly experience. Thus he divides perceptions between strong and lively "impressions" or direct sensations and fainter "ideas", which are copied from impressions.
He developed the position that mental behaviour is governed by "custom", that is acquired ability; our use of induction, for example, is justified only by our idea of the "constant conjunction" of causes and effects. Without direct impressions of a metaphysical "self", he concluded that humans have no actual conception of the self, only of a bundle of sensations associated with the self.
Hume advocated a compatibilist theory of free will that proved extremely influential on subsequent moral philosophy. He was also a sentimentalist who held that ethics are based on feelings rather than abstract moral principles. Hume also examined the normative is–ought problem. He held notoriously ambiguous views of Christianity, but famously challenged the argument from design in his "Dialogues Concerning Natural Religion" (1777).
Immanuel Kant credited Hume with waking him up from his "dogmatic slumbers" and Hume has proved extremely influential on subsequent philosophy, especially on utilitarianism, logical positivism, William James, philosophy of science, early analytic philosophy, cognitive philosophy, and other movements and thinkers. The philosopher Jerry Fodor proclaimed Hume's "Treatise" "the founding document of cognitive science". Also famous as a prose stylist, Hume pioneered the essay as a literary genre and engaged with contemporary intellectual luminaries such as Jean-Jacques Rousseau, Adam Smith (who acknowledged Hume's influence on his economics and political philosophy), James Boswell, Joseph Butler, and Thomas Reid.
Biography.
David Home, anglicised to David Hume, son of Joseph Home of Chirnside, advocate, and Katherine Falconer, was born on 26 April 1711 (Old Style) in a tenement on the north side of the Lawnmarket in Edinburgh. He changed the spelling of his name in 1734, because the fact that his surname 'Home' was pronounced 'Hume' in Scotland was not known in England. Throughout his life Hume, who never married, spent time occasionally at his family home at Ninewells by Chirnside, Berwickshire, which had belonged to his family since the sixteenth century.
Education.
Hume attended the University of Edinburgh at the unusually early age of twelve (possibly as young as ten) at a time when fourteen was normal. At first he considered a career in law, but came to have, in his words, "an insurmountable aversion to everything but the pursuits of Philosophy and general Learning; and while [my family] fanceyed I was poring over Voet and Vinnius, Cicero and Virgil were the Authors which I was secretly devouring." He had little respect for the professors of his time, telling a friend in 1735, "there is nothing to be learnt from a Professor, which is not to be met with in Books."
Hume made a philosophical discovery that opened up to him "...a new Scene of Thought," which inspired him "...to throw up every other Pleasure or Business to apply entirely to it." He did not recount what this "Scene" was, and commentators have offered a variety of speculations. Due to this inspiration, Hume set out to spend a minimum of ten years reading and writing. He came to the verge of nervous breakdown, after which he decided to have a more active life to better continue his learning.
Career.
As Hume's options lay between a travelling tutorship and a stool in a merchant's office, he chose the latter. In 1734, after a few months occupied with commerce in Bristol, he went to La Flèche in Anjou, France. There he had frequent discourse with the Jesuits of the College of La Flèche. As he had spent most of his savings during his four years there while writing "A Treatise of Human Nature", he resolved "to make a very rigid frugality supply my deficiency of fortune, to maintain unimpaired my independency, and to regard every object as contemptible except the improvements of my talents in literature". He completed the "Treatise" at the age of 26.
Although many scholars today consider the "Treatise" to be Hume's most important work and one of the most important books in Western philosophy, the critics in Great Britain at the time did not agree, describing it as "abstract and unintelligible".
Despite the disappointment, Hume later wrote, "Being naturally of a cheerful and sanguine temper, I soon recovered from the blow and prosecuted with great ardour my studies in the country." There, he wrote the "Abstract". Without revealing his authorship, he aimed to make his larger work more intelligible.
After the publication of "Essays Moral and Political" in 1744, Hume applied for the Chair of Pneumatics and Moral Philosophy at the University of Edinburgh. However, the position was given to William Cleghorn, after Edinburgh ministers petitioned the town council not to appoint Hume because he was seen as an atheist.
During the 1745 Jacobite Rebellion, Hume tutored the Marquis of Annandale (1720–92), who was officially described as a "lunatic". This engagement ended in disarray after about a year. However, it was then that Hume started his great historical work "The History of England". This took him fifteen years and ran to over a million words. It was published in six volumes in the period between 1754 and 1762, while he was also involved with the Canongate Theatre through his friend John Home.
In this context, he associated with Lord Monboddo and other Scottish Enlightenment luminaries in Edinburgh. From 1746, Hume served for three years as secretary to Lieutenant-General St Clair, and wrote "Philosophical Essays Concerning Human Understanding", later published as "An Enquiry Concerning Human Understanding". The "Enquiry" proved little more successful than the "Treatise", perhaps because of the publishing of his short autobiography, "My Own Life", which "made friends difficult for the first Enquiry".
Hume's friends averted a trial against him on the charge of heresy. However, he "would not have come and could not be forced to attend if he said he was not a member of the Established Church." Hume failed to gain the chair of philosophy at the University of Glasgow.
It was after returning to Edinburgh in 1752, as he wrote in "My Own Life", that "the Faculty of Advocates chose me their Librarian, an office from which I received little or no emolument, but which gave me the command of a large library." This resource enabled him to continue historical research for "The History of England".
Hume achieved great literary fame as a historian. His enormous "The History of England", tracing events from the Invasion of Julius Caesar to the Revolution of 1688, was a best-seller in its day.
Hume's volume of "Political Discourses" (published by Kincaid & Donaldson, 1752) was the only work he considered successful on first publication.
Religion.
There has been much discussion concerning Hume's personal position on religion. Although he wrote a great deal about religion, the question of what were Hume's personal views is a difficult one. Contemporaries seemed to consider him to be an atheist, or at least un-Christian, and the Church of Scotland seriously considered bringing charges of infidelity against him.The best theologian he ever met, he used to say, was the old Edinburgh fishwife who, having recognized him as Hume the atheist, refused to pull him out of the bog into which he had fallen until he declared he was a Christian and repeated the Lord's prayer.
However, in works such as "On Superstition and Enthusiasm", Hume specifically seems to support the standard religious views of his time and place. This still meant that he could be very critical of the Catholic Church, dismissing it with the standard Protestant accusations of superstition and idolatry as well as dismissing what his compatriots saw as uncivilised beliefs. He also considered extreme Protestant sects, the members of which he called "enthusiasts", to be corrupters of religion. Yet he also put forward arguments that suggested that polytheism had much to commend it in preference to monotheism.
It is likely that Hume was sceptical both about religious belief (at least as demanded by the religious organisations of his time) and of the complete atheism promoted by such contemporaries as Baron d'Holbach. Paul Russell suggests that perhaps Hume's position is best characterised by the term "irreligion". David O'Connor writes that Hume’s final position was "weakly deistic", but that this "position is deeply ironic. This is because, while inclining towards a weak form of deism, he seriously doubts that we can ever find a sufficiently favourable balance of evidence to justify accepting any religious position." He adds that Hume "did not believe in the God of standard theism. ... but he did not rule out all concepts of deity". Also, "ambiguity suited his purposes, and this creates difficulty in definitively pinning down his final position on religion".
Marc Hanvelt identifies Hume as an Aristotelian in his view that rhetoric is a form of ethical studies, which ultimately makes it political.
Later years.
From 1763 to 1765, Hume was secretary to Lord Hertford in Paris, where he met and later fell out with Jean-Jacques Rousseau. He wrote of his Paris life, "I really wish often for the plain roughness of The Poker Club of Edinburgh ... to correct and qualify so much lusciousness." For a year from 1767, Hume held the appointment of Under Secretary of State for the Northern Department. In 1768 he settled in Edinburgh where he lived from 1771 until his death in 1776 at the south-west corner of St. Andrew's Square in Edinburgh's New Town, at what is now 21 Saint David Street. (A popular story, consistent with some historical evidence, suggests the street was named after Hume.)
James Boswell saw Hume a few weeks before his death, which was from some form of abdominal cancer. Hume told him he sincerely believed it a "most unreasonable fancy" that there might be life after death. This meeting was dramatized in semi-fictional form for the BBC by Michael Ignatieff as "Dialogue in the Dark". Hume asked that his body be interred in a "simple roman tomb." In his will he requests that it be inscribed only with his name and the year of his birth and death, "leaving it to Posterity to add the Rest." It stands, as he wished it, on the south-western slope of Calton Hill, in the Old Calton Cemetery. Adam Smith later recounted Hume's amusing speculation that he might ask Charon to allow him a few more years of life in order to see "the downfall of some of the prevailing systems of superstition". The ferryman replied, "You loitering rogue, that will not happen these many hundred years... Get into the boat this instant..."
As historian of England.
In 1754 to 1762 Hume published the "History of England", a 6-volume work of immense sweep, which extends, says its subtitle, "From the Invasion of Julius Caesar to the Revolution in 1688." Inspired by Voltaire's sense of the breadth of history, Hume widened the focus of the field, away from merely kings, parliaments, and armies, to literature and science as well. He argued that the quest for liberty was the highest standard for judging the past, and concluded that after considerable fluctuation, England at the time of his writing had achieved "the most entire system of liberty, that was ever known amongst mankind."
Hume's coverage of the political upheavals of the 17th century relied in large part on the Earl of Clarendon's "History of the Rebellion and Civil Wars in England" (1646–69). Generally Hume took a moderate Royalist position and considered revolution was unnecessary to achieve necessary reform. Hume's was considered a Tory history, and emphasized religious differences more than constitutional issues. Laird Okie writes: "Hume preached the virtues of political moderation, but ... it was moderation with an anti-Whig, pro-Royalist coloring." For "Hume shared the ... Tory belief that the Stuarts were no more high-handed than their Tudor predecessors." Also, "Even though Hume wrote with an anti-Whig animus, it is, paradoxically, correct to regard the History as an establishment work, one which implicitly endorsed the ruling oligarchy"
Historians have debated whether Hume posited a universal unchanging human nature, or allowed for evolution and development.
Roth argues that Hume's histories display his biases against Presbyterians and Puritans. Roth says his anti-Whig pro-monarchy position diminished the influence of his work, and that his emphasis on politics and religion led to a neglect of social and economic history.
However Hume was an early cultural historian of science. His short biographies of leading scientists explored the process of scientific change. He developed new ways of seeing scientists in the context of their times by looking at how they interacted with society and each other. He covers over forty scientists, with special attention paid to Francis Bacon, Robert Boyle, and Isaac Newton. Hume awarded the palm of greatness to William Harvey.
The "History" sold well and was influential for nearly a century, despite competition from imitations by Smollett (1757), Goldsmith (1771) and others. By 1894, there were at least 50 editions. There was also an often-reprinted abridgement, "The Student's Hume" (1859).
Thought.
In the introduction to "A Treatise of Human Nature", Hume writes "'Tis evident, that all the sciences have a relation, more or less, to human nature ... Even "Mathematics", "Natural Philosophy", and "Natural Religion", are in some measure dependent on the science of Man." Also, "the science of man is the only solid foundation for the other sciences", and the method for this science assumes "experience and observation" as the foundations of a logical argument. Because "Hume's plan is to extend to philosophy in general the methodological limitations of Newtonian physics," Hume is characterised as an empiricist.
Until recently, Hume was seen as a forerunner of the logical positivist movement; a form of anti-metaphysical empiricism. According to the logical positivists, unless a statement could be verified by experience, or else was true or false by definition (i.e. either tautological or contradictory), then it was meaningless (this is a summary statement of their verification principle). Hume, on this view, was a proto-positivist, who, in his philosophical writings, attempted to demonstrate how ordinary propositions about objects, causal relations, the self, and so on, are semantically equivalent to propositions about one's experiences.
Many commentators have since rejected this understanding of Humean empiricism, stressing an epistemological, rather than a semantic reading of his project. According to this opposing view, Hume's empiricism consisted in the idea that it is our knowledge, and not our ability to conceive, that is restricted to what can be experienced. To be sure, Hume thought that we can form beliefs about that which extends beyond any possible experience, through the operation of faculties such as custom and the imagination, but he was sceptical about claims to "knowledge" on this basis.
Induction.
The cornerstone of Hume's epistemology is the problem of induction. This may be the area of Hume's thought where his scepticism about human powers of reason is most pronounced. Understanding the problem of induction is central to grasping Hume's philosophical system.
The problem concerns the explanation of how we are able to make inductive inferences. Inductive inference is reasoning from the observed behaviour of objects to their behaviour when unobserved. As Hume says, it is a question of how things behave when they go "beyond the present testimony of the senses, and the records of our memory". Hume notices that we tend to believe that things behave in a regular manner; so that patterns in the behaviour of objects seem to persist into the future, and throughout the unobserved present. This persistence of regularities is sometimes called Uniformitarianism or the Principle of the Uniformity of Nature.
Hume's argument is that we cannot rationally justify the claim that nature will continue to be uniform, as justification comes in only two varieties, and both of these are inadequate. The two sorts are: demonstrative reasoning, and probable reasoning. With regard to demonstrative reasoning, Hume argues that the uniformity principle cannot be demonstrated, as it is "consistent and conceivable" that nature might stop being regular. Turning to probable reasoning, Hume argues that we cannot hold that nature will continue to be uniform because it has been in the past. As this is using the very sort of reasoning (induction) that is under question, it would be circular reasoning. Thus no form of justification will rationally warrant our inductive inferences.
Hume's solution to this problem is to argue that, rather than reason, natural instinct explains the human practice of making inductive inferences. He asserts that "Nature, by an absolute and uncontroulable necessity has determin'd us to judge as well as to breathe and feel." Although many modern commentators have demurred from Hume's solution, some have notably concurred with it, seeing his analysis of our epistemic predicament as a major contribution to the theory of knowledge. For example, philosopher John D. Kenyon writes: Reason might manage to raise a doubt about the truth of a conclusion of natural inductive inference just for a moment in the study, but the forces of nature will soon overcome that artificial scepticism, and the sheer agreeableness of animal faith will protect us from excessive caution and sterile suspension of belief.
Causation.
The notion of causation is closely linked to the problem of induction. According to Hume, we reason inductively by associating constantly conjoined events. It is the mental act of association that is the basis of our concept of causation. There are three main interpretations of Hume's theory of causation represented in the literature: (1) the logical positivist; (2) the sceptical realist; and (3) the quasi-realist.
The logical positivist interpretation is that Hume analyses causal propositions, such as "A caused B", in terms of regularities in perception: "A causes B" is equivalent to "Whenever A-type events happen, B-type ones follow", where "whenever" refers to all possible perceptions.
power and necessity... are... qualities of perceptions, not of objects... felt by the soul and not perceived externally in bodies.
This view is rejected by sceptical realists, who argue that Hume thought that causation amounts to more than just the regular succession of events. When two events are causally conjoined, a necessary connection underpins the conjunction:
Shall we rest contented with these two relations of contiguity and succession, as affording a complete idea of causation? By no means ... there is a "necessary connexion" to be taken into consideration.
Angela Coventry writes that, for Hume, "there is nothing in any particular instance of cause and effect involving external objects which suggests the idea of power or necessary connection." Also, "we are ignorant of the powers that operate between objects"
However, referring to the Law of Causality, Hume wrote, "I never asserted so absurd a proposition as that something could arise without a cause."
It has been argued that, while Hume did not think causation is reducible to pure regularity, he was not a fully fledged realist either. Simon Blackburn calls this a quasi-realist reading. On this view, talk about causal necessity is an expression of a functional change in the human mind, whereby certain events are predicted or anticipated on the basis of prior experience. The expression of causal necessity is a "projection" of the functional change onto the objects involved in the causal connection. In Hume's words, "nothing is more usual than to apply to external bodies every internal sensation which they occasion."
The self.
According to the standard interpretation of Hume on personal identity, he was a bundle theorist, who held that the self is nothing but a bundle of experiences ("perceptions") linked by the relations of causation and resemblance; or, more accurately, that the empirically warranted idea of the self is just the idea of such a bundle. This view is forwarded by, for example, positivist interpreters, who saw Hume as suggesting that terms such as "self", "person", or "mind" referred to collections of "sense-contents". A modern-day version of the bundle theory of the mind has been advanced by Derek Parfit in his "Reasons and Persons"
However, some philosophers have criticised Hume's bundle-theory interpretation of personal identity. They argue that distinct selves can have perceptions that stand in relations of similarity and causality with one another. Thus perceptions must already come parcelled into distinct "bundles" before they can be associated according to the relations of similarity and causality. In other words, the mind must already possess a unity that cannot be generated, or constituted, by these relations alone. Since the bundle-theory interpretation portrays Hume as answering an ontological question, philosophers who see Hume as not very concerned with such questions have queried whether the view is really Hume's, or "only a decoy". Instead, it is suggested that Hume might have been answering an epistemological question about the causal origin of our concept of the self. In the Appendix to the Treatise, Hume declares himself dissatisfied with his account of the self in Book 1 of the Treatise, and the question of why he is dissatisfied has received a number of different answers.
Another interpretation of Hume's view of the self has been argued for by James Giles. According to his view, Hume is not arguing for a bundle theory, which is a form of reductionism, but rather for an eliminative view of the self. That is, rather than reducing the self to a bundle of perceptions, Hume is rejecting the idea of the self altogether. On this interpretation, Hume is proposing a 'No-Self Theory' and thus has much in common with Buddhist thought. On this point, Alison Gopnik has argued that Hume was in a position to learn about Buddhist thought during his time in France in the 1730s.
Practical reason.
Hume's anti-rationalism informed much of his theory of belief and knowledge, in his treatment of the notions of induction, causation, and the external world. But it was not confined to this sphere, and also permeated his theories of motivation, action, and morality. In a famous sentence in the "Treatise", Hume circumscribes reason's role in the production of action:
Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them.
It has been suggested that this position can be clarified through the metaphor of "direction of fit". In this, beliefs, which are the paradigmatic products of reason, are propositional attitudes that aim to have their content fit the world. Conversely, desires, which Hume calls passions, or sentiments, are states that aim to fit the world to their contents. Though a metaphor, it has been argued that this intuitive way of understanding Hume's theory, that desires are necessary for motivation, "captures something quite deep in our thought about their nature".
Hume's anti-rationalism has been very influential, and defended in contemporary philosophy of action by neo-Humeans such as Michael Smith and Simon Blackburn. The major opponents of the Humean view are cognitivists such as John McDowell, concerned with what it is to act for a reason, and Kantians, such as Christine Korsgaard.
Ethics.
Hume's views on human motivation and action formed the cornerstone of his ethical theory: he conceived moral or ethical sentiments to be intrinsically motivating, or the providers of reasons for action. Given that one cannot be motivated by reason alone, requiring the input of the passions, Hume argued that reason cannot be behind morality.
Morals excite passions, and produce or prevent actions. Reason itself is utterly impotent in this particular. The rules of morality, therefore, are not conclusions of our reason.
Hume's sentimentalism about morality was shared by his close friend Adam Smith, and Hume and Smith were mutually influenced by the moral reflections of Francis Hutcheson.
Hume's theory of ethics has been influential in modern day meta-ethical theory, helping to inspire various forms of emotivism, error theory and ethical expressivism and non-cognitivism, as well as Allan Gibbard's general theory of moral judgment and judgments of rationality.
Free will, determinism, and responsibility.
Hume, along with Thomas Hobbes, is cited as a classical compatibilist about the notions of freedom and determinism. The thesis of compatibilism seeks to reconcile human freedom with the mechanist belief that human beings are part of a deterministic universe, whose happenings are governed by the laws of physics. Hume, to this end, was influenced greatly by the scientific revolution and by in particular Sir Isaac Newton.
Hume argued that the dispute about the compatibility of freedom and determinism has been kept afloat by ambiguous terminology:
Hume defines the concepts of "necessity" and "liberty" as follows:
Necessity: "the uniformity, observable in the operations of nature; where similar objects are constantly conjoined together..."
Liberty: ""a power of acting or not acting, according to the determinations of the will"..."
Hume then argues that, according to these definitions, not only are the two compatible, but liberty "requires" necessity. For if our actions were not necessitated in the above sense, they would "...have so little in connexion with motives, inclinations and circumstances, that one does not follow with a certain degree of uniformity from the other." But if our actions are not thus connected to the will, then our actions can never be free: they would be matters of "chance; which is universally allowed not to exist."
Moreover, Hume goes on to argue that in order to be held morally responsible, it is required that our behaviour be caused, i.e. necessitated, for
Hume describes the link between causality and our capacity to rationally make a decision from this an inference of the mind. Human beings assess a situation based upon certain predetermined events and from that form a choice. Hume believes that this choice is made spontaneously. Hume calls this form of decision making the liberty of spontaneity.
Hume further illustrates his position by rejecting a famous moral puzzle by french philosopher Jean Buridan. Buridan starts his puzzle by describing a donkey that is hungry. This donkey has on both sides of him separate bales of hay which are of equal distance away from him. Which bale does the donkey choose? Buridan believes that the donkey would die as he has no autonomy. The donkey is incapable of forming a rational decision as there is no motive to choose one bale of hay over the other. For Buridan, human beings are different, because a human who is placed in a position where he is forced to choose one loaf of bread over another will make a decision to take one in lieu of the other. For Buridan, humans have the capacity of autonomy. Buridan recognizes the choice that's ultimately made will be based on chance as both loaves of bread are indifferent from one another. However, Hume completely rejects this notion; he argues that a human will spontaneously act in such a situation as he's faced with impending death if he fails to. Such a decision is not done on the basis of chance, but rather on necessity and spontaneity given the prior predetermined events leading up to the predicament.
This argument has inspired modern day commentators such as R. E. Hobart. However, it has been argued that the issue of whether or not we hold one another morally responsible does not ultimately depend on the truth or falsity of a metaphysical thesis such as determinism. This is because our so holding one another is a non-rational human sentiment that is not predicated on such theses. For this influential argument, which is still made in a Humean vein, see P. F. Strawson's essay, "Freedom and Resentment".
Writings on Religion.
"David Hume's various writings concerning problems of religion are among the most important and influential contributions on this topic." His writings in this field cover the philosophy, psychology, history, and anthropology of religious thought. All of these aspects were discussed in Hume's 1757 dissertation, "The Natural History of Religion". Here he argued that the monotheistic religions of Judaism, Christianity and Islam, all derive from earlier polytheistic religions. He also suggests that all religious belief "traces, in the end, to dread of the unknown." Hume had also written on religious subjects in his "Enquiry", as well as later in his Dialogues Concerning Natural Religion.
Design argument.
One of the traditional topics of Natural theology is that of the existence of God, and one of the a posteriori arguments for this is the "argument from design" or the Teleological argument. This is "the most popular, because the most accessible, of the theistic arguments ... which identifies evidences of design in nature, inferring from them a divine designer. ... The fact that the universe as a whole is a coherent and efficiently functioning system likewise, in this view, indicates a divine intelligence behind it."
Socrates, Plato and Aristotle all produced teleological arguments, and similar design arguments for the existence of God were expressed by St. Paul, with many others in the Greco-Roman world, who believed that the existence of God is evident from the appearances of nature. St Paul wrote: “Ever since the creation of the world his invisible nature, namely, his eternal power and deity, has been clearly perceived in the things that have been made”. The argument was also put forward by medieval Christian thinkers as well as by later writers such as Robert Boyle, John Ray, Samuel Clarke, and William Derham. William Paley, in the 19th century, produced a popular argument in his watchmaker analogy. Such writers asked questions like: Is not the eye as manifestly designed for seeing, and the ear for hearing, as a pen for writing or a clock for telling the time; and does not such design imply a designer?
In his "Enquiry", Hume wrote that the design argument seems to depend upon our experience, and its proponents "always suppose the universe, an effect quite singular and unparalleled, to be the proof of a Deity, a cause no less singular and unparalleled." In this connection, L. E. Loeb notes that "we observe neither God nor other universes, and hence no conjunction involving them. There is no observed conjunction to ground an inference either to extended objects or to God, as unobserved causes."
Hume also criticized the argument in his Dialogues Concerning Natural Religion (1779). He suggested that, even if the world is a more or less smoothly functioning system, this may only be a result of the "chance permutations of particles falling into a temporary or permanent self-sustaining order, which thus has the appearance of design".
A century later the idea of order without design was rendered more plausible by Charles Darwin's discovery that the adaptations of the forms of life are a result of the natural selection of inherited characteristics. J. F. Sennett and D. Groothuis write, "Suffice it to say that Hume, rivaled only by Darwin, has done the most to undermine in principle our confidence in arguments from design among all figures in the Western intellectual tradition."
Finally, Hume discussed a version of the Anthropic Principle. "According to the anthropic principle, we are entitled to infer facts about the universe and its laws from the undisputed fact that we (we anthropoi, we human beings) are here to do the inferring and observing." Hume has his skeptical mouthpiece Philo suggest that there may have been multiple universes, produced by an incompetent designer that he called a "stupid mechanic".Many worlds might have been botched and bungled, throughout an eternity, ere this system was struck out: Much labour lost: Many fruitless trials made: And a slow, but continued improvement carried on during infinite ages of world-making.This mechanical explanation of teleology anticipated the notion of natural selection.
Problem of miracles.
In his discussion of miracles in "An Enquiry concerning Human Understanding" (Section 10) Hume defines a miracle as "a transgression of a law of nature by a particular volition of the Deity, or by the interposition of some invisible agent". Given that Hume argues that it is impossible to deduce the existence of a Deity from the existence of the world (for he says that causes cannot be determined from effects), miracles (including prophecy) are the only possible support he would conceivably allow for theistic religions.
Hume discusses everyday belief as often resulting from probability. We believe an event that has occurred often as being likely to occur again, but we also take into account those instances where the event did not occur.Hume writes:In all cases where there are opposing experiments, we must balance them against one another and subtract the smaller number from the greater in order to know the exact force of the superior evidence.
In the context of miracles, Hume discusses the testimony of those reporting them. He writes that testimony might be doubted even from some great authority: "The incredibility of a fact ... might invalidate even that great an authority." Also, "The value of this testimony as evidence will be greater or less in proportion as the fact that is attested to is less or more unusual."
Although Hume leaves open the possibility for miracles to occur and be reported, he offers various arguments against this ever having happened in history:
Indeed, Hume was extremely pleased with his argument against miracles in his "Enquiry Concerning Human Understanding" he stated: “I flatter myself, that I have discovered an argument of a like nature, which, if just, will, with the wise and learned, be an everlasting check to all kinds of superstitious delusion, and consequently, will be useful as long as the world endures.” Thus Hume's argument against miracles had a more abstract basis founded upon the scrutiny not just primarily of miracles but of all forms of belief systems. It is a common sense notion of veracity based upon epistemological evidence. Founded on a principle of rationality, proportionality and reasonability greatly analogous to the evidence used in a Civil Court.
The criteria for assessing a belief system for Hume is on the balance of probabilities whether something is more likely than not to have occurred. Since the weight of empirical experience contradicts the notion for the existence of miracles such accounts should be treated with scepticism. Further, the myriad of accounts of miracles contradict one another as some people who receive miracles will aim to prove the authority of Jesus whereas others will aim to prove the authority of Muhammad or some other religious prophet or deity. These various differing accounts weaken the overall evidential power of miracles.
Despite all this, Hume observes that belief in miracles is popular, and that "The gazing populace receive greedily, without examination, whatever soothes superstition and promotes wonder."
Critics have argued that Hume's position assumes the character of miracles and natural laws prior to any specific examination of miracle claims, and thus it amounts to a subtle form of begging the question. They have also noted that it requires an appeal to inductive inference, as none have observed every part of nature nor examined every possible miracle claim, for instance those in the future. This in Hume's philosophy was especially problematic.
Hume's main argument concerning miracles is that miracles by definition are singular events that differ from the established Laws of Nature. The Laws of Nature are codified as a result of past experiences. Therefore a miracle is a violation of all prior experience and thus incapable on this basis of reasonable belief. However, the probability that something has occurred in contradiction of all past experience should always be judged to be less than the probability that either my senses have deceived me or the person recounting the miraculous occurrence is lying or mistaken, all of which I have past experience of. For Hume, this refusal to grant credence does not guarantee correctness – he offers the example of an Indian Prince, who having grown up in a hot country refuses to believe that water has frozen. By Hume's lights this refusal is not wrong and the Prince "reasoned soundly"; it is presumably only when he has had extensive experience of the freezing of water that he has warrant to believe that the event could occur.
So for Hume, either the miraculous event will become a recurrent event or else it will never be rational to believe it occurred. The connection to religious belief is left explained throughout, except for the close of his discussion where Hume notes the reliance of Christianity upon testimony of miraculous occurrences. He makes an ironic remark that anyone who "is moved by faith to assent" to revealed testimony "is aware of a continued miracle in his own person, which subverts all principles of his understanding, and gives him a determination to believe what is most contrary to custom and experience."
Political theory.
It is difficult to categorize Hume's political affiliations. His thought contains elements that are, in modern terms, both conservative and liberal, as well as ones that are both contractarian and utilitarian, though these terms are all anachronistic. Thomas Jefferson banned Hume's "History" from the University of Virginia, fearing that it "has spread universal toryism over the land". Yet, Samuel Johnson thought Hume "a Tory by chance... for he has no principle. If he is anything, he is a Hobbist." His central concern is to show the importance of the rule of law, and stresses throughout his political "Essays" the importance of moderation in politics.
This outlook needs to be seen within the historical context of eighteenth century Scotland. Here the legacy of religious civil war, combined with the relatively recent memory of the 1715 and 1745 Jacobite risings, fostered in a historian such as Hume a distaste for enthusiasm and factionalism that appeared to threaten the fragile and nascent political and social stability of a country that was deeply politically and religiously divided. He thinks that society is best governed by a general and impartial system of laws; he is less concerned about the form of government that administers these laws, so long as it does so fairly. However, he does write that republics must produce laws, while "Monarchy, when absolute, contains even something repugnant to law."
Hume expressed suspicion of attempts to reform society in ways that departed from long-established custom, and he counselled peoples not to resist their governments except in cases of the most egregious tyranny. However, he resisted aligning himself with either of Britain's two political parties, the Whigs and the Tories. Hume writes My views of "things" are more conformable to Whig principles; my representations of persons to Tory prejudices.McArthur writes that Hume believed that we should try to balance our demands for liberty with the need for strong authority, without sacrificing either. McArthur characterizes Hume as a 'precautionary conservative', whose actions would have been "determined by prudential concerns about the consequences of change, which often demand we ignore our own principles about what is ideal or even legitimate". He supported liberty of the press, and was sympathetic to democracy, when suitably constrained. Douglass Adair has argued that Hume was a major inspiration for James Madison's writings, and the "Federalist No. 10" in particular.
Hume was also, in general, an optimist about social progress, believing that, thanks to the economic development that comes with the expansion of trade, societies progress from a state of "barbarism" to one of "civilisation". Civilised societies are open, peaceful and sociable, and their citizens are as a result much happier. It is therefore not fair to characterise him, as Leslie Stephen did, as favouring "...that stagnation which is the natural ideal of a sceptic."
Hume offered his view on the best type of society in an essay titled "Idea of a Perfect Commonwealth", which lays out what he thought was the best form of government. He hoped that, "in some future age, an opportunity might be afforded of reducing the theory to practice, either by a dissolution of some old government, or by the combination of men to form a new one, in some distant part of the world". He defended a strict separation of powers, decentralisation, extending the franchise to anyone who held property of value and limiting the power of the clergy. The Swiss militia system was proposed as the best form of protection. Elections were to take place on an annual basis and representatives were to be unpaid. Strauss and Cropsey write that Hume thought that "the wise statesman ... 'will bear a reverence to what carries the marks of age.' In attempting to improve a constitution he will adapt his innovations to the 'ancient fabric,' so as not to disturb society. His caution may be reinforced by reflection on the limits of human foresight."
In the political analysis of George Sabine, the scepticism of Hume extended to the doctrine of government by consent. He notes that "...allegiance is a habit enforced by education and consequently as much a part of human nature as any other motive."
Contributions to economic thought.
Through his discussions on politics, Hume developed many ideas that are prevalent in the field of economics. This includes ideas on private property, inflation, and foreign trade. Referring to his essay "Of the Balance of Trade," Paul Krugman has remarked "... David Hume created what I consider the first true economic model." 
In contrast to Locke, Hume believes that private property isn't a natural right. Hume argues it is justified, because resources are limited. Private property would be an unjustified, "idle ceremonial", if all goods were unlimited and available freely. Hume also believed in an unequal distribution of property, because perfect equality would destroy the ideas of thrift and industry. Perfect equality would thus lead to impoverishment.
Influence.
Attention to Hume's philosophical works grew after the German philosopher Immanuel Kant credited Hume with awakening him from "dogmatic slumbers" ("circa" 1770).
According to Schopenhauer, "there is more to be learned from each page of David Hume than from the collected philosophical works of Hegel, Herbart and Schleiermacher taken together".
A. J. Ayer (1936), introducing his classic exposition of logical positivism, claimed: "The views which are put forward in this treatise derive from ... doctrines ..., which are themselves the logical outcome of the empiricism of Berkeley and David Hume." Albert Einstein (1915) wrote that he was inspired by Hume's positivism when formulating his Special Theory of Relativity.
Hume was called "the prophet of the Wittgensteinian revolution" by N. Phillipson, referring to his view that mathematics and logic are closed systems, disguised tautologies, and have no relation to the world of experience. David Fate Norton (1993) asserted that Hume was "the first post-sceptical philosopher of the early modern period".
Hume's Problem of Induction was also of fundamental importance to the philosophy of Karl Popper. In his autobiography, "Unended Quest", he wrote: "'Knowledge' ... is "objective"; and it is hypothetical or conjectural. This way of looking at the problem made it possible for me to reformulate Hume's "problem of induction"". This insight resulted in Popper's major work "The Logic of Scientific Discovery". In his "Conjectures and Refutations", p. 55, he writes:
I approached the problem of induction through Hume. Hume, I felt, was perfectly right in pointing out that induction cannot be logically justified.

</doc>
<doc id="7928" url="http://en.wikipedia.org/wiki?curid=7928" title="Dalton Trumbo">
Dalton Trumbo

James Dalton Trumbo (December 9, 1905 – September 10, 1976) was an American screenwriter and novelist. As one of the Hollywood Ten, he refused to testify before the House Un-American Activities Committee (HUAC) in 1947 during the committee's investigation of Communist influences in the motion picture industry. Trumbo won two Academy Awards while blacklisted; one was originally given to a front writer, and one was awarded to "Robert Rich", Trumbo's pseudonym.
Blacklisting effectively ended in 1960 when it lost credibility. Trumbo was publicly given credit for two blockbuster films: Otto Preminger made public that Trumbo wrote the screenplay for the smash hit, "Exodus", and Kirk Douglas publicly announced that Trumbo was the screenwriter of "Spartacus". Further, President John F. Kennedy crossed picket lines to see the movie.
On December 19, 2011, the Writers Guild of America announced that Trumbo was given full credit for his work on the screenplay of the 1953 romantic comedy "Roman Holiday", sixty years after the fact.
Early life.
Trumbo was born in Montrose, Colorado, the son of Maud (née Tillery) and Orus Bonham Trumbo. His family moved to Grand Junction in 1908. He was proud of his paternal ancestor, a Franco-Swiss immigrant named Jacob Trumbo (likely anglicized spelling), who settled in the colony of Virginia in 1736. Trumbo graduated from Grand Junction High School. While still in high school, he worked as a cub reporter for the "Grand Junction Daily Sentinel", covering courts, the high school, the mortuary and civic organizations. He attended the University of Colorado at Boulder for two years, working as a reporter for the "Boulder Daily Camera" and contributing to the campus humor magazine, the yearbook and the campus newspaper. He was also a member of Delta Tau Delta International Fraternity.
For nine years after his father died, he worked the night shift wrapping bread at a Los Angeles bakery, attended USC, reviewed some movies, and wrote 88 short stories and six novels that were rejected for publication.
Career.
Trumbo began his writing career in the early 1930s when several of his articles and stories appeared in magazines including the "Saturday Evening Post", "McCall's Magazine", "Vanity Fair", and the "Hollywood Spectator" In 1934 he became managing editor of the Hollywood Spectator and subsequently left to become a reader in the story department at Warner Bros. studio.
He wrote his first published novel, "Eclipse" (1935), about a town and its people, in the social realist style, drawing on his years in Grand Junction. The book was controversial in Grand Junction and many people were unhappy with his portrayal. Years after his death, he would be honored with a statue in front of the Avalon Theater on Main Street, where he was depicted writing a screenplay in a bathtub.
He started in movies in 1937 and became one of Hollywood's highest paid writers at about $4000 per week while on assignment, as much as $80,000 in one year. He worked on such films as "Thirty Seconds Over Tokyo" (1944), "Our Vines Have Tender Grapes" (1945), and "Kitty Foyle" (1940), for which he was nominated for an Academy Award for Writing Adapted Screenplay.
Trumbo's 1939 anti-war novel, "Johnny Got His Gun", won one of the early National Book Awards: the Most Original Book of 1939.
It was inspired by an article Trumbo read several years earlier, concerning the Prince of Wales' hospital visit to a Canadian soldier who had lost all his limbs in World War I.
Involvement with communism.
Dalton Trumbo aligned himself with the Communist Party USA before the 1940s, although he did not join the party until 1943.
After the outbreak of World War II in 1939, American communists argued that the United States should not get involved in the war on the side of the United Kingdom, since the Molotov-Ribbentrop Pact of nonaggression meant that the Soviet Union was at peace with Germany. In 1941, Trumbo wrote a novel "The Remarkable Andrew", in which, in one scene, the ghost of Andrew Jackson appears in order to caution the United States not to get involved in the war. In a review of the book, "Time Magazine" wrote, "General Jackson's opinions need surprise no one who has observed George Washington and Abraham Lincoln zealously following the Communist Party Line in recent years."
Shortly after the 1941 German invasion of the Soviet Union, Trumbo and his publisher decided to suspend reprinting of "Johnny Got His Gun" until the end of the war. During the war, Trumbo received letters from individuals "denouncing Jews" and using "Johnny" to support their arguments for "an immediate negotiated peace" with Nazi Germany; Trumbo reported these correspondents to the FBI. Trumbo regretted this decision, which he called "foolish". After two FBI agents showed up at his home, he understood that "their interest lay not in the letters but in me."
In a 1946 article titled "The Russian Menace" published in Rob Wagner's Script Magazine Trumbo writes from the perspective of a post-World War II Russian Citizen. He argued that Russians were likely fearful of the mass of US military power that surrounded them on all sides at a time when any sympathetic view towards communist countries was viewed with suspicion. He ends the articles by stating, "If I were a Russian...I would be alarmed, and I would petition my government to take measures at once against what would seems an almost certain blow aimed at my existence. This is how it must appear in Russia today," The fact that, in reading the article it is clear that Trumbo was arguing that the US was a "menace" to Russia, rather than the more popular American view that Russia was the "red menace" did little to help Trumbo 11 months later when he would be called to explain himself to the House Un-American Activities Committee.
Trumbo was a member of the Communist Party USA from 1943 until 1948. The scholar Kenneth Billingsley found that Trumbo wrote in "The Daily Worker" about films which he said communist influence in Hollywood had prevented from being made: among them were proposed adaptations of Arthur Koestler's anti-totalitarian works "Darkness at Noon" and "The Yogi and the Commissar," which described the rise of communism in Russia.
Blacklisting.
In October 1947, drawing upon a list of names that had appeared in the "Hollywood Reporter", the House Committee on Un-American Activities (HUAC) summoned a number of persons working in the Hollywood film industry to testify. The committee had declared its intention to investigate whether Communist agents and sympathizers had been surreptitiously planting propaganda in U.S. films. Trumbo was one of ten people called. He and the other nine refused to give information. After conviction for contempt of Congress, Trumbo and the others were blacklisted from working in Hollywood by the heads of the major studios. In 1950, Dalton served 11 months in prison as punishment for the contempt conviction, in the federal penitentiary in Ashland, Kentucky.
After Trumbo and the others were blacklisted, some Hollywood actors and directors, such as Elia Kazan and Clifford Odets, agreed to testify and to provide names of Communist party members to Congress.
Trumbo said in a speech given in 1970 that there was blame on all sides:
There was bad faith and good, honesty and dishonesty, courage and cowardice, selflessness and opportunism, wisdom and stupidity, good and bad on both sides; and almost every individual involved, no matter where he stood, combined some or all of these antithetical qualities in his own person, in his own acts.
Later life.
After completing his sentence, Trumbo could not get work in California, so he sold his ranch and his family moved to Mexico City with Hugo Butler and his wife Jean Rouverol, who had also been blacklisted. He recalled earning $1750 average fee for 18 screenplays in two years and said, "None was very good." In Mexico he wrote 30 scripts under pseudonyms. In the case of "Gun Crazy" (1950), based on a short story by MacKinlay Kantor, the author was the front for Trumbo's screenplay. It was not until 1992 that Trumbo's role was revealed.
Gradually the blacklist began to be weakened. With the support of Otto Preminger, Trumbo was credited for his screenplay for the 1960 film "Exodus", adapted from the novel by Leon Uris. Shortly thereafter, Kirk Douglas made public Trumbo's credit for the screenplay for "Spartacus" (1960), an event which has been cited as the beginning of the end of the blacklist. Trumbo was reinstated in the Writers Guild of America, West, and was credited on all subsequent scripts, including assigning full credit in 2011 for the script of Roman Holiday.
In 1971, Trumbo directed the film adaptation of his novel "Johnny Got His Gun", which starred Timothy Bottoms, Diane Varsi, Jason Robards and Donald Sutherland.
One of Trumbo's last films, "Executive Action" (1973), was based on various conspiracy theories about the Kennedy assassination.
"The Devil in the Book" (1956) was Trumbo's analysis about the conviction of fourteen California Smith Act defendants; the statute set criminal penalties for advocating the overthrow of the U.S. government and required all non-citizen adult residents to register with the government.
His son Christopher Trumbo wrote a play based on his letters during the period of the blacklist, entitled "Red, White and Blacklisted" (2003), produced in New York in 2003. He adapted it as a film, adding material from documentary footage, "Trumbo" (2007).
Academy Awards.
Trumbo won an Oscar for "The Brave One" (1956), written under the name Robert Rich. In 1975, the Academy officially recognized Trumbo as the winner and presented him with a statuette.
In 1993, Trumbo was posthumously awarded the Academy Award for writing "Roman Holiday" (1953). The screen credit and award were previously given to Ian McLellan Hunter, who had been a "front" for Trumbo.
Personal life.
In 1939, Trumbo married Cleo Fincher. She was born in Fresno on July 17, 1916, and later moved with her divorced mother and her brother and sister to Los Angeles. Cleo Trumbo died of natural causes at the age of 93 on October 9, 2009, in the Bay Area city of Los Altos. At the time she was living with her eldest daughter Mitzi.
They had three children: the filmmaker and screenwriter Christopher Trumbo, who became an expert on the Hollywood blacklist; Melissa, known as Mitzi, a photographer; and Nikola Trumbo, a psychotherapist.
His daughter Mitzi dated comedian Steve Martin when they were both in their early 20s, which is recounted in Steve Martin's 2007 book "Born Standing Up". Many of Martin's early promotional photographs were taken by Ms. Trumbo.
Death.
Dalton Trumbo died in Los Angeles of a heart attack at the age of 70 on September 10, 1976. He donated his body to science.
Depictions.
In 2003, Christopher Trumbo mounted a Broadway play based on his father's letters called "Trumbo: Red, White and Blacklisted," in which a wide variety of actors played his father during the run, including Nathan Lane, Tim Robbins, Brian Dennehy, Ed Harris, Chris Cooper and Gore Vidal. A documentary about Dalton Trumbo called "Trumbo" was produced in 2007 incorporating elements of the play as well as footage of Dalton Trumbo and a panoply of interviews.
In September 2013, it was announced that actor Bryan Cranston would be playing Trumbo in a 2015 biopic directed by Jay Roach.

</doc>
<doc id="7930" url="http://en.wikipedia.org/wiki?curid=7930" title="Delaware">
Delaware

Delaware () is a U.S. state located on the Atlantic Coast in the Mid-Atlantic region of the United States. It is bordered to the south and west by Maryland, to the northeast by New Jersey, and to the north by Pennsylvania. The state takes its name from Thomas West, 3rd Baron De La Warr, an English nobleman and Virginia's first colonial governor, after whom what is now called Cape Henlopen was originally named.
Delaware is located in the northeastern portion of the Delmarva Peninsula and is the second smallest, the sixth least populous, but the sixth most densely populated of the 50 United States. Delaware is divided into three counties. Delaware has the fewest number of counties of any state. From north to south, the three counties are New Castle, Kent, and Sussex. While the southern two counties have historically been predominantly agricultural, New Castle County has been more industrialized. 
Before its coastline was explored by Europeans in the 16th century, Delaware was inhabited by several groups of American Indians, including the Lenape in the north and Nanticoke in the south. It was initially colonized by Dutch traders at Zwaanendael, located near the present town of Lewes, in 1631. Delaware was one of the 13 colonies participating in the American Revolution and on December 7, 1787, became the first state to ratify the Constitution of the United States, thereby becoming known as "The First State".
Etymology.
The state was named after the Delaware River which in turn derived its name from Thomas West, 3rd Baron De La Warr (1577–1618), the governor of the Colony of Virginia at the time the river was first explored by Europeans. The Delaware Indians, a name used by Europeans for Lenape people indigenous to the Delaware Valley, also derive their name from the same source.
Geography.
Delaware is long and ranges from to across, totaling , making it the second-smallest state in the United States after Rhode Island. Delaware is bounded to the north by Pennsylvania; to the east by the Delaware River, Delaware Bay, New Jersey and the Atlantic Ocean; and to the west and south by Maryland. Small portions of Delaware are also situated on the eastern side of the Delaware River sharing land boundaries with New Jersey. The state of Delaware, together with the Eastern Shore counties of Maryland and two counties of Virginia, form the Delmarva Peninsula, which stretches down the Mid-Atlantic Coast.
The definition of the northern boundary of the state is unusual. Most of the boundary between Delaware and Pennsylvania was originally defined by an arc extending from the cupola of the courthouse in the city of New Castle. This boundary is often referred to as the Twelve-Mile Circle. This is the only nominally circular state boundary in the United States.
This border extends all the way east to the low-tide mark on the New Jersey shore, then continues south along the shoreline until it again reaches the 12-mile (19 km) arc in the south; then the boundary continues in a more conventional way in the middle of the main channel (thalweg) of the Delaware River. To the west, a portion of the arc extends past the easternmost edge of Maryland. The remaining western border runs slightly east of due south from its intersection with the arc. The Wedge of land between the northwest part of the arc and the Maryland border was claimed by both Delaware and Pennsylvania until 1921, when Delaware's claim was confirmed.
Topography.
Delaware is on a level plain, with the lowest mean elevation of any state in the nation. Its highest elevation, located at Ebright Azimuth, near Concord High School, does not rise fully above sea level. The northernmost part of the state is part of the Piedmont Plateau with hills and rolling surfaces. The Atlantic Seaboard fall line approximately follows the Robert Kirkwood Highway between Newark and Wilmington; south of this road is the Atlantic Coastal Plain with flat, sandy, and, in some parts, swampy ground. A ridge about in elevation extends along the western boundary of the state and separates the watersheds that feed Delaware River and Bay to the east and the Chesapeake Bay to the west.
Climate.
Since almost all of Delaware is a part of the Atlantic Coastal Plain, the effects of the ocean moderate its climate. The state is in a transitional zone between a humid subtropical climate and a continental climate. Despite its small size (roughly from its northernmost to southernmost points), there is significant variation in mean temperature and amount of snowfall between Sussex County and New Castle County. Moderated by the Atlantic Ocean and Delaware Bay, the southern portion of the state has a milder climate and a longer growing season than the northern portion of the state. Delaware's all time record high of was recorded at Millsboro on July 21, 1930; the all time record low of was also recorded at Millsboro on January 17, 1893.
Environment.
The transitional climate of Delaware supports a wide variety of vegetation. In the northern third of the state are found Northeastern coastal forests and mixed oak forests typical of the northeastern United States. In the southern two-thirds of the state are found Middle Atlantic coastal forests. Trap Pond State Park in Sussex County, for example, supports what may be one of the northernmost stands of bald cypress.
Environmental management.
Delaware provides government subsidy support for the clean-up of property "lightly contaminated" by hazardous waste, the proceeds for which come from a tax on wholesale petroleum sales.
History.
Native Americans.
Before Delaware was settled by European colonists, the area was home to the Eastern Algonquian tribes known as the Unami Lenape or Delaware throughout the Delaware valley, and the Nanticoke along the rivers leading into the Chesapeake Bay. The Unami Lenape in the Delaware Valley were closely related to Munsee Lenape tribes along the Hudson River. They had a settled hunting and agricultural society, and they rapidly became middlemen in an increasingly frantic fur trade with their ancient enemy, the Minqua or Susquehannock. With the loss of their lands on the Delaware River and the destruction of the Minqua by the Iroquois of the Five Nations in the 1670s, the remnants of the Lenape who wished to remain identified as such left the region and moved over the Alleghany Mountains by the mid-18th century. Generally, those who did not relocate out of the State of Delaware were baptized, became Christian and were grouped together with other persons of color in official records and in the minds of their non-Native American neighbors.
Colonial Delaware.
The Dutch were the first Europeans to settle in present-day Delaware in the Middle region by establishing a trading post at Zwaanendael, near the site of Lewes in 1631. Within a year all the settlers were killed in a dispute with area Native American Tribes. In 1638 New Sweden, a Swedish trading post and colony, was established at Fort Christina (now in Wilmington) by Peter Minuit at the head of a group of Swedes, Finns and Dutch. The colony of New Sweden lasted for 17 years. In 1651, the Dutch, reinvigorated by the leadership of Peter Stuyvesant, established a fort at present-day New Castle, and in 1655 they conquered the New Sweden colony, annexing it into the Dutch New Netherland. Only nine years later, in 1664, the Dutch were conquered by a fleet of English ships by Sir Robert Carr under the direction of James, the Duke of York. Fighting off a prior claim by Cecilius Calvert, 2nd Baron Baltimore, Proprietor of Maryland, the Duke passed his somewhat dubious ownership on to William Penn in 1682. Penn strongly desired access to the sea for his Pennsylvania province and leased what then came to be known as the "Lower Counties on the Delaware" from the Duke.
Penn established representative government and briefly combined his two possessions under one General Assembly in 1682. However, by 1704 the Province of Pennsylvania had grown so large that their representatives wanted to make decisions without the assent of the Lower Counties and the two groups of representatives began meeting on their own, one at Philadelphia, and the other at New Castle. Penn and his heirs remained proprietors of both and always appointed the same person Governor for their Province of Pennsylvania and their territory of the Lower Counties. The fact that Delaware and Pennsylvania shared the same governor was not unique. From 1703 to 1738, New York and New Jersey shared a governor. Massachusetts and New Hampshire also shared a governor for some time.
Dependent in early years on indentured labor, Delaware imported more slaves as the number of English immigrants decreased with better economic conditions in England. The colony became a slave society and cultivated tobacco as a cash crop, although English immigrants continued to arrive.
American Revolution.
Like the other middle colonies, the Lower Counties on the Delaware initially showed little enthusiasm for a break with Britain. The citizenry had a good relationship with the Proprietary government, and generally were allowed more independence of action in their Colonial Assembly than in other colonies. Merchants at the port of Wilmington had trading ties with the British.
So it was that New Castle lawyer Thomas McKean denounced the Stamp Act in the strongest terms, and Kent County native John Dickinson became the "Penman of the Revolution." Anticipating the Declaration of Independence, Patriot leaders Thomas McKean and Caesar Rodney convinced the Colonial Assembly to declare itself separated from British and Pennsylvania rule on June 15, 1776. The person best representing Delaware's majority, George Read, could not bring himself to vote for a Declaration of Independence. Only the dramatic overnight ride of Caesar Rodney gave the delegation the votes needed to cast Delaware's vote for independence.
Initially led by John Haslet, Delaware provided one of the premier regiments in the Continental Army, known as the "Delaware Blues" and nicknamed the "Blue Hen's Chicks." In August 1777, General Sir William Howe led a British army through Delaware on his way to a victory at the Battle of Brandywine and capture of the city of Philadelphia. The only real engagement on Delaware soil was the Battle of Cooch's Bridge, fought on September 3, 1777, at Cooch's Bridge in New Castle County.
Following the Battle of Brandywine, Wilmington was occupied by the British, and State President John McKinly was taken prisoner. The British remained in control of the Delaware River for much of the rest of the war, disrupting commerce and providing encouragement to an active Loyalist portion of the population, particularly in Sussex County. Because the British promised slaves of rebels freedom for fighting with them, escaped slaves flocked north to join their lines.
Following the American Revolution, statesmen from Delaware were among the leading proponents of a strong central United States with equal representation for each state.
Slavery and race.
Many colonial settlers came to Delaware from Maryland and Virginia, which had been experiencing a population boom. The economies of these colonies were chiefly based on tobacco culture and were increasingly dependent on slave labor for its intensive cultivation. Most of the English colonists arrived as indentured servants, hiring themselves out as laborers for a fixed period to pay for their passage. In the early years the line between indentured servants and African slaves or laborers was fluid. Most of the free African-American families in Delaware before the Revolution had migrated from Maryland to find more affordable land. They were descendants chiefly of relationships or marriages between servant women and enslaved, servant or free African or African-American men. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, more slaves were imported for labor.
At the end of the colonial period, the number of enslaved people in Delaware began to decline. Shifts in the agriculture economy from tobacco to mixed farming created less need for slaves' labor. Local Methodists and Quakers encouraged slaveholders to free their slaves following the American Revolution, and many did so in a surge of individual manumissions for idealistic reasons. By 1810 three-quarters of all blacks in Delaware were free. When John Dickinson freed his slaves in 1777, he was Delaware's largest slave owner with 37 slaves. By 1860, the largest slaveholder owned only 16 slaves.
Although attempts to abolish slavery failed by narrow margins in the legislature, in practical terms, the state had mostly ended the practice. By the 1860 census on the verge of the Civil War, 91.7 percent of the black population were free; 1,798 were slaves, as compared to 19,829 "free colored persons".
The independent black denomination was chartered by freed slave Peter Spencer in 1813 as the "Union Church of Africans". This followed the 1793 establishment of the African Methodist Episcopal Church in Philadelphia, which had ties to the Methodist Episcopal Church until 1816. Spencer built a church in Wilmington for the new denomination.
This was renamed the African Union First Colored Methodist Protestant Church and Connection, more commonly known as the A.U.M.P. Church. Begun by Spencer in 1814, the annual gathering of the Big August Quarterly still draws people together in a religious and cultural festival, the oldest such cultural festival in the nation.
Delaware voted against secession on January 3, 1861 and so remained in the Union. While most Delaware citizens who fought in the war served in the regiments of the state, some served in companies on the Confederate side in Maryland and Virginia Regiments. Delaware is notable for being the only slave state from which no Confederate regiments or militia groups were assembled. Delaware essentially freed the few slaves that were still in bondage shortly after the Civil War, but rejected the 13th, 14th, and 15th Amendments to the Constitution; the 13th Amendment was rejected on February 8, 1865, the 14th Amendment was rejected on February 8, 1867, and the 15th Amendment was rejected on March 18, 1869. Delaware officially ratified the 13th, 14th, and 15th amendments on February 12, 1901.
Demographics.
The United States Census Bureau estimates that the population of Delaware was 925,749 on July 1, 2013, a 3.1% increase since the 2010 United States Census.
Race and ancestry.
According to the 2010 United States Census, Delaware had a population of 897,934. The racial composition of the state was:
Ethnically, Hispanics and Latinos of any race made up 8.2% of the population.
Delaware is the sixth most densely populated state, with a population density of 442.6 people per square mile, 356.4 per square mile more than the national average, and ranking 45th in population. Delaware is one of five states that do not have a single city with a population over 100,000 as of the 2010 census, the other four being West Virginia, Vermont, Maine and Wyoming. The center of population of Delaware is located in New Castle County, in the town of Townsend.
As of 2011, 49.7% of Delaware's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry).
The largest ancestry groups in Delaware are, according to 2012 Census Bureau estimates:
Languages.
As of 2000, 91% of Delaware residents age 5 and older speak only English at home; 5% speak Spanish. French is the third most spoken language at 0.7%, followed by Chinese at 0.5% and German at 0.5%.
Legislation had been proposed in both the House and the Senate in Delaware to designate English as the official language. Neither bill was passed in the legislature.
Religion.
The religious affiliations of the people of Delaware are:
As of the year 2010, The Association of Religion Data Archives reported that the three largest denominational groups in Delaware by number of adherents are the Catholic Church at 182,532 adherents, the United Methodist Church with 53,656 members reported, and non-denominational Evangelical Protestant with 22,973 adherents reported. The religious body with the largest number of congregations is the United Methodist Church (with 158 congregations) followed by non-denominational Evangelical Protestant (with 106 congregations), then the Catholic Church (with 45 congregations).
The Roman Catholic Diocese of Wilmington and the Episcopal Diocese of Delaware oversee the parishes within their denominations. The A.U.M.P. Church, the oldest African-American denomination in the nation, was founded in Wilmington. It still has a substantial presence in the state. Reflecting new immigrant populations, an Islamic mosque has been built in the Ogletown area, and a Hindu temple in Hockessin.
A 2012 survey of religious attitudes in the United States found that 34% of Delaware residents considered themselves "moderately religious," 33% "very religious," and 33% as "non-religious."
Sexual orientation.
A 2012 poll by Gallup found that Delaware's proportion of lesbian, gay, bisexual, and transgender adults stood at 3.4 per cent of the population. This constitutes a total LGBT adult population estimate of 23,698 individuals. The number of same-sex couple households in 2010 stood at 2,646. This grew by 41.65% from a decade earlier. In July 1, 2013, same-sex marriage was legalized, and all civil unions would be converted into marriages.
Economy.
For the years of 2007 through 2010, the gross state product of Delaware was in the range, having risen from US$40 billion in 2000.
Affluence.
The per capita personal income was $34,199, ranking 9th in the nation. In 2005, the average weekly wage was $937, ranking 7th in the nation.
In common with many counties in the United States, each of the three Delaware counties have experienced a year-on-year decreasing in the sales price of new and existing homes when comparing 2010 to 2011.
According to a 2013 study by Phoenix Marketing International, Delaware had the ninth-largest number of millionaires per capita in the United States, with a ratio of 6.20 percent.
Agriculture.
Delaware's agricultural output consists of poultry, nursery stock, soybeans, dairy products and corn.
Industries.
As of January 2011, the state's unemployment rate was 8.5%. The state's largest employers are:
The Dover Air Force Base, located next to the state capital of Dover, is one of the largest Air Force bases in the country and is a major employer in Delaware. In addition to its other responsibilities in the United States Air Force Air Mobility Command, this air base serves as the entry point and mortuary for American military personnel and some U.S. government civilians who die overseas.
Incorporation in Delaware.
More than 50% of all U.S. publicly traded companies and 63% of the Fortune 500 are incorporated in Delaware. The state's attractiveness as a corporate haven is largely because of its business-friendly corporation law. Franchise taxes on Delaware corporations supply about one-fifth of its state revenue. Although "USA (Delaware)" ranked as the world's most opaque jurisdiction on the Tax Justice Network's 2009 Financial Secrecy Index, the same group's 2011 Index ranks the USA fifth and does not specify Delaware.
Food and drink.
 stipulates that alcoholic liquor only be sold in specifically licensed establishments, and only between 9:00 am and 1:00 am. Until 2003, Delaware was among the several states enforcing blue laws and banned sale of liquor on Sunday.
Transportation.
The transportation system in Delaware is under the governance and supervision of the Delaware Department of Transportation, also known as "DelDOT". Funding for DelDOT projects is drawn, in part, from the Delaware Transportation Trust Fund, established in 1987 to help stabilize transportation funding; the availability of the Trust led to a gradual separation of DelDOT operations from other Delaware state operations. DelDOT manages programs such as a Delaware Adopt-a-Highway program, major road route snow removal, traffic control infrastructure (signs and signals), toll road management, Delaware Division of Motor Vehicles, the Delaware Transit Corporation (branded as "DART First State", the state government public transportation organization), among others. In 2009, DelDOT maintained 13,507 lane miles of roads, totaling 89 percent of the state's public roadway system; the remaining public road miles are under the supervision of individual municipalities. This far exceeds the United States national average of 20 percent for state department of transportation maintenance responsibility.
The "DART First State" public transportation system was named "Most Outstanding Public Transportation System" in 2003 by the American Public Transportation Association. Coverage of the system is broad within northern New Castle County with close association to major highways in Kent and Sussex counties. The system includes bus, subsidized passenger rail operated by Philadelphia transit agency SEPTA, and subsidized taxi and paratransit modes. The paratransit system, consisting of a state-wide door-to-door bus service for the elderly and disabled, has been described by a Delaware state report as "the most generous paratransit system in the United States." , fees for the paratransit service have not changed since 1988.
Roads.
One major branch of the U.S. Interstate Highway System, Interstate 95 (I-95), crosses Delaware southwest-to-northeast across New Castle County. In addition to I-95, there are six U.S. highways that serve Delaware: U.S. Route 9 (US 9), US 13, US 40, US 113, US 202, and US 301. There are also several state highways that cross the state of Delaware; a few of them include Delaware Route 1 (DE 1), DE 9, and DE 404. US 13 and DE 1 are primary north-south highways connecting Wilmington and Pennsylvania with Maryland, with DE 1 serving as the main route between Wilmington and the Delaware beaches. DE 9 is a north-south highway connecting Dover and Wilmington via a scenic route along the Delaware Bay. US 40, is a primary east-west route, connecting Maryland with New Jersey. DE 404 is another primary east-west highway connecting the Chesapeake Bay Bridge in Maryland with the Delaware beaches. The state also operates two toll highways, the Delaware Turnpike, which is I-95, between Maryland and New Castle and the Korean War Veterans Memorial Highway, which is DE 1, between Wilmington and Dover.
A bicycle route, Delaware Bicycle Route 1, spans the north-south length of the state from the Maryland border in Fenwick Island to the Pennsylvania border north of Montchanin. It is the first of several signed bike routes planned in Delaware.
Delaware has around 1,450 bridges, 95 percent of which are under the supervision of DelDOT. About 30 percent of all Delaware bridges were built prior to 1950 and about 60 percent of the number are included in the National Bridge Inventory. Some bridges not under DelDOT supervision includes the four bridges on the Chesapeake and Delaware Canal, which are under the jurisdiction of the U.S. Army Corps of Engineers, and the Delaware Memorial Bridge, which is under the bi-state Delaware River and Bay Authority.
It has been noted that the tar and chip composition of secondary roads in Sussex County make them more prone to deterioration than asphalt roadways found in the rest of the state. Among these roads, Sussex (county road) 238 is among the most problematic.
Ferries.
There are three ferries that operate in the state of Delaware:
Rail and bus.
Amtrak has two stations in Delaware along the Northeast Corridor; the relatively quiet Newark Rail Station in Newark, and the busier Wilmington Rail Station in Wilmington. The Northeast Corridor is also served by SEPTA's Wilmington/Newark Line of Regional Rail, which serves Claymont, Wilmington, Churchmans Crossing, and Newark. The major freight railroad in Delaware is the Class I railroad Norfolk Southern, which provides service to most of Delaware. It connects with two shortline railroads, the Delaware Coast Line Railroad and the Maryland and Delaware Railroad, which serve local customers in Sussex County. Another Class I railroad, CSX, passes through northern New Castle County parallel to the Amtrak Northeast Corridor. CSX connects with the freight/heritage operation, the Wilmington and Western Railroad, based in Wilmington and the East Penn Railroad, which operates a line from Wilmington to Coatesville, Pennsylvania.
The last north-south passenger train through the main part of Delaware was the Pennsylvania Railroad's "The Cavalier," which ended service from Philadelphia through the state's interior in 1956.
Air.
Wilmington-Philadelphia Regional Airport near Wilmington is currently served by commercial airline Frontier Airlines, providing service to various locations in the country. In the past, Skybus Airlines also serviced in Wilmington, which provided service to Columbus, Ohio and Greensboro, North Carolina from March 7, 2008 until its bankruptcy on April 5, 2008.
Delaware is centrally situated in the Northeast Corridor region of cities along I-95. Therefore, Delaware commercial airline passengers most frequently use Philadelphia International Airport (PHL), Baltimore-Washington International Thurgood Marshall Airport (BWI) and Washington Dulles International Airport (IAD) for domestic and international transit. Residents of Sussex County will also use Wicomico Regional Airport (SBY), as it is located less than from the Delaware border. Newark Liberty International Airport (EWR) and Ronald Reagan Washington National Airport (DCA) are also within a radius of New Castle County.
The Dover Air Force Base of the Air Mobility Command is located in the central part of the state, and it is the home of the 436th Airlift Wing and the 512th Airlift Wing.
Other general aviation airports in Delaware include Summit Airport near Middletown, Delaware Airpark near Cheswold, and Sussex County Airport near Georgetown.
Law and government.
Delaware's fourth and current constitution, adopted in 1897, provides for executive, judicial and legislative branches.
Legislative branch.
The Delaware General Assembly consists of a House of Representatives with 41 members and a Senate with 21 members. It sits in Dover, the state capital. Representatives are elected to two-year terms, while senators are elected to four-year terms. The Senate confirms judicial and other nominees appointed by the governor.
Delaware's U.S. Senators are Tom Carper (Democrat) and Chris Coons (Democrat). Delaware's single U.S. Representative is John Carney (Democrat).
Judicial branch.
The Delaware Constitution establishes a number of courts:
Minor non-constitutional courts include the Justice of the Peace Courts and Aldermen's Courts.
Significantly, Delaware has one of the few remaining Courts of Chancery in the nation, which has jurisdiction over equity cases, the vast majority of which are corporate disputes, many relating to mergers and acquisitions. The Court of Chancery and the Supreme Court have developed a worldwide reputation for rendering concise opinions concerning corporate law which generally (but not always) grant broad discretion to corporate boards of directors and officers. In addition, the Delaware General Corporation Law, which forms the basis of the Courts' opinions, is widely regarded as giving great flexibility to corporations to manage their affairs. For these reasons, Delaware is considered to have the most business-friendly legal system in the United States; therefore a great number of companies are incorporated in Delaware, including 60% of the companies listed on the New York Stock Exchange. Delaware was the last US state to use judicial corporal punishment, in 1952.
Executive branch.
The executive branch is headed by the Governor of Delaware. The present governor is Jack Markell (Democrat), who took office January 20, 2009. The lieutenant governor is Matthew P. Denn. The governor presents a "State of the State" speech to a joint session of the Delaware legislature annually.
Counties.
Delaware is subdivided into three counties; from north to south they are New Castle, Kent County and Sussex. This is the fewest among all states. Each county elects its own legislative body (known in New Castle and Sussex counties as County Council, and in Kent County as Levy Court), which deal primarily in zoning and development issues. Most functions which are handled on a county-by-county basis in other states – such as court and law enforcement – have been centralized in Delaware, leading to a significant concentration of power in the Delaware state government. The counties were historically divided into hundreds, which were used as tax reporting and voting districts until the 1960s, but now serve no administrative role, their only current official legal use being in real-estate title descriptions.
Politics.
The Democratic Party holds a plurality of registrations in Delaware. Until the 2000 presidential election, the state tended to be a Presidential bellwether, sending its three electoral votes to the winning candidate since 1952. This trend ended in 2000, when Delaware's electoral votes went to Al Gore; in 2004, John Kerry won Delaware by eight percentage points. In 2008, Democrat Barack Obama defeated Republican John McCain in Delaware 62.63% to 37.37%. Obama's running mate was Joe Biden, who had represented Delaware in the United States Senate since 1973.
Delaware's swing to the Democrats is in part due to a strong Democratic trend in New Castle County, home to 55 percent of Delaware's population—more than the populations of Kent and Sussex counties combined (the two smaller counties have only 359,000 people between them to New Castle's 535,000). New Castle has not gone Republican in a presidential election since 1988. In 1992, 2000 and 2004, the Republican presidential candidate carried both Kent and Sussex but lost by double-digits each time in New Castle, which was a large enough margin to swing the state to the Democrats. New Castle also elects a substantial majority of the legislature; 27 of the 41 state house districts and 14 of the 21 state senate districts are based in New Castle.
The Democrats have held the governorship since 1993, having won the last six gubernatorial elections in a row. Democrats presently hold eight of the nine statewide elected offices, while the Republicans hold only one statewide office, State Auditor.
Freedom of information.
Each of the 50 states of the United States has passed some form of freedom of information legislation, which provides a mechanism for the general public to request information of the government. In 2011, Delaware passed legislation placing a 15 business day time limit on addressing freedom-of-information requests, to either produce information or an explanation of why such information would take longer than this time to produce.
Government revenue.
Delaware has six different income tax brackets, ranging from 2.2% to 5.95%. The state does not assess sales tax on consumers. The state does, however, impose a tax on the gross receipts of most businesses. Business and occupational license tax rates range from 0.096% to 1.92%, depending on the category of business activity.
Delaware does not assess a state-level tax on real or personal property. Real estate is subject to county property taxes, school district property taxes, vocational school district taxes, and, if located within an incorporated area, municipal property taxes.
Gambling provides significant revenue to the state. For instance, the casino at Delaware Park Racetrack provided more than $100 million USD to the state in 2010.
Municipalities.
Wilmington is the state's largest city and its economic hub. It is located within commuting distance of both Philadelphia and Baltimore. All regions of Delaware are enjoying phenomenal growth, with Dover and the beach resorts expanding at a rapid rate.
Ten wealthiest places in Delaware.
Ranked by per capita income
Education.
Delaware was the origin of "Belton v. Gebhart", one of the four cases which was combined into "Brown v. Board of Education", the Supreme Court of the United States decision that led to the end of segregated public schools. Significantly, "Belton" was the only case in which the state court found for the plaintiffs, thereby ruling that segregation was unconstitutional.
Unlike many states, Delaware's educational system is centralized in a state Superintendent of Education, with local school boards retaining control over taxation and some curriculum decisions.
, the Delaware Department of Education had authorized the founding of 25 charter schools in the state, among them one all-girls facility.
All teachers in the State's public school districts are unionized. , none of the State's charter schools are members of a teachers union. One of the State's teachers' unions is Delaware State Education Association (DSEA), which President as of January 2012 is Frederika Jenner.
Sister cities and states.
Delaware's sister state in Japan is Miyagi Prefecture.
Media.
Television.
The northern part of the state is served by network stations in Philadelphia and the southern part by network stations in Baltimore and Salisbury, Maryland. Philadelphia's ABC affiliate, WPVI-TV, maintains a news bureau in downtown Wilmington. Salisbury's ABC affiliate, WMDT covers Sussex and lower Kent County; while CBS affiliate, WBOC-TV, maintains bureaus in Dover and Milton.
Few television stations are based solely in Delaware; the local PBS station from Philadelphia (but licensed to Wilmington), WHYY-TV, maintains a studio and broadcasting facility in Wilmington and Dover, while Ion Television affiliate WPPX is licensed to Wilmington but maintains their offices in Philadelphia and their digital transmitter outside of that city and an analog tower in New Jersey.
In April 2014, it was revealed that Rehoboth Beach's WRDE-LD would affiliate with NBC, becoming the first major network-affiliated station in Delaware.
Tourism.
In addition to First State National Monument, Delaware has several museums, , , , , and other .
Rehoboth Beach, together with the towns of Lewes, Dewey Beach, Bethany Beach, South Bethany, and Fenwick Island, comprise Delaware's beach resorts. Rehoboth Beach often bills itself as "The Nation's Summer Capital" because it is a frequent summer vacation destination for Washington, D.C. residents as well as visitors from Maryland, Virginia, and in lesser numbers, Pennsylvania. Vacationers are drawn for many reasons, including the town's charm, artistic appeal, nightlife, and tax free shopping.
Delaware is home to several festivals, fairs, and events. Some of the more notable festivals are the Riverfest held in Seaford, the World Championship Punkin Chunkin held at various locations throughout the county since 1986, the Rehoboth Beach Chocolate Festival, the Bethany Beach Jazz Funeral to mark the end of summer, the Apple Scrapple Festival held in Bridgeville, the Clifford Brown Jazz Festival in Wilmington, the Rehoboth Beach Jazz Festival, the Sea Witch Halloween Festival and Parade in Rehoboth Beach, the Rehoboth Beach Independent Film Festival, the Nanticoke Indian Pow Wow in Oak Orchard, Firefly Music Festival, and the Return Day Parade held after every election in Georgetown.
Culture and entertainment.
Sports.
As Delaware has no franchises in the major American professional sports leagues, many Delawareans follow either Philadelphia or Baltimore teams, depending on their location within the state. The University of Delaware's football team has a large following throughout the state with the Delaware State University and Wesley College teams also enjoying a smaller degree of support.
Delaware is home to Dover International Speedway and Dover Downs. DIS, also known as the "Monster Mile", hosts two NASCAR races each year. Dover Downs is a popular harness racing facility. It is the only co-located horse and car-racing facility in the nation, with the Dover Downs track located inside the DIS track.
Delaware has been home to professional wrestling outfit Combat Zone Wrestling (CZW). CZW has been affiliated with the annual Tournament of Death and ECWA with its annual Super 8 Tournament.
Delaware Native Americans.
Delaware is also the name of a Native American group (called in their own language Lenni Lenape) that was influential in the colonial period of the United States and is today headquartered in Cheswold, Kent County, Delaware. A band of the Nanticoke tribe of American Indians today resides in Sussex County and is headquartered in Millsboro, Sussex County, Delaware.

</doc>
<doc id="7931" url="http://en.wikipedia.org/wiki?curid=7931" title="Dictionary">
Dictionary

A dictionary is collection of words in one or more specific languages, often listed alphabetically (or by radical and stroke for ideographic languages), with usage of information, definitions, etymologies, phonetics, pronunciations, translation, and other information; or a book of words in one language with their equivalents in another, also known as a lexicon. According to Nielsen (2008) a dictionary may be regarded as a lexicographical product that is characterised by three significant features: (1) it has been prepared for one or more functions; (2) it contains data that have been selected for the purpose of fulfilling those functions; and (3) its lexicographic structures link and establish relationships between the data so that they can meet the needs of users and fulfill the functions of the dictionary.
A broad distinction is made between general and specialized dictionaries. Specialized dictionaries do not contain information about words that are used in language for general purposes—words used by ordinary people in everyday situations. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that don't fit neatly in the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), or rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a monolingual general-purpose dictionary.
A different dimension on which dictionaries (usually just general-purpose ones) are sometimes distinguished is whether they are "prescriptive" or "descriptive", the latter being in theory largely based on linguistic corpus studies—this is the case of most modern dictionaries. However, this distinction cannot be upheld in the strictest sense. The choice of headwords is considered itself of prescriptive nature; for instance, dictionaries avoid having too many taboo words in that position. Stylistic indications (e.g. ‘informal’ or ‘vulgar’) present in many modern dictionaries is considered less than objectively descriptive as well.
Although the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of "astonishing" lack of method and critical-self reflection.
History.
The oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE "Urra=hubullu" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE "Erya", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a "dictionary", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary "Disorderly Words" (Ἄτακτοι γλῶσσαι, "") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the "Nihon Shoki", the first Japanese dictionary was the long-lost 682 CE "Niina" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE "Tenrei Banshō Meigi", was also a glossary of written Chinese. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compliled the Khaliq-e-bari which mainly dealt with Hindvi and Persian words.
Arabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the "Lisan al-`Arab" (13th century, still the best-known large-scale dictionary of Arabic) and "al-Qamus al-Muhit" (14th century) listed words in the alphabetical order of the radicals. The "Qamus al-Muhit" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the "Lisan" and the "Oxford English Dictionary".
In medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The "Catholicon" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's "Dictionarium" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the "Thesaurus linguae latinae" and in 1572 his son Henri Estienne published the "Thesaurus linguae graecae", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in a Romance language was Sebastián Covarrubias' "Tesoro de la lengua castellana o española", published in 1611 in Madrid. In 1612 the first edition of the "Vocabolario dell'Accademia della Crusca", for Italian, was published. It served as the model for similar works in French, Spanish and English. In 1690 in Rotterdam was published, posthumously, the "Dictionnaire Universel" by Antoine Furetière for French. In 1694 appeared the first edition of the "Dictionnaire de l'Académie française". Between 1712 and 1721 was published the "Vocabulario portughez e latino" written by Raphael Bluteau. The Real Academia Española published the first edition of the "Diccionario de la lengua española" in 1780, but their "Diccionario de Autoridades", which included quotes taken from literary works, was published in 1726. The "Totius Latinitatis lexicon" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.
The first edition of "A Greek-English Lexicon" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the "Dizionario della lingua italiana" by Niccolò Tommaseo. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the "Woordenboek der Nederlandsche Taal" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the "Explanatory Dictionary of the Living Great Russian Language". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. In 1898 was printed the first volume of the "Svenska Akademiens ordbok", whose publication is still in progress.
English Dictionaries.
The earliest dictionaries in the English language were glossaries of French, Italian or Latin words along with definitions of the foreign words in English. Of note, the word "dictionary" was invented by an Englishman called John of Garland in 1220 - he had written a book "Dictionarius" to help with Latin "diction". An early non-alphabetical list of 8000 English words was the "Elementarie" created by Richard Mulcaster in 1582.
The first purely English alphabetical dictionary was "A Table Alphabeticall", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. Yet this early effort, as well as the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is "a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title." 
It was not until Samuel Johnson's "A Dictionary of the English Language" (1755) that a truly noteworthy, reliable English Dictionary was deemed to have been produced, and the fact that today many people still mistakenly believe Johnson to have written the first English Dictionary is a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first 'modern' dictionary.
Johnson's "Dictionary" remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the "Oxford English Dictionary" in short fascicles from 1884 onwards. It took nearly 50 years to finally complete the huge work, and they finally released the complete "OED" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern day dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.
American English Dictionaries.
In 1806, American Noah Webster published his first dictionary, "". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, "An American Dictionary of the English Language;" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.
Webster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing "colour" with "color", substituting "wagon" for "waggon", and printing "center" instead of "centre". He also added American words, like "skunk" and "squash", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.
Austin (2005) explores the intersection of lexicographical and poetic practices in American literature, and attempts to map out a "lexical poetics" using Webster's definitions as his base. He explores how American poets used Webster's dictionaries, often drawing upon his lexicography in order to express their word play. Austin explicates key definitions from both the "Compendious" (1806) and "American" (1828) dictionaries, and brings into its discourse a range of concerns, including the politics of American English, the question of national identity and culture in the early moments of American independence, and the poetics of citation and of definition. Austin concludes that Webster's dictionaries helped redefine Americanism in an era of an emergent and unstable American political and cultural identity. Webster himself saw the dictionaries as a nationalizing device to separate America from Britain, calling his project a "federal language", with competing forces towards regularity on the one hand and innovation on the other. Austin suggests that the contradictions of Webster's lexicography were part of a larger play between liberty and order within American intellectual discourse, with some pulled toward Europe and the past, and others pulled toward America and the new future.
For an international appreciation of the importance of Webster's dictionaries in setting the norms of the English language, see Forque (1982).
Types.
In a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.
In many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the "New Oxford American Dictionary" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.
Specialized dictionaries.
According to the "Manual of Specialized Lexicographies" a specialized dictionary (also referred to as a technical dictionary) is a lexicon that focuses upon a specific subject field. Following the description in "The Bilingual LSP Dictionary" lexicographers categorize specialized dictionaries into three types. A multi-field dictionary broadly covers several subject fields (e.g., a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g., law), and a sub-field dictionary covers a singular field (e.g., constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the above coverage distinction between "minimizing dictionaries" and "maximizing dictionaries", multi-field dictionaries tend to minimize coverage across subject fields (for instance, "Oxford Dictionary of World Religions") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field ("The Oxford Dictionary of English Etymology").
Another variant is the glossary, an alphabetical list of defined terms in a specialised field, such as medicine (medical dictionary).
Defining dictionaries.
The simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.
Prescriptive vs. descriptive.
Lexicographers apply two basic philosophies to the defining of words: "prescriptive" or "descriptive". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling "color" while the rest of the English-speaking world prefers "colour". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)
Large 20th-century dictionaries such as the "Oxford English Dictionary" (OED) and "Webster's Third" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, add information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. "Merriam-Webster" is subtle, only adding italicized notations such as, "sometimes offensive" or "nonstand" (nonstandard). "American Heritage" goes further, discussing issues separately in numerous "usage notes." "Encarta" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, "an offensive term for..." or "a taboo term meaning...".
Because of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to "El otro, el mismo": "It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature."
Dictionaries for natural language processing.
In contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. (The final user is a human being but the direct user is a program.) Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex graph ( see Diathesis alternation ). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.
Pronunciation.
Dictionaries for languages for which the pronunciation of words is not apparent from their spelling, such as the English language, usually provide the pronunciation, often using the International Phonetic Alphabet. For example, the definition for the word "dictionary" might be followed by the phonemic spelling . American English dictionaries, however, often use their own pronunciation spelling systems, for example "dictionary" , while the IPA is more commonly used within the British Commonwealth countries. Yet others use a respelling system; for example, "dictionary" may respelled . Some on-line or electronic dictionaries provide recordings of words being spoken.
Examples.
Dictionaries of other languages.
Histories and descriptions of the dictionaries of other languages include:
Online dictionaries.
The age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that, "Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well."
There exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:

</doc>
<doc id="7935" url="http://en.wikipedia.org/wiki?curid=7935" title="David D. Friedman">
David D. Friedman

David Director Friedman (born February 12, 1945) is an economist, physicist, legal scholar, and libertarian theorist. He is known for his writings in market anarchist theory, which is the subject of his most popular book, "The Machinery of Freedom" (1973, revised 1989 and 2014). He has authored several other books and articles, including "Price Theory: An Intermediate Text" (1986), "Law's Order: What Economics Has to Do with Law and Why It Matters" (2000), "Hidden Order: The Economics of Everyday Life" (1996), and "Future Imperfect" (2008).
Life and work.
David Friedman is the son of economists Rose and Milton Friedman. His son, Patri Friedman, has also written about libertarian theory and market anarchism, particularly seasteading. He graduated "magna cum laude" from Harvard University in 1965, with a bachelor's degree in chemistry and physics. He later earned a master's (1967) and a Ph.D. (1971) in theoretical physics from the University of Chicago. He was previously a Markle Senior Fellow at New America Foundation from 2000 to 2002. He is currently a professor of law at Santa Clara University, and a contributing editor for "Liberty" magazine. He is an atheist.
The Machinery of Freedom.
In his book "The Machinery of Freedom" (1973), Friedman sketched a form of anarcho-capitalism where all goods and services including law itself can be produced by the free market. This differs from the version proposed by Murray Rothbard, where a legal code would first be consented to by the parties involved in setting up the anarcho-capitalist society. Friedman advocates an incrementalist approach to achieve anarcho-capitalism by gradual privatization of areas that government is involved in, ultimately privatizing law and order itself. In the book, he states his opposition to violent anarcho-capitalist revolution.
He advocates a consequentialist version of anarcho-capitalism. Friedman's version of individualist anarchism is not based on the assumption of inviolable natural rights but rather rests on a cost-benefit analysis of state versus no state. It is contrasted with the natural-rights approach as propounded most notably by economist and libertarian theorist Murray Rothbard.
Non-academic interests.
Friedman is a longtime member of the Society for Creative Anachronism, where he is known as "Duke Cariadoc of the Bow". He is known throughout the worldwide society for his articles on the philosophy of recreationism and practical historical recreations, especially those relating to the medieval Middle East. His work is compiled in the popular "Cariadoc's Miscellany". He is sometimes credited with founding the largest and longest-running SCA event, the Pennsic War; as king of the Middle Kingdom he challenged the East Kingdom, and later as king of the East accepted the challenge...and lost.
He is a long-time science fiction fan, and has written two fantasy novels, "Harald" (Baen Books, 2006) and "Salamander" (2011).

</doc>
<doc id="7938" url="http://en.wikipedia.org/wiki?curid=7938" title="Diatomic molecule">
Diatomic molecule

Diatomic molecules are molecules composed of only two atoms, of either the same or different chemical elements. The prefix "di-" is of Greek origin, meaning "two". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H2) or oxygen (O2), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear.
Homonuclear molecules.
The only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H2), nitrogen (N2), oxygen (O2), fluorine (F2), and chlorine (Cl2).
The noble gases (helium, neon, argon, krypton, xenon, and radon) are also gases at STP, but they are monatomic. The homonuclear diatomic gases and noble gases together are called "elemental gases" or "molecular gases", to distinguish them from other gases that are chemical compounds.
At slightly elevated temperatures, the halogens bromine (Br2) and iodine (I2) also form diatomic gases. All halogens have been observed as diatomic molecules, except for astatine, which is uncertain.
Other elements form diatomic molecules when evaporated, but these diatomic species repolymerize when cooled. Heating ("cracking") elemental phosphorus gives diphosphorus, P2. Sulfur vapor is mostly disulfur (S2). Dilithium (Li2) is known in the gas phase. Ditungsten (W2) and dimolybdenum (Mo2) form with sextuple bonds in the gas phase. The bond in a homonuclear diatomic molecule is non-polar.
Heteronuclear molecules.
All other diatomic molecules are chemical compounds of two different elements. Many elements can combine to form heteronuclear diatomic molecules, depending on temperature and pressure.
Common examples include the gases carbon monoxide (CO), nitric oxide (NO), and hydrogen chloride (HCl).
Many 1:1 binary compounds are not normally considered diatomic because they are polymeric at room temperature, but they form diatomic molecules when evaporated, for example gaseous MgO, SiO, and many others.
Occurrence.
Hundreds of diatomic molecules have been identified in the environment of the Earth, in the laboratory, and in interstellar space. About 99% of the Earth's atmosphere is composed of two species of diatomic molecules: nitrogen (78%) and oxygen (21%). The natural abundance of hydrogen (H2) in the Earth's atmosphere is only of the order of parts per million, but H2 is the most abundant diatomic molecule in the universe. The interstellar medium is, indeed, dominated by hydrogen atoms.
Molecular geometry.
Diatomic molecules cannot have any geometry but linear, as any two points always lie in a straight line. This is the simplest spatial arrangement of atoms.
Historical significance.
Diatomic elements played an important role in the elucidation of the concepts of element, atom, and molecule in the 19th century, because some of the most common elements, such as hydrogen, oxygen, and nitrogen, occur as diatomic molecules. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed water's formula to be HO, giving the atomic weight of oxygen as eight times that of hydrogen, instead of the modern value of about 16. As a consequence, confusion existed regarding atomic weights and molecular formulas for about half a century.
As early as 1805, Gay-Lussac and von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen, and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the assumption of diatomic elemental molecules. However, these results were mostly ignored until 1860, partly due to the belief that atoms of one element would have no chemical affinity toward atoms of the same element, and partly due to apparent exceptions to Avogadro's law that were not explained until later in terms of dissociating molecules.
At the 1860 Karlsruhe Congress on atomic weights, Cannizzaro resurrected Avogadro's ideas and used them to produce a consistent table of atomic weights, which mostly agree with modern values. These weights were an important prerequisite for the discovery of the periodic law by Dmitri Mendeleev and Lothar Meyer.
Excited electronic states.
Diatomic molecules are normally in their lowest or ground state, which conventionally is also known as the formula_1 state. When a gas of diatomic molecules is bombarded by energetic electrons, some of the molecules may be excited to higher electronic states, as occurs, for example, in the natural aurora; high-altitude nuclear explosions; and rocket-borne electron gun experiments. Such excitation can also occur when the gas absorbs light or other electromagnetic radiation. The excited states are unstable and naturally relax back to the ground state. Over various short time scales after the excitation (typically a fraction of a second, or sometimes longer than a second if the excited state is metastable), transitions occur from higher to lower electronic states and ultimately to the ground state, and in each transition results a photon is emitted. This emission is known as fluorescence. Successively higher electronic states are conventionally named formula_2, formula_3, formula_4, etc. (but this convention is not always followed, and sometimes lower case letters and alphabetically out-of-sequence letters are used, as in the example given below). The excitation energy must be greater than or equal to the energy of the electronic state in order for the excitation to occur.
In quantum theory, an electronic state of a diatomic molecule is represented by
where formula_6 is the total electronic spin quantum number, formula_7 is the total electronic angular momentum quantum number along the internuclear axis, and formula_8 is the vibrational quantum number. formula_7 takes on values 0, 1, 2, …, which are represented by the electronic state symbols formula_10, formula_11, formula_12,….
For example, the following table lists the common electronic states (without vibrational quantum numbers) along with the energy of the lowest vibrational level (formula_13) of diatomic nitrogen (N2), the most abundant gas in the Earth's atmosphere. In the table, the subscripts and superscripts after formula_7 give additional quantum mechanical details about the electronic state.
Note: The "energy" units in the above table are actually the reciprocal of the wavelength of a photon emitted in a transition to the lowest energy state. The actual energy can be found by multiplying the given statistic by the product of "c" (the speed of light) and "h" (Planck's constant), i.e., about 1.99 × 10−25 Joule metres, and then multiplying by a further factor of 100 to convert from cm−1 to m−1.
The aforementioned fluorescence occurs in distinct regions of the electromagnetic spectrum, called "emission bands": each band corresponds to a particular transition from a higher electronic state and vibrational level to a lower electronic state and vibrational level (typically, many vibrational levels are involved in an excited gas of diatomic molecules). For example, N2 formula_2-formula_1 emission bands (a.k.a. Vegard-Kaplan bands) are present in the spectral range from 0.14 to 1.45 μm (micrometres). A given band can be spread out over several nanometers in electromagnetic wavelength space, owing to the various transitions that occur in the molecule's rotational quantum number, formula_17. These are classified into distinct sub-band branches, depending on the change in formula_17. The formula_19 branch corresponds to formula_20, the formula_21 branch to formula_22, and the formula_23 branch to formula_24. Bands are spread out even further by the limited spectral resolution of the spectrometer that is used to measure the spectrum. The spectral resolution depends on the instrument's point spread function.
Energy levels.
The molecular term symbol is a shorthand expression of the angular momenta that characterize the electronic quantum states of a diatomic molecule, which are eigenstates of the electronic molecular Hamiltonian. It is also convenient, and common, to represent a diatomic molecule as two point masses connected by a massless spring. The energies involved in the various motions of the molecule can then be broken down into three categories: the translational, rotational, and vibrational energies.
Translational energies.
The translational energy of the molecule is given by the kinetic energy expression:
where formula_26 is the mass of the molecule and formula_8 is its velocity.
Rotational energies.
Classically, the kinetic energy of rotation is
For microscopic, atomic-level systems like a molecule, angular momentum can only have specific discrete values given by
Also, for a diatomic molecule the moment of inertia is
So, substituting the angular momentum and moment of inertia into Erot, the rotational energy levels of a diatomic molecule are given by:
Vibrational energies.
Another type of motion of a diatomic molecule is for each atom to oscillate—or vibrate—along the line connecting the two atoms. The vibrational energy is approximately that of a quantum harmonic oscillator:
Comparison between rotational and vibrational energy spacings.
The spacing, and the energy of a typical spectroscopic transition, between vibrational energy levels is about 100 times greater than that of a typical transition between rotational energy levels.
Hund's cases.
The good quantum numbers for a diatomic molecule, as well as good approximations of rotational energy levels, can be obtained by modeling the molecule using Hund's cases.

</doc>
<doc id="7939" url="http://en.wikipedia.org/wiki?curid=7939" title="Duopoly">
Duopoly

A true duopoly (from Greek "duo" δύο (two) + "polein" πωλεῖν (to sell)) is a specific type of oligopoly where only two producers exist in one market. In reality, this definition is generally used where only two firms have dominant control over a market. In the field of industrial organization, it is the most commonly studied form of oligopoly due to its simplicity.
Duopoly models in economics.
There are two principal duopoly models, Cournot duopoly and Bertrand duopoly:
Politics.
Modern American politics, in particular the electoral college system has been described as duopolistic since the Republican and Democratic parties have dominated and framed policy debate as well as the public discourse on matters of national concern for about a century and a half. Third Parties have encountered various blocks in getting onto ballots at different levels of government as well as other electoral obstacles, more so in recent decades.
Examples in business.
The most commonly cited duopoly is that between Visa and Mastercard, who between them control a large proportion of the electronic payment processing market. In 2000 they were the defendants in a US Department of Justice antitrust lawsuit. An appeal was upheld in 2004.
Examples where two companies control a large proportion of a market are:
Media.
In Finland, the state-owned broadcasting company Yleisradio and the private broadcaster Mainos-TV had a legal duopoly (in the economists' sense of the word) from the 1950s to 1993. No other broadcasters were allowed. Mainos-TV operated by leasing air time from Yleisradio, broadcasting in reserved blocks between Yleisradio's own programming on its two channels. This was a unique phenomenon in the world. Between 1986 and 1992 there was an independent third channel but it was jointly owned by Yle and MTV; only in 1993 did MTV get its own channel.
Safaricom mobile service provider and Airtel in Kenya are perfect examples of Duopoly market in African telecommunication industry.
Broadcasting.
Duopoly is also used in the United States broadcast television and radio industry to refer to a single company owning two outlets in the same city.
This usage is technically incompatible with the normal definition of the word and leads to confusion, inasmuch as there are generally more than two owners of broadcast television stations in markets with broadcast duopolies. In Canada, this definition is therefore more commonly called a "twinstick".

</doc>
<doc id="7940" url="http://en.wikipedia.org/wiki?curid=7940" title="Dungeons &amp; Dragons">
Dungeons &amp; Dragons

Dungeons & Dragons (abbreviated as D&D or DnD) is a fantasy tabletop role-playing game (RPG) originally designed by Gary Gygax and Dave Arneson, and first published in 1974 by Tactical Studies Rules, Inc. (TSR). The game has been published by Wizards of the Coast (now a subsidiary of Hasbro) since 1997. It was derived from miniature wargames with a variation of the "Chainmail" game serving as the initial rule system. "D&D" publication is widely regarded as the beginning of modern role-playing games and the role-playing game industry.
"D&D" departs from traditional wargaming and assigns each player a specific character to play instead of a military formation. These characters embark upon imaginary adventures within a fantasy setting. A Dungeon Master serves as the game's referee and storyteller, while also maintaining the setting in which the adventures occur and playing the role of the inhabitants. The characters form a party that interacts with the setting's inhabitants (and each other). Together they solve dilemmas, engage in battles and gather treasure and knowledge. In the process the characters earn experience points to become increasingly powerful over a series of sessions.
The early success of "Dungeons & Dragons" led to a proliferation of similar game systems. Despite this competition, "D&D" remains the market leader in the role-playing game industry. In 1977, the game was split into two branches: the relatively rules-light game system of "Dungeons & Dragons" and the more structured, rules-heavy game system of "Advanced Dungeons & Dragons" (abbreviated as "AD&D" or "ADnD"). "AD&D" 2nd Edition was published in 1989. In 2000, the original line of the game was discontinued and the "AD&D" version was renamed "Dungeons & Dragons" with the release of its 3rd edition with a new system. These rules formed the basis of the d20 System which is available under the Open Game License (OGL) for use by other publishers. "Dungeons & Dragons" version 3.5 was released in June 2003, with a (non-OGL) 4th edition in June 2008. A 5th edition is scheduled for a staggered release during the second half of 2014.
, "Dungeons & Dragons" remained the best-known and best-selling role-playing game, with an estimated 20 million people having played the game and more than US$1 billion in book and equipment sales. The game has been supplemented by many pre-made adventures as well as commercial campaign settings suitable for use by regular gaming groups. "Dungeons & Dragons" is known beyond the game for other "D&D"-branded products, references in popular culture and some of the controversies that have surrounded it, particularly a moral panic in the 1980s falsely linking it to Satanism and suicide. The game has won multiple awards and has been translated into many languages beyond the original English.
Play overview.
"Dungeons & Dragons" is a structured yet open-ended role-playing game. It is normally played indoors with the participants seated around a tabletop. Typically, each player controls only a single character, which represents an individual in a fictional setting. When working together as a group, these player characters (PCs) are often described as a 'party' of adventurers, with each member often having his or her own areas of specialty that contributes to the success of the whole. During the course of play, each player directs the actions of his or her character and its interactions with other characters in the game. This activity is performed through the verbal impersonation of the characters by the players, while also employing a variety of social and other useful cognitive skills, such as logic, basic mathematics and imagination. A game often continues over a series of meetings to complete a single adventure, and longer into a series of related gaming adventures, called a 'campaign'.
The results of the party's choices and the overall storyline for the game are determined by the Dungeon Master (DM) according to the rules of the game and the DM's interpretation of those rules. The DM selects and describes the various non-player characters (NPCs) that the party encounters, the settings in which these interactions occur, and the outcomes of those encounters based on the players' choices and actions. Encounters often take the form of battles with 'monsters' – a generic term used in "D&D" to describe potentially hostile beings such as animals, aberrant beings, or mythical creatures. The game's extensive rules – which cover diverse subjects such as social interactions, magic use, combat, and the effect of the environment on PCs – help the DM to make these decisions. The DM may choose to deviate from the published rules or make up new ones if they feel it is necessary.
The most recent versions of the game's rules are detailed in three core rulebooks: The "Player's Handbook", the "Dungeon Master's Guide" and the "Monster Manual". A "Basic Game" boxed set contains abbreviated rules to help beginners learn the game.
The only items required to play the game are the rulebooks, a character sheet for each player and a number of polyhedral dice. The current editions also assume, but do not require, the use of miniature figures or markers on a gridded surface. Earlier editions did not make this assumption. Many optional accessories are available to enhance the game, such as expansion rulebooks, pre-designed adventures and various campaign settings.
Game mechanics.
Before the game begins, each player creates his or her player character and records the details (described below) on a character sheet. First, a player determines his or her character's ability scores, which consist of Strength, Constitution, Dexterity, Intelligence, Wisdom, and Charisma. Each edition of the game has offered differing methods of determining these statistics; as of 4th Edition, players generally assign their ability scores from a list or use points to "buy" them. The player then chooses a race (species) such as Human or Elf, a character class (occupation) such as Fighter or Wizard, an alignment (a moral and ethical outlook), and a number of powers, skills and feats to enhance the character's basic abilities. Additional background history, usually not covered by specific rules, is often also used to further develop the character.
During the game, players describe their PC's intended actions, such as punching an opponent or picking a lock, and converse with the DM in character – who then describes the result or response. Trivial actions, such as picking up a letter or opening an unlocked door, are usually automatically successful. The outcomes of more complex or risky actions are determined by rolling dice. Factors contributing to the outcome include the character's ability scores, skills and the difficulty of the task. In circumstances where a character does not have control of an event, such as when a trap or magical effect is triggered or a spell is cast, a saving throw can be used to determine whether the resulting damage is reduced or avoided. In this case the odds of success are influenced by the character's class, levels and (with the 3rd and later editions) ability scores.
As the game is played, each PC changes over time and generally increases in capability. Characters gain (or sometimes lose) experience, skills and wealth, and may even alter their alignment or add additional character classes. The key way characters progress is by earning experience points (XP/EXP), which happens when they defeat an enemy or accomplish a difficult task. Acquiring enough XP allows a PC to advance a level, which grants the character improved class features, abilities and skills. Up through the 3rd edition, XP can also be lost in some circumstances, such as encounters with creatures that drain life energy, or by use of certain magical powers that require payment of an XP cost.
Hit points (HP) are a measure of a character's vitality and health and are determined by the class, level and constitution of each character. They can be temporarily lost when a character sustains wounds in combat or otherwise comes to harm, and loss of HP is the most common way for a character to die in the game. Death can also result from the loss of key ability scores or character levels. When a PC dies, it is often possible for the dead character to be resurrected through magic, although some penalties may be imposed as a result. If resurrection is not possible or not desired, the player may instead create a new PC to resume playing the game.
Adventures, campaigns, and modules.
A typical "Dungeons & Dragons" game consists of an 'adventure', which is roughly equivalent to a single story. The DM can either design an adventure on his or her own, or follow one of the many additional pre-made adventures (previously known as "modules") that have been published throughout the history of "Dungeons & Dragons". Published adventures typically include a background story, illustrations, maps and goals for PCs to achieve. Some also include location descriptions and handouts. Although a small adventure entitled 'Temple of the Frog' was included in the "Blackmoor" rules supplement in 1975, the first stand-alone "D&D" module published by TSR was 1978's "Steading of the Hill Giant Chief", written by Gygax.
A linked series of adventures is commonly referred to as a 'campaign'. The locations where these adventures occur, such as a city, country, planet or an entire fictional universe, are also sometimes called 'campaigns' but are more correctly referred to as 'worlds' or 'campaign settings'. "D&D" settings are based in various fantasy subgenres and feature varying levels of magic and technology. Popular commercially published campaign settings for "Dungeons & Dragons" include Greyhawk, Dragonlance, Forgotten Realms, Mystara, Spelljammer, Ravenloft, Dark Sun, Planescape, Birthright, and Eberron. Alternatively, DMs may develop their own fictional worlds to use as campaign settings.
Miniature figures.
The wargames from which "Dungeons & Dragons" evolved used miniature figures to represent combatants. "D&D" initially continued the use of miniatures in a fashion similar to its direct precursors. The original "D&D" set of 1974 required the use of the "Chainmail" miniatures game for combat resolution. By the publication of the 1977 game editions, combat was mostly resolved verbally. Thus miniatures were no longer required for game play, although some players continued to use them as a visual reference.
In the 1970s, numerous companies began to sell miniature figures specifically for "Dungeons & Dragons" and similar games. Licensed miniature manufacturers who produced official figures include Grenadier Miniatures (1980–1983), Citadel Miniatures (1984–1986), Ral Partha, and TSR itself. Most of these miniatures used the 25 mm scale, with the exception of Ral Partha's 15 mm scale miniatures for the 1st edition Battlesystem.
Periodically, "Dungeons & Dragons" has returned to its wargaming roots with supplementary rules systems for miniatures-based wargaming. Supplements such as "Battlesystem" (1985 & 1989) and a new edition of "Chainmail" (2001) provided rule systems to handle battles between armies by using miniatures.
"Dungeons & Dragons" 3rd edition (2000) assumes the use of miniatures to represent combat situations in play, an aspect of the game that was further emphasized in the v3.5 revision. The "Dungeons & Dragons Miniatures Game" (2003) is sold as sets of plastic, randomly assorted, pre-painted miniatures, and can be used as either part of a standard "Dungeons & Dragons" game or as a stand-alone collectible miniatures game.
Game history.
Sources and influences.
An immediate predecessor of "Dungeons & Dragons" was a set of medieval miniature rules written by Jeff Perren. These were expanded by Gary Gygax, whose additions included a fantasy supplement, before the game was published as "Chainmail". When Dave Wesely entered the Army in 1970, his friend and fellow Napoleonics wargamer Dave Arneson began a medieval variation of Wesely's Braunstein games, where players control individuals instead of armies. Arneson used "Chainmail" to resolve combats. As play progressed, Arneson added such innovations as character classes, experience points, level advancement, armor class, and others. Having partnered previously with Gygax on "Don't Give Up the Ship!", Arneson introduced Gygax to his Blackmoor game and the two then collaborated on developing "The Fantasy Game", the role-playing game (RPG) that became "Dungeons & Dragons", with the final writing and preparation of the text being done by Gygax.
Many "Dungeons & Dragons" elements appear in hobbies of the mid-to-late 20th century. For example, character-based role playing can be seen in improvisational theatre. Game-world simulations were well developed in wargaming. Fantasy milieus specifically designed for gaming could be seen in Glorantha's board games among others. Ultimately, however, "Dungeons & Dragons" represents a unique blending of these elements.
The world of "D&D" was influenced by world mythology, history, pulp fiction, and contemporary fantasy novels. The importance of J.R.R. Tolkien's "The Lord of the Rings" and "The Hobbit" as an influence on "D&D" is controversial. The presence in the game of halflings, elves, half-elves, dwarves, orcs, rangers, and the like, draw comparisons to these works. The resemblance was even closer before the threat of copyright action from Tolkien Enterprises prompted the name changes of hobbit to 'halfling', ent to 'treant', and balrog to 'balor'. For many years Gygax played down the influence of Tolkien on the development of the game. However, in an interview in 2000, he acknowledged that Tolkien's work had a "strong impact".
The D&D magic system, in which wizards memorize spells that are used up once cast and must be re-memorized the next day, was heavily influenced by the "Dying Earth" stories and novels of Jack Vance. The original alignment system (which grouped all characters and creatures into 'Law', 'Neutrality' and 'Chaos') was derived from the novel "Three Hearts and Three Lions" by Poul Anderson. A troll described in this work also influenced the "D&D" definition of that monster.
Other influences include the works of Robert E. Howard, Edgar Rice Burroughs, A. Merritt, H. P. Lovecraft, Fritz Leiber, L. Sprague de Camp, Fletcher Pratt, Roger Zelazny, and Michael Moorcock. Monsters, spells, and magic items used in the game have been inspired by hundreds of individual works such as A. E. van Vogt's "Black Destroyer", Coeurl (the Displacer Beast), Lewis Carroll's "Jabberwocky" (vorpal sword) and the Book of Genesis (the clerical spell 'Blade Barrier' was inspired by the "flaming sword which turned every way" at the gates of Eden).
Edition history.
"Dungeons & Dragons" has gone through several revisions. Parallel versions and inconsistent naming practices can make it difficult to distinguish between the different editions.
The original "Dungeons & Dragons", now referred to as OD&D, was a small box set of three booklets published in 1974. It was amateurish in production and written from a perspective that assumed the reader was familiar with wargaming. Nevertheless it grew rapidly in popularity, first among wargamers and then expanding to a more general audience of college and high school students. Roughly 1,000 copies of the game were sold in the first year followed by 3,000 in 1975, with sales increasing thereafter. This first set went through many printings and was supplemented with several official additions, such as the original Greyhawk and Blackmoor supplements (both 1975), as well as magazine articles in TSR's official publications and countless fanzines.
Two-pronged strategy.
In 1977, TSR created the first element of a two-pronged strategy that would divide the "D&D" game for over two decades. A "Dungeons & Dragons Basic Set" boxed edition was introduced that cleaned up the presentation of the essential rules, made the system understandable to the general public, and was sold in a package that could be stocked in toy stores. In 1977, "Advanced Dungeons & Dragons" ("AD&D") was published, which brought together the various published rules, options and corrections, then expanded them into a definitive, unified game for hobbyist gamers. The basic set directed players who exhausted the possibilities of that game to switch to the advanced rules.
As a result of this dual marketing approach, the basic game included many rules and concepts which contradicted comparable ones in the advanced game. Gygax, who wrote the advanced game, wanted an expansive game with rulings on almost any conceivable situation which might come up during play. J. Eric Holmes, the editor of the basic game, preferred a lighter tone with more room for personal improvisation. Confusing matters further, the original "D&D" boxed set remained in publication until 1979, since it remained a healthy seller for TSR.
"Advanced Dungeons & Dragons" was designed to create a tighter, more structured game system than the loose framework of the original game. While seen by many as a revision of the original "D&D", "AD&D" was at the time declared to be "neither an expansion nor a revision of the old game, it is a new game". The "AD&D" game was not intended to be directly compatible with "D&D" and it required some conversion to play between the rule sets. The term "Advanced" described the more complex rules and did not imply "for higher-level gaming abilities". Between 1977 and 1979, three hardcover rulebooks, commonly referred to as the "core rulebooks", were released: the "Player's Handbook" (PHB), the "Dungeon Master's Guide" (DMG), and the "Monster Manual" (MM). Several supplementary books were published throughout the 1980s, notably "Unearthed Arcana" (1985) that included a large number of new rules.
Revised editions.
In the 1980s, the rules for "Advanced Dungeons & Dragons" and "basic" "Dungeons & Dragons" remained separate, each developing along different paths.
In 1981, the "basic" version of "Dungeons & Dragons" was revised by Tom Moldvay and split into several versions. This game was promoted as a continuation of the original "D&D" tone, whereas "AD&D" was promoted as advancement of the mechanics. Although simpler overall than the "Advanced" game, it included rules for some situations not covered in "AD&D". There were five sets: "Basic" (1977, revised in 1981 and again in 1983, 1991 and 1994), "Expert" (1981, revised in 1983), "Companion" (1983), "Master" (1985), and "Immortals" (1986, revised in 1991). Revisions of 1983 and expansions like Master Rules and Immortals were introduced by Frank Mentzer. Each set covered game play for more powerful characters than the previous. The first four sets were later compiled as a single hardcover book, the "Dungeons & Dragons Rules Cyclopedia" (1991).
"Advanced Dungeons & Dragons 2nd Edition", was published in 1989, again as three core rulebooks; the primary designer was David "Zeb" Cook. The "Monster Manual" was replaced by the "Monstrous Compendium", a loose-leaf binder that was subsequently replaced by the hardcover "Monstrous Manual" in 1993. In 1995, the core rulebooks were slightly revised, although still referred to by TSR as the 2nd Edition, and a series of "Player's Option" manuals were released as optional rulebooks.
The release of "AD&D2" deliberately excluded some aspects of the game that had attracted negative publicity. References to demons and devils, sexually suggestive artwork, and playable, evil-aligned character types – such as assassins and half-orcs – were removed. The edition moved away from a theme of 1960s and 1970s "sword and sorcery" fantasy fiction to a mixture of medieval history and mythology. The rules underwent minor changes, including the addition of non-weapon proficiencies – skill-like abilities that originally appeared in 1st Edition supplements. The game's magic spells were divided into schools and spheres. A major difference was the promotion of various game settings beyond that of traditional fantasy. This included blending fantasy with other genres, such as horror (Ravenloft), science fiction (Spelljammer), and apocalyptic (Dark Sun), as well as alternative historical and non-European mythological settings.
Wizards of the Coast.
In 1997, a near-bankrupt TSR was purchased by Wizards of the Coast. Following three years of development, "Dungeons & Dragons" 3rd edition was released in 2000. The new release folded the Basic and Advanced lines back into a single unified game. It was the largest revision of the "D&D" rules to date, and also served as the basis for a multi-genre role-playing system designed around 20-sided dice, called the d20 System. The 3rd Edition rules were designed to be internally consistent and less restrictive than previous editions of the game, allowing players more flexibility to create the characters they wanted to play. Skills and feats were introduced into the core rules to encourage further customization of characters. The new rules also standardized the mechanics of action resolution and combat.
In 2003, "Dungeons & Dragons v.3.5" was released as a revision of the 3rd Edition rules. This release incorporated hundreds of rule changes, mostly minor, and expanded the core rulebooks.
In early 2005, Wizards of the Coast's R&D team started to develop "Dungeons & Dragons" 4th Edition, prompted mainly by the feedback obtained from the "D&D" playing community and a desire to make the game faster, more intuitive, and with a better play experience than under the 3rd Edition. The new game was developed through a number of design phases spanning from May 2005 until its release.
"Dungeons & Dragons 4th Edition" was announced at Gen Con in August 2007, and the initial three core books were released June 6, 2008. 4th Edition streamlined the game into a simplified form and introduced numerous rules changes. Many character abilities were restructured into "Powers". These altered the spell-using classes by adding abilities that could be used at will, per encounter, or per day. Likewise, non-magic-using classes were provided with parallel sets of options. Wizards of the Coast is releasing other supplementary material virtually through their website, including player character and monster building programs.
On January 9, 2012, Wizards of the Coast announced that it was working on a 5th edition of the game. The company planned to take suggestions from players and let them playtest the rules. Public playtesting began on May 24, 2012. At Gen Con 2012 in August, Mike Mearls said that Wizards of the Coast had received feedback from more than 75,000 playtesters, but that the entire development process would take two years, adding, "I can't emphasize this enough ... we're very serious about taking the time we need to get this right." The staggered release of the 5th Edition, coinciding with "D&D"s 40th anniversary, is scheduled for the second half of 2014.
Acclaim and influence.
The game had more than 3 million players around the world by 1981, and copies of the rules were selling at a rate of about 750,000 per year by 1984. Beginning with a French language edition in 1982, "Dungeons & Dragons" has been translated into many languages beyond the original English. By 2004, consumers had spent more than US$1 billion on "Dungeons & Dragons" products and the game had been played by more than 20 million people. As many as 6 million people played the game in 2007.
The various editions of "Dungeons & Dragons" have won many Origins Awards, including "All Time Best Roleplaying Rules of 1977", "Best Roleplaying Rules of 1989", and "Best Roleplaying Game of 2000" for the three flagship editions of the game. Both "Dungeons & Dragons" and "Advanced Dungeons & Dragons" are Origins Hall of Fame Games inductees as they were deemed sufficiently distinct to merit separate inclusion on different occasions. The independent "Games" magazine placed "Dungeons & Dragons" on their "Games 100" list from 1980 through 1983, then entered the game into the magazine's Hall of Fame in 1984.
"Dungeons & Dragons" was the first modern role-playing game and it established many of the conventions that have dominated the genre. Particularly notable are the use of dice as a game mechanic, character record sheets, use of numerical attributes and gamemaster-centered group dynamics. Within months of "Dungeons & Dragons"'s release, new role-playing game writers and publishers began releasing their own role-playing games, with most of these being in the fantasy genre. Some of the earliest other role-playing games inspired by "D&D" include "Tunnels & Trolls" (1975), "Empire of the Petal Throne" (1975), and "Chivalry & Sorcery" (1976).
The role-playing movement initiated by "D&D" would lead to release of the science fiction game "Traveller" (1977), the fantasy game "RuneQuest" (1978), and subsequent game systems such as Chaosium's "Call of Cthulhu" (1981), "Champions" (1982), "GURPS" (1986), and "" (1991). "Dungeons & Dragons" and the games it influenced fed back into the genre's origin – miniatures wargames – with combat strategy games like "Warhammer Fantasy Battles". "D&D" also had a large impact on modern video games.
Director Jon Favreau credits "Dungeons & Dragons" with giving him "... a really strong background in imagination, storytelling, understanding how to create tone and a sense of balance."
Licensing.
Early in the game's history, TSR took no action against small publishers' production of "D&D" compatible material, and even licensed Judges Guild to produce "D&D" materials for several years, such as "City State of the Invincible Overlord." This attitude changed in the mid-1980s when TSR took legal action to try to prevent others from publishing compatible material. This angered many fans and led to resentment by the other gaming companies. Although TSR took legal action against several publishers in an attempt to restrict third-party usage, it never brought any court cases to completion, instead settling out of court in every instance. TSR itself ran afoul of intellectual property law in several cases.
With the launch of "Dungeons & Dragons"'s 3rd Edition, Wizards of the Coast made the d20 System available under the Open Game License (OGL) and d20 trademark license. Under these licenses, authors are free to use the d20 System when writing games and game supplements. The OGL and d20 Trademark License made possible new games, some based on licensed products like "Star Wars", and new versions of older games, such as "Call of Cthulhu".
During the 2000s, there has been a trend towards recreating older editions of "D&D". Necromancer Games, with its slogan "Third Edition Rules, First Edition Feel" and Goodman Games "Dungeon Crawl Classics" range are both examples of this in material for d20 System. Other companies have created complete game systems based on earlier editions of "D&D". An example is "HackMaster" (2001) by Kenzer and Company, a licensed, non-OGL, semi-satirical follow-on to 1st and 2nd Edition. "Castles & Crusades" (2005), by Troll Lord Games, is a reimagining of early editions by streamlining rules from OGL that was supported by Gary Gygax prior to his death.
With the release of the fourth edition, Wizards of the Coast has introduced its Game System License, which represents a significant restriction compared to the very open policies embodied by the OGL. In part as a response to this, some publishers (such as Paizo Publishing with its "Pathfinder Roleplaying Game") who previously produced materials in support of the "D&D" product line, have decided to continue supporting the 3rd Edition rules, thereby competing directly with Wizards of the Coast. Others, such as Kenzer & Company, are returning to the practice of publishing unlicensed supplements and arguing that copyright law does not allow Wizards of the Coast to restrict third-party usage.
Controversy and notoriety.
At various times in its history, "Dungeons & Dragons" has received negative publicity, in particular from some Christian groups, for alleged promotion of such practices as devil worship, witchcraft, suicide, and murder, and for the presence of naked breasts in drawings of female humanoids in the original "AD&D" manuals (mainly monsters such as Harpies, Succubi, etc.). These controversies led TSR to remove many potentially controversial references and artwork when releasing the 2nd Edition of "AD&D". Many of these references, including the use of the names "devils" and "demons", were reintroduced in the 3rd edition. The moral panic over the game also led to problems for fans of "D&D" who faced social ostracism, unfair treatment, and false association with the occult and Satanism, regardless of an individual fan's actual religious affiliation and beliefs.
"Dungeons & Dragons" has also been the subject of rumors regarding players having difficulty separating fantasy from reality, even leading to psychotic episodes. The most notable of these was the saga of James Dallas Egbert III, the facts of which were fictionalized in the novel "Mazes and Monsters" and later made into a TV movie. The game was also blamed for some of the actions of Chris Pritchard, who was convicted in 1990 of murdering his stepfather. Research by various psychologists, the first being that of Armando Simon, has concluded that no harmful effects are related to the playing of "D&D".
The game's commercial success was a factor that led to lawsuits regarding distribution of royalties between original creators Gygax and Arneson. Gygax later became embroiled in a political struggle for control of TSR which culminated in a court battle and Gygax's decision to sell his ownership interest in the company in 1985.
Related products.
"D&D"'s commercial success has led to many other related products, including "Dragon" Magazine, "Dungeon" Magazine, an animated television series, a film series, an official role-playing soundtrack, novels, and computer games such as the MMORPG "Dungeons & Dragons Online". Hobby and toy stores sell dice, miniatures, adventures, and other game aids related to "D&D" and its game offspring.
In popular culture.
"D&D" grew in popularity through the late 1970s and 1980s. Numerous games, films, and cultural references based on "D&D" or "D&D"-like fantasies, characters or adventures have been ubiquitous since the end of the 1970s. "D&D" players are (sometimes pejoratively) portrayed as the epitome of geekdom, and have become the basis of much geek and gamer humor and satire. Famous D&D players include professional basketball player Tim Duncan, comedian Stephen Colbert, and actors Vin Diesel and Robin Williams. "D&D" and its fans have been the subject of spoof films, including "Fear of Girls" and "".

</doc>
<doc id="7941" url="http://en.wikipedia.org/wiki?curid=7941" title="Double jeopardy">
Double jeopardy

Double jeopardy is a procedural defence that forbids a defendant from being tried again on the same (or similar) charges following a legitimate acquittal or conviction. In common law countries, a defendant may enter a peremptory plea of autrefois acquit or autrefois convict ("autrefois" means "in the past" in French), meaning the defendant has been acquitted or convicted of the same offence.
If this issue is raised, evidence will be placed before the court, which will normally rule as a preliminary matter whether the plea is substantiated; if it is, the projected trial will be prevented from proceeding. In some countries, including Canada, Mexico and the United States, the guarantee against being "twice put in jeopardy" is a constitutional right. In other countries, the protection is afforded by statute.
International Covenant on Civil and Political Rights.
The 72 signatories and 166 parties to the International Covenant on Civil and Political Rights recognise, under Article 14 (7): No one shall be liable to be tried or punished again for an offence for which he has already been finally convicted or acquitted in accordance with the law and penal procedure of each country.
European Convention on Human Rights.
All members of the Council of Europe (which includes nearly all European countries, and every member of the European Union) have signed the European Convention on Human Rights. The optional Seventh Protocol to the Convention, Article Four, protects against double jeopardy and says:
No one shall be liable to be tried or punished again in criminal proceedings under the jurisdiction of the same State for an offence for which he or she has already been finally acquitted or convicted in accordance with the law and penal procedure of that State.
Member states may, however, implement legislation which allows reopening of a case in the event that new evidence is found or if there was a fundamental defect in the previous proceedings:
The provisions of the preceding paragraph shall not prevent the reopening of the case in accordance with the law and penal procedure of the State concerned, if there is evidence of new or newly discovered facts, or if there has been a fundamental defect in the previous proceedings, which could affect the outcome of the case.
This optional protocol has been ratified by all EU states except five: (Belgium, Germany, Spain, the Netherlands, and the United Kingdom). In those member states, national rules governing double jeopardy may or may not comply with the provision cited above.
In many European countries, the prosecution may appeal an acquittal to a higher court (similar to the provisions of Canadian law)-this is not counted as double jeopardy but as a continuation of the same trial. This is allowed by the European Convention on Human Rights (note the word "finally" in the above quotation).
By country.
Australia.
In contrast to other common law nations, Australian double jeopardy law has been held to further prevent the prosecution for perjury following a previous acquittal where a finding of perjury would controvert the acquittal. This was confirmed in the case of "R v Carroll", where the police found new evidence convincingly disproving Carroll's sworn alibi two decades after he had been acquitted of murder charges in the death of Ipswich child Deidre Kennedy, and successfully prosecuted him for perjury. Public outcry following the overturn of his conviction (for perjury) by the High Court has led to widespread calls for reform of the law along the lines of the England and Wales legislation.
During a Council of Australian Governments (COAG) meeting of 2007, model legislation to rework double jeopardy laws was drafted, but there was no formal agreement for each state to introduce it. All states have now chosen to introduce legislation that mirrors COAG's recommendations on "fresh and compelling" evidence.
In New South Wales, retrials of serious cases with a minimum sentence of 20 years or more are now possible, whether or not the original trial preceded the 2006 reform. On 17 October 2006, the New South Wales Parliament passed legislation abolishing the rule against double jeopardy in cases where:
On 30 July 2008, South Australia also introduced legislation to scrap parts of its double jeopardy law, legalising retrials for serious offences with "fresh and compelling" evidence, or if the acquittal was tainted.
In Western Australia, on 8 September 2011 amendments were introduced that would allow also retrial if "new and compelling" evidence was found. It would apply to serious offences where the penalty was life imprisonment or imprisonment for 14 years or more. Acquittal because of tainting (witness intimidation, jury tampering, or perjury) would also allow retrial.
In Tasmania, on 19 August 2008, amendments were introduced to allow retrial in serious cases, if there is "fresh and compelling" evidence.
In Victoria on 21 December 2011, legislation was passed allowing new trials where there is "fresh and compelling DNA evidence, where the person acquitted subsequently admits to the crime, or where it becomes clear that key witnesses have given false evidence". Retrial applications however could only be made for serious offences such as murder, manslaughter, arson causing death, serious drug offences and aggravated forms of rape and armed robbery.
In Queensland on 18 October 2007, the double jeopardy laws were modified to allow a retrial where fresh and compelling evidence becomes available after an acquittal for murder or a "tainted acquittal" for a crime carrying a 25-year or more sentence. A "tainted acquittal" requires a conviction for an administration of justice offence, such as perjury, that led to the original acquittal. Unlike reforms in the United Kingdom, New South Wales, Tasmania, Victoria, South Australia, Western Australia, this law does not have a retrospective effect, which is unpopular with some advocates of the reform.
Canada.
The Canadian Charter of Rights and Freedoms includes provisions such as section 11(h) prohibiting double jeopardy. However, this prohibition applies only after an accused person has been "finally" convicted or acquitted. Canadian law allows the prosecution to appeal an acquittal: if the acquittal is thrown out, the new trial is not considered to be double jeopardy, as the verdict of the first trial would have been annulled. In rare circumstances, a court of appeal might also substitute a conviction for an acquittal. This is not considered to be double jeopardy, either – in this case, the appeal and subsequent conviction are deemed to be a continuation of the original trial.
For an appeal from an acquittal to be successful, the Supreme Court of Canada requires that the Crown show that an error in law was made during the trial and that the error contributed to the verdict. It has been suggested that this test is unfairly beneficial to the prosecution. For instance, lawyer Martin Friedland, in his book "My Life in Crime and Other Academic Adventures", contends that the rule should be changed so that a retrial is granted only when the error is shown to be "responsible" for the verdict, not just a factor.
A notable example of this is Guy Paul Morin, who was wrongfully convicted in his second trial after the acquittal in his first trial was vacated by the Supreme Court of Canada.
France.
Once all appeals have been exhausted on a case, the judgement is final and the action of the prosecution is closed (code of penal procedure, art. 6), except if the final ruling was forged. Prosecution for a crime already judged is impossible even if incriminating evidence has been found. However, a person who has been convicted may request another trial on grounds of new exculpating evidence through a procedure known as "révision".
Germany.
The Basic Law ("Grundgesetz") for the Federal Republic of Germany does provide protection against double jeopardy, if a final verdict is pronounced. A verdict is final, if nobody appeals against it.
However each trial party can appeal against a verdict in the first instance. This means the prosecution and/or the defendants can appeal against a judgement, if they don´t agree with it. In this case the trial starts again in the second instance, the court of appeal ("Berufungsgericht"), which considers the facts and reasons again and delivers the final judgement then.
If one of the parties disagrees with the judgement of the second instance, he or she can appeal for it, but only on formal judicial reasons. The case will checked in the third instance ("Revisionsgericht"), whether all laws are applied correctly.
The rule applies to the whole "historical event, which is usually considered a single historical course of actions the separation of which would seem unnatural". This is true even if new facts occur that indicate other and/or much serious crimes.
The Penal Procedural Code ("Strafprozessordnung") permits a retrial ("Wiederaufnahmeverfahren"), if it is in favor of the defendant or if following events had happened:
In the case of an order of summary punishment, which can be issued by the court without a trial for lesser misdemeanours, there is a further exception:
In Germany, a felony is defined as a crime which (usually) has a minimum of one year of imprisonment.
India.
A partial protection against double jeopardy is a Fundamental Right guaranteed under Article 20 (2) of the Constitution of India, which states, "No person shall be prosecuted and punished for the same offence more than once". This provision enshrines the concept of "autrefois convict", that no one convicted of an offence can be tried or punished a second time. However it does not extend to "autrefois acquit", and so if a person is acquitted of a crime, he can be retried. In India, protection against "autrefois acquit" is a statutory right, not a fundamental one. Such protection is provided by provisions of the Code of Criminal Procedure rather than by the Constitution.
Japan.
The Constitution of Japan states in Article 39 that
In practice, however, if someone is acquitted in a lower District Court, then the prosecutor can appeal to the High Court, and then to the Supreme Court. Only the acquittal in the Supreme Court is the final acquittal which prevents any further retrial. This process sometimes takes decades.
The above is not considered a violation of the constitution. Because of Supreme Court precedent, this process is all considered part of a single proceeding.
The Netherlands.
In the Netherlands, the state prosecution can appeal a not-guilty verdict at the bench. New evidence can be brought to bear during a retrial at a district court. Thus one can be tried twice for the same alleged crime. If one is convicted at the district court, the defence can make an appeal on procedural grounds to the supreme court. The supreme court might admit this complaint, and the case will be reopened yet again, at another district court. Again, new evidence might be introduced by the prosecution.
According to Dutch legal experts Crombag, Wagenaar, van Koppen, the Dutch system contravenes the provisions of the European Human Rights convention, in the imbalance between the power of the prosecution service and the defence.
On 9 April 2013 the Dutch senate voted 36 "yes" versus 35 "no" in favor of a new law that allows the prosecutor to re-try a person who was found not guilty in court. This new law is limited to crimes where someone died and new evidence must have been gathered. The new law also works retroactively.
Pakistan.
Article 13 of the Constitution of Pakistan protects a person from being punished or prosecuted more than once for the same offence.
Serbia.
This principle is incorporated in to the Constitution of the Republic of Serbia and further elaborated in its Criminal Procedure Act.
South Africa.
The Bill of Rights in the Constitution of South Africa forbids a retrial when there has already been an acquittal or a conviction.
South Korea.
Article 13 of the South Korean constitution provides that no citizen shall be placed in double jeopardy.
United Kingdom.
England and Wales.
Double jeopardy has been permitted in England and Wales in certain (exceptional) circumstances since the Criminal Justice Act 2003.
Pre-2003.
The doctrines of "autrefois acquit" and "autrefois convict" persisted as part of the common law from the time of the Norman conquest of England; they were regarded as essential elements of protection of the liberty of the subject and respect for due process of law in that there should be finality of proceedings. There were only three exceptions, all relatively recent, to the rules:
In Connelly v DPP ([1964] AC 1254), the Law Lords ruled that a defendant could not be tried for any offence arising out of substantially the same set of facts relied upon in a previous charge of which he had been acquitted, unless there are "special circumstances" proven by the prosecution. There is little case law on the meaning of "special circumstances", but it has been suggested that the emergence of new evidence would suffice.
A defendant who had been convicted of an offence could be given a second trial for an aggravated form of that offence if the facts constituting the aggravation were discovered after the first conviction. By contrast, a person who had been acquitted of a lesser offence could not be tried for an aggravated form even if new evidence became available.
Post-2003.
Following the murder of Stephen Lawrence, the Macpherson Report recommended that the double jeopardy rule should be abrogated in murder cases, and that it should be possible to subject an acquitted murder suspect to a second trial if "fresh and viable" new evidence later came to light. The Law Commission later added its support to this in its report "Double Jeopardy and Prosecution Appeals" (2001). A parallel report into the criminal justice system by Lord Justice Auld, a past Senior Presiding Judge for England and Wales, had also commenced in 1999 and was published as the Auld Report six months after the Law Commission report. It opined that the Law Commission had been unduly cautious by limiting the scope to murder and that "the exceptions should [...] extend to other grave offences punishable with life and/or long terms of imprisonment as Parliament might specify."
Both the Home Office resident Jack Straw and Leader of the Opposition William Hague favoured this measure. These recommendations were implemented—not uncontroversially at the time—within the Criminal Justice Act 2003, and this provision came into force in April 2005. It opened certain serious crimes (including murder, manslaughter, kidnapping, rape, armed robbery, and serious drug crimes) to a retrial, regardless of when committed, with two conditions: the retrial must be approved by the Director of Public Prosecutions, and the Court of Appeal must agree to quash the original acquittal due to "new and compelling evidence". Pressure by Ann Ming, the mother of 1989 murder victim Julie Hogg—whose killer, William Dunlop, was initially acquitted in 1991 and subsequently confessed—also contributed to the demand for legal change.
On 11 September 2006, Dunlop became the first person to be convicted of murder following a prior acquittal for the same crime, in his case his 1991 acquittal of Julie Hogg's murder. Some years later he had confessed to the crime, and was convicted of perjury, but was unable to be retried for the killing itself. The case was re-investigated in early 2005, when the new law came into effect, and his case was referred to the Court of Appeal in November 2005 for permission for a new trial, which was granted. Dunlop pleaded guilty to murdering Julie Hogg and was sentenced to life imprisonment, with a recommendation he serve no less than 17 years.
On 13 December 2010, Mark Weston became the first person to be retried and found guilty of murder by a jury (Dunlop having confessed). In 1996 Weston had been acquitted of the murder of Vikki Thompson at Ascott-under-Wychwood on 12 August 1995, but following the discovery in 2009 of compelling new evidence (Thompson's blood on Weston's boots) he was arrested and tried for a second time. He was sentenced to life imprisonment, to serve a minimum of 13 years.
Scotland.
The double jeopardy rule no longer applies absolutely in Scotland since the Double Jeopardy (Scotland) Act 2011 came into force on 28 November 2011. The Act introduced three broad exceptions to the rule: where the acquittal had been tainted by an attempt to pervert the course of justice; where the accused admitted his guilt after acquittal; and where there was new evidence.
Northern Ireland.
In Northern Ireland the Criminal Justice Act 2003, effective 18 April 2005, makes certain "qualifying offence" (including murder, rape, kidnapping, specified sexual acts with young children, specified drug offences, defined acts of terrorism, as well as in certain cases attempts or conspiracies to commit the foregoing) subject to retrial after acquittal (including acquittals obtained before passage of the Act) if there is a finding by the Court of Appeal that there is "new and compelling evidence."
United States.
The Fifth Amendment to the United States Constitution provides:
The Double Jeopardy Clause encompasses four distinct prohibitions: subsequent prosecution after acquittal, subsequent prosecution after conviction, subsequent prosecution after certain mistrials, and multiple punishment in the same indictment. Jeopardy "attaches" when the jury is empanelled, the first witness is sworn, or a plea is accepted.
The government is not permitted to appeal or retry the defendant once jeopardy attaches to a trial unless the case does not conclude. Conditions which constitute "conclusion" of a case include
In these cases the trial is concluded and the prosecution is precluded from appealing or retrying the defendant over the offense to which they were acquitted.
This principle does not prevent the government from appealing a pre-trial motion to dismiss or other non-merits dismissal, or a directed verdict after a jury conviction, nor does it prevent the trial judge from entertaining a motion for reconsideration of a directed verdict, if the jurisdiction has so provided by rule or statute. Nor does it prevent the government from retrying the defendant after an appellate reversal other than for sufficiency, including "habeas corpus", or "thirteenth juror" appellate reversals notwithstanding sufficiency on the principle that jeopardy has not "terminated." There may also be an exception for judicial bribery, but not jury bribery.
The "dual sovereignty" doctrine allows a federal prosecution of an offense to proceed regardless of a previous state prosecution for that same offense and vice versa because "an act denounced as a crime by both national and state sovereignties is an offense against the peace and dignity of both and may be punished by each." The doctrine is solidly entrenched in the law, but there has been a traditional reluctance in the federal executive branch to gratuitously wield the power it grants.
Another exception is that the perpetrator can be retried by court martial in a military court, if they have been previously acquitted by a civilian court, and are members of the military.
In "Blockburger v. United States" (1932), the Supreme Court announced the following test: the government may separately try and punish the defendant for two crimes if each crime contains an element that the other does not. An example of this is the successful prosecution under 18 U.S.C. § 242 (deprivation of rights under color of law) of some of the Los Angeles police officers involved in the Rodney King beating after their acquittal in California state court on charges of assault with a deadly weapon and excessive use of force by a police officer. "Blockburger" is the default rule, unless the governing statute legislatively intends to depart; for example, Continuing Criminal Enterprise (CCE) may be punished separately from its predicates, as can conspiracy.
The "Blockburger" test, originally developed in the multiple punishments context, is also the test for prosecution after conviction. In "Grady v. Corbin" (1990), the Court held that a double jeopardy violation could lie even where the "Blockburger" test was not satisfied, but "Grady" was overruled in "United States v. Dixon" (1993).
The rule for mistrials depends upon who sought the mistrial. If the defendant moves for a mistrial, there is no bar to retrial, unless the prosecutor acted in "bad faith," i.e. goaded the defendant into moving for a mistrial because the government specifically wanted a mistrial. If the prosecutor moves for a mistrial, there is no bar to retrial if the trial judge finds "manifest necessity" for granting the mistrial. The same standard governs mistrials granted sua sponte.
Retrials are not common, due to the legal expenses to the government. However, in the mid-1980s Georgia antiques dealer James Arthur Williams was tried a record "four" times for the murder of Danny Hansford and (after three mistrials) was finally acquitted on the grounds of self-defense. The case is recounted in the book Midnight in the Garden of Good and Evil which was adapted into a film directed by Clint Eastwood (the movie omits the first three murder trials).
External links.
United Kingdom.
Research and Notes produced for the UK Parliament, summarising the history of legal change, views and responses, and analyses:

</doc>
<doc id="7942" url="http://en.wikipedia.org/wiki?curid=7942" title="Disbarment">
Disbarment

Disbarment is the removal of a lawyer from a bar association or the practice of law, thus revoking his or her law license or admission to practice law. Disbarment is usually a punishment for unethical or criminal conduct. Procedures vary depending on the law society.
In the U.S..
Overview.
Generally disbarment is imposed as a sanction for conduct indicating that an attorney is not fit to practice law, willfully disregarding the interests of a client, or engaging in fraud which impedes the administration of justice. In addition, any lawyer who is convicted of a felony is automatically disbarred in most jurisdictions, a policy that, although opposed by the American Bar Association, has been described as a convicted felon's just deserts.
In the United States legal system, disbarment is specific to regions; one can be disbarred from some courts, while still being a member of the bar in another jurisdiction. However, under the American Bar Association's Model Rules of Professional Conduct, which have been adopted in most states, disbarment in one state or court is grounds for disbarment in a jurisdiction which has adopted the Model Rules.
Disbarment is quite rare. Instead, lawyers are usually sanctioned by their own clients through civil malpractice proceedings, or via fine, censure, suspension, or other punishments from the disciplinary boards. To be disbarred is considered a great embarrassment and shame, even if one no longer wishes to pursue a career in the law.
Because disbarment rules vary by area, different rules can apply depending on where a lawyer is disbarred. Notably, the majority of US states have no procedure for permanently disbarring a person. Depending on the jurisdiction, a lawyer may reapply to the bar immediately, after five to seven years, or be banned for life.
Notable U.S. disbarments.
The 20th and the 21st centuries have seen one former U.S. president and one former U.S. vice president disbarred, and another president suspended from one bar and caused to resign from another bar rather than face disbarment.
Former Vice President Spiro Agnew, having pleaded no contest (which subjects a person to the same criminal penalties as a guilty plea, but is not an admission of guilt for a civil suit) to charges of bribery and tax evasion, was disbarred from Maryland, the state of which he had previously been governor.
Former President Richard Nixon was disbarred from New York in 1976 for obstruction of justice related to the Watergate scandal.
In October 2001 the United States Supreme Court barred President Bill Clinton from practicing before it. He then resigned from the Supreme Court bar in November.
Alger Hiss was disbarred for a felony conviction, but later became the first person reinstated to the bar in Massachusetts after disbarment.
In 2007, Mike Nifong, the District Attorney of Durham County, North Carolina who presided over the 2006 Duke University lacrosse case, was disbarred for prosecutorial misconduct related to his handling of the case.
In April 2012, a three member panel appointed by the Arizona Supreme Court voted unanimously to disbar Andrew Thomas, former County Attorney of Maricopa County, Arizona, and a former close confederate of Maricopa County Sheriff Joe Arpaio. According to the panel, Thomas "outrageously exploited power, flagrantly fostered fear, and disgracefully misused the law" while serving as Maricopa County Attorney. The panel found "clear and convincing evidence" that Thomas brought unfounded and malicious criminal and civil charges against political opponents, including four state judges and the state attorney general. "Were this a criminal case," the panel concluded, "we are confident that the evidence would establish this conspiracy beyond a reasonable doubt."
Jack Thompson, the Florida lawyer noted for his activism against video games and rap music, was permanently disbarred for various charges of misconduct. The action was the result of several grievances claiming that Thompson had made defamatory, false statements and attempted to humiliate, embarrass, harass or intimidate his opponents. The order was made on September 25, 2008, effective October 25. However, Thompson attempted to appeal to the higher courts in order to avoid the penalty actually taking effect. Neither the US District court, nor the US Supreme Court would hear his appeal, rendering the judgement of the Florida Supreme Court final.
Ed Fagan, a New York lawyer who prominently represented Holocaust victims against Swiss banks, was disbarred in New York (in 2008) and New Jersey (in 2009) for failing to pay court fines and fees; and for misappropriating client and escrow trust funds.
F. Lee Bailey, noted criminal defense attorney, was disbarred by the state of Florida in 2001, with reciprocal disbarment in Massachusetts in 2002. The Florida disbarment was the result of his handling of stock in the DuBoc marijuana case. Bailey was found guilty of 7 counts of attorney misconduct by the Florida Supreme Court. Bailey had transferred a large portion of DuBoc's assets into his own accounts, using the interest gained on those assets to pay for personal expenses. In March 2005, Bailey filed to regain his law license in Massachusetts. The book "Florida Pulp Nonfiction" details the peculiar facts of the DuBoc case along with extended interviews with Bailey that include his own defense.

</doc>
<doc id="7946" url="http://en.wikipedia.org/wiki?curid=7946" title="Dog tag">
Dog tag

A dog tag is the informal name given to the identification tags worn by military personnel, because of their resemblance to actual dog tags. The tag is primarily used for the identification of dead and wounded and essential basic medical information, such as blood type and history of inoculations, along with providing religious preference. Dog tags are usually fabricated from a corrosion-resistant metal although they have been made from whatever was available.
It may contain two copies of the information and be designed to break easily into two pieces. Some nations use two identical tags. This allows half the tag to be collected from a soldier's body for notification while the other half remains with the corpse when battle conditions do not allow the body to be immediately recovered.
History.
Dog tags were provided to Chinese soldiers as early as the mid-19th century. During the Taiping revolt (1851–66), both the Imperialists (i.e. the Chinese Imperial Army regular servicemen) and those Taiping rebels wearing a uniform wore a wooden dog-tag at the belt, bearing the soldier's name, age, birthplace, unit, and date of enlistment.
During the American Civil War of 1861–1865, some soldiers pinned paper notes with their name and home address to the backs of their coats. Other soldiers stenciled identification on their knapsacks or scratched it in the soft lead backing of the army belt buckle.
Manufacturers of identification badges recognized a market and began advertising in periodicals. Their pins were usually shaped to suggest a branch of service and engraved with the soldier's name and unit. Machine-stamped tags were also made of brass or lead with a hole and usually had (on one side) an eagle or shield and such phrases as "War for the Union" or "Liberty, Union, and Equality". The other side had the soldier's name and unit and sometimes a list of battles in which he had participated.
Franco-Prussian War.
The Prussian Army issued identification tags for its troops at the beginning of the Franco-Prussian War in 1870. They were nicknamed "Hundemarken" (the German equivalent of "dog tags") and compared to a similar identification system instituted for dogs in the Prussian capital city of Berlin at about the same time.
First World War.
The British Army and their Imperial forces in Canada, Australia and New Zealand issued identification discs from the beginning of the First World War. The discs were made of fiber, one in red and one in green, and suspended around the neck by butcher's twine. The same pattern was worn into the Second World War and the Korean War.
The U.S. Army first authorized identification tags in War Department General Order No. 204, dated December 20, 1906, which essentially prescribes the Kennedy identification tag:
An aluminum identification tag, the size of a silver half dollar and of suitable thickness, stamped with the name, rank, company, regiment, or corps of the wearer, will be worn by each officer and enlisted man of the Army whenever the field kit is worn, the tag to be suspended from the neck, underneath the clothing, by a cord or thong passed through a small hole in the tab. It is prescribed as a part of the uniform and when not worn as directed herein will be habitually kept in the possession of the owner. The tag will be issued by the Quartermaster's Department gratuitously to enlisted men and at cost price to officers...
The army changed regulations on July 6, 1916, so that all soldiers were issued two tags: one to stay with the body and the other to go to the person in charge of the burial for record-keeping purposes. In 1918, the army adopted and allotted the serial number system, and name and serial numbers were ordered stamped on the identification tags. (Serial number 1 was assigned to enlisted man Arthur B. Crane of Chicago in the course of his fifth enlistment period.)
World War II.
There is a recurring myth about the notch situated in one end of the dog tags issued to United States Army personnel during World War II. It was rumored that the notch's purpose was that, if a soldier found one of his comrades on the battlefield, he could take one tag to the commanding officer and stick the other between the teeth of the soldier to ensure that the tag would remain with the body and be identified. In reality, the notch was designed to hold the tag in place when being imprinted on the carbon-paper medical form by the Model 70 Addressograph (a pistol-type imprinting machine used by the Medical Department during World War II). It appears instructions that would confirm the notch's mythical use were issued at least unofficially by the Graves Registration Service during the Vietnam War to Army troops headed overseas.
Dog tags are traditionally part of the makeshift battlefield memorials soldiers created for their fallen comrades. The casualty's rifle with bayonet affixed is stood vertically atop the empty boots, with the helmet over the stock of the rifle. The dog tags hang from the rifle's handle or trigger guard.
Non-military usage.
Medical condition identification.
Some tags (along with similar items such as MedicAlert bracelets) are used also by civilians to identify their wearers and specify them as having health problems that may <br>
"(a)" suddenly incapacitate their wearers and render them incapable of providing treatment guidance (as in the cases of heart problems, epilepsy, diabetic coma, accident or major trauma) and/or <br> 
"(b)" interact adversely with medical treatments, especially standard or "first-line" ones (as in the case of an allergy to common medications) and/or <br>
"(c)" provide in case of emergency ("ICE") contact information and/or <br> 
"(d)" state a religious, moral, or other objection to artificial resuscitation), if a first responder attempts to administer such treatment when the wearer is non-responsive and thus unable to warn against doing so.
Military personnel in some jurisdictions may wear a supplementary medical information tag.
Fashion.
Dog tags have recently found their way into youth fashion by way of . Originally worn as a part of a military uniform by youth wishing to present a tough or militaristic image, dog tags have since seeped out into wider fashion circles. They may be inscribed with a person's details, their beliefs or tastes, a favorite quote, or may bear the name or logo of a band or performer. Some people also prefer to have the information on their tags transferred to a smaller, sometimes golden or silver tag by a jeweler, as the original tag can be considered too large and bulky by some.
Since the late 1990s, custom dog tags have become fashionable amongst musicians (particularly rappers), and as a marketing give-away item. Numerous companies offer customers the opportunity to create their own personalized dog tags with their own photos, logos, and text. Even high-end jewelers have featured gold and silver dog tags encrusted with diamonds and other jewels. For example, to market the release of "", dog tags inscribed with the words "CALL OF DUTY BLACK OPS II", and the same for "Battlefield 4" , Halo3 were given when a person pre-ordered the game.
Variations by country.
Austria.
The Austrian Bundesheer utilized a single long, rectangular tag with oval ends, stamped with blood group & Rh factor at the end, with ID number underneath. Two slots and a hole stamped beneath the number allow the tag to be broken in half, and the long bottom portion has both the ID number and a series of holes which allows the tag to be inserted into a dosimeter. This has been replaced with a more conventional, wider and rounded rectangle which can still be halved, but lacks the dosimeter reading holes.
Australia.
The Australian Defence Force issues soldiers two tags of different shapes: Number 1 Tag (the octagonal shaped disc) and Number 2 Tag (the circular disc). The format is as follows:
Example:
The information is printed exactly the same on both discs. In the event of a casualty, the circular tag is removed from the body.
Belgium.
Belgian Forces identity tags are, like the Canadian and Norwegian, designed to be broken in two in case of fatality; the lower half is returned to Belgian Defence tail, while the upper half remains on the body. The tags contain the following information:
Canada.
Canadian Forces identity discs (abbreviated "I discs") are designed to be broken in two in the case of fatality; the lower half is returned to National Defence Headquarters with the member's personal documents, while the upper half remains on the body. The tags contain the following information:
Before the Service Number was introduced in the 1990s, military personnel were identified on the I discs (as well as other documents) by their Social Insurance Number.
China.
The People's Liberation Army issues two long, rectangular tags. All information are stamped in Simplified Chinese:
Colombia.
The Ejército Nacional de Colombia uses long, rectangular metal tags with oval ends tags stamped with the following information:
Duplicate tags are issued. Often, tags are issued with a prayer inscribed on the reverse.
Cyprus.
In Cyprus, identification tags include the following information:
Denmark.
Military of Denmark dog tags are a small metallic plate, designed to be broken into two pieces. The information on the tag is: 
On the right hand side of the tag it says "Danmark", the Danish word for "Denmark".
On new issue dog tags ca 1985 there is social security number this is the same as service no, army navy air force has blood group, national guard has normally no blood group on it.
East Germany.
The Nationale Volksarmee used a tag nearly identical with that used by both the Wehrmacht and the West German Bundeswehr. The oval aluminum tag was stamped "DDR" (Deutsche Demokratische Republik) above the personal ID number; this information was repeated on the bottom half, which was intended to be broken off in case of death. Oddly, the tag was not worn, but required to be kept in a plastic sleeve in the back of the WDA identity booklet.
Ecuador.
The "Placas de identificación de campaña" consists of two long, rectangular steel or aluminum tags with rounded corners and a single hole punched in one end. It is suspended by a US-type ball chain, with a shorter chain for the second tag. The information on the tag is:
Estonia.
Estonian dog tags are designed to be broken in two. The dog tag is a metallic rounded rectangle suspended by a ball chain. Information consists of four fields: 
Example: 
Finland.
Finnish dog tags (sometimes called "raatolaatta"; "corpse plaque"/"corpse plate"-) are also designed to be broken in two; however, the only text on it is the personal identification number and the letters "SF" (rarely FI), which stands for Suomi Finland, within a tower stamped atop of the upper half.
France.
France issues either a metallic rounded rectangle (army) or disk (navy), designed to be broken in half, bearing family name & first name above the ID number.
Germany.
German Bundeswehr ID tags are an oval-shaped discs designed to be broken in half. The two sides contain different information which are mirrored upside-down on the lower half of the ID tag. They feature the following information on segmented and numbered fields:
On the front
On the back
Greece.
In Greece, identification tags include the following information:
Hungary.
The Hungarian army dog tag is made out of steel, forming a 25×35 mm tag designed to split diagonally. Both sides contain the same information: the soldier's personal identity code, blood group and the word HUNGARIA. Some may not have the blood group on them. These are only issued to soldiers who are serving outside of the country. If the soldier should die, one side is removed and kept for the army's official records, while the other side is left attached to the body.
Iraq.
The Saddam-era Iraqi Army utilized a single, long, rectangular metal tag with oval ends, inscribed (usually by hand) with Name and Number or Unit, and occasionally Blood Type.
Israel.
Israeli dog tags are designed to be broken in two. The information appears in three lines (twice):
Another dog tag is kept inside the military boot in order to identify dead soldiers.
Originally the IDF issued two circular aluminum tags (1948 – late 1950s) stamped in three lines with serial number, family name, and first name. The tags were threaded together through a single hole onto a cord worn around the neck.
Japan.
Japan follows a similar system to the US Army for its Self Defence Force personnel, and the appearance of the tags is similar, although laser etched. The exact information order is as follows.
Malaysia.
Malaysian Armed Forces have two identical oval tags with this information:
If more information needed, another two oval wrist tags are provided. The term "wrist tags" can be used to refer to the bracelet-like wristwatch. The additional tags only need to be worn on the wrist, with the main tags still on the neck. All personnel are allowed to attach a small religious pendant or locket; this makes a quick identifiable reference for their funeral services.
Mexico.
The Ejército de Mexico uses a single long, rectangular metal tag with oval ends, embossed with Name, serial number, and blood type plus Rh factor.
Netherlands.
Military of the Netherlands identity tags, like the Canadian and Norwegian ones, are designed to be broken in two in case of a fatality; the lower end is returned to Dutch Defence Headquarters, while the upper half remains on the body. There is a difference in the Army and Airforce service number and the Navy service number:
The tags contain the following information:
Norway.
Norwegian dog tags are designed to be broken in two like the Canadian version:
Poland.
The first dog tags were issued in Poland following the order of the General Staff of December 12, 1920. The earliest design (dubbed "kapala" in Polish, more properly called "kapsel legitymacyjny" - meaning "identification cap") consisted of a tin-made 30×50 mm rectangular frame and a rectangular cap fitting into the frame. Soldiers' details were filled in a small ID card placed inside the frame, as well as on the inside of the frame itself. The dog tag was similar to the tags used by the Austro-Hungarian Army during World War I. In case the soldier died, the frame was left with his body, while the lid was returned to his unit together with a note on his death. The ID card was handed over to the chaplain or the rabbi.
In 1928, a new type of dog tag was proposed by gen. bryg. Stanisław Rouppert, Poland's representative at the International Red Cross. It was slightly modified and adopted in 1931 under the name of Nieśmiertelnik wz. 1931 (literally, Immortalizer mark 1931). The new design consisted of an oval piece of metal (ideally steel, but in most cases aluminum alloy was used), roughly 40 by 50 millimeters. There were two notches on both sides of the tag, as well as two rectangular holes in the middle to allow for easier breaking of the tag in two halves. The halves contained the same set of data and were identical, except the upper half had two holes for a string or twine to go through. The data stamped on the dog tag included:
Sometimes the rank of the soldier was added to the reverse, and most members of the medical corps had a tiny cross stamped near the string holes, regardless of their religion.
Rhodesia.
The former Republic of Rhodesia used two WW2 British-style compressed asbestos fiber tags, a No. 1 octagonal (green) tag and a No. 2 circular (red) tag, stamped with identical information. The red tag was supposedly fireproof and the green tag rotproof. The following information was stamped on the tags: Number, Name, Initials, & Religion; Blood Type was stamped on reverse. The air force and BSAP often stamped their service on the reverse side above the blood group.
Many soldiers state they were issued blank tags and told to punch the information in themselves. 
Russia.
Russian Armed Forces uses metal oval tags. Russian dog tags contains the title "ВС РОССИИ" ("Armed Forces of Russia") and the alphanumeric individual numbers.
Singapore.
The Singapore Armed Forces-issued dog tags are inscribed (not embossed) with up to four items:
The dog tags consist of two metal pieces, one oval with two holes and one round with one hole. A synthetic lanyard is threaded through both holes in the oval piece and tied around the wearer's neck. The round piece is tied to the main loop on a shorter loop.
South Africa.
The former South African Defense Force used two long, rectangular aluminum tags with oval ends, stamped with serial number, name and initials, religion, and blood type.
South Korea.
The South Korean Army issues two long, rectangular tags with oval ends, stamped (in Korean lettering) with "Yuk-Gun" (English: Army) above a personal number, with the name below that and the blood group at the bottom.
South Vietnam.
The South Vietnamese Army used two American-style dog tags. Some tags added religion, e.g., Công Giáo for Catholic. They were stamped or inscribed with: 
Spain.
Issues a single metal oval, worn vertically, stamped "ESPAÑA" above and below the 3-slot horizontal break line. It is stamped in 4 lines with:
Sweden.
Swedish dog tags are designed to be able to break apart. The information on them was prior to 2010:
Swedish dog tags issued to troops after 2010 are, for personal security reasons, only marked with personal identity number.
During the Cold War dog tags were issued to everyone, often soon after birth, since the threat of total war also meant the risk of severe civilian casualties. However in the late 1990s the Swedish government decided that the dog tags were not needed anymore.
Switzerland.
Swiss Armed Forces ID tag is an oval shaped non reflective plaque, containing the following information:
On the back side the letters CH standing for (Confoederatio Helvetica) are engraved next to a Swiss cross.
United Kingdom.
The British Armed Forces currently utilize two circular non-reflecting stainless steel tags engraved with the "Big 6":
The disks are suspended from one long chain (24 inches long) and one short chain (4.5 inches long)
During World War One and Two, service personnel were issued pressed fiber identity disks, one green octagonal shaped disc, and a red round disc (some army units issued a second red round disk to be attached to the service respirator). The identity disks were hand stamped with the surname, initials, service number and religion of the holder and if in the Royal Air Force, the initials RAF. The disks were worn around the neck on a 38" length of cotton cord, this was often replaced by the wearer with a leather bootlace. One tag was suspended below the main tag.
From 1960 these were replaced with stainless steel ID tags on a green nylon cord, two circular and one oval. The oval was withdrawn around 1990.
United States of America.
In the 1990s, the U.S. Army stopped using the term "dog tags", replacing it with the designation "ID tags".
A persistent rumor is that debossed (imprinted with stamped in letters) dog tags were issued from World War II till the end of the Vietnam War and that currently the U.S. Armed Forces is issuing embossed (imprinted with raised letters) dog tags. In actuality, the U.S. Armed Forces issues dog tags with both types of imprinting, depending on the machine used at a given facility. The military issued 95% of their identification tags up until recently (within the past 10 years) with debossed text.
The U.S. Armed Forces typically carry two identical oval dog tags containing:
Religious designation.
During World War II, an American dog tag could indicate only one of three religions through the inclusion of one letter: "P" for Protestant, "C" for Catholic, or "H" for Jewish (from the word, "Hebrew"), or (according to at least one source) "NO" to indicate no religious preference. Army regulations (606-5) soon included X and Y in addition to P,C, and H: the X indicating any religion not included in the first three, and the Y indicating either no religion or a choice not to list religion.
By the time of the Vietnam War, some IDs spelled out the broad religious choices such as PROTESTANT and CATHOLIC, rather than using initials, and also began to show individual denominations such as "METHODIST" or "BAPTIST." Tags did vary by service, however, such as the use of "CATH," not "CATHOLIC" on some Navy tags. For those with no religious affiliation and those who chose not to list an affiliation, either the space for religion was left blank or the words "NO PREFERENCE" or "NO RELIGIOUS PREF" were included.
Although American dog tags include the recipient's religion as a way of ensuring that religious needs will be met, some personnel have them reissued without religious affiliation listed—or keep two sets, one with the designation and one without—out of fear that identification as a member of a particular religion could increase the danger to their welfare or their lives if they fell into enemy hands. Some Jewish personnel avoided flying over German lines during WWII with ID tags that indicated their religion, and some Jewish personnel avoid the religious designation today out of concern that they could be captured by extremists who are anti-Semitic. Additionally, when American troops were first sent to Saudi Arabia during the Gulf War there were allegations that some U.S. military authorities were pressuring Jewish military personnel to avoid listing their religions on their ID tags.

</doc>
<doc id="7950" url="http://en.wikipedia.org/wiki?curid=7950" title="Drum">
Drum

The drum is a member of the percussion group of musical instruments. In the Hornbostel-Sachs classification system, it is a membranophone. Drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with the player's hands, or with a drum stick, to produce sound. There is usually a "resonance head" on the underside of the drum, typically tuned to a slightly lower pitch than the top drumhead. Other techniques have been used to cause drums to make sound, such as the thumb roll. Drums are the world's oldest and most ubiquitous musical instruments, and the basic design has remained virtually unchanged for thousands of years.
Drums may be played individually, with the player using a single drum, and some drums such as the djembe are almost always played in this way. Others are normally played in a set of two or more, all played by the one player, such as bongo drums and timpani. A number of different drums together with cymbals form the basic modern drum kit.
Uses.
Drums are usually played by striking with the hand, or with one or two sticks. In many traditional cultures, drums have a symbolic function and are used in religious ceremonies. Drums are often used in music therapy, especially hand drums, because of their tactile nature and easy use by a wide variety of people.
In popular music and jazz, "drums" usually refers to a drum kit or a set of drums (with some cymbals), and "drummer" to the person who plays them.
Drums acquired even divine status in places such as Burundi, where the "karyenda" was a symbol of the power of the king.
Construction.
The shell almost invariably has a circular opening over which the drumhead is stretched, but the shape of the remainder of the shell varies widely. In the western musical tradition, the most usual shape is a cylinder, although timpani, for example, use bowl-shaped shells. Other shapes include a frame design (tar, Bodhrán), truncated cones (bongo drums, Ashiko), goblet shaped (djembe), and joined truncated cones (talking drum).
Drums with cylindrical shells can be open at one end (as is the case with timbales), or can have two drum heads. Single-headed drums typically consist of a skin stretched over an enclosed space, or over one of the ends of a hollow vessel. Drums with two heads covering both ends of a cylindrical shell often have a small hole somewhat halfway between the two heads; the shell forms a resonating chamber for the resulting sound. Exceptions include the African slit drum, also known as a log drum as it is made from a hollowed-out tree trunk, and the Caribbean steel drum, made from a metal barrel. Drums with two heads can also have a set of wires, called snares, held across the bottom head, top head, or both heads, hence the name snare drum.
On modern band and orchestral drums, the drumhead is placed over the opening of the drum, which in turn is held onto the shell by a "counterhoop" (or "rim"), which is then held by means of a number of tuning screws called "tension rods" that screw into lugs placed evenly around the circumference. The head's tension can be adjusted by loosening or tightening the rods. Many such drums have six to ten tension rods. The sound of a drum depends on many variables—including shape, shell size and thickness, shell materials, counterhoop material, drumhead material, drumhead tension, drum position, location, and striking velocity and angle.
Prior to the invention of tension rods, drum skins were attached and tuned by rope systems—as on the Djembe—or pegs and ropes such as on Ewe Drums. These methods are rarely used today, though sometimes appear on regimental marching band snare drums. The head of a talking drum, for example, can be temporarily tightened by squeezing the ropes that connect the top and bottom heads. Similarly, the tabla is tuned by hammering a disc held in place around the drum by ropes stretching from the top to bottom head. Orchestral timpani can be quickly tuned to precise pitches by using a foot pedal.
Sound of a drum.
Several factors determine the sound a drum produces, including the type, shape and construction of the drum shell, the type of drum heads it has, and the tension of these drumheads. Different drum sounds have different uses in music. Take, for example, the modern Tom-tom drum. A jazz drummer may want drums that are high pitched, resonant and quiet whereas a rock drummer may prefer drums that are loud, dry and low-pitched. Since these drummers want different sounds, their drums are constructed a little differently.
The drum head has the most effect on how a drum sounds. Each type of drum head serves its own musical purpose and has its own unique sound. Double-ply drumheads dampen high frequency harmonics because they are heavier and they are suited to heavy playing. Drum heads with a white, textured coating on them muffle the overtones of the drum head slightly, producing a less diverse pitch. Drum heads with central silver or black dots tend to muffle the overtones even more. And drum heads with perimeter sound rings mostly eliminate overtones (Howie 2005). Some jazz drummers avoid using thick drum heads, preferring single ply drum heads or drum heads with no muffling. Rock drummers often prefer the thicker or coated drum heads.
The second biggest factor that affects drum sound is head tension against the shell. When the hoop is placed around the drum head and shell and tightened down with tension rods, the tension of the head can be adjusted. When the tension is increased, the amplitude of the sound is reduced and the frequency is increased, making the pitch higher and the volume lower.
The type of shell also affects the sound of a drum. Because the vibrations resonate in the shell of the drum, the shell can be used to increase the volume and to manipulate the type of sound produced. The larger the diameter of the shell, the lower the pitch. The larger the depth of the drum, the louder the volume. Shell thickness also determines the volume of drums. Thicker shells produce louder drums. Mahogany raises the frequency of low pitches and keeps higher frequencies at about the same speed. When choosing a set of shells, a jazz drummer may want smaller maple shells, while a rock drummer may want larger birch shells. For more information about tuning drums or the physics of a drum, visit the external links listed below.
History.
Drums made with alligator skins have been found in Neolithic cultures located in China, dating to a period of 5500–2350 BC. In literary records, drums manifested shamanistic characteristics and were often used in ritual ceremonies.
Bronze Dong Son drums are were fabricated by the Bronze Age Dong Son culture of northern Vietnam. They include the ornate Ngoc Lu drum.
Animal drumming.
Macaque monkeys drum objects in a rhythmic way to show social dominance and this has been shown to be processed in a similar way in their brains to vocalizations suggesting an evolutionary origin to drumming as part of social communication. Other primates make drumming sounds by chest beating or hand clapping, and rodents such as kangaroo rats also make similar sounds using their paws on the ground.
Talking drums.
Drums are used not only for their musical qualities, but also as a means of communication over great distances. The talking drums of Africa are used to imitate the tone patterns of spoken language. Throughout Sri Lankan history drums have been used for communication between the state and the community, and Sri Lankan drums have a history stretching back over 2500 years.
Drums in art.
Drumming may be a purposeful expression of emotion for entertainment, spiritualism and communication. Many cultures practice drumming as a spiritual or religious passage and interpret drummed rhythm similarly to spoken language or prayer. Drumming has developed over millennia to be a powerful art form. Drumming is commonly viewed as the root of music and is sometimes performed as a kinesthetic dance. As a discipline, drumming concentrates on training the body to punctuate, convey and interpret musical rhythmic intention to an audience and to the performer.
Military uses.
Chinese troops used tàigǔ drums to motivate troops, to help set a marching pace, and to call out orders or announcements. For example, during a war between Qi and Lu in 684 BC, the effect of drum on soldier's morale is employed to change the result of a major battle. Fife-and-drum corps of Swiss mercenary foot soldiers also used drums. They used an early version of the snare drum carried over the player's right shoulder, suspended by a strap (typically played with one hand using traditional grip). It is to this instrument that the English word "drum" was first used. Similarly, during the English Civil War rope-tension drums would be carried by junior officers as a means to relay commands from senior officers over the noise of battle. These were also hung over the shoulder of the drummer and typically played with two drum sticks. Different regiments and companies would have distinctive and unique drum beats only they recognized. In the mid-19th century, the Scottish military started incorporating pipe bands into their Highland Regiments.
During pre-Columbian warfare, Aztec nations were known to have used drums to send signals to the battling warriors. The Nahuatl word for drum is roughly translated as huehuetl.
The Rig Veda, one of the oldest religious scriptures in the world, contain several references to the use of Dundhubi (war drum). Arya tribes charged into battle to the beating of the war drum and chanting of a hymn that appears in Book VI of the Rig Veda and also the Atharva Veda where it is referred to as the "Hymn to the battle drum".

</doc>
<doc id="7951" url="http://en.wikipedia.org/wiki?curid=7951" title="Delphi">
Delphi

Delphi ( or ; , )
is both an archaeological site and a modern town in Greece on the south-western spur of Mount Parnassus in the valley of Phocis. In myths dating to the classical period of Ancient Greece (510-323 BC), the site of Delphi was believed to be determined by Zeus when he sought to find the centre of his "Grandmother Earth" (Ge, Gaea, or Gaia). He sent two eagles flying from the eastern and western extremities, and the path of the eagles crossed over Delphi where the omphalos, or navel of Gaia was found. 
Earlier myths include traditions that Pythia, or the Delphic oracle, already was the site of an important oracle in the pre-classical Greek world (as early as 1400 BC) and, rededicated, served as the major site during classical times for the worship of the god Apollo after he slew Python, "a dragon" who lived there and protected the navel of the Earth. "Python" (derived from the verb pythein, "to rot") is claimed by some to be the original name of the site in recognition of Python which Apollo defeated. The Homeric Hymn to Delphic Apollo recalled that the ancient name of this site had been "Krisa". Others relate that it was named Pytho and that Pythia, the priestess serving as the oracle, was chosen from their ranks by a group of priestesses who officiated at the temple. 
Apollo's sacred precinct in Delphi was a panhellenic sanctuary, where every four years, starting in 586 BC athletes from all over the Greek world competed in the Pythian Games, one of the four panhellenic (or stephanitic) games, precursors of the Modern Olympics. The victors at Delphi were presented with a laurel crown ("stephanos") which was ceremonially cut from a tree by a boy who re-enacted the slaying of the Python. Delphi was set apart from the other games sites because it hosted the mousikos agon, musical competitions.
These Pythian Games rank second among the four stephanitic games chronologically and based on importance. These games, though, were different from the games at Olympia in that they were not of such vast importance to the city of Delphi as the games at Olympia were to the area surrounding Olympia. Delphi would have been a renowned city whether or not it hosted these games; it had other attractions that led to it being labeled the "omphalos" (navel) of the earth, in other words, the center of the world.
In the inner "hestia" ("hearth") of the Temple of Apollo, an eternal flame burned. After the battle of Plataea, the Greek cities extinguished their fires and brought new fire from the hearth of Greece, at Delphi; in the foundation stories of several Greek colonies, the founding colonists were first dedicated at Delphi.
Location.
The site of Delphi is located in upper central Greece, on multiple plateaux/terraces along the slope of Mount Parnassus, and includes the Sanctuary of Apollo, the site of the ancient Oracle. This semicircular spur is known as Phaedriades, and overlooks the Pleistos Valley.
Southwest of Delphi, about away, is the harbor-city of Kirrha on the Corinthian Gulf. Delphi was thought of by the Greeks as the middle of the entire earth.
Dedication to Apollo.
The name "Delphoi" comes from the same root as δελφύς "delphys", "womb" and may indicate archaic veneration of Gaia at the site. Apollo is connected with the site by his epithet Δελφίνιος "Delphinios", "the Delphinian". The epithet is connected with dolphins (Greek δελφίς,-ῖνος) in the Homeric "Hymn to Apollo" (line 400), recounting the legend of how Apollo first came to Delphi in the shape of a dolphin, carrying Cretan priests on his back. The Homeric name of the oracle is "Pytho" ("Πυθώ").
Another legend held that Apollo walked to Delphi from the north and stopped at Tempe, a city in Thessaly, to pick laurel (also known as bay tree) which he considered to be a sacred plant. In commemoration of this legend, the winners at the Pythian Games received a wreath of laurel picked in the Temple.
Delphi became the site of a major temple to Phoebus Apollo, as well as the Pythian Games and the famous prehistoric oracle. Even in Roman times, hundreds of votive statues remained, described by Pliny the Younger and seen by Pausanias.
Carved into the temple were three phrases: ("gnōthi seautón" = "know thyself") and ("mēdén ágan" = "nothing in excess"), and ("eggýa pára d'atē" = "make a pledge and mischief is nigh"), In ancient times, the origin of these phrases was attributed to one or more of the Seven Sages of Greece.
Additionally, according to Plutarch's essay on the meaning of the "E at Delphi"—the only literary source for the inscription—there was also inscribed at the temple a large letter E. Among other things epsilon signifies the number 5.
However, ancient as well as modern scholars have doubted the legitimacy of such inscriptions. According to one pair of scholars, "The actual authorship of the three maxims set up on the Delphian temple may be left uncertain. Most likely they were popular proverbs, which tended later to be attributed to particular sages."
According to the Homeric-hymn to the Pythian, Apollo shot his first arrow as an infant which effectively slew the serpent Pytho, the son of Gaia, who guarded the spot. To atone the murder of Gaia's son, Apollo was forced to fly and spend eight years in menial service before he could return forgiven. A festival, the Septerla, was held every year, at which the whole story was represented: the slaying of the serpent, and the flight, atonement, and return of the god.
The Pythian Games took place every four years to commemorate Apollo's victory. Another regular Delphi festival was the "Theophania" (Θεοφάνεια), an annual festival in spring celebrating the return of Apollo from his winter quarters in Hyperborea. The culmination of the festival was a display of an image of the gods, usually hidden in the sanctuary, to worshippers.
The "Theoxenia" was held each summer, centred on a feast for "gods and ambassadors from other states". Myths indicate that Apollo killed the chthonic serpent Python, Pythia in older myths, but according to some later accounts his wife, Pythia, who lived beside the Castalian Spring. Some sources say it is because Python had attempted to rape Leto while she was pregnant with Apollo and Artemis. 
This spring flowed toward the temple but disappeared beneath, creating a cleft which emitted chemical vapors that caused the Oracle at Delphi to reveal her prophecies. Apollo killed Python but had to be punished for it, since she was a child of Gaia. The shrine dedicated to Apollo was originally dedicated to Gaia and shared with Poseidon. The name Pythia remained as the title of the Delphic Oracle.
Erwin Rohde wrote that the Python was an earth spirit, who was conquered by Apollo, and buried under the Omphalos, and that it is a case of one deity setting up a temple on the grave of another. Another view holds that Apollo was a fairly recent addition to the Greek pantheon coming originally from Lydia . The Etruscans coming from northern Anatolia also worshipped Apollo, and it may be that he was originally identical with Mesopotamian Aplu, an Akkadian title meaning "son", originally given to the plague God Nergal, son of Enlil. Apollo Smintheus (Greek ), the mouse killer eliminates mice, a primary cause of disease, hence he promotes preventive medicine.
Oracle.
Delphi is perhaps best known for the oracle at the sanctuary that was dedicated to Apollo during the classical period. According to Aeschylus in the prologue of the "Eumenides", it had origins in prehistoric times and the worship of Gaea. In the last quarter of the 8th century BC there is a steady increase in artifacts found at the settlement site in Delphi, which was a new, post-Mycenaean settlement of the late 9th century. Pottery and bronze work as well as tripod dedications continue in a steady stream, in comparison to Olympia. Neither the range of objects nor the presence of prestigious dedications proves that Delphi was a focus of attention for a wide range of worshippers, but the large quantity of high value goods, found in no other mainland sanctuary, certainly encourages that view.
Apollo spoke through his oracle: the sibyl or priestess of the oracle at Delphi was known as the Pythia; she had to be an older woman of blameless life chosen from among the peasants of the area. She sat on a tripod seat over an opening in the earth. When Apollo slew Python, its body fell into this fissure, according to legend, and fumes arose from its decomposing body. Intoxicated by the vapors, the sibyl would fall into a trance, allowing Apollo to possess her spirit. In this state she prophesied. It has been speculated that a gas high in ethylene, known to produce violent trances, came out of this opening, though this theory remains debatable.
One theory states that a goat herder fed his flocks on Parnassus. It happened one day the goats started playing with great agility upon nearing a chasm in the rock; the goat herd noticing this held his head over the chasm causing the fumes to go to his brain; throwing him into a strange trance.
While in a trance the Pythia "raved" – probably a form of ecstatic speech – and her ravings were "translated" by the priests of the temple into elegant hexameters. People consulted the Delphic oracle on everything from important matters of public policy to personal affairs. The oracle could not be consulted during the winter months, for this was traditionally the time when Apollo would live among the Hyperboreans. Dionysus would inhabit the temple during his absence.
H.W. Parke writes that the foundation of Delphi and its oracle took place before recorded history and its origins are obscure, but dating to the worship of Gaia.
History.
The Delphic Oracle exerted considerable influence throughout the Greek world, and she was consulted before all major undertakings: wars, the founding of colonies, and so forth. She also was respected by the Greek-influenced countries around the periphery of the Greek world, such as Lydia, Caria, and even Egypt. The oracle was also known to the early Romans. Rome's seventh and last king, Lucius Tarquinius Superbus, after witnessing a snake near his palace, sent a delegation including two of his sons to consult the oracle.
The Oracle benefited from the patronage of the Macedonian kings. Later it was placed under the protection of the Aetolian League, which played the leading role in repelling the Gallic invasion of Brennus in 279 BC and saving the sanctuary. After a brief period the influence of the Roman Republic started to emerge, and they protected the Oracle from a dangerous barbarian invasion in 109 BC and 105 BC. A major reorganization was initiated, but was interrupted by the Mithridatic Wars and the wars of Sulla who took many rich offerings from the Oracle.
Barbarian invasions burned the Temple, which had been severely damaged by an earthquake in 83 BC. Thus the Oracle fell in decay and the surrounding area became impoverished. The sparse local population led to difficulties in filling the posts required. The Oracle's credibility waned due to doubtful predictions.
When Nero came to Greece in AD 66, he took away over 500 of the best statues from Delphi to Rome. Subsequent Roman emperors of the Flavian dynasty contributed significantly towards its restoration. Hadrian offered complete autonomy. Also Plutarch was a significant factor by his presence as a chief priest. Barbarian raids commenced again during the reign of Marcus Aurelius.
By the 4th century, Delphi had acquired the status of a city, which was located to the west of the sanctuary grounds. Constantine the Great looted several monuments, most notably the Tripod of Plataea, which he used to decorate his new capital, Constantinople. Despite the rise of Christianity across the Roman Empire, the oracle remained an active pagan centre throughout the 4th century, and the Pythian Games continued to be held at least until 424. Excavations have revealed a large three-aisled basilica in the city, as well as traces of a church building in the sanctuary's gymnasium. The site was abandoned in the 6th or 7th centuries, although a single bishop of Delphi is attested in an episcopal list of the late 8th/early 9th centuries.
The "Delphic Sibyl".
The Delphic Sibyl was a legendary prophetic figure who was said to have given prophecies at Delphi shortly after the Trojan War. The prophecies attributed to her circulated in written collections of prophetic sayings, along with the oracles of figures such as Bakis. The Sibyl had no connection to the oracle of Apollo, and should not be confused with the Pythia.
Buildings and structures.
Occupation of the site at Delphi can be traced back to the Neolithic period with extensive occupation and use beginning in the Mycenaean period (1600–1100 BCE). Most of the ruins that survive today date from the most intense period of activity at the site in the 6th century BCE.
Temple of Apollo.
The ruins of the Temple of Delphi visible today date from the 4th century BC, and are of a peripteral Doric building. It was erected on the remains of an earlier temple, dated to the 6th century BCE which itself was erected on the site of a 7th-century BC construction attributed to the architects Trophonios and Agamedes.
The 6th-century BC temple was named the "Temple of Alcmonidae" in tribute to the Athenian family who funded its reconstruction following a fire, which had destroyed the original structure. The new building was a Doric hexastyle temple of 6 by 15 columns. This temple was destroyed in 373 BC by an earthquake. The pediment sculptures are a tribute to Praxias and Androsthenes of Athens. Of a similar proportion to the second temple it retained the 6 by 15 column pattern around the stylobate. Inside was the adyton, the centre of the Delphic oracle and seat of Pythia. The temple had the statement "Know thyself", one of the Delphic maxims, carved into it (and some modern Greek writers say the rest were carved into it), and the maxims were attributed to Apollo and given through the oracle and/or the Seven Sages of Greece ("know thyself" perhaps also being attributed to other famous philosophers). The monument was partly restored during 1938(?)–1300.
The temple survived until 390 AD, when the Christian emperor Theodosius I silenced the oracle by destroying the temple and most of the statues and works of art in the name of Christianity. The site was completely destroyed by zealous Christians in an attempt to remove all traces of Paganism.
Amphictyonic Council.
The Amphictyonic Council was a council of representatives from six Greek tribes that controlled Delphi and also the quadrennial Pythian Games. They met biannually and came from Thessaly and central Greece. Over time, the town of Delphi gained more control of itself and the council lost much of its influence.
Treasuries.
From the entrance of the site, continuing up the slope almost to the temple itself, are a large number of votive statues, and numerous treasuries. These were built by the various Greek city states — those overseas as well as those on the mainland — to commemorate victories and to thank the oracle for her advice, which was thought to have contributed to those victories. They are called "treasuries" because they held the offerings made to Apollo; these were frequently a "tithe" or tenth of the spoils of a battle. The most impressive is the now-restored Athenian Treasury, built to commemorate the Athenians' victory at the Battle of Marathon in 490 BCE.
Several of the treasuries can be identified, among them the Siphnian Treasury, dedicated by the city of Siphnos whose citizens gave a tithe of the yield from their gold mines until the mines came to an abrupt end when the sea flooded the workings.
Other identifiable treasuries are those of the Sikyonians, the Boeotians, the Thebans, and the Athenians. One of the largest of the treasuries was that of Argos. Built in the late Doric period, the Argives took great pride in establishing their place amongst the other city states. Completed in 380, the treasury draws inspiration mostly from the Temple of Hera located in the Argolis, the acropolis of the city. However, recent analysis of the Archaic elements of the treasury suggest that its founding preceded this. Currently the rebuilt Treasury of the Athenians is the most impressive. Much of the architectural program is on display in the nearby museum.
Altar of the Chians.
Located in front of the Temple of Apollo, the main altar of the sanctuary was paid for and built by the people of Chios. It is dated to the 5th century BC by the inscription on its cornice. Made entirely of black marble, except for the base and cornice, the altar would have made a striking impression. It was restored in 1920.
Stoa of the Athenians.
The stoa leads off north-east from the main sanctuary. It was built in the Ionic order and consists of seven fluted columns, unusually carved from single pieces of stone (most columns were constructed from a series of discs joined together). The inscription on the stylobate indicates that it was built by the Athenians after their naval victory over the Persians in 478 BC, to house their war trophies. The rear wall of the stoa contains nearly a thousand inscriptions; supposedly any slave manumitted in Athens was obliged to record a short biography here, explaining why he had deserved his freedom.
Sibyl rock.
The Sibyl rock is a pulpit-like outcrop of rock between the Athenian Treasury and the Stoa of the Athenians upon the sacred way which leads up to the temple of Apollo in the archaeological area of Delphi. It is claimed to be where an ancient Sibyl pre-dating the Pythia of Apollo sat to deliver her prophecies.
Theatre.
The ancient theatre at Delphi was built further up the hill from the Temple of Apollo giving spectators a view of the entire sanctuary and the valley below. It was originally built in the 4th century BC but was remodeled on several occasions since. Its 35 rows can seat 5,000 spectators.
Tholos.
The Tholos at the sanctuary of Athena Pronoia (Ἀθηνᾶ Πρόνοια, "Athena of forethought") is a circular building that was constructed between 380 and 360 BCE. It consisted of 20 Doric columns arranged with an exterior diameter of 14.76 meters, with 10 Corinthian columns in the interior.
The Tholos is located approximately a half a mile (800 m) from the main ruins at Delphi. Three of the Doric columns have been restored, making it the most popular site at Delphi for tourists to take photographs.
Vitruvius (vii, introduction) notes Theodorus of Samos as the architect of the Round Building which is at Delphi.
Gymnasium.
The gymnasium, which is half a mile away from the main sanctuary, was a series of buildings used by the youth of Delphi. The building consisted of two levels: a stoa on the upper level providing open space, and a palaestra, pool and baths on lower floor. These pools and baths were said to have magical powers, and imparted the ability to communicate to Apollo himself.
Stadium.
The stadium is located further up the hill, beyond the "via sacra" and the theatre. It was originally built in the 5th century BC but was altered in later centuries. The last major remodeling took place in the 2nd century AD under the patronage of Herodes Atticus when the stone seating was built and (arched) entrance. It could seat 6500 spectators and the track was 177 metres long and 25.5 metres wide.
Hippodrome.
The hippodrome of Delphi was the location where the running events took place during the Pythian Games. No trace of it has been found, but the location of the stadium and some remnants of retaining walls lead to the conclusion that is was set on a plain apart from the main part of the city and well away from the Peribolos of Apollo.
Polygonal wall.
The retaining wall was built to support the terrace housing the construction of the second temple of Apollo in 548 BCE. Its name is taken from the polygonal masonry of which it is constructed.
Castalian spring.
The sacred spring of Delphi lies in the ravine of the Phaedriades. The preserved remains of two monumental fountains that received the water from the spring date to the Archaic period and the Roman, with the latter cut into the rock.
Athletic statues.
Delphi is famous for its many preserved athletic statues. It is known that Olympia originally housed far more of these statues, but time brought ruin to many of them, leaving Delphi as the main site of athletic statues. Kleobis and Biton, two brothers renowned for their strength, are modeled in two of the earliest known athletic statues at Delphi. The statues commemorate their feat of pulling their mother's cart several miles to the Sanctuary of Hera in the absence of oxen. The neighbors were most impressed and their mother asked Hera to grant them the greatest gift. When they entered Hera's temple, they fell into a slumber and never woke, dying at the height of their admiration, the perfect gift.
The Charioteer of Delphi is another ancient relic that has withstood the centuries. It is one of the best known statues from antiquity. The charioteer has lost many features, including his chariot and his left arm, but he stands as a tribute to athletic art of antiquity.
Architectural traditions.
Ancient tradition accounted for four temples that successively occupied the site before the 548/7 BC fire, following which the Alcmaeonids built a fifth. The poet Pindar celebrated the Alcmaeonid's temple in "Pythian" 7.8-9 and he also provided details of the third building ("Paean" 8. 65-75). Other details are given by Pausanias (10.5.9-13) and the Homeric Hymn to Apollo (294 ff.). The first temple was said to have been constructed out of olive branches from Tempe. The second was made by bees out of wax and wings but was miraculously carried off by a powerful wind and deposited among the Hyperboreans. The third, as described by Pindar, was created by the gods Hephaestus and Athena, but its architectural details included Siren-like figures or 'Enchantresses', whose baneful songs eventually provoked the Olympian gods to bury the temple in the earth (according to Pausanias, it was destroyed by earthquake and fire). In Pindar's words, addressed to the Muses:<br>
<br>
<br>
The fourth temple was said to have been constructed from stone by Trophonius and Agamedes.
Excavations.
The site had been occupied by the village of Kastri since medieval times. Before a systematic excavation of the site could be undertaken, the village had to be relocated but the residents resisted. The opportunity to relocate the village occurred when it was substantially damaged by an earthquake, with villagers offered a completely new village in exchange for the old site. In 1893 the French Archaeological School removed vast quantities of soil from numerous landslides to reveal both the major buildings and structures of the sanctuary of Apollo and of Athena Pronoia along with thousands of objects, inscriptions and sculptures.
The site is now an archaeological one, and a very popular tourist destination. It is easily accessible from Athens as a day trip, and is often combined with the winter sports facilities available on Mount Parnassus, as well as the beaches and summer sports facilities of the nearby coast of Phocis.
The site is also protected as a site of extraordinary natural beauty, and the views from it are also protected: no industrial artefacts are to be seen from Delphi other than roads and traditional architecture residences (for example high voltage power lines and the like are routed so as to be invisible from the area of the sanctuary).
Modern Delphi.
Town.
Modern Delphi is situated immediately west of the archaeological site and hence is a popular tourist destination. It is on a major highway linking Amfissa along with Itea and Arachova. There are many hotels and guest houses in the town, and many taverns and bars. The main streets are narrow, and often one-way. The E4 European long distance path passes through the east end of the town. In addition to the archaeological interest, Delphi attracts tourists visiting the Parnassus Ski Center and the popular coastal towns of the region.
In the Middle Ages a town called Kastri was built on the archaeological site. The residents had used the marble columns and structures as support beams and roofs for their improvised houses, a usual way of rebuilding towns that were partially or totally destroyed, especially after the earthquake in 1580, which demolished several towns in Phocis. In 1893 archaeologists from the École française d'Athènes finally located the actual site of ancient Delphi and the village was moved to a new location, west of the site of the temples.
The Delphi Archaeological Museum is at the foot of the main archaeological complex, on the east side of the village, and on the north side of the main road. The museum houses an impressive collection associated with ancient Delphi, including the earliest known notation of a melody, the famous Charioteer, golden treasures discovered beneath the Sacred Way, and fragments of reliefs from the Siphnian Treasury. Immediately adjacent to the exit (and overlooked by most tour guides) is the inscription that mentions the Roman proconsul Gallio.
Entries to the museum and to the main complex are separate and chargeable, and a reduced rate ticket gets entry to both. There is a small cafe, and a post office by the museum. Slightly further east, on the south side of the main road, is the Gymnasium and the Tholos. Entry to these is free.
Municipality.
The municipality Delphi was formed at the 2011 local government reform by the merger of the following 8 former municipalities, that became municipal units:
The administrative seat of the municipality is in the largest town, Amfissa. The total population of the municipality is 32,263. The town Delphi has a population of 2,373 people while the population of the municipal unit of Delphi, including Chrisso (ancient Krissa), is 3,511.
See also.
General:

</doc>
<doc id="7952" url="http://en.wikipedia.org/wiki?curid=7952" title="Digital Equipment Corporation">
Digital Equipment Corporation

Digital Equipment Corporation, also known as DEC and using the trademark Digital, was a major American company in the computer industry from the 1960s to the 1990s. It was a leading vendor of computer systems, including computers, software, and peripherals, and its PDP and successor VAX products were the most successful of all minicomputers in terms of sales.
From 1957 until 1992 its headquarters were located in a former wool mill in Maynard, Massachusetts, since renamed Clock Tower Place and now home to multiple companies. DEC was acquired in June 1998 by Compaq, which subsequently merged with Hewlett-Packard in May 2002. Some parts of DEC, notably the compiler business and the Hudson, Massachusetts facility, were sold to Intel.
Digital Equipment Corporation should not be confused with the unrelated companies Digital Research, Inc or Western Digital, although the latter manufactured the LSI-11 chipsets used in DEC's low end PDP-11/03 computers.
Overview.
Initially focusing on the small end of the computer market allowed DEC to grow without its potential competitors making serious efforts to compete with them. Their PDP series of machines became popular in the 1960s, especially the PDP-8, widely considered to be the first successful minicomputer. Looking to simplify and update their line, DEC replaced most of their smaller machines with the PDP-11 in 1970, eventually selling over 600,000 units and cementing DECs position in the industry. Originally designed as a follow-on to the PDP-11, DEC's VAX-11 series was the first widely used 32-bit minicomputer, sometimes referred to as "superminis". These were able to compete in many roles with larger mainframe computers, such as the IBM System/370. The VAX was a best-seller, with over 400,000 sold, and its sales through the 1980s propelled the company into the second largest in the industry. At its peak, DEC was the second largest employer in Massachusetts, second only to the state government.
The rapid rise of the business microcomputer in the late 1980s, and especially the introduction of powerful 32-bit systems in the 1990s, quickly eroded the value of DEC's systems. DEC's last major attempt to find a space in the rapidly changing market was the DEC Alpha 64-bit RISC processor architecture. DEC initially started work on Alpha as a way to re-implement their VAX series, but also employed it in a range of high-performance workstations. Although the Alpha processor family met both of these goals, and, for most of its lifetime, was the fastest processor family on the market, extremely high asking prices were outsold by lower priced x86 chips from Intel and clones such as AMD.
The company was acquired in June 1998 by Compaq, in what was at that time the largest merger in the history of the computer industry. At the time, Compaq was focused on the enterprise market and had recently purchased several other large vendors. DEC was a major player overseas where Compaq had less presence. However, Compaq had little idea what to do with its acquisitions, and soon found itself in financial difficulty of its own. The company subsequently merged with Hewlett-Packard in May 2002. some of DEC's product lines were still produced under the HP name.
History.
Origins.
Ken Olsen and Harlan Anderson were two engineers who had been working at MIT Lincoln Laboratory on the lab's various computer projects. The Lab is best known for their work on what would today be known as "interactivity", and their machines were among the first where operators had direct control over programs running in real time. These had started in 1944 with the famed Whirlwind which was originally developed to make a flight simulator for the US Navy, although this was never completed. Instead, this effort evolved into the SAGE system for the US Air Force, which used large screens and light guns to allow operators to interact with radar data stored in the computer.
When the Air Force project wound down, the Lab turned their attention to an effort to build a version of the Whirlwind using transistors in place of vacuum tubes. In order to test their new circuitry, they first built a small 18-bit machine known as TX-0 which first ran in 1956. When the TX-0 successfully proved the basic concepts, attention turned to a much larger system, the 36-bit TX-2 with a then-enormous 64 kWords of core memory. Core was so expensive that parts of TX-0's memory were stripped for the TX-2, and what remained of the TX-0 was then given to MIT on permanent loan.
At MIT, Olsen and Anderson noticed something odd: students would line up for hours to get a turn to use the stripped-down TX-0, while largely ignoring a faster IBM machine that was also available. The two decided that the draw of interactive computing was so strong that they felt there was a market for a small machine dedicated to this role, essentially a commercialized TX-0. They could sell this to users where graphical output or realtime operation would be more important than outright performance. Additionally, as the machine would cost much less than the larger systems then available, it would also be able to serve users that needed a lower-cost solution dedicated to a specific task, where a larger 36-bit machine would not be needed.
In 1957 when the pair and Ken's brother Stan went looking for capital, they found that the American business community was hostile to investing in computer companies. Many smaller computer companies had come and gone in the 1950s, wiped out when new technical developments rendered their platforms obsolete, and even large companies like RCA and General Electric were failing to make a profit in the market. The only serious expression of interest came from Georges Doriot and his American Research and Development Corporation (AR&D). Worried that a new computer company would find it difficult to arrange further financing, Doriot suggested the fledgling company change its business plan to focus less on computers, and even change their name from "Digital Computer Corporation".
The pair returned with an updated business plan that outlined two phases for the company's development. They would start by selling computer modules as stand-alone devices that could be purchased separately and wired together to produce a number of different digital systems for lab use. Then, if these "digital modules" were able to build a self-sustaining business, the company would be free to use them to develop a complete computer in their Phase II. The newly christened "Digital "Equipment" Corporation" received $70,000 from AR&D for a 70% share of the company, and began operations in a Civil War era textile mill in Maynard, Massachusetts, where plenty of inexpensive manufacturing space was available.
Digital modules.
In early 1958 DEC shipped its first products, the "Digital Laboratory Module" line. The Modules consisted of a number of individual electronic components and germanium transistors mounted to a circuit board, the actual circuits being based on those from the TX-2.
The Laboratory Modules were packaged in an extruded aluminum housing, intended to sit on an engineer's workbench, although a rack-mount bay was sold that held 9 laboratory modules. They were then connected together using banana plug patch cords inserted at the front of the modules. Three versions were offered, running at 5 MHz (1957), 500 kHz (1959), or 10 MHz (1960). The Modules proved to be in high demand in other computer companies, who used them to build equipment to test their own systems. Despite the recession of the late 1950s, the company sold $94,000 worth of these modules during 1958 alone, turning a profit at the end of its first year.
The original Laboratory Modules were soon supplemented with the "Digital Systems Module" line, which were identical internally but packaged differently. The Systems Modules were designed with all of the connections at the back of the module using 22-pin Amphenol connectors, and were attached to each other by plugging them into a backplane that could be mounted in a 19-inch rack. The backplanes allowed 25 modules in a single 5-1/4 inch section of rack, and allowed the high densities needed to build a computer.
The original laboratory and system module lines were offered in 500 kilocycle, 5 megacycle and 10 megacycle versions. In all cases, the supply voltages were -15 and +10 volts, with logic levels of -3 volts (passive pull-down) and 0 volts (active pull-up).
DEC used the Systems Modules to build their "Memory Test" machine for testing core memory systems, selling about 50 of these pre-packaged units over the next eight years. The PDP-1 and LINC computers were also built using Systems Modules (see below).
Modules were part of DEC's product line into the 1970s, although they went through several evolutions during this time as technology changed. The same circuits were then packaged as the first "R" (red) series "Flip-Chip" modules. Later, other module series provided additional speed, much higher logic density, and industrial I/O capabilities. Digital published extensive data about the modules in free catalogs that became very popular.
PDP-1 family.
With the company established and a successful product on the market, DEC turned its attention to the computer market once again as part of its planned "Phase II". In August 1959, Ben Gurley started design of the company's first computer, the PDP-1. In keeping with Doriot's instructions, the name was an initialism for "Programmable Data Processor", leaving off the term "computer". As Gurley put it, "We aren't building computers, we're building 'Programmable Data Processors'." The prototype was first shown publicly at the Joint Computer Conference in Boston in December 1959. The first PDP-1 was delivered to Bolt, Beranek and Newman in November 1960, and formally accepted the next April. The PDP-1 sold in basic form for $120,000, or about $900,000 in 2011 US dollars. By the time production ended in 1969, 53 PDP-1s had been delivered.
The PDP-1 was supplied standard with 4096 words of core memory, 18-bits per word, and ran at a basic speed of 100,000 operations per second. It was constructed using many System Building Blocks that were packaged into several 19-inch racks. The racks were themselves packaged into a single large mainframe case, with a hexagonal control panel containing switches and lights mounted to lay at table-top height at one end of the mainframe. Above the control panel was the system's standard input/output solution, a punch tape reader and writer. Most systems were purchased with two peripherals, the Type 30 vector graphics display, and a Soroban Engineering modified IBM Model B Electric typewriter that was used as a printer. The Soroban system was notoriously unreliable, and often replaced with a modified Friden Flexowriter, which also contained its own punch tape system. A variety of more-expensive add-ons followed, including magnetic tape systems, punched card readers and punches, and faster punch tape and printer systems.
When DEC introduced the PDP-1, they also mentioned larger machines at 24, 30 and 36 bits, based on the same design. During construction of the prototype PDP-1, some design work was carried out on a 24-bit PDP-2, and the 36-bit PDP-3. Although the PDP-2 never proceeded beyond the initial design, the PDP-3 found some interest and was designed in full. Only one PDP-3 appears to have been built, in 1960, by the CIA's Scientific Engineering Institute (SEI) in Waltham, Massachusetts. According to the limited information available, they used it to process radar cross section data for the Lockheed A-12 reconnaissance aircraft. Gordon Bell remembered that it was being used in Oregon some time later, but could not recall who was using it.
In November 1962 DEC introduced the $65,000 PDP-4. The PDP-4 was similar to the PDP-1 and used a similar instruction set, but used slower memory and different packaging to lower the price. Like the PDP-1, about 54 PDP-4's were eventually sold, most to a customer base similar to the original PDP-1.
In 1964 DEC introduced its new Flip Chip module design, and used it to re-implement the PDP-4 as the PDP-7. The PDP-7 was introduced in December 1964, and about 120 were eventually produced. An upgrade to the Flip Chip led to the R series, which in turn led to the PDP-7A in 1965. The PDP-7 is most famous as the original machine for the Unix operating system.
A more dramatic upgrade to the PDP-1 series was introduced in August 1966, the PDP-9. The PDP-9 was instruction compatible with the PDP-4 and −7, but ran about twice as fast as the −7 and was intended to be used in larger deployments. At only $19,900 in 1968, the PDP-9 was a big seller, eventually selling 445 machines, more than all of the earlier models combined.
Even while the PDP-9 was being introduced, its replacement was being designed, and was introduced as 1969's PDP-15, which re-implemented the PDP-9 using integrated circuits in place of modules. Much faster than the PDP-9 even in basic form, the PDP-15 also included a floating point unit and a separate input/output processor for further performance gains. Over 400 PDP-15's were ordered in the first eight months of production, and production eventually amounted to 790 examples in 12 basic models. However, by this time other machines in DEC's lineup could fill the same niche at even lower price points, and the PDP-15 would be the last of the 18-bit series.
PDP-8 family.
In 1962, Lincoln Laboratory used a selection of System Building Blocks to implement a small 12-bit machine, and attached it to a variety of analog-to-digital (A to D) input/output (I/O) devices that made it easy to interface with various analog lab equipment. The LINC proved to attract intense interest in the scientific community, and has since been referred to as the first real minicomputer, a machine that was small and inexpensive enough to be dedicated to a single task even in a small lab.
Seeing the success of the LINC, in 1963 DEC took the basic logic design but stripped away the extensive A to D systems to produce the PDP-5. The new machine, the first outside the PDP-1 mould, was introduced at WESTCON on 11 August 1963. A 1964 ad expressed the main advantage of the PDP-5, "Now you can own the PDP-5 computer for what a core memory alone used to cost: $27,000" 116 PDP-5s were produced until the lines were shut down in early 1967. Like the PDP-1 before it, the PDP-5 inspired a series of newer models based on the same basic design that would go on to be more famous than its parent.
On 22 March 1965, DEC introduced the PDP-8, which replaced the PDP-5's modules with the new R-series modules using Flip Chips. The machine was re-packaged into a small tabletop case, which remains distinctive for its use of smoked plastic over the CPU which allowed one to easily see the wire-wrapped internals of the CPU. Sold standard with 4 kWords of 12-bit core memory and a Teletype Model 33 ASR for basic input/output, the machine listed for only $18,000. The PDP-8 is referred to as the first "real" minicomputer because of its sub-$25,000 price. Sales were, unsurprisingly, very strong, and helped by the fact that several competitors had just entered the market with machines aimed directly at the PDP-5's market space, which the PDP-8 trounced. This gave the company two years of unrestricted leadership, and eventually 1450 "straight eight" machines were produced before it was replaced by newer implementations of the same basic design.
DEC hit an even lower price-point with the PDP-8/S, the S for "serial". As the name implies the /S used a serial arithmetic unit, which was much slower but reduced costs so much that the system sold for under $10,000. DEC then used the new PDP-8 design as the basis for a new LINC, the two-processor LINC-8. The LINC-8 used one PDP-8 CPU and a separate LINC CPU, and included instructions to switch from one to the other. This allowed customers to run their existing LINC programs, or "upgrade" to the PDP-8, all in software. Although not a huge seller, 142 LINC-8s were sold starting at $38,500. Like the original LINC to PDP-5 evolution, the LINC-8 was then modified into the single-processor PDP-12, adding another 1000 machines to the 12-bit family. Newer circuitry designs led to the PDP-8/I and PDP-8/L in 1968. In 1975, one year after an agreement between Digital and Intersil, the Intersil 6100 chip was launched, effectively a PDP-8 on a chip. This was a way to allow PDP-8 software to be run even after the official end-of-life announcement for the Digital PDP-8 product line.
PDP-10 family.
While the PDP-5 introduced a lower-cost line, 1963's PDP-6 was intended to take DEC into the mainframe market with a 36-bit machine. However, the PDP-6 proved to be a "hard sell" with customers, as it offered few advantages over similar machines from the better established vendors like IBM or Honeywell, in spite of its low cost around $300,000. Only 23 were sold, or 26 depending on the source, and unlike earlier models the low sales meant the PDP-6 was not improved with intermediate versions. However, the PDP-6 is historically important as the platform that introduced "Monitor", an early time-sharing operating system that would evolve into the widely used TOPS-10.
In spite of the PDP-6's limited commercial success, it introduced many features that clearly had commercial benefit. When the Flip Chip packaging allowed the PDP-6 to be re-implemented at a much lower cost, DEC took the opportunity to carry out a similar evolution of their 36-bit design and introduced the PDP-10 in 1968. The PDP-10 was as much a success as the PDP-6 was a failure; during its lifetime about 700 mainframe PDP-10s were sold before production ended in 1984. The PDP-10 was widely used in university settings, and thus was the basis of many advances in computing and operating system design during the 1970s. DEC later re-branded all of the models in the 36-bit series as the "DECsystem-10", and PDP-10s are generally referred to by the model of their CPU, like "KA10", soon upgraded to the "KI10" (I:Integrated Circuits Version); then to "KL10" (L:Large Scale Integration - ECL logics Version); also the "KS10" (S: Semiconductor Version). Later, unified the product lines upgrades produced the compatible DECSYSTEM-20, along with TOPS-20 that included virtual memory. The Jupiter Project was supposed to continue the Mainframe product line into the future by using "Gate Arrays" with the innovating Air Mover Cooling System, coupled with built in Floating Point Processing Engine called "FBOX", delivering a top notch scientific computing niche, yet the critical performance measurement was based upon COBOL compilation which did not fully utilize the primary design features of "Jupiter"; Though "Jupiter Project" failed, the engineers immediately adapted the 36 bits design into 32 bits design and seamlessly came out as VAX8600 released in 1985.
DECtape.
One of the most unusual peripherals produced for the PDP-10 was the DECtape. The DECtape was a length of special 3/4-inch wide magnetic tape wound on 5-inch reels. The recording format was a 10-track approach using fixed-length numbered 'blocks' organized into a standard file structure, including a directory. Files could be written, read, changed, and deleted on a DECtape as though it were a disk drive. For greater efficiency, the DECtape drive could read and write to a DECtape in both directions.
In fact, some PDP-10 systems had no disks at all, using DECtapes alone for their primary data storage. The DECtape was also widely used on other PDP models, since it was much easier to use than hand-loading multiple paper tapes. Primitive early time-sharing systems could use DECtapes as system devices and swapping devices. Although superior to paper tape, DECtapes were relatively slow, and were supplanted as reliable disk drives became affordable.
PDP-11.
The PDP-11 16-bit computer was designed in a crash program by Harold McFarland, Gordon Bell, Roger Cady, and others. The project was able to leap forward in design with the arrival of Harold McFarland, who had been researching 16-bit designs at Carnegie Mellon University. One of his simpler designs became the PDP-11, although when they first viewed the proposal, management was not impressed and almost cancelled it.
In particular, the new design did not include many of the addressing modes that were intended to make programs smaller in memory, a technique that was widely used on other DEC machines and CISC designs in general. This would mean the machine would spend more time accessing memory, which would slow it down. However, the machine also extended the idea of multiple "General Purpose Registers" (GPRs), which gave the programmer flexibility to use these high-speed memory caches as they needed, potentially addressing the performance issues.
A major advance in the PDP-11 design was Digital's Unibus, which supported all peripherals through memory mapping. This allowed a new device to be added easily, generally only requiring plugging a hardware interface board into the backplane and possibly adding a jumper to the wire wrapped backplane, and then installing software that read and wrote to the mapped memory to control it. The relative ease of interfacing spawned a huge market of third party add-ons for the PDP-11, which made the machine even more useful.
The combination of architectural innovations proved superior to competitors and the "11" architecture was soon the industry leader, propelling DEC back to a strong market position. The design was later expanded to allow paged physical memory and memory protection features, useful for multitasking and time-sharing. Some models supported separate instruction and data spaces for an effective virtual address size of 128 kB within a physical address size of up to 4 MB. Smaller PDP-11s, implemented as single-chip CPUs, continued to be produced until 1996, by which time over 600,000 had been sold.
The PDP-11 supported several operating systems, including Bell Labs' new Unix operating system as well as DEC's DOS-11, RSX-11, IAS, RT-11, DSM-11, and RSTS/E. Many early PDP-11 applications were developed using standalone paper-tape utilities. DOS-11 was the PDP-11's first disk operating system, but was soon supplanted by more capable systems. RSX provided a general-purpose multitasking environment and supported a wide variety of programming languages. IAS was a time-sharing version of RSX-11D. Both RSTS and Unix were time-sharing systems available to educational institutions at little or no cost, and these PDP-11 systems were destined to be the "sandbox" for a rising generation of engineers and computer scientists. Large numbers of PDP-11/70s were deployed in telecommunications and industrial control applications. AT&T Corporation became DEC's largest customer.
RT-11 provided a practical real-time operating system in minimal memory, allowing the PDP-11 to continue Digital's critical role as a computer supplier for embedded systems. Historically, RT-11 also served as the inspiration for many microcomputer OS's, as these were generally being written by programmers who cut their teeth on one of the many PDP-11 models. For example, CP/M used a command syntax similar to RT-11's, and even retained the awkward PIP program used to copy data from one computer device to another. As another historical footnote, DEC's use of "/" for "switches" (command-line options) would lead to the adoption of "\" for pathnames in MS-DOS and Microsoft Windows as opposed to "/" in Unix.
The evolution of the PDP-11 followed earlier systems, eventually including a single-user deskside personal computer form, the MicroPDP-11. In total, around 600,000 PDP-11s of all models were sold. and a wide variety of third-party peripheral vendors had also entered the computer product ecosystem.
VAX.
In 1976, DEC decided to extend the PDP-11 architecture to 32 bits while adding a complete virtual memory system to the simple paging and memory protection of the PDP-11. The result was the VAX architecture, where VAX stands for Virtual Address eXtension (from 16 to 32 bits). The first computer to use a VAX CPU was the VAX-11/780, which DEC referred to as a "superminicomputer". Although it was not the first 32-bit minicomputer, the VAX-11/780's combination of features, price, and marketing almost immediately propelled it to a leadership position in the market after it was released in 1978. VAX systems were so successful that in 1983, DEC canceled its Jupiter project, which had been intended to build a successor to the PDP-10 mainframe, and instead focused on promoting the VAX as the single computer architecture for the company.
Supporting the VAX's success was the VT52, one of the most successful smart terminals. Building on earlier less successful models (the VT05 and VT50), the VT52 was the first terminal that did everything one might want in a single chassis. The VT52 was followed by the even more successful VT100 and its follow-ons, making DEC one of the largest terminal vendors in the industry. With the VT series, DEC could now offer a complete top-to-bottom system from computer to all peripherals, which formerly required collecting the required devices from different suppliers.
The VAX processor architecture and family of systems evolved and expanded through several generations during the 1980s, culminating in the NVAX microprocessor implementation and VAX 7000/10000 series in the early 1990s.
Early microcomputers.
The introduction of the first general purpose microprocessors inevitably led to the first microcomputers around 1975. At the time these systems were of limited utility, and Ken Olsen famously derided them in 1977, stating "There is no reason for any individual to have a computer in his home." Unsurprisingly, DEC did not put much effort into the microcomputer area in the early days of the market. Interestingly in 1977, the Heathkit H11 was announced; a PDP-11 in kit form. At the beginning of the 1980s, DEC built the VT180 (codenamed "Robin"), which was a VT100 terminal with an added Z80-based microcomputer running CP/M, but this product was initially available only to DEC employees.
It was only after IBM had successfully launched the IBM PC in 1981 that DEC responded with their own systems. In 1982, Digital introduced not one, but three incompatible machines which were each tied to different proprietary architectures. The first, the DEC Professional, was based on the PDP-11/23 (and later, the 11/73) running the RSX-11M+ derived, but menu-driven, P/OS ("Professional Operating System"). This DEC machine easily outperformed the PC, but was more expensive than, and completely incompatible with IBM PC hardware and software, offering far fewer options for customizing a system.
Unlike CP/M and DOS microcomputers, every copy of every program for the Professional had to be provided with a unique key for the particular machine and CPU for which it was bought. At that time this was mainstream policy, because most computer software was either bought from the company that built the computer or custom-constructed for one client. However, the emerging third-party software industry disregarded the PDP-11/Professional line and concentrated on other microcomputers where distribution was easier. At DEC itself, creating better programs for the Professional was not a priority, perhaps from fear of cannibalizing the PDP-11 line. As a result the Professional was a superior machine, running inferior software. In addition, a new user would have to learn an awkward, slow, and inflexible menu-based user interface which appeared to be radically different from PC DOS or CP/M, which were more commonly used on the 8080 and 8088 based microcomputers of the time. A second offering, the DECmate II was the latest version of the PDP-8 based word processors, but not really suited to general computing, nor competitive with Wang Laboratories' popular word processing equipment.
The best known of DEC's early microcomputers was the dual-processor (Z80 and 8088) Rainbow 100, which ran the 8-bit CP/M operating system on the Z80 and the 16-bit CP/M-86 operating system on the Intel 8086 processor. It could also run a UNIX System III implementation called VENIX. Applications from standard CP/M could be re-compiled for the Rainbow, but by this time users were expecting custom-built (pre-compiled binary) applications such as Lotus 1-2-3, which was eventually ported along with MS-DOS 2.0 and introduced in late 1983. Although the Rainbow generated some press, it was unsuccessful due to its high price and lack of marketing and sales support.
The way the DEC standard RX50 floppy disk drive supported DEC's initial offerings seemed to encapsulate their approach to the personal computer market. Although the mechanical drive hardware was nearly identical to other 5¼" floppy disk drives available on competing systems, DEC sought to differentiate their product by using a proprietary disk format for the data written on the disk. The DEC format had a higher capacity for data, but the RX50 drives were incompatible with other PC floppy drives. This required DEC owners to buy higher-priced, specially formatted floppy media, which was harder to obtain through standard distribution channels. DEC attempted to enforce exclusive control over its floppy media sales by copyrighting its proprietary disk format, and requiring a negotiated license agreement and royalty payments from anybody selling compatible media. The proprietary data format meant that RX50 floppies were not interchangeable with other PC floppies, further isolating DEC products from the developing de facto standard PC market. Hardware hackers and DEC enthusiasts eventually reverse-engineered the RX50 format, but the damage had already been done, in terms of market confusion and isolation.
A further system was introduced in 1986 as the VAXmate, which included Microsoft Windows 1.0 and used VAX/VMS-based file and print servers along with integration into DEC's own DECnet-family, providing LAN/WAN connection from PC to mainframe or supermini. The VAXmate replaced the Rainbow, and in its standard form was the first widely marketed diskless workstation.
Networking and clusters.
In 1984, DEC launched its first 10 Mbit/s Ethernet. Ethernet allowed scalable networking, and VAXcluster allowed scalable computing. Combined with DECnet and Ethernet-based terminal servers (LAT), DEC had produced a networked storage architecture which allowed them to compete directly with IBM. Ethernet replaced token ring, and went on to become the dominant networking model in use today.
In September 1985, DEC became the fifth company to register a .com domain name (dec.com).
Along with the hardware and protocols, DEC also introduced the VAXcluster concept, which allowed several VAX machines to be tied together into a single larger storage system. VAXclusters allowed a DEC-based company to scale their services by adding new machines to the cluster at any time, as opposed to buying a faster machine and using that to replace a slower one. The flexibility this offered was compelling, and allowed DEC to attack high-end markets formerly out of their reach.
Diversification.
Although their microcomputer efforts were eventually considered failures, the PDP-11 and VAX lines continued to sell in record numbers. Better yet, DEC was competing very well against the market leader, IBM, taking an estimated $2 billion away from them in the mid-80s. In 1986, Digital's profits rose 38 percent when the rest of the computer industry experienced a downturn, and by 1987 the company was threatening IBM's number one position in the computer industry.
At its peak, Digital was the second-largest computer company in the world, with over 100,000 employees. It was during this time that the company branched out development into a wide variety of projects that were far from its core business in computer equipment. The company invested heavily in custom software. In the 1970s and earlier most software was custom-written to serve a specific task, but by the 1980s the introduction of relational databases and similar systems allowed powerful software to be built in a modular fashion, potentially saving enormous amounts of development time. Software companies like Oracle became the new darlings of the industry, and DEC started their own efforts in every "hot" niche, in some cases several projects for the same niche. Some of these products competed with DEC's own partners, notably Rdb which competed with Oracle's products on the VAX, part of a major partnership only a few years earlier.
Although many of these products were well designed, most of them were DEC-only or DEC-centric, and customers frequently ignored them and used third-party products instead. This problem was further exacerbated by Olsen's aversion to traditional advertising and his belief that well-engineered products would sell themselves. Hundreds of millions of dollars were spent on these projects, at the same time that workstations using RISC microprocessors were starting to approach VAX CPUs in performance.
Faltering in the market.
As microprocessors continued to improve in the 1980s, it soon became clear that the next generation would offer performance and features equal to the best of DECs low-end minicomputer lineup. Worse, the Berkeley RISC and Stanford MIPS designs were aiming to introduce 32-bit designs that would outperform the fastest members of the VAX family, DEC's cash cow.
Constrained by the huge success of their VAX/VMS products, which followed the proprietary model, the company was very late to respond to these threats. In the early 1990s, DEC found its sales faltering and its first layoffs followed. The company that created the minicomputer, a dominant networking technology, and arguably the first computers for personal use, had abandoned the "low end" market, whose dominance with the PDP-8 had built the company in a previous generation. Decisions about what to do about this threat led to infighting within the company that seriously delayed their responses.
One group suggested that every possible development in the industry be poured into the construction of a new VAX family that would leapfrog the performance of the existing machines. This would limit the market erosion in the top-end segment, where profit margins were maximized and DEC could continue to survive as a minicomputer vendor. This line of thought led, eventually, to the VAX 9000 series, which were plagued with problems when they were first introduced in October 1989, already two years late. The problems took so long to work out, and the prices of the systems were so high, that DEC was never able to make the line the success they hoped.
Others within the company felt that the proper response was to introduce their own RISC designs and use those to build new machines. However, there was little official support for these efforts, and no less than four separate small projects ran in parallel at various labs around the US. Eventually these were gathered into the DEC PRISM project, which delivered a credible 32-bit design with some unique features allowing it to serve as the basis of a new VAX implementation. Infighting with teams dedicated to DEC's big iron made funding difficult, and the design was not finalized until April 1988, and then cancelled shortly thereafter.
Another group concluded that new workstations like those from Sun Microsystems and Silicon Graphics would take away a large part of DEC's existing customer base before the new VAX systems could address the issues, and that the company needed its own Unix workstation as soon as possible. Fed up with slow progress on both the RISC and VAX fronts, a group in Palo Alto started a skunkworks project to introduce their own systems. Selecting the MIPS processor, which was widely available, introducing the new DECstation series with the model 3100 on 11 January 1989. These systems would see some success in the market, but were later displaced by similar models running the Alpha.
32-bit MIPS and 64-bit Alpha systems.
Eventually, in 1992, DEC launched the DECchip 21064 processor, the first implementation of their Alpha instruction set architecture, initially named Alpha AXP (the "AXP" was a "non-acronym" and was later dropped). This was a 64-bit RISC architecture (as opposed to the 32-bit CISC architecture used in the VAX) and one of the first "pure" (not an extension of an earlier 32-bit architecture) 64-bit microprocessor architectures and implementations. The Alpha offered class-leading performance at its launch, and subsequent variants continued to do so into the 2000s. An AlphaServer SC45 supercomputer was still ranked No. 6 in the world in November 2004. Alpha-based computers (the DEC AXP series, later the AlphaStation and AlphaServer series) superseded both the VAX and MIPS architecture in DEC's product lines, and could run OpenVMS, DEC OSF/1 AXP (later, Digital Unix or Tru64 UNIX) and Microsoft's then-new operating system, Windows NT.
In 1998, following the takeover by Compaq Computers, a decision was made that Microsoft would no longer support and develop Windows NT for the Alpha series computers, a decision that was seen as the beginning of the end for the Alpha series computers.
StrongARM.
In the mid-1990s, Digital Semiconductor collaborated with ARM Limited to produce the StrongARM microprocessor. This was based in part on ARM7 and in part on DEC technologies like Alpha, and was targeted at embedded systems and portable devices. It was highly compatible with the ARMv4 architecture and was very successful, competing effectively against rivals such as the SuperH and MIPS architectures in the portable digital assistant market. Microsoft subsequently dropped support for these other architectures in their Pocket PC platform. In 1997, as part of a lawsuit settlement, the StrongARM intellectual property was sold to Intel. They continued to produce StrongARM, as well as developing it into the XScale architecture. Intel subsequently sold this business to Marvell Technology Group in 2006.
Designing solutions.
Beyond DECsystem-10/20, PDP, VAX and Alpha, Digital was well respected for its communication subsystem designs, such as Ethernet, DNA (DIGITAL Network Architecture – predominantly DECnet products), DSA (Digital Storage Architecture – disks/tapes/controllers), and its "dumb terminal" subsystems including VT100 and DECserver products.
Final years.
At its peak in the late 1980s, Digital had $14 billion in sales and ranked among the most profitable companies in the USA. With its strong staff of engineers, Digital was expected to usher in the age of personal computers, but the autocratic and trend-resistant Mr. Olsen was openly skeptical of the desktop machines, saying “the personal computer will fall flat on its face in business”, and regarding them as “toys” used for playing video games. Digital's fortunes declined after missing out on some critical market shifts, particularly toward the personal computer. The board forced Olsen to resign as president in July 1992.
In June 1992, Ken Olsen was replaced by Robert Palmer as the company's president. Digital's board of directors also granted Palmer the title of chief executive officer ("CEO"), a title that had never been used during Digital's 35-year existence. Palmer had joined DEC in 1985 to run Semiconductor Engineering and Manufacturing. His relentless campaign to be CEO, and success with the Alpha microprocessor family, made him a candidate to succeed Olsen. At the same time a more modern logo was designed
Palmer restructured Digital into nine business units that reported directly to him. Nonetheless, Digital continued to suffer record losses in recent quarters, including a loss of $260.5 million for the quarter that ended on September 30, 1992. It reported $2.8 billion in losses for its fiscal year 1992. January 5, 1993 saw the retirement of John F. Smith as senior vice president of operations, the second in command at Digital, and his position was not filled. A 35-year company veteran, he had joined Digital in 1958 as the company's 12th employee, passing up a chance to work for Bell Laboratories in New Jersey to work for Digital, then a tiny start-up company in the mill town of Maynard, Mass. Smith rose to become one of the three senior vice presidents in 1987 and was widely considered among the potential successors to Ken Olsen, especially when Smith was appointed chief operating officer in 1991. Smith became a corporate spokesman on financial issues, and had filled in at trouble spots for which Olsen ordered more attention. However Smith was passed over in favor of Palmer when Olsen was forced to resign in July 1992, though Smith stayed on for a time to help turn around the struggling company.
In June 1993, Palmer and several of his top lieutenants presented their reorganization plans to applause from the board of directors, and several weeks later Digital reported its first profitable quarter in several years. However on April 15, 1994, Digital reported a loss of $183 million—three to four times higher than the loss many people on Wall Street had predicted (compared with a loss of $30 million in the comparable period a year earlier), causing the stock price on the NYSE to plunge $5.875 to $23, a 20 percent drop. The losses at that point totaled $339 million for the current fiscal year. Sales of the VAX, long the company's biggest moneymaker, continued to decline, which in turn also hurt Digital's lucrative service and maintenance business (that made up more than a third of Digital Equipment's revenue of $14 billion in the 1993 fiscal year), which declined 11 percent year over year to $1.5 billion in the most recent quarter. Market's acceptance of Digital Alpha computers and chips has been slower than the company had hoped, even though Alpha's sales for the quarter estimated at $275 million were up significantly from $165 million in the December quarter. Digital also made a strong push into personal computers and workstations, which had even lower margins than Alpha computers and chips. Also, Digital was playing catchup with its own Unix offerings for client-server networks, as it long emphasized its own VMS software, while corporate computer users based their client-server networks on the industry-standard Unix software (of which Hewlett Packard was one of the market leaders). Digital's problems were similar to that of larger rival I.B.M., due to the fundamental shift in the computer industry that made it unlikely that Digital could ever again operate profitably at its former size of 120,000 employees, and while its workforce had been reduced to 92,000 people many analysts expected that they would have to cut another 20,000.
During the profitable years up until the early 1990s, DEC was a company that boasted that it never had a general layoff. Following the 1992 economic downturn, layoffs became regular events as the company continually downsized to try to stay afloat. Palmer was tasked with the goal of bringing DEC back to profitability, which he attempted to do by changing the established DEC business culture, hiring new executives from outside the company, and selling off various non-core business units:
By 1997, Digital had subsidiary companies in more than two dozen countries including Austria, Australia, Belgium, Brazil, Canada, China (People's Republic), Colombia, Cyprus, Czech Republic, Denmark, Finland, France, Germany, Ireland, Israel, Japan, Jersey States, New Zealand, Netherlands, Norway, Russia, Singapore, Spain, Sweden, Switzerland, Taiwan, and the United Kingdom.
Eventually, on 26 January 1998, what remained of the company (including Digital's multivendor global services organization and customer support centers) was sold to PC manufacturer Compaq in what was the largest merger up to that time in the computer industry. Several years earlier, Compaq had considered a bid for Digital but became seriously interested only after Digital's major divestments and refocusing on the Internet in 1997. At the time of Compaq's acquisition announcement, Digital had a total of 53,500 employees, down from a peak of 130,000 in the 1980s, but it still employed about 65 percent more people than Compaq to produce about half the volume of sales revenues. After the merger closed, Compaq moved aggressively to reduce Digital’s high selling, general, and administrative (SG&A) costs (equal to 24 percent of total 1997 revenues) and bring them more in line with Compaq’s SG&A expense ratio of 12 percent of revenues.
Compaq used the acquisition to move into enterprise services and compete with IBM, and by 2001 services made up over 20% of Compaq's revenues, largely due to the Digital employees inherited from the merger. Digital's own PC manufacturing was discontinued after the merger closed. As Compaq did not wish to compete with one of its key partner suppliers, the remainder of Digital Semiconductor (the Alpha microprocessor group) was sold to Intel, which placed those employees back in their Hudson (Massachusetts) office, which they had vacated when the site was sold to Intel in 1997.
Compaq struggled as a result of the merger with Digital, and was acquired by Hewlett-Packard in 2002. Compaq, and later HP, continued to sell many of the former Digital products but re-branded with their own logos. For example, HP now sells what were formerly Digital's StorageWorks disk/tape products, as a result of the Compaq acquisition.
The Digital logo survived for a while after the company ceased to exist, as the logo of Digital GlobalSoft, an IT services company in India (which was a 51 percent subsidiary of Compaq). Digital GlobalSoft was later renamed "HP GlobalSoft" (also known as the "HP Global Delivery India Center" or HP GDIC), and no longer uses the Digital logo.
The digital.com and DEC.com domain names are now owned by Hewlett-Packard and redirect to their US website. Digital once held the Class A IP address block 16.0.0.0/8.
The Digital Federal Credit Union (DCU, now DFCU), which was chartered in 1979 for employees of DEC, is now open to essentially everyone. DFCU has over 700 different sponsors, including the companies that acquired pieces of DEC.
Research.
DEC's Research Laboratories (or Research Labs, as they were commonly known) conducted Digital's corporate research. Some of them were operated by Compaq and are still operated by Hewlett-Packard. The laboratories were:
Some of the former employees of Digital's Research Labs or Digital's R&D in general include:
Some of the former employees of Digital Equipment Corp who were responsible for developing Alpha and StrongARM
Some of the work of the Research Labs was published in the "Digital Technical Journal", which was in published from 1985 until 1998.
Accomplishments.
Digital supported the ANSI standards, especially the ASCII character set, which survives in Unicode and the ISO 8859 character set family. Digital's own Multinational Character Set also had a large influence on ISO 8859-1 (Latin-1) and, by extension, Unicode .
The first versions of the C language and the Unix operating system ran on Digital's PDP series of computers (first on a PDP-7, then the PDP-11's), which were among the first commercially viable minicomputers, although for several years Digital itself did not encourage the use of Unix.
Digital produced widely used and influential interactive operating systems, including OS-8, TOPS-10, TOPS-20, RSTS/E, RSX-11, RT-11, and OpenVMS. PDP computers, in particular the PDP-11 model, inspired a generation of programmers and software developers. Some PDP-11 systems more than 25 years old (software and hardware) are still being used to control and monitor factories, transportation systems and nuclear plants. Digital was an early champion of time-sharing systems.
The command-line interfaces found in Digital's systems, eventually codified as DCL, would look familiar to any user of modern microcomputer CLIs; those used in earlier systems, such as CTSS, IBM's JCL, or Univac's time-sharing systems, would look utterly alien. Many features of the CP/M and MS-DOS CLI show a recognizable family resemblance to Digital's OSes, including command names such as DIR and HELP and the "name-dot-extension" file naming conventions.
VAX and MicroVAX computers (very widespread in the 1980s) running VMS formed one of the most important proprietary networks, DECnet, which linked business and research facilities. The DECnet protocols formed one of the first peer-to-peer networking standards, with DECnet phase I being released in the mid-1970s. Email, file sharing, and distributed collaborative projects existed within the company long before their value was recognized in the market.
Digital, Intel and Xerox through their collaboration to create the DIX standard, were champions of Ethernet, but Digital is the company that made Ethernet commercially successful. Initially, Ethernet-based DECnet and LAT protocols interconnected VAXes with DECserver terminal servers. Starting with the Unibus to Ethernet adapter, multiple generations of Ethernet hardware from Digital were the de facto standard. The CI "computer interconnect" adapter was the industry's first network interface controller to use separate transmit and receive "rings".
Digital also invented clustering, an operating system technology that treated multiple machines as one logical entity. Clustering permitted sharing of pooled disk and tape storage via the HSC50/70/90 and later series of Hierarchical Storage Controllers (HSC). The HSCs delivered the first hardware RAID 0 and RAID 1 capabilities and the first serial interconnects of multiple storage technologies. This technology was the forerunner to architectures such as Network of Workstations which are used for massively cooperative tasks such as web-searches and drug research.
The LA36 and LA120 dot matrix printers became industry standards and may have hastened the demise of the Teletype Corporation.
The VT100 computer terminal became the industry standard, implementing a useful subset of the ANSI X3.64 standard, and even today terminal emulators such as HyperTerminal, PuTTY and Xterm still emulate a VT100 (or its more capable successor, the VT220).
The X Window System, the network transparent window system used on UNIX and Linux, and also available on other operating systems, was developed at MIT jointly between Project Athena and the Laboratory for Computer Science. Digital was the primary sponsor for this project, which was a contemporary of the GNU Project but not associated with it.
 In the period 1994-1999 Linus Torvalds developed versions of Linux on early AlphaServer systems made available to him by the engineering department. Compaq software engineers developed special Linux kernel modules. A well-known Linux distribution that ran on AlphaServer systems was Red Hat 7.2. Another distribution that ran on Alpha was Gentoo Linux.
Microsoft was not exclusively bound to the Alpha chip so it pursued other processor makers such as IBM with the PowerPC architecture and eventually capitalized on the emerging strength of the Intel x86 based processors.
Notes-11 and its follow-on product, VAX Notes, were two of the first examples of online collaboration software, a category that has become to be known as groupware. Len Kawell, one of the original Notes-11 developers later joined Lotus Development Corporation and contributed to their Lotus Notes product.
Digital was one of the first businesses connected to the Internet, with "dec.com", registered in 1985, being one of the first of the now ubiquitous ".com" domains. DEC's "gatekeeper.dec.com" was a well-known software repository during the pre-World Wide Web days, and Digital was also the first computer vendor to open a public website, on 1 October 1993. The popular AltaVista, created by Digital, was one of the first comprehensive Internet search engines. (Although Lycos was earlier, it was much more limited.)
DEC invented Digital Linear Tape (DLT), formerly known as CompacTape, which began as a compact backup medium for MicroVAX systems, and later grew to capacities of 800 gigabytes.
Work on the first hard-disk-based MP3-player, the Personal Jukebox, started at the DEC Systems Research Center. (The project was started about a month before the merger into Compaq was completed.)
DEC's Western Research Lab created the Itsy Pocket Computer. This was developed into the Compaq iPaq line of PDAs, which replaced the Compaq Aero PDA.
User organizations.
Originally the users' group was called DECUS (Digital Equipment Computer User Society) during the 1960s to 1990s. When Compaq acquired Digital in 1998, the users group was renamed CUO, the Compaq Users' Organisation. When HP acquired Compaq in 2002, CUO became HP-Interex, although there are still DECUS groups in several countries. In the United States, the organization is represented by the Encompass organization; currently Connect.

</doc>
<doc id="7954" url="http://en.wikipedia.org/wiki?curid=7954" title="Dead Kennedys">
Dead Kennedys

Dead Kennedys are an American hardcore punk band formed in San Francisco, California in 1978. The band became part of the American hardcore punk movement of the early 1980s. They gained a large underground fanbase in the international punk rock community, and were one of the first American hardcore bands to make a significant impact in the United Kingdom.
While they were one of the earliest, most popular and most influential of the American hardcore bands, their music was highly unique in the realm of punk rock in general, bearing traces of surf music, spaghetti western, psychedelic rock, garage rock and rockabilly. Jello Biafra's biting lyrics tackled the sociopolitical concerns of the Reagan era with a distinct sense of morbid humor and satire, following in the footsteps of such earlier rock satirists as Frank Zappa (particularly his 1960's work with The Mothers of Invention) and non-musical countercultural figures like Abbie Hoffman and Lenny Bruce.
In the mid-1980s, the band was embroiled in an obscenity trial in the United States over the artwork of their album "Frankenchrist" (1985), which included the explicit titular subject of H. R. Giger's "Penis Landscape". The band was charged with "distribution of harmful matter to minors", but the trial ended with a hung jury.
Dead Kennedys released five studio albums before disbanding in 1986. In 2001, the band reformed without Biafra, who had been in a legal dispute with the remaining members over royalties.
The band played three performances in October 2010. 
Since the dissolution of Dead Kennedys, Biafra has continued to collaborate and record with other artists, including Mojo Nixon, Al Jourgensen of Ministry, NoMeansNo, and Melvins, and has become a spoken word performer, covering political topics in particular.
History.
Formation of the band (1979).
Dead Kennedys formed in June 1978 in San Francisco, California, when East Bay Ray (Raymond Pepperell) advertised for bandmates in the newspaper "The Recycler", after seeing a ska-punk show at Mabuhay Gardens in San Francisco. The original band lineup consisted of Jello Biafra (Eric Reed Boucher) on vocals, East Bay Ray on guitar, Klaus Flouride (Geoffrey Lyall) on bass, and Ted (Bruce Slesinger) on drums and percussion. This lineup recorded their first demos. In early to mid July, the band recruited 6025 (Carlos Cadona) as a secondary guitarist. Their first show was on July 19, 1978, at the Mabuhay Gardens in San Francisco, California.
Dead Kennedys played numerous shows at local venues afterwards. Due to the provocative name of the band, they sometimes played under pseudonyms, including "The DK's", "The Sharks", "The Creamsicles" and "The Pink Twinkies". The band's real name generated controversy. "San Francisco Chronicle" columnist Herb Caen wrote in November 1978, "Just when you think tastelessness has reached its nadir, along comes a punk rock group called The Dead Kennedys, which will play at Mabuhay Gardens on Nov. 22, the 15th anniversary of John F. Kennedy's assassination." Despite mounting protests, the owner of Mabuhay declared, "I can't cancel them NOW—there's a contract. Not, apparently, the kind of contract some people have in mind." However, despite popular belief, the name was not meant to insult the Kennedy family, but according to Biafra, "to bring attention to the end of the American Dream".
6025 left the band in March 1979 under somewhat unclear circumstances, generally considered to be musical differences. In June, the band released their first single, "California Über Alles", on Biafra and East Bay Ray's independent label, Alternative Tentacles. The band followed with a well-received East Coast tour.
Disruption of music awards show (1980).
On March 25, 1980, Dead Kennedys were invited to perform at the Bay Area Music Awards in San Francisco to major record label artists to give the event some "new wave credibility", in the words of the organizers. The day of the performance was spent practicing the song they were asked to play, the underground hit, "California Über Alles". In typically subversive, perverse style, the band became the talking point of the ceremony when after about 15 seconds into the song, Biafra said, "Hold it! We've gotta prove that we're adults now. We're not a punk rock band, we're a new wave band."
The band, who all wore white shirts with a big, black S painted on the front, pulled black ties from around the backs of their necks to form a dollar sign, then started playing a new song titled "Pull My Strings", a barbed, satirical attack on the ethics of the mainstream music industry, which contained the lyrics, "Is my cock big enough, is my brain small enough, for you to make me a star?". The song also referenced The Knack's song "My Sharona". "Pull My Strings" was never recorded for a studio release, though the performance at the Bay Area Music Awards, which was the only time the song was ever performed, was released on the band's compilation album "Give Me Convenience or Give Me Death".
"Holiday in Cambodia" and "Fresh Fruit for Rotting Vegetables" (1980–1981).
In early 1980, they recorded and released the single "Holiday in Cambodia". Later that year, the band released their debut album, "Fresh Fruit for Rotting Vegetables". The album reached number 33 on the UK Albums Chart. In January 1981, Ted announced that he wanted to leave to pursue a career in architecture and would help look for a replacement. He played his last concert in February 1981. His replacement was D.H. Peligro (Darren Henley).
Around the same time, East Bay Ray had tried to pressure the rest of the band to sign to the major record label Polydor Records; Biafra stated that he was prepared to leave the group if the rest of the band wanted to sign to the label, though East Bay Ray asserts that he recommended against signing with Polydor. Polydor decided not to sign the band after they learned that Dead Kennedys' next single was to be entitled "Too Drunk to Fuck".
When "Too Drunk to Fuck" came out in May 1981, the song caused much controversy in the UK as the BBC feared the single would reach the Top 30; this would require a mention of the song on "Top of the Pops". It was never played although it was called "'Too Drunk' by the Kennedys" by presenter Tony Blackburn.
"In God We Trust, Inc.", "Plastic Surgery Disasters" and Alternative Tentacles Records (1981–1985).
After Peligro joined the band, the extended play "In God We Trust, Inc." (1981) saw them move toward a more aggressive hardcore/thrash sound. In addition to the EP's controversial artwork depicting a gold Christ figure on a cross of dollar bills, the lyrics contained Biafra's most biting social and political commentary yet, and songs such as "Moral Majority", "Nazi Punks Fuck Off!" and "We've Got a Bigger Problem Now" placed Dead Kennedys as the spokesmen of social protest, while "Dog Bite", a cover version of "Rawhide" and various joke introductions showed a much more whimsical side. In 1982, they released their second studio album, "Plastic Surgery Disasters". The album's cover features a withered starving African child's hand being held and dwarfed by a white man's hand. This picture won the World Press Photo award in 1980, and was taken in Karamoja district in Uganda by Mike Wells.
The band's music had evolved much in a short time, moving away from hardcore formulae toward a more innovative jazz-informed style, featuring musicianship and dynamics far beyond other bands in the genre (thus effectively removing the music from that genre). By now the group had become a de facto political force, pitting itself against rising elements of American social and political life such as the religious right, Ronald Reagan and the idle rich. The band continued touring all over the United States, as well as Europe and Australia, and gained a large underground following. While they continued to play live shows during 1983 and 1984, they took a break from releasing new records to concentrate on the Alternative Tentacles record label, which would become synonymous with DIY alternative culture. The band continued to write and perform new material during this time, which would appear on their next album (some of these early performances can be seen in the "Live at DMPO's on Broadway" video, originally released by Dirk Dirksen and later reissued on Rhino).
"Frankenchrist" and obscenity trial (1985–1986).
The release of the album "Frankenchrist" in 1985 showed the band had grown in terms of musical proficiency and lyrical maturity. While there were still a number of loud/fast songs, much of the music featured an eclectic mix of instruments including trumpets and synthesizers. Around this time Klaus Flouride released the similarly experimental solo EP "Cha Cha Cha With Mr. Flouride". Lyrically, the band continued their trademark social commentary, with songs such as "MTV Get Off The Air" and "Jock-o-rama" poking fun at mainstream America.
However, the controversy that erupted over H.R. Giger's "Penis Landscape", included as an insert with the album, dwarfed the notoriety of its music. The artwork caused a furor with the newly formed Parents Music Resource Center (PMRC). In December 1985 a teenage girl purchased the album at the Wherehouse Records store in Los Angeles County. The girl's mother wrote letters of complaint to the California Attorney General and to Los Angeles prosecutors. In 1986 members of the band, along with other parties involved in the distribution of "Frankenchrist", were charged criminally with distribution of harmful matter to minors. The store where the teen actually purchased the album was never named in the lawsuit. The criminal charges focused on an illustration by H.R. Giger, titled "Work 219: Landscape XX" (also known as "Penis Landscape"). Included as a poster with the album, "Penis Landscape" depicts nine copulating penises and vaginas.
Members of the band and others in the chain of distribution were charged with violating the California Penal Code on a misdemeanor charge carrying a maximum penalty of up to a year in county jail and a base fine of up to $2,000. Biafra says that during this time government agents invaded and searched his home. The prosecution tried to present the poster to the jury in isolation for consideration as obscene material, but Judge Susan Isacoff ruled that the poster must be considered along with the music and lyrics. The charges against three of the original defendants, Ruth Schwartz (owner of Mordam Records), Steve Boudreau (a distributor involved in supplying "Frankenchrist" to the Los Angeles Wherehouse store), and Salvatore Alberti (owner of the factory where the record was pressed), were dismissed for lack of evidence.
In August 1987, the criminal trial was submitted to the jury with the two remaining defendants: Jello Biafra and Michael Bonanno (former Alternative Tentacles label manager). In August 1987, the criminal trial ended with a hung jury. The split on the jury was 7 to 5 in favor of acquittal for all of the defendants. District Attorneys Michael Guarino and Ira Riener made a motion for a retrial which was denied by Judge Isacoff, Superior Court Judge for the County of Los Angeles. The album, however, was banned from many record stores nationwide.
Jello Biafra brought up the court case after the break-up of the band on The Oprah Winfrey Show. Biafra was on the show with Tipper Gore as part of a panel discussion on the issues of "controversial music lyrics" and censorship.
"Bedtime for Democracy" and break-up (1986).
In addition to the obscenity lawsuit and being ignored by the mainstream media (MTV and most radio stations gave such groups scant notice, not to mention airplay), the band became increasingly disillusioned with the underground scene as well. The hardcore scene, which had been a haven for free-thinking intellectuals and downtrodden nonconformists, was attracting a more violent audience that imposed an increasing level of brutality on other concertgoers and began to alienate many of the bands and individuals who had helped pioneer the movement in the early 80's. In earlier years the band had criticized neo-Nazi skinheads for trying to ruin the punk scene, but just as big a problem was the popularity of thrash metal and increasingly metal-influenced, macho hardcore bands, which brought the group (and their genre) an audience that had little to do with the ideas/ideals they stood for. In January 1986, frustrated and alienated from their own scene, the DKs decided to break up to pursue other interests and played their last concert on February 21. The band continued to work on songs, with Biafra penning songs such as "Chickenshit Conformist" and "Anarchy for Sale", which articulated their feelings about the "dumbing down" of punk rock.
During the summer they recorded these songs for their final album, "Bedtime for Democracy", which was released in November. The artwork, depicting a defaced Statue of Liberty overrun with Nazis, media, opportunists, Klan members, corrupt government officials, and religious zombies, echoed the idea that neither America itself or the punk scene were safe havens anymore for "your tired, your poor, your huddled masses yearning to breathe free". The album contains a number of fast/short songs interspersed with jazz ("D.M.S.O."), spoken word ("A Commercial") and psychedelia ("Cesspools In Eden"). The lyrical focus is more introspective and earnest ("Where Do Ya Draw The Line?"), with an anti-war, anti-violence ("Rambozo The Clown") bent, moving away from the violent imagery of their early records, while remaining as subversive as ever ("I Spy", "D.M.S.O."). In December, the band announced their split. Biafra went on to speak about his political beliefs on numerous television shows and he released a number of spoken-word albums. Ray, Flouride, and Peligro also went on to solo careers.
Legal conflicts.
Lawsuits over royalty payments.
In the late 1990s, former band members discovered problems with the amount of payments which each band member had received from their record label Alternative Tentacles. Former band members claimed that Jello Biafra had conspired to pay lower royalty rates to the band members. Although both sides agreed that the failure to pay these royalties was an accounting mistake, they were upset that Biafra failed to inform the band of the mistake after he and his co-workers discovered it.
Biafra claims that their lawyers had told him only to correspond through lawyers and not directly with the band, as the conflict over payment had apparently arisen before the accounting mistake was discovered. Both sides claim they attempted to resolve the matter without legal action, but the ultimately complicated legal dispute (involving royalties, publishing rights, and a number of other issues) soon led to the courts, where Biafra was found liable for the royalties after the jury determined that he had committed fraud and malice, and was ordered to pay damages of nearly $200,000, including $20,000 in punitive damages, to the band members.
Malice was defined for the jury as "conduct which is intended to cause injury or despicable conduct which is carried with a willful and conscious disregard for the rights of others". Biafra's appeal was denied; he had to pay the outstanding royalties and punitive damages, and was forced to hand over the rights to the majority of Dead Kennedys' back catalogue to the Decay Music partnership.
The jury and judges also noted, in their words, that Biafra “lacked credibility” on the songwriting issue and found from evidence presented by both sides that the songwriting credits were due to the entire band, using a clause in the band's written partnership giving a small share of every Dead Kennedys song royalty directly to the band partnership.
Biafra had received sole songwriting credit for most Dead Kennedys songs on all released albums for the last 20 years or so without complaints from the band, though a minority of songs had given credit to certain group members or the entire band as a whole, indicating a system designed to reflect the primary composers rather than a regimented system like the Jagger/Richards partnership; today, most Kennedys reissues list the songwriters as "Biafra, Dead Kennedys", indicating Biafra's lyrical contributions—which the band doesn't dispute, or else simply as "Dead Kennedys"). Ray, Flouride and Peligro found new distribution through another label, Manifesto Records.
This dispute was hotly contested by all concerned who felt passionately for their cause, and the case caused minor waves within punk circles. Biafra claims that guitarist East Bay Ray had long expressed displeasure with Alternative Tentacles and with the amount of money he received from them, thus the original incentive for the discovery of the back payments. It was found out that Alternative Tentacles was paying Dead Kennedys less per CD than all the other bands, including Biafra himself, and not informing his other bandmates, which was the fraud. Biafra accused the band of wanting to license the famous Dead Kennedys song "Holiday in Cambodia" for use in a Levi's jeans commercial, which the band denied.
Biafra apparently pushed this issue in court, although there was no hard evidence and the jurors were apparently unconcerned with corporate use of independently produced political music. Biafra would later complain that the jury was not sympathetic toward underground music and punk culture. The song never appeared in a Levi's commercial, although in interviews Biafra described the situation surrounding the commercial in detail and was able to give specifics about the advertisement, including the name of the advertising agency that had created the commercial's script.
Biafra's former bandmates maintain that they sued because of Jello Biafra's deliberate withholding of money, though when pressed they have acknowledged that the payment was an accounting mistake, but insist that Biafra was wrong in failing to inform the band directly. Details about this issue remain scarce. The band also maintains that the Levi's story was completely fictitious and invented by Biafra to discredit them. Ultimately, these issues have led to a souring of relationships with the erstwhile bandmates, who still have not resolved their personal differences as of 2012.
Disputes over new commercial activities.
Matters were stirred up even further when the three bandmates invited Jello Biafra to "bury the hatchet" in the form of a band reunion. Jello Biafra felt it was unprofessional because no one contacted him directly. In addition, Biafra was disdainful of the reunion, and having long expressed his disdain for nostalgia and rock reunion/oldies tours in particular, argued that the whole affair was motivated by greed.
Several DVDs, re-issues, and live albums have been released since the departure of Biafra most recently on Manifesto Records. According to Biafra, the live albums are "cash-ins" on Dead Kennedys' name and his music. Biafra also accused the releases of the new live material of having poor sound quality. Furthermore he has stated he is not receiving any royalties from the sale of any Manifesto Records releases. Consequently, he has discouraged fans from buying any Dead Kennedy reissues. The other band members denied Biafra's accusations regarding the live releases, and have defended the mixes as an effort of hard work. Biafra dismissed the new group as "the world's greediest karaoke band." Nevertheless, in 2003, Klaus Flouride said of performances without the band's former frontman: "There hasn't been a show yet that people didn't really like."
Biafra further criticized them for advertising shows using his own image taken from the original 1980s incarnation of the band, which he labeled as false advertising. He attacked the reformed Dead Kennedys in a song called "Those Dumb Punk Kids (Will Buy Anything)", which appears on his second collaboration with sludge metal band The Melvins, "Sieg Howdy!".
Biafra told an audience at a speaking gig in Trenton, New Jersey, that the remaining Dead Kennedys have licensed their single "Too Drunk to Fuck" to be used in a rape scene in a Robert Rodriguez movie. The reference is to a lounge cover of the song, recorded by the band Nouvelle Vague, played during a scene in the "Planet Terror" segment of "Grindhouse", although no rape takes place, and in fact the would-be rapist is killed by the would-be victim. The scene in "Planet Terror" has would-be rapist, "Rapist No. 1" (Quentin Tarantino) order one-legged stripper "Cherry Darlin" (Rose McGowan) to get up off the floor and dance. At this point Tarantino hits play on a cassette recorder and Nouvelle Vague's cover of "Too Drunk To Fuck" plays.
Jello, clearly disapproving of the situation, later wrote, "This is their lowest point since Levi's… This goes against everything the Dead Kennedys stands for in spades… The terrified woman later 'wins' by killing Tarantino, but that excuse does not rescue this at all. I wrote every note of that song and this is not what it was meant for…. Some people will do anything for money. I can't help but think back to how prudish Klaus Flouride was when he objected to H.R. Giger's painting on the "Frankenchrist" (sic) poster, saying he couldn't bear to show it to his parents. I'd sure love to be a fly on the wall when he tries to explain putting a song in a rape scene for money to his teenage daughter… The deal was pushed through by a new business manager the other three hired."
Reforming of new band line-up.
The reformed Dead Kennedys followed their court victory by announcing a number of tour dates, releasing reissues of all Dead Kennedys albums (except "Fresh Fruit for Rotting Vegetables", to which they did not have the rights until 2005), releasing several new archival concert DVDs, and licensing several songs to "The Manchurian Candidate" remake and the Tony Hawk Pro Skater video game. East Bay Ray claims he received a fax from Alternative Tentacles purporting Biafra approved the licensing for the game, which Biafra denies happening.
The band claims on their website that they still pay close attention to an anti-corporate ideology, despite performing on September 5, 2003 at a festival in Turkey that was sponsored by Coca-Cola, noting that they have since pulled out of a show in Los Angeles when they found that it was being sponsored by Coors. However, Biafra claims the above mentioned licensing deals prove otherwise. Some have found difficulty reconciling this claim when Biafra also licensed to major corporations, approving with the other band members use of Dead Kennedys’ songs in major studio film releases such as "Neighbors", "Freddy Got Fingered", and "Fear and Loathing in Las Vegas".
In 2001, Ray, Peligro, and Flouride chose former Dr. Know singer Brandon Cruz to replace Biafra on vocals. The band played under name "DK Kennedys" for a few concerts, but later reverted to "Dead Kennedys" permanently. They played across the continental United States, Europe, Asia, South America, and Russia. Brandon Cruz left the band in May 2003 and was replaced by Jeff Penalty. The band has released two live albums of archival performances on Manifesto Records: "Mutiny on the Bay", compiled from various live shows including a recording from their last show with Biafra in 1986, and "Live at the Deaf Club", a recording of a 1979 performance at the Deaf Club in San Francisco which was greeted with more enthusiasm.
On October 9, 2007, a best of album titled "Milking the Sacred Cow" was released. It includes two previously unreleased live versions of "Soup Is Good Food" and "Jock-O-Rama", originally found on "Frankenchrist".
Jeff Penalty left the band in March 2008 in what he describes as a "not amicable split." He was replaced by former Wynona Riders singer Ron "Skip" Greer. D.H. also left the band to "take some personal time off". He was replaced for a tour by Translator drummer Dave Scheff.
Break from touring.
On August 21, 2008, the band announced an extended break from touring due to the health-related issues of Flouride and Peligro. They stated their plans to collaborate on new projects. The band performed a gig in Santa Rosa, California in June 2009, with Peligro returning to the drum kit.
In August 2010, the Dead Kennedys announced plans for a short East Coast tour. The lineup assembled for this tour contained East Bay Ray, Peligro, Greer, and bassist Greg Reeves replacing Flouride, who was taking "personal time off" from the band. The tour dates included performances in Philadelphia, New York City, Boston, Washington, D.C., Portland, Maine and Hawaii.
Back on the road.
Dead Kennedys had world tours in 2013 and in 2014, the latter mostly in North American cities.
Reworked material.
The band has played a reworked version of their song "MTV Get Off the Air", re-titled "MP3 Get Off the Web", with lyrics criticizing music piracy during their October 16, 2010, concert at the Rock and Roll Hotel in Washington, D.C..
"DK" logo.
The original logo was created by Winston Smith. He later contributed artwork for the covers of "In God We Trust, Inc.", "Plastic Surgery Disasters", "Frankenchrist", "Bedtime for Democracy", "Give Me Convenience or Give Me Death", the back cover of the "Kill the Poor" single and the Alternative Tentacles logo. When asked about the "DK" logo in an interview, Jello Biafra explained, "...I wanted to make sure it was something simple and easy to spray-paint so people would graffiti it all over the place, and then I showed it to Winston Smith. He played around with it, came back with a bunch of designs that had the circle and slightly 3-D looking letters and he had ones with different patterns behind it. I liked the one with bricks, but ultimately I thought simple red behind it was the boldest and the best."
Lyrics.
Dead Kennedys were noted for the harshness of their lyrics, which generally combined biting Juvenalian social satire while expressing a staunchly left-wing view of contemporary America. Unlike other leftist punk bands who use more direct sloganeering, Dead Kennedys' lyrics were often snide. For example, "Holiday in Cambodia" is a multi-layered satire targeting both yuppies and Cambodia's then-current Khmer Rouge regime.
Members.
Current members
Former members

</doc>
<doc id="7955" url="http://en.wikipedia.org/wiki?curid=7955" title="DNA">
DNA

Deoxyribonucleic acid (; DNA) is a molecule that encodes the genetic instructions used in the development and functioning of all known living organisms and many viruses. DNA is a nucleic acid; alongside proteins and carbohydrates, nucleic acids compose the three major macromolecules essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides. Each nucleotide is composed of a nitrogen-containing nucleobase—either guanine (G), adenine (A), thymine (T), or cytosine (C)—as well as a monosaccharide sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA.
DNA is well-suited for biological information storage. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve a function of encoding proteins.
The two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, "bases"). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.
Within cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
Scientists use DNA as a molecular tool to explore physical laws and theories, such as the ergodic theorem and the theory of elasticity. The unique material properties of DNA have made it an attractive molecule for material scientists and engineers interested in micro- and nano-fabrication. Among notable advances in this field are DNA origami and DNA-based hybrid materials.
The obsolete synonym "desoxyribonucleic acid" may occasionally be encountered, for example, in pre-1953 genetics.
Properties.
DNA is a long polymer made from repeating units called nucleotides. DNA was first identified and isolated by Friedrich Miescher and the double helix structure of DNA was first discovered by James Watson and Francis Crick, using experimental data collected by Rosalind Franklin and Maurice Wilkins. The structure of DNA of all species comprises two helical chains each coiled round the same axis, and each with a pitch of 34 ångströms (3.4 nanometres) and a radius of 10 ångströms (1.0 nanometres). According to another study, when measured in a particular solution, the DNA chain measured 22 to 26 ångströms wide (2.2 to 2.6 nanometres), and one nucleotide unit measured 3.3 Å (0.33 nm) long. Although each individual repeating unit is very small, DNA polymers can be very large molecules containing millions of nucleotides. For instance, the largest human chromosome, chromosome number 1, consists of approximately 220 million base pairs and is 85 nm long.
In living organisms DNA does not usually exist as a single molecule, but instead as a pair of molecules that are held tightly together. These two long strands entwine like vines, in the shape of a double helix. The nucleotide repeats contain both the segment of the backbone of the molecule, which holds the chain together, and a nucleobase, which interacts with the other DNA strand in the helix. A nucleobase linked to a sugar is called a nucleoside and a base linked to a sugar and one or more phosphate groups is called a nucleotide. A polymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide.
The backbone of the DNA strand is made from alternating phosphate and sugar residues. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined together by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings. These asymmetric bonds mean a strand of DNA has a direction. In a double helix the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are "antiparallel". The asymmetric ends of DNA strands are called the 5′ ("five prime") and 3′ ("three prime") ends, with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the alternative pentose sugar ribose in RNA.
The DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. In the aqueous environment of the cell, the conjugated π bonds of nucleotide bases align perpendicular to the axis of the DNA molecule, minimizing their interaction with the solvation shell and therefore, the Gibbs free energy. The four bases found in DNA are adenine (abbreviated A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar/phosphate to form the complete nucleotide, as shown for adenosine monophosphate.
Nucleobase classification.
The nucleobases are classified into two types: the purines, A and G, being fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA a large number of artificial nucleic acid analogues have also been created to study the properties of nucleic acids, or for use in biotechnology.
Uracil is not usually found in DNA, occurring only as a breakdown product of cytosine. However, in a number of bacteriophages – "Bacillus subtilis" bacteriophages PBS1 and PBS2 and "Yersinia" bacteriophage piR1-37 – thymine has been replaced by uracil. Another phage - Staphylococcal phage S6 - has been identified with a genome where thymine has been replaced by uracil.
Base J (beta-d-glucopyranosyloxymethyluracil), a modified form of uracil, is also found in a number of organisms: the flagellates "Diplonema" and "Euglena", and all the kinetoplastid genera Biosynthesis of J occurs in two steps: in the first step a specific thymidine in DNA is converted into hydroxymethyldeoxyuridine; in the second HOMedU is glycosylated to form J. Proteins that bind specifically to this base have been identified. These proteins appear to be distant relatives of the Tet1 oncogene that is involved in the pathogenesis of acute myeloid leukemia. J appears to act as a termination signal for RNA polymerase II.
Grooves.
Twin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide a binding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. One groove, the major groove, is 22 Å wide and the other, the minor groove, is 12 Å wide. The narrowness of the minor groove means that the edges of the bases are more accessible in the major groove. As a result, proteins like transcription factors that can bind to specific sequences in double-stranded DNA usually make contacts to the sides of the bases exposed in the major groove. This situation varies in unusual conformations of DNA within the cell "(see below)", but the major and minor grooves are always named to reflect the differences in size that would be seen if the DNA is twisted back into the ordinary B form.
Base pairing.
In a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is called complementary base pairing. Here, purines form hydrogen bonds to pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix is called a base pair. As hydrogen bonds are not covalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can therefore be pulled apart like a zipper, either by a mechanical force or high temperature. As a result of this complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. Indeed, this reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in living organisms.
Top, a GC base pair with three hydrogen bonds. Bottom, an AT base pair with two hydrogen bonds. Non-covalent hydrogen bonds between the pairs are shown as dashed lines.
The two types of base pairs form different numbers of hydrogen bonds, AT forming two hydrogen bonds, and GC forming three hydrogen bonds (see figures, right).
DNA with high GC-content is more stable than DNA with low GC-content.
As noted above, most DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double stranded structure (dsDNA) is maintained largely by the intrastrand base stacking interactions, which are strongest for G,C stacks. The two strands can come apart – a process known as melting – to form two single-stranded DNA molecules (ssDNA) molecules. Melting occurs at high temperature, low salt and high pH (low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).
The stability of the dsDNA form depends not only on the GC-content (% G,C basepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is the "melting temperature", which is the temperature at which 50% of the ds molecules are converted to ss molecules; melting temperature is dependent on ionic strength and the concentration of DNA.
As a result, it is both the percentage of GC base pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a high GC-content have stronger-interacting strands, while short helices with high AT content have weaker-interacting strands. In biology, parts of the DNA double helix that need to separate easily, such as the TATAAT Pribnow box in some promoters, tend to have a high AT content, making the strands easier to pull apart.
In the laboratory, the strength of this interaction can be measured by finding the temperature necessary to break the hydrogen bonds, their melting temperature (also called "Tm" value). When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules ("ssDNA") have no single common shape, but some conformations are more stable than others.
Sense and antisense.
A DNA sequence is called "sense" if its sequence is the same as that of a messenger RNA copy that is translated into protein. The sequence on the opposite strand is called the "antisense" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e. both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear. One proposal is that antisense RNAs are involved in regulating gene expression through RNA-RNA base pairing.
A few DNA sequences in prokaryotes and eukaryotes, and more in plasmids and viruses, blur the distinction between sense and antisense strands by having overlapping genes. In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. In bacteria, this overlap may be involved in the regulation of gene transcription, while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.
Supercoiling.
DNA can be twisted like a rope in a process called DNA supercoiling. With DNA in its "relaxed" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound. If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced by enzymes called topoisomerases. These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such as transcription and DNA replication.
Alternate DNA structures.
DNA exists in many possible conformations that include A-DNA, B-DNA, and Z-DNA forms, although, only B-DNA and Z-DNA have been directly observed in functional organisms. The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metal ions, as well as the presence of polyamines in solution.
The first published reports of A-DNA X-ray diffraction patterns—and also B-DNA—used analyses based on Patterson transforms that provided only a limited amount of structural information for oriented fibers of DNA. An alternate analysis was then proposed by Wilkins "et al.", in 1953, for the "in vivo" B-DNA X-ray diffraction/scattering patterns of highly hydrated DNA fibers in terms of squares of Bessel functions. In the same journal, James Watson and Francis Crick presented their molecular modeling analysis of the DNA X-ray diffraction patterns to suggest that the structure was a double-helix.
Although the "B-DNA form" is most common under the conditions found in cells, it is not a well-defined conformation but a family of related DNA conformations that occur at the high hydration levels present in living cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecular paracrystals with a significant degree of disorder.
Compared to B-DNA, the A-DNA form is a wider right-handed spiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partially dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, as well as in enzyme-DNA complexes. Segments of DNA where the bases have been chemically modified by methylation may undergo a larger change in conformation and adopt the Z form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form. These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.
Alternative DNA chemistry.
For a number of years exobiologists have proposed the existence of a shadow biosphere, a postulated microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that use arsenic instead of phosphorus in DNA. A report in 2010 of the possibility in the bacterium GFAJ-1, was announced, though the research was disputed, and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.
Quadruplex structures.
At the ends of the linear chromosomes are specialized regions of DNA called telomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzyme telomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes. These specialized chromosome caps also help protect the DNA ends, and stop the DNA repair systems in the cell from treating them as damage to be corrected. In human cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.
These guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases form a flat plate and these flat four-base units then stack on top of each other, to form a stable G-quadruplex structure. These structures are stabilized by hydrogen bonding between the edges of the bases and chelation of a metal ion in the centre of each four-base unit. Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.
In addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.
Branched DNA can form networks containing multiple branches.
Branched DNA.
In DNA fraying occurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible. Branched DNA can be used in nanotechnology to construct geometric shapes, see the section on uses in technology below.
Chemical modifications and altered DNA packaging.
Structure of cytosine with and without the 5-methyl group. Deamination converts 5-methylcytosine into thymine.
Base modifications and DNA packaging.
The expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure called chromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels of methylation of cytosine bases. DNA packaging and its influence on gene expression can also occur by covalent modifications of the histone protein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (see Chromatin remodeling). There is, further, crosstalk between DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.
For one example, cytosine methylation, produces 5-methylcytosine, which is important for X-chromosome inactivation. The average level of methylation varies between organisms – the worm "Caenorhabditis elegans" lacks cytosine methylation, while vertebrates have higher levels, with up to 1% of their DNA containing 5-methylcytosine. Despite the importance of 5-methylcytosine, it can deaminate to leave a thymine base, so methylated cytosines are particularly prone to mutations. Other base modifications include adenine methylation in bacteria, the presence of 5-hydroxymethylcytosine in the brain, and the glycosylation of uracil to produce the "J-base" in kinetoplastids.
Damage.
DNA can be damaged by many sorts of mutagens, which change the DNA sequence. Mutagens include oxidizing agents, alkylating agents and also high-energy electromagnetic radiation such as ultraviolet light and X-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producing thymine dimers, which are cross-links between pyrimidine bases. On the other hand, oxidants such as free radicals or hydrogen peroxide produce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks. A typical human cell contains about 150,000 bases that have suffered oxidative damage. Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can produce point mutations, insertions and deletions from the DNA sequence, as well as chromosomal translocations. These mutations can cause cancer. Because of inherent limitations in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. DNA damages that are naturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.
Many mutagens fit into the space between two adjacent base pairs, this is called "intercalation". Most intercalators are aromatic and planar molecules; examples include ethidium bromide, acridines, daunomycin, and doxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations. As a result, DNA intercalators may be carcinogens, and in the case of thalidomide, a teratogen. Others such as benzo["a"]pyrene diol epoxide and aflatoxin form DNA adducts that induce errors in replication. Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used in chemotherapy to inhibit rapidly growing cancer cells.
Biological functions.
DNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell makes up its genome; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes. The information carried by DNA is held in the sequence of pieces of DNA called genes. Transmission of genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matching protein sequence in a process called translation, which depends on the same interaction between RNA nucleotides. In alternative fashion, a cell may simply copy its genetic information in a process called DNA replication. The details of these functions are covered in other articles; here we focus on the interactions between DNA and other molecules that mediate the function of the genome.
Genes and genomes.
Genomic DNA is tightly and orderly packed in the process called DNA condensation to fit the small available volumes of the cell. In eukaryotes, DNA is located in the cell nucleus, as well as small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete set of this information in an organism is called its genotype. A gene is a unit of heredity and is a region of DNA that influences a particular characteristic in an organism. Genes contain an open reading frame that can be transcribed, as well as regulatory sequences such as promoters and enhancers, which control the transcription of the open reading frame.
In many species, only a small fraction of the total sequence of the genome encodes protein. For example, only about 1.5% of the human genome consists of protein-coding exons, with over 50% of human DNA consisting of non-coding repetitive sequences. The reasons for the presence of so much noncoding DNA in eukaryotic genomes and the extraordinary differences in genome size, or "C-value", among species represent a long-standing puzzle known as the "C-value enigma". However, some DNA sequences that do not code protein may still encode functional non-coding RNA molecules, which are involved in the regulation of gene expression.
Some noncoding DNA sequences play structural roles in chromosomes. Telomeres and centromeres typically contain few genes, but are important for the function and stability of chromosomes. An abundant form of noncoding DNA in humans are pseudogenes, which are copies of genes that have been disabled by mutation. These sequences are usually just molecular fossils, although they can occasionally serve as raw genetic material for the creation of new genes through the process of gene duplication and divergence.
Transcription and translation.
A gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter 'words' called "codons" formed from a sequence of three nucleotides (e.g. ACT, CAG, TTT).
In transcription, the codons of a gene are copied into messenger RNA by RNA polymerase. This RNA copy is then decoded by a ribosome that reads the RNA sequence by base-pairing the messenger RNA to transfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (43 combinations). These encode the twenty standard amino acids, giving most amino acids more than one possible codon. There are also three 'stop' or 'nonsense' codons signifying the end of the coding region; these are the TAA, TGA, and TAG codons.
Replication.
Cell division is essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism for DNA replication. Here, the two strands are separated and then each strand's complementary DNA sequence is recreated by an enzyme called DNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing, and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix. In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.
Interactions with proteins.
All the functions of DNA depend on interactions with proteins. These protein interactions can be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.
DNA-binding proteins.
Interaction of DNA (shown in orange) with histones (shown in blue). These proteins' basic amino acids bind to the acidic phosphate groups on DNA.
Structural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure called chromatin. In eukaryotes this structure involves DNA binding to a complex of small basic proteins called histones, while in prokaryotes multiple types of proteins are involved. The histones form a disk-shaped complex called a nucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones making ionic bonds to the acidic sugar-phosphate backbone of the DNA, and are therefore largely independent of the base sequence. Chemical modifications of these basic amino acid residues include methylation, phosphorylation and acetylation. These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible to transcription factors and changing the rate of transcription. Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA. These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.
A distinct group of DNA-binding proteins are the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replication protein A is the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination and DNA repair. These binding proteins seem to stabilize single-stranded DNA and protect it from forming stem-loops or being degraded by nucleases.
In contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the various transcription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription. Alternatively, transcription factors can bind enzymes that modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.
As these DNA targets can occur throughout an organism's genome, changes in the activity of one type of transcription factor can affect thousands of genes. Consequently, these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development. The specificity of these transcription factors' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to "read" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.
DNA-modifying enzymes.
Nucleases and ligases.
Nucleases are enzymes that cut DNA strands by catalyzing the hydrolysis of the phosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are called exonucleases, while endonucleases cut within strands. The most frequently used nucleases in molecular biology are the restriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the vertical line. In nature, these enzymes protect bacteria against phage infection by digesting the phage DNA when it enters the bacterial cell, acting as part of the restriction modification system. In technology, these sequence-specific nucleases are used in molecular cloning and DNA fingerprinting.
Enzymes called DNA ligases can rejoin cut or broken DNA strands. Ligases are particularly important in lagging strand DNA replication, as they join together the short segments of DNA produced at the replication fork into a complete copy of the DNA template. They are also used in DNA repair and genetic recombination.
Topoisomerases and helicases.
Topoisomerases are enzymes with both nuclease and ligase activity. These proteins change the amount of supercoiling in DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break. Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix. Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.
Helicases are proteins that are a type of molecular motor. They use the chemical energy in nucleoside triphosphates, predominantly ATP, to break hydrogen bonds between bases and unwind the DNA double helix into single strands. These enzymes are essential for most processes where enzymes need to access the DNA bases.
Polymerases.
Polymerases are enzymes that synthesize polynucleotide chains from nucleoside triphosphates. The sequence of their products are created based on existing polynucleotide chains—which are called "templates". These enzymes function by repeatedly adding a nucleotide to the 3′ hydroxyl group at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction. In the active site of these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.
In DNA replication, DNA-dependent DNA polymerases make copies of DNA polynucleotide chains. In order to preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have a proofreading activity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′ exonuclease activity is activated and the incorrect base removed. In most organisms, DNA polymerases function in a large complex called the replisome that contains multiple accessory subunits, such as the DNA clamp or helicases.
RNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They include reverse transcriptase, which is a viral enzyme involved in the infection of cells by retroviruses, and telomerase, which is required for the replication of telomeres. Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure.
Transcription is carried out by a DNA-dependent RNA polymerase that copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into a messenger RNA transcript until it reaches a region of DNA called the terminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases, RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a large protein complex with multiple regulatory and accessory subunits.
Genetic recombination.
Structure of the Holliday junction intermediate in genetic recombination. The four separate DNA strands are coloured red, blue, green and yellow.
A DNA helix usually does not interact with other segments of DNA, and in human cells the different chromosomes even occupy separate areas in the nucleus called "chromosome territories". This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is during chromosomal crossover when they recombine. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.
Recombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency of natural selection and can be important in the rapid evolution of new proteins. Genetic recombination can also be involved in DNA repair, particularly in the cell's response to double-strand breaks.
The most common form of chromosomal crossover is homologous recombination, where the two chromosomes involved share very similar sequences. Non-homologous recombination can be damaging to cells, as it can produce chromosomal translocations and genetic abnormalities. The recombination reaction is catalyzed by enzymes known as recombinases, such as RAD51. The first step in recombination is a double-stranded break caused by either an endonuclease or damage to the DNA. A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least one Holliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA.
Evolution.
DNA contains the genetic information that allows all modern living things to function, grow and reproduce. However, it is unclear how long in the 4-billion-year history of life DNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material. RNA may have acted as the central part of early cell metabolism as it can both transmit genetic information and carry out catalysis as part of ribozymes. This ancient RNA world where nucleic acid would have been used for both catalysis and genetics may have influenced the evolution of the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes.
However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible. This is because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution. Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old, but these claims are controversial.
On 8 August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of DNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space.
Uses in technology.
Genetic engineering.
Methods have been developed to purify DNA from organisms, such as phenol-chloroform extraction, and to manipulate it in the laboratory, such as restriction digests and the polymerase chain reaction. Modern biology and biochemistry make intensive use of these techniques in recombinant DNA technology. Recombinant DNA is a man-made DNA sequence that has been assembled from other DNA sequences. They can be transformed into organisms in the form of plasmids or in the appropriate format, by using a viral vector. The genetically modified organisms produced can be used to produce products such as recombinant proteins, used in medical research, or be grown in agriculture.
Forensics.
Forensic scientists can use DNA in blood, semen, skin, saliva or hair found at a crime scene to identify a matching DNA of an individual, such as a perpetrator. This process is formally termed DNA profiling, but may also be called "genetic fingerprinting". In DNA profiling, the lengths of variable sections of repetitive DNA, such as short tandem repeats and minisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA. However, identification can be complicated if the scene is contaminated with DNA from several people. DNA profiling was developed in 1984 by British geneticist Sir Alec Jeffreys, and first used in forensic science to convict Colin Pitchfork in the 1988 Enderby murders case.
The development of forensic science, and the ability to now obtain genetic matching on minute samples of blood, skin, saliva or hair has led to a re-examination of a number of cases. Evidence can now be uncovered that was not scientifically possible at the time of the original examination. Combined with the removal of the double jeopardy law in some places, this can allow cases to be reopened where previous trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defence to DNA matches obtained forensically is to claim that cross-contamination of evidence has taken place. This has resulted in meticulous strict handling procedures with new cases of serious crime.
DNA profiling is also used to identify victims of mass casualty incidents. As well as positively identifying bodies or body parts in serious accidents, DNA profiling is being successfully used to identify individual victims in mass war graves – matching to family members.
Bioinformatics.
Bioinformatics involves the manipulation, searching, and data mining of biological data, and this includes DNA sequence data. The development of techniques to store and search DNA sequences have led to widely applied advances in computer science, especially string searching algorithms, machine learning and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.
DNA nanotechnology.
DNA nanotechnology uses the unique molecular recognition properties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties. DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using the "DNA origami" method) as well as three-dimensional structures in the shapes of polyhedra. Nanomechanical devices and algorithmic self-assembly have also been demonstrated, and these DNA structures have been used to template the arrangement of other molecules such as gold nanoparticles and streptavidin proteins.
History and anthropology.
Because DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, their phylogeny. This field of phylogenetics is a powerful tool in evolutionary biology. If DNA sequences within a species are compared, population geneticists can learn the history of particular populations. This can be used in studies ranging from ecological genetics to anthropology; For example, DNA evidence is being used to try to identify the Ten Lost Tribes of Israel.
DNA has also been used to look at modern family relationships, such as establishing family relationships between the descendants of Sally Hemings and Thomas Jefferson. This usage is closely related to the use of DNA in criminal investigations detailed above. Indeed, some criminal investigations have been solved when DNA from crime scenes has matched relatives of the guilty individual.
Information storage.
In a paper published in Nature in January 2013, scientists from the European Bioinformatics Institute and Agilent Technologies proposed a mechanism to use DNA's ability to code information as a means of digital data storage. The group was able to encode 739 kilobytes of data into DNA code, synthesize the actual DNA, then sequence the DNA and decode the information back to its original form, with a reported 100% accuracy. The encoded information consisted of text files and audio files. A prior experiment was published in August 2012. It was conducted by researchers at Harvard University, where the text of a 54,000-word book was encoded in DNA.
History of DNA research.
DNA was first isolated by the Swiss physician Friedrich Miescher who, in 1869, discovered a microscopic substance in the pus of discarded surgical bandages. As it resided in the nuclei of cells, he called it "nuclein". In 1878, Albrecht Kossel isolated the non-protein component of "nuclein", nucleic acid, and later isolated its five primary nucleobases. In 1919, Phoebus Levene identified the base, sugar and phosphate nucleotide unit. Levene suggested that DNA consisted of a string of nucleotide units linked together through the phosphate groups. However, Levene thought the chain was short and the bases repeated in a fixed order. In 1937, William Astbury produced the first X-ray diffraction patterns that showed that DNA had a regular structure.
In 1927, Nikolai Koltsov proposed that inherited traits would be inherited via a "giant hereditary molecule" made up of "two mirror strands that would replicate in a semi-conservative fashion using each strand as a template". In 1928, Frederick Griffith in his experiment discovered that traits of the "smooth" form of "Pneumococcus" could be transferred to the "rough" form of the same bacteria by mixing killed "smooth" bacteria with the live "rough" form. This system provided the first clear suggestion that DNA carries genetic information—the Avery–MacLeod–McCarty experiment—when Oswald Avery, along with coworkers Colin MacLeod and Maclyn McCarty, identified DNA as the transforming principle in 1943. DNA's role in heredity was confirmed in 1952, when Alfred Hershey and Martha Chase in the Hershey–Chase experiment showed that DNA is the genetic material of the T2 phage.
In 1953, James Watson and Francis Crick suggested what is now accepted as the first correct double-helix model of DNA structure in the journal "Nature". Their double-helix, molecular model of DNA was then based on a single X-ray diffraction image (labeled as "Photo 51") taken by Rosalind Franklin and Raymond Gosling in May 1952, as well as the information that the DNA bases are paired — also obtained through private communications from Erwin Chargaff in the previous years. Chargaff's rules played a very important role in establishing double-helix configurations for B-DNA as well as A-DNA.
Experimental evidence supporting the Watson and Crick model was published in a series of five articles in the same issue of "Nature". Of these, Franklin and Gosling's paper was the first publication of their own X-ray diffraction data and original analysis method that partially supported the Watson and Crick model; this issue also contained an article on DNA structure by Maurice Wilkins and two of his colleagues, whose analysis and "in vivo" B-DNA X-ray patterns also supported the presence "in vivo" of the double-helical DNA configurations as proposed by Crick and Watson for their double-helix molecular model of DNA in the previous two pages of "Nature". In 1962, after Franklin's death, Watson, Crick, and Wilkins jointly received the Nobel Prize in Physiology or Medicine. Nobel Prizes were awarded only to living recipients at the time. A debate continues about who should receive credit for the discovery.
In an influential presentation in 1957, Crick laid out the central dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the "adaptor hypothesis". Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through the Meselson–Stahl experiment. Further work by Crick and coworkers showed that the genetic code was based on non-overlapping triplets of bases, called codons, allowing Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg to decipher the genetic code. These findings represent the birth of molecular biology.

</doc>
<doc id="7957" url="http://en.wikipedia.org/wiki?curid=7957" title="Kennedy family">
Kennedy family

The Kennedy family is an American family of Irish descent who are prominent in American politics and government. The first Kennedys to reside in America were farmer Patrick Kennedy (1823–1858) and his wife Bridget Murphy (c. 1824–1888), who sailed from Ireland to America in 1849. Their son P.J. (1858–1929) went into politics and business. P.J. and his wife Mary Hickey (1857–1923) were the parents of businessman/politician Joseph P. Kennedy, Sr. (1888–1969). The four sons of Joe Sr. and his wife philanthropist/socialite Rose Fitzgerald (1890–1995) were Joseph, Jr. (1915–1944), John F. (1917–1963), Robert F. (1925–1968), and Ted Kennedy (1932–2009). John served as president from January 1961 to his assassination in November 1963 while Robert and Ted both became prominent senators. The Kennedys' political involvement has revolved around the Democratic Party. Harvard University educations have been common among them, and they have contributed heavily to that university's John F. Kennedy School of Government. The wealth and glamor of the family members, as well as their extensive and continuing involvement in public service, has elevated them to iconic status over the past half-century, with the Kennedys sometimes referred to as "America's Royal Family".
Joseph Sr. originally pinned his hopes on eldest son, Joseph Jr., to enter politics and be elected president. After Joseph Jr. was killed in World War II, those hopes then fell on his second eldest son, John, to become president. Soon after John was elected in November 1960, he, Robert, and Ted all held prominent positions in the federal government. They received intense publicity, often emphasizing their relative youth, allure, education, and future in politics. From 1947, when John became a member of Congress, to 2011, when Ted's younger son Patrick (born 1967) departed Congress, there were 64 years with a Kennedy in elective office in Washington (excluding a short gap of less than a month when John resigned his Senate seat prior to his inauguration as president). This spans more than a quarter of the nation's existence.
The family has suffered numerous tragedies, contributing to the idea of the "Kennedy curse". Joe Sr. and Rose's eldest daughter, Rosemary (1918–2005), was made to undergo a lobotomy which turned out to be crippling. John and Robert were both assassinated in the 1960s. Ted was involved in the Chappaquiddick incident. Joe Jr., Kathleen (1920–1948), and John F., Jr. (1960–1999) were killed in plane crashes. Ted was also involved in a plane crash, but survived. Most recently, Robert F., Jr.'s (born 1954) second wife, Mary Richardson (1959–2012), committed suicide on May 16, 2012.

</doc>
<doc id="7958" url="http://en.wikipedia.org/wiki?curid=7958" title="Deflation (disambiguation)">
Deflation (disambiguation)

Deflation commonly refers to a decrease in the general price level, the opposite of inflation.
Deflation may also refer to:

</doc>
<doc id="7959" url="http://en.wikipedia.org/wiki?curid=7959" title="Democracy">
Democracy

Democracy is a form of government in which all eligible citizens are meant to participate equallyin the proposal, development and establishment of the laws by which their society is run. The term originates from the Greek ("") "rule of the people", which was found from δῆμος ("dêmos") "people" and κράτος ("kratos") "power" or "rule" in the 5th century BCE to denote the political systems then existing in Greek city-states, notably Athens; the term is an antonym to ἀριστοκρατία ("aristokratia") "rule of an elite". While theoretically these definitions are in opposition, in practice the distinction has been blurred historically. The political system of Classical Athens, for example, granted democratic citizenship to an elite class of free men and excluded slaves and women from political participation. In virtually all democratic governments throughout ancient and modern history, democratic citizenship consisted of an elite class until full enfranchisement was won for all adult citizens in most modern democracies through the suffrage movements of the 19th and 20th centuries. The English word dates to the 16th century, from the older Middle French and Middle Latin equivalents.
Democracy contrasts with forms of government where power is either held by an individual, as in an absolute monarchy, or where power is held by a small number of individuals, as in an oligarchy. Nevertheless, these oppositions, inherited from Greek philosophy, are now ambiguous because contemporary governments have mixed democratic, oligarchic, and monarchic elements. Karl Popper defined democracy in contrast to dictatorship or tyranny, thus focusing on opportunities for the people to control their leaders and to oust them without the need for a revolution.
Several variants of democracy exist, but there are two basic forms, both of which concern how the whole body of all eligible citizens executes its will. One form of democracy is direct democracy, in which all eligible citizens have direct and active participation in the political decision making. In most modern democracies, the whole body of eligible citizens remain the sovereign power but political power is exercised indirectly through elected representatives; this is called a representative democracy or democratic republic.
Characteristics.
No consensus exists on how to define democracy, but legal equality, freedom and rule of law have been identified as important characteristics since ancient times. These principles are reflected in all eligible citizens being equal before the law and having equal access to legislative processes. For example, in a representative democracy, every vote has equal weight, no unreasonable restrictions can apply to anyone seeking to become a representative, and the freedom of its eligible citizens is secured by legitimised rights and liberties which are typically protected by a constitution.
One theory holds that democracy requires three fundamental principles: 1) upward control, i.e. sovereignty residing at the lowest levels of authority, 2) political equality, and 3) social norms by which individuals and institutions only consider acceptable acts that reflect the first two principles of upward control and political equality.
The term "democracy" is sometimes used as shorthand for liberal democracy, which is a variant of representative democracy that may include elements such as political pluralism; equality before the law; the right to petition elected officials for redress of grievances; due process; civil liberties; human rights; and elements of civil society outside the government. Roger Scruton argues that democracy alone cannot provide personal and political freedom unless the institutions of civil society are also present.
In some countries, notably in the United Kingdom which originated the Westminster system, the dominant principle is that of parliamentary sovereignty, while maintaining judicial independence. In the United States, separation of powers is often cited as a central attribute. In India, the world's largest democracy, parliamentary sovereignty is subject to a constitution which includes judicial review. Other uses of "democracy" include that of direct democracy. Though the term "democracy" is typically used in the context of a political state, the principles also are applicable to private organisations.
Majority rule is often listed as a characteristic of democracy. Hence, democracy allows for political minorities to be oppressed by the "tyranny of the majority" in the absence of legal protections of individual or group rights. An essential part of an "ideal" representative democracy is competitive elections that are fair both substantively and procedurally. Furthermore, freedom of political expression, freedom of speech, and freedom of the press are considered to be essential rights that allow eligible citizens to be adequately informed and able to vote according to their own interests.
It has also been suggested that a basic feature of democracy is the capacity of all voters to participate freely and fully in the life of their society. With its emphasis on notions of social contract and the collective will of the all voters, democracy can also be characterised as a form of political collectivism because it is defined as a form of government in which all eligible citizens have an equal say in lawmaking.
While representative democracy is sometimes equated with the republican form of government, the term "republic" classically has encompassed both democracies and aristocracies. Many democracies are constitutional monarchies, such as the United Kingdom.
Nondemocracy.
Nondemocracies are governments that are not democratic. Examples include totalitarian states, autocracies, despots, autarchies, and dictatorships.
History.
Ancient origins.
The term "democracy" first appeared in ancient Greek political and philosophical thought in the city-state of Athens during classical antiquity. Led by Cleisthenes, Athenians established what is generally held as the first democracy in 508–507 BC. Cleisthenes is referred to as "the father of Athenian democracy."
Athenian democracy took the form of a direct democracy, and it had two distinguishing features: the random selection of ordinary citizens to fill the few existing government administrative and judicial offices, and a legislative assembly consisting of all Athenian citizens. All eligible citizens were allowed to speak and vote in the assembly, which set the laws of the city state. However, Athenian citizenship excluded women, slaves, foreigners (μέτοικοι "metoikoi"), non-landowners, and males under 20 years old.
Of the estimated 200,000 to 400,000 inhabitants of Athens, there were between 30,000 and 60,000 citizens. The exclusion of large parts of the population from the citizen body is closely related to the ancient understanding of citizenship. In most of antiquity the benefit of citizenship was tied to the obligation to fight war campaigns.
Athenian democracy was not only "direct" in the sense that decisions were made by the assembled people, but also the "most direct" in the sense that the people through the assembly, boule and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business. Even though the rights of the individual were not secured by the Athenian constitution in the modern sense (the ancient Greeks had no word for "rights"), the Athenians enjoyed their liberties not in opposition to the government but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person.
Range voting appeared in Sparta as early as 700 BC. The Apella was an assembly of the people, held once a month, in which every male citizen of age 30 could participate. In the Apella, Spartans elected leaders and cast votes by range voting and shouting. Aristotle called this "childish," as compared with the stone voting ballots used by the Athenians. Sparta adopted it because of its simplicity, and to prevent any bias voting, buying, or cheating that was predominant in the early democratic elections.
Even though the Roman Republic contributed significantly to many aspects of democracy, only a minority of Romans were citizens with votes in elections for representatives. The votes of the powerful were given more weight through a system of gerrymandering, so most high officials, including members of the Senate, came from a few wealthy and noble families. However, many notable exceptions did occur. In addition, the Roman Republic was the first government in the western world to have a Republic as a nation-state, although it didn't have much of a democracy. The Romans invented the concept of classics and many works from Ancient Greece were preserved. Additionally, the Roman model of governance inspired many political thinkers over the centuries, and today's modern representative democracies imitate more the Roman than the Greek models because it was a state in which supreme power was held by the people and their elected representatives, and which had an elected or nominated leader. Other cultures, such as the Iroquis Nation in the Americas between around 1450 and 1600 AD also developed a form of democratic society before they came in contact with the Europeans. This indicates that forms of democracy may have been invented in other societies around the world, even though much later than the first democracy established in Athens.
Middle Ages.
During the Middle Ages, there were various systems involving elections or assemblies, although often only involving a small part of the population. These included:
Most regions in medieval Europe were ruled by clergy or feudal lords.
The Kouroukan Fouga divided the Mali Empire into ruling clans (lineages) that were represented at a great assembly called the "Gbara". However, the charter made Mali more similar to a constitutional monarchy than a democratic republic. A little closer to modern democracy were the Cossack republics of Ukraine in the 16th and 17th centuries: Cossack Hetmanate and Zaporizhian Sich. The highest post – the Hetman – was elected by the representatives from the country's districts.
The Parliament of England had its roots in the restrictions on the power of kings written into Magna Carta (1215), which explicitly protected certain rights of the King's subjects, whether free or fettered – and implicitly supported what became the English writ of habeas corpus, safeguarding individual freedom against unlawful imprisonment with right to appeal. The first elected parliament was De Montfort's Parliament in England in 1265. The emergence of petitioning is some of the earliest evidence of parliament being used as a forum to address the general grievances of ordinary people. However, the power to call parliament remained at the pleasure of the monarch.
Modern era.
Early modern period.
During the early modern period, the power of the Parliament of England continually increased. Passage of the Petition of Right in 1628 and Habeas Corpus Act in 1679 established certain liberties and remain in effect. The idea of a political party took form with groups freely debating rights to political representation during the Putney Debates of 1647. After the English Civil Wars (1642–1651) and the Glorious Revolution of 1688, the Bill of Rights was enacted in 1689, which codified certain rights and liberties, and is still in effect. The Bill set out the requirement for regular elections, rules for freedom of speech in Parliament and limited the power of the monarch, ensuring that, unlike much of Europe at the time, royal absolutism would not prevail.
In North America, representative government began in Jamestown, Virginia, with the election of the House of Burgesses (forerunner of the Virginia General Assembly) in 1619. English Puritans who migrated from 1620 established colonies in New England whose local governance was democratic and which contributed to the democratic development of the United States; although these local assemblies had some small amounts of devolved power, the ultimate authority was held by the Crown and the English Parliament. The Puritans (Pilgrim Fathers), Baptists, and Quakers who founded these colonies applied the democratic organisation of their congregations also to the administration of their communities in worldly matters.
18th and 19th centuries.
The first Parliament of Great Britain was established in 1707, after the merger of the Kingdom of England and the Kingdom of Scotland under the Acts of Union. Although the monarch increasingly became a figurehead,
only a small minority actually had a voice; Parliament was elected by only a few percent of the population (less than 3% as late as 1780).
The creation of the short-lived Corsican Republic in 1755 marked the first nation in modern history to adopt a democratic constitution. This Corsican Constitution was the first based on Enlightenment principles and included female suffrage, something that was not granted in most other democracies until the 20th century.
In the American colonial period before 1776, and for some time after, often only adult white male property owners could vote; enslaved Africans, most free black people and most women were not extended the franchise. On the American frontier, democracy became a way of life, with more widespread social, economic and political equality. Although not described as a democracy by the founding fathers, they shared a determination to root the American experiment in the principles of natural freedom and equality.
The American Revolution led to the adoption of the United States Constitution in 1787. The Constitution provided for an elected government and protected civil rights and liberties for some, but did not end slavery nor give voting rights to women. This constitution is the oldest surviving, still active, governmental codified constitution. The Bill of Rights in 1791 set limits on government power to protect personal freedoms.
In 1789, Revolutionary France adopted the Declaration of the Rights of Man and of the Citizen and, although short-lived, the National Convention was elected by all males in 1792. But after the Napoleonic Wars (1803–1815), little of democracy - as theory, practice, or even as word - remained in the North Atlantic world.
During this period, slavery remained a social and economic institution in places around the world. This was particularly the case in the eleven states of the American South. A variety of organisations were established advocating the movement of black people from the United States to locations where they would enjoy greater freedom and equality.
The United Kingdom's Slave Trade Act 1807 banned the trade across the British Empire, enforced internationally by the Royal Navy's West Africa Squadron under treaties Britain negotiated with other nations. As the voting franchise in the U.K. was increased, it also was made more uniform; many rotten boroughs, with a small number of voters electing a Member of Parliament, were eliminated in a series of reforms beginning with the Reform Act of 1832. In 1833, the United Kingdom passed the Slavery Abolition Act which took effect across the British Empire.
Universal male suffrage was established in France in March 1848 in the wake of the French Revolution of 1848. In 1848, several revolutions broke out in Europe as rulers were confronted with popular demands for liberal constitutions and more democratic government.
In the 1860 United States Census, the slave population in the United States had grown to four million, and in Reconstruction after the Civil War (late 1860s), the newly freed slaves became citizens with a nominal right to vote for men. Full enfranchisement of citizens was not secured until after the African-American Civil Rights Movement (1955–1968) gained passage by the United States Congress of the Voting Rights Act of 1965.
20th and 21st centuries.
20th-century transitions to liberal democracy have come in successive "waves of democracy," variously resulting from wars, revolutions, decolonisation, and religious and economic circumstances. World War I and the dissolution of the Ottoman and Austro-Hungarian empires resulted in the creation of new nation-states from Europe, most of them at least nominally democratic.
In the 1920s democracy flourished and women's suffrage advanced, but the Great Depression brought disenchantment and most of the countries of Europe, Latin America, and Asia turned to strong-man rule or dictatorships. Fascism and dictatorships flourished in Nazi Germany, Italy, Spain and Portugal, as well as nondemocratic regimes in the Baltics, the Balkans, Brazil, Cuba, China, and Japan, among others.
World War II brought a definitive reversal of this trend in western Europe. The democratisation of the American, British, and French sectors of occupied Germany (disputed), Austria, Italy, and the occupied Japan served as a model for the later theory of regime change. However, most of Eastern Europe, including the Soviet sector of Germany fell into the non-democratic Soviet bloc.
The war was followed by decolonisation, and again most of the new independent states had nominally democratic constitutions. India emerged as the world's largest democracy and continues to be so. Countries that were once part of the British Empire often adopted the British Westminster system.
By 1960, the vast majority of country-states were nominally democracies, although most of the world's populations lived in nations that experienced sham elections, and other forms of subterfuge (particularly in Communist nations and the former colonies.)
A subsequent wave of democratisation brought substantial gains toward true liberal democracy for many nations. Spain, Portugal (1974), and several of the military dictatorships in South America returned to civilian rule in the late 1970s and early 1980s (Argentina in 1983, Bolivia, Uruguay in 1984, Brazil in 1985, and Chile in the early 1990s). This was followed by nations in East and South Asia by the mid-to-late 1980s.
Economic malaise in the 1980s, along with resentment of Soviet oppression, contributed to the collapse of the Soviet Union, the associated end of the Cold War, and the democratisation and liberalisation of the former Eastern bloc countries. The most successful of the new democracies were those geographically and culturally closest to western Europe, and they are now members or candidate members of the European Union.
The liberal trend spread to some nations in Africa in the 1990s, most prominently in South Africa. Some recent examples of attempts of liberalisation include the Indonesian Revolution of 1998, the Bulldozer Revolution in Yugoslavia, the Rose Revolution in Georgia, the Orange Revolution in Ukraine, the Cedar Revolution in Lebanon, the Tulip Revolution in Kyrgyzstan, and the Jasmine Revolution in Tunisia.
According to Freedom House, in 2007 there were 123 electoral democracies (up from 40 in 1972). According to "World Forum on Democracy", electoral democracies now represent 120 of the 192 existing countries and constitute 58.2 percent of the world's population. At the same time liberal democracies i.e. countries Freedom House regards as free and respectful of basic human rights and the rule of law are 85 in number and represent 38 percent of the global population.
In 2010 the United Nations declared September 15 the International Day of Democracy.
Countries and regions.
The following countries or regions are categorised by the Democracy Index 2012, complied by the Economist Intelligence Unit, as a "Full democracy":
The Index assigns 53 countries or regions to the lower category, "Flawed democracy": Argentina, Benin, Botswana, Brazil, Bulgaria, Cape Verde, Chile, Colombia, Croatia, Cyprus, Dominican Republic, El Salvador, Estonia, France, Ghana, Greece, Guyana, Hungary, Indonesia, India, Israel, Italy, Jamaica, Latvia, Lesotho, Lithuania, Macedonia, Malaysia, Mali, Mexico, Moldova, Mongolia, Montenegro, Namibia, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Poland, Portugal, Indonesia, Romania, Serbia, Slovakia, Slovenia, South Africa, Sri Lanka, Suriname, Taiwan, Thailand, Timor-Leste, Trinidad and Tobago, Zambia.
Types.
Democracy has taken a number of forms, both in theory and practice. Some varieties of democracy provide better representation and more freedom for their citizens than others. However, if any democracy is not structured so as to prohibit the government from excluding the people from the legislative process, or any branch of government from altering the separation of powers in its own favour, then a branch of the system can accumulate too much power and destroy the democracy.
The following kinds of democracy are not exclusive of one another: many specify details of aspects that are independent of one another and can co-exist in a single system.
Basic forms.
Representative democracy is a form of democracy in which people vote for representatives who then vote on policy initiatives as opposed to a direct democracy, a form of democracy in which people vote on policy initiatives directly.
Direct.
Direct democracy is a political system where the citizens participate in the decision-making personally, contrary to relying on intermediaries or representatives. The supporters of direct democracy argue that democracy is more than merely a procedural issue. A direct democracy gives the voting population the power to:
Direct democracy only exists in the Swiss cantons of Appenzell Innerrhoden and Glarus.
Representative.
Representative democracy involves the election of government officials by the people being represented. If the head of state is also democratically elected then it is called a democratic republic. The most common mechanisms involve election of the candidate with a majority or a plurality of the votes. Most western countries have representative systems.
Representatives may be elected or become diplomatic representatives by a particular district (or constituency), or represent the entire electorate through proportional systems, with some using a combination of the two. Some representative democracies also incorporate elements of direct democracy, such as referendums. A characteristic of representative democracy is that while the representatives are elected by the people to act in the people's interest, they retain the freedom to exercise their own judgement as how best to do so. Such reasons have driven criticism upon representative democracy, pointing out the contradictions of representation mechanisms' with democracy
Parliamentary.
Parliamentary democracy is a representative democracy where government is appointed by, or can be dismissed by, representatives as opposed to a "presidential rule" wherein the president is both head of state and the head of government and is elected by the voters. Under a parliamentary democracy, government is exercised by delegation to an executive ministry and subject to ongoing review, checks and balances by the legislative parliament elected by the people.
Parliamentary systems have the right to dismiss a Prime Minister at any point in time that they feel he or she is not doing their job to the expectations of the legislature. This is done through a Vote of No Confidence where the legislature decides whether or not to remove the Prime Minister from office by a majority support for his or her dismissal. In some countries, the Prime Minister can also call an election whenever he or she so chooses, and typically the Prime Minister will hold an election when he or she knows that they are in good favour with the public as to get re-elected. In other parliamentary democracies extra elections are virtually never held, a minority government being preferred until the next ordinary elections. An important feature of the parliamentary democracy is the concept of the "loyal opposition". The essence of the concept is that the second largest political party (or coalition) opposes the governing party (or coalition), while still remaining loyal to the state and its democratic principles.
Presidential.
Presidential Democracy is a system where the public elects the president through free and fair elections. The president serves as both the head of state and head of government controlling most of the executive powers. The president serves for a specific term and cannot exceed that amount of time. Elections typically have a fixed date and aren't easily changed. The president has direct control over the cabinet, specifically appointing the cabinet members.
The president cannot be easily removed from office by the legislature, but he or she cannot remove members of the legislative branch any more easily. This provides some measure of separation of powers. In consequence however, the president and the legislature may end up in the control of separate parties, allowing one to block the other and thereby interfere with the orderly operation of the state. This may be the reason why presidential democracy is not very common outside the Americas, Africa, and Central and Southeast Asia.
A semi-presidential system is a system of democracy in which the government includes both a prime minister and a president. The particular powers held by the prime minister and president vary by country.
Hybrid or semi-direct.
Some modern democracies that are predominately representative in nature also heavily rely upon forms of political action that are directly democratic. These democracies, which combine elements of representative democracy and direct democracy, are termed "hybrid democracies", "semi-direct democracies" or "participatory democracies". Examples include Switzerland and some U.S. states, where frequent use is made of referendums and initiatives.
The Swiss confederation is a semi-direct democracy. At the federal level, citizens can propose changes to the constitution (federal popular initiative) or ask for a referendum to be held on any law voted by the parliament. Between January 1995 and June 2005, Swiss citizens voted 31 times, to answer 103 questions (during the same period, French citizens participated in only two referendums). Although in the past 120 years less than 250 initiatives have been put to referendum. The populace has been conservative, approving only about 10% of the initiatives put before them; in addition, they have often opted for a version of the initiative rewritten by government.
In the United States, no mechanisms of direct democracy exists at the federal level, but over half of the states and many localities provide for citizen-sponsored ballot initiatives (also called "ballot measures", "ballot questions" or "propositions"), and the vast majority of states allow for referendums. Examples include the extensive use of referendums in the US state of California, which is a state that has more than 20 million voters.
In New England Town meetings are often used, especially in rural areas, to manage local government. This creates a hybrid form of government, with a local direct democracy and a state government which is representative. For example, most Vermont towns hold annual town meetings in March in which town officers are elected, budgets for the town and schools are voted on, and citizens have the opportunity to speak and be heard on political matters.
Variants.
Republic.
In contemporary usage, the term "democracy" refers to a government chosen by the people, whether it is direct or representative. The term "republic" has many different meanings, but today often refers to a representative democracy with an elected head of state, such as a president, serving for a limited term, in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies with an elected or appointed head of government such as a prime minister.
The Founding Fathers of the United States rarely praised and often criticised democracy, which in their time tended to specifically mean direct democracy, often without the protection of a Constitution enshrining basic rights; James Madison argued, especially in "The Federalist" No. 10, that what distinguished a "democracy" from a "republic" was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combats faction by its very structure.
What was critical to American values, John Adams insisted, was that the government be "bound by fixed laws, which the people have a voice in making, and a right to defend." As Benjamin Franklin was exiting after writing the U.S. constitution, a woman asked him "Well, Doctor, what have we got—a republic or a monarchy?". He replied "A republic—if you can keep it."
Constitutional monarchy.
Initially after the American and French revolutions, the question was open whether a democracy, in order to restrain unchecked majority rule, should have an élite upper chamber, the members perhaps appointed meritorious experts or having lifetime tenures, or should have a constitutional monarch with limited but real powers. Some countries (such as the United Kingdom, the Netherlands, Belgium, Scandinavian countries, Thailand, Japan and Bhutan) turned powerful monarchs into constitutional monarchs with limited or, often gradually, merely symbolic roles.
Often the monarchy was abolished along with the aristocratic system (as in France, China, Russia, Germany, Austria, Hungary, Italy, Greece and Egypt). Many nations had élite upper houses of legislatures which often had lifetime tenure, but eventually these lost power (as in Britain) or else became elective and remained powerful.
Liberal democracy.
A liberal democracy is a representative democracy in which the ability of the elected representatives to exercise decision-making power is subject to the rule of law, and moderated by a constitution or laws that emphasise the protection of the rights and freedoms of individuals, and which places constraints on the leaders and on the extent to which the will of the majority can be exercised against the rights of minorities (see civil liberties).
In a liberal democracy, it is possible for some large-scale decisions to emerge from the many individual decisions that citizens are free to make. In other words, citizens can "vote with their feet" or "vote with their dollars", resulting in significant informal government-by-the-masses that exercises many "powers" associated with formal government elsewhere.
Socialist.
Socialist thought has several different views on democracy. Social democracy, democratic socialism, and the dictatorship of the proletariat (usually exercised through Soviet democracy) are some examples. Many democratic socialists and social democrats believe in a form of participatory democracy and/or workplace democracy combined with a representative democracy.
Within Marxist orthodoxy there is a hostility to what is commonly called "liberal democracy", which they simply refer to as parliamentary democracy because of its often centralised nature. Because of their desire to eliminate the political elitism they see in capitalism, Marxists, Leninists and Trotskyists believe in direct democracy implemented through a system of communes (which are sometimes called soviets). This system ultimately manifests itself as council democracy and begins with workplace democracy. (See Democracy in Marxism.)
Anarchist.
Anarchists are split in this domain, depending on whether they believe that a majority-rule is tyrannic or not.
The only form of democracy considered acceptable to many anarchists is direct democracy. Pierre-Joseph Proudhon argued that the only acceptable form of direct democracy is one in which it is recognised that majority decisions are not binding on the minority, even when unanimous. However, anarcho-communist Murray Bookchin criticised individualist anarchists for opposing democracy, and says "majority rule" is consistent with anarchism.
Some anarcho-communists oppose the majoritarian nature of direct democracy, feeling that it can impede individual liberty and opt in favour of a non-majoritarian form of consensus democracy, similar to Proudhon's position on direct democracy. Henry David Thoreau, who did not self-identify as an anarchist but argued for "a better government" and is cited as an inspiration by some anarchists, argued that people should not be in the position of ruling others or being ruled when there is no consent.
Anarcho-capitalists, voluntaryists and other right-anarchists oppose institutional democracy as they consider it in conflict with widely held moral values and ethical principles and their conception of individual rights. The "a priori" Rothbardian argument is that the state is a coercive institution which necessarily violates the non-aggression principle (NAP). Some right-anarchists also criticise democracy on "a posteriori" consequentialist grounds, in terms of inefficiency or disability in bringing about maximisation of individual liberty. They maintain the people who participate in democratic institutions are foremost driven by economic self-interest.
Demarchy.
Sometimes called "democracy without elections", demarchy uses sortition to choose decision makers via a random process. The intention is that those chosen will be representative of the opinions and interests of the people at large, and be more fair and impartial than an elected official. The technique was in widespread use in Athenian Democracy and is still used in modern jury selection.
Consociational.
A consociational democracy allows for simultaneous majority votes in two or more ethno-religious constituencies, and policies are enacted only if they gain majority support from both or all of them.
Consensus democracy.
A consensus democracy, in contrast, would not be dichotomous. Instead, decisions would be based on a multi-option approach, and policies would be enacted if they gained sufficient support, either in a purely verbal agreement, or via a consensus vote - a multi-option preference vote. If the threshold of support were at a sufficiently high level, minorities would be as it were protected automatically. Furthermore, any voting would be ethno-colour blind.
Supranational.
Qualified majority voting is designed by the Treaty of Rome to be the principal method of reaching decisions in the European Council of Ministers. This system allocates votes to member states in part according to their population, but heavily weighted in favour of the smaller states. This might be seen as a form of representative democracy, but representatives to the Council might be appointed rather than directly elected.
Inclusive.
Inclusive democracy is a political theory and political project that aims for direct democracy in all fields of social life: political democracy in the form of face-to-face assemblies which are confederated, economic democracy in a stateless, moneyless and marketless economy, democracy in the social realm, i.e. self-management in places of work and education, and ecological democracy which aims to reintegrate society and nature. The theoretical project of inclusive democracy emerged from the work of political philosopher Takis Fotopoulos in "Towards An Inclusive Democracy" and was further developed in the journal "Democracy & Nature"and its successor "The International Journal of Inclusive Democracy".
The basic unit of decision making in an inclusive democracy is the demotic assembly, i.e. the assembly of demos, the citizen body in a given geographical area which may encompass a town and the surrounding villages, or even neighbourhoods of large cities. An inclusive democracy today can only take the form of a confederal democracy that is based on a network of administrative councils whose members or delegates are elected from popular face-to-face democratic assemblies in the various demoi. Thus, their role is purely administrative and practical, not one of policy-making like that of representatives in representative democracy.
The citizen body is advised by experts but it is the citizen body which functions as the ultimate decision-taker . Authority can be delegated to a segment of the citizen body to carry out specific duties, for example to serve as members of popular courts, or of regional and confederal councils. Such delegation is made, in principle, by lot, on a rotation basis, and is always recallable by the citizen body. Delegates to regional and confederal bodies should have specific mandates.
Participatory politics.
A Parpolity or Participatory Polity is a theoretical form of democracy that is ruled by a Nested Council structure. The guiding philosophy is that people should have decision making power in proportion to how much they are affected by the decision. Local councils of 25–50 people are completely autonomous on issues that affect only them, and these councils send delegates to higher level councils who are again autonomous regarding issues that affect only the population affected by that council.
A council court of randomly chosen citizens serves as a check on the tyranny of the majority, and rules on which body gets to vote on which issue. Delegates may vote differently from how their sending council might wish, but are mandated to communicate the wishes of their sending council. Delegates are recallable at any time. Referendums are possible at any time via votes of most lower-level councils, however, not everything is a referendum as this is most likely a waste of time. A parpolity is meant to work in tandem with a participatory economy.
Cosmopolitan.
Cosmopolitan democracy, also known as "Global democracy" or "World Federalism", is a political system in which democracy is implemented on a global scale, either directly or through representatives. An important justification for this kind of system is that the decisions made in national or regional democracies often affect people outside the constituency who, by definition, cannot vote. By contrast, in a cosmopolitan democracy, the people who are affected by decisions also have a say in them.
According to its supporters, any attempt to solve global problems is undemocratic without some form of cosmopolitan democracy. The general principle of cosmopolitan democracy is to expand some or all of the values and norms of democracy, including the rule of law; the non-violent resolution of conflicts; and equality among citizens, beyond the limits of the state. To be fully implemented, this would require reforming existing international organisations, e.g. the United Nations, as well as the creation of new institutions such as a World Parliament, which ideally would enhance public control over, and accountability in, international politics.
Cosmopolitan Democracy has been promoted, among others, by physicist Albert Einstein, writer Kurt Vonnegut, columnist George Monbiot, and professors David Held and Daniele Archibugi. The creation of the International Criminal Court in 2003 was seen as a major step forward by many supporters of this type of cosmopolitan democracy.
Non-governmental.
Aside from the public sphere, similar democratic principles and mechanisms of voting and representation have been used to govern other kinds of groups. Many non-governmental organisations decide policy and leadership by voting. Most trade unions and cooperatives are governed by democratic elections. Corporations are controlled by shareholders on the principle of one share, one vote.
Theory.
Aristotle.
Aristotle contrasted rule by the many (democracy/polity), with rule by the few (oligarchy/aristocracy), and with rule by a single person (tyranny or today autocracy/monarchy). He also thought that there was a good and a bad variant of each system (he considered democracy to be the degenerate counterpart to polity).
For Aristotle the underlying principle of democracy is freedom, since only in a democracy the citizens can have a share in freedom. In essence, he argues that this is what every democracy should make its aim. There are two main aspects of freedom: being ruled and ruling in turn, since everyone is equal according to number, not merit, and to be able to live as one pleases.
Rationale.
Among modern political theorists, there are three contending conceptions of the fundamental rationale for democracy: "aggregative democracy," "deliberative democracy," and "radical democracy."
Aggregative.
The theory of "aggregative democracy" claims that the aim of the democratic processes is to solicit citizens' preferences and aggregate them together to determine what social policies society should adopt. Therefore, proponents of this view hold that democratic participation should primarily focus on voting, where the policy with the most votes gets implemented.
Different variants of aggregative democracy exist. Under "minimalism", democracy is a system of government in which citizens have given teams of political leaders the right to rule in periodic elections. According to this minimalist conception, citizens cannot and should not "rule" because, for example, on most issues, most of the time, they have no clear views or their views are not well-founded. Joseph Schumpeter articulated this view most famously in his book "Capitalism, Socialism, and Democracy". Contemporary proponents of minimalism include William H. Riker, Adam Przeworski, Richard Posner.
According to the theory of direct democracy, on the other hand, citizens should vote directly, not through their representatives, on legislative proposals. Proponents of direct democracy offer varied reasons to support this view. Political activity can be valuable in itself, it socialises and educates citizens, and popular participation can check powerful elites. Most importantly, citizens do not really rule themselves unless they directly decide laws and policies.
Governments will tend to produce laws and policies that are close to the views of the median voter – with half to their left and the other half to their right. This is not actually a desirable outcome as it represents the action of self-interested and somewhat unaccountable political elites competing for votes. Anthony Downs suggests that ideological political parties are necessary to act as a mediating broker between individual and governments. Downs laid out this view in his 1957 book "An Economic Theory of Democracy".
Robert A. Dahl argues that the fundamental democratic principle is that, when it comes to binding collective decisions, each person in a political community is entitled to have his/her interests be given equal consideration (not necessarily that all people are equally satisfied by the collective decision). He uses the term polyarchy to refer to societies in which there exists a certain set of institutions and procedures which are perceived as leading to such democracy. First and foremost among these institutions is the regular occurrence of free and open elections which are used to select representatives who then manage all or most of the public policy of the society. However, these polyarchic procedures may not create a full democracy if, for example, poverty prevents political participation.
Deliberative.
"Deliberative democracy" is based on the notion that democracy is government by deliberation. Unlike aggregative democracy, deliberative democracy holds that, for a democratic decision to be legitimate, it must be preceded by authentic deliberation, not merely the aggregration of preferences that occurs in voting. "Authentic deliberation" is deliberation among decision-makers that is free from distortions of unequal political power, such as power a decision-maker obtained through economic wealth or the support of interest groups. If the decision-makers cannot reach consensus after authentically deliberating on a proposal, then they vote on the proposal using a form of majority rule.
Radical.
"Radical democracy" is based on the idea that there are hierarchical and oppressive power relations that exist in society. Democracy's role is to make visible and challenge those relations by allowing for difference, dissent and antagonisms in decision making processes.
Criticism.
Inefficiencies.
Economists like Milton Friedman have strongly criticised the efficiency of democracy. They base this on their premise of the irrational voter. Their argument is that voters are highly uninformed about many political issues, especially relating to economics, and have a strong bias about the few issues on which they are fairly knowledgeable.
Popular rule as a façade.
The 20th-century Italian thinkers Vilfredo Pareto and Gaetano Mosca (independently) argued that democracy was illusory, and served only to mask the reality of elite rule. Indeed, they argued that elite oligarchy is the unbendable law of human nature, due largely to the apathy and division of the masses (as opposed to the drive, initiative and unity of the elites), and that democratic institutions would do no more than shift the exercise of power from oppression to manipulation. As Louis Brandeis once professed, "We may have democracy, or we may have wealth concentrated in the hands of a few, but we can't have both."
All political parties in Canada are now cautious about criticism of the high level of immigration, because, as noted by "The Globe and Mail", "in the early 1990s, the old Reform Party was branded 'racist' for suggesting that immigration levels be lowered from 250,000 to 150,000." As Professor of Economics Don J. DeVoretz pointed out, "In a liberal democracy such as Canada, the following paradox persists. Even though the majority of respondents answer yes to the question: 'Are there too many immigrant arrivals each year?' immigrant numbers continue to rise until a critical set of economic costs appear."
Mob rule.
Plato's "The Republic" presents a critical view of democracy through the narration of Socrates: "Democracy, which is a charming form of government, full of variety and disorder, and dispensing a sort of equality to equals and unequaled alike." In his work, Plato lists 5 forms of government from best to worst. Assuming that "the Republic" was intended to be a serious critique of the political thought in Athens, Plato argues that only Kallipolis, an aristocracy led by the unwilling philosopher-kings (the wisest men), is a just form of government.
James Madison critiqued direct democracy (which he referred to simply as "democracy") in Federalist No. 10, arguing that representative democracy—which he described using the term "republic"—is a preferable form of government, saying: "... democracies have ever been spectacles of turbulence and contention; have ever been found incompatible with personal security or the rights of property; and have in general been as short in their lives as they have been violent in their deaths." Madison offered that republics were superior to democracies because republics safeguarded against tyranny of the majority, stating in Federalist No. 10: "the same advantage which a republic has over a democracy, in controlling the effects of faction, is enjoyed by a large over a small republic".
Political instability.
More recently, democracy is criticised for not offering enough political stability. As governments are frequently elected on and off there tends to be frequent changes in the policies of democratic countries both domestically and internationally. Even if a political party maintains power, vociferous, headline grabbing protests and harsh criticism from the mass media are often enough to force sudden, unexpected political change. Frequent policy changes with regard to business and immigration are likely to deter investment and so hinder economic growth. For this reason, many people have put forward the idea that democracy is undesirable for a developing country in which economic growth and the reduction of poverty are top priorities.
This opportunist alliance not only has the handicap of having to cater to too many ideologically opposing factions, but it is usually short lived since any perceived or actual imbalance in the treatment of coalition partners, or changes to leadership in the coalition partners themselves, can very easily result in the coalition partner withdrawing its support from the government.
Fraudulent elections.
In representative democracies, it may not benefit incumbents to conduct fair elections. A study showed that incumbents who rig elections stay in office 2.5 times as long as those who permit fair elections. In countries with income above per capita, democracies have been found to be less prone to violence, but below that threshold, more prone violence. Election misconduct is more likely in countries with low per capita incomes, small populations, rich in natural resources, and a lack of institutional checks and balances. Sub-Saharan countries, as well as Afghanistan, all tend to fall into that category.
Governments that have frequent elections tend to have significantly more stable economic policies than those governments who have infrequent elections. However, this trend does not apply to governments that hold fraudulent elections.
Opposition.
Democracy in modern times has almost always faced opposition from the previously existing government, and many times it has faced opposition from social elites. The implementation of a democratic government within a non-democratic state is typically brought about by democratic revolution. Monarchy had traditionally been opposed to democracy, and to this day remains opposed to the abolition of its privileges, although often political compromise has been reached in the form of shared government.
Post-Enlightenment ideologies such as fascism, Nazism and neo-fundamentalism oppose democracy on different grounds, generally citing that the concept of democracy as a constant process is flawed and detrimental to a preferable course of development.
Development.
Several philosophers and researchers outlined historical and social factors supporting the evolution of democracy.
"Cultural factors" like "Protestantism" influenced the development of democracy, rule of law, human rights and political liberty (the faithful elected priests, religious freedom and tolerance has been practiced).
Others mentioned the influence of "wealth" (e.g. S. M. Lipset, 1959). In a related theory, Ronald Inglehart suggests that the increase in living standards has convinced people that they can take their basic survival for granted, and led to increased emphasis on self-expression values, which is highly correlated to democracy.
Carroll Quigley concludes that the characteristics of weapons are the main predictor of democracy: Democracy tends to emerge only when the best weapons available are easy for individuals to buy and use. By the 1800s, guns were the best weapon available, and in America, almost everyone could afford to buy a gun, and could learn how to use it fairly easily. Governments couldn't do any better: It became the age of mass armies of citizen soldiers with guns Similarly, Periclean Greece was an age of the citizen soldier and democracy.
Recently established theories stress the relevance of "education" and "human capital" and within them of "cognitive ability" to increasing tolerance, rationality, political literacy and participation. Two effects of education and cognitive ability are distinguished: a cognitive effect (competence to make rational choices, better information processing) and an ethical effect (support of democratic values, freedom, human rights etc.), which itself depends on intelligence.
Evidence that is consistent with conventional theories of why democracy emerges and is sustained has been hard to come by. Recent statistical analyses have challenged modernisation theory by demonstrating that there is no reliable evidence for the claim that democracy is more likely to emerge when countries become wealthier, more educated, or less unequal. Neither is there convincing evidence that increased reliance on oil revenues prevents democratisation, despite a vast theoretical literature called "The Resource Curse" that asserts that oil revenues sever the link between citizen taxation and government accountability, the key to representative democracy. The lack of evidence for these conventional theories of democratisation have led researchers to search for the "deep" determinants of contemporary political institutions, be they geographical or demographic.
In the 21st century, democracy has become such a popular method of reaching decisions that its application beyond politics to other areas such as entertainment, food and fashion, consumerism, urban planning, education, art, literature, science and theology has been criticised as "the reigning dogma of our time". The argument is that applying a populist or market-driven approach to art and literature for example, means that innovative creative work goes unpublished or unproduced. In education, the argument is that essential but more difficult studies are not undertaken. Science, which is a truth-based discipline, is particularly corrupted by the idea that the correct conclusion can be arrived at by popular vote.
Robert Michels asserts that although democracy can never be fully realised, democracy may be developed automatically in the act of striving for democracy: "The peasant in the fable, when on his death-bed, tells his sons that a treasure is buried in the field. After the old man's death the sons dig everywhere in order to discover the treasure. They do not find it. But their indefatigable labor improves the soil and secures for them a comparative well-being. The treasure in the fable may well symbolise democracy."
Dr. Harald Wydra, in his book "Communism and The Emergence of Democracy", maintains that the development of democracy should not be viewed as a purely procedural or as a static concept but rather as an ongoing "process of meaning formation". Drawing on Claude Lefort's idea of the empty place of power, that "power emanates from the people […] but is the power of nobody", he remarks that democracy is reverence to a symbolic mythical authority as in reality, there is no such thing as the people or "demos". Democratic political figures are not supreme rulers but rather temporary guardians of an empty place. Any claim to substance such as the collective good, the public interest or the will of the nation is subject to the competitive struggle and times of for gaining the authority of office and government. The essence of the democratic system is an empty place, void of real people which can only be temporarily filled and never be appropriated. The seat of power is there, but remains open to constant change. As such, what "democracy" is or what is "democratic" progresses throughout history as a continual and potentially never ending process of social construction.
In 2010 a study by a German military think tank has analyzed how peak oil might change the global economy. The study raises fears for the survival of democracy itself. It suggests that parts of the population could perceive the upheaval triggered by peak oil as a general systemic crisis. This would create "room for ideological and extremist alternatives to existing forms of government".

</doc>
<doc id="7960" url="http://en.wikipedia.org/wiki?curid=7960" title="Deduction and induction">
Deduction and induction

Deduction and induction may refer to:

</doc>
<doc id="7962" url="http://en.wikipedia.org/wiki?curid=7962" title="Logical disjunction">
Logical disjunction

In logic and mathematics, or is a truth-functional operator also known as (inclusive) disjunction and alternation. The logical connective that represents this operator is also known as "or", and typically written as formula_1 or formula_2. The "or" operator produces a result of true whenever "one or more" of its operands are true. For example, in this context, ""A" or "B"" is true if "A" is true, or if "B" is true, or if both "A" and "B" are true. In grammar, or is a coordinating conjunction. An operand of a disjunction is called a disjunct.
The "or" operator differs from the exclusive or in that the latter returns false when both of its inputs are true, while "or" returns true. In ordinary language, outside of contexts such as formal logic, mathematics and programming, "or" sometimes has the meaning of "exclusive or". For example, "Please ring me or send an email" likely means "do one or the other, but not both". On the other hand, "Her grades are so good that she's either very bright or studies hard" allows for the possibility that the person is both bright and works hard. In other words, in ordinary language "or" can mean inclusive or exclusive or. Usually the intended meaning is clear from the context.
Notation.
Or is usually expressed with the prefix operator A, or with an infix operator. In mathematics and logic, the infix operator is usually ∨; in electronics, +; and in programming languages, | or or. Some programming languages have a related control structure, the short-circuit or, written ||, or else, etc.
Definition.
Logical disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of "false" if and only if both of its operands are false. More generally a disjunction is a logical formula that can have one or more literals separated only by ORs. A single literal is often considered to be a degenerate disjunction.
The disjunctive identity is 0, which is to say that OR-ing an expression with 0 will never change the value of the expression. In keeping with the concept of vacuous truth, when disjunction is defined as an operator or function of arbitrary arity, the empty disjunction (OR-ing over an empty set of operands) is often defined as having the result 0.
Truth table.
The truth table of formula_3:
Properties.
When all inputs are true, the output is true.
When all inputs are false, the output is false.
If using binary values for true (1) and false (0), then "logical disjunction" works almost like binary addition. The only difference is that formula_4, while formula_5.
Symbol.
The mathematical symbol for logical disjunction varies in the literature. In addition to the word "or", and the formula "A"pq"", the symbol "formula_6", deriving from the Latin word "vel" (“either”, “or”) is commonly used for disjunction. For example: ""A" formula_6 "B" " is read as ""A" or "B" ". Such a disjunction is false if both "A" and "B" are false. In all other cases it is true.
All of the following are disjunctions:
The corresponding operation in set theory is the set-theoretic union.
Applications in computer science.
Operators corresponding to logical disjunction exist in most programming languages.
Bitwise operation.
Disjunction is often used for bitwise operations. Examples:
The codice_1 operator can be used to set bits in a bit field to 1, by codice_1-ing the field with a constant field with the relevant bits set to 1. For example, codice_3 will force the final bit to 1 while leaving other bits unchanged.
Logical operation.
Many languages distinguish between bitwise and logical disjunction by providing two distinct operators; in languages following C, bitwise disjunction is performed with the single pipe (codice_4) and logical disjunction with the double pipe (codice_5) operators.
Logical disjunction is usually short-circuited; that is, if the first (left) operand evaluates to codice_6 then the second (right) operand is not evaluated. The logical disjunction operator thus usually constitutes a sequence point.
In a parallel (concurrent) language, it is possible to short-circuit both sides: they are evaluated in parallel,
and if one terminates with value true, the other is interrupted. This operator is thus called the parallel or.
Although in most languages the type of a logical disjunction expression is boolean and thus can only have the value codice_6 or codice_8, in some (such as Python and JavaScript) the logical disjunction operator returns one of its operands: the first operand if it evaluates to a true value, and the second operand otherwise.
Constructive disjunction.
The Curry–Howard correspondence relates a constructivist form of disjunction to tagged union types.
Union.
The membership of an element of an union set in set theory is defined in terms of a logical disjunction: "x" ∈ "A" ∪ "B" if and only if ("x" ∈ "A") ∨ ("x" ∈ "B"). Because of this, logical disjunction satisfies many of the same identities as set-theoretic union, such as associativity, commutativity, distributivity, and de Morgan's laws.

</doc>
<doc id="7963" url="http://en.wikipedia.org/wiki?curid=7963" title="Disjunctive syllogism">
Disjunctive syllogism

In classical logic disjunctive syllogism (historically known as modus tollendo ponens) is a valid argument form which is a syllogism having a disjunctive statement for one of its premises. 
In propositional logic, disjunctive syllogism (also known as disjunction elimination and or elimination, or abbreviated ∨E), is a valid rule of inference. If we are told that at least one of two statements is true; and also told that it is not the former that is true; we can infer that it has to be the latter that is true. If either "P" or "Q" is true and "P" is false, then "Q" is true. The reason this is called "disjunctive syllogism" is that, first, it is a syllogism, a three-step argument, and second, it contains a logical disjunction, which simply means an "or" statement. "Either P or Q" is a disjunction; P and Q are called the statement's "disjuncts". The rule makes it possible to eliminate a disjunction from a logical proof. It is the rule that:
where the rule is that whenever instances of "formula_2", and "formula_3" appear on lines of a proof, "formula_4" can be placed on a subsequent line.
Disjunctive syllogism is closely related and similar to hypothetical syllogism, in that it is also type of syllogism, and also the name of a rule of inference.
Formal notation.
The "disjunctive syllogism" rule may be written in sequent notation:
where formula_6 is a metalogical symbol meaning that formula_4 is a syntactic consequence of formula_8, and formula_9 in some logical system;
and expressed as a truth-functional tautology or theorem of propositional logic:
where formula_11, and formula_4 are propositions expressed in some formal system.
Natural language examples.
Here is an example:
Here is another example:
Inclusive and exclusive disjunction.
Please observe that the disjunctive syllogism works whether 'or' is considered 'exclusive' or 'inclusive' disjunction. See below for the definitions of these terms.
There are two kinds of logical disjunction:
The widely used English language concept of "or" is often ambiguous between these two meanings, but the difference is pivotal in evaluating disjunctive arguments. 
This argument:
is valid and indifferent between both meanings. However, only in the "exclusive" meaning is the following form valid:
however if the fact is true it does not commit the fallacy
With the "inclusive" meaning you could draw no conclusion from the first two premises of that argument. See affirming a disjunct.
Related argument forms.
Unlike modus ponendo ponens and modus ponendo tollens, with which it should not be confused, disjunctive syllogism is often not made an explicit rule or axiom of logical systems, as the above arguments can be proven with a (slightly devious) combination of reductio ad absurdum and disjunction elimination.
"Other forms of syllogism:" 
Disjunctive syllogism holds in classical propositional logic and intuitionistic logic, but not in some paraconsistent logics.

</doc>
<doc id="7964" url="http://en.wikipedia.org/wiki?curid=7964" title="Definition">
Definition

A definition is a statement of the meaning of a term (a word, phrase, or other set of symbols). The term to be defined is the "definiendum". The term may have many different senses and multiple meanings. For each meaning, a "definiens" is a cluster of words that defines that term (and clarifies the speaker's intention).
A definition will vary in aspects like precision or popularity. There are also different types of definitions with different purposes and focuses (e.g. intensional, extensional, descriptive, stipulative, and so on). 
A chief difficulty in the management of definitions is the necessity of using other terms that are already understood or whose definitions are easily obtainable or demonstrable (e.g. a need, sometimes, for ostensive definitions).
A dictionary definition typically contains additional details about a word, such as an etymology and the language or languages of its origin, or obsolete meanings.
Basic considerations.
In formal languages like mathematics, a "stipulative" definition guides a specific discussion. A stipulative definition can only be disproved by showing a logical contradiction. A stipulative definition might be considered a temporary, working definition. On the other hand, a "descriptive" definition can be shown to be "right" or "wrong" with reference to general usage.
A "precising definition" extends the descriptive dictionary definition (lexical definition) of a term for a specific purpose by including additional criteria, which narrow the set of things that meet the definition.
C.L. Stevenson has identified "persuasive definition" as a form of stipulative definition which purports to state the "true" or "commonly accepted" meaning of a term, while in reality stipulating an altered use (perhaps as an argument for some specific belief). Stevenson has also noted that some definitions are "legal" or "coercive" – their object is to create or alter rights, duties, or crimes.
Intension and extension.
An "intensional definition", also called a "connotative" definition, specifies the necessary and sufficient conditions for a thing being a member of a specific set. Any definition that attempts to set out the essence of something, such as that by genus and differentia, is an intensional definition.
An "extensional definition", also called a "denotative" definition, of a concept or term specifies its "extension". It is a list naming every object that is a member of a specific set.
Thus, the "seven deadly sins" can be defined "intensionally" as those singled out by Pope Gregory I as particularly destructive of the life of grace and charity within a person, thus creating the threat of eternal damnation. An "extensional" definition would be a list of the seven. In contrast, while an intensional definition of "Prime Minister" might be "the most senior minister of a cabinet in the executive branch of government in a parliamentary system", an extensional definition is not possible since it is not known who future prime ministers will be.
One important form of the extensional definition is "ostensive definition". This gives the meaning of a term by pointing, in the case of an individual, to the thing itself, or in the case of a class, to examples of the right kind. So one can explain who "Alice" (an individual) is by pointing her out to another; or what a "rabbit" (a class) is by pointing at several and expecting another to understand. The process of ostensive definition itself was critically appraised by Ludwig Wittgenstein.
An "enumerative definition" of a concept or term is an "extensional definition" that gives an explicit and exhaustive listing of all the objects that fall under the concept or term in question. Enumerative definitions are only possible for finite sets and only practical for relatively small sets.
"Divisio" and "partitio".
"Divisio" and "partitio" are classical terms for definitions. A "partitio" is simply an intensional definition. A "divisio" is not an extensional definition, but an exhaustive list of subsets of a set, in the sense that every member of the "divided" set is a member of one of the subsets. An extreme form of "divisio" lists all sets whose only member is a member of the "divided" set. The difference between this and an extensional definition is that extensional definitions list "members", and not sets.
Definition by genus and differentia.
A genus–differentia definition is a type of intensional definition, and it is composed by two parts:
For example, consider these two definitions:
Those definitions can be expressed as a genus and two differentiae:
When multiple definitions could serve equally well, then all such definitions apply simultaneously. For instance, given the following:
both of these definitions of "square" are equally acceptable:
Thus, a "square" is a member of both the genus "rectangle" and the genus "rhombus". In such a case, it is notationally convenient to consolidate the definitions into one definition that is expressed with multiple genera (and possibly no differentia, as in the following):
or completely equivalently:
Rules for definition by genus and differentia.
Certain rules have traditionally been given for this particular type of definition.
Essence.
In classical thought, a definition was taken to be a statement of the essence of a thing. Aristotle had it that an object's essential attributes form its "essential nature", and that a definition of the object must include these essential attributes.
The idea that a definition should state the essence of a thing led to the distinction between "nominal" and "real" essence, originating with Aristotle. In a passage from the Posterior Analytics, he says that the meaning of a made-up name can be known (he gives the example "goat stag"), without knowing what he calls the "essential nature" of the thing that the name would denote, if there were such a thing. This led medieval logicians to distinguish between what they called the "quid nominis" or "whatness of the name", and the underlying nature common to all the things it names, which they called the "quid rei" or "whatness of the thing". (Early modern philosophers like Locke used the corresponding English terms "nominal essence" and "real essence"). The name "hobbit", for example, is perfectly meaningful. It has a "quid nominis". But one could not know the real nature of hobbits, even if there were such things, and so the real nature or "quid rei" of hobbits cannot be known. By contrast, the name "man" denotes real things (men) that have a certain "quid rei". The meaning of a name is distinct from the nature that thing must have in order that the name apply to it.
This leads to a corresponding distinction between "nominal" and "real" definitions. A nominal definition is the definition explaining what a word means, i.e. which says what the "nominal essence" is, and is definition in the classical sense as given above. A real definition, by contrast, is one expressing the real nature or "quid rei" of the thing.
This preoccupation with essence dissipated in much of modern philosophy. Analytic philosophy in particular is critical of attempts to elucidate the essence of a thing. Russell described it as "a hopelessly muddle-headed notion". 
More recently Kripke's formalisation of possible world semantics in modal logic led to a new approach to essentialism. Insofar as the essential properties of a thing are "necessary" to it, they are those things it possesses in all possible worlds. Kripke refers to names used in this way as rigid designators.
Recursive definitions.
A recursive definition, sometimes also called an "inductive" definition, is one that defines a word in terms of itself, so to speak, albeit in a useful way. Normally this consists of three steps:
For instance, we could define a natural number as follows (after Peano): 
So "0" will have exactly one successor, which for convenience can be called "1". In turn, "1" will have exactly one successor, which could be called "2", and so on. Notice that the second condition in the definition itself refers to natural numbers, and hence involves self-reference. Although this sort of definition involves a form of circularity, it is not vicious, and the definition has been quite successful.
Working definitions.
A working definition is chosen for an occasion and may not fully conform with established or authoritative definitions. Not knowing of established definitions would be grounds for selecting or devising a working definition. Or it refers to a definition being developed; a tentative definition that can be tailored to create an authoritative definition.
Limitations of definition.
Given that a natural language such as English contains, at any given time, a finite number of words, any comprehensive list of definitions must either be circular or rely upon primitive notions. If every term of every "definiens" must itself be defined, "where at last should we stop?" A dictionary, for instance, insofar as it is a comprehensive list of lexical definitions, must resort to circularity.
Many philosophers have chosen instead to leave some terms undefined. The scholastic philosophers claimed that the highest genera (the so-called ten "generalissima") cannot be defined, since a higher genus cannot be assigned under which they may fall. Thus being, unity and similar concepts cannot be defined. Locke supposes in "An Essay Concerning Human Understanding" that the names of simple concepts do not admit of any definition. More recently Bertrand Russell sought to develop a formal language based on logical atoms. Other philosophers, notably Wittgenstein, rejected the need for any undefined simples. Wittgenstein pointed out in his "Philosophical Investigations" that what counts as a "simple" in one circumstance might not do so in another. He rejected the very idea that every explanation of the meaning of a term needed itself to be explained: "As though an explanation hung in the air unless supported by another one", claiming instead that explanation of a term is only needed to avoid misunderstanding.
Locke and Mill also argued that individuals cannot be defined. Names are learned by connecting an idea with a sound, so that speaker and hearer have the same idea when the same word is used. This is not possible when no one else is acquainted with the particular thing that has "fallen under our notice". Russell offered his theory of descriptions in part as a way of defining a proper name, the definition being given by a definite description that "picks out" exactly one individual. Saul Kripke pointed to difficulties with this approach, especially in relation to modality, in his book "Naming and Necessity".
There is a presumption in the classic example of a definition that the "definiens" can be stated. Wittgenstein argued that for some terms this is not the case. The examples he used include "game", "number" and "family". In such cases, he argued, there is no fixed boundary that can be used to provide a definition. Rather, the items are grouped together because of a family resemblance. For terms such as these it is not possible and indeed not necessary to state a definition; rather, one simply comes to understand the "use" of the term.
In medicine.
In medical dictionaries, definitions should to the greatest extent possible be:

</doc>
<doc id="7965" url="http://en.wikipedia.org/wiki?curid=7965" title="Disruption">
Disruption

Disruption in the context of radical change due to the introduction of a new idea driving a different way of doing things is a revolutionary change as opposed to an evolutionary change. "For example:" Dick Fosbury was a high jump athlete, who went over the bar backwards as opposed to the forwards style practiced up to that time. As a result, he broke records and permanently changed the world-wide approach to high jump. Other examples are the Google search engine, Sony Walkman, the Apple iPod, Apple Mac, Apple iPhone, etc. Businesses thrive on Disruptive innovation, because it gives them a competitive advantage in their market, which in turn assures greater margins and the opportunity to increase profits. This speaks to the importance of entrepreneurship and innovation in business: “"When you realise you have to change its too late"”.
In the context of Business continuity management, Disruption is an event which causes an "unplanned, negative deviation from the expected delivery ... according to the organization’s objectives". The objectives of information security include avoidance of disruption in this context.

</doc>
<doc id="7966" url="http://en.wikipedia.org/wiki?curid=7966" title="Disco">
Disco

Disco is a genre of dance music containing elements of funk, pop, psychedelic and Latin music that was most popular in the 1970s, though it has since enjoyed brief resurgences, including in 2013. The term is derived from "discothèque" (French for "library of phonograph records", but subsequently used as proper name for nightclubs in Paris). Its initial audiences were club-goers from the African American, Latino, and psychedelic communities in New York City and Philadelphia during the late 1960s and early 1970s. Disco also was a reaction against both the domination of rock music and the stigmatization of dance music by the counterculture during this period. Women embraced disco as well, and the music eventually expanded to several other popular groups of the time.
In what is considered a forerunner to disco-style clubs, New York City DJ David Mancuso opened The Loft, a members-only private dance club set in his own home, in February 1970.
The first article about disco was written in September 1973 by Vince Aletti for "Rolling Stone" magazine. In 1974 New York City's WPIX-FM premiered the first disco radio show.
The disco sound has soaring, often reverberated vocals over a steady "four-on-the-floor" beat, an eighth note (quaver) or 16th note (semi-quaver) hi-hat pattern with an open hi-hat on the off-beat, and a prominent, syncopated electric bass line sometimes consisting of octaves. The Fender Jazz Bass is often associated with disco bass lines, because the instrument itself has a very prominent "voice" in the musical mix. In most disco tracks, strings, horns, electric pianos, and electric guitars create a lush background sound. Orchestral instruments such as the flute are often used for solo melodies, and lead guitar is less frequently used in disco than in rock. Many disco songs employ the use of electronic instruments such as synthesizers.
Well-known late 1970s disco performers included ABBA, Giorgio Moroder, Donna Summer, The Bee Gees, KC and the Sunshine Band, The Trammps, Gloria Gaynor and Chic. Various critics would also claim that Kraftwerk, who were an electronic band played a large part in pioneering disco as well as the electronic sound that became a big element of disco. While performers and singers garnered some public attention, producers working behind the scenes played an equal, if not more important role in disco, since they often wrote the songs and created the innovative sounds and production techniques that were part of the "disco sound."
Many non-disco artists recorded disco songs at the height of disco's popularity, and films such as "Saturday Night Fever" and "Thank God It's Friday" contributed to disco's rise in mainstream popularity. Disco was the last mass popular music movement that was driven by the baby boom generation. Disco music was a worldwide phenomenon, but its popularity declined in the United States in the late 1970s. On July 12, 1979, an anti-disco protest in Chicago called "Disco Demolition Night" had shown that an angry backlash against disco and its culture had emerged in the United States. In the subsequent months and years, many musical acts associated with disco struggled to get airplay on the radio. A few artists still managed to score disco hits in the early 1980s, but the term "disco" became unfashionable in the new decade and was eventually replaced by "dance music", "dance pop", and other identifiers. Although the production techniques have changed, many successful acts since the 1970s have retained the basic disco beat and mentality, and dance clubs have remained popular.
A disco revival was seen in 2013, as disco-styled songs by artists like Daft Punk (with Nile Rodgers), Justin Timberlake, Breakbot, and Bruno Mars filled the pop charts in the UK and the US.
Disco was more than just music; the disco scene also had a subculture centred around club-going, dancing, fashion, drug use, and sexual promiscuity. By the late 1970s most major U.S. cities had thriving disco club scenes centered on discotheques, nightclubs, and private loft parties where DJs would play disco hits through powerful PA systems. Studio 54 was arguably the most well known of these nightclubs. Popular dances included the "Robot" and The Hustle, a very sexually-suggestive dance. Discothèque-goers often wore expensive and extravagant fashions. There was also a thriving drug subculture in the disco scene, particularly for drugs that would enhance the experience of dancing to the loud music and the flashing lights, such as cocaine (nicknamed "blow"), amyl nitrite "poppers", and Quaaludes" The other cultural phenomenon of the disco era was promiscuity and public sex in the clubs.
History.
Origins of the term and type of nightclub.
By the early 1940s, the terms "DJ" and "Disc Jockey" were in use to describe radio presenters. Because of restrictions, jazz dance halls in Occupied France played records instead of using live music. Eventually more than one of these venues had the proper name "discothèque". By 1959, the term was used in Paris to describe any of these type of nightclubs. That year a young reporter Klaus Quirini spontaneously started to select and introduce records at the Scotch-Club in Aachen, West Germany. By the following year the term was being used in the United States to describe that type of club, and a type of dancing in those clubs. By 1964, "discotheque" and the shorthand "disco" were used to describe a type of sleeveless dress used when going out to nightclubs. In September 1964, "Playboy Magazine" used the word "disco" as a shorthand for a discothèque-styled nightclub.
Proto-disco and early history of disco music.
In New York City musicians and audiences from the female, homosexual, black, and Latino communities adopted several traits from the hippies and psychedelia. They included overwhelming sound, free-form dancing, weird lighting, colorful costumes, and hallucinogens. Psychedelic soul groups like the Chambers Brothers and especially Sly and The Family Stone influenced proto-disco acts such as Isaac Hayes, Willie Hutch and the Philadelphia Sound. In addition, the perceived positivity, lack of irony, and earnestness of the hippies informed proto-disco music like M.F.S.B.'s album "Love Is the Message". To the mainstream public M.F.S.B. stood for "Mother Father Sister Brother"; to the tough areas where they came from it was understood to stand for "Mother Fuckin' Son of a Bitch".
Philadelphia and New York soul were evolutions of the Motown sound, and were typified by the lavish percussion and lush strings that became a prominent part of mid-1970s disco songs. Early songs with disco elements include "You Keep Me Hangin' On" (The Supremes, 1966), "Only the Strong Survive" (Jerry Butler, 1968), "Message to Love" (Jimi Hendrix's Band of Gypsys, 1970), "Soul Makossa" (Manu Dibango, 1972), Superstition by Stevie Wonder (1972) Eddie Kendricks' Keep on Truckin' (1973) and "The Love I Lost" by Harold Melvin & The Blue Notes (1973). "Love Train" by The O'Jays (1972), with M.F.S.B. playing backup band hit Billboard Number 1 in March 1973, and has been called "disco".
The early disco was dominated with producers and labels such as SalSoul Records (Ken, Stanley, and Joseph Cayre), West End Records (Mel Cheren), Casablanca (Neil Bogart), and Prelude (Marvin Schlachter) to name a few. The genre was also shaped by Tom Moulton, who wanted to extend the enjoyment — thus creating the extended mix or "remix". DJs and remixers would often remix (re-edit) existing songs using reel-to-reel tape machines. Their remixed versions would add in percussion breaks, new sections, and new sounds. Other influential DJs and remixers who helped to establish what became known as the "disco sound" included David Mancuso, Nicky Siano, Shep Pettibone, Larry Levan, Walter Gibbons, and Chicago-based "Godfather of House" Frankie Knuckles.
Disco hit the television airwaves with Soul Train in 1971 hosted by Don Cornelius, then Marty Angelo's "Disco Step-by-Step Television Show" in 1975, Steve Marcus' "Disco Magic/Disco 77", Eddie Rivera's "Soap Factory", and Merv Griffin's "Dance Fever", hosted by Deney Terrio, who is credited with teaching actor John Travolta to dance for his upcoming role in the hit movie "Saturday Night Fever", as well as "DANCE" based out of Columbia, South Carolina.
Rise to the mainstream.
From 1974 through 1977, disco music continued to increase in popularity as many disco songs topped the charts. In 1974, "Love's Theme" by Barry White's Love Unlimited Orchestra became the second disco song to reach number one on the "Billboard" Hot 100, after "Love Train". MFSB also released "TSOP (The Sound of Philadelphia)", featuring vocals by The Three Degrees, and this was the third disco song to hit number one; "TSOP" was written as the theme song for "Soul Train".
The Hues Corporation's 1974 "Rock the Boat", a U.S. #1 single and million-seller, was one of the early disco songs to hit #1. The same year saw the release of "Kung Fu Fighting", performed by Carl Douglas and produced by Biddu, which reached #1 in both the U.K. and U.S., and became the best-selling single of the year and one of the best-selling singles of all time with eleven million records sold worldwide, helping to popularize disco music to a great extent. Other chart-topping disco hits that year included "Rock Your Baby" by George McCrae.
In the northwestern sections of the United Kingdom the Northern Soul explosion, which started in late 1960s and peaked in 1974, made the region receptive to Disco which the region's Disc Jockeys were bringing back from New York. George McCrae's "Rock Your Baby" became the United Kingdom's first number one disco single.
Also in 1974, Gloria Gaynor released the first side-long disco mix vinyl album, which included a remake of The Jackson 5's "Never Can Say Goodbye" and two other songs, "Honey Bee" and "Reach Out (I'll Be There)".
Formed by Harry Wayne Casey ("KC") and Richard Finch, Miami's KC and the Sunshine Band had a string of disco-definitive top-five hits between 1975 and 1977, including "Get Down Tonight", "That's the Way (I Like It)", "(Shake, Shake, Shake) Shake Your Booty", "I'm Your Boogie Man" and "Keep It Comin' Love".
Electric Light Orchestra's 1975 hit Evil Woman, although described as Orchestral Rock, featured a violin sound that became a staple of disco. In 1979, however, ELO did release two "true" disco songs: "Last Train To London" and "Shine A Little Love."
In 1975, American singer and songwriter Donna Summer recorded a song which she brought to her producer Giorgio Moroder entitled "Love to Love You Baby" which contained a series of simulated orgasms. The song was never intended for release but when Moroder played it in the clubs it caused a sensation. Moroder released it and it went to number 1. It has been described as the arrival of the expression of raw female sexual desire in pop music. A 17 minute 12 inch single was released. The 12" single became and remains a standard in discos today.
In 1978, a multi-million selling vinyl single disco version of "MacArthur Park" by Summer was number one on the "Billboard" Hot 100 chart for three weeks and was nominated for the Grammy Award for Best Female Pop Vocal Performance. Summer's recording, which was included as part of the "MacArthur Park Suite" on her double album Live and More, was eight minutes and forty seconds long on the album. The shorter seven-inch vinyl single version of the MacArthur Park was Summer's first single to reach number one on the Hot 100; it doesn't include the balladic second movement of the song, however. A 2013 remix of "Mac Arthur Park" by Summer hit #1 on the Billboard Dance Charts marking five consecutive decades with a #1 hit on the charts.
From 1978 to 1979, Summer continued to release hits such as "Last Dance", "Bad Girls", "Heaven Knows", "No More Tears (Enough Is Enough)", "Hot Stuff" and "On the Radio", all very successful disco songs.
The Bee Gees used Barry Gibb's falsetto to garner hits such as "You Should Be Dancing", "Stayin' Alive", "Night Fever", "More Than A Woman" and "Love You Inside Out". Andy Gibb, a younger brother to the Bee Gees, followed with similarly-styled solo hits such as "I Just Want to Be Your Everything", "(Love Is) Thicker Than Water" and "Shadow Dancing". In 1975, hits such as Van McCoy's "The Hustle" and "Could It Be Magic" brought disco further into the mainstream. Other notable early disco hits include The Jackson 5's "Dancing Machine" (1974), Barry White's "You're the First, the Last, My Everything" (1974), LaBelle's "Lady Marmalade" (1975) and Silver Convention's "Fly Robin Fly" (1975).
Pop pre-eminence.
In December 1977, the film "Saturday Night Fever" was released. The film was marketed specifically to broaden disco's popularity beyond its primarily black and Latin audiences. It was a huge success and its soundtrack became one of the best-selling albums of all time. The idea for the film was sparked by a 1976 New York Magazine article titled: "Tribal Rites Of The New Saturday Night" which chronicled the disco culture in mid-1970's New York City.
Chic was formed by Nile Rodgers — a self described "street hippie" from late 1960s New York — and Martin Dow, a DJ from Key West, Florida who pioneered the NYC sound across that state. "Le Freak" was a popular 1978 single that is regarded as an iconic song of the genre. Other hits by Chic include the often-sampled "Good Times" (1979) and "Everybody Dance". The group regarded themselves as the disco movement's rock band that made good on the hippie movements ideals of peace, love, and freedom. Every song they wrote was written with an eye toward giving it "deep hidden meaning" or D.H.M.
The Jacksons (previously The Jackson 5) did many disco songs from 1975 to 1980, including "Shake Your Body (Down to the Ground)" (1978), "Blame it on the Boogie" (1978), "Lovely One" (1980), and "Can You Feel It" (1980)—all sung by Michael Jackson, whose 1979 solo album, "Off the Wall", included several disco hits, including the album's title song, "Rock with You", "Workin' Day and Night", and his second chart-topping solo hit in the disco genre, "Don't Stop 'til You Get Enough".
Crossover appeal.
Disco's popularity led many non-disco artists to record disco songs at the height of its popularity. Many of their songs were not "pure" disco, but were instead rock or pop songs with (sometimes inescapable) disco influence or overtones. Notable examples include Mike Oldfield's "Guilty" (1979) Blondie's "Heart of Glass" (1978), Cher's "Hell on Wheels" and "Take Me Home" (both 1979), Barry Manilow's "Copacabana" (1978), David Bowie's "John I'm Only Dancing (Again)" (1975), Rod Stewart's "Da Ya Think I'm Sexy?" (1979), Electric Light Orchestra's "Shine a Little Love" and "Last Train to London" (both 1979), George Benson's "Give Me the Night" (1980), Elton John and Kiki Dee's "Don't Go Breaking My Heart" (1976), M's "Pop Muzik" (1979), and Diana Ross' "Upside Down" (1980).
Even hard-core mainstream rockers mixed elements of disco with their typical rock 'n roll style in songs. Progressive rock group Pink Floyd, when creating their rock opera "The Wall", used disco-style components in their song, "Another Brick in the Wall, Part 2" (1979)—which became the group's only #1 hit single (in both the US and UK). The Eagles gave nods to disco with "One of These Nights" (1975) and "Disco Strangler" (1979), Paul McCartney & Wings did "Goodnight Tonight" (1979), Queen did "Another One Bites the Dust" (1980), The Rolling Stones did "Miss You" (1978), Chicago did "Street Player" (1979), The Beach Boys did "Here Comes the Night" (1979), The Kinks did "(Wish I Could Fly Like) Superman" (1979), and the J. Geils Band did "Come Back" (1980). Even heavy metal music group Kiss jumped in with "I Was Made For Lovin' You" (1979).
The disco fad was also picked up even by "non-pop" artists, including the 1979 U.S. number one hit "No More Tears (Enough Is Enough)" by Easy listening singer Barbra Streisand in a duet with Donna Summer. Country music artist, Connie Smith covered Andy Gibb's "I Just Want to Be Your Everything" in 1977, Bill Anderson did "Double S" in 1978, and Ronnie Milsap covered Tommy Tucker's "High Heel Sneakers" in 1979.
Disco revisions of songs.
Pre-existing non-disco songs and standards would frequently be "disco-ized" in the 1970s. The rich orchestral accompaniment that became identified with the disco era conjured up the memories of the big band era—which brought out several artists that recorded and disco-ized some big band arrangements including Perry Como, who re-recorded his 1929 and 1939 hit, "Temptation", in 1975, as well as Ethel Merman, who released an album of disco songs entitled "The Ethel Merman Disco Album" in 1979.
Myron Floren, second-in-command on "The Lawrence Welk Show", released a recording of the Clarinet Polka entitled "Disco Accordion". Easy listening icon Percy Faith, in one of his last recordings, released an album entitled "Disco Party" (1975) and recorded a disco version of his famous "Theme from A Summer Place" in 1976. Classical music was even adapted for disco, notably Walter Murphy's "A Fifth of Beethoven" (1976, based on the first movement of Beethoven's 5th Symphony) and "Flight 76" (1976, based on Rimsky-Korsakov's "Flight of the Bumblebee"), and Louis Clark's "Hooked On Classics" series of albums and singles.
Notable disco hits based on movie and television themes included a medley from "Star Wars", "Star Wars Theme/Cantina Band" (1977) by Meco, and "Twilight Zone/Twilight Tone" (1979) by The Manhattan Transfer. Even the "I Love Lucy" theme wasn't spared from being disco-ized. Many original television theme songs of the era also showed a strong disco influence, such as "Keep Your Eye On The Sparrow" (theme from "Baretta", performed by Sammy Davis, Jr. and later a hit single for Rhythm Heritage), Theme from "S.W.A.T." (from "S.W.A.T", original and single versions by Rhythm Heritage), and Mike Post's theme from "Magnum, P.I.".
Parodies.
Several parodies of the disco style were created. Rick Dees, at the time a radio DJ in Memphis, Tennessee, recorded "Disco Duck" (1976) and "Dis-Gorilla" (1977); Frank Zappa parodied the lifestyles of disco dancers in "Disco Boy" on his 1976 "Zoot Allures" album, and in "Dancin' Fool" on his 1979 "Sheik Yerbouti" album; "Weird Al" Yankovic's eponymous 1983 debut album includes a disco song called "Gotta Boogie", an extended pun on the similarity of the disco subgenre name "boogie" to the American slang word "booger" and its British counterpart "bogey".
Backlash and decline.
By the late 1970s, a strong anti-disco sentiment developed among rock fans and musicians, particularly in the United States. The slogans "disco sucks" and "death to disco" became common. Rock artists such as Rod Stewart and David Bowie who added disco elements to their music were accused of being sell outs.
The punk subculture in the United States and United Kingdom was often hostile to disco. Jello Biafra of The Dead Kennedys, in the song "Saturday Night Holocaust", likened disco to the cabaret culture of Weimar-era Germany for its apathy towards government policies and its escapism. Mark Mothersbaugh of Devo said that disco was "like a beautiful woman with a great body and no brains", and a product of political apathy of that era. New Jersey rock critic Jim Testa wrote "Put a Bullet Through the Jukebox", a vitriolic screed attacking disco that was considered a punk call to arms.
Anti-disco sentiment was expressed in some television shows and films. A recurring theme on the show "WKRP in Cincinnati" was a hostile attitude towards disco music. In one scene of the comedy film "Airplane!", a city skyline features a radio tower with a neon-lighted station callsign. A disc jockey voiceover says: "WZAZ in Chicago, where disco lives forever!" Then a wayward airplane slices the radio tower with its wing, the voiceover goes silent, and the lighted callsign goes dark.
July 12, 1979 became known as "the day disco died" because of Disco Demolition Night, an anti-disco demonstration in a baseball double-header at Comiskey Park in Chicago. Rock station DJs Steve Dahl and Garry Meier, along with Michael Veeck, son of Chicago White Sox owner Bill Veeck, staged the promotional event for disgruntled rock fans between the games of a White Sox doubleheader. The event, which involved exploding disco records, ended with a riot, during which the raucous crowd tore out seats and pieces of turf, and caused other damage. The Chicago Police Department made numerous arrests, and the extensive damage to the field forced the White Sox to forfeit the second game to the Detroit Tigers, who had won the first game.
On July 21, 1979, the top six records on the U.S. music charts were disco songs.
By September 22 there were no disco songs in the US Top 10 chart.
Some in the media, in celebratory tones, declared disco "dead" and rock revived.
Impact on music industry.
The anti-disco backlash, combined with other societal and radio industry factors, changed the face of pop radio in the years following Disco Demolition Night. Starting in the 1980s, country music began a slow rise in American main pop charts. Emblematic of country music's rise to mainstream popularity was the commercially successful 1980 movie "Urban Cowboy". Somewhat ironically, the star of the film was John Travolta, who only three years before had starred in "Saturday Night Fever", a film that featured disco culture.
During this period of decline in disco's popularity, several record companies folded, were reorganized, or were sold. In 1979, MCA Records purchased ABC Records, absorbed some of its artists, and then shut the label down. RSO Records founder Robert Stigwood left the label in 1981 and TK Records closed in the same year. Salsoul Records continues to exist today, but primarily is used as a reissue brand. Casablanca Records had been releasing fewer records in the 1980s, and was shut down in 1986 by parent company PolyGram.
Many groups that were popular during the disco period subsequently struggled to maintain their success—even those that tried to adapt to evolving musical tastes. The Bee Gees, for instance, only had two top-40 hits in the United States after the 1970s ("One" in 1989 and "Alone" in 1997)—even though later songs they wrote and had "others" perform were successful. Of the handful of groups "not" taken down by disco's fall from favor, Kool and the Gang, The Jacksons—and Michael Jackson in particular—stand out: In spite of having helped define the disco sound early on, they continued to make popular and danceable, if more refined, songs for yet another generation of music fans in the 1980s and beyond.
Factors contributing to disco's decline.
Factors that have been cited as leading to the decline of disco in the United States include economic and political changes at the end of the 1970s as well as burnout from the hedonistic lifestyles led by participants. In the years since Disco Demolition Night, some social critics have described the backlash as implicitly macho and bigoted, and an attack on non-white and non-heterosexual cultures.
In January 1979, rock critic Robert Christgau argued that homophobia, and most likely racism, were reasons behind the backlash, a conclusion seconded by John Rockwell. Craig Werner wrote: "The Anti-disco movement represented an unholy alliance of funkateers and feminists, progressives and puritans, rockers and reactionaries. Nonetheless, the attacks on disco gave respectable voice to the ugliest kinds of unacknowledged racism, sexism and homophobia." Legs McNeil, founder of the fanzine "Punk", was quoted in an interview as saying, "the hippies always wanted to be black. We were going, 'fuck the blues, fuck the black experience'." He also said that disco was the result of an unholy union between homosexuals and blacks.
Steve Dahl, who had spearheaded Disco Demolition Night, denied any racist or homophobic undertones to the promotion, saying, "It's really easy to look at it historically, from this perspective, and attach all those things to it. But we weren't thinking like that." It has been noted that British punk rock critics of disco were very supportive of the pro-black/anti-racist reggae genre. Robert Christgau and Jim Testa have said that there were legitimate artistic reasons for being critical of disco.
In 1979 the music industry in the United States was undergoing its worst slump in decades, and disco, despite its mass popularity, was blamed. The producer-oriented sound was having difficulty mixing well with the industry's artist-oriented marketing system. Harold Childs, senior vice president at A&M Records, told the "Los Angeles Times" that "radio is really desperate for rock product" and "they're all looking for some white rock-n-roll". Gloria Gaynor argued that the music industry supported the destruction of disco because rock music producers were losing money and rock musicians were losing the spotlight.
Revivals.
In 2013, several 1970s-style disco and R&B songs charted, and the pop charts had more dance songs than at any other point since the late 1970s. The biggest disco hit of the year as of June was "Get Lucky" by Daft Punk, featuring Nile Rodgers on guitar. The song was initially thought likely to be a leading candidate to become the summer's biggest hit that year; however, the song ended up peaking at number 2 on the Billboard Hot 100 chart for five weeks behind another major disco-styled song, Robin Thicke's "Blurred Lines", which spent twelve weeks at number 1 on the Hot 100, and in the process became the eventual song of the summer itself. Both were popular with a wide variety of demographic groups. Other disco-styled songs that made it into the top 40 were Justin Timberlake's "Take Back The Night" (No. 29), and Bruno Mars' "Treasure" (No. 5). In addition, Arcade Fire's "Reflektor" and Daft Punk's "Random Access Memories", both featured strong disco elements, topped the "Billboard" 200 in 2013. In 2014, disco music could be found in Lady Gaga's "Artpop" and Katy Perry's "Birthday".
Euro disco.
As disco's popularity sharply declined in the United States, abandoned by major U.S. record labels and producers, European disco continued evolving within the broad mainstream pop music scene. European acts Silver Convention, Love and Kisses, Munich Machine, and American acts Donna Summer and the Village People, were acts that defined the late 1970s Euro disco sound. Producers Giorgio Moroder, whom AllMusic described as "one of the principal architects of the disco sound" with the Donna Summer hit "I Feel Love" (1977), and Jean-Marc Cerrone were involved with Euro disco. The German group Kraftwerk also had an influence on Euro disco.
By far the most successful Euro disco act was ABBA. This Swedish quartet—with such hits as "Waterloo" (1974), "Fernando" (1976), "Take a Chance on Me" (1978), "Gimme! Gimme! Gimme! (A Man After Midnight)" (1979), and their signature smash "Dancing Queen" (1976)—ranks as the eighth best-selling act of all time. Other prominent European pop and disco groups were Luv' from the Netherlands and Boney M., a group of four West Indian singers and dancers masterminded by West German record producer Frank Farian. Boney M. charted worldwide hits with such songs as "Daddy Cool", "Ma Baker" and "Rivers of Babylon".
Another euro-disco act was Amanda Lear, where euro-disco sound is most heard in Enigma ("Give a bit of Mmh to me") song (1978).
In France, Claude François who re-invented himself as the king of French disco, released "La plus belle chose du monde", a French version of the Bee Gees hit record, "Massachusetts", which became a big hit in Canada and Europe and "Alexandrie Alexandra" was posthumously released on the day of his burial and became a worldwide hit. Dalida released "J'attendrai", which became a big hit in Canada and Japan, and Cerrone's early hit songs, "Love in C Minor", "Give Me Love" and "Supernature" became major hits in the U.S. and Europe.
Role of Motown.
Diana Ross was one of the first Motown artists to embrace the disco sound with her hugely successful 1976 outing "Love Hangover" from her self-titled album. Ross would continue to score disco hits for the rest of the disco era, including the 1980 dance classics "Upside Down" and "I'm Coming Out" (the latter immediately becoming a favorite in the gay community). The Supremes, the group that made Ross famous, scored a handful of hits in the disco clubs without Ross, most notably 1976's "I'm Gonna Let My Heart Do the Walking" and, their last charted single before disbanding, 1977's "You're My Driving Wheel".
Also noteworthy are Cheryl Lynn's "Got to Be Real" (1978), Evelyn "Champagne" King's "Shame" (1978), Cher's "Take Me Home" (1979), Sister Sledge's "We Are Family" (1979), Geraldine Hunt's "Can't Fake the Feeling" (1980), and Walter Murphy's various attempts to bring classical music to the mainstream, most notably his hit "A Fifth of Beethoven" (1976).
Musical characteristics.
The music tended to layer soaring, often-reverberated vocals, which are often doubled by horns, over a background "pad" of electric pianos and wah-pedaled "chicken-scratch" guitars. Other backing keyboard instruments include the piano, organ (during early years), string synth, and electroacoustic keyboards such as the Fender Rhodes piano, Wurlitzer electric piano, and Hohner Clavinet. Synthesizers are also fairly common in disco, especially in the late 1970s.
The rhythm is laid down by prominent, syncopated basslines (with heavy use of octaves) played on the bass guitar and by drummers using a drum kit, African/Latin percussion, and electronic drums such as Simmons and Roland drum modules. The sound is enriched with solo lines and harmony parts played by a variety of orchestral instruments, such as harp, violin, viola, cello, trumpet, saxophone, trombone, clarinet, flugelhorn, French horn, tuba, English horn, oboe, flute (sometimes especially the alto flute and occasionally bass flute), piccolo, timpani and synth strings or a full-blown string orchestra.
Most disco songs have a steady four-on-the-floor beat, a quaver or semi-quaver hi-hat pattern with an open hi-hat on the off-beat, and a heavy, syncopated bass line. This basic beat would appear to be related to the Dominican merengue rhythm. Other Latin rhythms such as the rhumba, the samba and the cha-cha-cha are also found in disco recordings, and Latin polyrhythms, such as a rhumba beat layered over a merengue, are commonplace. The quaver pattern is often supported by other instruments such as the rhythm guitar and may be implied rather than explicitly present.
It often involves syncopation, rarely occurring on the beat unless a synthesizer is used to replace the bass guitar. In general, the difference between a disco, or any dance song, and a rock or popular song is that in dance music the bass hits "four to the floor", at least once a beat (which in 4/4 time is 4 beats per measure), whereas in rock the bass hits on one and three and lets the snare take the lead on two and four. Disco is further characterized by a 16th note division of the quarter notes established by the bass as shown in the second drum pattern below, after a typical rock drum pattern.
The orchestral sound usually known as "disco sound" relies heavily on strings and horns playing linear phrases, in unison with the soaring, often reverberated vocals or playing instrumental fills, while electric pianos and chicken-scratch guitars create the background "pad" sound defining the harmony progression. Typically, a rich "wall of sound" results. There are, however, more minimalistic flavors of disco with reduced, transparent instrumentation, pioneered by Chic (band).
In 1977, Giorgio Moroder again became responsible for a development in disco. Alongside Donna Summer and Pete Bellotte he wrote the song "I Feel Love" for Summer to perform. It became the first well-known disco hit to have a completely synthesised backing track. The song is still considered to have been well ahead of its time. Other disco producers, most famously Tom Moulton, grabbed ideas and techniques from dub music (which came with the increased Jamaican migration to New York City in the seventies) to provide alternatives to the four on the floor style that dominated. Larry Levan utilized style keys from dub and jazz and more as one of the most successful remixers of all time to create early versions of house music that sparked the genre.
Production.
The "disco sound" was much more costly to produce than many of the other popular music genres from the 1970s. Unlike the simpler, four-piece band sound of the funk, soul of the late 1960s, or the small jazz organ trios, disco music often included a large pop band, with several chordal instruments (guitar, keyboards, synthesizer), several drum or percussion instruments (drumkit, Latin percussion, electronic drums), a horn section, a string orchestra, and a variety of "classical" solo instruments (for example, flute, piccolo, and so on).
Disco songs were arranged and composed by experienced arrangers and orchestrators, and producers added their creative touches to the overall sound. Recording complex arrangements with such a large number of instruments and sections required a team that included a conductor, copyists, record producers, and mixing engineers. Mixing engineers had an important role in the disco production process, because disco songs used as many as 64 tracks of vocals and instruments. Mixing engineers compiled these tracks into a fluid composition of verses, bridges, and refrains, complete with orchestral builds and breaks. Mixing engineers helped to develop the "disco sound" by creating a distinctive-sounding disco mix.
Early records were the "standard" 3 minute version until Tom Moulton came up with a way to make songs longer, wanting to take a crowd to another level that was impossible with 45-RPM vinyl discs of the time (which could usually hold no more than 5 minutes of good-quality music). With the help of José Rodriguez, his remasterer, he pressed a single on a 10" disc instead of 7". They cut the next single on a 12" disc, the same format as a standard album. This method fast became the standard format for all DJs of the genre.
Because record sales were often dependent on floor play in clubs, DJs were also important to the development and popularization of disco music. Notable DJs include Rex Potts (Loft Lounge, Sarasota, Florida), Karen Cook, Jim Burgess, Walter Gibbons, John "Jellybean" Benitez, Richie Kaczar of Studio 54, Rick Gianatos, Francis Grasso of Sanctuary, Larry Levan, Ian Levine, Neil "Raz" Rasmussen & Mike Pace of L'amour Disco in Brooklyn, Preston Powell of Magique, Jennie Costa of Lemontrees, Tee Scott, Tony Smith of Xenon, John Luongo, Robert Ouimet of The Limelight, and David Mancuso.
Disco clubs and culture.
By the late 1970s most major U.S. cities had thriving disco club scenes, but the largest scenes were in San Francisco, Miami, and most notably New York City. The scene was centered on discotheques, nightclubs, and private loft parties where DJs would play disco hits through powerful PA systems for the patrons who came to dance. The DJs played "... a smooth mix of long single records to keep people 'dancing all night long'". Some of the most prestigious clubs had elaborate lighting systems that throbbed to the beat of the music.
In October 1975 notable discos included "Studio One" in Los Angeles, "Leviticus" in New York and "The Library" in Atlanta. The library Disco chain had locations in New City, Syracuse N.Y., Pittsburgh Pa., a short lived version in Denver, Co. as well as Atlanta Ga.
In the late 1970s, Studio 54 was arguably the most well known nightclub in the world. This club played a major formative role in the growth of disco music and nightclub culture in general.
Disco dancing.
In the early years dancers in discos danced in a "hang loose" style. Popular dances included "Bump", "Penguin", "Boogaloo", "Watergate" and the "Robot". By October 1975 The Hustle reigned. It was highly stylized, sophisticated and overtly sexual. Variations included the Brooklyn Hustle, New York Hustle and Latin Hustle.
During the disco era, many nightclubs would commonly host disco dance competitions or offer free instructional lessons. Some cities had disco dance instructors or dance schools, which taught people how to do popular disco dances such as ""touch dancing," ""the hustle," and "the cha cha." The pioneer of disco dance instruction was Karen Lustgarten in San Francisco in 1973. Her book The Complete Guide to Disco Dancing (Warner Books, 1978) was the first to name, break down and codify popular disco dances as a dance form and distinguish between disco freestyle, partner and line dances. The book hit the New York Times Best Seller List for 13 weeks and was translated into Chinese, German and French.
In Chicago, Step By Step launched with the sponsorship support of the Coca-Cola company. Produced in the same studio that Don Cornelius used for the nationally syndicated television show, Soul Train, Step by Step's audience grew and became an overnight success. The dynamic dance duo, Robin and Reggie spent the week teaching disco dancing in the disco clubs. The instructional show which aired on Saturday mornings had a following that would stay up all night on Fridays so they could be on the set the next morning, ready to return to the disco Saturday night equipped with the latest personalized dance steps. The producers of the show, John Reid and Greg Roselli routinely made appearances at disco functions with Robin and Reggie to scout out talent and promote upcoming events such as Disco Night at White Sox Park.
Some notable professional dance troupes of the 1970s included Pan's People and Hot Gossip. For many dancers, the primary influence of the 1970s disco age is still predominantly the film "Saturday Night Fever" (1977). This developed into the music and dance style of such films as "Fame" (1980), "Disco Dancer" (1982), "Flashdance" (1983), and "The Last Days of Disco" (1998). It also helped spawn dance competition TV shows such as Dance Fever (1979).
Disco fashion.
Disco fashions were very trendy in the late 1970s. Discothèque-goers often wore expensive and extravagant fashions for nights out at their local disco, such as sheer, flowing Halston dresses for women and shiny polyester Qiana shirts for men with pointy collars, preferably open at the chest, often worn with double-knit polyester shirt jackets with matching trousers known as the leisure suit. Necklaces and medallions were a common fashion accessory.
Drug subculture and sexual promiscuity.
In addition to the dance and fashion aspects of the disco club scene, there was also a thriving drug subculture, particularly for drugs that would enhance the experience of dancing to the loud music and the flashing lights, such as cocaine (nicknamed "blow"), amyl nitrite "poppers", and the "... other quintessential 1970s club drug Quaalude, which suspended motor coordination and gave the sensation that one's arms and legs had turned to Jell-O."
According to Peter Braunstein, the "massive quantities of drugs ingested in discotheques produced the next cultural phenomenon of the disco era: rampant promiscuity and public sex. While the dance floor was the central arena of seduction, actual sex usually took place in the nether regions of the disco: bathroom stalls, exit stairwells, and so on. In other cases the disco became a kind of 'main course' in a hedonist's menu for a night out."
Famous disco bars included the very important Paradise Garage and Crisco Disco as well as "... cocaine-filled celeb hangouts such as Manhattan's Studio 54," which was operated by Steve Rubell and Ian Schrager. Studio 54 was notorious for the hedonism that went on within; the balconies were known for sexual encounters, and drug use was rampant. Its dance floor was decorated with an image of the "Man in the Moon" that included an animated cocaine spoon.
Influence on other music.
1982–1990: Post-disco and dance.
The transition from the late-1970s disco styles to the early-1980s dance styles was marked primarily by the change from complex arrangements performed by large ensembles of studio session musicians (including a horn section and an orchestral string section), to a leaner sound, in which one or two singers would perform to the accompaniment of synthesizer keyboards and drum machines.
In addition, dance music during the 1981–83 period borrowed elements from blues and jazz, creating a style different from the disco of the 1970s. This emerging music was still known as disco for a short time, as the word had become associated with any kind of dance music played in discothèques. Examples of early 1980s dance sound performers include D. Train, Kashif, and Patrice Rushen. These changes were influenced by some of the notable R&B and jazz musicians of the 1970s, such as Stevie Wonder, Kashif and Herbie Hancock, who had pioneered "one-man-band"-type keyboard techniques. Some of these influences had already begun to emerge during the mid-1970s, at the height of disco's popularity.
During the first years of the 1980s, the disco sound began to be phased out, and faster tempos and synthesized effects, accompanied by guitar and simplified backgrounds, moved dance music toward the funk and pop genres. This trend can be seen in singer Billy Ocean's recordings between 1979 and 1981. Whereas Ocean's 1979 song "American Hearts" was backed with an orchestral arrangement played by the Los Angeles Symphony Orchestra, his 1981 song "One of Those Nights (Feel Like Gettin' Down)" had a more bare, stripped-down sound, with no orchestration or symphonic arrangements. This drift from the original disco sound is called post-disco. In this music scene there are rooted sub-genres, such as Italo disco, techno, house, dance-pop, boogie, and early alternative dance. During the early 1980s, dance music dropped the complicated melodic structure and orchestration that typified the disco sound.
TV themes.
During the 1970s, many TV theme songs were produced (or older themes updated) with disco influenced music. Examples include "S.W.A.T." (1975), "Wonder Woman" (1975), "Charlie's Angels" (1976), "NBC Saturday Night At The Movies" (1976), "The Love Boat" (1977), "The Donahue Show" (1977), "CHiPs" (1977), "The Professionals" (1977), "Three's Company" (1977), "Dallas" (1978), "Kojak" (1978), "The Hollywood Squares" (1979). The British Science Fiction program "" (1975) also featured a soundtrack strongly influenced by disco. This was especially evident in the show's second season.
DJ culture.
The rising popularity of disco came in tandem with developments in turntablism and the use of records to create a continuous mix of songs. The resulting DJ mix differed from previous forms of dance music, which were oriented towards live performances by musicians. This in turn affected the arrangement of dance music, with songs since the disco era typically containing beginnings and endings marked by a simple beat or riff that can be easily slipped into the mix.
Rave culture.
As the Disco era came to a close in the late 1970s, rave culture began to see significant growth. Rave culture incorporated disco culture's same love of dance music, drug exploration, sexual promiscuity, and hedonism. Although disco culture had thrived in the mainstream, the rave culture would make an effort to stay underground to avoid the animosity that was still surrounding disco and dance music.
Hip hop and electro.
The disco sound had a strong influence on early hip hop. Most of the early rap/hip-hop songs were created by isolating existing disco bass-guitar lines and dubbing over them with MC rhymes. The Sugarhill Gang used Chic's "Good Times" as the foundation for their 1979 hit "Rapper's Delight", generally considered to be the song that first popularized rap music in the United States and around the world. In 1982, Afrika Bambataa released the single "Planet Rock", which incorporated electronica elements from Kraftwerk's "Trans-Europe Express" and "Numbers" as well as YMO's "Riot in Lagos".
The Planet Rock sound also spawned a hip-hop electronic dance trend, electro music, which included songs such as Planet Patrol's "Play at Your Own Risk" (1982), C Bank's "One More Shot" (1982), Cerrone's "Club Underworld" (1984), Shannon's "Let the Music Play" (1983), Freeez's "I.O.U." (1983), Midnight Star's "Freak-a-Zoid" (1983), Chaka Khan's "I Feel For You" (1984).
Post-punk.
The post-punk movement that originated in the late 1970s both supported punk rock's rule breaking while rejecting its back to raw rock music element. Post-punk's mantra of constantly moving forward lent itself to both openness to and experimentation with elements of disco and other styles. Public Image Limited is considered the first post-punk group. The group's second album "Metal Box" fully embraced the studio as instrument methodology of disco. The group's founder John Lydon told the press that disco was the only music he cared for at the time. No Wave was a sub genre of post-punk centered in New York City.
For shock value, James Chance who was a notable member of the No Wave scene penned an article in the East Village Eye urging his readers to move uptown and get "trancin' with some superadioactive disco voodoo funk". His band James White and the Blacks wrote a disco album "Off White". Their performances resembled those of disco performers (horn section, dancers and so on). In 1981 ZE Records led the transition from No Wave into the more subtle mutant disco (post-disco/punk) genre. Mutant disco acts such as Kid Creole and the Coconuts, Was Not Was, ESG and Liquid Liquid influenced several British post-punk acts such as New Order, Orange Juice and A Certain Ratio.
Dance-punk.
In the early 2000s the dance-punk (new rave in the United Kingdom) emerged as a part of a broader post punk revival. It fused elements of punk related rock with different forms of dance music including disco. Klaxons, LCD Soundsystem, Death From Above 1979, The Rapture and Shitdisco were among acts associated with the genre.
Nu-disco.
Nu-disco is a 21st-century dance music genre associated with the renewed interest in 1970s and early 1980s disco, mid-1980s Italo disco, and the synthesizer-heavy Euro disco aesthetics. The moniker appeared in print as early as 2002, and by mid-2008 was used by record shops such as the online retailers Juno and Beatport. These vendors often associate it with re-edits of original-era disco music, as well as with music from European producers who make dance music inspired by original-era American disco, electro and other genres popular in the late 1970s and early 1980s. It is also used to describe the music on several American labels that were previously associated with the genres electroclash and french house.

</doc>
<doc id="7970" url="http://en.wikipedia.org/wiki?curid=7970" title="Darwin">
Darwin

Darwin may refer to:

</doc>
<doc id="7973" url="http://en.wikipedia.org/wiki?curid=7973" title="Donegal fiddle tradition">
Donegal fiddle tradition

The Donegal fiddle tradition is the way of playing the fiddle that is traditional in County Donegal, Ireland. It is one of the distinct fiddle traditions within Irish traditional music.
The distinctness of the Donegal tradition developed due to the close relations between Donegal and Scotland, and the Donegal repertoire and style has influences from Scottish fiddle music. For example in addition to the standard tune types such as Jigs and Reels, the Donegal tradition also has Highlands (influenced by the Scottish Strathspey). The distinctiveness of the Donegal tradition led to some conflict between Donegal players and representatives of the mainstream tradition when Irish traditional music was organised in the 1960s.
The tradition has several distinguishing traits compared to other fiddle traditions such as the Sliabh Luachra style of southern ireland, most of which involves styles of bowing and the ornamentation of the music, and rhythm. Due to the frequency of double stops and the strong bowing it is often compared to the Cape Breton. Tradition Another characteristic of the style is the rapid pace at which it tends to proceed. Modern players, such as the fiddle group Altan, continue to be popular due to a variety of reasons.
Among the most famous Donegal style players are John Doherty from the early twentieth century and James Byrne, Paddy Glackin, Tommy Peoples and Mairéad Ní Mhaonaigh in recent decades.
History.
The fiddle has ancient roots in Ireland, the first report of bowed instruments similar to the violin being in the Book of Leinster (ca. 1160). The modern violin was ubiquitous in Ireland by the early 1700s. However the first mention of the fiddle being in use in Donegal is from the blind harper Arthur O'Neill who in his 1760 memoirs described a wedding in Ardara as having "plenty of pipers and fiddlers". Donegal fiddlers participated in the development of the Irish music tradition in the 18th century during which jigs and slipjigs and later reels and hornpipes became the dominant musical forms. However, Donegal musicians, many of them being fishermen, also frequently travelled to Scotland, where they acquired tune types from the Scottish repertoire such as the Strathspey which was integrated into the Donegal tradition as "Highland" tunes. The Donegal tradition derives much of its unique character from the synthesis of Irish and Scottish stylistic features and repertoires. Aoidh notes however that while different types of art music were commonly played among the upper classes of Scottish society in the 18th century, the Donegal tradition drew exclusively from the popular types of Scottish music. Like some Scottish fiddlers (who, like Donegal fiddlers, tend to use a short bow and play in a straight-ahead fashion), some Donegal fiddlers worked at imitating the sound of the bagpipes. Workers from Donegal would bring their music to Scotland and also bring back Scottish tunes with them such music of J. Scott Skinner and Mackenzie Murdoch. Lilting, unaccompanied singing of wordless tunes, was also an important part of the Donegal musical tradition often performed by women in social settings. Describing the musical life of Arranmore Island in the late 19th century singer Róise Rua Nic Gríanna describes the most popular dances: "The Sets, the Lancers, the maggie Piggy, the Donkey, the Mazurka and the Barn dances". Among the travelling fiddlers of the late 19th century players such as John Mhosaí McGinley, Anthony Hlferty, the McConnells and the Dohertys are best known. As skill levels increased through apprenticeships several fiddle masters appeared such as the Cassidy's, Connie Haughey, Jimmy Lyons and Miock McShane of Teelin and Francie Dearg and Mickey Bán Byrne of Kilcar. These virtuosos played unaccompanied listening pieces in addition to the more common dance music.
The influences between Scotland and Donegal went both ways and were furthered by a wave of immigration from Donegal to Scotland in the 19th century (the regions share common names of dances), as can be heard in the volume of strathspeys, schottisches, marches, and Donegal's own strong piping tradition, has influenced and been influenced by music, and by the sounds, ornaments, and repertoire of the Píob Mhór, the traditional bagpipes of Ireland and Scotland. There are other differences between the Donegal style and the rest of Ireland. Instruments such as the tin whistle, flute, concertina and accordion were very rare in Donegal until modern times. Traditionally the píob mór and the fiddle were the only instruments used and the use of pipe or fiddle music was common in old wedding customs. Migrant workers carried their music to Scotland and also brought back a number of tunes of Scottish origin. The Donegal fiddlers may well have been the route by which Scottish tunes such as Lucy Campbell, Tarbolton Lodge (Tarbolton) and The Flagon (The Flogging Reel), that entered the Irish repertoire. These players prided themselves on their technical abilities, which included playing in higher positions (fairly uncommon among traditional Irish fiddlers), and sought out material which would demonstrate their skills.
As Irish music was consolidated and organised under the Comhaltas Ceoltóirí Éireann movement in the 1960s, both strengthened the interest in traditional music but sometimes conflicted with the Donegal tradition and its social conventions. The rigidly organised sessions of the Comhaltas reflected the traditions of Southern Ireland and Donegal fiddlers like John Doherty considered the National repertoire with its strong focus on reels to be less diverse than that of Donegal with its varied rhythms. Other old fiddlers dislike the ways comhaltas sessions were organised with a committee player, often not himself a musician, in charge. Sometimes Comhaltas representatives would even disparage the Donegal tradition, with its Scottish flavour, as being un-Irish, and prohibit them from playing local tunes with Scottish genealogies such as the "Highlands" at Comhaltas sessions. This sometimes cause antagonism between Donegal players and the main organisation of Traditinoal music in ireland.
Outside of the Comhaltas movement however, Donegal fiddling stood strong with Paddy Glackin of Ceoltorí Laighean and the Bothy Band and later Tommy Peoples also with the Bothy Band and Mairead Ni Mhaonaigh with Altan, who all drew attention and prestige to the Donegal tradition within folk music circles throughout Ireland.
Description of style.
The Donegal style of fiddling is a label often applied to music from this area, though one also might plausibly identify several different, but related, styles within the county. To the extent to which there is one common style in the county, it is characterised by a rapid pace; a tendency to be more un-swung in the playing of the fast dance tune types (reel and jigs); short (non-slurred), aggressive bowing, sparse ornamentation, the use of bowed triplets more often than trills as ornaments, the use of double stops and droning; and the occurrence of "playing the octave", with one player playing the melody and the other playing the melody an octave lower. None of these characteristics are universal, and there is some disagreement as to the extent to which there is a common style at all. In general, however, the style is rather aggressive.
Another feature of Donegal fiddling that makes it distinctive among Irish musical traditions is the variety of rare tune types that are played. Highlands, a type of tune in 4/4 time with some similarities to Scottish strathspeys, which are also played in Donegal, are one of the most commonly played types of tune in the county. Other tune types common solely in the county include barndances, also called "Germans," and mazurkas.
Fiddlers of the Donegal tradition.
Historical.
There are a number of different strands to the history of fiddle playing in County Donegal. Perhaps the best-known and, in the last half of the twentieth century, the most influential has been that of the Doherty family. Hugh Doherty is the first known musician of this family. Born in 1790, he headed an unbroken tradition of fiddlers and pipers in the Doherty family until the death, in 1980, of perhaps the best-known Donegal fiddler, John Doherty. John, a travelling tinsmith, was known for his extremely precise and fast finger- and bow-work and vast repertoire, and is considered to be one of the greatest Irish fiddlers ever recorded. John's older brother, Mickey, was also recorded and, though Mickey was another of the great Irish fiddlers, his reputation has been overshadowed by John's.
There is no single Donegal style but several distinctive styles. These styles traditionally come from the geographical isolated regions of Donegal including Inishowen, eastern Donegal, The Rosses and Gweedore, Croaghs, Teelin, Kilcar, Glencolmcille, Ballyshannon and Bundoran. Even with improved communications and transport, these regions still have recognisably different ways of fiddle playing. Notable deceased players of the older Donegal styles include Neillidh ("Neilly") Boyle, Francie Byrne, Con Cassidy,Frank Cassidy, James Byrne (1946–2008), and P.V. O'Donnell (2011). Currently living Donegal fiddlers, include, Vincent Campbell, John Gallagher, Paddy Glackin, Danny O'Donnell, and Tommy Peoples.
Modern.
Fiddle playing continues to be popular in Donegal. The three fiddlers of the Donegal "supergroup" Altan, Mairéad Ní Mhaonaigh, Paul O'Shaughnessy, and Ciarán Tourish, are generally admired within Donegal. An example of another fiddler-player from Donegal is Liz Doherty.
The fiddle, and traditional music in general, remained popular in Donegal not only because of the international coverage of certain artists but because of local pride in the music. Traditional music "Seisiúns" are still common place both in pubs and in houses. The Donegal fiddle music has been influenced by recorded music, but this is claimed to have had a positive impact on the tradition. Modern Donegal fiddle music is often played in concerts and recorded on albums.

</doc>
<doc id="7975" url="http://en.wikipedia.org/wiki?curid=7975" title="Double-barreled shotgun">
Double-barreled shotgun

A double-barreled shotgun ("double" in context) is a shotgun with two parallel barrels, allowing two shots to be fired in quick succession.
Construction.
Modern double-barreled shotguns, often known as "doubles", are almost universally break open actions, with the barrels tilting up at the rear to expose the breech ends of the barrels for unloading and reloading. Since there is no reciprocating action needed to eject and reload the shells, doubles are more compact than repeating designs such as pump action or lever-action shotguns.
Barrel configuration.
Double-barreled shotguns come in two basic configurations: the side by side shotgun (SxS) and the over/under shotgun ("over and under", O/U, etc.), indicating the arrangement of barrels. The original double-barreled guns were nearly all SxS designs, which was a more practical design in the days of muzzle-loading firearms. Early cartridge shotguns also used the SxS action, because they kept the exposed hammers of the earlier muzzle-loading shotguns they evolved from. When hammerless designs started to become common, the O/U design was introduced, and most modern sporting doubles are O/U designs.
One significant advantage that doubles have over single barrel repeating shotguns is the ability to provide access to more than one choke at a time. Some shotgun sports, such as skeet, use crossing targets presented in a narrow range of distance, and only require one level of choke. Others, like sporting clays, give the shooter targets at differing ranges, and targets that might approach or recede from the shooter, and so must be engaged at differing ranges. Having two barrels lets the shooter use a more open choke for near targets, and a tighter choke for distant targets, providing the optimal shot pattern for each distance.
Their disadvantage lies in the fact that the barrels of a double-barreled shotgun, whether "O/U" or "SxS", are not parallel, but, slightly angled so that shots from the barrels converge, usually at "40 yards out". For the "SxS" configuration, the shotstring continues on its path to the opposite side of the rib after the converging point; for example, the left barrel's discharge travels on the left of the rib till it hits dead center at 40 yards out, after that, the discharge continues on to the right. In the "O/U" configuration with a parallel rib, both barrels' discharges will keep to the dead center, but the discharge from the "under" barrel will shoot higher than the discharge from the "over" barrel after 40 yards. Thus, double-barreled shotguns are accurate only at practical shotgun ranges, though the range of their ammunition easily exceeds four to six times that range.
"SxS" shotguns are often more expensive, and may take more practice to aim effectively than a "O/U". The off-center nature of the recoil in a SxS gun may make shooting the body-side barrel slightly more painful by comparison to an OU, single-shot, or pump/lever action shotgun. Gas-operated, and to a lesser extent recoil-operated, designs will recoil less than either. More "SxS" than "O/U" guns have traditional 'cast-off' stocks, where the end of the buttstock veers to the right, allowing a right-handed user to point the gun more easily.
Trigger mechanism.
The early doubles used two triggers, one for each barrel. These were located front to back inside the trigger guard, the index finger being used to pull either trigger, as having two fingers inside the trigger guard can cause a recoil induced double-discharge. Double trigger designs are typically set up for right-handed users. In double trigger designs, it is often possible to pull both triggers at once, firing both barrels simultaneously, though this is generally not recommended as it doubles the recoil, battering both shooter and shotgun. Discharging both barrels at the same time has long been a hunting trick employed by hunters using 8 gauge "elephant" shotguns, firing the two two-ounce slugs for sheer stopping power at close range.
Later models use a single trigger that alternately fires both barrels, called a "single selective trigger" or "SST". The SST does not allow firing both barrels at once, since the single trigger must be pulled twice in order to fire both barrels. The change from one barrel to the other may be done by a clockwork type system, where a cam alternates between barrels, or by an inertial system where the recoil of firing the first barrel toggles the trigger to the next barrel. A double-barreled shotgun with an inertial trigger works best with full power shotshells; shooting low recoil shotshells often will not reliably toggle the inertial trigger, causing an apparent failure to fire occasionally when attempting to depress the trigger a second time to fire the second barrel. Generally there is a method of selecting the order in which the barrels of an SST shotgun fire; commonly this is done through manipulation of the safety, pushing to one side to select top barrel first and the other side to select bottom barrel first. In the event that an inertial trigger does not toggle to the second barrel when firing low recoil shotshells, manually selecting the order to the second barrel will enable the second barrel to fire when the trigger is depressed again.
One of the advantages of the double, with double triggers or SST, is that a second shot can be taken almost immediately after the first, utilizing different chokes for the two shots. (Assuming, of course, that full power shotshells are fired, at least for a double-barreled shotgun with an inertial type SST, as needed to toggle the inertial trigger.)
Regulation.
Regulation is a term used for multi-barreled firearms that indicates how close to the same point of aim the barrels will shoot. Regulation is very important, because a poorly regulated gun may hit consistently with one barrel, but miss consistently with the other, making the gun nearly useless for anything requiring two shots. Fortunately, the short ranges and spread of shot provide a significant overlap, so a small error in regulation in a double will often be too small to be noticed. Generally the shotguns are regulated to hit the point of aim at a given distance, usually the maximum expected range since that is the range at which a full choke would be used, and where precise regulation matters most.
Regional use.
The double-barreled shotgun is seen as a weapon of prestige and authority in rural parts of India, where it is known as "dunali" (literally "two pipes"). It is especially common in Tamil Nadu, Bihar, Purvanchal, Uttar Pradesh, Haryana and Punjab.

</doc>
<doc id="7976" url="http://en.wikipedia.org/wiki?curid=7976" title="Dessert">
Dessert

Dessert () is a typically sweet course that concludes an evening meal. The course usually consists of sweet foods, but may include other items.
There is a wide variety of desserts in the worlds cultures including cakes, tarts, cookies, biscuits, gelatins, pastries, ice creams, pies, puddings, custards, sweet soups and candies. Fruit is also commonly found in dessert courses because of its naturally occurring sweetness. Many different cultures have their own variations of similar desserts around the world, such as in Russia, where many breakfast foods such as blint, oladi, and syrniki can be served with honey and jam to make them popular as desserts. The loosely defined course called dessert can apply to many foods.
Etymology.
The word "dessert" originated from the French word "desservir" "to clean the table" and the negative of the Latin word "servire".
Usage.
The word dessert is most commonly used for this course in the United States, Canada, Australia, New Zealand the United Kingdom and Ireland, while "sweets" or "afters" or, informally, "pudding" are alternative terms that may also be used in the United Kingdom and some other Commonwealth countries, including Hong Kong, and India.
History.
Desserts were first made using natural ingredients that were locally available. In ancient civilizations people enjoyed dried fruits, honeycomb, or nuts. These were considered the first candies. When sugar began to be manufactured in the Middle Ages more sweet desserts became available. Even then sugar was so expensive usually only the wealthy could indulge on special occasions. Ice cream can be dated back to 3000BC and may be considered to be an early “dessert” in the modern sense of the word. The first apple pie recipe was printed in 1381. Also in 1740 the first cupcake recipes were recorded. Ice cream was a Chinese invention although Marco Polo expanded the technique to Europe in his travels. By the 1800s recipes for how to make ice cream were very popular. Vanilla also plays a large role many desserts including ice cream. Vanilla was mostly grown in Mexico where they discovered if the vanilla pod was picked and dried then vanillin was produced which can be sweetened into a dessert on its own.
Desserts are constantly changing with the new techniques and ingredients available at the time or in the local area. They have changed from natural candies and nuts to complex souffles and multi-layered cakes. The ingredients available affect the range of desserts that can be made in each region. The first desserts required minimal effort or preparation since ancient cultures were more focused on the nutrition in foods in order to survive. Now, however, modern day cultures have more options in the desserts available. Ice Cream went from being a shaved ice substance with flavoring to the dairy based, sweet treat modern children crave. Desserts also went through a major change in manufacturing. The Industrial Revolution in America and Europe changed desserts (and food in general) because they began to be mass-produced, processed, preserved, canned, and packaged. The iconic foods soon became a staple in many households because of their availability. Frozen foods became very popular starting in the 1920s when freezing emerged. Also around the 1920s lunch counters and fast food restaurants were established with increasing popularity. These processed foods became a huge part of diets in many industrialized nations. Food has always varied with each culture and area, and, despite the boom in mass-produced processed desserts, they have still represented regional and ethnic cultures. Many countries have desserts and foods distinctive to their nations or region. One example of this is the churro historically from Spain. These sticks of dough are now deep fried. They are available all throughout North and South America along with many other areas. They started with Spanish shepherds as a substitute for bread. Now they have been passed through generations and cultures and changed to a sweet confection different for each culture.
Ingredients.
Desserts usually contain sugar or a sweetening agent. Desserts contain a range of ingredients which makes the end product differ. Some of the more common ingredients in desserts are flour, dairy, eggs, and spices. Sugar gives many desserts their “addictive sweetness” and contributes to their moisture and tenderness. The flour or starch component in most desserts serves as a protein and gives the dessert structure. Different flours such as All-Purpose Flour or Pastry Flour provide a less rigid gluten network and therefore a different texture. Flour desserts may also contain dairy products. Different desserts use flour to various degrees. Desserts like ice cream and puddings have some form of dairy as their main ingredient, whereas desserts like cakes and cookies have relatively small amounts. The dairy products in baked goods keep the desserts moist. Many desserts also contain eggs, in order to form custard or to aid in the rising and thickening of a cake-like substance. Egg yolks specifically contribute to the richness of desserts. Egg whites can act as a leavening agent when the proteins uncoil and expand. Desserts can contain many different spices and extracts to add a variety of flavors. One example of this is salt. Salt is added to desserts to balance sweet flavors and create a contrast in flavors. All these ingredients contribute to desserts and make them different.
Varieties.
Dessert can come in variations of flavors, textures, and looks. Desserts can be defined as a usually sweeter course that concludes a meal. This definition includes a range of courses anywhere from fruits or dried nuts to multi-ingredient cakes and pies. With the many different varieties of desserts the many cultures have different variations. In modern times the variations of desserts have usually been passed down or come from geographical regions. This is one cause for the variation of desserts. These are some major categories in which desserts can be placed.
Market.
The market for desserts has grown over the last few decades, which was greatly increased by the commercialism of baking desserts and the rise of food productions. Desserts are present in most restaurants as the popularity has increased. Also many commercial stores have been established as solely desserts stores. Ice cream parlors have been around since before 1800. Many businesses started advertising campaigns focusing solely on desserts. The tactics used to market desserts are very different depending on the audience for example desserts can be advertised with popular movie characters to target children. The rise of companies like Food Network has marketed many shows which feature dessert and their creation. Shows like these have displayed extreme desserts and made a game show atmosphere which made desserts a more competitive field.
Desserts are a standard staple in restaurant menus, with different degrees of variety. Pie and cheesecake were among the most popular dessert courses ordered in U.S. restaurants in 2012.
Nutrition.
Desserts are by definition a sweet course. This usually means high content of sugar or fats. Desserts have historically been known as a smaller course to end a meal but in modern times they have become a more major part of people's diets. Although desserts are sweet a small amount of sugar is recommended in a daily diet. Certain desserts such as dark chocolate, that have a lower sugar content, are popularly considered healthier because of their other nutritional content. One example of a healthier dessert is fresh fruit cooked without sugars or extra fats.
Usage exceptions.
The word "dessert" is occasionally used euphemistically in some Wesleyan religious communities (esp. southern Indiana) to refer to an alcoholic beverage consumed during a social event with family or friends . The consumption of alcoholic beverages is generally discouraged in the Wesleyan tradition, as it is considered to be in conflict with the pursuit of Christian perfection articulated in the teachings of John Wesley. Thus the term "dessert" has emerged as a code word among more liberal Wesleyans not opposed to the moderate use of alcohol.

</doc>
<doc id="7978" url="http://en.wikipedia.org/wiki?curid=7978" title="Data Encryption Standard">
Data Encryption Standard

The Data Encryption Standard (DES, or ) was once a predominant symmetric-key algorithm for the encryption of electronic data. It was highly influential in the advancement of modern cryptography in the academic world. Developed in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS eventually selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977. The publication of an NSA-approved encryption standard simultaneously resulted in its quick international adoption and widespread academic scrutiny. Controversies arose out of classified design elements, a relatively short key length of the symmetric-key block cipher design, and the involvement of the NSA, nourishing suspicions about a backdoor. The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis.
DES is now considered to be unsecure for many applications. This is chiefly due to the 56-bit key size being too small; in January, 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see chronology). There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible to mount in practice. The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. In recent years, the cipher has been superseded by the Advanced Encryption Standard (AES). Furthermore, DES has been withdrawn as a standard by the National Institute of Standards and Technology (formerly the National Bureau of Standards).
Some documentation makes a distinction between DES as a standard and DES as an algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm).
History of DES.
The origins of DES go back to the early 1970s. In 1972, after concluding a study on the US government's computer security needs, the US standards body NBS (National Bureau of Standards) — now named NIST (National Institute of Standards and Technology) — identified a need for a government-wide standard for encrypting unclassified, sensitive information. Accordingly, on 15 May 1973, after consulting with the NSA, NBS solicited proposals for a cipher that would meet rigorous design criteria. None of the submissions, however, turned out to be suitable. A second request was issued on 27 August 1974. This time, IBM submitted a candidate which was deemed acceptable — a cipher developed during the period 1973–1974 based on an earlier algorithm, Horst Feistel's Lucifer cipher. The team at IBM involved in cipher design and analysis included Feistel, Walter Tuchman, Don Coppersmith, Alan Konheim, Carl Meyer, Mike Matyas, Roy Adler, Edna Grossman, Bill Notz, Lynn Smith, and Bryant Tuckerman.
NSA's involvement in the design.
On 17 March 1975, the proposed DES was published in the "Federal Register". Public comments were requested, and in the following year two open workshops were held to discuss the proposed standard. There was some criticism from various parties, including from public-key cryptography pioneers Martin Hellman and Whitfield Diffie, citing a shortened key length and the mysterious "S-boxes" as evidence of improper interference from the NSA. The suspicion was that the algorithm had been covertly weakened by the intelligence agency so that they — but no-one else — could easily read encrypted messages. Alan Konheim (one of the designers of DES) commented, "We sent the S-boxes off to Washington. They came back and were all different." The United States Senate Select Committee on Intelligence reviewed the NSA's actions to determine whether there had been any improper involvement. In the unclassified summary of their findings, published in 1978, the Committee wrote:
However, it also found that
Another member of the DES team, Walter Tuchman, stated "We developed the DES algorithm entirely within IBM using IBMers. The NSA did not dictate a single wire!"
In contrast, a declassified NSA book on cryptologic history states:
and
Some of the suspicions about hidden weaknesses in the S-boxes were allayed in 1990, with the independent discovery and open publication by Eli Biham and Adi Shamir of differential cryptanalysis, a general method for breaking block ciphers. The S-boxes of DES were much more resistant to the attack than if they had been chosen at random, strongly suggesting that IBM knew about the technique in the 1970s. This was indeed the case; in 1994, Don Coppersmith published some of the original design criteria for the S-boxes. According to Steven Levy, IBM Watson researchers discovered differential cryptanalytic attacks in 1974 and were asked by the NSA to keep the technique secret. Coppersmith explains IBM's secrecy decision by saying, "that was because [differential cryptanalysis] can be a very powerful tool, used against many schemes, and there was concern that such information in the public domain could adversely affect national security." Levy quotes Walter Tuchman: "[t]hey asked us to stamp all our documents confidential... We actually put a number on each one and locked them up in safes, because they were considered U.S. government classified. They said do it. So I did it". Bruce Schneier observed that "It took the academic community two decades to figure out that the NSA 'tweaks' actually improved the security of DES."
The algorithm as a standard.
Despite the criticisms, DES was approved as a federal standard in November 1976, and published on 15 January 1977 as FIPS PUB 46, authorized for use on all unclassified data. It was subsequently reaffirmed as the standard in 1983, 1988 (revised as FIPS-46-1), 1993 (FIPS-46-2), and again in 1999 (FIPS-46-3), the latter prescribing "Triple DES" (see below). On 26 May 2002, DES was finally superseded by the Advanced Encryption Standard (AES), following a public competition. On 19 May 2005, FIPS 46-3 was officially withdrawn, but NIST has approved Triple DES through the year 2030 for sensitive government information.
The algorithm is also specified in ANSI X3.92 (Now, X3 is now known as INCITS and ANSI X3.92 as ANSI INCITS 92), NIST SP 800-67 and ISO/IEC 18033-3 (as a component of TDEA).
Another theoretical attack, linear cryptanalysis, was published in 1994, but it was a brute force attack in 1998 that demonstrated that DES could be attacked very practically, and highlighted the need for a replacement algorithm. These and other methods of cryptanalysis are discussed in more detail later in this article.
The introduction of DES is considered to have been a catalyst for the academic study of cryptography, particularly of methods to crack block ciphers. According to a NIST retrospective about DES,
Description.
DES is the archetypal block cipher — an algorithm that takes a fixed-length string of plaintext bits and transforms it through a series of complicated operations into another ciphertext bitstring of the same length. In the case of DES, the block size is 64 bits. DES also uses a key to customize the transformation, so that decryption can supposedly only be performed by those who know the particular key used to encrypt. The key ostensibly consists of 64 bits; however, only 56 of these are actually used by the algorithm. Eight bits are used solely for checking parity, and are thereafter discarded. Hence the effective key length is 56 bits, and it is always quoted as such.
The key is nominally stored or transmitted as 8 bytes, each with odd parity. According to ANSI X3.92-1981 (Now, known as ANSI INCITS 92-1981), section 3.5:
Like other block ciphers, DES by itself is not a secure means of encryption but must instead be used in a mode of operation. FIPS-81 specifies several modes for use with DES. Further comments on the usage of DES are contained in FIPS-74.
Decryption uses the same structure as encryption but with the keys used in reverse order. (This has the advantage that the same hardware or software can be used in both directions.)
Overall structure.
The algorithm's overall structure is shown in Figure 1: there are 16 identical stages of processing, termed "rounds". There is also an initial and final permutation, termed "IP" and "FP", which are inverses (IP "undoes" the action of FP, and vice versa). IP and FP have no cryptographic significance, but were included in order to facilitate loading blocks in and out of mid-1970s 8-bit based hardware.
Before the main rounds, the block is divided into two 32-bit halves and processed alternately; this criss-crossing is known as the Feistel scheme. The Feistel structure ensures that decryption and encryption are very similar processes — the only difference is that the subkeys are applied in the reverse order when decrypting. The rest of the algorithm is identical. This greatly simplifies implementation, particularly in hardware, as there is no need for separate encryption and decryption algorithms.
The ⊕ symbol denotes the exclusive-OR (XOR) operation. The "F-function" scrambles half a block together with some of the key. The output from the F-function is then combined with the other half of the block, and the halves are swapped before the next round. After the final round, the halves are swapped; this is a feature of the Feistel structure which makes encryption and decryption similar processes.
The Feistel (F) function.
The F-function, depicted in Figure 2, operates on half a block (32 bits) at a time and consists of four stages:
The alternation of substitution from the S-boxes, and permutation of bits from the P-box and E-expansion provides so-called "confusion and diffusion" respectively, a concept identified by Claude Shannon in the 1940s as a necessary condition for a secure yet practical cipher.
Key schedule.
Figure 3 illustrates the "key schedule" for encryption  — the algorithm which generates the subkeys. Initially, 56 bits of the key are selected from the initial 64 by "Permuted Choice 1" ("PC-1") — the remaining eight bits are either discarded or used as parity check bits. The 56 bits are then divided into two 28-bit halves; each half is thereafter treated separately. In successive rounds, both halves are rotated left by one or two bits (specified for each round), and then 48 subkey bits are selected by "Permuted Choice 2" ("PC-2") — 24 bits from the left half, and 24 from the right. The rotations (denoted by "«<" in the diagram) mean that a different set of bits is used in each subkey; each bit is used in approximately 14 out of the 16 subkeys.
The key schedule for decryption is similar — the subkeys are in reverse order compared to encryption. Apart from that change, the process is the same as for encryption. The same 28 bits are passed to all rotation boxes.
Security and cryptanalysis.
Although more information has been published on the cryptanalysis of DES than any other block cipher, the most practical attack to date is still a brute force approach. Various minor cryptanalytic properties are known, and three theoretical attacks are possible which, while having a theoretical complexity less than a brute force attack, require an unrealistic number of known or chosen plaintexts to carry out, and are not a concern in practice.
Brute force attack.
For any cipher, the most basic method of attack is brute force — trying every possible key in turn. The length of the key determines the number of possible keys, and hence the feasibility of this approach. For DES, questions were raised about the adequacy of its key size early on, even before it was adopted as a standard, and it was the small key size, rather than theoretical cryptanalysis, which dictated a need for a replacement algorithm. As a result of discussions involving external consultants including the NSA, the key size was reduced from 128 bits to 56 bits to fit on a single chip.
In academia, various proposals for a DES-cracking machine were advanced. In 1977, Diffie and Hellman proposed a machine costing an estimated US$20 million which could find a DES key in a single day. By 1993, Wiener had proposed a key-search machine costing US$1 million which would find a key within 7 hours. However, none of these early proposals were ever implemented—or, at least, no implementations were publicly acknowledged. The vulnerability of DES was practically demonstrated in the late 1990s. In 1997, RSA Security sponsored a series of contests, offering a $10,000 prize to the first team that broke a message encrypted with DES for the contest. That contest was won by the DESCHALL Project, led by Rocke Verser, Matt Curtin, and Justin Dolske, using idle cycles of thousands of computers across the Internet. The feasibility of cracking DES quickly was demonstrated in 1998 when a custom DES-cracker was built by the Electronic Frontier Foundation (EFF), a cyberspace civil rights group, at the cost of approximately US$250,000 (see EFF DES cracker). Their motivation was to show that DES was breakable in practice as well as in theory: "There are many people who will not believe a truth until they can see it with their own eyes. Showing them a physical machine that can crack DES in a few days is the only way to convince some people that they really cannot trust their security to DES." The machine brute-forced a key in a little more than 2 days search.
The next confirmed DES cracker was the COPACOBANA machine built in 2006 by teams of the Universities of Bochum and Kiel, both in Germany. Unlike the EFF machine, COPACOBANA consists of commercially available, reconfigurable integrated circuits. 120 of these field-programmable gate arrays (FPGAs) of type XILINX Spartan-3 1000 run in parallel. They are grouped in 20 DIMM modules, each containing 6 FPGAs. The use of reconfigurable hardware makes the machine applicable to other code breaking tasks as well. One of the more interesting aspects of COPACOBANA is its cost factor. One machine can be built for approximately $10,000. The cost decrease by roughly a factor of 25 over the EFF machine is an example of the continuous improvement of digital hardware—see Moore's law. Adjusting for inflation over 8 years yields an even higher improvement of about 30x. Since 2007, SciEngines GmbH, a spin-off company of the two project partners of COPACOBANA has enhanced and developed successors of COPACOBANA. In 2008 their COPACOBANA RIVYERA reduced the time to break DES to less than one day, using 128 Spartan-3 5000's. Currently SciEngines RIVYERA holds the record in brute-force breaking DES, having utilized 128 Spartan-3 5000 FPGAs. Their 256 Spartan-6 LX150 model has even lowered this time.
Attacks faster than brute-force.
There are three attacks known that can break the full 16 rounds of DES with less complexity than a brute-force search: differential cryptanalysis (DC), linear cryptanalysis (LC), and Davies' attack. However, the attacks are theoretical and are unfeasible to mount in practice; these types of attack are sometimes termed certificational weaknesses.
There have also been attacks proposed against reduced-round versions of the cipher, that is, versions of DES with fewer than 16 rounds. Such analysis gives an insight into how many rounds are needed for safety, and how much of a "security margin" the full version retains. Differential-linear cryptanalysis was proposed by Langford and Hellman in 1994, and combines differential and linear cryptanalysis into a single attack. An enhanced version of the attack can break 9-round DES with 215.8 chosen plaintexts and has a 229.2 time complexity (Biham and others, 2002).
Minor cryptanalytic properties.
DES exhibits the complementation property, namely that
where formula_2 is the bitwise complement of formula_3 formula_4 denotes encryption with key formula_5 formula_6 and formula_7 denote plaintext and ciphertext blocks respectively. The complementation property means that the work for a brute force attack could be reduced by a factor of 2 (or a single bit) under a chosen-plaintext assumption. By definition, this property also applies also to TDES cipher.
DES also has four so-called "weak keys". Encryption ("E") and decryption ("D") under a weak key have the same effect (see involution):
There are also six pairs of "semi-weak keys". Encryption with one of the pair of semiweak keys, formula_10, operates identically to decryption with the other, formula_11:
It is easy enough to avoid the weak and semiweak keys in an implementation, either by testing for them explicitly, or simply by choosing keys randomly; the odds of picking a weak or semiweak key by chance are negligible. The keys are not really any weaker than any other keys anyway, as they do not give an attack any advantage.
DES has also been proved not to be a group, or more precisely, the set formula_14 (for all possible keys formula_15) under functional composition is not a group, nor "close" to being a group. This was an open question for some time, and if it had been the case, it would have been possible to break DES, and multiple encryption modes such as Triple DES would not increase the security.
It is known that the maximum cryptographic security of DES is limited to about 64 bits, even when independently choosing all round subkeys instead of deriving them from a key, which would otherwise permit a security of 768 bits.
Replacement algorithms.
Concerns about security and the relatively slow operation of DES in software motivated researchers to propose a variety of alternative block cipher designs, which started to appear in the late 1980s and early 1990s: examples include RC5, Blowfish, IDEA, NewDES, SAFER, CAST5 and FEAL. Most of these designs kept the 64-bit block size of DES, and could act as a "drop-in" replacement, although they typically used a 64-bit or 128-bit key. In the Soviet Union the GOST 28147-89 algorithm was introduced, with a 64-bit block size and a 256-bit key, which was also used in Russia later.
DES itself can be adapted and reused in a more secure scheme. Many former DES users now use Triple DES (TDES) which was described and analysed by one of DES's patentees (see FIPS Pub 46-3); it involves applying DES three times with two (2TDES) or three (3TDES) different keys. TDES is regarded as adequately secure, although it is quite slow. A less computationally expensive alternative is DES-X, which increases the key size by XORing extra key material before and after DES. GDES was a DES variant proposed as a way to speed up encryption, but it was shown to be susceptible to differential cryptanalysis.
On January 2, 1997, NIST announced that they wished to choose a successor to DES. In 2001, after an international competition, NIST selected a new cipher, the Advanced Encryption Standard (AES), as a replacement. The algorithm which was selected as the AES was submitted by its designers under the name Rijndael. Other finalists in the NIST AES competition included RC6, Serpent, MARS, and Twofish.

</doc>
<doc id="7983" url="http://en.wikipedia.org/wiki?curid=7983" title="Double-hulled tanker">
Double-hulled tanker

A double-hulled tanker refers to an oil tanker which has a double hull. They reduce the likelihood of leaks occurring than in single-hulled tankers, and their ability to prevent or reduce oil spills led to double hulls being standardized for oil tankers and other types of ships including by the International Convention for the Prevention of Pollution from Ships or MARPOL Convention. After the Exxon Valdez oil spill disaster in Alaska in 1989, the US Government required all new oil tankers built for use between US ports to be equipped with a full double hull. 
Reasons for use.
A number of manufacturers have embraced oil tankers with a double hull because it strengthens the hull of ships, reducing the likelihood of oil disasters in low-impact collisions and groundings over single-hull ships. They reduce the likelihood of leaks occurring at low speed impacts in port areas when the ship is under pilotage. Research of impact damage of ships has revealed that double-hulled tankers are unlikely to perforate both hulls in a collision, preventing oil from seeping out. However, for smaller tankers, U shaped tanks might be susceptible to "free flooding" across the double bottom and up to the outside water level each side of the cargo tank. Salvors prefer to salvage doubled-hulled tankers because they permit the use of air pressure to vacuum out the flood water. In the 1960s, collision proof double hulls for nuclear ships were extensively investigated, due to escalating concerns over nuclear accidents.
The ability of double-hulled tankers to prevent or reduce oil spills led to double hulls being standardized for other types of ships including oil tankers by the International Convention for the Prevention of Pollution from Ships or MARPOL Convention. In 1992, MARPOL was amended, making it "mandatory for tankers of 5,000 dwt and more ordered after 6 July 1993 to be fitted with double hulls, or an alternative design approved by IMO". However, in the aftermath of the Erika incident of the coast off France in December 1999, members of IMO adopted a revised schedule for the phase-out of single-hull tankers, which came into effect on 1 September 2003, with further amendments validated on 5 April 2005.
After the Exxon Valdez oil spill disaster, when that ship grounded on Bligh Reef outside the port of Valdez, Alaska in 1989, the US Government required all new oil tankers built for use between US ports to be equipped with a full double hull. However, the damage to the Exxon Valdez penetrated sections of the hull (the slops oil tanks, or slop tanks) that were protected by a double bottom, or partial double hull.
Maintenance issues.
Although double-hulled tankers reduce the likelihood of ships grazing rocks and creating holes in the hull, a double hull does not protect against major, high-energy collisions or groundings which cause the majority of oil pollution, despite this being the reason that the double hull was mandated by United States legislation. Double-hulled tankers, if poorly designed, constructed, maintained and operated can be as problematic, if not more problematic than their single-hulled counterparts. Double-hulled tankers have a more complex design and structure than their single-hulled counterparts, which means that they require more maintenance and care in operating, which if not subject to responsible monitoring and policing, may cause problems. Double hulls often result in the weight of the hull increasing by at least 20%, and because the steel weight of doubled-hulled tanks should not be greater than that of single-hulled ships, the individual hull walls are typically thinner and theoretically less resistant to wear. Double hulls by no means eliminate the possibility of the hulls breaking apart. Due to the air space between the hulls, there is also a potential problem with volatile gases seeping out through worn areas of the internal hull, increasing the risk of an explosion.
Although several international conventions against pollution are in place, as of 2003 there was still no formal body setting international mandatory standards, although the International Safety Guide for Oil Tankers and Terminals (ISGOTT) does provide guidelines giving advise on optimum use and safety, such as recommending that ballast tanks are not entered while loaded with cargo, and that weekly samples are made of the atmosphere inside for hydrocarbon gas. Due to the difficulties of maintenance, ship builders have been competitive in producing double-hulled ships which are easier to inspect, such as ballast and cargo tanks which are easily accessible and easier to spot corrosion in the hull. The Tanker Structure Cooperative Forum (TSCF) published the "Guide to Inspection and Maintenance of Double-Hull Tanker Structures" in 1995 giving advice based on experience of operating double-hulled tankers.

</doc>
<doc id="7984" url="http://en.wikipedia.org/wiki?curid=7984" title="Drink">
Drink

Drinks, or beverages, are liquids specifically prepared for human consumption. In addition to basic needs, beverages form part of the culture of human society. Although most beverages, including juice, soft drinks, and carbonated drinks, have some form of water in them, water itself is often not classified as a beverage, and the word "beverage" has been recurrently defined as not referring to water.
An alcoholic beverage is a drink containing ethanol, commonly known as alcohol, although in chemistry the definition of an alcohol includes many other compounds. Alcoholic beverages, such as wine, beer, and liquor, have been part of human culture and development for 8,000 years.
Non-alcoholic beverages often signify drinks that would normally contain alcohol, such as beer and wine but are made with less than .5 percent alcohol by volume. The category includes drinks that have undergone an alcohol removal process such as non-alcoholic beers and de-alcoholized wines.
Biology.
When the human body becomes dehydrated it experiences the sensation of "thirst". This craving of fluids results an instinctive need to drink. Thirst is regulated by the hypothalamus in response to subtle changes in the body's electrolyte levels, and also as a result of changes in the volume of blood circulating. The complete elimination of beverages, i.e. water, from the body will result in death faster than the removal of any other substance. Water and milk have been basic drinks throughout history. As water is essential for life, it has also been the carrier of many diseases.
As mankind evolved, new techniques were discovered to create drinks from the plants that were native to their areas. The earliest archaeological evidence of wine production yet found has been at sites in Georgia ( BC) and Iran ( BC). Beer may have been known in Neolithic Europe as far back as 3000 BC, and was mainly brewed on a domestic scale. The invention of beer (and bread) has been argued to be responsible for humanity's ability to develop technology and build civilization. Tea likely originated in Yunnan, China during the Shang Dynasty (1500 BC–1046 BC) as a medicinal drink.
History.
Drinking has been a large part of socialising throughout the centuries. In Ancient Greece, a social gathering for the purpose of drinking was known as a symposium, where watered down wine would be drunk. The purpose of these gatherings could be anything from serious discussions to direct indulgence. In Ancient Rome, a similar concept of a "convivium" took place regularly.
Many early societies considered alcohol a gift from the gods, leading to the creation of gods such as Dionysus. Other religions forbid, discourage, or restrict the drinking of alcoholic beverages for various reasons. In some regions with a dominant religion the production, sale, and consumption of alcoholic beverages is forbidden to everybody, regardless of religion.
Toasting is a method of honouring a person or wishing good will by taking a drink. Another tradition is that of the loving cup, at weddings or other celebrations such as sports victories a group will share a drink in a large receptacle, shared by everyone until empty.
In East Africa and Yemen, coffee was used in native religious ceremonies. As these ceremonies conflicted with the beliefs of the Christian church, the Ethiopian Church banned the secular consumption of coffee until the reign of Emperor Menelik II. The beverage was also banned in Ottoman Turkey during the 17th century for political reasons and was associated with rebellious political activities in Europe.
Production.
A beverage or drink is a form of liquid which has been prepared for human consumption. This can include a number of different steps, some prior to transport, others immediately prior to consumption.
Purification of water.
Water is the chief constituent in all drinks, and the primary ingredient in most. Water is purified prior to drinking. Methods for purification include filtration and the addition of chemicals, such as chlorination. The importance of purified water is highlighted by the World Health Organisation, who point out 94% of deaths from diarrhea - the third biggest cause of infectious death worldwide at 1.8 million annually - could be prevented by improving the quality of the victim's environment, particularly safe water.
Pasteurisation.
Pasteurisation is the process of heating a liquid to for a period of time at a specified temperature, then immediately cooling. The process reduces the growth of micro-organisms within the liquid, thereby increasing the time before spoilage. It is primarily used on milk, which prior to pasteurisation is commonly infected with pathogenic bacteria and therefore the more likely than any other part of the common diet in the developed world to cause illness.
Juicing.
The process of extracting juice from fruits and vegetables can take a number of forms. Simple crushing of most fruits will provide a significant amount of liquid, though a more intense pressure can be applied to get the maximum amount of juice from the fruit. Both crushing and pressing are processes used in the production of wine.
Infusion.
Infusion is the process of extracting flavours from plant material by allowing the material to remain suspended within water. This process is used in the production of teas, herbal teas and can be used to prepare coffee (when using a coffee press).
Percolation.
The name is derived from the word "percolate" which means "to cause (a solvent) to pass through a permeable substance especially for extracting a soluble constituent".
In the case of coffee-brewing the solvent is water, the permeable substance is the coffee grounds, and the soluble constituents are the chemical compounds that give coffee its color, taste, aroma, and stimulating properties.
Carbonation.
Carbonation is the process of dissolving Carbon Dioxide into a liquid, such as water.
Fermentation.
Fermentation is a metabolic process that converts sugar to alcohol. Fermentation has been used by humans for the production of beverages since the Neolithic age. In winemaking, grape juice is combined with yeast in an anaerobic environment to allow the fermentation. The amount of sugar in the wine and the length of time given for fermentation determine the alcohol level and the sweetness of the wine. 
When brewing beer, there are four primary ingredients - water, grain, yeast and hops. The grain is encouraged to germinate by soaking and drying in heat, a process known as malting. It is then milled before soaking again to create the sugars needed for fermentation. This process is known as mashing.Hops are added for flavouring, then the yeast is added to the mixture (now called wort) to start the fermentation process.
Distillation.
Distillation is a method of separating mixtures based on differences in volatility of components in a boiling liquid mixture. It is one of the methods used in the purification of water. It is also a method of producing spirits from milder alcoholic beverages.
Mixing.
An alcoholic mixed drink that contains two or more ingredients is referred to as a cocktail. Cocktails were originally a mixture of spirits, sugar, water, and bitters. The term is now often used for almost any mixed drink that contains alcohol, including mixers, mixed shots, etc. A cocktail today usually contains one or more kinds of spirit and one or more mixers, such as soda or fruit juice. Additional ingredients may be sugar, honey, milk, cream, and various herbs.
Types of drink.
Non-alcoholic drinks.
A non-alcoholic drink is one that contains little or no alcohol. This category includes low-alcohol beer, non-alcoholic wine, and apple cider if they contain less than 0.5% alcohol by volume. The term "soft drink" specifies the absence of alcohol in contrast to "hard drink" and "drink". The term "drink" is theoretically neutral, but often is used in a way that suggests alcoholic content. Beverages such as soda pop, sparkling water, iced tea, lemonade, root beer, fruit punch, milk, hot chocolate, tea, coffee, milkshakes, and tap water and energy drinks are all soft drinks.
Water.
Water is the world’s most consumed drink however, 97% of water on Earth is non-drinkable salt water. Fresh water is found in rivers, lakes, wetlands, groundwater, frozen glaciers. Less than 1% of the Earth’s fresh water supplies are accessible through surface water and underground sources - those which are cost-effect to retrieve.
Milk.
Regarded as one of the "original" drinks, milk is the primary source of nutrition for babies. In many cultures of the world, especially the Western world, humans continue to consume dairy milk beyond infancy, using the milk of other animals (especially cattle, goats and sheep) as a beverage. Plant milk, a general term for any milk-like product that is derived from a plant source, also has a long history of consumption in various countries and cultures. The most popular varieties internationally are soy milk, almond milk, rice milk and coconut milk.
Tea.
Tea, the second most consumed drink in the world, is produced from infusing dried leaves of the "camellia sinensis" shrub, in boiling water. There are many ways in which tea is prepared for consumption: lemon or milk and sugar are among the most common additives worldwide. Other additions include butter and salt in Bhutan, Nepal, and Tibet; chewy tapioca balls in Taiwan; fresh ginger in Indonesia, Malaysia and Singapore; mint in North Africa and Senegal; cardamom in Central Asia; rum to make Jagertee in Central Europe; and coffee to make yuanyang in Hong Kong. Tea is also served differently from country to country: in China and Japan tiny cups are used to serve tea; in Thailand and the United States tea is often served cold (as "iced tea") and/or with a lot of sweetener; Indians boil tea with milk and a blend of spices as masala chai; tea is brewed with a samovar in Iran, Kashmir, Russia and Turkey; and in the Australian Outback it is traditionally brewed in a billycan.
Tea leaves can be processed in different ways resulting in a drink which appears and tastes different. Chinese yellow and green tea are steamed, roasted and dried; Oolong tea is semi-fermented and appears green-black and black teas are fully fermented
Around the world, people refer to other herbal infusions as "teas"; it is also argued that these were popular long before the "Camellia sinensis" shrub was used for tea making. Leaves, flowers, roots or bark can be used to make a herbal infusion and can be bought fresh, dried or powered.
Coffee.
Coffee is a brewed beverage prepared from the roasted seeds of several species of an evergreen shrub of the genus "Coffea". The two most common sources of coffee beans are the highly regarded "Coffea arabica", and the "robusta" form of the hardier "Coffea canephora". Coffee plants are cultivated in more than 70 countries Once ripe, coffee "berries" are picked, processed, and dried to yield the seeds inside. The seeds are then roasted to varying degrees, depending on the desired flavor, before being ground and brewed to create coffee.
Coffee is slightly acidic (pH 5.0–5.1) and can have a stimulating effect on humans because of its caffeine content. It is one of the most popular drinks in the world. It can be prepared and presented in a variety of ways. The effect of coffee on human health has been a subject of many studies; however, results have varied in terms of coffee's relative benefit.
Coffee cultivation first took place in southern Arabia; the earliest credible evidence of coffee-drinking appears in the middle of the 15th century in the Sufi shrines of Yemen.
Carbonated drinks.
Carbonated drinks refer to drinks which have carbon dioxide dissolved into them. This can happen naturally through fermenting and in natural water spas or artificially by the dissolution of carbon dioxide under pressure. The first commercially available artificially carbonated drink is believed to have been produced by Thomas Henry in the late 1770s.
Cola, orange, various roots, ginger, and lemon/lime are commonly used to create non-alcoholic carbonated drinks; sugars and preservatives may be added later.
The most consumed carbonated soft drinks are produced by three major global brands: Coca-Cola, PepsiCo and Cadbury Schweppes.
Juice and juice drinks.
Fruit juice is a natural product that contains few or no additives. Citrus products such as orange juice and tangerine juice are familiar breakfast drinks, while Grapefruit juice, pineapple, apple, grape, lime, and lemon juice are also common. Coconut water is a highly nutritious and refreshing juice. Many kinds of berries are crushed; their juices are mixed with water and sometimes sweetened. Raspberry, blackberry and currants are popular juices drinks but the percentage of water also determines their nutritive value. Grape juice allowed to ferment produces wine.
Fruits are highly perishable so the ability to extract juices and store them was of significant value. Some fruits are highly acidic and mixing them with water and sugars or honey was often necessary to make them palatable. Early storage of fruit juices was labor-intensive, requiring the crushing of the fruits and the mixing of the resulting pure juices with sugars before bottling.
Vegetable juice are usually served warm or cold. Different types of vegetables can be used to make vegetable juice such as carrots, tomatoes, cucumbers, celery and many more. Some vegetable juices are mixed with some fruit juice to make the vegetable juice taste better. Many popular vegetable juices, particularly ones with high tomato content, are high in sodium, and therefore consumption of them for health must be carefully considered. Some vegetable juices provide the same health benefits as whole vegetables in terms of reducing risks of cardiovascular disease and cancer.
Alcoholic Drinks.
An alcoholic beverage is a drink that contains ethanol, commonly known as alcohol (although in chemistry the definition of "alcohol" includes many other compounds). Beer has been a part of human culture for 8,000 years.
In many countries, drinking alcoholic beverages in a local bar or pub is a cultural tradition.
Beer.
Beer is an alcoholic beverage produced by the saccharification of starch and fermentation of the resulting sugar. The starch and saccharificaton enzymes are often derived from malted cereal grains, most commonly malted barley and malted wheat. Most beer is also flavoured with hops, which add bitterness and act as a natural preservative, though other flavourings such as herbs or fruit may occasionally be included. The preparation of beer is called brewing. Beer is the world's most widely consumed alcoholic beverage, and is the third-most popular drink overall, after water and tea. It is thought by some to be the oldest fermented beverage.
Some of humanity's earliest known writings refer to the production and distribution of beer: the Code of Hammurabi included laws regulating beer and beer parlours, and "The Hymn to Ninkasi", a prayer to the Mesopotamian goddess of beer, served as both a prayer and as a method of remembering the recipe for beer in a culture with few literate people. Today, the brewing industry is a global business, consisting of several dominant multinational companies and many thousands of smaller producers ranging from brewpubs to regional breweries.
Cider.
Cider is a fermented alcoholic beverage made from fruit juice, most commonly and traditionally apple juice, but also the juice of peaches, pears ("Perry" cider) or other fruit. Cider may be made from any variety of apple, but certain cultivars grown solely for use in cider are known as cider apples. The United Kingdom has the highest per capita consumption of cider, as well as the largest cider-producing companies in the world, , the U.K. produces 600 million litres of cider each year (130 million imperial gallons).
Wine.
Wine is an alcoholic beverage made from fermented grapes or other fruits. The natural chemical balance of grapes lets them ferment without the addition of sugars, acids, enzymes, water, or other nutrients. Yeast consumes the sugars in the grapes and converts them into alcohol and carbon dioxide. Different varieties of grapes and strains of yeasts produce different styles of wine. The well-known variations result from the very complex interactions between the biochemical development of the fruit, reactions involved in fermentation, terroir and subsequent appellation, along with human intervention in the overall process. The final product may contain tens of thousands of chemical compounds in amounts varying from a few percent to a few parts per billion.
Wines made from produce besides grapes are usually named after the product from which they are produced (for example, rice wine, pomegranate wine, apple wine and elderberry wine) and are generically called fruit wine. The term "wine" can also refer to starch-fermented or fortified beverages having higher alcohol content, such as barley wine, huangjiu, or sake.
Wine has a rich history dating back thousands of years, with the earliest production so far discovered having occurred  BC in Georgia. It had reached the Balkans by  BC and was consumed and celebrated in ancient Greece and Rome.
From its earliest appearance in written records, wine has also played an important role in religion. Red wine was closely associated with blood by the ancient Egyptians, who, according to Plutarch, avoided its free consumption as late as the 7th-century BC Saite dynasty, "thinking it to be the blood of those who had once battled against the gods". The Greek cult and mysteries of Dionysus, carried on by the Romans in their Bacchanalia, were the origins of western theater. Judaism incorporates it in the Kiddush and Christianity in its Eucharist, while alcohol consumption was forbidden in Islam.
Spirits.
The term spirit refers to a distilled beverage that contains no added sugar and has at least 20% alcohol by volume (ABV). Popular spirits include brandy, gin, rum, tequila, vodka, and whisky. Brandy is a spirit created by distilling wine, whilst vodka may be distilled from any starch- or sugar-rich plant matter; most vodka today is produced from grains such as sorghum, corn, rye or wheat.
In culture.
Places to drink.
Throughout history, people have come together in establishments to socialise whilst drinking. This includes cafés and coffeehouses, focus on providing hot drinks as well as light snacks. Many coffee houses in the Middle East, and in West Asian immigrant districts in the Western world, offer "shisha" ("nargile" in Turkish and Greek), flavored tobacco smoked through a hookah. Espresso bars, such as Starbucks and Costa Coffee are a type of coffeehouse that specialize in serving espresso and espresso-based drinks.
In China and Japan, the establishment would be a tea house, were people would socialise whilst drinking tea. Chinese scholars have used the teahouse for places of sharing ideas.
Alcoholic drinks are served in drinking establishments, which have different cultural connotations. For example, pubs are fundamental to the culture of Britain, Ireland, Australia, Atlantic Canada, New England, Metro Detroit, South Africa and New Zealand. In many places, especially in villages, a pub can be the focal point of the community. The writings of Samuel Pepys describe the pub as the heart of England. Many pubs are controlled by breweries, so cask ale or keg beer may be a better value than wines and spirits.
In contrast, Types of bars range from seedy bars or nightclubs, sometimes termed "dive bars", to elegant places of entertainment for the elite. Bars provide stools or chairs that are placed at tables or counters for their patrons. The term "bar" is derived from the specialized counter on which drinks are served. Some bars have entertainment on a stage, such as a live band, comedians, go-go dancers, or strippers. Patrons may sit or stand at the bar and be served by the bartender, or they may sit at tables and be served by cocktail servers.
Matching with food.
Food and drink are often paired together to enhance the taste experience. This primarily happens with wine and a culture has grown up around the process. Weight, flavors and textures can either be contrasted or complemented. In recent years, food magazines began to suggest particular wines with recipes and restaurants would offer multi-course dinners matched with a specific wine for each course.
Presentation.
Different drinks have unique receptacles for their consumption. This is sometimes purely for presentations purposes, such as for cocktails. In other situations, the drinkware has practical application, such as coffee cups which are designed for insulation or brandy snifters which are designed to encourage evaporation but trap the aroma within the glass.
Many glasses include a stem, which allows the drinker to hold the glass without affecting the temperature of the drink. In champagne glasses, the bowl is designed to retain champagne's signature carbonation, by reducing the surface area at the opening of the bowl. Historically, champagne has been served in a champagne coupe, the shape of which allowed carbonation to dissipate even more rapidly than from a standard wine glass.
Commercial trade.
International exports and imports.
An important export commodity, coffee was the top agricultural export for twelve countries in 2004,
and it was the world's seventh-largest legal agricultural export by value in 2005. Green (unroasted) coffee is one of the most traded agricultural commodities in the world.
Investment.
Some drinks, such as wine, can be used as an alternative investment. This can be achieved by either purchasing and reselling individual bottles or cases of particular wines, or purchasing shares in an investment wine fund that pools investors' capital.

</doc>
<doc id="7985" url="http://en.wikipedia.org/wiki?curid=7985" title="Dill">
Dill

Dill (Anethum graveolens) is an annual herb in the celery family Apiaceae. It is the sole species of the genus "Anethum".
Growth.
Dill grows to , with slender hollow stems and alternate, finely divided, softly delicate leaves long. The ultimate leaf divisions are broad, slightly broader than the similar leaves of fennel, which are threadlike, less than broad, but harder in texture. The flowers are white to yellow, in small umbels diameter. The seeds are long and thick, and straight to slightly curved with a longitudinally ridged surface.
Etymology.
The name "dill" comes from Old English "dile", a West Germanic word of unknown origin the plant having the carminative property of relieving gas.
Culinary use.
Fresh and dried dill leaves (sometimes called "dill weed" to distinguish it from dill seed) are widely used as herbs in Europe and central Asia.
Like caraway, the fernlike leaves of dill are aromatic and are used to flavor many foods such as gravlax (cured salmon) and other fish dishes, borscht and other soups, as well as pickles (where the dill flower is sometimes used). Dill is best when used fresh as it loses its flavor rapidly if dried; however, freeze-dried dill leaves retain their flavor relatively well for a few months.
Dill seed, having a flavor similar to caraway but also resembling that of fresh or dried dill weed, is used as a spice. Dill oil is extracted from the leaves, stems and seeds of the plant. The oil from the seeds is distilled and used in the manufacturing of soaps.
Dill is the eponymous ingredient in dill pickles: cucumbers preserved in salty brine and/or vinegar.
European cuisine.
In Russia, Ukraine, and Poland dill is one of the most popular herbs used in the kitchen, and, along with parsley, is used for various purposes. Fresh, finely cut dill leaves are used as topping to various soups, especially the hot red borsht and the cold borsht mixed with curds, kefir, yoghurt, or sour cream, which is served during hot summer weather and is called okroshka. It is also popular in summer to drink fermented milk (curds, kefir, yoghurt, or buttermilk) mixed with finely cut dill (and sometimes other herbs). 
In the same way, prepared dill is used as a topping for boiled potatoes covered with fresh butter - especially in summertime when there are the so-called "new," or young, potatoes. The dill leaves can be mixed with butter beforehand, making it a dill butter, which can serve the same purpose. Dill leaves mixed with tvorog form one of the traditional cheese spreads used for sandwiches. Fresh dill leaves are used all year round as an ingredient for making fresh salads, "e.g.", one made of lettuce, fresh cucumbers and tomatoes, the way basil leaves are used in Italy and Greece. 
In Poland, fresh dill leaves mixed with sour cream are the basis for dressings, and it is especially popular to use this kind of sauce with freshly cut cucumbers, which practically are wholly immersed in the sauce, making thus a salad called "mizeria." The dill leaves serve as a basis for cooking dill sauce, used hot for baked fresh water fish and for chicken or turkey breast, or used hot or cold for hard boiled eggs (depending on the temperature of the eggs served).
In south-eastern Poland it is popular to cook a dill-based soup (zupa koperkowa), served with potatoes and hard boiled eggs. 
Whole stems including roots and flower buds are traditionally used to prepare Polish-style pickled cucumbers (ogórki kiszone), especially the so-called low-salt cucumbers ("ogórki małosolne"). Whole stems of dill (often including the roots) are also cooked with potatoes, especially the late potatoes of autumn and winter, so they resemble the flavor of the newer potatoes found in summer time. Some kinds of fish, especially trout and salmon, are also traditionally baked with both stems and leaves of dill. 
In Romania dill ("mărar") is widely used as an ingredient for soups such as "borş" (pronounced "borsh"), pickles and other dishes, especially those based on peas, beans and cabbage. It is also popular for dishes based on potatoes and mushrooms and can be found in many summer salads (especially cucumber salad, cabbage salad and lettuce salad). During springtime, it is used together with spring onions in omelets. It often complements sauces based on sour cream or yogurt and is often mixed with salted cheese and used as a filling. Another popular dish with dill as a main ingredient is dill sauce, which is served with eggs and fried sausages.
In Hungary, dill is very widely used. It is popular as a sauce or filling, especially in Langos, and mixed with a type of cottage cheese. Dill is also used for pickling and in salads. The Hungarian name for dill is "kapor".
In Serbia, dill is known as "mirodjija" and is used as an addition to soups, potato and cucumber salads and French fries. It also features in the Serbian proverb "бити мирођија у свакој чорби" /biti mirodjija u svakoj čorbi/ (to be a dill in every soup) which corresponds to the English proverb "to have a finger in every pie".
In Greece, dill is known as 'άνηθος'(anithos). In antiquity it was used as an add-in in wines, which they were called "anithites oinos" (wine with anithos-dill). In modern days, dill is used in salads, soups, sauces, fish dishes and vegetable dishes.
In Santa Maria, Azores, dill ("endro") is the most important ingredient of the traditional Holy Ghost soup ("sopa do Espírito Santo"). Dill is found practically everywhere in Santa Maria and is curiously rare in the other Azorean Islands.
Asian cooking.
In Iran, dill is known as "shevid" and is sometimes used with rice and called "shevid-polo". It is also used in Iranian "aash" recipes, and is also called "sheved" in Persian.
In India, dill is known as "shepu" (शेपू) in Marathi and Konkani, "savaa" in Hindi or "soa" in Punjabi. In Telugu, it is called "Soa-kura" (for herb greens). It is also called "sabbasige soppu" (ಸಬ್ಬಸಿಗೆ ಸೊಪ್ಪು) in Kannada. In Tamil it is known as "sada kuppi"(சதகுப்பி). In Malayalam, it is ചതകുപ്പ ("chathakuppa") or ശതകുപ്പ ("sathakuppa"). In Sanskrit, this herb is called "shatapushpa". In Gujarati, it is known as "suva"(સૂવા). In India, dill is prepared in the manner of yellow "moong dal" as a main-course dish. It is considered to have very good antigas properties,so it is used as "mukhwas", or an after-meal digestive. It is also traditionally given to mothers immediately after childbirth. In the state of Uttar Pradesh in India, a smaller amount of fresh dill is cooked along with cut potatoes and fresh fenugreek leaves(Hindi आलू-मेथी-सोया).
In Manipur, dill, locally known as "pakhon", is an essential ingredient of "chagem pomba" – a traditional Manipuri dish made with fermented soybean and rice.
In Laos and parts of northern Thailand, dill is known in English as Lao coriander (, ). In the Lao language, it is called "phak see", and in Thai, it is known as "phak chee Lao". In Lao cuisine, Lao coriander is used extensively in traditional Lao dishes such as "mok pa" (steamed fish in banana leaf) and several coconut milk-based curries that contain fish or prawns.
In China dill is colloquially called "huixiang" (茴香), or more properly "shiluo" (莳萝). It is a common filling in baozi and xianbing and can be used vegetarian, with rice vermicelli, or combined with either meat or eggs. Vegetarian dill baozi are a common part of a Beijing breakfast. In baozi and xianbing, it is often interchangeable with non-bulbing fennel and the term 茴香 can also refer to fennel, like caraway and coriander leaf share a name in Chinese as well. Dill is also stir fried as a potherb, often with egg, in the same manner as Chinese chives. It is commonly used in Taiwan as well. 
In Vietnam, the use of dill in cooking is regional; it is used mainly in northern Vietnamese cuisine.
Middle East uses.
In Arab countries, dill seed, called "ain jaradeh" (grasshopper's eye), is used as a spice in cold dishes such as "fattoush" and pickles. In Arab countries of the Persian Gulf, dill is called "shibint" and is used mostly in fish dishes. In Egypt, dillweed is commonly used to flavor cabbage dishes, including "koronb mahshi" (stuffed cabbage leaves).
Other regional cooking.
In Canada, dill is a favorite herb to accompany poached salmon.
Traditional uses.
In Anglo-Saxon England, as prescribed in "Leechdoms, Wortcunning, and Starcraft of Early England" (also called "Læceboc", many of whose recipes were borrowed from Greek medicinal texts), dill was used in many traditional medicines, including those against jaundice, headache, boils, lack of appetite, stomach problems, nausea, liver problems, and many other ills. Dill seeds can also be used to prepare herbal tea.
In India the leaves of dill and other greens are used to prepare a variety of local dishes which are served as an accompaniment to rotis or chapatis.
In ancient Greece fragrance was made from the leaves of dill. Also, athletes used to spread essence of dill all over their body, as muscle toner.
Cultivation.
Successful cultivation requires warm to hot summers with high sunshine levels; even partial shade will reduce the yield substantially. It also prefers rich, well drained soil. The seeds are viable for three to ten years.
The seed is harvested by cutting the flower heads off the stalks when the seed is beginning to ripen. The seed heads are placed upside down in a paper bag and left in a warm, dry place for a week. The seeds then separate from the stems easily for storage in an airtight container.
Companion planting.
When used as a companion planting, dill draws in many beneficial insects as the umbrella flower heads go to seed. Fittingly, it makes a good companion plant for cucumbers. It is a poor companion for carrots and tomatoes.

</doc>
<doc id="7988" url="http://en.wikipedia.org/wiki?curid=7988" title="Dual space">
Dual space

In mathematics, any vector space, "V", has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on "V" together with a naturally induced linear structure. Dual vector spaces for finite-dimensional vector spaces can be used for studying tensors. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are employed for defining and studying concepts like measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in the study of functional analysis.
There are two types of dual spaces: the "algebraic dual space", and the "continuous dual space". The algebraic dual space is defined for all vector spaces. When defined for a topological vector space there is a subspace of this dual space, corresponding to continuous linear functionals, which constitutes a continuous dual space.
Algebraic dual space.
Given any vector space "V" over a field "F", the dual space "V*" is defined as the set of all linear maps (linear functionals). The dual space "V*" itself becomes a vector space over "F" when equipped with the following addition and scalar multiplication:
for all "φ" and "ψ" ∈ "V*", "x" ∈ "V", and "a" ∈ "F". Elements of the algebraic dual space "V*" are sometimes called covectors or one-forms.
The pairing of a functional "φ" in the dual space "V*" and an element "x" of "V" is sometimes denoted by a bracket: 
or . The pairing defines a nondegenerate bilinear mapping .
Finite-dimensional case.
If "V" is finite-dimensional, then "V"∗ has the same dimension as "V". Given a basis in "V", it is possible to construct a specific basis in "V"∗, called the dual basis. This dual basis is a set of linear functionals on "V", defined by the relation
for any choice of coefficients . In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equations
where formula_4 is the Kronecker delta symbol. For example if "V" is R2, and its basis chosen to be , then e1 and e2 are one-forms (functions that map a vector to a scalar) such that , , , and . (Note: The superscript here is the index, not an exponent).
In particular, if we interpret R"n" as the space of columns of "n" real numbers, its dual space is typically written as the space of "rows" of "n" real numbers. Such a row acts on R"n" as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a Real number y. Then, seeing this functional as a matrix M, and x,y as a nx1 matrix and an 1x1 matrix (trivially, a Real number)respectively, if we have Mx=y, then, by dimension reasons, M must be an 1xn matrix itself, i.e., M must be a row vector. 
If "V" consists of the space of geometrical vectors (arrows) in the plane, then the level curves of an element of "V"∗ form a family of parallel lines in "V", because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element . So an element of "V"∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if "V" is a vector space of any dimension, then the level sets of a linear functional in "V"∗ are parallel hyperplanes in "V", and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.
Infinite-dimensional case.
If "V" is not finite-dimensional but has a basis e"α" indexed by an infinite set "A", then the same construction as in the finite-dimensional case yields linearly independent elements e"α" () of the dual space, but they will not form a basis.
Consider, for instance, the space R∞, whose elements are those sequences of real numbers which have only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for , e"i" is the sequence which is zero apart from the "i"th term, which is one. The dual space of R∞ is RN, the space of "all" sequences of real numbers: such a sequence ("an") is applied to an element ("xn") of R∞ to give the number ∑"anxn", which is a finite sum because there are only finitely many nonzero "xn". The dimension of R∞ is countably infinite, whereas RN does not have a countable basis.
This observation generalizes to any infinite-dimensional vector space "V" over any field "F": a choice of basis } identifies "V" with the space ("FA")0 of functions such that is nonzero for only finitely many , where such a function "f" is identified with the vector
in "V" (the sum is finite by the assumption on "f", and any may be written in this way by the definition of the basis).
The dual space of "V" may then be identified with the space "FA" of "all" functions from "A" to "F": a linear functional "T" on "V" is uniquely determined by the values it takes on the basis of "V", and any function (with ) defines a linear functional "T" on "V" by
Again the sum is finite because "fα" is nonzero for only finitely many "α".
Note that ("FA")0 may be identified (essentially by definition) with the direct sum
of infinitely many copies of "F" (viewed as a 1-dimensional vector space over itself) indexed by "A", i.e., there are linear isomorphisms
On the other hand "FA" is (again by definition), the direct product of infinitely many copies of "F" indexed by "A", and so the identification
is a special case of a general result relating direct sums (of modules) to direct products.
Thus if the basis is infinite, then the algebraic dual space is "always" of larger dimension than the original vector space. This is in marked contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.
Bilinear products and dual spaces.
If "V" is finite-dimensional, then "V" is isomorphic to "V*". But there is in general no natural isomorphism between these two spaces. Any bilinear form ⟨•,•⟩ on "V" gives a mapping of "V" into its dual space via
where the right hand side is defined as the functional on "V" taking each to 〈"v","w"〉. In other words, the bilinear form determines a linear mapping
defined by
If the bilinear form is nondegenerate, then this is an isomorphism onto a subspace of "V*". If "V" is finite-dimensional, then this is an isomorphism onto all of "V*". Conversely, any isomorphism Φ from "V" to a subspace of "V*" (resp., all of "V*") defines a unique nondegenerate bilinear form ⟨•,•⟩Φ on "V" by
Thus there is a one-to-one correspondence between isomorphisms of "V" to subspaces of (resp., all of) "V*" and nondegenerate bilinear forms on "V".
If the vector space "V" is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form ⟨•,•⟩ determines an isomorphism of "V" with the complex conjugate of the dual space
The conjugate space "V*" can be identified with the set of all additive complex-valued functionals such that
Injection into the double-dual.
There is a natural homomorphism Ψ from "V" into the double dual "V**", defined by for all , . This map Ψ is always injective; it is an isomorphism if and only if "V" is finite-dimensional. Indeed, the isomorphism of a finite-dimensional vector space with its double dual is an archetypal example of a natural isomorphism. Note that infinite-dimensional Hilbert spaces are not a counterexample to this, as they are isomorphic to their continuous duals, not to their algebraic duals.
Transpose of a linear map.
If is a linear map, then the "transpose" (or "dual") is defined by
for every . The resulting functional "f*"("φ") in "V*" is called the "pullback" of "φ" along "f".
The following identity holds for all and :
where the bracket [•,•] on the left is the duality pairing of "V" with its dual space, and that on the right is the duality pairing of "W" with its dual. This identity characterizes the transpose, and is formally similar to the definition of the adjoint.
The assignment produces an injective linear map between the space of linear operators from "V" to "W" and the space of linear operators from "W*" to "V*"; this homomorphism is an isomorphism if and only if "W" is finite-dimensional. If then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that . In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over "F" to itself. Note that one can identify ("f*")* with "f" using the natural injection into the double dual.
If the linear map "f" is represented by the matrix "A" with respect to two bases of "V" and "W", then "f*" is represented by the transpose matrix "A"T with respect to the dual bases of "W*" and "V*", hence the name. Alternatively, as "f" is represented by "A" acting on the left on column vectors, "f*" is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on R"n", which identifies the space of column vectors with the dual space of row vectors.
Quotient spaces and annihilators.
Let "S" be a subset of "V". The annihilator of "S" in "V*", denoted here "So", is the collection of linear functionals such that for all . That is, "So" consists of all linear functionals such that the restriction to "S" vanishes: .
The annihilator of a subset is itself a vector space. In particular, is all of "V*" (vacuously), whereas is the zero subspace. Furthermore, the assignment of an annihilator to a subset of "V" reverses inclusions, so that if , then
Moreover, if "A" and "B" are two subsets of "V", then
and equality holds provided "V" is finite-dimensional. If "Ai" is any family of subsets of "V" indexed by "i" belonging to some index set "I", then
In particular if "A" and "B" are subspaces of "V", it follows that
If "V" is finite-dimensional, and "W" is a vector subspace, then
after identifying "W" with its image in the second dual space under the double duality isomorphism . Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.
If "W" is a subspace of "V" then the quotient space "V"/"W" is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional factors through "V"/"W" if and only if "W" is in the kernel of "f". There is thus an isomorphism
As a particular consequence, if "V" is a direct sum of two subspaces "A" and "B", then "V*" is a direct sum of "Ao" and "Bo".
Continuous dual space.
When dealing with topological vector spaces, one is typically only interested in the continuous linear functionals from the space into the base field formula_23 (or formula_24). This gives rise to the notion of the "continuous dual space" or "topological dual" which is a linear subspace of the algebraic dual space formula_25, denoted by formula_26. For any "finite-dimensional" normed vector space or topological vector space, such as Euclidean "n-"space, the continuous dual and the algebraic dual coincide. This is however false for any infinite-dimensional normed space, as shown by the example of discontinuous linear maps. Nevertheless in the theory of topological vector spaces the terms "continuous dual space" and "topological dual space" are rarely used, as a rule they are replaced by "dual space", since there is no serious need to consider discontinuous maps in this field.
For a topological vector space formula_27 its "continuous dual space", or "topological dual space", or just "dual space" (in the sense of the theory of topological vector spaces) formula_26 is defined as the space of all continuous linear functionals formula_29.
There is a standard construction for introducing topology on the continuous dual formula_26 of a topological vector space formula_27: each given class formula_32 of bounded subsets in formula_27 defines a topology on formula_27 of uniform convergence on sets from formula_32, or what is the same a topology generated by seminorms of the form 
where formula_37 is a continuous linear functional on formula_27, and formula_39 runs over the class formula_32.
This means that a net of functionals formula_41 tends to a functional formula_37 in formula_26 if and only if 
Usually (but not necessarily) the class formula_32 is supposed to satisfy the following conditions:
If these requirements are fulfilled then the corresponding topology on formula_26 is Hausdorff and the sets 
form its local base.
Here are the three most important special cases.
Each of these three choices of topology on formula_26 leads to a variant of reflexivity property for topological vector spaces.
Examples.
Let 1 < "p" < ∞ be a real number and consider the Banach space "ℓ p" of all sequences for which
is finite. Define the number "q" by . Then the continuous dual of "ℓ p" is naturally identified with "ℓ q": given an element , the corresponding element of is the sequence ("φ"(e"n")) where e"n" denotes the sequence whose "n-"th term is 1 and all others are zero. Conversely, given an element , the corresponding continuous linear functional "φ" on is defined by for all (see Hölder's inequality).
In a similar manner, the continuous dual of is naturally identified with (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces "c" (consisting of all convergent sequences, with the supremum norm) and "c"0 (the sequences converging to zero) are both naturally identified with .
By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.
Transpose of a continuous linear map.
If is a continuous linear map between two topological vector spaces, then the (continuous) transpose is defined by the same formula as before:
The resulting functional is in. The assignment produces a linear map between the space of continuous linear maps from "V" to "W" and the space of linear maps from to . When "T" and "U" are composable continuous linear maps, then
When "V" and "W" are normed spaces, the norm of the transpose in is equal to that of "T" in. Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map "T" has dense range if and only if the transpose is injective.
When "T" is a compact linear map between two Banach spaces "V" and "W", then the transpose is compact. This can be proved using the Arzelà–Ascoli theorem.
When "V" is a Hilbert space, there is an antilinear isomorphism "iV" from "V" onto its continuous dual. For every bounded linear map "T" on "V", the transpose and the adjoint operators are linked by
When "T" is a continuous linear map between two topological vector spaces "V" and "W", then the transpose is continuous when and are equipped with"compatible" topologies: for example when, for and , both duals have the strong topology of uniform convergence on bounded sets of "X", or both have the weak-∗ topology of pointwise convergence on "X". The transpose is continuous from to , or from to .
Annihilators.
Assume that "W" is a closed linear subspace of a normed space "V", and consider the annihilator of "W" in,
Then, the dual of the quotient can be identified with "W"⊥, and the dual of "W" can be identified with the quotient . Indeed, let "P" denote the canonical surjection from "V" onto the quotient ; then, the transpose is an isometric isomorphism from into, with range equal to "W"⊥. If "j" denotes the injection map from "W" into "V", then the kernel of the transpose is the annihilator of "W":
and it follows from the Hahn–Banach theorem that induces an isometric isomorphism
Further properties.
If the dual of a normed space "V" is separable, then so is the space "V" itself. The converse is not true: for example the space is separable, but its dual is is not.
Topologies on the dual.
The topology of "V" and the topology of real or complex numbers can be used to induce on "V′" a dual space topology.
Double dual.
In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator from a normed space "V" into its continuous double dual, defined by
As a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning for all "x" in "V". Normed spaces for which the map Ψ is a bijection are called reflexive.
When "V" is a topological vector space, one can still define Ψ("x") by the same formula, for every , however several difficulties arise. First, when "V" is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial. However, if "V" is Hausdorff and locally convex, the map Ψ is injective from "V" to the algebraic dual of the continuous dual, again as a consequence of the Hahn–Banach theorem.
Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual , so that the continuous double dual is not uniquely defined as a set. Saying that Ψ maps from "V" to , or in other words, that Ψ("x") is continuous on for every , is a reasonable minimal requirement on the topology of , namely that the evaluation mappings
be continuous for the chosen topology on . Further, there is still a choice of a topology on , and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.

</doc>
<doc id="7989" url="http://en.wikipedia.org/wiki?curid=7989" title="Dianetics">
Dianetics

Dianetics is a set of ideas and practices regarding the metaphysical relationship between the mind and body which was created by L. Ron Hubbard and is practiced by followers of Scientology and separate independent Dianeticist groups. Hubbard coined "Dianetics" from the Greek stems "dia", meaning "through," and "nous", meaning "mind". Dianetics has achieved no acceptance as a scientific theory, and is an example of a pseudoscience.
Dianetics divides the mind into three parts: the conscious "analytical mind," the subconscious "reactive mind," and the somatic mind. The goal of Dianetics is to remove the "reactive mind," which Scientologists believe prevents people from becoming more ethical, more aware, happier and saner. The Dianetics procedure to achieve this is called "auditing". Auditing is a process whereby a series of questions are asked by the Scientology auditor, in an attempt to rid the audited person of the painful experiences of the past, which Scientologists believe to be the cause of the "reactive mind".
Scientologists believe that "the basic principle of existence is to survive," and that the motivation to survive is inhibited by aberrations "ranging from simple neuroses to different psychotic states to various kinds of sociopathic behavior patterns". Hubbard developed Dianetics, claiming that it could eradicate these aberrations.
When Hubbard formulated Dianetics, he described it as "a mix of Western technology and Oriental philosophy". He said that Dianetics "forms a bridge between" cybernetics and General Semantics (a set of ideas about education originated by Alfred Korzybski, which received much attention in the science fiction world in the 1940s) — a claim denied by scholars of General Semantics, including S. I. Hayakawa, who expressed strong criticism of Dianetics as early as 1951. Hubbard claimed that Dianetics could increase intelligence, eliminate unwanted emotions and alleviate a wide range of illnesses he believed to be psychosomatic. Among the conditions purportedly treated were arthritis, allergies, asthma, some coronary difficulties, eye trouble, ulcers, migraine headaches, 'sexual deviation' (a category which for Hubbard, like many of his time, included homosexuality) and even death. Hubbard asserted that "memories of painful physical and emotional experiences accumulate in a specific region of the mind, causing illness and mental problems." He taught that "once these experiences have been purged through cathartic procedures he developed, a person can achieve superior health and intelligence." Hubbard also variously defined Dianetics as "a spiritual healing technology" and "an organized science of thought."
Dianetics predates Hubbard's classification of Scientology as "applied religious philosophy". Early in 1951, he expanded his writings to include teachings related to the soul, or "thetan". Dianetics is practiced by several independent Dianetics-only groups not connected with Scientology, and also Free Zone or Independent Scientologists. The Church of Scientology disapproves of independent Scientology activities and has prosecuted them in court for misappropriation of Scientology and Dianetics copyrights and trademarks.
History.
Hubbard always claimed that his ideas of Dianetics originated in the 1920s and 1930s. By his own account, he spent a great deal of time in the Oak Knoll Naval Hospital's library, where he would have encountered the work of Freud and other psychoanalysts. In April 1950, Hubbard and several others established the Hubbard Dianetic Research Foundation in Elizabeth, New Jersey to coordinate work related for the forthcoming publication. Hubbard first introduced Dianetics to the public in the article ' published in the May 1950 issue of the magazine "Astounding Science Fiction". Hubbard wrote ' at that time, allegedly completing the 180,000-word book in six weeks.
The success of selling "Dianetics: The Modern Science of Mental Health" brought in a flood of money, which Hubbard used to establish Dianetics foundations in six major American cities. The scientific and medical communities were far less enthusiastic about Dianetics, viewing it with bemusement, concern, or outright derision. Complaints were made against local Dianetics practitioners for allegedly practicing medicine without a license. This eventually prompted Dianetics advocates to disclaim any medicinal benefits in order to avoid regulation.
Hubbard explained the backlash as a response from various entities trying to co-opt Dianetics for their own use. Hubbard blamed the hostile press coverage in particular on a plot by the American Communist Party. In later years, Hubbard decided that the psychiatric profession was the origin of all of the criticism of Dianetics, as he believed it secretly controlled most of the world's governments.
By the autumn of 1950, financial problems had developed, and by November 1950, the six foundations had spent around one million dollars and were more than $200,000 in debt. Disagreements emerged over the direction of the Dianetic Foundation's work, and relations between the board members became strained, with several leaving, even to support causes critical of Dianetics. One example was Harvey Jackins, founder of Re-evaluation Counselling, originally a sort of discrete reworking of Dianetics, which L Ron Hubbard later declared suppressive to Scientology.
In January 1951, the New Jersey Board of Medical Examiners instituted proceedings against the Hubbard Dianetic Research Foundation in Elizabeth for teaching medicine without a licence. The Foundation closed its doors, causing the proceedings to be vacated, but its creditors began to demand settlement of its outstanding debts. Don Purcell, a millionaire Dianeticist from Wichita, Kansas, offered a brief respite from bankruptcy, but the Foundation's finances failed again in 1952.
Because of a sale of assets resulting from the bankruptcy, Hubbard no longer owned the rights to the name "Dianetics", but its philosophical framework still provided the seed for Scientology to grow. Scientologists refer to the book "Dianetics: The Modern Science of Mental Health" as "Book One." In 1952, Hubbard published a new set of teachings as "Scientology, a religious philosophy." Scientology did not replace Dianetics but extended it to cover new areas: Where the goal of Dianetics is to rid the individual of his reactive mind engrams, the stated goal of Scientology is to rehabilitate the individual's spiritual nature so that he may reach his full potential.
In 1978, Hubbard released "New Era Dianetics" (NED), a revised version supposed to produce better results in a shorter period of time. The course consists of 11 rundowns and requires a specifically trained auditor. It is run (processed) exactly like Standard Dianetics, widely practiced before the advent of NED, except the pre-clear (parishioner) is encouraged to find the "postulate" he made as a result of the incident. ("Postulate" in Dianetics and Scientology has the meaning of "a conclusion, decision or resolution made by the individual himself; to conclude, decide or resolve a problem or to set a pattern for the future or to nullify a pattern of the past" in contrast to its conventional meanings.)
In the Church of Scientology, OTs study several levels of before reaching the highest level.
Basic concepts.
In the book, "", Hubbard describes techniques that he suggests can rid individuals of fears and psychosomatic illnesses. A basic idea in Dianetics is that the mind consists of two parts: the "analytical mind" and the "reactive mind." The "reactive mind", the mind which operates when a person is physically unconscious, acts as a record of shock, trauma, pain, and otherwise harmful memories. Experiences such as these, stored in the "reactive mind" are dubbed "engrams". Dianetics is a proposed method to erase these "engrams" in the "reactive mind" to achieve what is referred to in Scientology as a state of "Clear". A "Clear" is one who is thought to no longer possess his reactive mind.
By his own admission, Hubbard made what he considered was one of the greatest mistakes of his life when he used the biological definition of engram as a "trace on a cell", which was not in line with the proper biological definition.
Hubbard described Dianetics as "an organized science of thought built on definite axioms: statements of natural laws on the order of those of the physical sciences". These Dianetic "axioms" can be found in Hubbard books such as "Scientology 0-8: The Book of Basics" and "Advanced Procedure and Axioms". Unlike conventional therapies, Hubbard said, Dianetics would work every time if applied properly and "will invariably cure all psychosomatic ills and human aberrations." In April 1950, before the public release of Dianetics, he wrote: "To date, over two hundred patients have been treated; of those two hundred, two hundred cures have been obtained."
In Dianetics, the unconscious or reactive mind is described as a collection of "mental image pictures," which contain the recorded experience of past moments of unconsciousness, including all sensory perceptions and feelings involved, ranging from pre-natal experiences, infancy and childhood, even the traumatic feelings associated events from past lives and alien cultures. The type of mental image picture created during a period of unconsciousness involves the exact recording of a painful experience. Hubbard called this phenomenon an engram, and defined it as "a complete recording of a moment of unconsciousness containing physical pain or painful emotion and all perceptions."
Hubbard proposed that, via pain, physical or mental traumas caused "aberrations" (deviations from rational thinking) in the mind, which produced adverse physical and emotional effects. The conscious or analytical mind, out of a desire for survival, would instinctively shut down during moments of stress. The memories recorded during this period would be stored as engrams in the unconscious or reactive mind. (In Hubbard's earliest publications on the subject, engrams were variously referred to as "Norns", "Impediments," and "comanomes" before "engram" was adapted from its existing usage at the suggestion of Joseph Winter.) Some commentators noted Dianetics' blend of science fiction and occult orientations at the time.
Dianetics claims that these engrams are the cause of almost all psychological and physical problems. In addition to containing the experience of physical pain, engrams can also include words or phrases overheard by the patient while he was unconscious. For instance, Winter cites the example of a patient with a persistent headache supposedly tracing the problem to a doctor saying "Take him now" during the preclear's birth. (wordy/unclear) Hubbard similarly claims that the cause of leukemia is traceable to "an engram containing the phrase 'It turns my blood to water.'" While it is sometimes claimed that the Church of Scientology no longer stands by Hubbard's claims that Dianetics can treat physical conditions, it still publishes them: "... when the knee injuries of the past are located and discharged, the arthritis ceases, no other injury takes its place and the person is finished with arthritis of the knee." "[The reactive mind] can give a man arthritis, bursitis, asthma, allergies, sinusitis, coronary trouble, high blood pressure ... And it is the only thing in the human being which can produce these effects ... Discharge the content of [the reactive mind] and the arthritis vanishes, myopia gets better, heart illness decreases, asthma disappears, stomachs function properly and the whole catalog of ills goes away and stays away."
Some of the psychometric ideas in Dianetics can be traced to Sigmund Freud, whom Hubbard credited as an inspiration and was said to have used as a source. Freud had speculated 40 years previously that traumas with similar content join together in "chains," embedded in the unconscious mind, to cause irrational responses in the individual. Such a chain would be relieved by inducing the patient to remember the earliest trauma, "with an accompanying expression of emotion."
According to Bent Corydon, Hubbard created the illusion that Dianetics was the first psychotherapy to address traumatic experiences in their own time, but others had done so as standard procedure.
One treatment method Hubbard drew from in developing Dianetics was abreaction therapy. Abreaction is a psychoanalytical term that means bringing to consciousness, and thus adequate expression, material that has been unconscious." It includes not only the recollection of forgotten memories and experience, but also their reliving with appropriate emotional display and discharge of effect. This process is usually facilitated by the patient's gaining awareness of the causal relationship between the previously undischarged emotion and his symptoms."
According to Hubbard, before Dianetics psychotherapists may have been able to deal with very light and superficial incidents (e.g. an incident that reminds you of a moment of loss), but with Dianetic therapy, the patient can actually erase moments of pain and unconsciousness. He emphasizes: "The discovery of the engram is entirely the property of Dianetics. Methods of its erasure are also owned entirely by Dianetics..."
Whilst 1950’s style Dianetics could in some respects be considered similar to older therapies, after the release of New Era Dianetics in 1978 such claims become untenable. New Era Dianetics is only done using an E-Meter and is a rote procedure for running "chains" of related traumatic incidents.
Dianetics clarifies the understanding of psychosomatic illness in terms of "predisposition", "precipitation", and "prolongation". 
With the use of Dianetics techniques, Hubbard claimed, the reactive mind could be processed and all stored engrams could be refiled as experience. The central technique was "auditing," a two-person question-and-answer therapy designed to isolate and dissipate engrams (or "mental masses"). An auditor addresses questions to a subject, observes and records the subject's responses, and returns repeatedly to experiences or areas under discussion that appear painful until the troubling experience has been identified and confronted. Through repeated applications of this method, the reactive mind could be "cleared" of its content having outlived its usefulness in the process of evolution; a person who has completed this process would be "Clear".
The benefits of going Clear, according to Hubbard, were dramatic. A Clear would have no compulsions, repressions, psychoses or neuroses, and would enjoy a near-perfect memory as well as a rise in IQ of as much as 50 points. He also claimed that "the atheist is activated by engrams as thoroughly as the zealot". He further claimed that widespread application of Dianetics would result in "A world without insanity, without criminals and without war."
According to the Scientology journal "The Auditor", the total number of "Clears" as of May 2006 stands at 50,311.
Scientific evaluation and criticisms.
Hubbard's original book on Dianetics attracted highly critical reviews from science and medical writers and organizations. The American Psychological Association passed a resolution in 1950 calling "attention to the fact that these claims are not supported by empirical evidence of the sort required for the establishment of scientific generalizations." Subsequently, Dianetics has achieved no acceptance as a scientific theory and scientists cite Dianetics as an example of a pseudoscience.
Few scientific investigations into the effectiveness of Dianetics have been published. Professor John A. Lee states in his 1970 evaluation of Dianetics:
The MEDLINE database records two independent scientific studies on Dianetics, both conducted in the 1950s under the auspices of New York University. Harvey Jay Fischer tested Dianetics therapy against three claims made by proponents and found it does not effect any significant changes in intellectual functioning, mathematical ability, or the degree of personality conflicts; Jack Fox tested Hubbard's thesis regarding recall of engrams, with the assistance of the Dianetic Research Foundation, and could not substantiate it.
Hubbard claimed, in an interview with the "New York Times" in November 1950, that "he had already submitted proof of claims made in the book to a number of scientists and associations." He added that the public as well as proper organizations were entitled to such proof and that he was ready and willing to give such proof in detail. In January 1951, the Hubbard Dianetic Research Foundation of Elizabeth, NJ published "Dianetic Processing: A Brief Survey of Research Projects and Preliminary Results", a booklet providing the results of psychometric tests conducted on 88 people undergoing Dianetics therapy. It presents case histories and a number of X-ray plates to support claims that Dianetics had cured "aberrations" including manic depression, asthma, arthritis, colitis and "overt homosexuality," and that after Dianetic processing, test subjects experienced significantly increased scores on a standardized IQ test. The report's subjects are not identified by name, but one of them is clearly Hubbard himself ("Case 1080A, R. L.").
The authors provide no qualifications, although they are described in Hubbard's book "Science of Survival" (where some results of the same study were reprinted) as psychotherapists. Critics of Dianetics are skeptical of this study, both because of the bias of the source and because the researchers appear to ascribe all physical benefits to Dianetics without considering possible outside factors; in other words, the report lacks any scientific controls. J.A. Winter, M.D., originally an associate of Hubbard and an early adopter of Dianetics, had by the end of 1950 cut his ties with Hubbard and written an account of his personal experiences with Dianetics. He described Hubbard as "absolutistic and authoritarian", and criticized the Hubbard Dianetic Research Foundation for failing to undertake "precise scientific research into the functioning of the mind". He also recommended that auditing be done by experts only and that it was dangerous for laymen to audit each other. Hubbard writes: "Again, Dianetics is not being released to a profession, for no profession could encompass it."
Commentators from a variety of backgrounds have described Dianetics as an example of pseudoscience—that is, information presented as scientific that fails to meet the criteria for science. For example, philosophy professor Robert Carroll points to Dianetics' lack of empirical evidence:
W. Sumner Davis similarly comments that
Procedure in practice.
The procedure of Dianetics therapy (known as "auditing") is a two-person activity. One person, the "auditor", guides the other person, the "pre-clear". The pre-Clear's job is to look at the mind and talk to the auditor. The auditor acknowledges what the pre-Clear says and controls the process so the pre-Clear may put his full attention on his work.
The auditor and pre-Clear sit down in chairs facing each other. The process then follows in eleven distinct steps:
Auditing sessions are kept confidential. This has come into question, though, that confidential information has been used to blackmail possible defectors (see Fair Game). A few transcripts of auditing sessions with confidential information removed have been published as demonstration examples. Some extracts can be found in Dr. J.A. Winter's book "". Other, more comprehensive, transcripts of auditing sessions carried out by Hubbard himself can be found in volume 1 of the "Research & Discovery Series" (Bridge Publications, 1980). Examples of public group processing sessions can be found throughout the "Congresses" lecture series.
According to Hubbard, auditing enables the pre-Clear to "contact" and "release" engrams stored in the reactive mind, relieving him of the physical and mental aberrations connected with them. The pre-Clear is asked to inspect and familiarize himself with the exact details of his own experience; the auditor may not tell him anything about his case or evaluate any of the information the pre-Clear finds.
The validity and practice of auditing have been questioned by a variety of non-Scientologist commentators. Commenting on the example cited by Winter, the science writer Martin Gardner asserts that "nothing could be clearer from the above dialogue than the fact that the dianetic explanation for the headache existed only in the mind of the therapist, and that it was with considerable difficulty that the patient was maneuvered into accepting it."
Other critics and medical experts have suggested that Dianetic auditing is a form of hypnosis, although the Church of Scientology has strongly denied that hypnosis forms any part of Dianetics. To the contrary, L. Ron Hubbard expressly warns not to use any hypnosis or hypnosis-like methods, because a person under hypnosis would be receptive to suggestions. This would decrease his self-determinism instead of increasing it, which is one of the prime goals of Dianetics. Winter [1950] comments that the leading nature of the questions asked of a pre-Clear "encourage fantasy", a common issue also encountered with hypnosis, which can be used to form false memories. The auditor is instructed not to make any assessment of a recalled memory's reality or accuracy, but instead to treat it as if it were objectively real. Professor Richard J. Ofshe, a leading expert on false memories, suggests that the feeling of well-being reported by pre-Clear at the end of an auditing session may be induced by post-hypnotic suggestion. According to Hubbard: "Laughter is definitely the relief of painful emotion."
Autocontrol.
According to Hubbard, the majority of the people interested in the subject believed they could accomplish therapy alone. "It cannot be done" and he adds: "If a patient places himself in autohypnosis and regresses himself in an effort to reach illness or birth or prenatals, the only thing he will get is ill".

</doc>
<doc id="7990" url="http://en.wikipedia.org/wiki?curid=7990" title="Data warehouse">
Data warehouse

In computing, a data warehouse (DW, DWH), or an enterprise data warehouse (EDW), is a system used for reporting and data analysis. Integrating data from one or more disparate sources creates a central repository of data, a data warehouse (DW). Data warehouses store current and historical data and are used for creating trending reports for senior management reporting such as annual and quarterly comparisons.
The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.
Software tools.
The typical extract-transform-load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.
This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, cataloged and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support. However, the means to retrieve and analyze data, to extract, transform and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository, and tools to manage and retrieve metadata.
Benefits.
A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to :
Generic data warehouse environment.
The environment for data warehouses and marts includes the following:
In regards to source systems listed above, Rainer states, “A common source for the data in data warehouses is the company’s operational databases, which can be relational databases”.
Regarding data integration, Rainer states, “It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse”.
Rainer discusses storing data in an organization’s data warehouse or data marts.”.
Metadata are data about data. “IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures“.
Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers. A “data warehouse” is a repository of historical data that are organized by subject to support decision makers in the organization. Once data are stored in a data mart or warehouse, they can be accessed.
History.
The concept of data warehousing dates back to the late 1980s when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "data marts" that were tailored for ready access by users.
Key developments in early years of data warehousing were:
Information storage.
Facts.
A fact is a value or measurement, which represents a fact about the managed entity or system.
Facts as reported by the reporting entity are said to be at raw level.
E.g. if a BTS received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system:
Facts at raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information out of it. These are called aggregates or summaries or aggregated facts.
E.g. if there are 3 BTSs in a city, then facts above can be aggregated from BTS to city level in network dimension.
E.g.
Dimensional vs. normalized approach for storage of data.
There are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.
The dimensional approach refers to Ralph Kimball’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.
In a dimensional approach, transaction data are partitioned into "facts", which are generally numeric transaction data, and "dimensions", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.
A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly. Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008).
The main disadvantages of the dimensional approach are the following:
In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by "subject areas" that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008).
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.
Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).
In "Information-Driven Business", Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.
Top-down versus bottom-up design methodologies.
Bottom-up design.
Ralph Kimball created an approach to data warehouse design known as "bottom-up". In the "bottom-up" approach, data marts are first created to provide reporting and analytical capabilities for specific business processes.
These data marts can eventually be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.
Top-down design.
Bill Inmon has defined a data warehouse as a centralized repository for the entire enterprise. The "top-down" approach is designed using a normalized enterprise data model. "Atomic" data, that is, data at the lowest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse. In the Inmon vision, the data warehouse is at the center of the "Corporate Information Factory" (CIF), which provides a logical framework for delivering business intelligence (BI) and business management capabilities. Gartner released a research note confirming Inmon's definition in 2005 with additional clarity. They also added one attribute.
Hybrid design.
Data warehouse (DW) solutions often resemble the hub and spokes architecture. Legacy systems feeding the DW/BI solution often include customer relationship management (CRM) and enterprise resource planning solutions (ERP), generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load (ETL) process, DW solutions often make use of an operational data store (ODS). The information from the ODS is then parsed into the actual DW. To reduce data redundancy, larger systems will often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW solution.
The DW database in a hybrid solution is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW effectively provides a single source of information from which the data marts can read, creating a highly flexible solution from a BI point of view. The hybrid architecture allows a DW to be replaced with a master data management solution where operational, not static information could reside.
The Data Vault Modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both 3rd normal form and star schema. The Data Vault model is not a true 3rd normal form, and breaks some of the rules that 3NF dictates be followed. It is however, a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.
Data warehouses versus operational systems.
Operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow the Codd rules of database normalization in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.
Data warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.
Evolution in organization use.
These terms refer to the level of sophistication of a data warehouse:

</doc>
<doc id="7991" url="http://en.wikipedia.org/wiki?curid=7991" title="Disperser">
Disperser

A disperser is a one-sided extractor. Where an extractor requires that every event gets the same probability under the uniform distribution and the extracted distribution, only the latter is required for a disperser. So for a disperser, an event formula_1 we have:
formula_2
Definition (Disperser): "A" formula_3"-disperser is a function"
formula_4
"such that for every distribution" formula_5 "on" formula_6 "with" formula_7 "the support of the distribution" formula_8 "is of size at least" formula_9.
Graph theory.
An ("N", "M", "D", "K", "e")-disperser is a bipartite graph with "N" vertices on the left side, each with degree "D", and "M" vertices on the right side, such that every subset of "K" vertices on the left side is connected to more than (1 − "e")"M" vertices on the right.
An extractor is a related type of graph that guarantees an even stronger property; every ("N", "M", "D", "K", "e")-extractor is also an ("N", "M", "D", "K", "e")-disperser.
Other meanings.
A disperser is a high-speed mixing device used to disperse or dissolve pigments and other solids into a liquid.

</doc>
<doc id="7992" url="http://en.wikipedia.org/wiki?curid=7992" title="Devonian">
Devonian

The Devonian is a geologic period and system of the Paleozoic Era spanning from the end of the Silurian Period, about Mya (million years ago), to the beginning of the Carboniferous Period, about . It is named after Devon, England, where rocks from this period were first studied.
The Devonian period experienced the first significant adaptive radiation of terrestrial life. Free-sporing vascular plants began to spread across dry land, forming extensive forests which covered the continents. By the middle of the Devonian, several groups of plants had evolved leaves and true roots, and by the end of the period the first seed-bearing plants appeared. Various terrestrial arthropods also became well-established. Fish reached substantial diversity during this time, leading the Devonian to often be dubbed the "Age of Fish". The first ray-finned and lobe-finned bony fish appeared, while the placoderms began dominating almost every known aquatic environment.
The ancestors of all tetrapods began adapting to walking on land, their strong pectoral and pelvic fins gradually evolved into legs. In the oceans, primitive sharks became more numerous than in the Silurian and the late Ordovician. The first ammonite mollusks appeared. Trilobites, the mollusk-like brachiopods and the great coral reefs, were still common. The Late Devonian extinction which started about 375 million years ago, severely affected marine life, killing off all placoderms, and all trilobites, save for a few species of the order Proetida.
The paleogeography was dominated by the supercontinent of Gondwana to the south, the continent of Siberia to the north, and the early formation of the small continent of Euramerica in between.
History.
The period is named after Devon, a county in southwestern England, where a controversial argument in the 1830s over the age and structure of the rocks found distributed throughout the county was eventually resolved by the defining of the Devonian period in the geological timescale. The Great Devonian Controversy is a classic case of how the foundations of our present-day geological knowledge and classification of the rock record and geological timescale was socially as well as scientifically constructed. After a long period of vigorous argument and counter-argument between the main protagonists of Roderick Murchison with Adam Sedgwick against Henry de la Beche supported by George Bellas Greenough, Murchison and Sedgwick won the debate and named the period they proposed as 'The Devonian System'.
While the rock beds that define the start and end of the Devonian period are well identified, the exact dates are uncertain. According to the International Commission on Stratigraphy (Ogg, 2004), the Devonian extends from the end of the Silurian Period Mya, to the beginning of the Carboniferous Period Mya (in North America, the beginning of the Mississippian subperiod of the Carboniferous.
In nineteenth-century texts the Devonian has been called the "Old Red Age", after the red and brown terrestrial deposits known in the United Kingdom as the Old Red Sandstone in which early fossil discoveries were found. Another common term is "Age of the Fishes", referring to the evolution of several major groups of fish that took place during the period. Older literature on the Anglo-Welsh basin divides it into the Downtonian, Dittonian, Breconian and Farlovian stages, the latter three of which are placed in the Devonian.
The Devonian has also erroneously been characterized as a "greenhouse age", due to sampling bias: most of the early Devonian-age discoveries came from the strata of western Europe and eastern North America, which at the time straddled the Equator as part of the supercontinent of Euramerica where fossil signatures of widespread reefs indicate tropical climates that were warm and moderately humid but in fact the climate in the Devonian differed greatly between epochs and geographic regions. For example, during the Early Devonian, arid conditions were prevalent through much of the world including Siberia, Australia, North America, and China, but Africa and South America had a warm temperate climate. In the Late Devonian, by contrast, arid conditions were less prevalent across the world and temperate climates were more common.
Subdivisions.
The Devonian Period is formally broken into Early, Middle and Late subdivisions. The rocks corresponding to these epochs are referred to as belonging to the Lower, Middle and Upper parts of the Devonian System.
The Early Devonian lasts from and begins with the Lochkovian stage, which lasts until the Pragian. This spans from , and is followed by the Emsian, which lasts until the Middle Devonian begins, .
The Middle Devonian comprises two subdivisions, the Eifelian giving way to the Givetian .
During this time the armoured jawless ostracoderm fish were declining in diversity; the jawed fish were thriving and increasing in diversity in both the oceans and freshwater. The shallow, warm, oxygen-depleted waters of Devonian inland lakes, surrounded by primitive plants, provided the environment necessary for certain early fish to develop essential characteristics such as well developed lungs, and the ability to crawl out of the water and onto the land for short periods of time.
Finally, the Late Devonian starts with the Frasnian, , during which the first forests were taking shape on land. The first tetrapods appear in the fossil record in the ensuing Famennian subdivision, the beginning and end of which are marked with extinction events. This lasted until the end of the Devonian, .
Climate.
The Devonian was a relatively warm period, and probably lacked any glaciers. The temperature gradient from the equator to the poles was not as large as it today. The weather was also very arid, mostly along the equator where it was the driest. Reconstruction of tropical sea surface temperature from conodont apatite implies an average value of in the Early Devonian. levels dropped steeply throughout the Devonian period as the burial of the newly evolved forests drew carbon out of the atmosphere into sediments; this may be reflected by a Mid-Devonian cooling of around . The Late Devonian warmed to levels equivalent to the Early Devonian; while there is no corresponding increase in concentrations, continental weathering increases (as predicted by warmer temperatures); further, a range of evidence, such as plant distribution, points to Late Devonian warming. The climate would have affected the dominant organisms in reefs; microbes would have been the main reef-forming organisms in warm periods, with corals and stromatoporoid sponges taking the dominant role in cooler times. The warming at the end of the Devonian may even have contributed to the extinction of the stromatoporoids.
Paleogeography.
The Devonian period was a time of great tectonic activity, as Euramerica and Gondwana drew closer together.
The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn, which is formed as much in Paleozoic times as nowadays by the convergence of two great air-masses, the Hadley cell and the Ferrel cell. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions.
Near the equator, the plate of Euramerica and Gondwana were starting to meet, beginning the early stages of assembling Pangaea. This activity further raised the northern Appalachian Mountains and formed the Caledonian Mountains in Great Britain and Scandinavia.
The west coast of Devonian North America, by contrast, was a passive margin with deep silty embayments, river deltas and estuaries, in today's Idaho and Nevada; an approaching volcanic island arc reached the steep slope of the continental shelf in Late Devonian times and began to uplift deep water deposits, a collision that was the prelude to the mountain-building episode of Mississippian times called the Antler orogeny.
Sea levels were high worldwide, and much of the land lay under shallow seas, where tropical reef organisms lived. The deep, enormous Panthalassa (the "universal ocean") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean, and Ural Ocean (which was closed during the collision with Siberia and Baltica).
Biota.
Marine biota.
Sea levels in the Devonian were generally high. Marine faunas continued to be dominated by bryozoa, diverse and abundant brachiopods, the enigmatic hederelloids, microconchids and corals. Lily-like crinoids (animals, their resemblance to flowers notwithstanding) were abundant, and trilobites were still fairly common. Among vertebrates, jaw-less armored fish (ostracoderms) declined in diversity, while the jawed fish (gnathostomes) simultaneously increased in both the sea and fresh water. Armored placoderms were numerous during the lower stages of the Devonian Period and became extinct in the Late Devonian, perhaps because of competition for food against the other fish species. Early cartilaginous (Chondrichthyes) and bony fishes (Osteichthyes) also become diverse and played a large role within the Devonian seas. The first abundant genus of shark, "Cladoselache", appeared in the oceans during the Devonian Period. The great diversity of fish around at the time, have led to the Devonian being given the name "The Age of Fish" in popular culture.
The first ammonites also appeared during or slightly before the early Devonian Period around 400 Mya.
Reefs.
A now dry barrier reef, located in present day Kimberley Basin of northwest Australia, once extended a thousand kilometers, fringing a Devonian continent. Reefs in general are built by various carbonate-secreting organisms that have the ability to erect wave-resistant frameworks close to sea level. The main contributors of the Devonian reefs were unlike modern reefs, which are constructed mainly by corals and calcareous algae. They were composed of calcareous algae and coral-like stromatoporoids, and tabulate and rugose corals, in that order of importance.
Terrestrial biota.
By the Devonian Period, life was well underway in its colonization of the land. The moss forests and bacterial and algal mats of the Silurian were joined early in the period by primitive rooted plants that created the first stable soils and harbored arthropods like mites, scorpions and myriapods (although arthropods appeared on land much earlier than in the Early Devonian and the existence of fossils such as "Climactichnites" suggest that land arthropods may have appeared as early as the Cambrian period). Also the first possible fossils of insects appeared around 416 Mya in the Early Devonian. The first tetrapods, evolving from lobe-finned fish, appeared in the coastal water no later than middle Devonian, and gave rise to the first Amphibians.
The greening of land.
Early Devonian plants did not have roots or leaves like the plants most common today and many had no vascular tissue at all. They probably spread largely by vegetative growth, and did not grow much more than a few centimeters tall. By far the largest land organism was "Prototaxites", the fruiting body of an enormous fungus that stood more than 8 meters tall, towering over the low, carpet-like vegetation. By the Middle Devonian, shrub-like forests of primitive plants existed: lycophytes, horsetails, ferns, and progymnosperms had evolved. Most of these plants had true roots and leaves, and many were quite tall. The earliest known trees, from the genus "Wattieza", appeared in the Late Devonian around 385 Ma. In the Late Devonian, the tree-like ancestral Progymnosperm "Archaeopteris" which had conifer-like true wood and fern-like foliage and the cladoxylopsids grew. (See also: lignin.) These are the oldest known trees of the world's first forests. By the end of the Devonian, the first seed-forming plants had appeared. This rapid appearance of so many plant groups and growth forms has been called the "Devonian Explosion".
The 'greening' of the continents acted as a carbon dioxide sink, and atmospheric levels of this greenhouse gas may have dropped. This may have cooled the climate and led to a massive extinction event. See Late Devonian extinction.
Animals and the first soils.
Primitive arthropods co-evolved with this diversified terrestrial vegetation structure. The evolving co-dependence of insects and seed-plants that characterizes a recognizably modern world had its genesis in the Late Devonian period. The development of soils and plant root systems probably led to changes in the speed and pattern of erosion and sediment deposition. The rapid evolution of a terrestrial ecosystem containing copious animals opened the way for the first vertebrates to seek out a terrestrial living. By the end of the Devonian, arthropods were solidly established on the land.
Late Devonian extinction.
A major extinction occurred at the beginning of the last phase of the Devonian period, the Famennian faunal stage, (the Frasnian-Famennian boundary), about Mya, when all the fossil agnathan fishes, save for the psammosteid heterostracans, suddenly disappeared. A second strong pulse closed the Devonian period. The Late Devonian extinction was one of five major extinction events in the history of the Earth's biota, more drastic than the familiar extinction event that closed the Cretaceous.
The Devonian extinction crisis primarily affected the marine community, and selectively affected shallow warm-water organisms rather than cool-water organisms. The most important group to be affected by this extinction event were the reef-builders of the great Devonian reef-systems.
Amongst the severely affected marine groups were the brachiopods, trilobites, ammonites, conodonts, and acritarchs, as well as jawless fish, and all placoderms. Land plants as well as freshwater species, such as our tetrapod ancestors, were relatively unaffected by the Late Devonian extinction event.
The reasons for the Late Devonian extinctions are still unknown, and all explanations remain speculative. Canadian paleontologist Digby McLaren suggested in 1969 that the Devonian extinction events were caused by an asteroid impact. However, while there were Late Devonian collision events (see the Alamo bolide impact), little evidence supports the existence of a large enough Devonian crater.

</doc>
<doc id="7993" url="http://en.wikipedia.org/wiki?curid=7993" title="Dungeon Master (disambiguation)">
Dungeon Master (disambiguation)

Dungeon Master may mean:

</doc>
<doc id="7994" url="http://en.wikipedia.org/wiki?curid=7994" title="David Thompson (explorer)">
David Thompson (explorer)

David Thompson (April 30, 1770 – February 10, 1857) was a British-Canadian fur trader, surveyor, and map-maker, known to some native peoples as "Koo-Koo-Sint" or "the Stargazer." Over his career he mapped over 3.9 million square kilometers of North America and for this has been described as the "greatest land geographer who ever lived."
Biography.
Early life.
Thompson was born in Westminster to recent Welsh migrants, David and Ann Thompson. When Thompson was two, his father died and the financial hardship of this occurrence resulted in his and his brother's placement in the Grey Coat Hospital, a school for the disadvantaged of Westminster. He eventually graduated to the Grey Coat mathematical school and was introduced to basic navigation skills which would form the basis of his future career. In 1784, at the age of 14, he entered a seven-year apprenticeship with the Hudson's Bay Company. He set sail on May 28 of that year, and left England .
The Hudson's Bay Company (HBC).
He arrived in Churchill (now in Manitoba) and was put to work copying the personal papers of the governor of Fort Churchill, Samuel Hearne. The next year he was transferred to nearby York Factory, and over the next few years spent time as a clerk at Cumberland House, Saskatchewan and South Branch House before arriving at Manchester House in 1787. On December 23, 1788, Thompson seriously fractured his leg, forcing him to spend the next two winters at Cumberland House convalescing. It was during this time he greatly refined and expanded his mathematical, astronomical and surveying skills under the tutelage of Hudson's Bay Company surveyor Philip Turnor. It was also during this time that he lost sight in his right eye.
In 1790 with his apprenticeship nearing its end, Thompson made the unusual request of a set of surveying tools in place of the typical parting gift of fine clothes offered by the company to those completing their indenture. He received both. He then entered the employ of the Hudson's Bay Company as a fur trader and in 1792 completed his first significant survey, mapping a route to Lake Athabasca (presently straddling the Alberta/Saskatchewan border). In recognition of his map-making skills, the company promoted him to surveyor in 1794. Thompson continued working for the Hudson's Bay Company until May 23, 1797 when, frustrated with the Hudson's Bay Company's policies, he left and walked 80 miles in the snow to enter the employ of the competition, the North West Company where he continued to work as a fur trader and surveyor.
North West Company.
Thompson's decision to defect to the North West Company in 1797 without providing the customary one-year notice was not well received by his former employers. However, joining the North West Company allowed Thompson to pursue his interest in surveying and work on mapping the interior of what was to become Canada. In 1797, Thompson was sent south by his employers to survey part of the Canada-U.S. boundary along the water routes from Lake Superior to Lake of the Woods to satisfy unresolved questions of territory arising from the Jay Treaty between Great Britain and the United States. By 1798 Thompson had completed a survey of from Grand Portage, through Lake Winnipeg, to the headwaters of the Assiniboine and Mississippi Rivers, as well as two sides of Lake Superior. In 1798, the company sent him to Red Deer Lake (in present-day Alberta) to establish a trading post. The English translation of Lac La Biche-Red Deer Lake-first appeared on the Mackenzie map of 1793. Thompson spent the next few seasons trading based in Fort George (now in Alberta), and during this time led several expeditions into the Rocky Mountains.
In 1804, at the annual meeting of the North West Company in Kaministiquia, Thompson was made a full partner of the company and spent the next few seasons based there managing the fur trading operations but still finding time to expand his surveys of the waterways around Lake Superior. However, a decision was made at the 1806 company meeting to send Thompson back out into the interior. Concern over the American-backed expedition of Lewis and Clark prompted the North West Company to charge Thompson with the task of finding a route to the Pacific in order to open up the lucrative trading territories of the Pacific Northwest.
Columbia travels.
After the general meeting in 1806, Thompson travelled to Rocky Mountain House and prepared for an expedition to follow the Columbia River to the Pacific. In June 1807 Thompson crossed the Rocky Mountains and spent the summer surveying the Columbia basin and continuing to survey the area over the next few seasons. Thompson mapped and established trading posts in Northwestern Montana, Idaho, Washington, and Western Canada. Trading posts he founded included Kootenae House, Kullyspell House and Saleesh House; the latter two of which were the first trading posts west of the Rockies in Idaho and Montana, respectively. These posts established by Thompson extended North West Company fur trading territory into the Columbia Basin drainage area. The maps he made of the Columbia River basin east of the Cascade Mountains were of such high quality and detail that they continued to be regarded as authoritative well into the mid-20th century. 
In early 1810, Thompson was returning eastward towards Montreal but while on route at Rainy Lake, received orders to return to the Rocky Mountains and establish a route to the mouth of the Columbia. This was a response by the North West Company to the plans of John Jacob Astor to send a ship around the Americas to establish a fur trading post. During his return, Thompson was delayed by an angry group of Peigan natives at Howse Pass which ultimately forced him to seek a new route across the Rocky Mountains through the Athabasca Pass.
David Thompson was the first European to navigate the full length of the Columbia River. During Thompson's 1811 voyage down the Columbia River he camped at the junction with the Snake River on July 9, 1811, and erected a pole and a notice claiming the country for Great Britain and stating the intention of the North West Company to build a trading post at the site. This notice was found later that year by Astorians looking to establish an inland fur post, contributing to their selection of a more northerly site at Fort Okanogan. The North West Company's Fort Nez Percés was established near the Snake River junction several years later. Continuing down the Columbia, Thompson passed the barrier of The Dalles with much less difficulty than experienced by Lewis and Clark, as high water obscured Celilo Falls and many of the rapids. On July 14, 1811, Thompson reached the partially constructed Fort Astoria at the mouth of the Columbia, arriving two months after the Pacific Fur Company's ship, the "Tonquin".
Before returning upriver and across the mountains, Thompson hired Naukane, a Native Hawaiian laborer brought to Fort Astoria by the Pacific Fur Company's ship "Tonquin". Naukane, known as Coxe to Thompson, accompanied Thompson across the continent to Lake Superior before journeying on to England.
Thompson wintered at Saleesh House before beginning his final journey back to Montreal in 1812.
In his published journals, Thompson recorded seeing large footprints near what is now Jasper, Alberta, in 1811. It has been suggested that these prints were similar to what has since been called the sasquatch. However, Thompson noted that these tracks showed "a small Nail at the end of each [toe]", and stated that these tracks "very much resembles a large Bear's Track".
Appearance and personality.
In 1820, the English geologist, John Jeremiah Bigsby, attended a dinner party given by The Hon. William McGillivray at his home, Chateau St. Antoine, one of the early estates in Montreal's Golden Square Mile. He describes the party and some of the guests in his entertaining book "The Shoe and Canoe", giving an excellent description of David Thompson:
"I was well placed at table between one of the Miss McGillivray's and a singular-looking person of about fifty. He was plainly dressed, quiet, and observant. His figure was short and compact, and his black hair was worn long all round, and cut square, as if by one stroke of the shears, just above the eyebrows. His complexion was of the gardener's ruddy brown, while the expression of his deeply-furrowed features was friendly and intelligent, but his cut-short nose gave him an odd look. His speech betrayed the Welshman, although he left his native hills when very young. I might have been spared this description of Mr David Thompson by saying he greatly resembled Curran the Irish Orator..."
"I afterwards travelled much with him, and have now only to speak of him with great respect, or, I ought to say, with admiration... No living person possesses a tithe of his information respecting the Hudson's Bay countries... Never mind his Bunyan-like face and cropped hair; he has a very powerful mind, and a singular faculty of picture-making. He can create a wilderness and people it with warring savages, or climb the Rocky Mountains with you in a snow-storm, so clearly and palpably, that only shut your eyes and you hear the crack of the rifle, or feel the snow-flakes melt on your cheeks as he talks."
Marriage and children.
On June 10, 1799 at Île-à-la-Crosse, he married Charlotte Small, a mixed-blood child of a Scottish fur trader Patrick Small and a Cree mother. Their marriage was formalized at the Scotch Presbyterian Church in Montreal on October 30, 1812. He and Charlotte had 13 children together; five of them were born before he left the fur trade. The family did not adjust easily to life in Eastern Canada and two of the children, John (aged 5) and Emma (aged 7) died of round worms, a common parasite. Their marriage lasted 58 years, the longest Canadian pre-Confederation marriage known.
Later years.
Upon his arrival back in Montreal, Thompson retired with a generous pension from the North West Company. He settled in nearby Terrebonne and worked on completing his great map, a summary of his lifetime of exploring and surveying the interior of North America. The map covered the wide area stretching from Lake Superior to the Pacific, and was given by Thompson to the North West Company. Thompson's 1814 map, his greatest achievement, was so accurate that 100 years later it was still the basis for many of the maps issued by the Canadian government. It now resides in the Archives of Ontario.
In 1815, Thompson moved his family to Williamstown, Upper Canada and a few years later was employed to survey the newly established borders with the United States from Lake of the Woods to the Eastern Townships of Quebec, established by Treaty of Ghent after the War of 1812. In 1843 Thompson completed his atlas of the region from Hudson Bay to the Pacific Ocean.
Afterwards, Thompson returned to a life as a land owner, but soon financial misfortune would ruin him. By 1831 he was so deeply in debt he was forced to take up a position as a surveyor for the British American Land Company to provide for his family. His luck continued to worsen and he was forced to move in with his daughter and son-in-law in 1845. He began work on a manuscript chronicling his life exploring the continent, but this project was left unfinished when his sight failed him completely in 1851.
Death and afterward.
The land mass mapped by Thompson amounted to 3.9 million square kilometres of wilderness (one-fifth of the continent). His contemporary, the great explorer Alexander Mackenzie, remarked that Thompson did more in ten months than he would have thought possible in two years.
Despite these significant achievements, Thompson died in Montreal in near obscurity on February 10, 1857, his accomplishments almost unrecognized. He never finished the book of his 28 years in the fur trade, based on his 77 field notebooks, before he died. In the 1890s geologist J.B. Tyrrell resurrected Thompson's notes and in 1916 published them as "David Thompson's Narrative".
Thompson's body was interred in Montreal's Mount Royal Cemetery in an unmarked grave. It was not until 1926 that efforts by J.B. Tyrell and the Canadian Historical Society resulted in the placing of a tombstone to mark his grave.
In 1957, one hundred years after his death, the Canadian government honoured him with his image on a Canadian postage stamp. The David Thompson Highway in Alberta was named in his honour, along with David Thompson High School situated on the side of the highway near Leslieville, Alberta. His prowess as a geographer is now well-recognized. He has been called "the greatest land geographer who ever lived."
There is a monument dedicated to David Thompson (maintained by the state of North Dakota) near the former town site of the ghost town, Verendrye, North Dakota, located approximately two miles north and one mile west of Karlsruhe, North Dakota. Thompson Falls, Montana and British Columbia's Thompson River are also named after the explorer.
The year 2007 marked the 150th year of Thompson's death and the 200th anniversary of his first crossing of the Rocky Mountains. Commemorative events and exhibits were planned across Canada and the United States from 2007 to 2011 as a celebration of his accomplishments.
Thompson was the subject of a 1964 National Film Board of Canada short film "David Thompson: The Great Mapmaker ", as well as the BBC2 programme "Ray Mears' Northern Wilderness" (Episode 5), broadcast in November 2009.

</doc>
<doc id="7995" url="http://en.wikipedia.org/wiki?curid=7995" title="Dioscoreales">
Dioscoreales

Dioscoreales is a botanical name for an order of flowering plants. Of necessity it contains the family Dioscoreaceae which includes the yam that is used as an important food source in many regions around the globe.
Taxonomy.
In APG II system, of 2003, this order was placed in the clade monocots and comprised the families Burmanniaceae, Dioscoreaceae and Nartheciaceae.
Under the APG system of 1998, the order was placed in the clade monocots and comprised the families Burmanniaceae, Dioscoreaceae, Taccaceae, Thismiaceae and Trichopodaceae
In APG II, the family Thismiaceae has been included in family Burmanniaceae and the families Taccaceae and Trichopodaceae have been included in family Dioscoreaceae. Therefore the change is not as large as might appear, and the only plants to have moved across the borders of the order are those belonging to family Nartheciaceae.
The Cronquist system, of 1981, did not recognise such an order, but placed most such plants in order Liliales in subclass Liliidae in class Liliopsida (monocotyledons) of division Magnoliophyta (angiosperms).
Under the Dahlgren system, Dioscoreales was placed in the superorder Lilianae in subclass Liliidae (monocotyledons) of class Magnoliopsida (angiosperms) and comprised the families Dioscoreaceae, Petermanniaceae, Ripogonaceae, Smilacaceae, Stemonaceae, Taccaceae, Trichopodaceae and Trilliaceae.
Ecology and Evolution.
The three families included in order Dioscoreales also represent three different ecological groups of plants. Dioscoreaceae contains mainly vines ("Dioscorea") and other crawling species ("Epipetrum"). Nartheciaceae on the other hand is a family composed of herbeceous plants with a rather lily-like appearance ("Aletris") while Burmanniaceae is entirely myco-heterotrophic group.
The data for the evolution of the order is collected from molecular analyses since there are no such fossils found. It is estimated that Dioscoreales and its sister clade Pandanales split up around 121 millions of years ago during Early Cretaceous when the stem group was formed. Then it took 3 to 6 millions of years for the crown group to differentiate in Mid Cretaceous.
Description and Distribution.
Species from this order are distributed across all of the continents except Antarctica. They are mainly tropical representatives but however there are members of Dioscoreaceae and Nartheciaceae families found in cooler regions of Europe and North America. Order Dioscoreales contains plants that are able to form an underground organ for reservation of nutritions as many other monocots. An exception is the family Burmanniaceae which is entirely myco-heterotrophic and contains species that lack photosynthetic abilities.
All of the species except the genera placed in Nartheciaceae express simultaneous microsporogenesis. Plants in Nartheciaceae show successive microsporogenesis which is one of the traits indicating that the family is sister to all the other members included in the order.

</doc>
<doc id="8000" url="http://en.wikipedia.org/wiki?curid=8000" title="Default">
Default

Default may refer to:

</doc>
<doc id="8002" url="http://en.wikipedia.org/wiki?curid=8002" title="Deposition">
Deposition

Deposition may refer to:
Deposition, in science, may refer to:
The Deposition can also refer to depictions of Christ's descent from the cross, as in:
The Deposition can also refer to:

</doc>
<doc id="8005" url="http://en.wikipedia.org/wiki?curid=8005" title="Dentistry">
Dentistry

Dentistry is the branch of medicine that is involved in the study, diagnosis, prevention, and treatment of diseases, disorders and conditions of the oral cavity, commonly in the dentition but also the oral mucosa, and of adjacent and related structures and tissues, particularly in the maxillofacial (jaw and facial) area. Although primarily associated with teeth among the general public, the field of dentistry or dental medicine is thus not limited to odontology (from Ancient Greek ὀδούς (odoús, “tooth”) - the study of the structure, development, and abnormalities of the teeth. Because of their substantial overlap in concept, dentistry is often also understood to subsume the now largely defunct medical specialty of stomatology (the study of the mouth and its disorders and diseases) for which reason the two terms are used interchangeably in certain regions. 
Dentistry is widely considered important for overall health. Dental treatment is carried out by the dental team, which often consists of a dentist and dental auxiliaries (dental assistants, dental hygienists, dental technicians, and dental therapists). Most dentists work in private practices (primary care), although some work in hospitals (secondary care) and institutions (prisons, armed forces bases, etc.).
The history of dentistry is almost as ancient as the history of humanity and civilization with the earliest evidence dating from 7000BC. Prehistoric dental surgical techniques are seen in Ancient Egypt, where a mandible dated to approximately 2650 BCE shows two perforations just below the root of the first molar, indicating the draining of an abscessed tooth. Remains from the early Harappan periods of the Indus Valley Civilization (c. 3300 BCE) show evidence of teeth having been drilled dating back 9,000 years. It is thought that dental surgery was the first specialization from medicine.
Dental treatment.
Dentistry usually encompasses very important practices related to the oral cavity. Oral diseases are major public health problems due to their high incidence and prevalence across the globe with the disadvantaged affected more than other socio-economic groups.
The majority of dental treatments are carried out to prevent or treat the two most common oral diseases which are dental caries (tooth decay) and periodontal disease (gum disease or pyorrhea). Common treatments involve the restoration of teeth, extraction or surgical removal of teeth, scaling and root planing and endodontic root canal treatment.
All dentists in the United States undergo at least three years of undergraduate studies, but nearly all complete a bachelors degree. This schooling is followed by four years of dental school to qualify as a "Doctor of Dental Surgery" (DDS) or "Doctor of Dental Medicine" (DMD). Dentists need to complete additional qualifications or continuing education to carry out more complex treatments such as sedation, oral and maxillofacial surgery, and dental implants.
By nature of their general training they can carry out the majority of dental treatments such as restorative (fillings, crowns, bridges), prosthetic (dentures), endodontic (root canal) therapy, periodontal (gum) therapy, and extraction of teeth, as well as performing examinations, radiographs (x-rays) and diagnosis. Dentists can also prescribe medications such as antibiotics, sedatives, and any other drugs used in patient management.
Dentists also encourage prevention of oral diseases through proper hygiene and regular, twice yearly, checkups for professional cleaning and evaluation. Conditions in the oral cavity may be indicative of systemic diseases such as osteoporosis, diabetes, or cancer. Many studies have also shown that gum disease is associated with an increased risk of diabetes, heart disease, and preterm birth.
Education and licensing.
Dr. John M. Harris started the world's first dental school in Bainbridge, Ohio, and helped to establish dentistry as a health profession. It opened on 21 February 1828, and today is a dental museum. The first dental college, Baltimore College of Dental Surgery, opened in Baltimore, Maryland, USA in 1840. Philadelphia Dental College was founded in 1863 and is the second in the United States. In 1907 Temple University accepted a bid to incorporate the school.
Studies showed that dentists graduated from different countries, or even from different dental schools in one country, may have different clinical decisions for the same clinical condition. For example, dentists graduated from Israeli dental schools may recommend more often for the removal of asymptomatic impacted third molar (wisdom teeth) than dentists graduated from Latin American or Eastern European dental schools.
In the United Kingdom, the 1878 British Dentists Act and 1879 Dentists Register limited the title of "dentist" and "dental surgeon" to qualified and registered practitioners. However, others could legally describe themselves as "dental experts" or "dental consultants". The practice of dentistry in the United Kingdom became fully regulated with the 1921 Dentists Act, which required the registration of anyone practicing dentistry. The British Dental Association, formed in 1880 with Sir John Tomes as president, played a major role in prosecuting dentists practising illegally.
In Korea, Taiwan, Japan, Finland, Sweden, Brazil, Chile, the United States, and Canada, a dentist is a healthcare professional qualified to practice dentistry after graduating with a degree of either Doctor of Dental Surgery (DDS) or Doctor of Dental Medicine (DMD). This is equivalent to the Bachelor of Dental Surgery/Baccalaureus Dentalis Chirurgiae (BDS, BDent, BChD, BDSc) that is awarded in the UK and British Commonwealth countries. In most western countries, to become a qualified dentist one must usually complete at least four years of postgraduate study; within the European Union the education has to be at least five years. Dentists usually complete between five and eight years of post-secondary education before practising. Though not mandatory, many dentists choose to complete an internship or residency focusing on specific aspects of dental care after they have received their dental degree.
Specialties.
Some dentists undertake further training after their initial degree in order to specialize. Exactly which subjects are recognized by dental registration bodies varies according to location. Examples include:
History.
The Indus Valley Civilization (IVC) has yielded evidence of dentistry being practised as far back as 7000 BC. An IVC site in Mehrgarh indicates that this earliest form of dentistry involved curing tooth related disorders with bow drills operated, perhaps, by skilled bead craftsmen. The reconstruction of this ancient form of dentistry showed that the methods used were reliable and effective. The earliest dental filling, made of beeswax, was discovered in Slovenia and dates from 6500 years ago.
A Sumerian text from 5000 BC describes a "tooth worm" as the cause of dental caries. Evidence of this belief has also been found in ancient India, Egypt, Japan, and China. The legend of the worm is also found in the writings of Homer, and as late as the 14th century AD the surgeon Guy de Chauliac still promoted the belief that worms cause tooth decay.
The Edwin Smith Papyrus, written in the 17th century BC but which may reflect previous manuscripts from as early as 3000 BC, includes the treatment of several dental ailments. In the 18th century BC, the Code of Hammurabi referenced dental extraction twice as it related to punishment. Examination of the remains of some ancient Egyptians and Greco-Romans reveals early attempts at dental prosthetics and surgery.
Ancient Greek scholars Hippocrates and Aristotle wrote about dentistry, including the eruption pattern of teeth, treating decayed teeth and gum disease, extracting teeth with forceps, and using wires to stabilize loose teeth and fractured jaws. Some say the first use of dental appliances or bridges comes from the Etruscans from as early as 700 BC. In ancient Egypt, Hesi-Re is the first named "dentist" (greatest of the teeth). The Egyptians bound replacement teeth together with gold wire. Roman medical writer Cornelius Celsus wrote extensively of oral diseases as well as dental treatments such as narcotic-containing emollients and astringents. The earliest dental amalgams were first documented in a Tang Dynasty medical text written by the Chinese physician Su Kung in 659, and appeared in Germany in 1528.
Historically, dental extractions have been used to treat a variety of illnesses. During the Middle Ages and throughout the 19th century, dentistry was not a profession in itself, and often dental procedures were performed by barbers or general physicians. Barbers usually limited their practice to extracting teeth which alleviated pain and associated chronic tooth infection. Instruments used for dental extractions date back several centuries. In the 14th century, Guy de Chauliac invented the dental pelican (resembling a pelican's beak) which was used to perform dental extractions up until the late 18th century. The pelican was replaced by the dental key which, in turn, was replaced by modern forceps in the 20th century.
The first book focused solely on dentistry was the "Artzney Buchlein" in 1530, and the first dental textbook written in English was called "Operator for the Teeth" by Charles Allen in 1685.
Modern dentistry.
It was between 1650 and 1800 that the science of modern dentistry developed. The English physician Thomas Browne in his "A Letter to a Friend" (pub. post. 1690) made an early dental observation with characteristic humour –
The French surgeon Pierre Fauchard became known as the "father of modern dentistry". Despite the limitations of the primitive surgical instruments during the late 17th and early 18th century, Fauchard was a highly skilled surgeon who made remarkable improvisations of dental instruments, often adapting tools from watch makers, jewelers and even barbers, that he thought could be used in dentistry. He introduced dental fillings as treatment for dental cavities. He asserted that sugar derivate acids like tartaric acid were responsible for dental decay, and also suggested that tumors surrounding the teeth, in the gums, could appear in the later stages of tooth decay.
Fauchard was the pioneer of dental prosthesis, and he discovered many methods to replace lost teeth. He suggested that substitutes could be made from carved blocks of ivory or bone. He also introduced dental braces, although they were initially made of gold, he discovered that the teeth position could be corrected as the teeth would follow the pattern of the wires. Waxed linen or silk threads were usually employed to fasten the braces. His contributions to the world of dental science consist primarily of his 1728 publication Le chirurgien dentiste or The Surgeon Dentist. The French text included “basic oral anatomy and function, dental construction, and various operative and restorative techniques, and effectively separated dentistry from the wider category of surgery”.
After Fauchard, the study of dentistry rapidly expanded. Two important books, "Natural History of Human Teeth" (1771 ) and "Practical Treatise on the Diseases of the Teeth" (1778), were published by British surgeon John Hunter. In 1763 he entered into a period of collaboration with the London-based dentist James Spence. He began to theorise about the possibility of tooth transplants from one person to another. He realised that the chances of an (initially, at least) successful tooth transplant would be improved if the donor tooth was as fresh as possible and was matched for size with the recipient. These principles are still used in the transplantation of internal organs. Hunter conducted a series of pioneering operations, in which he attempted a tooth transplant. Although the donated teeth never properly bonded with the recipients' gums, one of Hunter's patients stated that he had three which lasted for six years, a remarkable achievement for the period. The profession came under government regulation by the end of the 19th century.
Priority patients.
UK NHS priority patients include patients with congenital abnormalities (such as cleft palates and hypodontia), patients who have suffered orofacial trauma and those being treated for cancer in the head and neck region. These are treated in a multidisciplinary team approach with other hospital based dental specialities orthodontics and maxillofacial surgery. Other priority patients include those with infections (either third molars or necrotic teeth) or avulsed permanent teeth, as well as patients with a history of smoking or smokeless tobacco with ulcers in the oral cavity also.

</doc>
<doc id="8007" url="http://en.wikipedia.org/wiki?curid=8007" title="Diameter">
Diameter

In geometry, the diameter of a circle is any straight line segment that passes through the center of the circle and whose endpoints lie on the circle. It can also be defined as the longest chord of the circle. Both definitions are also valid for the diameter of a sphere. The word "diameter" is derived from Greek "διάμετρος" ("diametros"), "diameter of a circle", from "δια-" ("dia-"), "across, through" + "μέτρον" ("metron"), "measure". It is often abbreviated DIA, dia, d, or ⌀.
In more modern usage, the length of a diameter is also called the diameter. In this sense one speaks of "the" diameter rather than "a" diameter (which refers to the line itself), because all diameters of a circle or sphere have the same length, this being twice the radius r.
For a convex shape in the plane, the diameter is defined to be the largest distance that can be formed between two opposite parallel lines tangent to its boundary, and the "width" is defined to be the smallest such distance. Both quantities can be calculated efficiently using rotating calipers. For a curve of constant width such as the Reuleaux triangle, the width and diameter are the same because all such pairs of parallel tangent lines have the same distance.
Generalizations.
The definitions given above are only valid for circles, spheres and convex shapes. However, they are special cases of a more general definition that is valid for any kind of "n"-dimensional convex or non-convex object, such as a hypercube or a set of scattered points. The diameter of a subset of a metric space is the least upper bound of the set of all distances between pairs of points in the subset. So, if "A" is the subset, the diameter is
If the distance function d is viewed here as having codomain R (the set of all real numbers), this implies that the diameter of the empty set (the case ) equals −∞ (negative infinity). Some authors prefer to treat the empty set as a special case, assigning it a diameter equal to 0, which corresponds to taking the codomain of d to be the set of nonnegative reals.
For any solid object or set of scattered points in n-dimensional Euclidean space, the diameter of the object or set is the same as the diameter of its convex hull. 
In differential geometry, the diameter is an important global Riemannian invariant. 
In plane geometry, a diameter of a conic section is typically defined as any chord which passes through the conic's centre; such diameters are not necessarily of uniform length, except in the case of the circle, which has eccentricity "e" = 0.
In medical parlance the diameter of a lesion is the longest line segment whose endpoints are within the lesion.
Diameter symbol.
The symbol or variable for diameter, , is similar in size and design to ø, the Latin small letter o with stroke. Unicode provides character number 8960 (hexadecimal 2300) for the symbol, which can be encoded in HTML webpages as &#8960; or &#x2300;. The character can be obtained in Microsoft Windows by holding the key down while entering on the numeric keypad. On an Apple Macintosh, the diameter symbol can be entered via the character palette (this is opened by pressing in most applications), where it can be found in the Technical Symbols category.
The character will sometimes not display correctly, however, since many fonts do not include it. In most situations the letter ø is acceptable, which is unicode 0248 (hexadecimal 00F8). It can be obtained in UNIX-like operating systems using a Compose key by pressing, in sequence, and on a Macintosh by pressing (in both cases, that is the letter o, not the number 0).
In LaTeX the symbol is achieved with the command \diameter which is part of the wasysym package.
The diameter symbol is distinct from the empty set symbol , from an (italic) uppercase phi "Φ", and from the Nordic vowel Ø.

</doc>
<doc id="8008" url="http://en.wikipedia.org/wiki?curid=8008" title="Direct examination">
Direct examination

The Direct Examination or Examination-in-Chief is one stage in the process of adducing evidence from witnesses in a court of law. Direct examination is the questioning of a witness by the party who called him or her, in a trial. Direct examination is usually performed to elicit evidence in support of facts which will satisfy a required element of a party's claim or defense.
In direct examination, one is generally prohibited from asking leading questions. This prevents a lawyer from feeding answers to a favorable witness. An exception to this rule occurs if one side has called a witness, but it is either understood, or soon becomes plain, that the witness is hostile to the questioner's side of the controversy. The lawyer may then ask the court to declare the person he or she has called to the stand a hostile witness. If the court does so, the lawyer may thereafter ply the witness with leading questions during direct examination. 
The techniques of direct examination are taught in courses on Trial Advocacy. Each direct examination is integrated with the overall case strategy through either a theme and theory or, with more advanced strategies, a line of effort.

</doc>
<doc id="8011" url="http://en.wikipedia.org/wiki?curid=8011" title="Alcohol intoxication">
Alcohol intoxication

Alcohol intoxication (also known as drunkenness or inebriation) is a physiological state (that may also include psychological alterations of consciousness) induced by the ingestion of ethyl alcohol (ethanol).
Alcohol intoxication is the consequence of alcohol entering the bloodstream faster than it can be metabolized by the liver, which breaks down the ethanol into non-intoxicating byproducts. Some effects of alcohol intoxication (such as euphoria and lowered social inhibitions) are central to alcohol's desirability as a beverage and its history as one of the world's most widespread recreational drugs. Despite this widespread use and alcohol's legality in most countries, many medical sources tend to describe any level of alcohol intoxication as a form of poisoning due to ethanol's damaging effects on the body in large doses; and some religions consider alcohol intoxication to be a sin.
Symptoms of alcohol intoxication include euphoria, flushed skin and decreased social inhibition at lower doses, with larger doses producing progressively severe impairments of balance, muscle coordination (ataxia), and decision-making ability (potentially leading to violent or erratic behavior) as well as nausea or vomiting from alcohol's disruptive effect on the semicircular canals of the inner ear and chemical irritation of the gastric mucosa. Sufficiently high levels of blood-borne alcohol will cause coma and death from the depressive effects of alcohol upon the central nervous system.
"Acute alcohol poisoning" is a related medical term used to indicate a dangerously high concentration of alcohol in the blood, high enough to induce coma, respiratory depression, or even death. It is considered a medical emergency. The term is mostly used by healthcare providers. Toxicologists use the term "alcohol intoxication" to discriminate between alcohol and other toxins.
Pathophysiology.
Alcohol is metabolized by a normal liver at the rate of about of spirits (roughly a typical drink-size serving of beer, wine, or spirits) every 90 minutes. An "abnormal" liver with conditions such as hepatitis, cirrhosis, gall bladder disease, and cancer are likely to result in a slower rate of metabolism.
Ethanol is metabolised to acetaldehyde by alcohol dehydrogenase (ADH), which is found in many tissues, including the gastric mucosa. Acetaldehyde is metabolised to acetate by acetaldehyde dehydrogenase (ALDH), which is found predominantly in liver mitochondria. Acetate is used by the muscle cells to produce acetyl-CoA using the enzyme acetyl-CoA synthetase, and the acetyl-CoA is then used in the citric acid cycle. It takes roughly 90 minutes for a healthy liver to metabolize a single ounce, approximately one hour per standard unit.
Ethanol's acute effects are due largely to its nature as a central nervous system depressant, and are dependent on blood alcohol concentrations:
As drinking increases, people become sleepy, or fall into a stupor. After a very high level of consumption, the respiratory system becomes depressed and the person will stop breathing. Comatose patients may aspirate their vomit (resulting in vomitus in the lungs, which may cause "drowning" and later pneumonia if survived). CNS depression and impaired motor co-ordination along with poor judgement increases the likelihood of accidental injury occurring. It is estimated that about one third of alcohol-related deaths are due to accidents (32%), and another 14% are from intentional injury.
In addition to respiratory failure and accidents caused by effects on the central nervous system, alcohol causes significant metabolic derangements. Hypoglycaemia occurs due to ethanol's inhibition of gluconeogenesis, especially in children, and may cause lactic acidosis, ketoacidosis, and acute renal failure. Metabolic acidosis is compounded by respiratory failure. Patients may also present with hypothermia.
Pharmacology.
In the past, alcohol was believed to be a non-specific pharmacological agent affecting many neurotransmitter systems in the brain. However, molecular pharmacology studies have shown that alcohol has only a few primary targets. In some systems, these effects are facilitatory and in others inhibitory.
Among the neurotransmitter systems with enhanced functions are: GABAA, 5-HT3 receptor agonism (responsible for GABAergic (GABAA receptor PAM), glycinergic, and cholinergic effects), nicotinic acetylcholine receptors.
Among those that are inhibited are: NMDA, dihydropyridine-sensitive L-type Ca2+ channels and G-protein-activated inwardly rectifying K+ channels.
The result of these direct effects is a wave of further indirect effects involving a variety of other neurotransmitter and neuropeptide systems, leading finally to the behavioural or symptomatic effects of alcohol intoxication.
Diagnosis.
Definitive diagnosis relies on a blood test for alcohol, usually performed as part of a toxicology screen.
Law enforcement officers often use breathalyzer units and field sobriety tests as more convenient and rapid alternatives to blood tests.
There are also various models of breathalyzer units that are available for consumer use. Because these may have varying reliability and may produce different results than the tests used for law-enforcement purposes, the results from such devices should be conservatively interpreted.
Many informal intoxication tests exist, which, in general, are unreliable and not recommended as deterrents to excessive intoxication or as indicators of the safety of activities such as motor vehicle driving, heavy equipment operation, machine tool use, etc.
For determining whether someone is intoxicated by alcohol by some means other than a blood-alcohol test, it is necessary to rule out other conditions such as hypoglycemia, stroke, usage of other intoxicants, mental health issues, and so on. It is best if his/her behavior has been observed while the subject is sober to establish a baseline. Several well-known criteria can be used to establish a probable diagnosis. For a physician in the acute-treatment setting, acute alcohol intoxication can mimic other acute neurological disorders, or is frequently combined with other recreational drugs that complicate diagnosis and treatment.
Acute alcohol poisoning.
Signs and symptoms.
The signs and symptoms of acute alcohol poisoning include:
Management.
Acute alcohol poisoning is a medical emergency due to the risk of death from respiratory depression and/or inhalation of vomit if emesis occurs while the patient is unconscious and unresponsive. Emergency treatment for acute alcohol poisoning strives to stabilize the patient and maintain a patent airway and respiration, while waiting for the alcohol to metabolize:
Also:
Additional medication may be indicated for treatment of nausea, tremor, and anxiety.
Prognosis.
A normal liver detoxifies the blood of alcohol over a period of time that depends on the initial level and the patient's overall physical condition. An abnormal liver will take longer but still succeed, provided the alcohol does not cause liver failure.
People having drunk heavily for several days or weeks may have withdrawal symptoms after the acute intoxication has subsided.
A person consuming a dangerous amount of alcohol persistently can develop memory blackouts and idiosyncratic intoxication or pathological drunkenness symptoms.
Long-term persistent consumption of excessive amounts of alcohol can cause liver damage and have other deleterious health effects.
Society and culture.
Alcohol intoxication is a risk factor in some cases of catastrophic injury, in particular for unsupervised recreational activity. A study in the province of Ontario in Canada based on epidemiological data from 1986, 1989, 1992, and 1995 states that 79.2% of the 2,154 catastrophic injuries recorded for the study were preventable, of which 346 involved alcohol consumption. The activities most commonly associated with alcohol-related catastrophic injury were snowmobiling (124), fishing (41), diving (40), boating (31) and canoeing (7), swimming (31), riding an all-terrain vehicle (24), and cycling (23). These events are often associated with unsupervised young males, often inexperienced in the activity, and many result in drowning.
Legal issues.
Laws on drunkenness vary. In the United States, it is a criminal offence for a person to be drunk while driving a motorized vehicle, except in Wisconsin, where it is only a fine for the first offence. It is also a criminal offence to fly an aircraft or (in some American states) to assemble or operate an amusement park ride while drunk. Similar laws also exist in the United Kingdom and most other countries.
In some countries, it is also an offence to serve alcohol to an already-intoxicated person, and, often, alcohol can be sold only by persons qualified to serve responsibly through alcohol server training.
The (BAC) for legal operation of a vehicle is typically measured as a percentage of a unit volume of blood. This percentage ranges from 0.00% in Romania and the United Arab Emirates; to 0.05% in Australia, South Africa, and Germany; to 0.08% in the United Kingdom, the United States, Canada, and New Zealand.
The United States Federal Aviation Administration prohibits crew members from performing their duties with a BAC greater than 0.04% within eight hours of consuming an alcoholic beverage, or while under the influence of alcohol.
In the United States, the United Kingdom, and Australia, people are arrested for public intoxication, called "being drunk and disorderly" or "being drunk and incapable."
In some countries, there are special facilities, sometimes known as "drunk tanks", for the temporary detention of persons found to be drunk.
Religious views.
Some religious groups permit the consumption of alcohol. Some permit consumption but prohibit intoxication, while others prohibit alcohol consumption altogether. In the Qur'an, there is a prohibition on the consumption of grape-based alcoholic beverages, and intoxication is considered as an abomination in the Hadith. Islamic schools of law (Madh'hab) have interpreted this as a strict prohibition of the consumption of all types of alcohol and declared it to be haraam ("sinful"), although other uses may be permitted.
Some Protestant Christian denominations prohibit the drinking of alcohol based upon Biblical passages that condemn drunkenness (such as Proverbs 23:21, Isaiah 28:1, Habakkuk 2:15), but others allow moderate use of alcohol. While Proverbs 31:4, warns against Kings and Rulers drinking wine and strong drink, Proverbs 31:6–7 promotes giving strong drink to the perishing (dying) and wine to those whose lives are bitter, to forget their poverty and troubles. In some Christian groups, a small amount of wine is part of the rite of communion. In The Church of Jesus Christ of Latter-day Saints, alcohol consumption is forbidden, and teetotalism has become a distinguishing feature of its members. Jehovah's Witnesses allow moderate alcohol consumption among its members.
In Buddhism, in general, the consumption of intoxicants is discouraged for both monastics and lay followers. Many followers of Buddhism observe a code of conduct known as the Five Precepts, of which the fifth precept is an undertaking to refrain from the consumption of intoxicating substances (except for medical reasons). In the Bodhisattva Vows of the Brahma Net Sutra, observed by some monastic communities and some lay followers, distribution of intoxicants is likewise discouraged as well as consumption.
In the branch of Hinduism known as Gaudiya Vaishnavism, one of the four regulative principles forbids the taking of intoxicants, including alcohol.
In Judaism, the Babylonian Talmud says in Megillah 7b that "Rava said: A person is obligated to become intoxicated on Purim until he is unaware of the difference between 'Cursed be Haman' and 'Blessed be Mordechai.'" This is taken to mean that on the Jewish festival of Purim one is commanded to drink alcohol to the point of intoxication. During all other times of year, though, Judaism stresses moderation—not cutting out alcohol entirely, but not getting too drunk either.

</doc>
<doc id="8013" url="http://en.wikipedia.org/wiki?curid=8013" title="Data compression">
Data compression

In computer science and information theory, data compression, source coding,
or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it.
The process of reducing the size of a data file is popularly referred to as data compression, although its formal name is source coding (coding done at the source of the data before it is stored or transmitted).
Compression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced ("e.g.", when using lossy data compression), and the computational resources required to compress and uncompress the data.
Lossless.
Lossless data compression algorithms usually exploit statistical redundancy to represent data more concisely without losing information, so that the process is reversible. Lossless compression is possible because most real-world data has statistical redundancy. For example, an image may have areas of colour that do not change over several pixels; instead of coding "red pixel, red pixel, ..." the data may be encoded as "279 red pixels". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.
The Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip and PNG. LZW (Lempel–Ziv–Welch) is used in GIF images. Also noteworthy is the LZR (Lempel-Ziv–Renau) algorithm, which serves as the basis for the Zip method. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded (e.g. SHRI, LZX).
A current LZ-based coding scheme that performs well is LZX, used in Microsoft's CAB format.
The best modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows–Wheeler transform can also be viewed as an indirect form of statistical modelling.
The class of grammar-based codes are gaining popularity because they can compress "highly repetitive text," extremely effectively, for instance, biological data collection of same or related species, huge versioned document collection, internet archives, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Sequitur and Re-Pair are practical grammar compression algorithms for which public codes are available.
In a further refinement of these techniques, statistical predictions can be coupled to an algorithm called arithmetic coding. Arithmetic coding, invented by Jorma Rissanen, and turned into a practical method by Witten, Neal, and Cleary, achieves superior compression to the better-known Huffman algorithm and lends itself especially well to adaptive data compression tasks where the predictions are strongly context-dependent. Arithmetic coding is used in the bi-level image compression standard JBIG, and the document compression standard DjVu. The text entry system Dasher is an inverse arithmetic coder.
Lossy.
Lossy data compression is the converse of lossless data compression. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are informed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video.
Lossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 Video codec for video compression.
In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from "audio compression". Different audio and speech compression standards are listed under audio codecs. "Voice compression" is used in Internet telephony, for example audio compression is used for CD ripping and is decoded by audio players.
Theory.
The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate–distortion theory for lossy compression. These areas of study were essentially forged by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related. The idea of data compression is deeply connected with statistical inference.
Machine learning.
There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as justification for data compression as a benchmark for "general intelligence."
Data differencing.
Data compression can be viewed as a special case of data differencing: Data differencing consists of producing a "difference" given a "source" and a "target," with patching producing a "target" given a "source" and a "difference," while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a "difference from nothing." This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.
When one wishes to emphasize the connection, one may use the term "differential compression" to refer to data differencing.
Outlook and currently unused potential.
It is estimated that the total amount of the data that are stored on the world's storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information.
Uses.
Audio.
Audio data compression, as distinguished from dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate less audible or meaningful sounds, thereby reducing the space required to store or transmit them.
In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data.
The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640MB.
Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50–60% of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression.
When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies.
A number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apple's Apple Lossless (ALAC), MPEG-4 ALS, Microsoft's Windows Media Audio 9 Lossless (WMA Lossless), Monkey's Audio, TTA, and WavPack. See list of lossless codecs for a complete listing.
Some audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.
Other formats are associated with a distinct system, such as:
Lossy audio compression.
Lossy audio compression is used in a wide range of applications. In addition to the direct applications (mp3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data.
The innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all.
Due to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minute's worth of music at adequate quality.
Coding methods.
To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous masking—the phenomenon wherein a signal is masked by another signal separated by frequency—and, in some cases, temporal masking—where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.
Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sound's generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder's quantization noise into the spectrum of the target signal, partially masking it.
Lossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen.
Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a "frame" to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.
In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)).
Speech encoding.
Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate.
If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.
This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches:
Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the µ-law algorithm.
History.
A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee.
The world's first commercial broadcast automation audio compression system was developed by Oscar Bonello, an Engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.
Video.
Video compression uses modern coding techniques to reduce redundancy in video data. Most video compression algorithms and codecs combine spatial image compression and temporal motion compensation. Video compression is a practical implementation of source coding in information theory. In practice, most video codecs also use audio compression techniques in parallel to compress the separate, but combined data streams as one package.
The majority of video compression algorithms use lossy compression. Uncompressed video requires a very high data rate. Although lossless video compression codecs perform an average compression of over factor 3, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.
Some video compression schemes typically operates on square-shaped groups of neighboring pixels, often called macroblocks. These pixel groups or blocks of pixels are compared from one frame to the next, and the video compression codec sends only the differences within those blocks. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.
Encoding theory.
Video data may be represented as a series of still image frames. The sequence of frames contains spatial and temporal redundancy that video compression algorithms attempt to eliminate or code in a smaller size. Similarities can be encoded by only storing differences between frames, or by using perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. Some of these methods are inherently lossy while others may preserve all relevant information from the original, uncompressed video.
One of the most powerful techniques for compressing video is interframe compression. Interframe compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression.
The most powerful used method works by comparing each frame in the video with the previous one. If the frame contains areas where nothing has moved, the system simply issues a short command that copies that part of the previous frame, bit-for-bit, into the next one. If sections of the frame move in a simple manner, the compressor emits a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Interframe compression works well for programs that will simply be played back by the viewer, but can cause problems if the video sequence needs to be edited.
Because interframe compression copies data from one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making 'cuts' in intraframe-compressed video is almost as easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and interframe compression is that, with intraframe systems, each frame uses a similar amount of data. In most interframe systems, certain frames (such as "I frames" in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.
It is possible to build a computer-based video editor that spots problems caused when I frames are edited out while other frames need them. This has allowed newer formats like HDV to be used for editing. However, this process demands a lot more computing power than editing intraframe compressed video with the same picture quality.
Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial redundancy reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT) have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.
Timeline.
The following table is a partial history of international video compression standards.
Genetics.
Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold—allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes).

</doc>
<doc id="8022" url="http://en.wikipedia.org/wiki?curid=8022" title="History of the Democratic Republic of the Congo">
History of the Democratic Republic of the Congo

The region that is now the Democratic Republic of the Congo was first settled about 80,000 years ago. Bantu migration arrived in the region from Nigeria in the 7th century AD. The Kingdom of Kongo remained present in the region between the 14th and the early 19th centuries. Belgian colonization began when King Leopold II founded the Congo Free State, a corporate state run solely by King Leopold. Reports of widespread murder and torture in the rubber plantations led the Belgian government to seize the Congo from Leopold II and establish the Belgian Congo. Under Belgian rule, the colony was run with the presence of numerous Christian organizations that wanted to Westernize the Congolese people.
After an uprising by the Congolese people, Belgium granted the Congo its independence in 1960. However, the Congo was left unstable because tribal leaders had more power than the central government. Prime Minister Patrice Lumumba tried to restore order with the aid of the Soviet Union as part of the Cold War, causing the United States to support a coup led by Colonel Joseph Mobutu in 1965. Mobutu quickly seized complete power of the Congo and later rename the country Zaire. He sought to Africanize the country, changing his own name to Mobuto Sese Seko, and demanded that African citizens to change their names to African names. Mobuto sought to repress any opposition to his rule, in which he successfully did throughout the 1980s. However, with his regime weakened during the early 1990s, Mobuto was forced to agree to a power-sharing government with the opposition party. Mobuto remained the head of state and promised elections for the next two years that never happened.
In the First Congo War, Rwanda invaded Zaire, which overthrow Mobuto during the process. Laurent-Desire Kabila later took power and rename the country back to the Democratic Republic of the Congo. After a disappointing rule under Kabila, the Second Congo War broke out, resulting in a regional war with many different African nations taking part. Kabila was assassinated by his bodyguard in 2001, and his son, Joseph, succeeded him and later elected president by the Congolese government in 2006. Upon taking office, Kabila quickly sought peace, ending the era of war in Africa. Soldiers were left in the Congo for a few years and a power-sharing government between Kabila and the opposition party was set up. Kabila later resumed complete control over the Congo and was re-elected in a disputed election in 2011. Today, the Congo remains dangerously unstable.
Early history.
The area now known as the Democratic Republic of the Congo was populated as early as 80,000 years ago, as shown by the 1988 discovery of the Semliki harpoon at Katanda, one of the oldest barbed harpoons ever found, and which is believed to have been used to catch giant river catfish. Congo was settled in the 7th and 8th centuries A.D. by Bantus from present-day Nigeria. During its history, the area has also been known as "Congo", "Congo Free State", "Belgian Congo", and "Zaire". The Kingdom of Kongo was a powerful kingdom that existed between the 14th and the early 19th century. It was the dominant force in the region until the arrival of the Portuguese. Second in importance was the Anziku Kingdom.
Colonial rule.
Congo Free State (1885–1908).
The Congo Free State was a corporate state privately controlled by Leopold II of Belgium through the "Association internationale africaine", a non-governmental organization. Leopold was the sole shareholder and chairman. The state included the entire area of the present Democratic Republic of the Congo. Under Leopold II's administration, the Congo Free State became the site of one of the most infamous international scandals of the turn of the twentieth century. The report of the British Consul Roger Casement led to the arrest and punishment of white officials who had been responsible for cold-blooded killings during a rubber-collecting expedition in 1903, including one Belgian national for causing the shooting of at least 122 Congolese natives. Estimates of the total death toll vary considerably. In the absence of a census, the first was made in 1924, it is even more difficult to quantify the population loss of the period. Roger Casement's famous 1904 report estimated ten million people. According to Casement's report, indiscriminate "war", starvation, reduction of births and tropical diseases caused the country's depopulation. The European and U.S. press agencies exposed the conditions in the Congo Free State to the public in 1900. By 1908 public and diplomatic pressure led Leopold II to annex the Congo as the Belgian Congo colony.
Belgian Congo (1908–1960).
On 15 November 1908 King Léopold II of Belgium formally relinquished personal control of the Congo Free State. The renamed Belgian Congo was put under the direct administration of the Belgian government and its Ministry of Colonies.
Belgian rule in the Congo was based around the "colonial trinity" ("trinité coloniale") of state, missionary and private company interests. The privileging of Belgian commercial interests meant that large amounts of capital flowed into the Congo and that individual regions became specialised. On many occasions, the interests of the government and private enterprise became closely tied and the state helped companies break strikes and remove other barriers imposed by the indigenous population. The country was split into nesting, hierarchically organised administrative subdivisions, and run uniformly according to a set "native policy" ("politique indigène")—in contrast to the British and the French, who generally favoured the system of indirect rule whereby traditional leaders were retained in positions of authority under colonial oversight. There was also a high degree of racial segregation. Large numbers of white immigrants who moved to the Congo after the end of World War II came from across the social spectrum, but were nonetheless always treated as superior to blacks.
During the 1940s and 1950s, the Congo experienced an unprecedented level of urbanisation and the colonial administration began various development programmes aimed at making the territory into a "model colony". Notable advances were made in treating diseases such as African trypanosomiasis. One of the results of these measures was the development of a new middle class of Europeanised African "évolué" in the cities. By the 1950s the Congo had a wage labour force twice as large as that in any other African colony. The Congo's rich natural resources, including uranium—much of the uranium used by the U.S. nuclear programme during World War II was Congolese—led to substantial interest in the region from both the Soviet Union and the United States as the Cold War developed.
The Congo Crisis (1960–1965).
Following riots in Leopoldville between 4–7 January 1959, and Stanleyville on 31 October 1959, the Belgians realised they could not maintain control of such a vast country in the face of rising demands for independence. The Belgians and Congolese political leaders held a Round Table Conference in Brussels beginning on 18 January 1960. At the end of the Conference on 27 January 1960 it was announced that elections would be held in the Congo on 22 May 1960, and full independence granted on 30 June 1960. The Congo was indeed granted its independence on 30 June 1960, adopting the name "Republic of the Congo" (République du Congo). As the French colony of Middle Congo (Moyen Congo) also chose the name Republic of Congo upon receiving its independence, the two countries were more commonly known as Congo-Léopoldville and Congo-Brazzaville, after their capital cities. President Mobutu changed the country's official name to Zaire in 1966.
In 1960, the country was in a very unstable state—regional tribal leaders held far more power than the central government—and with the departure of the Belgian administrators, there were almost no skilled bureaucrats left in the country. The first Congolese university graduate was only in 1956, and virtually no one in the new nation had any idea of how to manage a country of such size.
Parliamentary elections in 1960 produced the nationalist Patrice Lumumba as prime minister and pro-Western Joseph Kasavubu as president of the renamed Democratic Republic of the Congo.
Even from this fleeting moment of independence democracy began to unravel. On 5 July 1960 a military mutiny by Congolese soldiers against their European officers broke out in the capital and rampant looting began. On 11 July 1960 the richest province of the country, Katanga, seceded under Moise Tshombe. The United Nations sent 20,000 peacekeepers to protect Europeans in the country and try to restore order. Western paramilitaries and mercenaries, often hired by mining companies to protect their interests, also began to pour into the country. In this same period Congo's second richest province, Kasai, also announced its independence on 8 August 1960.
Prime Minister Lumumba turned to the USSR for assistance. Nikita Khrushchev agreed to help, offering advanced weaponry and technical advisors. The United States viewed the Soviet presence as an attempt to take advantage of the situation and gain a proxy state in sub-Saharan Africa. UN forces were ordered to block any shipments of arms into the country. The United States also looked for a way to replace Lumumba as leader. President Kasavubu had clashed with Prime Minister Lumumba and advocated an alliance with the West rather than the Soviets. The U.S. sent weapons and CIA personnel to aid forces allied with Kasavubu and combat the Soviet presence. On 14 September 1960, with U.S. and CIA support, Colonel Joseph Mobutu overthrew the government and arrested Lumumba.
On 17 January 1961 Mobutu sent Lumumba to Élisabethville (now Lubumbashi), capital of Katanga. In full view of the press he was beaten and forced to eat copies of his own speeches. For the next three weeks, he was not seen or heard from. Then Katangan radio announced implausibly that he had escaped and been killed by some villagers. In fact he had been tortured and killed along with two others shortly after his arrival. It was soon clear that he had been murdered in custody. In 2001, a Belgian inquiry established that he had been shot by Katangan gendarmes in the presence of Belgian officers, under Katangan command. Lumumba was beaten, placed in front of a firing squad with 2 other allies, cut up, buried, dug up and what remained was dissolved in acid.
In Stanleyville, those loyal to the deposed Lumumba set up a rival government under Antoine Gizenga which lasted from 31 March 1961 until it was reintegrated on 5 August 1961. After some reverses, UN and Congolese government forces succeeded in recapturing the breakaway provinces of South Kasai on 30 December 1961, and Katanga on 15 January 1963.
A new crisis erupted in the Simba Rebellion of 1964-1965 which saw half the country taken by the rebels. European mercenaries, US, and Belgian troops were called in by the Congolese government to defeat the rebellion.
Zaire (1965–1997).
Unrest and rebellion plagued the government until November 1965, when Lieutenant General Mobutu, by then commander in chief of the national army, seized control of the country and declared himself president for five years. Mobutu quickly consolidated his power and was elected unopposed as president in 1970. Embarking on a campaign of cultural awareness, Mobutu renamed the country the Republic of Zaire in 1971 and required citizens to adopt African names as well as drop their French-language ones. Relative peace and stability prevailed until 1977 and 1978 when Katangan rebels, based in Angola, launched a series of invasions (Shaba I and II) into the Shaba (Katanga) region. The rebels were driven out with the aid of Belgian paratroopers.
Zaire remained a one-party state in the 1980s. Although Mobutu successfully maintained control during this period, opposition parties, most notably the Union pour la Démocratie et le Progrès Social (UDPS), were active. Mobutu's attempts to quell these groups drew significant international criticism.
As the Cold War came to a close, internal and external pressures on Mobutu increased. In late 1989 and early 1990, Mobutu was weakened by a series of domestic protests, by heightened international criticism of his regime's human rights practices, by a faltering economy, and by government corruption, most notably his massive embezzlement of government funds for personal use.
In April 1990, Mobutu declared the Third Republic, agreeing to a limited multi-party system with elections and a constitution. As details of a reform package were delayed, soldiers in September 1991 began looting Kinshasa to protest their unpaid wages. Two thousand French and Belgian troops, some of whom were flown in on U.S. Air Force planes, arrived to evacuate the 20,000 endangered foreign nationals in Kinshasa.
In 1992, after previous similar attempts, the long-promised Sovereign National Conference was staged, encompassing over 2,000 representatives from various political parties. The conference gave itself a legislative mandate and elected Archbishop Laurent Monsengwo as its chairman, along with Étienne Tshisekedi wa Mulumba, leader of the UDPS, as prime minister. By the end of the year Mobutu had created a rival government with its own prime minister. The ensuing stalemate produced a compromise merger of the two governments into the High Council of Republic-Parliament of Transition (HCR-PT) in 1994, with Mobutu as head of state and Kengo Wa Dondo as prime minister. Although presidential and legislative elections were scheduled repeatedly over the next two years, they never took place.
First Congo War (1996–1997).
By 1996, tensions from the neighboring Rwanda war and genocide had spilled over to Zaire: "see History of Rwanda". Rwandan Hutu militia forces (Interahamwe), who had fled Rwanda following the ascension of a Tutsi-led government, had been using Hutu refugees camps in eastern Zaire as a basis for incursion against Rwanda. These Hutu militia forces soon allied with the Zairian armed forces (FAZ) to launch a campaign against Congolese ethnic Tutsis in eastern Zaire. In turn, these Tutsis formed a militia to defend themselves against attacks. When the Zairian government began to escalate its massacres in November 1996, the Tutsi militias erupted in rebellion against Mobutu.
The Tutsi militia was soon joined by various opposition groups and supported by several countries, including Rwanda and Uganda. This coalition, led by Laurent-Desire Kabila, became known as the Alliance des Forces Démocratiques pour la Libération du Congo-Zaïre (AFDL). The AFDL, now seeking the broader goal of ousting Mobutu, made significant military gains in early 1997. They were soon joined by various Zairean politicians, who had been unsuccessfully opposing the dictatorship of Mobutu for many years, and now saw an opportunity for them in the invasion of Zaire by two of the region's strongest military forces. Following failed peace talks between Mobutu and Kabila in May 1997, Mobutu left the country, and Kabila marched unopposed to Kinshasa on 20 May. Kabila named himself president, consolidated power around himself and the AFDL, and reverted the name of the country to the Democratic Republic of Congo.
Second Congo War (1998–2003).
Kabila demonstrated little ability to manage the problems of his country, and lost his allies. To counterbalance the power and influence of Rwanda in DRC, the Ugandan troops instigated the creation of another rebel movement called the Movement for the Liberation of Congo (MLC), led by the Congolese warlord Jean-Pierre Bemba. They attacked in August 1998, backed by Rwandan and Ugandan troops. Soon afterwards, Angola, Namibia, and Zimbabwe became involved militarily in the Congo, with Angola and Zimbabwe supporting the government. While the six African governments involved in the war signed a ceasefire accord in Lusaka in July 1999, the Congolese rebels did not and the ceasefire broke down within months. However, Kabila was assassinated in 2001 by one of his bodyguards and was succeeded by his son, Joseph. Upon taking office, Kabila called for multilateral peace talks to end the war. Kabila partly succeeded when a further peace deal was brokered between him, Uganda, and Rwanda leading to the apparent withdrawal of foreign troops.
Currently, the Ugandans and the MLC still hold a wide section of the north of the country; Rwandan forces and its front, the Rassemblement Congolais pour la Démocratie (RCD) control a large section of the east; and government forces or their allies hold the west and south of the country. There were reports that the conflict is being prolonged as a cover for extensive looting of the substantial natural resources in the country, including diamonds, copper, zinc, and coltan. The conflict was reignited in January 2002 by ethnic clashes in the northeast and both Uganda and Rwanda then halted their withdrawal and sent in more troops. Talks between Kabila and the rebel leaders, held in Sun City, lasted a full six weeks, beginning in April 2002. In June, they signed a peace accord in which Kabila would share power with former rebels. By June 2003, all foreign armies except those of Rwanda had pulled out of Congo. Few people in the Congo have been unaffected by the armed conflict. A survey conducted in 2009 by the ICRC and Ipsos shows that three quarters (76%) of the people interviewed have been affected in some way–either personally or due to the wider consequences of armed conflict.
The response of the international community has been incommensurate with the scale of the disaster resulting from the war in the Congo. Its support for political and diplomatic efforts to end the war has been relatively consistent, but it has taken no effective steps to abide by repeated pledges to demand accountability for the war crimes and crimes against humanity that were routinely committed in Congo. United Nations Security Council and the U.N. Secretary-General have frequently denounced human rights abuses and the humanitarian disaster that the war unleashed on the local population. But they had shown little will to tackle the responsibility of occupying powers for the atrocities taking place in areas under their control, areas where the worst violence in the country took place. Hence Rwanda, like Uganda, has escaped any significant sanction for its role.
Transitional government (2003–2006).
DR Congo had a transitional government in July 2003 until the election was over. A constitution was approved by voters and on 30 July 2006 the Congo held its first multi-party elections since independence in 1960. After this Joseph Kabila took 45% of the votes and his opponent Jean-Pierre Bemba took 20%. That was the origin of a fight between the two parts from 20–22 August 2006 in the streets of the capital, Kinshasa. Sixteen people died before policemen and UN mission MONUC took control of the city. A new election was held on 29 October 2006, which Kabila won with 70% of the vote. Bemba has publicly commented on election "irregularities," despite the fact that every neutral observer has praised the elections. On 6 December 2006 the Transitional Government came to an end as Joseph Kabila was sworn in as President.
Continued conflicts.
The fragility of the state has allowed continued violence and human rights abuses in the east. There are three significant centers of conflict.
Ituri, where MONUC has proved unable to contain the numerous militia and groups driving the Ituri conflict
Northern Katanga, where Mai-Mai created by Laurent Kabila slipped out of the control of Kinshasa.
In October 2009 a new conflict started in Dongo, Sud-Ubangi District where clashes had broken out over access to fishing ponds.
Kivu conflict.
North Kivu and South Kivu, where Democratic Forces for the Liberation of Rwanda (FDLR) continues to threaten the Rwandan border and the Banyamulenge, Rwanda supported RCD-Goma rebels (see Kivu war).
In April 2012, ethnic Tutsi soldiers mutinied against the government of the Democratic Republic of the Congo. Mutineers formed a rebel group called the March 23 Movement (M23), composed of former members of the rebel National Congress for the Defence of the People (CNDP). On 20 November 2012, M23 took control of Goma, a provincial capital with a population of one million people.
Re-election of Joseph Kabila.
In December 2011, Joseph Kabila was re-elected for a second term as president. After the results were announced on 9 December, there was violent unrest in Kinshasa and Mbuji-Mayi, where official tallies showed that a strong majority had voted for the opposition candidate Etienne Tshisekedi. Official observers from the Carter Center reported that returns from almost 2,000 polling stations in areas where support for Tshisekedi was strong had been lost and not included in the official results. They described the election as lacking credibility. On 20 December, Kabila was sworn in for a second term, promising to invest in infrastructure and public services. However, Tshisekedi maintained that the result of the election was illegitimate and said that he intended also to "swear himself in" as president.

</doc>
<doc id="8023" url="http://en.wikipedia.org/wiki?curid=8023" title="Geography of the Democratic Republic of the Congo">
Geography of the Democratic Republic of the Congo

The Democratic Republic of the Congo is by the Congo River Basin, which covers an area of almost . The country's only outlet to the Atlantic Ocean is a narrow strip of land on the north bank of the Congo River.
The vast, low-lying central area is a basin-shaped plateau sloping toward the west, covered by tropical rainforest and criss-crossed by rivers, a large area of this has been categorized by the World Wildlife Fund as the Central Congolian lowland forests ecoregion. The forest center is surrounded by mountainous terraces in the west, plateaus merging into savannas in the south and southwest. 
Dense grasslands extend beyond the Congo River in the north. High mountains of the Ruwenzori Range (some above ) are found on the eastern borders with Rwanda and Uganda (see Albertine Rift montane forests for a description of this area).
Climate.
The Democratic Republic of the Congo lies on the Equator, with one-third of the country to the north and two-thirds to the south. The climate is hot and humid in the river basin and cool and dry in the southern highlands, with a cold, alpine climate in the Ruwenzori Range. 
South of the Equator, the rainy season lasts from October to May and north of the Equator, from April to November. Along the Equator, rainfall is fairly regular throughout the year. During the wet season, thunderstorms often are violent but seldom last more than a few hours. The average rainfall for the entire country is about .
Data.
Location of Congo:
Central Africa, northeast of Angola
Geographic coordinates: 
Map references:
Africa
Area:
<br>"total:"
2,345,410 km2
<br>"land:"
2,267,600 km2
<br>"water:"
77,810 km2
Area - comparative:
slightly less than one-fourth the size of the US
Land boundaries:
<br>"total:"
10,744 km
<br>"border countries:"
Angola 2,511 km, Burundi 233 km, Central African Republic 1,577 km, Republic of the Congo 2,410 km, Rwanda 217 km, South Sudan 628 km, Tanzania 473 km, Uganda 765 km, Zambia 1,930 km
Coastline:
Maritime claims:
<br>"exclusive economic zone:"
boundaries with neighbors
<br>"territorial sea:"
Climate:
tropical; hot and humid in equatorial river basin; cooler and drier in southern highlands; cooler-cold and wetter in eastern highlands and the Ruwenzori Range; north of Equator - wet season April to October, dry season December to February; south of Equator - wet season November to March, dry season April to October
Terrain:
vast central plateau covered by tropical rainforest, surrounded by mountains in the west, plains and savanna in the south/southwest, and grasslands in the north. The high mountains of the Ruwenzori Range on the eastern borders.
Elevation extremes:
<br>"lowest point:"
Atlantic Ocean 0 m
<br>"highest point:"
Pic Marguerite on Mont Ngaliema (Mount Stanley) 5,110 m
Natural resources:
cobalt, copper, cadmium, petroleum, industrial and gem diamonds, gold, silver, zinc, manganese, tin, germanium, uranium, radium, bauxite, iron ore, coal, hydropower, timber
Land use:
<br>"arable land:"
2.96% (1998 est), 3% (1993 est.)
<br>"permanent crops:"
0.52% (1998 est.), 0% (1993 est.)
<br>"permanent pastures:"
7% (1993 est.)
<br>"forests and woodland:"
77% (1993 est.)
<br>"other:"
96.52 (1998 est.), 13% (1993 est.)
Irrigated land:
110 km2 (1998 est.), 100 km2 (1993 est.)
Natural hazards:
periodic droughts in south; Congo River floods (seasonal); in the east, in the Albertine Rift, there are active volcanoes
Environment - current issues:
poaching threatens wildlife populations (for example, the Painted Hunting Dog, "Lycaon pictus" is now considered extinct in the Congo due to human overpopulation and poaching); water pollution; deforestation (chiefly due to land conversion to agriculture by indigenous farmers); refugees responsible for significant deforestation, soil erosion, and wildlife poaching; mining of minerals (coltan — a mineral used in creating capacitors, diamonds, and gold) causing environmental damage
Environment - international agreements:
<br>"party to:"
Biodiversity, Climate Change, Desertification, Endangered Species, Hazardous Wastes, Law of the Sea, Marine Dumping, Nuclear Test Ban, Ozone Layer Protection, Tropical Timber 83, Tropical Timber 94, Wetlands
<br>"signed, but not ratified:"
Environmental Modification
Geography:
straddles Equator; very narrow strip of land that controls the lower Congo River and is the only outlet to South Atlantic Ocean; dense tropical rainforest in central river basin and eastern highlands
Extreme points.
This is a list of the extreme points of the Democratic Republic of the Congo, the points that are farther north, south, east or west than any other location.

</doc>
<doc id="8024" url="http://en.wikipedia.org/wiki?curid=8024" title="Demographics of the Democratic Republic of the Congo">
Demographics of the Democratic Republic of the Congo

This article is about the demographic features of the population of the Democratic Republic of the Congo, including ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
As many as 250 ethnic groups have been distinguished and named. The most numerous people are the Luba, Mongo, and Bakongo. 
Although 700 local languages and dialects are spoken, the linguistic variety is bridged both by the use of French and the intermediary languages Kongo, Luba-Kasai, Swahili, and Lingala.
Population.
According to the 2010 revison of the World Population Prospects the total population was 65 966 000 in 2010, compared to only 12 184 000 in 1950. The proportion of children below the age of 15 in 2010 was 46.3%, 51.1% was between 15 and 65 years of age, while 2.7% was 65 years or older
Vital statistics.
Registration of vital events in the Democratic Republic of the Congo is incomplete. The Population Departement of the United Nations prepared the following estimates.
Fertility and Births.
Total Fertility Rate (TFR) and Crude Birth Rate (CBR):
Ethnic groups.
More than 250 ethnic groups have been identified and named of which the majority are Bantu. The four largest groups - Mongo, Luba, Kongo (all Bantu), and the Mangbetu-Azande make up about 45% of the population. The country has also 60,000 Belgians, whose ancestors stepped out after it won independence.
Bantu peoples (80%):
Central Sudanic/Ubangian :
Nilotic peoples :
Pygmy peoples :
More than 600,000 pygmies (around 1% of the total population) are believed to live in the DR Congo's huge forests, where they survive by hunting wild animals and gathering fruits.
Languages.
The four major languages in the DRC are French (official), Lingala (a lingua franca trade language), Kingwana (a dialect of Swahili), Kikongo, and Tshiluba. There are over 200 ethnic languages.
French is generally the medium of instruction in schools. English is taught as a compulsory foreign language in Secondary and High School around the country. It is a required subject in the Faculty of Economics at major universities around the country and there are numerous language schools in the country that teach it. In the town of Beni, for instance, there is a Bilingual University that offer courses in both French and English. President Kabila himself is fluent in both English and French, as was his father.
Religions.
Roman Catholic 50%, Protestant 20%, Kimbanguist 10%, Muslim 10%, other (includes syncretic sects and indigenous beliefs) 10% official report according to the CIA The World Factbook
Roman Catholic 43.9%, Protestant 24.8%, Other Christian 23.7%, Muslim 1.6%, Non-religious 0.6%, Hindu 0.1% other syncretic sects and indigenous beliefs 5.3% according to Joshua project
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Net migration rate.
-0.54 migrant(s)/1,000 population
"note": fighting between the Congolese Government and Uganda- and Rwanda-backed Congolese rebels spawned a regional war in DRC in August 1998, which left 2.33 million Congolese internally displaced and caused 412,000 Congolese refugees to flee to surrounding countries (2011 est.)
Given the situation in the country and the condition of state structures, it is extremely difficult to obtain reliable data however evidence suggests that DRC continues to be a destination country for immigrants in spite of recent declines. Immigration is seen to be very diverse in nature, with refugees and asylum-seekers - products of the numerous and violent conflicts in the Great Lakes Region - constituting an important subset of the population in the country.
Additionally, the country’s large mine operations attract migrant workers from Africa and beyond and there is considerable migration for commercial activities from other African countries and the rest of the world, but these movements are not well studied. Transit migration towards South Africa and Europe also plays a role. Immigration in the DRC has decreased steadily over the past two decades, most likely as a result of the armed violence that the country has experienced.
According to the International Organization for Migration, the number of immigrants in the DRC has declined from just over 1 million in 1960, to 754,000 in 1990, to 480,000 in 2005, to an estimated 445,000 in 2010. Valid figures are not available on migrant workers in particular, partly due to the predominance of the informal economy in the DRC. Data are also lacking on irregular immigrants, however given neighbouring country ethnic links to nationals of the DRC, irregular migration is assumed to be a significant phenomenon in the country. 
Figures on the number of Congolese nationals abroad vary greatly depending on the source, from 3 to 6 million. This discrepancy is due to a lack of official, reliable data. Emigrants from the DRC are above all long-term emigrants, the majority of which live within Africa and to a lesser extent in Europe; 79.7% and 15.3% respectively, according to estimates on 2000 data. Most Congolese emigrants however, remain in Africa, with new destination countries including South Africa and various points en route to Europe.
In addition to being a host country, the DRC has also produced a considerable number of refugees and asylum-seekers located in the region and beyond. These numbers peaked in 2004 when, according to UNHCR, there were more than 460,000 refugees from the DRC; in 2008, Congolese refugees numbered 367,995 in total, 68% of which were living in other African countries.
Congolese diaspora.
The table below shows DRC born people who have emigrated abroad (although it excludes their descendants).
These are only estimates and do not account for Congolese migrants residing illegally in these countries. Among African countries, Congo's diaspora is second only to Nigeria in size.

</doc>
<doc id="8025" url="http://en.wikipedia.org/wiki?curid=8025" title="Economy of the Democratic Republic of the Congo">
Economy of the Democratic Republic of the Congo

Sparsely populated in relation to its area, the Democratic Republic of the Congo is home to a vast potential of natural resources and mineral wealth, its untapped deposits of raw minerals are estimated to be worth in excess of US$ 24 trillion, the economy of the Democratic Republic of the Congo has declined drastically since the mid-1980s.
At the time of its independence in 1960, the Democratic Republic of the Congo was the second most industrialized country in Africa after South Africa. It boasted a thriving mining sector and its agriculture sector was relatively productive. Since then, corruption, war and political instability have been a severe detriment to further growth, today leaving DRC with a GDP per capita among the world's lowest.
Economic Implications of Conflicts.
The two recent conflicts (the First and Second Congo Wars), which began in 1996, have dramatically reduced national output and government revenue, have increased external debt, and have resulted in deaths of more than five million people from war, and associated famine and disease. Malnutrition affects approximately two thirds of the country's population.
Agriculture is the mainstay of the economy, accounting for 57.9% of GDP in 1997. In 1996, agriculture employed 66% of the work force.
Rich in minerals, the Democratic Republic of the Congo has a difficult history of predatory mineral extraction, which has been at the heart of many struggles within the country for many decades, but particularly in the 1990s. The economy of the third largest country in Africa relies heavily on mining. However, much economic activity occurs in the informal sector and is not reflected in GDP data.
In 2006 Transparency International ranked the Democratic Republic of the Congo 156 out of 163 countries in the Corruption Perception Index, tying Bangladesh, Chad, and Sudan with a 2.0 rating. President Joseph Kabila established the Commission of Repression of Economic Crimes upon his ascension to power in 2001.
Economic history.
1990s.
International Bank for Reconstruction and Development (IBRD) Trust Fund for the Congo.
Poor infrastructure, an uncertain legal framework, corruption, and lack of openness in government economic policy and financial operations remain a brake on investment and growth. A number of International Monetary Fund (IMF) and World Bank missions have met with the new government to help it develop a coherent economic plan but associated reforms are on hold. 
Faced with continued currency depreciation, the government resorted to more drastic measures and in January 1999 banned the widespread use of U.S. dollars for all domestic commercial transactions, a position it later adjusted. The government has been unable to provide foreign exchange for economic transactions, while it has resorted to printing money to finance its expenditure. Growth was negative in 2000 because of the difficulty of meeting the conditions of international donors, continued low prices of key exports, and post-coup instability.
Although depreciated, congolese francs have been stable for few years (Ndonda, 2014)
2000s.
Conditions improved in late 2002 with the withdrawal of a large portion of the invading foreign troops. A number of IMF and World Bank missions have met with the government to help it develop a coherent economic plan, and President Kabila has begun implementing reforms.
2010s.
Special Economic Zone.
The DRC is embarking on the establishment of special economic zones (SEZ) to encourage the revival of its industry. The first SEZ was planned to come into being in 2012 in N'Sele, a commune of Kinshasa, and will focus on agro-industries. The Congolese authorities also planned to open another zone dedicated to mining (Katanga) and a third dedicated to cement (in the Bas-Congo).
Implications of Instability on Economy.
Ongoing conflicts dramatically reduced government revenue 
increased external debt. As Reyntjens wrote, “Entrepreneurs of insecurity are engaged in extractive activities that would be impossible in a stable state environment. The criminalization context in which these activities occur offers avenues for considerable factional and personal enrichment through the trafficking of arms, illegal drugs, toxic products, mineral resources and dirty money.”16
International Relations.
International Bank for Reconstruction and Development (IBRD) Trust Fund for the Congo.
Poor infrastructure, an uncertain legal framework, corruption, and lack of openness in government economic policy and financial operations remain a brake on investment and growth. A number of International Monetary Fund (IMF) and World Bank missions have met with the new government to help it develop a coherent economic plan but associated reforms are on hold. 
Faced with continued currency depreciation, the government resorted to more drastic measures and in January 1999 banned the widespread use of U.S. dollars for all domestic commercial transactions, a position it later adjusted. The government has been unable to provide foreign exchange for economic transactions, while it has resorted to printing money to finance its expenditure. Growth was negative in 2000 because of the difficulty of meeting the conditions of international donors, continued low prices of key exports, and post-coup instability.
World Bank.
Ease of Doing Business Rank (EDBR).
The Democratic Republic of Congo ranks 183 on the low end of the ease of doing business scale as ranked by the World Bank. This measures the difficulties of starting a business, enforcing contracts, paying taxes, resolving insolvency, protecting investors, trading across borders, getting credit, getting electricity, registering property, dealing with construction permits and registering property (World Bank 2014:8).
Sectors.
Agriculture.
Agriculture is the mainstay of the economy, accounting for 57.9% of the GDP in 1997. Main cash crops include coffee, palm oil, rubber, cotton, sugar, tea, and cocoa. Food crops include cassava, plantains, maize, groundnuts, and rice. In 1996, agriculture employed 66% of the work force.
Fishing.
The Democratic Republic of Congo also possesses 50 percent of Africa’s forests and a river system that could provide hydro-electric power to the entire continent, according to a United Nations report on the country’s strategic significance and its potential role as an economic power in central Africa. Fish are the single most important source of animal protein in the DRC. Total production of marine, river, and lake fisheries in 2003 was estimated at 222,965 tons, all but 5,000 tons from inland waters. PEMARZA, a state agency, carries on marine fishing.
Forestry.
Forests cover 60 percent of the total land area. There are vast timber resources, and commercial development of the country’s 61 million hectares (150 million acres) of exploitable wooded area is only beginning. The Mayumbe area of Bas-Congo was once the major center of timber exploitation, but forests in this area were nearly
depleted. The more extensive forest regions of the central cuvette and of the Ubangi River valley have increasingly been tapped. 
Roundwood removals were estimated at 72,170,000 m3 in 2003, about 95 percent for fuel. Some 14 species are presently being harvested. Exports of forest products in 2003 totalled $25.7 million. Foreign capital is necessary in order for forestry to expand, and the government recognizes that changes in tax structure and export procedures will be needed to facilitate economic growth.
Mining.
Rich in minerals, the DRC has a difficult history of predatory mineral extraction, which has been at the heart of many struggles within the country for many decades, but particularly in the 1990s. Although the economy of the Democratic Republic of the Congo, the second largest country in Africa has historically relied heavily on mining, this is no longer reflected in the GDP data as the mining industry has suffered from long-term "uncertain legal framework, corruption, and a lack of transparency in government policy." The informal sector .
In her book entitled "The Real Economy of Zaire", MacGaffey described a second, often illegal economy, "system D," which is outside the official economy (MacGaffey 1991:27). and therefore is not reflected in the GDP.
exploitation of mineral substances as MIBA EMAXON and De Beers 
The economy of the second largest country in Africa relies heavily on mining. The Congo is the world's largest producer of cobalt ore, and a major producer of copper and industrial diamonds. The Congo has 70% of the world’s coltan, and more than 30% of the world’s diamond reserves., mostly in the form of small, industrial diamonds. The coltan is a major source of tantalum, which is used in the fabrication of electronic components in computers and mobile phones. In 2002, tin was discovered in the east of the country, but, to date, mining has been on a small scale.
 Smuggling of the conflict minerals, coltan and cassiterite (ores of tantalum and tin, respectively), has helped fuel the war in the Eastern Congo.
Copper and Cobalt.
Katanga Mining Limited, a London-based company, owns the Luilu Metallurgical Plant, which has a capacity of 175,000 tonnes of copper and 8,000 tonnes of cobalt per year, making it the largest cobalt refinery in the world. After a major rehabilitation program, the company restarted copper production in December 2007 and cobalt production in May 2008.
Informal sector.
Much economic activity occurs in the informal sector and is not reflected in GDP data.
Transport.
Ground transport in the Democratic Republic of Congo has always been difficult. The terrain and climate of the Congo Basin present serious barriers to road and rail construction, and the distances are enormous across this vast country. Furthermore, chronic economic mismanagement and internal conflict has led to serious under-investment over many years.
On the other hand, the Democratic Republic of Congo has thousands of kilometres of navigable waterways, and traditionally water transport has been the dominant means of moving around approximately two-thirds of the country.

</doc>
<doc id="8026" url="http://en.wikipedia.org/wiki?curid=8026" title="Politics of the Democratic Republic of the Congo">
Politics of the Democratic Republic of the Congo

Politics of the Democratic Republic of Congo take place in a framework of a republic in transition from a civil war to a semi-presidential republic.
On 18 and 19 December 2005, a successful nationwide referendum was carried out on a draft constitution, which set the stage for elections in 2006. The voting process, though technically difficult due to the lack of infrastructure, was facilitated and organized by the Congolese Independent Electoral Commission with support from the UN mission to the Congo (MONUC). Early UN reports indicate that the voting was for the most part peaceful, but spurred violence in many parts of the war-torn east and the Kasais.
In 2006, many Congolese complained that the constitution was a rather ambiguous document and were unaware of its contents. This is due in part to the high rates of illiteracy in the country. However, interim President Kabila urged Congolese to vote 'Yes', saying the constitution is the country's best hope for peace in the future. 25 million Congolese turned out for the two-day balloting. According to results released in January 2006, the constitution was approved by 84% of voters. . The new constitution also aims to decentralize authority, dividing the vast nation into 25 semi-autonomous provinces, drawn along ethnic and cultural lines.
The country's first democratic elections in four decades were held on 30 July 2006, with a run-off between the incumbent, President Kabila, and his rival Bemba held on 29 October 2006. Polling was once again facilitated - yet not run - by UN peacekeepers. .
Political history.
From the day King Leopold II established colonial authority in what is now the Democratic Republic of Congo to today, the country's government has been unstable. This is reflected in its seven name changes since 1885:
From the day of the arguably ill-prepared independence of the Democratic Republic of the Congo, the tensions between the powerful leaders of the political elite, such as Joseph Kasa Vubu, Patrice Lumumba, Moise Tshombe, Joseph Mobutu and others, jeopardize the political stability of the new state. From Tshombe's secession of the Katanga, to the assassination of Lumumba, to the two coups d'état of Mobutu, the country has known periods of true nationwide peace, but virtually no period of genuine democratic rule.
The Mobutu era.
The Regime of Marshall Mobutu Sese Seko lasted 32 years (1965–1997), during which all but the first seven years the country was named Zaire. The dictatorial regime operated as a one-party-state, which saw most of the powers concentrated between President Mobutu, who was simultaneously the head of the state-party (Popular Movement of the Revolution), and a series of essentially rubber-stamping institutions.
One particularity of the Regime was the claim to be thriving for an "authentic" system, different from Western, or Soviet influences. This lasted roughly between the establishment of Zaire in 1971, and the official beginning of the transition towards democracy, on 24 April 1990. This was true at the regular people's level as everywhere else. People were ordered by law to drop their Western Christian names; the titles Mr. and Mrs. were abandoned for the male and female versions of the French word for "citizen"; Men were forbidden to wear suits, and women to wear pants. At the institutional level, many of the institutions also changed denominations, but the end result was a system that borrowed from both systems:
Every corporation, whether financial or union, as well as every division of the administration, were set up as branches of the party, the CEOs, Union leaders, and division directors being sworn-in as section presidents of the party. Every aspect of life was regulated to some degree by the party, and the will of its founding-president, Mobutu Sese Seko.
Most of the petty aspects of the regime disappeared after 1990, and the beginning of the democratic transition. The latter was intended to be fairly short-lived, but Mobutu's power plays dragged it in length, to ultimately 1997, when the forces-led by Laurent Kabila eventually toppled the regime, after a 9-month-long successful military campaign.
The Kabilas' governments and war.
The government of former president Mobutu Sese Seko was toppled by a rebellion led by Laurent Kabila in May 1997, with the support of Rwanda and Uganda. They were later to turn against Kabila and backed a rebellion against him in August 1998. Troops from Zimbabwe, Angola, Namibia, Chad, and Sudan intervened to support the Kinshasa regime. A cease-fire was signed on 10 July 1999 by the DROC, Zimbabwe, Angola, Uganda, Namibia, Rwanda, and Congolese armed rebel groups, but fighting continued.
Under Laurent Kabila's regime, all executive, legislative, and military powers were first vested in the President, Laurent-Désiré Kabila. The judiciary was independent, with the president having the power to dismiss or appoint. The president was first head of a 26-member cabinet dominated by the Alliance of Democratic Forces for the Liberation of Congo (ADFL). Towards the end of the 90s, Laurent Kabila created and appointed a Transitional Parliament, with a seat in the buildings of the former Katanga Parliament, in the southern town of Lubumbashi, in a move to unite the country, and to legitimate his regime. Kabila was assassinated on 16 January 2001 and his son Joseph Kabila was named head of state ten days later.
The younger Kabila continued with his father's Transitional Parliament, but overhauled his entire cabinet, replacing it with a group of technocrats, with the stated aim of putting the country back on the track of development, and coming to a decisive end of the Second Congo War. In October 2002, the new president was successful in getting occupying Rwandan forces to withdraw from eastern Congo; two months later, an agreement was signed by all remaining warring parties to end the fighting and set up a Transition Government, the make-up of which would allow representation for all negotiating parties. Two founding documents emerged from this: The , and the Global and Inclusive Agreement, both of which describe and determine the make-up and organization of the Congolese institutions, until planned elections in July 2006, at which time the provisions of the new constitution, democratically approved by referendum in December 2005, will take full effect and that is how it happened.
Under the Global and All-Inclusive Agreement, signed on 17 December 2002, in Pretoria, there was to be one President and four Vice-Presidents, one from the government, one from the Rally for Congolese Democracy, one from the MLC, and one from civil society. The position of Vice-President expired after the 2006 elections.
Present situation.
After being for three years (2003–06) in the interregnum between two constitutions, the Democratic Republic of the Congo is now under the regime of the Constitution of the Third Republic. The constitution, adopted by referendum in 2005, and promulgated by President Joseph Kabila in February 2006, establishes a decentralized semi-presidential republic, with a separation of powers between the three branches of government - executive, legislative and judiciary, and a distribution of prerogatives between the central government and the provinces.
Executive branch.
Since the July 2006 elections, the country is led by a semi-presidential, strongly-decentralized state. The executive at the central level, is divided between the President, and a Prime Minister appointed by him/her from the party having the majority of seats in Parliament. Should there be no clear majority, the President can appoint a "government former" that will then have the task to win the confidence of the National Assembly. The President appoints the government members (ministers) at the proposal of the Prime Minister. In coordination, the President and the government have the charge of the executive. The Prime minister and the government are responsible to the lower-house of Parliament, the National Assembly.
At the province level, the Provincial legislature (Provincial Assembly) elects a governor, and the governor, with his government of up to 10 ministers, is in charge of the provincial executive. Some domains of government power are of the exclusive provision of the Province, and some are held concurrently with the Central government. This is not a Federal state however, simply a decentralized one, as the majority of the domains of power are still vested in the Central government. The governor is responsible to the Provincial Assembly.
Criticisms.
The semi-presidential system has been described by some as "conflictogenic" and "dictatogenic", as it ensures frictions, and a reduction of pace in government life, should the President and the Prime Minister be from different sides of the political arena. This was seen several times in France, a country that shares the semi-presidential model. It was also, arguably, in the first steps of the Congo into independence, the underlying cause of the crisis between Prime Minister Patrice Lumumba and President Joseph Kasa Vubu, who ultimately dismissed each other, in 1960.
Legislative branch.
Under the Transition Constitution.
The Inter-Congolese dialogue, that set-up the transitional institutions, created a bicameral parliament, with a National Assembly and Senate, made up of appointed representatives of the parties to the dialogue. These parties included the preceding government, the rebel groups that were fighting against the government, with heavy Rwandan and Ugandan support, the internal opposition parties, and the Civil Society. At the beginning of the transition, and up until recently, the National Assembly is headed by the MLC with Speaker Hon. Olivier Kamitatu, while the Senate is headed by a representative of the Civil Society, namely the head of the Church of Christ in Congo, Mgr. Pierre Marini Bodho. Hon. Kamitatu has since left both the MLC and the Parliament to create his own party, and ally with current President Joseph Kabila. Since then, the position of Speaker is held by Hon. Thomas Luhaka, of the MLC.
Aside from the regular legislative duties, the Senate had the charge to draft a new constitution for the country. That constitution was adopted by referendum in December 2005, and decreed into law on 18 February 2006.
Under the New Constitution.
The Parliament of the third republic is also bicameral, with a National Assembly and a Senate. Members of the National Assembly, the lower - but the most powerful - house, are elected by direct suffrage. Senators are elected by the legislatures of the 26 provinces.
Administrative divisions.
Under the Transition Constitution.
10 provinces (provinces, singular - province) and one city* (ville): Bandundu, Bas-Congo, Équateur, Kasai-Occidental, Kasai-Oriental, Katanga, Kinshasa*, Maniema, North Kivu, Orientale, South Kivu.
Each province is divided into districts.
Under the New Constitution.
25 provinces (provinces, singular - province) and city* (ville): Bas-Uele | Équateur | Haut-Lomami | Haut-Katanga | Haut-Uele | Ituri | Kasaï | Kasaï oriental | Kongo central | Kwango | Kwilu | Lomami | Lualaba | Lulua | Mai-Ndombe | Maniema | Mongala | North Kivu | Nord-Ubangi | Sankuru | South Kivu | Sud-Ubangi | Tanganyika | Tshopo | Tshuapa | Kinshasa*
International organization participation.
ACCT, ACP, AfDB, AU, CEEAC, CEPGL, ECA, FAO, G-19, G-24, G-77, IAEA, IBRD, ICAO, ICC, ICRM, IDA, IFAD, IFC, IFRCS, IHO, ILO, IMF, International Maritime Organization, Intelsat, Interpol, IOC, IOM, ITU, ITUC, NAM, OPCW (signatory), PCA, SADC, UN, UNCTAD, UNESCO, UNHCR, UNIDO, UPU, WCO WFTU, WHO, WIPO, WMO, WToO, WTrO

</doc>
<doc id="8027" url="http://en.wikipedia.org/wiki?curid=8027" title="Telecommunications in the Democratic Republic of the Congo">
Telecommunications in the Democratic Republic of the Congo

Telecommunications in the Democratic Republic of the Congo include radio, television, fixed and mobile telephones, and the Internet.
Radio and television.
Radio is the dominant medium; a handful of stations, including state-run Radio-Télévision Nationale Congolaise (RTNC), broadcast across the country. The United Nations Mission (MONUSCO) and a Swiss-based NGO, Fondation Hirondelle, operate one of country's leading stations, Radio Okapi. The network employs mostly-Congolese staff and aims to bridge political divisions. Radio France Internationale (RFI), which is widely available on FM, is the most popular news station. The BBC broadcasts on FM in Kinshasa (92.7), Lubumbashi (92.0), Kisangani (92.0), Goma (93.3) and Bukavu (102.2).

</doc>
<doc id="8028" url="http://en.wikipedia.org/wiki?curid=8028" title="Transport in the Democratic Republic of the Congo">
Transport in the Democratic Republic of the Congo

Ground transport in the Democratic Republic of Congo (DRC) has always been difficult. The terrain and climate of the Congo Basin present serious barriers to road and rail construction, and the distances are enormous across this vast country. Furthermore, chronic economic mismanagement and internal conflict has led to serious under-investment over many years.
On the other hand, the DRC has thousands of kilometres of navigable waterways, and traditionally water transport has been the dominant means of moving around approximately two-thirds of the country.
Transport problems.
As an illustration of transport difficulties in the DRC, even before wars damaged the infrastructure, the so-called "national" route, used to get supplies to Bukavu from the seaport of Matadi, consisted of the following:
In other words, goods had to be loaded and unloaded eight times and the total journey would take many months.
Many of the routes listed below are in poor condition and may be operating at only a fraction of their original capacity (if at all), despite recent attempts to make improvements. Up to 2006 the United Nations Joint Logistics Centre (UNJLC) had an operation in Congo to support humanitarian relief agencies working there, and its bulletins and maps about the transport situation are archived on the UNJLC web site.
The First and Second Congo Wars saw great destruction of transport infrastructure from which the country has not yet recovered. Many vehicles were destroyed or commandeered by militias, especially in the north and east of the country, and the fuel supply system was also badly affected. Consequently, outside of Kinshasa, Matadi and Lubumbashi, private and commercial road transport is almost non-existent and traffic is scarce even where roads are in good condition. The few vehicles in use outside these cities are run by the United Nations, aid agencies, the DRC government, and a few larger companies such as those in the mining and energy sectors. It is notable that high-resolution satellite photos on the Internet show large cities such as Bukavu, Butembo and Kikwit virtually devoid of traffic, compared to similar photos of towns in neighbouring countries.
Air transport is the only effective means of moving between many places within the country. The Congolese government, the United Nations, aid organisations and large companies use air rather than ground transport to move personnel and freight. The UN operates a large fleet of aircraft and helicopters, and compared to other African countries the DRC has a large number of small domestic airlines and air charter companies. The transport (and smuggling) of minerals with a high value for weight is also carried out by air, and in the east, some stretches of paved road isolated by destroyed bridges or impassable sections have been turned into airstrips.
For the ordinary citizen though, especially in rural areas, often the only options are to cycle, walk or go by dugout canoe.
Some parts of the DRC are more accessible from neighbouring countries than from Kinshasa. For example Bukavu itself and Goma and other north-eastern towns are linked by paved road from the DRC border to the Kenyan port of Mombasa, and most goods for these cities have been brought via this route in recent years. Similarly, Lubumbashi and the rest of Katanga Province is linked to Zambia, through which the paved highway and rail networks of Southern Africa can be accessed. Such links through neighbouring countries are generally more important for the east and south-east of the country, and are more heavily used, than surface links to the capital.
Major infrastructure programs.
In 2007 China agreed to lend the DRC US$5bn for two major transport infrastructure projects to link mineral-rich Katanga, specifically Lubumbashi, by rail to an ocean port (Matadi) and by road to the Kisangani river port, and to improve its links to the transport network of Southern Africa in Zambia. The two projects would also link the major parts of the country not served by water transport, and the main centres of the economy. Loan repayments will be from concessions for raw materials which China desperately needs: copper, cobalt, gold and nickel, as well as by toll revenues from the road and railway. In the face of reluctance by the international business community to invest in DRC, this represents a revitalisation of DRC's infrastructure much needed by its government.
The China Railway Seventh Group Co. Ltd will be in charge of the contract, under signed by the China Railway Engineering Corporation, with construction to be started from June 2008.
Highways.
The Democratic Republic of the Congo has fewer all-weather paved highways than any country of its population and size in Africa — a total of 2250 km, of which only 1226 km is in good condition (see below). To put this in perspective, the road distance across the country in any direction is more than 2500 km (e.g. Matadi to Lubumbushi, 2700 km by road). The figure of 2250 km converts to 35 km of paved road per 1,000,000 of population. Comparative figures for Zambia and Botswana are 721 km and 3427 km respectively.
Categories.
The road network is theoretically divided into four categories (national roads, priority regional roads, secondary regional roads and local roads), however, the United Nations Joint Logistics Centre (UNJLC) reports that this classification is of little practical use because some roads simply do not exist. For example, National Road 9 is not operational and cannot be detected by remote sensing methods.
The two principal highways are:
Inventory.
The total road network in 2005, according to the UNJLC, consisted of:
The UNJLC also points out that the pre-Second Congo War network no longer exists, and is dependent upon 20,000 bridges and 325 ferries, most of which are in need of repair or replacement. In contrast, a Democratic Republic of the Congo government document shows that, also in 2005, the network of main highways in good condition was as follows:
The 2000 Michelin "Motoring and Tourist Map 955 of Southern and Central Africa", which categorizes roads as "surfaced", "improved" (generally unsurfaced but with gravel added and graded), "partially improved" and "earth roads" and "tracks" shows that there were 2694 km of paved highway in 2000. These figures indicate that, compared to the more recent figures above, there has been a deterioration this decade, rather than improvement.
International highways.
Three routes in the Trans-African Highway network pass through DR Congo:
Waterways.
The DRC has more navigable rivers and moves more passengers and goods by boat and ferry than any other country in Africa. Kinshasa, with 7 km of river frontage occupied by wharfs and jetties, is the largest inland waterways port on the continent. However, much of the infrastructure — vessels and port handling facilities — has, like the railways, suffered from poor maintenance and internal conflict.
The total length of waterways is estimated at 15,000 km including the Congo River, its tributaries, and unconnected lakes.
The 1000-kilometre Kinshasa-Kisangani route on the Congo River is the longest and best-known. It is operated by river tugs pushing several barges lashed together, and for the hundreds of passengers and traders these function like small floating towns. Rather than mooring at riverside communities along the route, traders come out by canoe and small boat alongside the river barges and transfer goods on the move.
Most waterway routes do not operate to regular schedules. It is common for an operator to moor a barge at a riverside town and collect freight and passengers over a period of weeks before hiring a river tug to tow or push the barge to its destination.
Domestic links via inland waterways.
The middle Congo River and its tributaries from the east are the principal domestic waterways in the DRC. The two principal river routes are:
See the diagrammatic transport map above for other river waterways.
The most-used domestic lake waterways are:
Most large Congo river Ferry boats were destroyed during the civil war. Only smaller boats are running and they are irregular.
Pipelines.
petroleum products 390 km
Merchant marine.
1 petroleum tanker
Airports.
Kemal Saiki, a United Nations spokesman, said that the Democratic Republic of the Congo does not "even have 2,000 miles of roads" and that many people traveling around the country fly on aircraft.
The main airlines of the country are Hewa Bora Airways, Bravo Air Congo, and Wimbi Dira Airways. All of their hubs are at Kinshasa's N'djili Airport
The country had 229 airports in 2002 and 232 around 1999.
Airports - with paved runways.
<br>"total:"
24
<br>"over 3,047 m:"
4
<br>"2,438 to 3,047 m:"
2
<br>"1,524 to 2,437 m:"
16
<br>"914 to 1,523 m:"
2 (2002 est.)
Airports - with unpaved runways.
<br>"total:"
205
<br>"1,524 to 2,437 m:"
19
<br>"914 to 1,523 m:"
95
<br>"under 914 m:"
91 (2002 est.)
Transport safety and incidents.
All air carriers certified by the Democratic Republic of the Congo have been banned from operating at airports in the European Community by the European Commission because of inadequate safety standards.
Rocketry.
The Democratic Republic of the Congo has a rocketry program called Troposphere.

</doc>
<doc id="8029" url="http://en.wikipedia.org/wiki?curid=8029" title="Military of the Democratic Republic of the Congo">
Military of the Democratic Republic of the Congo

The Armed Forces of the Democratic Republic of Congo () is the state organisation responsible for defending the Democratic Republic of the Congo. The FARDC is being rebuilt as part of the peace process which followed the end of the Second Congo War in July 2003.
The majority of FARDC members are land forces, but it also has a small air force and an even smaller navy. Together the three services may number between 144,000 and 159,000 personnel. In addition, there is a presidential force called the Republican Guard, but it and the National Congolese Police (PNC) are not part of the Armed Forces.
The government in the capital city Kinshasa, the United Nations, the European Union, and bilateral partners which include Angola, South Africa, and Belgium are attempting to create a viable force with the ability to provide the Democratic Republic of Congo with stability and security. However, this process is being hampered by corruption, inadequate donor coordination, and competition between donors. The various military units now grouped under the FARDC banner are some of the most unstable in Africa after years of war and underfunding.
To assist the new government, since February 2000 the United Nations has had the United Nations Mission in the Democratic Republic of Congo (now called MONUSCO), which currently has a strength of over 16,000 peacekeepers in the country. Its principal tasks are to provide security in key areas, such as the South Kivu and North Kivu in the east, and to assist the government in reconstruction. Foreign rebel groups are also in the Congo, as they have been for most of the last half-century. The most important is the Democratic Forces for the Liberation of Rwanda (FDLR), against which Laurent Nkunda's troops were fighting, but other smaller groups such as the anti-Ugandan Lord's Resistance Army are also present.
The legal standing of the FARDC was laid down in the Transitional Constitution, articles 118 and 188. This was then superseded by provisions in the , articles 187 to 192. Law 04/023 of November 12, 2004 establishes the General Organisation of Defence and the Armed Forces. In mid-2010, the Congolese Parliament was debating a new defence law, provisionally designated Organic Law 130.
History.
The first organized Congolese troops, known as the , were created in 1888 when King Leopold II of Belgium, who held the Congo Free State as his private property, ordered his Secretary of the Interior to create military and police forces for the state. In 1908, under international pressure, Leopold ceded administration of the colony to the government of Belgium as the Belgian Congo. It remained under the command of a Belgian officer corps through to the independence of the colony in 1960. The "Force Publique" saw combat in Cameroun, and successfully invaded and conquered areas of German East Africa, notably present day Rwanda, during World War I. Elements of the "Force Publique" were also used to form Belgian colonial units that fought in the East African Campaign during World War II.
At independence on 30 June 1960, the army suffered from a dramatic deficit of trained leaders, particularly in the officer corps. This was because the "Force Publique" had always only been officered by Belgian or other expatriate whites. The Belgian Government made no effort to train Congolese commissioned officers until the very end of the colonial period and there were only about 20 African cadets in training on the eve of independence. Ill-advised actions by Belgian officers led to an enlisted ranks' rebellion on 5 July 1960, which helped spark the Congo Crisis. Lieutenant General Émile Janssens, the "Force Publique" commander, wrote during a meeting of soldiers that 'Before independence=After Independence', pouring cold water on the soldiers' desires for an immediate raise in their status.
Vanderstraeten says that on the morning of 8 July 1960, following a night during which all control had been lost over the soldiers, numerous ministers arrived at Camp Leopold with the aim of calming the situation. Both Lumumba and Kasa-Vubu eventually arrived, and the soldiers listened to Kasa-Vubu "religiously." After his speech, Kasa-Vubu and the ministers present retired into the camp canteen to hear a delegation from the soldiers. Vanderstraeten says that, according to Joseph Ileo, their demands ("revendications") included the following:
The "laborious" discussions which then followed were later retrospectively given the label of an "extraordinary ministerial council." Gérald-Libois writes that '..the special meeting of the council of ministers took steps for the immediate Africanisation of the officer corps and ..named Victor Lundula, who was born in Kasai and was burgomaster of Jadotville, as Commander-in-Chief of the "Armée Nationale Congolaise" (ANC); Colonel Joseph-Désiré Mobutu as chief of staff; and the Belgian, Colonel Henniquiau, as chief advisor to the ANC.' Thus General Janssens was dismissed. Both Lundula and Mobutu were former sergeants of the "Force Publique". It appears that Maurice Mpolo, Minister of Youth and Sports, was given the defence portfolio.
On 8–9 July 1960, the soldiers were invited to appoint black officers, and 'command of the army passed securely into the hands of former sergeants,' as the soldiers in general chose the most-educated and highest-ranked Congolese army soldiers as their new officers. Most of the Belgian officers were retained as advisors to the new Congolese hierarchy, and calm returned to the two main garrisons at Leopoldville and Thysville. The "Force Publique" was renamed the "Armée nationale congolaise" (ANC), or Congolese National Armed Forces. However in Katanga Belgian officers resisted the Africanisation of the army.
On 9 July 1960, there was an "Force Publique" mutiny at Camp Massart at Elizabethville; five or seven Europeans were killed. The army revolt and resulting rumours caused severe panic across the country, and Belgium despatched troops and the naval Task Group 218.2 to protect its citizens. Belgian troops intervened in Elisabethville and Luluabourg (10 July), Matadi (11 July), Leopoldville (13 July) and elsewhere. There were immediate suspicions that Belgium planned to re-seize the country while doing so. Large numbers of Belgian colonists fled the country. At the same time, on 9 July, Albert Kalonji proclaimed the independence of South Kasai. Two days later on 11 July, Moise Tshombe declared the independence of Katanga province in the south-east, closely backed by remaining Belgian administrators and soldiers.
On 14 July 1960, in response to requests by Prime Minister Lumumba, the UN Security Council adopted United Nations Security Council Resolution 143. This called upon Belgium to remove its troops and for the UN to provide 'military assistance' to the Congolese forces to allow them 'to meet fully their tasks'. Lumumba demanded that Belgium remove its troops immediately, threatening to seek help from the Soviet Union if they did not leave within two days. The UN reacted quickly and established the United Nations Operation in the Congo (ONUC). The first UN troops arrived the next day but there was instant disagreement between Lumumba and the UN over the new force's mandate. Because the Congolese army had been in disarray since the mutiny, Lumumba wanted to use the UN troops to subdue Katanga by force. Referring to the resolution, Lumumba wrote to UN Secretary General Dag Hammarskjöld, 'From these texts it is clear that, contrary to your personal interpretation, the UN force may be used to subdue the rebel government of Katanga.' Secretary General Hammarskjöld refused. To Hammarskjöld, the secession of Katanga was an internal Congolese matter and the UN was forbidden to intervene by Article 2 of the United Nations Charter. Disagreements over what the UN force could and could not do continued throughout its deployment.
The last Belgian troops left the country by 23 July, as United Nations forces continued to deploy throughout the Congo.
During the crucial period of July–August 1960, Joseph-Désiré Mobutu built up "his" national army by channeling foreign aid to units loyal to him, by exiling unreliable units to remote areas, and by absorbing or dispersing rival armies. He tied individual officers to him by controlling their promotion and the flow of money for payrolls. Researchers working from the 1990s have concluded that money was directly funnelled to the army by the U.S. Central Intelligence Agency, the UN, and Belgium. Despite this, by September 1960, following the four-way division of the country, there were four separate armed forces: Mobotu's ANC itself, numbering about 12,000, the South Kasai Constabulary loyal to Albert Kalonji (3,000 or less), the Katanga Gendarmerie which were part of Moise Tshombe's regime (totalling about 10,000), and the Stanleyville dissident ANC loyal to Antoine Gizenga (numbering about 8,000).
In August 1960, due to rejection of requests to the UN for aid to suppress the South Kasai and Katanga revolts, Lumumba's government decided to request Soviet help. de Witte writes that 'Leopoldville asked the Soviet Union for planes, lorries, arms, and equipment. .. Shortly afterwards, on 22 or 23 August, about 1,000 soldiers left for Kasai.' de Witte goes on to write that on 26–27 August, the ANC seized Bakwanga, Albert Kalonji's capital in South Kasai, without serious resistance. 'In the next two days it temporarily put an end to the secession of Kasai.'
The Library of Congress Country Study for the Congo says at this point that:
"[On 5 September 1960] Kasavubu also appointed Mobutu as head of the ANC. Joseph Ileo was chosen as the new prime minister and began trying to form a new government. Lumumba and his cabinet responded by accusing Kasa-Vubu of high treason and voted to dismiss him. Parliament refused to confirm the dismissal of either Lumumba or Kasavubu and sought to bring about a reconciliation between them. After a week's deadlock, Mobutu announced on September 14 that he was assuming power until December 31, 1960, in order to "neutralize" both Kasavubu and Lumumba."
In early January 1961, ANC units loyal to Lumumba invaded northern Katanga to support a revolt of Baluba tribesmen against Tshombe's secessionist regime.
United Nations Security Council Resolution 161 of 21 February 1961, called for the withdrawal of Belgian officers from command positions in the ANC, and the training of new Congolese officers with UN help. The various efforts made by ONUC to retrain the ANC from August 1960 to their effective end in June 1963 are described in Arthur House's book , pages 145-155. By March 1963 however, after the visit of Colonel Michael Greene of the United States Army, and the resulting 'Greene Plan,' the pattern of bilaterally agreed military assistance to various Congolese military components, instead of a single unified effort, was already taking shape.
In early 1964, a new crisis broke out as Congolese rebels calling themselves "Simba" (Swahili for "Lion") rebelled against the government. They were led by Pierre Mulele, Gaston Soumialot and Christophe Gbenye who were former members of Gizenga's Parti Solidaire Africain (PSA). The rebellion affected Kivu and Eastern (Orientale) provinces. By August they had captured Stanleyville and set up a rebel government there. As the rebel movement spread, discipline became more difficult to maintain, and acts of violence and terror increased. Thousands of Congolese were executed, including government officials, political leaders of opposition parties, provincial and local police, school teachers, and others believed to have been Westernized. Many of the executions were carried out with extreme cruelty, in front of a monument to Lumumba in Stanleyville. Tshombe decided to use foreign mercenaries as well as the ANC to suppress the rebellion. Mike Hoare was employed to created the English-speaking 5 Commando ANC at Kamina, with the assistance of a Belgian officer, Colonel Frederic Vanderwalle, while 6 Commando ANC was French-speaking and originally under the command of a Belgian Army colonel, Lamouline. By August 1964, the mercenaries, with the assistance of other ANC troops, were making headway against the Simba rebellion. Fearing defeat, the rebels started taking hostages of the local white population in areas under their control. These hostages were rescued in Belgian airdrops (Dragon Rouge and Dragon Noir) over Stanleyville and Paulis with U.S. airlift support. The operation coincided with the arrival of mercenary units (seemingly including the hurriedly-formed 5th Mechanised Brigade) at Stanleyville which was quickly captured. It took until the end of the year to completely put down the remaining areas of rebellion.
After five years of turbulence, in 1965 Mobutu used his position as ANC Chief of Staff to seize power in the Congo. Although Mobutu succeeded in taking power, his position was soon threatened by the Kisangani Mutinies, also known as the Stanleyville Mutinies or Mercenaries' Mutinies, which were eventually suppressed.
As a general rule, since that time, the armed forces have not intervened in politics as a body, rather being tossed and turned as ambitious men have shaken the country. In reality, the larger problem has been the misuse and sometimes abuse of the military and police by political and ethnic leaders.
On 16 May 1968 a parachute brigade of two regiments (each of three battalions) was formed which eventually was to grow in size to a full division.
Zaire 1971–1997.
The country was renamed Zaire in 1971 and the army was consequently designated the (FAZ). In 1971 the army's force consisted of the 1st Groupement at Kananga, with one guard battalion, two infantry battalions, and a gendarmerie battalion attached, and the 2nd Groupement (Kinshasa), the 3rd Groupement (Kisangani), the 4th Groupement (Lubumbashi), the 5th Groupement (Bukavu), the 6th Groupement (Mbandaka), and the 7th Groupement (Boma). Each was about the size of a brigade, and commanded by 'aging generals who have had no military training, and often not much positive experience, since they were NCOs in the Belgian Force Publique.' BY the late 1970s the number of groupements reached nine, one per administrative region. The parachute division (Division des Troupes Aéroportées Renforcées de Choc, DITRAC) operated semi-independently from the rest of the army.
In July 1972 a number of the aging generals commanding the "groupements" were retired. Général d'armée Louis Bobozo, and Generaux de Corps d'Armée Nyamaseko Mata Bokongo, Nzoigba Yeu Ngoli, Muke Massaku, Ingila Grima, Itambo Kambala Wa Mukina, Tshinyama Mpemba, and General de Division Yossa Yi Ayira, the last having been commander of the Kamina base, were all retired on 25 July 1972. Taking over as military commander-in-chief, now titled Captain General, was newly promoted General de Division Bumba Moaso, former commander of the parachute division.
A large number of countries supported the FAZ in the early 1970s. Three hundred Belgian personnel were serving as staff officers and advisors throughout the Ministry of Defence, Italians were supporting the Air Force, Americans were assisting with transport and communications, Israelis with airborne forces training, and there were British advisors with the engineers.
On 11 June 1975 several military officers were arrested in what became known as the "coup monte et manque." Amongst those arrested were Générals Daniel KATSUVA wa Katsuvira, Land Forces Chief of Staff, UTSHUDI Wembolenga, Commandant of the 2nd Military Region at Kalemie; FALLU Sumbu, Military Attaché of Zaïre in Washington, Colonel MUDIAYI wa Mudiayi, the military attaché of Zaïre in Paris, the military attache in Brussels, a paracommando battalion commander, and several others. The regime alleged these officers and others (including Mobutu's civil "secrétaire particulier") had plotted the assassination of Mobutu, high treason, and disclosure of military secrets, among other offences. The alleged coup was investigated by a revolutionary commission headed by Boyenge Mosambay Singa, at that time head of the Gendarmerie. Writing in 1988, Michael Schatzberg said the full details of the coup had yet to emerge.
In late 1975, Mobutu, in a bid to install a pro-Kinshasa government in Angola and thwart the Marxist Popular Movement for the Liberation of Angola (MPLA)'s drive for power, deployed FAZ armored cars, paratroopers, and three infantry battalions to Angola in support of the National Liberation Front of Angola (FNLA).
On 10 November 1975, an anti-Communist force made up of 1,500 FNLA fighters, 100 Portuguese Angolan soldiers, and two FAZ battalions passed near the city of Quifangondo, only 30 km north of Luanda, at dawn on 10 November. The force, supported by South African aircraft and three 140 mm artillery pieces, marched in a single line along the Bengo River to face an 800-strong Cuban force across the river. Thus the Battle of Quifangondo began. The Cubans and MPLA fighters bombarded the FNLA with mortar and 122 mm rockets, destroying most of the FNLA's armored cars and six Jeeps carrying antitank rockets in the first hour of fighting.
Mobutu's support for the FNLA policy backfired when the MPLA won in Angola. The MPLA, then, acting ostensibly at least as the (Front for the National Liberation of the Congo), occupied Zaire's Katanga Province, then known as Shaba, in March 1977, facing little resistance from the FAZ. This invasion is sometimes known as Shaba I. Mobutu had to request assistance, which was provided by Morocco in the form of regular troops who routed the MPLA and their Cuban advisors out of Katanga. The humiliation of this episode led to civil unrest in Zaire in early 1978, which the FAZ had to put down.
The poor performance of Zaire's military during Shaba I gave evidence of chronic weaknesses (which extend to this day). One problem was that some of the Zairian soldiers in the area had not received pay for extended periods. Senior officers often kept the money intended for the soldiers, typifying a generally disreputable and inept senior leadership in the FAZ. As a result, many soldiers simply deserted rather than fight. Others stayed with their units but were ineffective. During the months following the Shaba invasion, Mobutu sought solutions to the military problems that had contributed to the army's dismal performance. He implemented sweeping reforms of the command structure, including wholesale firings of high-ranking officers. He merged the military general staff with his own presidential staff and appointed himself chief of staff again, in addition to the positions of minister of defence and supreme commander that he already held. He also redeployed his forces throughout the country instead of keeping them close to Kinshasa, as had previously been the case. The Kamanyola Division, at the time considered the army's best formation, and considered the president's own, was assigned permanently to Shaba. In addition to these changes, the army's strength was reduced by 25 percent. Also, Zaire's allies provided a large influx of military equipment, and Belgian, French, and American advisers assisted in rebuilding and retraining the force.
Despite these improvements, a second invasion by the former Katangan gendarmerie, known as Shaba II in May–June 1978, was only dispersed with the despatch of the French 2e régiment étranger de parachutistes and a battalion of the Belgian Paracommando Regiment. Kamanyola Division units collapsed almost immediately. French units fought the Battle of Kolwezi to recapture the town from the FLNC. The U.S. provided logistical assistance.
In July 1975, according to the IISS Military Balance, the FAZ was made up of 14 infantry battalions, seven "Guard" battalions, and seven other infantry battalions variously designated as "parachute" (or possibly "commando"; probably the units of the new parachute brigade originally formed in 1968). There were also an armored car regiment and a mechanized infantry battalion. Organisationally, the army was made up of seven brigade groups and one parachute division. In addition to these units, a tank battalion was reported to have formed by 1979.
In January 1979 "General de Division" Boyenge Mosambay Singa was named as both military region commander and Region Commissioner for Shaba.
In 1984, a militarised police force, the Guard Civile, was formed. It was eventually commanded by Général d'armée Kpama Baramoto Kata.
Further details of FAZ operations in the 1980s and onwards can be found in John W. Turner's book "A Continent Ablaze."
Thomas Turner wrote in the late 1990s that.. '[m]ajor acts of violence, such as the killings that followed the 'Kasongo uprising' in Bandundu Region in 1978, the killings of diamond miners in Kasai-Oriental Region in 1979, and, more recently, the massacre of students in Lubumbashi in 1990, continued to intimidate the population.'
The authors of the Library of Congress Country Study on Zaire commented in 1992-93 that: "The maintenance status of equipment in the inventory has traditionally varied, depending on a unit's priority and the presence or absence of foreign advisers and technicians. A considerable portion of military equipment is not operational, primarily as a result of shortages of spare parts, poor maintenance, and theft. For example, the tanks of the 1st Armored Brigade often have a nonoperational rate approaching 70 to 80 percent. After a visit by a Chinese technical team in 1985, most of the tanks operated, but such an improved status generally has not lasted long beyond the departure of the visiting team. Several factors complicate maintenance in Zairian units. Maintenance personnel often lack the training necessary to maintain modern military equipment. Moreover, the wide variety of military equipment and the staggering array of spare parts necessary to maintain it not only clog the logistic network but also are expensive.
The most important factor that negatively affects maintenance is the low and irregular pay that soldiers receive, resulting in the theft and sale of spare parts and even basic equipment to supplement their meager salaries. When not stealing spare parts and equipment, maintenance personnel often spend the better part of their duty day looking for other ways to profit. American maintenance teams working in Zaire found that providing a free lunch to the work force was a good, sometimes the only, technique to motivate personnel to work at least half of the duty day.
The army's logistics corps is to provide logistic support and conduct direct, indirect, and depot-level maintenance for the FAZ. But because of Zaire's lack of emphasis on maintenance and logistics, a lack of funding, and inadequate training, the corps is understaffed, underequipped, and generally unable to accomplish its mission. It is organized into three battalions assigned to Mbandaka, Kisangani, and Kamina, but only the battalion at Kamina is adequately staffed; the others are little more than skeleton" units.
The poor state of discipline of the Congolese forces became apparent again in 1990. Foreign military assistance to Zaire ceased following the end of the Cold War and Mobutu deliberately allowed the military's condition to deteriorate so that it did not threaten his hold on power. Protesting low wages and lack of pay, paratroopers began looting Kinshasa in September 1991 and were only stopped after intervention by French ('Operation Baumier') and Belgian ('Operation Blue Beam') forces.
In 1993, according to the Library of Congress Country Studies, the 25,000-member FAZ ground forces consisted of one infantry division (with three infantry brigades); one airborne brigade (with three parachute battalions and one support battalion); one special forces (commando/counterinsurgency) brigade; the Special Presidential Division; one independent armored brigade; and two independent infantry brigades (each with three infantry battalions, one support battalion). These units were deployed throughout the country, with the main concentrations in Shaba Region (approximately half the force). The Kamanyola Division, consisting of three infantry brigades operated generally in western Shaba Region; the 21st Infantry Brigade was located in Lubumbashi; the 13th Infantry Brigade was deployed throughout eastern Shaba; and at least one battalion of the 31st Airborne Brigade stayed at Kamina. The other main concentration of forces was in and around Kinshasa: the 31st Airborne Brigade was deployed at N'djili Airport on the outskirts of the capital; the Special Presidential Division (DSP) resided adjacent to the presidential compound; and the 1st Armored Brigade was at Mbanza-Ngungu (in Bas-Congo, approximately 120 kilometers southwest of Kinshasa). Finally the 41st Commando Brigade was at Kisangani.
This superficially impressive list of units overstates the actual capability of the armed forces at the time. Apart from privileged formations such as the Presidential Division and the 31st Airborne Brigade, most units were poorly trained, divided and so badly paid that they regularly resorted to looting. What operational abilities the armed forces had were gradually destroyed by politicisation of the forces, tribalisation, and division of the forces, included purges of suspectedly disloyal groups, intended to allow Mobutu to divide and rule. All this occurred against the background of increasing deterioration of state structures under the kleptocratic Mobutu regime.
For a concise general description of the FAZ in the 1990s, see René Lemarchand, "The dynamics of violence in Central Africa", University of Pennsylvania Press, 2009, pages 226-228.
Mobutu's overthrow and after.
Much of the origins of the recent conflict in what is now the Democratic Republic of the Congo stems from the turmoil following the Rwandan Genocide of 1994, which then led to the Great Lakes refugee crisis. Within the largest refugee camps, beginning in Goma in Nord-Kivu, were Rwandan Hutu fighters, which were eventually organised into the Rassemblement Démocratique pour le Rwanda, who launched repeated attacks into Rwanda. Rwanda eventually backed Laurent-Désiré Kabila and his quickly organised Alliance of Democratic Forces for the Liberation of Congo in invading Zaire, aiming to stop the attacks on Rwanda in the process of toppling Mobutu's government. When the militias rebelled, backed by Rwanda, the FAZ, weakened as is noted above, proved incapable of mastering the situation and preventing the overthrow of Mobutu in 1997.
When Kabila took power in 1997, the country was renamed the Democratic Republic of the Congo and so the name of the national army changed once again, to the "Forces armées congolaises" (FAC). Tanzania sent six hundred military advisors to train Kabila's new army in May 1997. Command over the armed forces in the first few months of Kabila's rule was vague. Gérard Prunier writes that 'there was no minister of defence, no known chief of staff, and no ranks; all officers were Cuban-style 'commanders' called 'Ignace', 'Bosco', Jonathan', or 'James', who occupied connecting suites at the Intercontinental Hotel and had presidential list cell-phone numbers. None spoke French or Lingala, but all spoke Kinyarwanda, Swahili, and, quite often, English.' On being asked by Belgian journalist Colette Braeckman what was the actual army command structure apart from himself, Kabila answered 'We are not going to expose ourselves and risk being destroyed by showing ourselves openly... . We are careful so that the true masters of the army are not known. It is strategic. Please, let us drop the matter.' Kabila's new "Forces armées congolaises" were riven with internal tensions. The new FAC had Banyamulenge fighters from South Kivu, "kadogo" child soldiers from various eastern tribes, such as Thierry Nindaga, Safari Rwekoze, etc... [the mostly] Lunda Katangese Tigers of the former FNLC, and former FAZ personnel. Mixing these disparate and formerly warring elements together led to mutuny. On 23 February 1998, a mostly Banyamulenge unit mutiniued at Bukavu after its officers tried to disperse the soldiers into different units spread all around the Congo. By mid-1998, formations on the outbreak of the Second Congo War included the Tanzanian-supported 50th Brigade, headquartered at Camp Kokolo in Kinshasa, and the 10th Brigade — one of the best and largest units in the army — stationed in Goma, as well as the 12th Brigade in Bukavu. The declaration of the 10th Brigade's commander, former DSP officer Jean-Pierre Ondekane, on 2 August 1998 that he no longer recognised Kabila as the state's president was one of the factors in the beginning of the Second Congo War.
The FAC performed poorly throughout the Second Congo War and "demonstrated little skill or recognisable military doctrine". At the outbreak of the war in 1998 the Army was ineffective and the DRC Government was forced to rely on assistance from Angola, Chad, Namibia and Zimbabwe. As well as providing expeditionary forces, these countries unsuccessfully attempted to retrain the DRC Army. North Korea and Tanzania also provided assistance with training. During the first year of the war the Allied forces defeated the Rwandan force which had landed in Bas-Congo and the rebel forces south-west of Kinshasa and eventually halted the rebel and Rwandan offensive in the east of the DRC. These successes contributed to the Lusaka Ceasefire Agreement which was signed in July 1999. Following the Lusaka Agreement, in mid-August 1999 President Kabila issued a decree dividing the country into eight military regions. The first military region, Congolese state television reported, would consist of the two Kivu provinces, Orientale Province would form the second region, and Maniema and Kasai-Oriental provinces the third. Katanga and Équateur would fall under the fourth and fifth regions, respectively, while Kasai-Occidental and Bandundu would form the sixth region. Kinshasa and Bas-Congo would form the seventh and eighth regions, respectively. In November 1999 the Government attempted to form a 20,000-strong paramilitary force designated the People's Defence Forces. This force was intended to support the FAC and national police but never became effective.
1999-present.
The Lusaka Ceasefire Agreement was not successful in ending the war, and fighting resumed in September 1999. The FAC's performance continued to be poor and both the major offensives the Government launched in 2000 ended in costly defeats. President Kabila's mismanagement was an important factor behind the FAC's poor performance, with soldiers frequently going unpaid and unfed while the Government purchased advanced weaponry which could not be operated or maintained. The defeats in 2000 are believed to have been the cause of President Kabila's assassination in January 2001. Following the assassination, Joseph Kabila assumed the presidency and was eventually successful in negotiating an end to the war in 2002-2003.
The December 2002 Global and All-Inclusive Agreement devoted Chapter VII to the armed forces. It stipulated that the armed forces chief of staff, and the chiefs of the army, air force, and navy were not to come from the same warring faction. The new 'national, restructured and integrated' army would be made up from Kabila's government forces (the FAC), the RCD, and the MLC. Also stipulated in VII(b) was that the RCD-N, RCD-ML, and the Mai-Mai would become part of the new armed forces. An intermediate mechanism for physical identification of the soldiers, and their origin, date of enrolment, and unit was also called for (VII(c)). It also provided for the creation of a Conseil Superieur de la Defense (Superior Defence Council) which would declare states of siege or war and give advice on security sector reform, disarmament/demobilization, and national defence policy.
A decision on which factions were to name chiefs of staff and military regional commanders was announced on 19 August 2003 as the first move in military reform, superimposed on top of the various groups of fighters, government and former rebels. Kabila was able to name the armed forces chief of staff, Lieutenant General Liwanga Mata, who previously served as navy chief of staff under Laurent Kabila. Kabila was able to name the air force commander (John Numbi), the RCD-Goma received the Land Force commander's position (Sylvain Buki) and the MLC the navy (Dieudonne Amuli Bahigwa). Three military regional commanders were nominated by the former Kinshasa government, two commanders each by the RCD-Goma and the MLC, and one region commander each by the RCD-K/ML and RCD-N. However these appointments were announced for Kabila's "Forces armées congolaises" (FAC), not the later FARDC. Another report however says that the military region commanders were only nominated in January 2004, and that the troop deployment on the ground did not change substantially until the year afterward.
On 24 January 2004, a decree created the "Structure Militaire d'Intégration" (SMI, Military Integration Structure). Together with the SMI, CONADER also was designated to manage the combined "tronc commun" DDR element and military reform programme. The first post-Sun City military law appears to have been passed on 12 November 2004, which formally created the new national Forces Armées de la République Démocratique du Congo (FARDC). Included in this law was article 45, which recognized the incorporation of a number of armed groups into the FARDC, including the former government army Forces Armées Congolaises (FAC), ex-FAZ personnel also known as former President Mobutu's 'les tigres', the RCD-Goma, RCD-ML, RCD-N, MLC, the Mai-Mai, as well as other government-determined military and paramilitary groups.
Turner writes that the two most prominent opponents of military integration ("brassage") were Colonel Jules Mutebusi, a Munyamulenge from South Kivu, and Laurent Nkunda, a Rwandaphone Tutsi who Turner says was allegedly from Rutshuru in North Kivu. In May–June 2004 Mutebusi led a revolt against his superiors from Kinshasa in South Kivu. Nkunda began his long series of revolts against central authority by helping Mutebusi in May–June 2004. In November 2004 a Rwandan government force entered North Kivu to attack the FDLR, and, it seems, reinforced and resupplied RCD-Goma (ANC) at the same time. Kabila despatched 10,000 government troops to the east in response, launching an attack which was called 'Operation Bima.' In the midst of this tension, Nkunda's men launched attacks in North Kivu in December 2004.
There was another major personnel reshuffle on 12 June 2007. FARDC chief General Kisempia Sungilanga Lombe was replaced with General Dieudonne Kayembe Mbandankulu. General Gabriel Amisi Kumba retained his post as Land Forces commander. John Numbi, a trusted member of Kabila's inner circle, was shifted from air force commander to Police Inspector General. U.S. diplomats reported that the former Naval Forces Commander Maj. General Amuli Bahigua (ex-MLC) became the FARDC's Chief of Operations; former FARDC Intelligence Chief General Didier Etumba (ex-FAC) was promoted to Vice Admiral and appointed Commander of Naval Forces; Maj. General Rigobert Massamba (ex-FAC), a former commander of the Kitona air base, was appointed as Air Forces Commander; and Brig. General Jean-Claude Kifwa, commander of the Republican Guard, was appointed as a regional military commander.
Much of the east of the country remains insecure, however. In the far northeast this is due primarily to the Ituri conflict. In the area around Lake Kivu, primarily in North Kivu, fighting continues among the Democratic Forces for the Liberation of Rwanda and between the government FARDC and Laurent Nkunda's troops, with all groups greatly exacerbating the issues of internal refugees in the area of Goma, the consequent food shortages, and loss of infrastructure from the years of conflict. In 2009, several United Nations officials stated that the army is a major problem, largely due to corruption that results in food and pay meant for soldiers being diverted and a military structure top-heavy with colonels, many of whom are former warlords. In a 2009 report itemizing FARDC abuses, Human Rights Watch urged the UN to stop supporting government offensives against eastern rebels until the abuses ceased.
On 22 November 2012, Gabriel Amisi Kumba was suspended from his position in the Forces Terrestres by president Joseph Kabila due to an inquiry into his alleged role in the sale of arms to various rebel groups in the eastern part of the country, which may have implicated the rebel group M23. In December 2012 it was reported that members of Army units in the north east of the country are often not paid due to corruption, and these units rarely made against villages by the Lord's Resistance Army.
The FARDC deployed 850 soldiers and 150 PNC police officers as part of an international force in the Central African Republic, which the DRC borders to the north. The country had been in a state of civil war since 2012, when the president was ousted by rebel groups. The DRC was urged by French president Hollande to keep its troops in CAR.
In July 2014, the Congolese army carried out a joint operation with UN troops in the Masisi and Walikale territories of the North Kivu province. In the process, they liberated over 20 villages and a mine from rebel control, specifically, from the Mai Mai Cheka and the Alliance for the Sovereign and Patriotic Congo rebel groups.
Current organisation.
The President, Major General Joseph Kabila is the Commander-in-Chief of the Armed Forces. The Minister of Defence, formally Ministers of Defence, Disarmament, and Veterans (Ancien Combattants), with the French acronym MDNDAC, is Alexandre Luba Ntambo.
The Colonel Tshatshi Military Camp in the Kinshasa suburb of Ngaliema hosts the defence department and the Chiefs of Staff central command headquarters of the FARDC. Jane's data from 2002 appears inaccurate; there is at least one ammunition plant in Katanga.
Below the Chief of Staff, the current organisation of the FARDC is not fully clear. There is known to be a Military Intelligence branch - Service du Renseignement militaire (SRM), the former DEMIAP. The FARDC is known to be broken up into the Land Forces ("Forces Terrestres"), Navy and Air Force. The Land Forces are distributed around ten military regions, up from the previous eight, following the ten provinces of the country. There is also a training command, the Groupement des Écoles Supérieurs Militaires (GESM) or Group of Higher Military Schools, which, in January 2010, was under the command of Major General Marcellin Lukama. The Navy and Air Forces are composed of various "groupments" (see below). There is also a central logistics base.
It should be made clear also that Joseph Kabila does not trust the military; the Republican Guard is the only component he trusts. Major General John Numbi, former Air Force chief, now inspector general of police, ran a parallel chain of command in the east to direct the 2009 Eastern Congo offensive, Operation Umoja Wetu; the regular chain of command was by-passed. Previously Numbi negotiated the agreement to carry out the "mixage" process with Laurent Nkunda. Commenting on a proposed vote of no confidence in the Minister of Defence in September 2012, Baoudin Amba Wetshi of "lecongolais.cd" described Ntolo as a 'scapegoat'. Wetshi said that all key military and security questions were handled in total secrecy by the President and other civil and military personalities trusted by him, such as John Numbi, Gabriel Amisi Kumba ('Tango Four'), Delphin Kahimbi, and others such as Kalev Mutond and Pierre Lumbi Okongo.
Armed Forces Chiefs of Staff.
The available information on the following officers is incomplete and sometimes contradictory. In addition to armed forces chiefs of staff, in 1966 Lieutenant Colonel Ferdinand Malila was listed as Army Chief of Staff.
Command structure in January 2005.
Virtually all officers have now changed positions, but this list gives an outline of the present structure. Despite the planned subdivision of the country into more numerous provinces, the actual splitting of the former provinces has not taken place.
Land forces.
The land forces are made up of about 14 integrated brigades, of fighters from all the former warring factions which have gone through an "brassage" integration process (see next paragraph), and a not-publicly known number of non-integrated brigades which remain solely made up from single factions (the Congolese Rally for Democracy (RCD)'s "Armée national congolaise", the ex-government former Congolese Armed Forces (FAC), the ex-RCD KML, the ex-Movement for the Liberation of Congo, the armed groups of the Ituri conflict (the Mouvement des Révolutionnaires Congolais (MRC), Forces de Résistance Patriotique d'Ituri (FRPI) and the Front Nationaliste Intégrationniste (FNI)) and the Mai-Mai).
It appears that about the same time that Presidential Decree 03/042 of 18 December 2003 established the National Commission for Demobilisation and Reinsertion (CONADER), '..all ex-combatants were officially declared as FARDC soldiers and the then FARDC brigades [were to] rest deployed until the order to leave for "brassage."
The reform plan adopted in 2005 envisaged the formation of eighteen integrated brigades through the "brassage" process as its first of three stages. The process consists firstly of regroupment, where fighters are disarmed. Then they are sent to orientation centres, run by CONADER, where fighters take the choice of either returning to civilian society or remaining in the armed forces. Combatants who choose demobilisation receive an initial cash payment of US $110. Those who choose to stay within the FARDC are then transferred to one of six integration centres for a 45-day training course, which aims to build integrated formations out of factional fighters previously heavily divided along ethnic, political and regional lines. The centres are spread out around the country at Kitona, Kamina, Kisangani, Rumangabo and Nyaleke (within the Virunga National Park) in Nord-Kivu, and Luberizi (on the border with Burundi) in South Kivu. The process has suffered severe difficulties due to construction delays, administration errors, and the amount of travel former combatants have to do, as the three stages' centres are widely separated. Following the first 18 integrated brigades, the second goal is the formation of a ready reaction force of two to three brigades, and finally, by 2010 when MONUC is anticipated to have withdrawn, the creation of a Main Defence Force of three divisions.
In February 2008, the current reform plan was described as:
"The short term, 2008-2010, will see the setting in place of a Rapid Reaction Force; the medium term, 2008 -2015, with a Covering Force; and finally the long term, 2015-2020, with a Principal Defence Force." He added that the reform plan rests on a programme of synergy based on the four pillars of dissuasion, production, reconstruction and excellence. "The Rapid Reaction Force is expected to focus on dissuasion, through a Rapid Reaction Force of 12 battalions, capable of aiding MONUC to secure the east of the country and to realise constitutional missions," Defence Minister Chikez Diemu said.
Amid the other difficulties in building new armed forces for the DRC, in early 2007 the integration and training process was distorted as the DRC government under Kabila attempted to use it to gain more control over the dissident general Laurent Nkunda. A hastily-negotiated verbal agreement in Rwanda saw three government FAC brigades integrated with Nkunda's former ANC 81st and 83rd Brigades in what was called "mixage". "Mixage" brought multiple factions into composite brigades, but without the 45-day retraining provided by "brassage", and it seems that actually, the process was limited to exchanging battalions between the FAC and Nkunda brigades in North Kivu, without further integration. Due to Nkunda's troops having greater cohesion, Nkunda effectively gained control of all five brigades - not what the DRC central government had been hoping! However after Nkunda used the "mixage" brigades to fight the FDLR, strains arose between the FARDC and Nkunda-loyalist troops within the brigades and they fell apart in the last days of August 2007. The International Crisis Group says that 'by 30 August [2007] Nkunda's troops had left the mixed brigades and controlled a large part of the Masisi and Rutshuru territories' (of North Kivu).
Both formally integrated brigades and the non-integrated units continue to conduct arbitrary arrests, rapes, robbery, and other crimes and these human rights violations are "regularly" committed by both officers and members of the rank and file. Members of the Army also often strike deals to gain access to resources with the militias they are meant to be fighting.
The various brigades and other formations and units number at least 100,000 troops. The status of these brigades has been described as "pretty chaotic." A 2007 disarmament and repatriation study said "army units that have not yet gone through the process of brassage are usually much smaller than what they ought to be. Some non-integrated brigades have only 500 men (and are thus nothing more than a small battalion) whereas some battalions may not even have the size of a normal company (over a 100 men)."
Known integrated brigades in 2007.
See also U.S. State Department, , 19 April 2007. Like the Force Publique in the Congo Free State, FARDC brigades have been deploying to their areas of operation with their families in tow. 2nd Commando Battalion of the Belgian Paracommando Brigade trained one of the first integrated brigades from January to June 2004. By 13 September 2006, the Government had established 13 out of the 18 integrated brigades it had planned to create before the elections. (S/2006/759, 21 September 2006, 12) A fourteenth brigade was created by March 2007. (S/2007/156, 20 March 2007, 7)
A number of outside donor countries are also carrying out separate training programmes for various parts of the Forces du Terrestres (Land Forces). The People's Republic of China has trained Congolese troops at Kamina in Katanga from at least 2004 to 2009, and the Belgian government is training at least one 'rapid reaction' battalion. When Kabila visited U.S. President George W. Bush in Washington D.C., he also asked the U.S. Government to train a battalion, and as a result, a private contractor, Protection Strategies Incorporated, started training a FARDC battalion at Camp Base, Kisangani, in February 2010. The company is being supervised by Special Operations Command-Africa Command. The various international training programmes are not well integrated.
Equipment.
Attempting to list the equipment available to the DRC's land forces is difficult; most figures are unreliable estimates based on known items delivered in the past. The IISS's Military Balance 2007 and Orbat.com's Concise World Armies 2005 give only slightly differing figures however (the figures below are from the IISS Military Balance 2007). Much of the Army's equipment is non-operational due to insufficient maintenance—in 2002 only 20 percent of the Army's armoured vehicles were estimated as being serviceable.
In addition to these 2007 figures, In March 2010, it was reported that the DRC's land forces had ordered USD $80 million worth of military equipment from Ukraine which included 20 T-72 main battle tanks, 100 trucks and various small arms. 20 x T-72 have been reported by World Defence Almanac. Tanks have been used in the Kivus in the 2005-9 period.
In February 2014, Ukraine revealed that it had achieved the first export order for the T-64 tank to the DRC Land Forces for 50 T-64BV-1s.
Republican Guard.
In addition to the other land forces, President Joseph Kabila also has a Republican Guard presidential force, formerly known as the Special Presidential Security Group (GSSP). FARDC military officials state that the Garde Républicaine is not the responsibility of FARDC, but the Head of State. Apart from Article 140 of the Law on the Army and Defence, no legal stipulation on the DRC's Armed Forces makes provision for the GR as a distinct unit within the national army. In February 2005, President Joseph Kabila passed a decree which appointed the GR's commanding officer and 'repealed any previous provisions contrary' to that decree. The GR is more than 10,000 strong (the ICG said 10,000–15,000 in January 2007), and has better working conditions and is paid regularly, but still commits rapes and robberies nearby their bases.
In an effort to extend his personal control across the country, Joseph Kabila has deployed the GR at key airports, ostensibly in preparation for an impending presidential visit. At the end of 2005, there were Guards deployed in Mbandaka, Kindu, Lubumbashi, Bukavu, Kolwezi, staying many months after the President had left. They are still deployed at Kisangani's Bangoka airport, where they appear to answer to no local commander and have caused trouble with MONUC troops there.
The GR is also supposed to undergo the integration process, but in January 2007, only one battalion had been announced as been integrated. Formed at a brassage centre in the Kinshasa suburb of Kibomango, the battalion included 800 men, half from the former GSSP and half from the MLC and RCD Goma.
Other forces active in the country.
There are currently large numbers of United Nations troops stationed in the DRC. The United Nations Organization Stabilization Mission in the Democratic Republic of the Congo (MONUSCO), on had a strength of over 19,000 peacekeepers (including 16,998 military personnel) and has a mission of assisting Congolese authorities maintain security. The UN and foreign military aid missions, the most prominent being EUSEC RD Congo, are attempting to assist the Congolese in rebuilding the armed forces, with major efforts being made in trying to assure regular payment of salaries to armed forces personnel and also in military justice. Retired Canadian Lieutenant General Marc Caron also served for a time as Security Sector Reform advisor to the head of MONUC.
Groups of anti-Rwandan government rebels like the FDLR, and other foreign fighters remain inside the DRC. The FDLR which is the greatest concern, was some 6,000 strong, in July 2007. By late 2010 the FDLR's strength however was estimated at 2,500. The other groups are smaller: the Ugandan Lord's Resistance Army, the Ugandan rebel group the Allied Democratic Forces in the remote area of Mt Rwenzori, and the Burundian Parti pour la Libération du Peuple Hutu—Forces Nationales de Liberation (PALIPEHUTU-FNL).
Finally there is a government paramilitary force, created in 1997 under President Laurent Kabila. The National Service is tasked with providing the army with food and with training the youth in a range of reconstruction and developmental activities. There is not much further information available, and no internet-accessible source details the relationship of the National Service to other armed forces bodies; it is not listed in the constitution. President Kabila, in one of the few comments available, says National Service will provide a gainful activity for street children. Obligatory civil service administered through the armed forces was also proposed under the Mobutu regime during the 'radicalisation' programme of December 1974-January 1975; the FAZ was opposed to the measure and the plan 'took several months to die.'
Air Force.
All military aircraft in the DRC are operated by the Air Force. Jane's World Air Forces states that the Air Force has an estimated strength of 1,800 personnel and is organised into two Air Groups. These Groups command five wings and nine squadrons, of which not all are operational. 1 Air Group is located at Kinshasa and consists of Liaison Wing, Training Wing and Logistical Wing and has a strength of five squadrons. 2 Tactical Air Group is located at Kaminia and consists of Pursuit and Attack Wing and Tactical Transport Wing and has a strength of four squadrons. Foreign private military companies have reportedly been contracted to provide the DRC's aerial reconnaissance capability using small propeller aircraft fitted with sophisticated equipment. Jane's states that National Air Force of Angola fighter aircraft would be made available to defend Kinshasa if it came under attack.
Like the other services, the Congolese Air Force is not capable of carrying out its responsibilities. Few of the Air Force's aircraft are currently flyable or capable of being restored to service and it is unclear whether the Air Force is capable of maintaining even unsophisticated aircraft. Moreover, Jane's states that the Air Force's Ecole de Pilotage is 'in near total disarray' though Belgium has offered to restart the Air Force's pilot training program.
Navy.
The 2002 edition of "Jane's Sentinel" described the Navy as being "in a state of near total disarray" and stated that it did not conduct any training or have operating procedures. The Navy shares the same discipline problems as the other services. It was initially placed under command of the MLC when the transition began: the current situation is uncertain.
The 2007 edition of "Jane's Fighting Ships" states that the Navy is organised into four commands, based at Matadi, near the coast; the capital Kinshasa, further up the Congo river; Kalemie, on Lake Tanganyika; and Goma, on Lake Kivu.
The IISS, in its 2007 edition of the "Military Balance", confirms the bases listed in "Jane's" and adds a fifth base at Boma, a coastal city near Matadi.
Various sources also refer to numbered Naval Regions. Operations of the 1st Naval Region have been reported in Kalemie, the 4th near the northern city of Mbandaka, and the 5th at Goma.
The IISS lists the Navy at 1,000 personnel and a total of eight patrol craft, of which only one is operational, a Shanghai II Type 062 class gunboat designated "102". There are five other 062s as well as two Swiftships which are not currently operational, though some may be restored to service in the future. According to "Jane's", the Navy also operates barges and small craft armed with machine guns.
Before the downfall of Mobutu, a small navy operated on the Congo river. One of its installations was at the village of N'dangi near the presidential residence in Gbadolite. The port at N'dangi was the base for several patrol boats, helicopters and the presidential yacht.

</doc>
<doc id="8032" url="http://en.wikipedia.org/wiki?curid=8032" title="Geography of Denmark">
Geography of Denmark

Denmark is a Nordic country located in Northern Europe. It consists of the Jutland peninsula and several islands in the Baltic sea, referred to as the Danish Archipelago. Denmark is located southwest of Sweden and due south of Norway and is bordered by the German state (and former possession) Schleswig-Holstein to the south, on Denmark's only land border, 68 kilometres (42 miles) long.
Denmark borders both the Baltic and North Seas along its tidal shoreline. Denmark's general coastline is much shorter, at , as it would, among other geographical features, not include most of the 1,419 offshore islands (each defined as exceeding 100 square meters in area) and the 180 km long Limfjorden, which separates Denmark's second largest island, North Jutlandic Island, 4,686 km2 in size, from the rest of Jutland.A circle enclosing the same area as Denmark would be 742 km (461 miles) long. Denmark has 443 named islands, of which 72 are inhabited (, Statistics Denmark).
Denmark experiences a temperate climate. This means that the winters are mild and windy and the summers are cool. The local terrain is generally flat with a few gently rolling plains. The territory of Denmark includes the island of Bornholm in the Baltic Sea and the rest of metropolitan Denmark, but excludes the Faroe Islands and Greenland. Its position gives Denmark complete control of the Danish Straits (Skagerrak and Kattegat) linking Baltic and North Seas. The country's natural resources include petroleum, natural gas, fish, salt, limestone, stone, gravel and sand.
Environment.
Land use.
Irrigated land: 4 350 km² (1993 est.)
Transnational issues.
Population.
 Denmark has a population of 5,543,453. About a quarter of Danes live in the capital Copenhagen.

</doc>
<doc id="8033" url="http://en.wikipedia.org/wiki?curid=8033" title="Demographics of Denmark">
Demographics of Denmark

This article is about the demographic features of the population of Denmark, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
National demographics.
According to 2012 figures from Statistics Denmark, 89.6% of Denmark’s population of over 5,580,516 was of Danish descent, defined as having at least one parent who was born in Denmark and has Danish citizenship. Many of the remaining 10.4% were immigrants—or descendants of recent immigrants (defined as people born in Denmark from migrant parents, or parents without Danish citizenship) —near half of whom are from the neighbouring Scandinavian countries and Germany. Others include people from Turkey, Iraq, Somalia, Bosnia and Herzegovina, South Asia, and from Western Asia.
More than 590 000 individuals (10.4%) are migrants and their descendants (142 000 second generation migrants born in Denmark).
Of these 590 000 immigrants and their descendants:
Ethnic groups.
Non-Scandinavian ethnic minorities include:
Historic minorities.
Ethnic minorities in Denmark include a handful of groups:
Vital statistics since 1900.
Data according to Statistics Denmark, which collects the official statistics for Denmark.
Religion.
The Church of Denmark () is state-supported and, according to statistics from January 2006, accounts for about 80% of Denmark's religious affiliation. Denmark has had religious freedom guaranteed since 1849 by the Constitution, and numerous other religions are officially recognised, including several Christian denominations, Muslim, Jewish, Buddhist, Hindu and other congregations as well as Forn Siðr, a revival of Scandinavian pagan tradition. The Department of Ecclesiastical Affairs recognizes roughly a hundred religious congregations for tax and legal purposes such as conducting wedding ceremonies.
For historical reasons, there is a formal distinction between 'approved' ("godkendte") and 'recognised' ("anerkendte") congregations of faith. The latter include 11 traditional denominations, such as Roman Catholics, the Reformed Church, the Mosaic Congregation, Methodists and Baptists, some of whose privileges in the country date hundreds of years back. These have the additional rights of having priests appointed by royal resolution and to christen/name children with legal effect.
Demographic statistics.
Population.
Denmark's population from 1769 to 2007.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population:
Age structure:
Median age:
Population growth rate:
Net migration rate:
Urbanization:
Sex ratio:
Infant mortality rate:
Life expectancy at birth:
Total fertility rate:
HIV/AIDS - adult prevalence rate:
HIV/AIDS - people living with HIV/AIDS:
HIV/AIDS - deaths:
Nationality:
Ethnic groups:
Religions:
Languages:
Literacy:
School life expectancy (primary to tertiary education):
Education expenditures:

</doc>
<doc id="8035" url="http://en.wikipedia.org/wiki?curid=8035" title="Economy of Denmark">
Economy of Denmark

Denmark has a diverse, mixed economy, but one that relies almost entirely on human resources, as there are few mineral resources available, except mature oil and gas wells in the North Sea. Cooperatives form a large part of some sectors, be it in housing, agriculture or retail. Foundations play a large role as owners of private sector companies. Denmark's nominal GDP was estimated to be $333,238 million, the 32nd largest in the world. It has the world's lowest level of income inequality, according to the World Bank Gini (%), and the world's highest minimum wage, according to the IMF. As of June 2010 the unemployment rate is at 7.4%, which is below the EU average of 9.6%. As of 28 February 2014 Denmark is among the countries with the highest credit rating.
Denmark's main exports are: industrial production/manufactured goods 73.3% (of which machinery and instruments were 21.4%, and fuels, chemicals, etc. 26%); agricultural products and others for consumption 18.7% (in 2009 meat and meat products were 5.5% of total export; fish and fish products 2.9%). Denmark is a net exporter of food and energy and has since the 1990s had a balance of payments surplus. The accumulated value of service and merchandise exports in 2013 amounted to 54% of GDP, and imports in 2013 amounted to 49% of GDP. Notable among the service exports are container shipping. There is no net foreign debt as other countries owe more money to Denmark than Denmark owes to them, but because of large deficits due to increased unemployment levels the "central" government has increased its debt level since the end of September 2008, when it stood at 21 percent (gross debt) of GDP, according to the central bank - in accordance with the Eurostat EMU- "gross" debt numbers, which only take liabilities into account. (See below (Budgets)). Taking assets into account as well "net" debt of the "central" government was 11 percent. The public sector as a "whole" had net assets of 108 billion kroner in 2008. Within the European Union, Denmark advocates a liberal trade policy. Its standard of living is average among the Western European countries - and for many years the most equally distributed as shown by the Gini coefficient - in the world, and the Danes devote 0.8% of gross national income (GNI) to foreign aid. It is a society based on consensus (dialogue and compromise) with the Danish Confederation of Trade Unions and the Confederation of Danish Employers in 1899 in "Septemberforliget" (The September Settlement) recognising each other's right to organise, thus, negotiate. The employer's right to hire and fire their employees "whenever" they find it necessary is recognised. There is no official minimum wage () set by the government; the minimum of wages () is determined by negotiations between the organisations of employers and employees. Denmark produces oil, natural gas, wind- and bio-energy. Its principal exports are machinery, instruments and food products. The US is Denmark's largest non-European trading partner, accounting for around 5% of total Danish merchandise trade. Aircraft, computers, machinery, and instruments are among the major US exports to Denmark. Among major Danish exports to the U.S. are industrial machinery, chemical products, furniture, pharmaceuticals, Lego and canned ham and pork.
Overview.
This thoroughly modern market economy features high-tech agriculture, up-to-date small-scale and corporate industry, extensive government welfare measures, comfortable living standards, and high dependence on foreign trade. Denmark is a net exporter of food. The center-left coalition government (1993–2001) concentrated on reducing the unemployment rate and turning the budget deficit into a surplus, as well as following the previous government's policies of maintaining low inflation and a current account surplus. The coalition also committed itself to maintaining a stable currency. The coalition lowered marginal income tax rates while maintaining overall tax revenues; boosted industrial competitiveness through labor market and tax reforms, increased research and development funds. The availability and duration of unemployment benefit has been restricted to four years and because of rapidly rising prices on housing this has led to an increase in poverty from below 4% in 1995 to 5% in 2006 according to the Danish Economic Council . Despite these cuts, the part of the public sector in Denmark which buys goods and services from the private sector and provides the public sector administration and direct service to the public - nursing institutions for the young or old, hospitals, schools, police, and so on. - has risen from 25.5% of GDP during the former government to 26% today and is projected to be at 26.5% in 2015 if current policies continue .
Denmark chose not to join the 11 other European Union members who launched the euro on 1 January 1999. Especially from 2006, economists and political pundits have expressed concern that the lack of skilled labour will result in higher pay increases and an overheating of the economy, which would repeat the boom-and-bust cycle in 1986, when government introduced a tax reform and restricted the private loan market because of a record balance-of-payments deficit. As a consequence, the trade balance showed a surplus in 1987, and the balance-of-payments in 1990 (first surplus since 1963). They have remained in surplus since, except for the balance of payments in 1998.
Welfare state.
Denmark has a broad-reaching welfare system, which ensures that all Danes receive tax-funded health care and unemployment insurance. Denmark ranked the first in the European pensions barometer survey for the past two years. The lowest-income group before retirement from the age of 65 receive 120% of their pre-retirement income in pension and miscellaneous subsidies.
The largest public sector (30% of the entire workforce on a full-time basis) is financed by the world's highest taxes. A value added tax of 25% is levied on the sale of most goods and services (including groceries). The income tax in Denmark ranges from 37.4% to 63% progressively, levied on 4 out of 10 full-time employees. Such high rates mean that 1,010,000 Danes before the end of 2008 (44% of all full-time employees) will be paying a marginal income tax of 63% and a combined marginal tax of 70.9% resulting in warnings from organisations such as the OECD. "TV2 (Denmark)" reported in April 2008 that abolishing the middle- and top-level income tax brackets would amount to two (2) and one (1) percent of public sector revenue, respectively, which equals one and a half percent of GDP. The public sector as a whole had a budget surplus of 4.4% of GDP in 2007, but the tax cuts would increase private consumption and the labor shortage, thus, resulting in a deficit on the trade balance and pressure to increase wages even further. Proceeds from selling ones home are not taxed, as the marginal tax rate on capital income from housing savings is around 0 percent. A survey by Standard & Poor's found that the total debt secured by mortgages in Danish homes amounts to 89.8% of GDP, which is above the debt level in other EU countries (and the USA at 74.6% of GDP).
Discussions on increasing the labor supply include abolishing a labor market arrangement called "efterløn" (eng.:early retirement pay), at the present (end of 3rd quarter 2008) with more than 130,000 participants (60 years until 64 years of age). Participation in this scheme is also open for self-employed people (farmers, fishermen, lawyers, and so on). Shortening the time unemployment benefit can be received (four years at the present), as an example, is also discussed. The Danish Economic Council in its 2008 spring report (27 May) proposes limiting the "dagpengeperiode" to 2.5 years, which is still half a year more than at present in Norway and one and a half years more than in Sweden, said in an interview by the chairman (da: "overvismand") (professor of economics, University of Copenhagen) Peter Birch Sørensen 27 May 2008 on the TV program "Deadline" (10.30 pm), channel DR2, the Danish Broadcasting Corporation.
Taxation and employment.
Taxation.
With a GDP of 1,642,215 million DKK and revenue from taxes and ownership at 803,693 million DKK (2006), 49.07% of GDP, it is of extreme importance what happens in the tax-financed part of the economy. According to newly revised statistics, Denmark had the world's highest tax level in 2005 and 2006, when tax revenue collected amounted to 50.7% and 49.1% of GDP respectively and also held this position 1970-74 and 1993-95. These figures do not include income from ownership.In 2013, the tax revenues collected amounted to 47.9 % of GDP.
Budgets.
The overall surpluses after operating and capital expenditure in the "whole" public sector for the years 2004-2008: (million DKK) 27,327; 77,362; 79,937; 75,560 ('07 preliminary); 69,140 ('08 estimate). The public sector debt-liabilities still outstanding 1 January 2008 in accordance with the Eurostat EMU-debt numbers (gross debt) are 440.9 billion DKK (26.0% of GDP). In spite of falling surpluses this debt is expected to fall until 2015. As of 2008 there is no "net" debt in the public sector as a whole, but instead net assets of 43 billion DKK. The central government is determined to pay off the debt as fast as possible, avoiding the temptation to increase spending which might overheat the economy (increase wages and eventually prices drastically) because of a short supply of skilled labor and in the end require financial austerity measures to cool off the economy. Reporting on the record low unemployment numbers of "under" 50,000 persons in April 2008 published 9.30 am 29 May by Statistics Denmark, "TV2" (Denmark), at 10 pm, with comments from Nordea Bank's (Denmark) chief economist Helge Pedersen, and "DR2" (Danish Broadcasting Corporation), at 10.30 pm stressed the danger of overheating the economy and keeping public sector spending in check or otherwise risk economical-political measures. Being surprised at how low unemployment was, the economist said (TV2) that compared with previous periods with such a low unemployment rate, a trade deficit was avoided mainly because of the oil export.
The EMU-"gross" debt was 730 billion DKK at the end of 1993, 80.1% of GDP. During the four-year period 2004-2007 the public sector EMU-gross debt fell from 43.8% (641.9 billion DKK) to 26.0% (440.9 billion DKK) of GDP. The budget surpluses were (in billion DKK) 1.9% (27.2), 5.0% (77.4), 4.8% (79.3), and 4.4% (74.6) of GDP, respectively.
Employment.
Public sector employment (full-time and part-time) has been relatively steady at more than 800,000 a year this first decade, making up around 38% of total full-time (28% of full-time and part-time) employment, whereas private sector employment has risen by over 300,000 since the 1990s to slightly over 2 million in 2007 (full-time and part-time). With the information based partly on payments to the "Arbejdsmarkedets Tillægspension" pension fund of "all" employees and insured but unemployed members of an unemployment fund in Denmark, full-time employment is calculated at over 2.3 million persons in the third quarter of 2007. The increase in the fourth quarter of 2007 from a year ago in the number of employed persons was 1.0% and the amount of hours worked was 2.9% higher.
The share of employees leaving jobs every year (for a new job, retirement or unemployment (unempl.:15% of job leavers)) in the private sector is around 30% (of 1.25 million), at more than 300,000 - a level also observed in the U.K. and U.S.- but much higher than in continental Europe, where the corresponding figure is around 10%, and in Sweden. This attrition can be very costly, with new and old employees requiring half a year to return to old productivity levels, but with attrition bringing the number of people that have to be fired down. Productivity increased at an average of 2.3% a year in 2004, 2005 and 2006, recently being revised upward from an average of just 0.9% and previously with a too high employment level estimated. The upward revision is good, because a high wage economy like Denmark's with very few valuable natural resources needs to be highly productive, or efficient, and innovative to compete with other countries for a market share in the global economy. However, according to OECD, the distortions imposed by a combined marginal tax wedge of 70% (60% income tax plus 25% VAT, not counting elevated excise duties on certain goods) are hurting productivity and in turn the country's competitiveness.
Public sector reform.
To gain synergies through economies of scale (critical mass) (greater professional and financial sustainability) and big item discounts and to offer a wider array of services closer to the public (be a one-stop place of access to the public sector not unlike the unitary councils), it was deemed necessary to merge the municipalities and other administrative entities in the public sector. This would also help alleviate the financial problems of depopulation due to limited job opportunities, high unemployment and aging and make introduction of new information technology more affordable With the tax burden at around half of GDP, a survey July 2008 found that 81% of Danes are of the opinion that the public sector can deliver more service for the same money, harnessing the advantages of the recent reform. Mainly from 1 January 2007, the new center-right government streamlined the public sector extensively by decreasing the number of administrative units drastically in the different tiers of government, that is, in the number of city court circuits (from 82 to 24), police districts (from 54 to 12), tax districts (before 2007 the responsibility of the municipalities;after that part of the central government Ministry of Taxation), reshuffling tasks among the three government levels and abolished the counties in "Kommunalreformen" ("The Municipal Reform" of 2007), thereby reducing the number of local and regional politicians by almost half to 2,522 (municipal councillors) (council elections November 2005;reduced in the 2009 elections to 2,468;in 2013 to 2,444) (1978: 4,735;1998: 4,685; reduced somewhat in council elections November 2001 (Bornholm)) and 205 (regional councillors) (1998: 374) respectively. Before 1970 (a previous reform in effect from 1 April that year) the number of councillors (both categories) was around 11,000 in around 1,000 parish municipalities ("sognekommuner"), being supervised by their county, and market city municipalities ("købstadskommuner"), the latter numbering 86 (including Bornholm whose county as an exception supervised the county's 6 market city municipalities (of 22 in total)) and not being part of a county but being supervised by the Interior Ministry. This distinction (having independent municipalities not being under county supervision) ending (except for Copenhagen, Frederiksberg and Bornholm (2003–06)) with the reform of 1970, the term municipality ("kommune") replaced the previous two terms, which are now never used except for historical purposes. The number of municipalities had been reduced when during the period from April 1962 to 1966 398 municipalities merged to form 118 voluntarily. The number of municipalities was the highest in 1965, at 1345, of which 88 were market city municipalities, including Copenhagen and Frederiksberg, and 1257 were parish municipalities . Many of the 275 municipalities after 1 April 1974 built large city halls to consolidate the administration, thus, changing the cityscape of Denmark. It also consolidated other municipal enterprises and the purchase of goods and services from the private sector, as will some of the present 98 municipalities over time."TV2"(Denmark) reported 24 September 2007, that SKI, a mutual purchasing service company for central government, regions, and municipalities, made purchases of 140 billion DKK (almost 9% of GDP) of goods and services in bulk every year, prompting private sector companies to complain over razorthin profit margins and that for instance innovative (but expensive) products and energy efficiency sometimes were better than a very low price.
Agriculture.
Denmark is home to various types of agricultural production. Within animal husbandry, it includes dairy and beef cattle, pigs, poultry and fur animals – primarily mink, all sectors with a major export. Regarding vegetable production, Denmark is a leading producer of grass-, clover- and horticultural seeds.
The Danish agricultural industry is characterized by freehold and family ownership but due to structural development farms have become fewer and larger. With modern trade patterns the profitability increasingly depends on global market trends. The arable land in Denmark is approximately 2,646,000 hectares, and the number of farms approximately 40,000, out of which approximately one third is owned by full-time farmers.
The agriculture is intensive with 64 per cent of the area being used for production. This equals production of food for 15 million people. The value of Danish agricultural export, including the agribusiness sector, has risen steadily in recent years and accounted for 16 billion Euros in 2011. The agriculture and food sector as a whole represents 20 per cent of total Danish commodity exports.
Animal production.
The tendency towards fewer and larger farms has been accompanied by an increased animal production using fewer resources pr. produced unit.
The number of dairy farmers is reduced to about 3,800 with an average herd size of 150 cows. The milk quota is 1,142 tonnes. Danish dairy farmers are among the largest and most modern in Europe. More than half of the cows live in new loose-housing systems. Export of dairy products accounts for more than 20 per cent of the total Danish agricultural export. The total number of cattle in 2011 was approximately 1.5 million. Of these 565,000 were dairy cows and 99,000 were suckler cows. The yearly number of slaughtering of beef cattle is around 550,000.
For more than 100 years the production of pigs and pig meat has been a major source of income in Denmark. The Danish pig industry is among the world’s leaders in areas such as breeding, quality, food safety, animal welfare and traceability creating the basis for Denmark being among the world’s largest pig meat exporters. Approximately 90 per cent of the production is exported. This accounts for almost half of all agricultural exports and for more than 5 per cent of Denmark’s total exports. About 4,200 farmers produce 28 million pigs annually. Of these 20.9 million are slaughtered in Denmark.
Fur animal production started in the 1930s in Denmark. Denmark is the world’s second largest producer of mink skin. In 2012, 1525 farmers produced 15.6 million mink skin of the highest quality. Approximately 98 per cent of the skins sold at Kopenhagen Fur Auction are exported. Fur ranges as Danish agriculture’s third largest export article. Special attention is given to the welfare of the mink, and regular “Open Farm” arrangements are made for the general public.
Two hundred professional producers are responsible for the Danish egg production, which was 66 million kg in 2011. Chickens for slaughter are often produced in units with 40,000 broilers. In 2012, 100 million chickens were slaughtered. In the minor productions of poultry, 13 million ducks, 1.4 million geese and 5.0 million turkeys were slaughtered in 2012.
Transport.
Significant investment has been made in building road and rail links between Copenhagen and Malmö, Sweden (the Øresund Bridge), and between Zealand and Funen (the Great Belt Fixed Link). The Copenhagen Malmö Port was also formed between the two cities as the common port for the cities of both nations.
The main railway operator is Danske Statsbaner (Danish State Railways) for passenger services and DB Schenker Rail for freight trains. The railway tracks are maintained by Banedanmark. Copenhagen has a small Metro system, the Copenhagen Metro and the greater Copenhagen area has an extensive electrified suburban railway network, the S-train.
Private vehicles are increasingly used as a means of transport. Because of the high registration tax (180%) and VAT (25%), and the world's highest income tax rate, new cars are very expensive. The purpose of the tax is to discourage car ownership. Whether a smaller fleet of aging cars is better than a larger fleet of modern cars is a matter for debate, however as the car fleet has increased by 45% over the last 30 years the effect of high taxation on the fleet size seems small. The motorway network now covers 1,111 km
In 2007, an attempt was made by the government to favour environmentally friendly cars by slightly reducing taxes on high mileage vehicles. However, this has had little effect, and in 2008 Denmark experienced an increase in the import of fuel inefficient old cars (mostly older than 10 years), primarily from Germany as their costs including taxes keeps these cars within the budget of many Danes.
Denmark is in a strong position in terms of integrating fluctuating and unpredictable energy sources such as wind power in the grid. It is this knowledge that Denmark now aims to exploit in the transport sector by focusing on intelligent battery systems (V2G) and plug-in vehicles.
Energy.
Denmark is a long time leader in wind energy and a prominent exporter of wind turbines, and Denmark derives 3.1% of its gross domestic product from renewable (clean) energy technology and energy efficiency, or around €6.5 billion ($9.4 billion). It has integrated fluctuating and unpredictable energy sources such as wind power into the grid. Denmark now aims to focus on intelligent battery systems (V2G) and plug-in vehicles in the transport sector.
Energinet.dk is the Danish national transmission system operator for electricity and natural gas.
Oil and Natural Gas.
Denmark has considerable sources of oil and natural gas in the North Sea and ranks as number 32 in the world among net exporters of crude oil.
Esbjerg is Denmark's main city for the oil and gas industry, this is because of its ideal location close to the north sea, which is where Denmark's Oil and gas deposits are found. Companies like Maersk, Ramboll, Stimwell Services, ABB, Schlumberger, COWI and Atkins all have offshore related activities in the city.
Greenland and the Faroe Islands.
Greenland suffered negative economic growth in the early 1990s, but since 1993 the economy has improved. A tight fiscal policy by the Greenland Home Rule Government since the late 1980s helped create a low inflation rate and surpluses in the public budget, but at the cost of rising foreign debt in the Home Rule Government's commercial entities. Since 1990, Greenland has registered a foreign trade deficit.
Following the closure of Greenland's last lead and zinc mine in 1989, Greenland's economy is solely dependent on the fishing and tourism and financial transfers from the Danish central government. Despite resumption of several interesting hydrocarbon and mineral exploration activities, it will take several years before production will begin. Greenland's shrimp fishery is by far the largest source of income, since cod catches have dropped to historically low levels. Greenland also has a prominent whaling industry, Greenlandic Inuit whalers catch around 175 whales per year, making them the third largest hunt in the world after Japan and Norway, though their take is small compared to those nations, who annually averaged around 730 and 590 whales respectively in 1998–2007.[12][13] The IWC treats the west and east coasts of Greenland as two separate population areas and sets separate quotas for each coast. The far more densely populated west coast accounts for over 90 percent of the catch. In a typical year around 150 minke and 10 fin whales are taken from west coast waters and around 10 minkes are from east coast waters. Since the fishing industry is in decline including whaling, tourism is the only sector offering any near-term potential, and even this is limited due to the short season and high costs. The public sector plays a dominant role in Greenland's economy. Grants from mainland Denmark and EU fisheries payments make up about one-half of the home-rule government's revenues. Recently, Greenland has seen interest from other countries due to the possibilities of large amounts of natural resources, which include: coal, iron ore, lead, zinc, molybdenum, diamonds, gold, platinum, niobium, tantalite, uranium, fish, seals, whales, hydropower, possible oil and gas.
The Faroe Islands also depend almost entirely on fisheries, tourism and related exports. Without Danish Government bailouts in 1992 and 1993, the Faroese economy would have gone bankrupt. Since 1995, the Faroese economy has seen a noticeable upturn, but remains extremely vulnerable. Recent off-shore oil finds close to the Faroese area give hope for Faroese deposits, too, which may form the basis for an economic rebound over the longer term. Like Greenland, the Faroe Islands are also known for its whaling industry, around 950 long-finned pilot whales (Globicephala melaena) are caught each year, mainly during the summer. Other species are not hunted, though occasionally Atlantic white-sided dolphin can be found among the pilot whales. The hunt is known as the Grindadráp. The Faroese whaling industry has recently been under attack by the media because of the way it is generally perceived.
Neither Greenland, nor the Faroe Islands are members of the European Union. Greenland left the European Economic Community in 1986 and the Faroe Islands declined membership in 1973, when Denmark joined.
GDP.
Table showing selected PPP GDPs and growth - 2002 to 2007 est.:
Major companies.
Denmark is home to many multi-national companies, among them: 

</doc>
<doc id="8037" url="http://en.wikipedia.org/wiki?curid=8037" title="Transport in Denmark">
Transport in Denmark

Transport in Denmark is developed and modern. The motorway network now covers 1,111 km while the railway network totals 2,667 km of operational track. The Great Belt Fixed Link (opened in 1997) connecting the islands of Zealand and Funen and the New Little Belt Bridge (opened in 1970) connecting Funen and Jutland have improved the traffic flow across the country on both motorways and rail. The airports of Copenhagen and Billund provide a variety of domestic and international connections while ferries provide services to the Faroe Islands, Greenland, Iceland, Germany, Sweden, Norway and the United Kingdom as well as routes to the Danish islands.
Air.
In 2011, a total of appr. 28 million passengers used Danish airports. 
Copenhagen Airport is the largest airport in Scandinavia, handling approximately 23m passengers per year (2011). It is located at Kastrup, 8 km south-east of central Copenhagen. It is connected by train to Copenhagen Central Station and beyond as well as to Malmö and other towns in Sweden.
For the west of the country, the major airport is Billund (2.7m passengers in 2011) although both Aalborg (1.4m passengers in 2011) and Aarhus (591.000 passengers in 2011) have smaller airports with regular connections to Copenhagen.
List of airports.
Denmark's main airports are:
Other airports include: 
Sea.
Being an island state with a long coastline and always close to the sea, maritime transport has always been important in Denmark. From the primitive dugouts of the Stone Age to the complex designs of the Viking ships in the Viking Age, often built to exactly facilitate large scale cargo and passenger transportation. Denmark also engaged in the large scale cargo freights and slave transports of the European colonization endeavours in the middle ages and had several smaller colonies of its own across the globe.
Today Denmark's ports handle some 48 million passengers and 109 million tonnes of cargo per year.
Passenger traffic.
Passenger traffic is made up partly of ferry crossings within Denmark, partly of international ferry crossings and partly of cruise ship passengers.
Among the most important ports for passenger traffic (thousands of passengers per year in 2007) are:
In 2007, 288 cruise ships visited Copenhagen.
Cargo traffic.
Among the most important ports for cargo traffic (millions of tonnes per year in 2007) are:
Waterways.
Waterways has historically and traditionally been crucial to local transportation in Denmark proper. Especially the Gudenå river-system in central Jutland, has played an important role. The waterways was navigated by wooden barges and later on steamboats. A few historical steamboats are still in operation, like the SS Hjejlen from 1861 at Silkeborg.
There is a 160 km natural canal through the shallow Limfjorden in northern Jutland, linking the North Sea to the Kattegat. 
Many waterways has formerly been redirected and led through manmade canals in the 1900s, but mainly for agricultural purposes and not to facilitate transportation on any major scale. Several cities have manmade canals used for transportation and traffic purposes. Of special mention are the and the Odense Canal, ferrying large numbers of both tourists and local citizens.
Railways.
The largest railway operator in Denmark is Danske Statsbaner (DSB) — Danish State Railways. Arriva operates some routes in Jutland, and several other smaller operators provide local services.
The total length of operational track is 2,667 km, 640 km electrified at 25 kV AC, 946 km double track (2008). 508 km is privately owned and operated. Track is standard gauge.
The railway system is connected to Sweden by bridge in Copenhagen and ferry in Helsingør and Frederikshavn, by land to Germany in Padborg and ferry in Rødby and to Norway by ferry in Hirtshals.
Roads.
The road network in 2008 totalled 73,197 km of paved road, including 1,111 km of motorway. Motorways are toll-free except for the Great Belt Bridge joining Zealand and Funen and the Øresund Bridge linking Copenhagen to Malmö in Sweden.
Cycling.
Bicycling in Denmark is a common and popular utilitarian and recreational activity. Bicycling infrastructure is a dominant feature of both city and countryside infrastructure, with bicycle paths and bicycle ways in many places and an extensive network of bicycle routes, extending more than nationwide. In comparison, Denmark's coastline is . As a unique feature, Denmark has a VIN-system for bicycles which is mandatory by law. Often bicycling and bicycle-culture in Denmark is compared to the Netherlands as a bicycle-nation.
Pipelines.
Figures for 2007:

</doc>
<doc id="8038" url="http://en.wikipedia.org/wiki?curid=8038" title="Danish Defence">
Danish Defence

The armed forces of the Kingdom of Denmark, known as the Danish Defence () is charged with the defence of Denmark and its overseas territories, Greenland and the Faroe Islands.
Queen Margrethe II is the de jure Commander-in-Chief per the Danish constitution, however according to the Danish Defence Law the Minister of Defence serves as the commander of the Danish Defence (through the Chief of Defence and the Defence Command) and the Danish Home Guard (through the Home Guard Command). De facto the Danish Cabinet is the commanding authority of the Defence, though it cannot mobilize the armed forces, for purposes that are not strictly defence oriented, without the consent of parliament.
Denmark also has a concept of "total defence" ().
Purpose and task.
The purpose and task of the armed forces of Denmark is defined in Law no. 122 of February 27, 2001 and in force since March 1, 2001. It defines three purposes and six tasks.
Its primary purpose is to prevent conflicts and war, preserve the sovereignty of Denmark, secure the continuing existence and integrity of the independent Kingdom of Denmark and further a peaceful development in the world with respect to human rights.
Its primary tasks are: NATO participation in accordance with the strategy of the alliance, detect and repel any sovereignty violation of Danish territory (including Greenland and the Faroe Islands), defence cooperation with non-NATO members, especially Central and East European countries, international missions in the area of conflict prevention, crises-control, humanitarian, peacemaking, peacekeeping, participation in "Total Defence" in cooperation with civilian resources and finally maintenance of a sizable force to execute these tasks at all times.
Defence budget.
Since 1988, Danish defence budgets and security policy have been set by multi-year agreements supported by a wide parliamentary majority including government and opposition parties. However, public opposition to increases in defence spending—during a period when economic constraints require reduced spending for social welfare—has created differences among the political parties regarding a broadly acceptable level of new defence expenditure.
The latest Defence agreement ("Defence agreement 2005–2009") was signed June 10, 2004, and calls for a significant re-construction of the entire military. From now about 60% support structure and 40% combat operational capability, it is to be 40% support structure and 60% combat operational capability, i.e. more combat soldiers and fewer "paper"-soldiers.
The reaction speed is increased, with an entire brigade on standby readiness; the military retains the capability to continually deploy 2,000 soldiers in international service or 5,000 over a short time span. The standard mandatory conscription is modified. Generally this means fewer conscripts, less service time for them and only those who choose so, will continue into the reaction force system.
Expenditures.
In 2006 the Danish military budget was the fifth largest single portion of the Danish Government's total budget, significantly less than that of the Ministry of Social Affairs (~110 billion DKK), Ministry of Employment (~67 billion DKK), Ministry of the Interior and Health (~66 billion DKK) and Ministry of Education (~30 billion DKK) and only slightly larger than that of the Ministry of Science, Technology and Innovation (~14 billion DKK). This list lists the complete expenditures for the Danish Ministry of Defence.
The Danish Defence, counting all branches and all departments, itself has an income equal to about 1–5% of its expenditures, depending on the year. They are not deducted in this listing.
Approximately 95% of the budget goes directly to running the Danish military including the Home guard. Depending on year, 50–53% accounts for payment to personnel, roughly 14–21% on acquiring new material, 2–8% for larger ships, building projects or infrastructure and about 24–27% on other items, including purchasing of goods, renting, maintenance, services and taxes.
The remaining 5% is special expenditures to NATO, branch shared expenditures, special services and civil structures, here in including running the Danish Maritime Safety Administration, Danish national rescue preparedness and the Administration of Conscientious Objectors ().
Because Denmark has a small and highly specialized military industry, the vast majority of the Danish Defence's equipment is imported from NATO and the Nordic countries.
Current deployments.
Current deployment of Danish forces:
Conscription.
Technically all Danish 18-year-old males are conscripts (37,897 in 2010), and 53% (2010) were considered suitable for duty. There is no need for all of them so only a few of them serve (5129 in 2010). Ninety-one percent of the conscripts were volunteers. There were additionally 567 female volunteers in 2010, who pass training on "conscript-like" conditions.
Conscripts in the Danish Defence (army, navy and air force) generally serve four months, except:

</doc>
<doc id="8039" url="http://en.wikipedia.org/wiki?curid=8039" title="Foreign relations of Denmark">
Foreign relations of Denmark

The foreign policy of Denmark is based on its identity as a sovereign nation in Europe. As such its primary foreign policy focus is on its relations with other nations as a sovereign independent nation. Denmark has long had good relations with other nations.
It has been involved in coordinating Western assistance to the Baltic states (Estonia, Latvia, and Lithuania). The country is a strong supporter of international peacekeeping. Danish forces were heavily engaged in the former Yugoslavia in the UN Protection Force (UNPROFOR), with IFOR, and now SFOR. Denmark also strongly supported American operations in Afghanistan and has contributed both monetarily and materially to the ISAF. These initiatives are a part of the "active foreign policy" of Denmark. Instead of the traditional adaptative foreign policy of the small country, Denmark is today pursuing an active foreign policy, where human rights, democracy and other crucial values is to be defended actively. In recent years, Greenland and the Faroe Islands have been guaranteed a say in foreign policy issues, such as fishing, whaling and geopolitical concerns.
Following World War II, Denmark ended its two-hundred year long policy of neutrality. Denmark has been a member of NATO since its founding in 1949, and membership in NATO remains highly popular. There were several serious confrontations between the U.S. and Denmark on security policy in the so-called "footnote era" (1982–88), when an alternative parliamentary majority forced the government to adopt specific national positions on nuclear and arms control issues. The alternative majority in these issues was because the Social liberal Party (Radikale Venstre) supported the governing majority in economic policy issues, but was against certain NATO policies and voted with the left in these issues. The conservative led Centre-right government accepted this variety of "minority parliamentarism", that is, without making it a question of the government's parliamentary survival.
With the end of the Cold War, however, Denmark has been supportive of U.S. policy objectives in the Alliance.
Danes have enjoyed a reputation as "reluctant" Europeans. When they rejected ratification of the Maastricht Treaty on 2 June 1992, they put the EC's plans for the European Union on hold. In December 1992, the rest of the EC agreed to exempt Denmark from certain aspects of the European Union, including a common defense, a common currency, EU citizenship, and certain aspects of legal cooperation (the 4 Danish Opt-outs). The Amsterdam Treaty was approved in the referendum of 28 May 1998. In the autumn of 2000, Danish citizens rejected membership of the Euro currency group in a referendum. The Lisbon treaty was ratified by the Danish parliament alone. It was not considered a surrendering of national sovereignty, which would have implied the holding of a referendum according to article 20 of the constitution. Currently, the Danish government wants a referendum on the opt-outs from the EU-treaty, but the prospect of the opt-outs perhaps being rejected does not look appealing. The issue is being postponed for the time being, or until a large coalition of political parties support holding a referendum.

</doc>
<doc id="8041" url="http://en.wikipedia.org/wiki?curid=8041" title="History of Djibouti">
History of Djibouti

Djibouti is a country in the Horn of Africa. It is bordered by Somalia to the southeast, Eritrea and the Red Sea to the northwest, Ethiopia to the west and south, and the Gulf of Aden and Yemen to the northeast.
In antiquity, the territory was part of the Land of Punt. The Djibouti area, along with other localities in the Horn region, was later the seat of the medieval Adal and Ifat Sultanates. In the late 19th century, the colony of French Somaliland was established following treaties signed by the ruling Issa Somali and Afar Sultans with the French. It was subsequently renamed to the French Territory of the Afars and the Issas in 1967. A decade later, the Djiboutian people voted for independence, officially marking the establishment of the Republic of Djibouti.
Prehistory.
The Djibouti area has been inhabited since at least the Neolithic. Pottery predating the mid-2nd millennium has been found at Asa Koma, an inland lake area on the Gobaad Plain. The site's ware is characterized by punctate and incision geometric designs, which bear a similarity to the Sabir culture phase 1 ceramics from Ma'layba in Southern Arabia. Long-horned humpless cattle bones have also been discovered at Asa Koma, suggesting that domesticated cattle was present by around 3,500 years ago. Rock art of what appear to be antelopes and a giraffe are likewise found at Dorra and Balho.
Antiquity.
Between Djibouti City and Loyada are a number of anthropomorphic and phallic stelae. The structures are associated with graves of rectangular shape flanked by vertical slabs, as also found in central Ethiopia. The Djibouti-Loyada stelae are of uncertain age, and some of them are adorned with a T-shaped symbol.
Together with northern Somalia, Eritrea and the Red Sea coast of Sudan, Djibouti is considered the most likely location of the land known to the ancient Egyptians as "Punt" (or "Ta Netjeru", meaning "God's Land"). The old territory's first mention dates to the 25th century BC. The Puntites were a nation of people that had close relations with Ancient Egypt during the times of Pharaoh Sahure and Queen Hatshepsut. They "traded not only in their own produce of incense, ebony and short-horned cattle, but also in goods from other neighbouring regions, including gold, ivory and animal skins." According to the temple reliefs at Deir el-Bahari, the Land of Punt was ruled at that time by King Parahu and Queen Ati.
Adal Sultanate.
Islam was introduced to the area early on from the Arabian peninsula, shortly after the hijra. Zeila's two-mihrab Masjid al-Qiblatayn dates to the 7th century, and is the oldest mosque in the city. In the late 800s, Al-Yaqubi wrote that Muslims were living along the northern Horn seaboard. He also mentioned that the Adal kingdom had its capital in Zeila, a port city in the northwestern Awdal region abutting Djibouti. This suggests that the Adal Sultanate with Zeila as its headquarters dates back to at least the 9th or 10th centuries. According to I.M. Lewis, the polity was governed by local dynasties consisting of Somalized Arabs or Arabized Somalis, who also ruled over the similarly-established Sultanate of Mogadishu in the Benadir region to the south. Adal's history from this founding period forth would be characterized by a succession of battles with neighbouring Abyssinia. At its height, the Adal kingdom controlled large parts of modern-day Djibouti, Somalia, Eritrea and Ethiopia.
Ifat Sultanate.
The Ifat Sultanate was a medieval kingdom in the Horn of Africa. Founded in 1285 by the Walashma dynasty, it was centered in Zeila. Ifat established bases in Djibouti and northern Somalia, and from there expanded southward to the Ahmar Mountains. Its Sultan Umar Walashma (or his son Ali, according to another source) is recorded as having conquered the Sultanate of Shewa in 1285. Taddesse Tamrat explains Sultan Umar's military expedition as an effort to consolidate the Muslim territories in the Horn, in much the same way as Emperor Yekuno Amlak was attempting to unite the Christian territories in the highlands during the same period. These two states inevitably came into conflict over Shewa and territories further south. A lengthy war ensued, but the Muslim sultanates of the time were not strongly unified. Ifat was finally defeated by Emperor Amda Seyon I of Ethiopia in 1332, and withdrew from Shewa.
Egypt Eyalet.
Governor Abou Baker ordered the Egyptian garrison at Sagallo to retire to Zeila. The cruiser Seignelay reached Sagallo shortly after the Egyptians had departed. French troops occupied the fort despite protests from the British Agent in Aden, Major Frederick Mercer Hunter, who dispatched troops to safeguard British and Egyptian interests in Zeila and prevent further extension of French influence in that direction.
On the 14 April 1884 the Commander of the patrol sloop L’Inferent reported on the Egyptian occupation in the Gulf of Tadjoura. The Commander of the patrol sloop Le Vaudreuil reported that the Egyptians were occupying the interior between Obock and Tadjoura. Emperor Johannes IV of Ethiopia signed an accord with Great Britain to cease fighting the Egyptians and to allow the evacuation of Egyptian forces from Ethiopia and the Somali Coast ports.
The Egyptian garrison was withdrawn from Tadjoura. Léonce Lagarde deployed a patrol sloop to Tadjoura the following night. A British warship arrived the next morning to find the French sloop already anchored before the town.
French Somaliland.
It was Rochet d'Hericourt's exploration into Shoa (1839–42) that marked the beginning of French interest in the Djiboutian coast of the Red Sea. Further exploration by Henri Lambert, French Consular Agent at Aden, and Captain Fleuriot de Langle led to a treaty of friendship and assistance between France and the sultans of Raheita, Tadjoura, and Gobaad, from whom the French purchased the anchorage of Obock in 1862.
Growing French interest in the area took place against a backdrop of British activity in Egypt and the opening of the Suez Canal in 1869. Between 1883-87, France signed various treaties with the then ruling Issa Somali and Afar Sultans, which allowed it to expand the protectorate to include the Gulf of Tadjoura. Léonce Lagarde was subsequently installed as the protectorate's governor. In 1894, he established a permanent French administration in the city of Djibouti and named the region "Côte française des Somalis" (French Somaliland), a name which continued until 1967. The territory's border with Ethiopia, marked out in 1897 by France and Emperor Menelik II of Ethiopia, was later reaffirmed by agreements with Emperor Haile Selassie I of Ethiopia in 1945 and 1954.
In 1889, a Russian by the name of Nikolay Ivanovitch Achinov (b. 1856), arrived with settlers, infantry and an Orthodox priest to Sagallo on the Gulf of Tadjoura. The French considered the presence of the Russians as a violation of their territorial rights and dispatched two gunboats. The Russians were bombarded and after some loss of life, surrendered. The colonists were deported to Odessa and the dream of Russian expansion in East Africa came to an end in less than one year.
The administrative capital was moved from Obock in 1896. The city of Djibouti, which had a harbor with good access that attracted trade caravans crossing East Africa, became the new administrative capital. The Franco-Ethiopian railway, linking Djibouti to the heart of Ethiopia, began in 1897 and reached Addis Ababa in June 1917, increasing the volume of trade passing through the port.
World War II.
After the Italian invasion and occupation of Ethiopia in the mid-1930s, constant border skirmishes occurred between French forces in French Somaliland and Italian forces in Italian East Africa. In June 1940, during the early stages of World War II, France fell and the colony was then ruled by the pro-Axis Vichy (French) government.
British and Commonwealth forces fought the neighboring Italians during the East African Campaign. In 1941, the Italians were defeated and the Vichy forces in French Somaliland were isolated. The Vichy French administration continued to hold out in the colony for over a year after the Italian collapse. In response, the British blockaded the port of Djibouti City but it could not prevent local French from providing information on the passing ship convoys. In 1942, about 4,000 British troops occupied the city. A local battalion from French Somaliland participated in the Liberation of Paris in 1944.
Referendums.
In 1958, on the eve of neighboring Somalia's independence in 1960, a referendum was held in Djibouti to decide whether or not to join the Somali Republic or to remain with France. The referendum turned out in favour of a continued association with France, partly due to a combined yes vote by the sizable Afar ethnic group and resident Europeans. There was also reports of widespread vote rigging, with the French expelling thousands of Somalis before the referendum reached the polls. The majority of those who voted no were Somalis who were strongly in favour of joining a united Somalia as had been proposed by Mahmoud Harbi, Vice President of the Government Council. Harbi died in a plane crash two years later under mysterious circumstances.
In 1960, with the fall of the ruling Dini administration, Ali Aref Bourhan, a Harbist politician, assumed the seat of Vice President of the Government Council of French Somaliland, representing the UNI party. He would hold that position until 1966.
That same year, France rejected the United Nations' recommendation that it should grant French Somaliland independence. In August, an official visit to the territory by then French President, General Charles de Gaulle, was also met with demonstrations and rioting. In response to the protests, de Gaulle ordered another referendum.
On 19 March 1967, a second plebiscite was held to determine the fate of the territory. Initial results supported a continued but looser relationship with France. Voting was also divided along ethnic lines, with the resident Somalis generally voting for independence, with the goal of eventual reunion with Somalia, and the Afars largely opting to remain associated with France. However, the referendum was again marred by reports of vote rigging on the part of the French authorities, with some 10,000 Somalis deported under the pretext that they did not have valid identity cards. According to official figures, although the territory was at the time inhabited by 58,240 Somali and 48,270 Afar, only 14,689 Somali were allowed to register to vote versus 22,004 Afar. Somali representatives also charged that the French had simultaneously imported thousands of Afar nomads from neighboring Ethiopia to further tip the odds in their favor, but the French authorities denied this, suggesting that Afars already greatly outnumbered Somalis on the voting lists. Announcement of the plebiscite results sparked civil unrest, including several deaths. France also increased its military force along the frontier.
French Territory of the Afars and Issas.
In 1967, shortly after the second referendum was held, the former "Côte française des Somalis" (French Somaliland) was renamed to "Territoire français des Afars et des Issas". This was both in acknowledgement of the large Afar constituency and to downplay the significance of the Somali composition (the Issa being a Somali sub-clan).
The French Territory of Afars and Issas also differed from French Somaliland in terms of government structure, as the position of Governor General changed to that of High Commissioner. A nine member council of government was also implemented.
With a steadily enlarging Somali population, the likelihood of a third referendum appearing successful had grown even more dim. The prohibitive cost of maintaining the colony, France's last outpost on the continent, was another factor that compelled observers to doubt that the French would attempt to hold on to the territory.
On June 27, 1977, a third vote took place. A landslide 98.8% of the electorate supported disengagement from France, officially marking Djibouti's independence. Hassan Gouled Aptidon, a Somali politician who had campaigned for a yes vote in the referendum of 1958, eventually became the nation's first president (1977–1999).
Djibouti Republic.
In 1981, Aptidon turned the country into a one party state by declaring that his party, the Rassemblement Populaire pour le Progrès (RPP) (People's Rally for Progress), was the sole legal one. A civil war broke out in 1991, between the government and a predominantly Afar rebel group, the Front for the Restoration of Unity and Democracy (FRUD). The FRUD signed a peace accord with the government in December 1994, ending the conflict. Two FRUD members were made cabinet members, and in the presidential elections of 1999 the FRUD campaigned in support of the RPP.
Aptidon resigned as president 1999, at the age of 83, after being elected to a fifth term in 1997. His successor was his nephew, Ismail Omar Guelleh.
On May 12, 2001, President Ismail Omar Guelleh presided over the signing of what is termed the final peace accord officially ending the decade-long civil war between the government and the armed faction of the FRUD, led by Ahmed Dini Ahmed, an Afar nationalist and former Gouled political ally. The peace accord successfully completed the peace process begun on February 7, 2000 in Paris. Ahmed Dini Ahmed represented the FRUD.
In the presidential election held April 8, 2005 Ismail Omar Guelleh was re-elected to a second 6-year term at the head of a multi-party coalition that included the FRUD and other major parties. A loose coalition of opposition parties again boycotted the election. Currently, political power is shared by a Somali president and an Afar prime minister, with an Afar career diplomat as Foreign Minister and other cabinet posts roughly divided. However, Issas are predominate in the government, civil service, and the ruling party. That, together with a shortage of non-government employment, has bred resentment and continued political competition between the Issa Somalis and the Afars. In March 2006, Djibouti held its first regional elections and began implementing a decentralization plan. The broad pro-government coalition, including FRUD candidates, again ran unopposed when the government refused to meet opposition preconditions for participation. In the 2008 elections, the opposition Union for a Presidential Majority (UMP) party boycotted the election, leaving all 65 seats to the ruling RPP. Voter turnout figures were disputed. Guelleh was re-elected in the 2011 presidential election.
Due to its strategic location at the mouth of the Bab el Mandeb gateway to the Red Sea and the Suez Canal, Djibouti also hosts various foreign military bases. Camp Lemonnier is a United States Naval Expeditionary Base, situated at Djibouti-Ambouli International Airport and home to the Combined Joint Task Force - Horn of Africa (CJTF-HOA) of the U.S. Africa Command (USAFRICOM). In 2011, Japan also opened a local naval base staffed by 180 personnel to assist in marine defense. This initiative is expected to generate $30 million in revenue for the Djiboutian government.

</doc>
